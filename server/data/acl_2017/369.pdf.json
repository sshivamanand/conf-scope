{
  "name" : "369.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Morphology Generation for Statistical Machine Translation using Deep Learning Techniques",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Machine Translation (MT) is evolving from different perspectives. One of the most popular paradigms is still Statistical Machine Translation (SMT), which consists in finding the most probable target sentence given the source sentence using probabilistic models based on co-ocurrences. Recently, deep learning techniques applied to natural language processing, speech recognition and image processing and even in MT have reached quite successful results. Early stages of deep learning applied to MT include using neural language modeling for rescoring (Schwenk et al., 2007). Later,\ndeep learning has been integrated in MT in many different steps (Zhand and Zong, 2015). Nowadays, deep learning has allowed to develop an entire new paradigm, which within one-year of development has achieved state-of-the-art results (Jean et al., 2015) for some language pairs.\nIn this paper, we are focusing on a challenging translation task, which is Chinese-to-Spanish. This translation task has the characteristic that we are going from an isolated language in terms of morphology (Chinese) to a fusional language (Spanish). This means that for a simple word in Chinese (e.g. 鼓励 ), the corresponding translation has many different morphology inflexions (e.g. alentar, alienta, alentamos, alientan ...), which depend on the context. It is still difficult for MT in general (no matter which paradigm) to extract information from the source context to give the correct translation.\nWe propose to divide the problem of translation into translation and then a postprocessing of morphology generation. This has been done before, e.g. (Toutanova et al., 2008; Formiga et al., 2013), as we will review in the next section. However, the main contribution of our work is that we are using deep learning techniques in morphology generation. This gives us significant improvements in translation quality.\nThe rest of the paper is organised as follows. Section 2 describes the related work both in morphology generation approaches and in ChineseSpanish translation. Section 3 overviews the phrase-based MT approach together with an explanation of the divide and conquer approach of translating and generating morphology. Section 4 details the architecture of the morphology generation module and it reports the main classification techniques that are used for morphology generation. Section 5 describes the experimental framework. Section 6 reports and discusses both clas-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nsification and translation results, which show significant improvements. Finally, section 7 summarises the main conclusions and further work."
    }, {
      "heading" : "2 Related Work",
      "text" : "In this section we are reviewing the previous related works on morphology generation for MT and on Chinese-Spanish MT approaches.\nMorphology generation There have been many works in morphological generation and some of them are in the context of the application of MT. In this cases, MT is faced in two-steps: first step where the source is translated to a simplified target text that has less morphology variation than the original target; and then, second step, a postprocessing module (morphology generator) adds the proper inflections. To name a few of these works, for example, (Toutanova et al., 2008) build maximum entropy markov models for inflection prediction of stems; (Clifton and Sarkar, 2011) and (Kholy and Habash, 2012) use conditional random fields (CFR) to predict one or more morphological features; and (Formiga et al., 2013) use Support Vector Machines (SVMs) to predict verb inflections. Other related works are in the context of Part-of-Speech (PoS) tagging generation such as (Giménez and Màrquez, 2004) in which a model is trained to predict each individual fragment of a PoS tag by means of machine learning algorithms. The main difference is that in PoS tagging the word itself has information about morphological inflection, whereas in our task, we do not have this information.\nIn this paper, we use deep learning techniques to morphology generation or classification. Based on the fact that Chinese does not have number and gender inflections and Spanish does, (Costajussà, 2015) show that simplification in gender and number has the best trade-off between improving translation and keeping the morphology generation complexity at a low level.\nChinese-Spanish There are few works in Chinese-Spanish MT despite being two of the most spoken languages in the world. Most of these works are based on comparing different pivot strategies like standard cascade or pseudo-corpus (Costa-jussà et al., 2012). Also it is important to mention that, in 2008, there were two tasks organised by the popular IWSLT\nevaluation campaign1 (International Workshop on Spoken Language Translation) between these two languages (Paul, 2008). The first task was based on a direct translation for Chinese-Spanish. The second task provided corpus in Chinese-English and English-Spanish and asked participants to provide Chinese-Spanish translation through pivot techniques. The second task obtained better results than direct translation because of the larger corpus provided. Differently, (Costa-jussà and Centelles, 2016) present the first rule-based MT system for Chinese to Spanish. Authors describe a hybrid method for constructing this system taking advantage of available resources such as parallel corpora that are used to extract dictionaries and lexical and structural transfer rules. Finally, it is worth mentioning that novel successful neural approximations (Jean et al., 2015), already mentioned in the introduction, have not yet achieved state-of-the-art results for this language pair (Costa-jussà et al., 2017)."
    }, {
      "heading" : "3 Machine Translation Architecture",
      "text" : "In this section, we review the baseline system which is a standard phrase-based MT system and explain the architecture that we are using by dividing the problem of translation into: morphologically simplified translation and morphology generation."
    }, {
      "heading" : "3.1 Phrase-based MT baseline system",
      "text" : "The popular phrase-based MT system (Koehn et al., 2003) focuses on finding the most probable target text given a source text. In the last 20 years, the phrase-based system has dramatically evolved introducing new techniques and modifying the architecture; for example, replacing the noisychannel for the log-linear model which combines a set of feature functions in the decoder, including the translation and language model, the reordering model and the lexical models. There is a widely used open-source software, Moses (Koehn et al., 2007), which englobes a large community that helps in the progress of the system. As a consequence, phrase-based MT is a commoditized technology used at the academic and commercial level. However, there are still many challenges to solve, such as morphology generation.\n1http://iwslt2010.fbk.eu\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299"
    }, {
      "heading" : "3.2 Divide and conquer MT architecture: simplified translation and morphology generation",
      "text" : "Morphology generation is not always achieved in the standard phrase-based system. This may be due to the fact that phrase-based MT uses a limited source context information to translate. Therefore, we are proposing to follow a similar strategy to previous works (Toutanova et al., 2008; Formiga et al., 2013), where authors do a first translation from source to a morphology-based simplified target and then, use the morphology generation module that transforms the simplified translation into the full form output."
    }, {
      "heading" : "4 Morphological Generation Module",
      "text" : "In order to design the morphology generation module, we have to decide the morphology simplification we are applying to the translation. Since we are focusing on Chinese-to-Spanish task and based on (Costa-jussà, 2015), the simplification which achieves the best trade-off among highest translation gain and lowest complexity of morphological generation is the simplification in number and gender. Table 1 shows examples of this simplification. The main challenge of this task is that number and gender are generated from words where this inflection information (both number and gender) has been removed beforehand.\nWith these results at hand, we propose an architecture of the morphology generation module, which is language independent and it is easily generalizable to other simplification schemes.\nThe morphology generation procedure is summarised as follows and further detailed in the next subsections.\n• Feature selection. We have investigated different set of features including information from both source and target languages.\n• Classification. We propose a new deep learning classification architecture composed of different layers.\n• Rescoring and rules. We generate different alternatives of the classification output and rerank them using a language model. After, we use hand-crafted rules that allow to solve some specific problems.\nThis procedure is depicted in Figure 1, in which we can see that each of the above processes gener-\nFigure 1: Block Diagram for Morphology Generation\nates the needed input for the next step. Figure also shows in red the main subprocesses that have been developed on this work."
    }, {
      "heading" : "4.1 Feature selection",
      "text" : "We propose to compare several features for recovering morphological information. Given that both Chinese and simplified Spanish languages do not contain explicit morphology information, we start by simply using windows of words as source of information. We follow Collobert’s approach in which each word is represented by a fixed size window of words in which the central element is the one to classify (Collobert et al., 2011).\nIn our case, we experiment with three different inputs: (1) Chinese window; (2) simplified Spanish window; (3) Spanish window adding information about its correspondant word in Chinese, i.e. information about pronouns and the number of characters in the word. The main advantage of the second one is that it is not dependant on the alignment file generated during translation.\nOur classifiers did not have to train all types of words. Some types of words, such as prepositions (a, ante, cabo, de...), do not have gender or number. Therefore our system was trained using only determiners, adjectives, verbs, pronouns and nouns which are the ones that present morphology\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nEsnum decidir[VMIP3N0] examinar[VMN0000] el[DA0MN0] c uestión[NCFN000] en[SPS00] el[DA0MN0] perı́odo[NCM N000] de[SPS00] sesión[NCFN000] el[DA0MN0] tema[NCMN000] titular [AQ0MN0] “[Fp] cuestión[NCFN000] relativo[AQ0FN0] a[SPS00] el[DA0MN0] derecho[NCMN000] humano[AQ0MN0] “[Fp] .[Fp] Esgen decidir[VMIP3S0] examinar[VMN0000] el[DA0GS0] cuestión [NCGS000] en[SPS00] el[DA0GS0] perı́odo[NCGS000] de [SPS00] sesión[NCGS000] el[DA0GS0] tema[NCGS000] titular [AQ0GS0] “[Fp] cuestión[NCGS000] relativo[AQ0GS0] a[SPS00] el[DA0GS0] derecho[NCGS000] humano[AQ0GS0] “[Fp] .[Fp] Esnumgen decidir[VMIP3N0] examinar[VMN0000] el[DA0GN0 ] cuestión[NCGN000] en[SPS00] el[DA0GN0] perı́odo[NCGN000] de[SPS00] sesión[NCGN000] el[DA0GN0] tema[NCGN000] titular [AQ0GN0] “[Fp] cuestión[NCGN000] relativo[AQ0GN00 ] a[SPS00] el[DA0GN0] derecho[NCGN000] humano[AQ0GN0] “[Fp] .[Fp] Es Decide examinar la cuestión en el perı́odo de sesiones el tema titulad o “ Cuestiones relativas a los derechos humanos ” .\nTable 1: Example of Spanish simplification into number, gender and both\n.\nvariations in gender or number. However, note that all types of words are used in the windows."
    }, {
      "heading" : "4.2 Classification architecture",
      "text" : "Description We propose to train two different models: one to retrieve gender and another to retrieve number. Each model decides among three different classes. Classes for gender classifier are masculine (M ), femenine (F ) and none (N ); and classes for number classifier are singular (S), plural (P ) and none (N ) 2. Again, we inspire our architecture in previous Collobert’s proposal and we modify it by adding a recurrent neural network. This recurrent neural network is relevant because it keeps information about previous elements in a sequence and, in our classification problem, context words are very relevant. As a recurrent neural network, we use a Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) that is proven efficient to deal with sequence NLP challenges (Sutskever et al., 2014). This kind of recurrent neural network is able to maintain information for several elements in the sequence and to forget it when needed. Figure 2 shows an overview of the different layers involved in the final classification architecture, which are detailed as follows:\nEmbedding. We represent each word as its index in the vocabulary, i.e. every word is represented as one discrete value: E(w) = W,W ∈ Rd, w being the index of the word in the sorted vocabulary and d the user chosen size of the array. Then, each word is represented as a numeric array and each window is a matrix.\nConvolutional. We add a convolutional neural network. This step allows the system to detect some common patterns between the different\n2None means that there is no need to specify gender or number information because the word is invariant in these terms. This happens for types of words (determiners, nouns, verbs, pronouns and adjectives) that can have gender and number in other cases.\nwords. This layer’s input consists in W l matrix of multidimensional arrays of size n · d, where n is the window length (in words) and d is the size of the array created by the previous embedding layer. This layer’s output is a matrix of the same size as the input.\nMax Pooling. This layer allows to extract most relevant features from the input data and reduces feature vectors to half.\nLSTM. Each feature array is treated individually, generating a fixed size representation hi of the ith word using information of all the previous words (in the sequence). This layer’s output, h, is the result of the last element of the sequence using information from all previous words.\nSigmoid. This layer smoothes results obtained by previous layer and compresses results to the interval [−1, 1]. This layer’s input is a fixed size vector of shape 1 ·n where n is the number of neurons in the previous LSTM layer. This layer’s output is a vector of length c equal to the number of classes to predict.\nSoftmax. This layer allows to show results as probabilities by ensuring that the returned value of each class belongs to the [0, 1) interval and all classes add up 1.\nMotivation Our input data is PoS tagged and morphogically simplified before the classification architecture which largely reduces the information that can be extracted from individual words in the vocabulary. In addition, we can encounter out-ofvocabulary words for which no morphological information can be extracted.\nThe main source of information available is the context in which the word can be found in the sentence. Considering the window as a sequence enforces the behaviour a human would have while reading the sentence. The information of a word consists in itself and the words that surround it. Sometimes information preceeds the word and sometimes information is after the word. Words\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nFigure 2: Neural network overview.\n(like adjetives), which are modifying or complementing another word, generally take information from preceeding words. For example, in the sequence casa blanca, the word blanca could also be blanco, blancos or blancas but because noun and adjective are required to have gender and number agreement, the femenine word casa forces the femenine for blanca. While, for example, determiners usually take information from posterior words. This fact motivates that the word to classify has to be placed at the center of the window.\nFinally, given that we rely only on the context information since words themselves may not have any information, makes the recurrent neural network a key element in our architecture. The output h of the layer can be considered a context vector of the whole window maintaining information of all the previously encountered words (in the same window)."
    }, {
      "heading" : "4.3 Rescoring and rules",
      "text" : "At this point in the pipeline, we have two models (gender and number) that allow us to generate the full Part-of-Speech (PoS) tag by combining the best results of both classifiers.\nHowever, in order to improve the overall performance, we add a rescoring step followed by some hand-crafted rules. To generate the different alternatives, we represent every sentence as a graph (see Figure 3), with the following properties:\n• Each word is represented as a layer of the graph and each node represents a class of the\nclassification model.\n• A node only has edges with all the nodes of the next layer. This way we force the original sentence order.\n• An edge’s weight is the probability given by the classification model.\n• Each accepted path in the graph starts in the first layer and ends in the last one. This acyclic structure finds the best path in linear time, due to the fact that it goes through all layers and it picks the node with the greatest weight. One layer can have either 1 element (the word does not need to be classified, e.g. prepositions) or 3 elements (the word needs to be classified among the three number or gender categories).\n• Add the weight of a previously trained target language model.\nWe used Yen’s algorithm (Yen, 1971) to find the best path, which has an associated cost of O(KN2logN), being K the number of paths to find.\nSee pseudo code in Algorithm 1, where A is the set paths chosen in the graph. The algorithm ends when this A set contains K paths or no further paths available to explore. B contains all suboptimal paths that can be elected in future iterations.\nThere are two special cases that the models were not able to treat and we apply specific rules: (1) conjunctions y and o are replaced by e and u if they are placed in front of vowels. This could not be generated during translation because both words share the same tag and lemma; (2) verbs with a pronoun as a suffix, producirse, second to\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nlast syllabe stretched (palabras llanas) and ending in a vowel are not accentuated. However, after adding the suffix, these words should be accentuated because they become palabras esdrújulas, which happen to be always accentuated.\nData: G Graph of the sentence, K Result: best k paths in G initialization; A[0] = bestPath(G,0,final); B = [] ; i = 0; for i <K do\nfor i in range(0, len(A[K-1])-1) do spurNode = A[K-1][i]; root = A[K-1][0;i]; for path in A do\nif root = path[0:i] then remove edge(i-1,i) from G;\nend end for node in root and node != spurNode do\nremoves node node from G; end spurPath = bestPath(G,spurnode, final) totalPath = root + spurPath; B.append(totalPath); restore edges from G; restore nodes from G; if B is empty then\nbreak; end B.sort(); A.append(B[0]); B.pop();\nend end\nAlgorithm 1: Pseudo-code for k-best paths generation."
    }, {
      "heading" : "5 Experimental framework",
      "text" : "In this section, we describe the data used for experimentation together with the corresponding preprocessing. In addition, we detail chosen parameters for the MT system and the classification algorithm."
    }, {
      "heading" : "5.1 Data and preprocessing",
      "text" : "One of the main contributions of this work is using the Chinese-Spanish language pair. In the last years, there has appeared more and more resources for this language pair available in (Ziemski et al.,\nL Set S W V ES Train Small 58.6K 2.3M 22.5K Large 3.0M 51.7M 207.5K\nDevelopment 990 43.4K 5.4k Test 1K 44.2K 5.5K\nZH Train Small 58.6K 1.6M 17.8K Large 3.0M 43.9M 373.5K\nDevelopment 990 33K 3.7K Test 1K 33.7K 3.8K\nTable 2: Corpus Statistics. Number of sentences (S),words (W), vocabulary (V). M stands for millions and K stands for thousands.\n2016) or from TAUS corporation3. Therefore, differently from previous works on this language pair, we can test our approach in both a small and large data sets.\n• A small training corpus by using the United Nations Corpus (UN) (Rafalovitch and Dale, 2009).\n• A large training corpus by using, in addition to the UN corpus, the TAUS corpus, the Bible corpus (Chew et al., 2006) and the BTEC (Basic Traveller Expressions Corpus) (Takezawa, 2006). The TAUS corpus is around 2,890,000 sentences, the Bible corpus about 30,000 sentences and the BTEC corpus about 20,000 sentences.\nCorpus statistics are shown in Table 2. Development and test sets are taken from UN corpus.\nCorpus preprocessing consisted in tokenization, filtering empty sentences and longer than 50 words, Chinese segmentation by means of the ZhSeg (Dyer, 2016), Spanish lowercasing, filtering pairs of sentences with more than 10% of nonChinese characters in the Chinese side and more than 10% of non-Spanish characters in the Spanish side. Spanish PoS tagging was done using Freeling (Padró and Stanilovsky, 2012). All chunking or name entity recognition was disabled to preserve the original number of words."
    }, {
      "heading" : "5.2 MT Baseline",
      "text" : "Moses has been trained using default parameters, which include: grow-diag-final word alignment symmetrization, lexicalized reordering, relative frequencies (conditional and posterior probabilities) with phrase discounting, lexical weights, phrase bonus, accepting phrases up to length 10, 5-gram language model with kneser-ney smoothing, word bonus and MERT optimisation."
    }, {
      "heading" : "5.3 Classification parameters",
      "text" : "To generate the classification architecture we used the library keras (Chollet, 2015) for creating and ensambling the different layers. Using NVIDIA GTX Titan X GPUs with 12GB of memory and 3072 CUDA Cores, each classifier is trained on aproximately 1h and 12h for the small and large corpus, respectively.\n3http://www.taus.net\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nRegarding classification parameters, experimentation has shown that number and gender classification tasks have different requirements. Table 3 summarizes these parameters. The best window size is 9 and 7 words for number and gender, respectively. In both cases increasing this size lowers the accuracy of the system. The vocabulary size is fixed as a trade-off between giving enough information to the system to perform the classification while removing enough words to train the classifier for unknown words. The embedding size of 128 results in stable training, while further increasing this value augmented the training time and hardware cost. The filter size in the convolutional layer reached best results when it was slightly smaller than the window size, being 7 and 5 the best values for number and gender classification, respectively. Finally, increasing LSTM nodes up to 70 improved significantly for both classifiers.\nTable 3: Values of the different parameters of the classifiers\nParameter Small Large Num Gen Num Gen Window size 9 7 9 7 Vocabulary size 7000 9000 15000 15000 Embedding 128 128 128 128 Filter size 7 5 7 5 LSTM nodes 70 70 70 70\nFor windows, we only used the simplified Spanish translation. In Table 4 we can see that testing different sources of information with the classifier of number for the small corpus. Adding Chinese has a negative effect in the classifier accuracy."
    }, {
      "heading" : "5.4 Rescoring and Full form generation",
      "text" : "As a rescoring tool, we use the one available in Moses 4. We trained a standard n-gram language model with the SRILM toolkit (Stolcke, 2002).\nIn order to generate the final full form we use the full PoS tag, generated from the postprocessing step, and the lemma, taken from the morphology-simplified translation output. Then, we use the vocabulary and conjugations rules pro-\n4https://github.com/mosessmt/mosesdecoder/tree/master/scripts/nbest-rescore\nTable 4: Accuracy of the classifier of number using different sources of information.\nFeatures Accuracy (%) Chinese window 72 Spanish window 93,4 Chinese + Spanish window 86\nvided by Freeling. Freeling’s coverage raises the 99%. When a word is not found in the dictionary, we test all gender and/or number inflections in descendant order of probability until a match is found. If none matched, the lemma is used as translation, which usually happens only in the case of cities or demonyms."
    }, {
      "heading" : "6 Evaluation",
      "text" : "In this section we discuss the results obtained both in classification and in the final translation task.\nTable 5 shows results for the classification task both number and gender and with the different corpus sets. We have contrasted our proposed classification architecture based on neural networks with standard machine learning techniques such as linear, cuadratic and sigmoid kernels SVMs (Cortes and Vapnik, 1995), random forests (Breiman, 2001), convolutional(Fukushima, 1980) and LSTM(Hochreiter and Schmidhuber, 1997) neural networks (NN). All algorithms were tested using features and parameters described in previous sections with the exception of random forests in which we added the one hot encoding representation of the words to the features.\nWe observe that our proposed architecture achieves by large the best results in all tasks. It is also remarkable that the accuracy is lower using the bigger corpus, this is due to the fact that the small set consisted in texts of the same domain and the vocabulary had a better representation of specific words such as country names.\nTable 5: Classification results. In bold, best results. Num stands for Number and Gen, for Gender\nAlgorithm Small Large Num Gen Num Gen Naive Bayes 61.3 53.5 61.3 53.5 Lineal kernel SVM 68.1 71.7 65.8 69.3 Cuadratic kernel SVM 77.8 81.3 77.6 82.7 Sigmoid kernel SVM 83,1 87.4 81.5 84.2 Random Forest 81.6 91.8 77.8 88.1 Convolutional NN 81.3 93.9 83.9 94.2 LSTM NN 68.1 73.3 70.8 71.4 CNN + LSTM 93.7 98.4 88.9 96.1\nTabla 6 shows translation results. We show both the Oracle and the result in terms of METEOR (Banerjee and Lavie, 2005). We observe improvement in most cases (when classifying number, gender, both and rescoring), but best results are obtained when classifying number and gender and rescoring number in the large corpus, obtaining a\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nTable 6: METEOR results. In bold, best results. Num stands for Number and Gen, for Gender\nSet System UN Oracle Result Small Baseline - 55.29 +Num 55.60 55.35 +Gen 55.45 55.39 +Num&Gen 56.81 55.48 +Num&Gen +Rescoring(Num&Gen) - 54.91 +Num&Gen +Rescoring(Num) - 55.56 Large Baseline - 56.98 +Num 58.87 57.51 +Gen 57.56 57.32 +Num&Gen 62.41 57.13 +Num&Gen Rescoring - 57.74\ngain up to +0.7 METEOR. Rescoring step improves final results. Note that rescoring was only applied to number classification because gender classification model has a low classification error (bellow 2%) which makes it harder to further decrease it. Additionally, gender and number classification scores are not be comparable and not easily integrated in Yen’s algorithm."
    }, {
      "heading" : "7 Conclusions",
      "text" : "Chinese-to-Spanish translation task is challenging, specially because of Spanish being morphologically rich compared to Chinese. Main contributions of this paper include correctly de-coupling the translation and morphological generation tasks and proposing a new classification architecture, based on deep learning, for number and gender.\nStandard phrase-based MT procedure is changed to first translating into a morphologically simplified target (in terms of number and gender); then, introducing the classification algorithm, based on a new proposed neural network-based architecture, that retrieves the simplified morphology; and composing the final full form by using the standard Freeling dictionary.\nResults of the proposed neural-network architecture in the classification task compared to standard algorithms (SVM or random forests) are significantly better and results in the translation task achieve up to 0.7 METEOR improvement. As further work, we intend to further simplify morphology and extend the scope of the classification."
    }, {
      "heading" : "Acknowledgements",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Random forests",
      "author" : [ "Leo Breiman." ],
      "venue" : "Machine learning, 45(1):5–32.",
      "citeRegEx" : "Breiman.,? 2001",
      "shortCiteRegEx" : "Breiman.",
      "year" : 2001
    }, {
      "title" : "Evaluation Of The Bible As A Resource For Cross-language Informatio n Retrieval",
      "author" : [ "Peter A Chew", "Steve J Verzi", "Travis L Bauer", "Jonathan T McClain." ],
      "venue" : "Proceedings of the Workshop on Multilingual Language Resources a nd Interoperability,",
      "citeRegEx" : "Chew et al\\.,? 2006",
      "shortCiteRegEx" : "Chew et al\\.",
      "year" : 2006
    }, {
      "title" : "Combining morpheme-based machine translation with postprocessing morpheme prediction",
      "author" : [ "Ann Clifton", "Anoop Sarkar." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-",
      "citeRegEx" : "Clifton and Sarkar.,? 2011",
      "shortCiteRegEx" : "Clifton and Sarkar.",
      "year" : 2011
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of Machine Learning Research, 12(Aug):2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Supportvector networks",
      "author" : [ "Corinna Cortes", "Vladimir Vapnik." ],
      "venue" : "Machine learning, 20(3):273–297.",
      "citeRegEx" : "Cortes and Vapnik.,? 1995",
      "shortCiteRegEx" : "Cortes and Vapnik.",
      "year" : 1995
    }, {
      "title" : "Description of the chinese-to-spanish rule-based machine translation system developed using a hybrid combination of human annotation and statistical techniques",
      "author" : [ "Marta R. Costa-jussà", "Jordi Centelles." ],
      "venue" : "ACM Transactions on Asian and Low-",
      "citeRegEx" : "Costa.jussà and Centelles.,? 2016",
      "shortCiteRegEx" : "Costa.jussà and Centelles.",
      "year" : 2016
    }, {
      "title" : "Evaluating indirect strategies for chinese-spanish statistical machine translation",
      "author" : [ "Marta R. Costa-jussà", "Carlos A. Henrı́quez Q", "Rafael E. Banchs" ],
      "venue" : "Journal Of Artificial Intelligence Research,",
      "citeRegEx" : "Costa.jussà et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Costa.jussà et al\\.",
      "year" : 2012
    }, {
      "title" : "Chinese-spanish neural machine translation enhanced with character and word bitmap fonts",
      "author" : [ "Marta R. Costa-jussà", "David Aldón", "José A.R. Fonollosa." ],
      "venue" : "Machine Translation, page Accepted for publication.",
      "citeRegEx" : "Costa.jussà et al\\.,? 2017",
      "shortCiteRegEx" : "Costa.jussà et al\\.",
      "year" : 2017
    }, {
      "title" : "Ongoing study for enhancing chinese-spanish translation with morphology strategies",
      "author" : [ "Marta R. Costa-jussà." ],
      "venue" : "Proc. of the ACL Workshop on Hybrid Approaches to Translation, Beijing.",
      "citeRegEx" : "Costa.jussà.,? 2015",
      "shortCiteRegEx" : "Costa.jussà.",
      "year" : 2015
    }, {
      "title" : "Neocognitron: A selforganizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
      "author" : [ "Kunihiko Fukushima." ],
      "venue" : "Biological cybernetics, 36(4):193–202.",
      "citeRegEx" : "Fukushima.,? 1980",
      "shortCiteRegEx" : "Fukushima.",
      "year" : 1980
    }, {
      "title" : "Svmtool: A general pos tagger generator based on support vector machines",
      "author" : [ "Jesús Giménez", "Lluı́s Màrquez" ],
      "venue" : "Proceedings of the 4th International Conference on Language Resources and Evaluation. Citeseer",
      "citeRegEx" : "Giménez and Màrquez.,? \\Q2004\\E",
      "shortCiteRegEx" : "Giménez and Màrquez.",
      "year" : 2004
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Montreal neural machine translation systems for wmt15",
      "author" : [ "Sebastien Jean", "Orhan Firat", "Kyunghun Cho", "Roland Memĩsevic", "Yoshua Bengio." ],
      "venue" : "Proc. of the 10th Workshop on Statistical Machine Translation, Lisbon.",
      "citeRegEx" : "Jean et al\\.,? 2015",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Rich morphology generation using statistical machine translation",
      "author" : [ "Ahmed El Kholy", "Nizar Habash." ],
      "venue" : "Proceedings of the Seventh International Natural Language Generation Conference, INLG ’12, pages 90–94, Stroudsburg, PA, USA. Associ-",
      "citeRegEx" : "Kholy and Habash.,? 2012",
      "shortCiteRegEx" : "Kholy and Habash.",
      "year" : 2012
    }, {
      "title" : "Statistical Phrase-Based Translation",
      "author" : [ "Philipp Koehn", "Franz Joseph Och", "Daniel Marcu." ],
      "venue" : "Proc. of the ACL.",
      "citeRegEx" : "Koehn et al\\.,? 2003",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2003
    }, {
      "title" : "Overview of the iwslt 2008 evaluation campaign",
      "author" : [ "Michael Paul." ],
      "venue" : "Proc. of the International Workshop on Spoken Language Translation, pages 1–17, Hawaii, USA.",
      "citeRegEx" : "Paul.,? 2008",
      "shortCiteRegEx" : "Paul.",
      "year" : 2008
    }, {
      "title" : "United Nations General Assembly Resolutions: A SixLanguage Parallel Corpus",
      "author" : [ "Alexandre Rafalovitch", "Robert Dale." ],
      "venue" : "Proc. of the MT Summit XII, pages 292–299, Ottawa.",
      "citeRegEx" : "Rafalovitch and Dale.,? 2009",
      "shortCiteRegEx" : "Rafalovitch and Dale.",
      "year" : 2009
    }, {
      "title" : "Smooth bilingual ngram translation",
      "author" : [ "Holger Schwenk", "Marta R. Costa-jussà", "José A.R. Fonollosa." ],
      "venue" : "Proc. of the EMNLP, Prague. A. Stolcke. 2002. SRILM - an extensible language modeling toolkit. In 7th International Conference",
      "citeRegEx" : "Schwenk et al\\.,? 2007",
      "shortCiteRegEx" : "Schwenk et al\\.",
      "year" : 2007
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "CoRR.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Multilingual spoken language corpus development for communicatio n research",
      "author" : [ "Toshiyuki Takezawa." ],
      "venue" : "Chinese Spoken Language Processing, 5th International Symposium, ISCSLP 2006, Singapore, December 13-16, 2006, Proceedings, pages",
      "citeRegEx" : "Takezawa.,? 2006",
      "shortCiteRegEx" : "Takezawa.",
      "year" : 2006
    }, {
      "title" : "Applying morphology generation models to machine translation",
      "author" : [ "Kristina Toutanova", "Hisami Suzuki", "Achim Ruopp." ],
      "venue" : "Proc. of the conference of the Association for Computational Linguistics and Human Language Technology (ACL-HLT), pages",
      "citeRegEx" : "Toutanova et al\\.,? 2008",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2008
    }, {
      "title" : "Finding the k shortest loopless paths in a network",
      "author" : [ "Jin Y Yen." ],
      "venue" : "management Science, 17(11):712– 716.",
      "citeRegEx" : "Yen.,? 1971",
      "shortCiteRegEx" : "Yen.",
      "year" : 1971
    }, {
      "title" : "Deep neural networks in machine translation: An overview",
      "author" : [ "Jiajun Zhand", "Chengqing Zong." ],
      "venue" : "IEEE Intelligent Systems, pages 1541–1672.",
      "citeRegEx" : "Zhand and Zong.,? 2015",
      "shortCiteRegEx" : "Zhand and Zong.",
      "year" : 2015
    }, {
      "title" : "The united nations parallel corpus v1.0",
      "author" : [ "Michał Ziemski", "Marcin Junczys-Dowmunt", "Bruno Pouliquen" ],
      "venue" : "In Proceedings of the Tenth International Conference on Language Re sources and Evaluation (LREC 2016),",
      "citeRegEx" : "Ziemski et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ziemski et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Early stages of deep learning applied to MT include using neural language modeling for rescoring (Schwenk et al., 2007).",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 23,
      "context" : "Later, deep learning has been integrated in MT in many different steps (Zhand and Zong, 2015).",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 13,
      "context" : "Nowadays, deep learning has allowed to develop an entire new paradigm, which within one-year of development has achieved state-of-the-art results (Jean et al., 2015) for some language pairs.",
      "startOffset" : 146,
      "endOffset" : 165
    }, {
      "referenceID" : 21,
      "context" : "(Toutanova et al., 2008; Formiga et al., 2013), as we will review in the next section.",
      "startOffset" : 0,
      "endOffset" : 46
    }, {
      "referenceID" : 21,
      "context" : "To name a few of these works, for example, (Toutanova et al., 2008) build maximum entropy markov models for inflection prediction of stems; (Clifton and Sarkar, 2011) and (Kholy and Habash, 2012) use conditional random fields (CFR) to predict one or more morphological features; and (Formiga et al.",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 3,
      "context" : ", 2008) build maximum entropy markov models for inflection prediction of stems; (Clifton and Sarkar, 2011) and (Kholy and Habash, 2012) use conditional random fields (CFR) to predict one or more morphological features; and (Formiga et al.",
      "startOffset" : 80,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : ", 2008) build maximum entropy markov models for inflection prediction of stems; (Clifton and Sarkar, 2011) and (Kholy and Habash, 2012) use conditional random fields (CFR) to predict one or more morphological features; and (Formiga et al.",
      "startOffset" : 111,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "Other related works are in the context of Part-of-Speech (PoS) tagging generation such as (Giménez and Màrquez, 2004) in which a model is trained to predict each individual fragment of a PoS tag by means of machine learning algorithms.",
      "startOffset" : 90,
      "endOffset" : 117
    }, {
      "referenceID" : 7,
      "context" : "Most of these works are based on comparing different pivot strategies like standard cascade or pseudo-corpus (Costa-jussà et al., 2012).",
      "startOffset" : 109,
      "endOffset" : 135
    }, {
      "referenceID" : 16,
      "context" : "Also it is important to mention that, in 2008, there were two tasks organised by the popular IWSLT evaluation campaign1 (International Workshop on Spoken Language Translation) between these two languages (Paul, 2008).",
      "startOffset" : 204,
      "endOffset" : 216
    }, {
      "referenceID" : 6,
      "context" : "Differently, (Costa-jussà and Centelles, 2016) present the first rule-based MT system for Chinese to Spanish.",
      "startOffset" : 13,
      "endOffset" : 46
    }, {
      "referenceID" : 13,
      "context" : "Finally, it is worth mentioning that novel successful neural approximations (Jean et al., 2015), already mentioned in the introduction, have not yet achieved state-of-the-art results for this language pair (Costa-jussà et al.",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 8,
      "context" : ", 2015), already mentioned in the introduction, have not yet achieved state-of-the-art results for this language pair (Costa-jussà et al., 2017).",
      "startOffset" : 118,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : "The popular phrase-based MT system (Koehn et al., 2003) focuses on finding the most probable target text given a source text.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 21,
      "context" : "Therefore, we are proposing to follow a similar strategy to previous works (Toutanova et al., 2008; Formiga et al., 2013), where authors do a first translation from source to a morphology-based simplified target and then, use the morphology generation module that transforms the simplified translation into the full form output.",
      "startOffset" : 75,
      "endOffset" : 121
    }, {
      "referenceID" : 9,
      "context" : "Since we are focusing on Chinese-to-Spanish task and based on (Costa-jussà, 2015), the simplification which achieves the best trade-off among highest translation gain and lowest complexity of morphological generation is the simplification in number and gender.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "We follow Collobert’s approach in which each word is represented by a fixed size window of words in which the central element is the one to classify (Collobert et al., 2011).",
      "startOffset" : 149,
      "endOffset" : 173
    }, {
      "referenceID" : 12,
      "context" : "As a recurrent neural network, we use a Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) that is proven efficient to deal with sequence NLP challenges (Sutskever et al.",
      "startOffset" : 70,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : "As a recurrent neural network, we use a Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) that is proven efficient to deal with sequence NLP challenges (Sutskever et al., 2014).",
      "startOffset" : 167,
      "endOffset" : 191
    }, {
      "referenceID" : 22,
      "context" : "We used Yen’s algorithm (Yen, 1971) to find the best path, which has an associated cost of O(KN2logN), being K the number of paths to find.",
      "startOffset" : 24,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : "• A small training corpus by using the United Nations Corpus (UN) (Rafalovitch and Dale, 2009).",
      "startOffset" : 66,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "• A large training corpus by using, in addition to the UN corpus, the TAUS corpus, the Bible corpus (Chew et al., 2006) and the BTEC (Basic Traveller Expressions Corpus) (Takezawa, 2006).",
      "startOffset" : 100,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : ", 2006) and the BTEC (Basic Traveller Expressions Corpus) (Takezawa, 2006).",
      "startOffset" : 58,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "We have contrasted our proposed classification architecture based on neural networks with standard machine learning techniques such as linear, cuadratic and sigmoid kernels SVMs (Cortes and Vapnik, 1995), random forests (Breiman, 2001), convolutional(Fukushima, 1980) and LSTM(Hochreiter and Schmidhuber, 1997) neural networks (NN).",
      "startOffset" : 178,
      "endOffset" : 203
    }, {
      "referenceID" : 1,
      "context" : "We have contrasted our proposed classification architecture based on neural networks with standard machine learning techniques such as linear, cuadratic and sigmoid kernels SVMs (Cortes and Vapnik, 1995), random forests (Breiman, 2001), convolutional(Fukushima, 1980) and LSTM(Hochreiter and Schmidhuber, 1997) neural networks (NN).",
      "startOffset" : 220,
      "endOffset" : 235
    }, {
      "referenceID" : 10,
      "context" : "We have contrasted our proposed classification architecture based on neural networks with standard machine learning techniques such as linear, cuadratic and sigmoid kernels SVMs (Cortes and Vapnik, 1995), random forests (Breiman, 2001), convolutional(Fukushima, 1980) and LSTM(Hochreiter and Schmidhuber, 1997) neural networks (NN).",
      "startOffset" : 250,
      "endOffset" : 267
    }, {
      "referenceID" : 12,
      "context" : "We have contrasted our proposed classification architecture based on neural networks with standard machine learning techniques such as linear, cuadratic and sigmoid kernels SVMs (Cortes and Vapnik, 1995), random forests (Breiman, 2001), convolutional(Fukushima, 1980) and LSTM(Hochreiter and Schmidhuber, 1997) neural networks (NN).",
      "startOffset" : 276,
      "endOffset" : 310
    }, {
      "referenceID" : 0,
      "context" : "We show both the Oracle and the result in terms of METEOR (Banerjee and Lavie, 2005).",
      "startOffset" : 58,
      "endOffset" : 84
    } ],
    "year" : 2017,
    "abstractText" : "Morphology in unbalanced languages remains a big challenge in the context of machine translation. In this paper, we propose to de-couple machine translation from morphology generation in order to better deal with the problem. We investigate the morphology simplification with a reasonable trade-off between expected gain and generation complexity. For the Chinese-Spanish task, optimum morphological simplification is in gender and number. For this purpose, we design a new classification architecture which, compared to other standard machine learning techniques, obtains the best results. This proposed neural-based architecture consists of several layers: an embedding, a convolutional followed by a recurrent neural network and, finally, ends with sigmoid and softmax layers. We obtain classification results over 98% accuracy in gender classification, over 93% in number classification, and an overall translation improvement of 0.7 METEOR.",
    "creator" : "TeX"
  }
}