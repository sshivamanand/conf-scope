{
  "name" : "150.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Deep Character-Level Neural Machine Translation By Learning Morphology",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural machine translation (NMT) attempts to build a single large neural network that reads a sentence and outputs a translation (Sutskever et al., 2014). Most of the extant neural machine translations models belong to a family of word-level encoder-decoders (Sutskever et al., 2014; Cho et al., 2014). Recently, Bahdanau et al. (2015) proposed a model with attention mechanism which automati-\ncally searches the alignments and greatly improves the performance. However, the use of a large vocabulary seems necessary for the word-level neural machine translation models to improve performance (Sutskever et al., 2014; Cho et al., 2015).\nChung et al. (2016b) listed three reasons behind the wide adoption of word-level modeling: (i) word is a basic unit of a language, (ii) data sparsity, (iii) vanishing gradient of character-level modeling. Consider that a language itself is an evolving system. So it is impossible to cover all words in the language. The problem of rare words that are out of vocabulary (OOV) is a critical issue which can effect the performance of neural machine translation. In particular, using larger vocabulary does improve performance (Sutskever et al., 2014; Cho et al., 2015). However, the training becomes much harder and the vocabulary is often filled with many similar words that share a lexeme but have different morphology.\nThere are many approaches to dealing with the out-of-vocabulary issue. For example, Gulcehre et al. (2016), Luong et al. (2015) and Cho et al. (2015) proposed to obtain the alignment information of target unknown words, after which simple word dictionary lookup or identity copy can be performed to replace the unknown words in translation. However, these approaches ignore several important properties of languages such as monolinguality and crosslinguality as pointed out by Luong and Manning (2016). Thus, Luong and Manning (2016) proposed a hybrid neural machine translation model which leverages the power of both words and characters to achieve the goal of open vocabulary neural machine translation.\nIntuitively, it is elegant to directly model pure characters. However, as the length of sequence grows significantly, character-level translation models have failed to produce competitive results compared with word-based models. In addition, they\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nrequire more memory and computation resource. Especially, it is much difficult to train the attention component. For example, Ling et al. (2015a) proposed a compositional character to word (C2W) model and applied it to machine translation (Ling et al., 2015b). They also used a hierarchical decoder which has been explored before in other context (Serban et al., 2015). However, they found it slow and difficult to train the character-level models, and one has to resort to layer-wise training the neural network and applying supervision for the attention component. In fact, such RNNs often struggle with separating words that have similar morphologies but very different meanings.\nIn order to address the issues mentioned earlier, we introduce a novel architecture by exploiting the structure of words. It is built on two recurrent neural networks: one for learning the representation of preceding characters and another for learning the weight of this representation of the whole word. Unlike subword-level model based on the byte pair encoding (BPE) algorithm (Sennrich et al., 2016), we learn the subword unit automatically. Compared with the CNN word encoder (Kim et al., 2016; Lee et al., 2016), our model is able to generate a meaningful representation of the word. To decode at character level, we devise a hierarchical decoder which sets the state of the second-level RNN (character-level decoder) to the output of the first-level RNN (word-level decoder), which will generate a character sequence until generating a delimiter. In this way, our model almost keeps the same encoding length for encoder as word-based models but eliminates the use of a large vocabulary. Furthermore, we are able to efficiently train the deep model which consists of six recurrent networks, achieving higher performance.\nIn summary, we propose a hierarchical architecture (character→ subword→ word→ source sentence → target word → target character) to train a deep character-level neural machine translator. We show that the model achieves a high translation performance which is comparable to the state-ofthe-art neural machine translation model on the task of En-Fr, En-Cs and Cs-En translation. The experiments and analyses further support the statement that our model is able to learn the morphology."
    }, {
      "heading" : "2 Neural Machine Translation",
      "text" : "Neural machine translation is often implemented as an encoder-decoder architecture. The encoder\nusually uses a recurrent neural network (RNN) or a bidirectional recurrent neural network (BiRNN) (Schuster and Paliwal, 1997) to encode the input sentence x = {x1, . . . , xTx} into a sequence of hidden states h = {h1, . . . ,hTx}:\nht = f1(e(xt),ht−1),\nwhere e(xt) ∈ Rm is an m-dimensional embedding of xt. The decoder, another RNN, is often trained to predict next word yt given previous predicted words {y1, . . . , yt−1} and the context vector ct; that is,\np(yt | {y1, . . . , yt−1}) = g(e(yt−1), st, ct),\nwhere\nst = f2(e(yt−1), st−1, ct) (1)\nand g is a nonlinear and potentially multi-layered function that computes the probability of yt. The context ct depends on the sequence of {h1, . . . ,hTx}. Sutskever et al. (2014) encoded all information in the source sentence into a fixedlength vector, i.e., ct = hTx . Bahdanau et al. (2015) computed ct by the alignment model which handles the bottleneck that the former approach meets.\nThe whole model is jointly trained by maximizing the conditional log-probability of the correct translation given a source sentence with respect to the parameters of the model θ:\nθ∗ = argmax θ Ty∑ t=1 log p(yt | {y1, . . . , yt−1},x,θ).\nFor the detailed description of the implementation, we refer the reader to the papers (Sutskever et al., 2014; Bahdanau et al., 2015)."
    }, {
      "heading" : "3 Deep Character-Level Neural Machine Translation",
      "text" : "We consider two problems in the word-level neural machine translation models. First, how can we map a word to a vector? It is usually done by a lookup table (embedding matrix) where the size of vocabulary is limited. Second, how do we map a vector to a word when predicting? It is usually done via a softmax function. However, the large vocabulary will make the softmax intractable computationally.\nWe correspondingly devise two novel architectures, a word encoder which utilizes the morphology and a hierarchical decoder which decodes at\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nFigure 1: The representation of the word ’anyone.’\ncharacter level. Accordingly, we propose a deep character-level neural machine translation model (DCNMT)."
    }, {
      "heading" : "3.1 Learning Morphology in a Word Encoder",
      "text" : "Many words can be subdivided into smaller meaningful units called morphemes, such as “any-one”, “any-thing” and “every-one.” At the basic level, words are made of morphemes which are recognized as grammatically significant or meaningful. Different combinations of morphemes lead to different meanings. Based on these facts, we introduce a word encoder to learn the morphemes and the rules of how they are combined. Even if the word encoder had never seen “everything” before, with an understanding of English morphology, the word encoder could gather the meaning easily. Thus learning morphology in a word encoder might speedup training.\nThe word encoder is based on two recurrent neural networks, as illustrated in Figure 1. We compute the representation of the word ‘anyone’ as\nranyone = tanh( 6∑\nt=1\nwtrt),\nwhere rt is an RNN hidden state at time t, computed by\nrt = f(e(xt), rt−1).\nEach rt contains information about the preceding characters. The weight wt of each representation rt is computed by\nwt = exp(aff(ht)),\nwhere ht is another RNN hidden state at time t and aff() is an affine function which maps ht to a scalar. Here, we use a BiRNN to compute ht as shown in Figure 1. Instead of normalizing it by ∑\nt exp(aff(ht)), we use an activation function tanh as it performs best in experiments.\nWe can regard the weight wi as the energy that determines whether ri is a representation of a morpheme and how it contributes to the representation of the word. Compared with an embedding lookup table, the decoupled RNNs learn the representation of morphemes and the rules of how they are combined respectively, which may be viewed as learning distributed representations of words explicitly. For example, we are able to translate “convenienter” correctly which validates our idea.\nAfter obtaining the representation of the word, we could encode the sentence using a bidirectional RNN as RNNsearch (Bahdanau et al., 2015). The detailed architecture is shown in Figure 2."
    }, {
      "heading" : "3.2 Hierarchical Decoder",
      "text" : "To decode at the character level, we introduce a hierarchical decoder. The first-level decoder is similar to RNNsearch which contains the information of the target word. Specifically, st in Eqn. (1) contains the information of target word at time t. Instead of using a multi-layer network following a softmax function to compute the probability of each target word using st, we employ a second-level decoder which generates a character sequence based on st. The second-level decoder either continues the character-level states or resets using word-level states at boundaries. However, it will be intractable or inefficient to conditionally pick outputs from the the first-level decoder when training in batch manner. For example, Luong and Manning (2016) uses two forward passes (one for word-level and another for character-level) in batch training which is less efficient.\nIn order to decode efficiently, we proposed a variant of the gate recurrent unit (GRU) (Cho et al., 2014; Chung et al., 2014) that used in the secondlevel decoder and we denote it as HGRU (It is possible to use the LSTM (Hochreiter and Schmidhuber, 1997) units instead of the GRU described here). HGRU has a settable state and generates character sequence based on the given state until generating a delimiter. In our model, the state is initialized by the output of the first-level decoder. Once HGRU generates a delimiter, it will\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nset the state to the next output of the first-level decoder. Given the previous output character sequence {y0, y1, . . . , yt−1} where y0 is a token representing the start of sentence, and the auxiliary sequence {a0, a1, . . . , at−1} which only contains 0 and 1 to indicate whether yi is a delimiter (a0 is set to 1), HGRU updates the state as follows:\ngt−1 = (1− at−1)gt−1 + at−1sit , (2) qjt = σ([Wqe(yt−1)] j + [Uqgt−1] j), (3)\nzjt = σ([Wze(yt−1)] j + [Uzgt−1] j), (4) g̃jt = φ([We(yt−1)] j + [U(qt gt−1)]j),\n(5)\ngjt = z j tg j t−1 + (1− z j t )g̃ j t , (6)\nwhere sit is the output of the first-level decoder which calculated as Eqn. (8). We can compute the probability of each target character yt based on gt with a softmax function:\np(yt | {y1, . . . , yt−1},x) = softmax(gt). (7)\nThe current problem is that the number of outputs of the first-level decoder is much fewer than the target character sequence. In our model, we use a matrix to unfold the outputs of the first-level decoder, which makes the batch training process more efficient. It is a Ty × T matrix R, where Ty is the number of delimiter (number of words) in the target character sequence and T is the length of the target character sequence. R[i, j1 + 1] to R[i, j2] are set as 1 if j1 is the index of the (i−1)-th delimiter and j2 is the index of the i-th delimiter in the target character sequence. The index of the 0-th delimiter is set as 0. For example, when the target output is “g o ! ” and the output of the first-level decoder is [s1, s2], the unfolding step will be:\n[s1, s2] [ 1 1 1 0 0 0 0 0 1 1 ] = [s1, s1, s1, s2, s2],\ntherefore {si1 , si2 , si3 , si4 , si5} is correspondingly set to {s1, s1, s1, s2, s2} in HGRU iterations. After this procedure, we can compute the probability of each target character by the second-level decoder according to Eqns. (2) to (7)."
    }, {
      "heading" : "3.3 Model Architectures",
      "text" : "There are totally six recurrent neural networks in our model, which can be divided into four layers as shown in Figure 2. Figure 2 illustrates the training procedure of a basic deep character-level neural\nmachine translation. It is possible to use multilayer recurrent neural networks to make the model deeper. The first layer is a source word encoder which contains two RNNs as shown in Figure 1. The second layer is a bidirectional RNN sentence encoder which is identical to that of Bahdanau et al. (2015). The third layer is the first-level decoder. It takes the representation of previous target word as a feedback, which is produced by the target word encoder in our model. As the feedback is less important, we use an ordinary RNN to encode the target word. The feedback rYt−1 then combines the previous hidden state ut−1 and the context ct from the sentence encoder to generate the vector st:\nst = W1ct +W2rYt−1 +W3ut−1 + b. (8)\nWith the state of HGRU in the second-level decoder setting to st and the information of previous generated character, the second-level decoder generates the next character until generating an end of sentence token (denoted as </s> in Figure 2). With such a hierarchical architecture, we can train our character-level neural translation model perfectly well in an end-to-end fashion."
    }, {
      "heading" : "3.4 Generation Procedure",
      "text" : "We first encode the source sequence as in the training procedure, then we generate the target sequence character by character based on the output st of the first-level decoder. Once we generate a delimiter, we should compute next vector st+1 according to Eqn. (8) by combining feedback rYt from the target word encoder, the context ct+1 from the sentence encoder and the hidden state ut. The generation procedure will terminate once an end of sentence (EOS) token is produced."
    }, {
      "heading" : "4 Experiments",
      "text" : "We implement the model using Theano (Bergstra et al., 2010; Bastien et al., 2012) and Blocks (van Merriënboer et al., 2015). We train our model on a single GTX Titan X with 12GB RAM. First we evaluate our model on English-to-French translation task where the languages are morphologically poor. For fair comparison, we use the same dataset as in RNNsearch which is the bilingual, parallel corpora provided by ACL WMT’14. In order to show the strengths of our model, we conduct on the English-to-Czech and Czech-to-English translation tasks where Czech is a morphologically rich language. We use the same dataset as Chung et al.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nFigure 2: Deep character-level neural machine translation. The HGRUs with red border indicate that the state should be set to the output of the first-level decoder.\n(2016b) and Lee et al. (2016) which is provided by ACL WMT’151."
    }, {
      "heading" : "4.1 Dataset",
      "text" : "We use the parallel corpora for two language pairs from WMT: En-Cs and En-Fr. They consist of 15.8M and 12.1M sentence pairs, respectively. In terms of preprocessing, we only apply the usual tokenization. We choose a list of 120 most frequent characters for each language which coveres nearly 100% of the training data. Those characters not included in the list are mapped to a special token (<unk>). We use newstest2013 (Dev) as the development set and evaluate the models on newstest2015 (Test). We do not use any monolingual corpus."
    }, {
      "heading" : "4.2 Training Details",
      "text" : "We follow Bahdanau et al. (2015) to use similar hyperparameters. The bidirectional RNN sentence encoder and the hierarchical decoder both consists of two-layer RNNs, each has 1024 hidden units; We choose 120 most frequent characters for DCNMT and the character embedding dimensionality is 64. The source word is encoded into a 600-dimensional\n1http://www.statmt.org/wmt15/translation-task.html\nvector. The other GRUs in our model have 512 hidden units.\nWe use the ADAM optimizer (Kingma and Ba, 2015) with minibatch of 56 sentences to train each model (for En-Fr we use a minibatch of 72 examples). The learning rate is first set to 10−3 and then annealed to 10−4.\nWe use a beam search to find a translation that approximately maximizes the conditional logprobability which is a commonly used approach in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015). In our DCNMT model, it is reasonable to search directly on character level to generate a translation."
    }, {
      "heading" : "5 Result and Analysis",
      "text" : "We conduct comparison of quantitative results on the En-Fr, En-Cs and Cs-En translation tasks in Section 5.1. Apart from measuring translation quality, we analyze the efficiency of our model and effects of character-level modeling in more details."
    }, {
      "heading" : "5.1 Quantitative Results",
      "text" : "We illustrate the efficiency of the deep characterlevel neural machine translation by comparing with the bpe-based subword model (Sennrich et al.,\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n2016) and other character-level models. We measure the performance by BLEU score (Papineni et al., 2002).\nIn Table 1, “Length” indicates the maximum sentence length in training (based on the number of words or characters), “Size” is the total number of parameters in the models. The only difference between CNMT and DCNMT is CNMT uses an ordinary RNN to encode source words (takes the last hidden state). For each test set, the best scores among the models per language pair are bold-faced. Obviously, character-level models are better than the subword-level models, and our model is comparable to the start-of-the-art character-level models. Note that, the purely character model of (5) (Luong and Manning, 2016) took 3 months to train and yielded +0.5 BLEU points compared to our result. We have analyzed the efficiency of our decoder in Section 3.2. Though our model consists of six RNNs, our model is the simplest and the smallest one in terms of the model size. Thus, our model is efficient to train as shown in Table 1."
    }, {
      "heading" : "5.2 Learning Morphology",
      "text" : "In this section, we investigate whether our model could learn morphology.\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2 -2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n← notable\n← notability\n← solvable\n← solvability\n← reliable\n← reliability\n← capable\n← capability\n← flexible ← flexibility\n← possible ← possibility\n(a) ordinary RNN word encoder\n-2 -1.5 -1 -0.5 0 0.5 1 1.5 2 -2\n-1.5\n-1\n-0.5\n0\n0.5\n1\n1.5\n2\n← notable\n← notability\n← solvable\n← solvability\n← reliable ← reliability\n← capable ← capability\n← flexible ← flexibility\n← possible ← possibility\n(b) our word encoder\nFigure 3: Two-dimensional PCA projection of the 600-dimensional representation of the words.\nFirst we want to figure out the difference between an ordinary RNN word encoder and our word encoder. We choose some words with similar meaning but different in morphology as shown in Figure 3. We could find in Figure 3(a) that the words ending with “ability”, which are encoded by the ordinary RNN word encoder, are jammed together. In contrast, the representations produced by our encoder are more reasonable and the words with similar meaning are closer. Compared with the CNN word encoder (Kim et al., 2016; Lee et al., 2016), the RNN word encoder in our model is able to generate a meaningful representation of the word. In order to obtain such meaningful representations of words, we need the explicit segmentation which indicates the boundary of words.\nThen we analyze how our word encoder learns morphemes and the rules of how they are combined. We demonstrate the encoding details on “any*” and “every*”. Figure 4(a) shows the energy of each character, more precisely, the energy of preceding characters. We could see that the last character of a morpheme will result a relative large energy (weight) like “any” and “every” in these words. Moreover, even the preceding characters are different, it will produce a similar weight for the same morpheme like “way” in “anyway” and “everyway”. The two-dimensional PCA projection\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nModel Size Src Trgt Length Epochs Days Dev Test\nE n-\nFr bpe2bpe(1) - bpe bpe 50 50 - - 26.91 29.70\nC2W(2) ∼ 54 M char char 300 300 ∼ 2.8 ∼ 27 25.89 27.04 CNMT ∼ 52 M char char 300 300 ∼ 3.8 ∼ 21 28.19 29.38\nDCNMT ∼ 54 M char char 300 300 1 ∼ 7 27.02 28.13 ∼ 2.8 ∼ 19 29.31 30.56\nE n-\nC s\nbpe2bpe(1) - bpe bpe 50 50 - - 15.90 13.84 bpe2char(3) ∼ 76 M bpe char 50 500 - - - 16.86\nchar(5) - char char 600 600 > 4 ∼ 90 - 17.5\nDCNMT ∼ 54 M char char 450 450 1 ∼ 5 15.50 14.87 ∼ 2.9 ∼ 15 17.89 16.96\nC s-\nE n bpe2bpe(1) - bpe bpe 50 50 - - 21.24 20.32 bpe2char(3) ∼ 76 M bpe char 50 500 ∼ 6.1 ∼ 14 23.27 22.42 char2char(4) ∼ 69 M char char 450 450 ∼ 7.9 ∼ 30 23.38 22.46\nDCNMT ∼ 54 M char char 450 450 1 ∼ 5 20.50 19.75 ∼ 4.6 ∼ 22 23.24 22.48\nTable 1: BLEU scores of different models on three language pairs. We report the BLEU scores of DCNMT when trained after one epoch in the above line and the final scores in the following line. The results of other models are taken from (1) Firat et al. (2016), (3) Chung et al. (2016b), (4) Lee et al. (2016) and (5) Luong and Manning (2016) respectively, except (2) is trained according to Ling et al. (2015b). The training efficiency of each model is compared in terms of training epochs and training days.\nin Figure 4(b) further validates our idea. The word encoder may be able to guess the meaning of “everything” even it had never seen “everything” before, thus speedup learning. More interestingly, we find that not only the ending letter has high energy, but also the beginning letter is important. It matches the behavior of human perception (White et al., 2008).\nMoreover, we apply our trained word encoder to Penn Treebank Line 1. Unlike Chung et al. (2016a), we are able to detect the boundary of the subword units. As shown in Figure 5, “consumers”, “monday”, “football” and “greatest” are segmented into “consum-er-s”,“mon-day”, “foot-ball” and “greatest” respectively. Since there are no explicit delimiters, it may be more difficult to detect the subword units."
    }, {
      "heading" : "5.3 Benefiting from learning morphology",
      "text" : "As analyzed in Section 5.2, learning morphology could speedup learning. This has also been shown in Table 1 (En-Fr and En-Cs task) from which we see that when we train our model just for one epoch, the obtained result even outperforms the final result with bpe baseline.\nAnother advantage of our model is the ability to translate the misspelled words or the nonce words. The character-level model has a much better chance recovering the original word or sentence. In Table 2, we list some examples where the source sentences are taken from newstest2013 but we change some words to misspelled words or nonce words. We also list the translations from Google translate2 and online demo of neural machine translation by LISA.\nAs listed in Table 2(a), DCNMT is able to translate out the misspelled words correctly. For a wordbased translator, it is never possible because the misspelled words are mapped into <unk> token before translating. Thus, it will produce an <unk> token or just take the word from source sentence (Gulcehre et al., 2016; Luong et al., 2015). More interestingly, DCNMT could translate “convenienter” correctly as shown in Table 2(b). By concatenating “convenient” and “er”, we get the comparative adjective form of “convenient” which never appears in the training set; however, our model guessed it\n2The translations by Google translate were made on February 2, 2017.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\npeak\nenergy c o n s u me r s ma y wa n t t o mo v e t h e i r t e l e p h o n e s a l i t t l e c l o s e r t o t h e t v s e t < u n k > < u n k > wa t c h i n g\npeak\nenergy a b c ' s mo n d a y n i g h t f o o t b a l l c a n n o w v o t e d u r i n g < u n k > f o r t h e g r e a t e s t p l a y i n N y e a r s f r o m\npeak\nenergy a mo n g f o u r o r f i v e < u n k > < u n k > t w o w e e k s a g o v i e w e r s o f s e v e r a l n b c < u n k > c o n s u me r s e g me n t s\nFigure 5: Subword-level boundary detected by our word encoder.\n(a) Misspelled words\nSource For the time being howeve their research is unconclusive.\nReference Leurs recherches ne sont toutefois pas concluantes pour l’instant.\nGoogle translate Pour l’instant, leurs recherches ne sont pas concluantes.\nLISA Pour le moment UNK leur recherche est UNK.\nDCNMT Pour le moment, cependant, leur recherche n’est pas concluante.\n(b) Nonce words (morphological change)\nSource Then we will be able to supplement the real world with virtual objects in a much convenienter form .\nReference Ainsi , nous pourrons complter le monde rel par des objets virtuels dans une forme plus pratique .\nGoogle translate Ensuite, nous serons en mesure de complter le monde rel avec des objets virtuels dans une forme beaucoup plus pratique.\nLISA Ensuite, nous serons en mesure de complter le vrai monde avec des objets virtuels sous une forme bien UNK.\nDCNMT Ensuite, nous serons en mesure de complter le monde rel avec des objets virtuels dans une forme beaucoup plus pratique.\nTable 2: Sample translations. The word-level model is unable to recognize the misspelled words. Our model has a much better chance to recover the original word.\ncorrectly based on the morphemes and the rules. More sample translations are provided in the supplementary material."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper we have proposed an hierarchical architecture to train the deep character-level neural machine translation model by introducing a novel word encoder and a multi-leveled decoder. We have demonstrated the efficiency of the training process and the effectiveness of the model in comparison with the word-level and other characterlevel models. The BLEU score implies that our deep character-level neural machine translation model likely outperforms the word-level models and is competitive with the state-of-the-art character-based models. It is possible to further improve performance by using deeper recurrent net-\nworks (Wu et al., 2016), training for more epochs and training with longer sentence pairs.\nAs a result of the character-level modeling, we have solved the out-of-vocabulary (OOV) issue that word-level models suffer from, and we have obtained a new functionality to translate the misspelled or the nonce words. More importantly, the deep character-level model is able to learn the similar embedding of the words with similar meanings like the word-level models. Finally, it would be potentially possible that the idea behind our approach could be applied to many other tasks such as speech recognition and text summarization.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representation .",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio." ],
      "venue" : "Deep Learning and Unsupervised Feature Learning",
      "citeRegEx" : "Bastien et al\\.,? 2012",
      "shortCiteRegEx" : "Bastien et al\\.",
      "year" : 2012
    }, {
      "title" : "Theano: a CPU and GPU math expression compiler",
      "author" : [ "James Bergstra", "Olivier Breuleux", "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Bergstra et al\\.,? 2010",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "On using very large target vocabulary for neural machine translation",
      "author" : [ "Sébastien Jean Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics .",
      "citeRegEx" : "Cho et al\\.,? 2015",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical multiscale recurrent neural networks",
      "author" : [ "Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1609.01704 .",
      "citeRegEx" : "Chung et al\\.,? 2016a",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2016
    }, {
      "title" : "A character-level decoder without explicit segmentation for neural machine translation",
      "author" : [ "Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics .",
      "citeRegEx" : "Chung et al\\.,? 2016b",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2016
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1412.3555 .",
      "citeRegEx" : "Chung et al\\.,? 2014",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-way, multilingual neural machine translation with a shared attention mechanism",
      "author" : [ "Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational",
      "citeRegEx" : "Firat et al\\.,? 2016",
      "shortCiteRegEx" : "Firat et al\\.",
      "year" : 2016
    }, {
      "title" : "Pointing the unknown words",
      "author" : [ "Caglar Gulcehre", "Sungjin Ahn", "Ramesh Nallapati", "Bowen Zhou", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics .",
      "citeRegEx" : "Gulcehre et al\\.,? 2016",
      "shortCiteRegEx" : "Gulcehre et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush." ],
      "venue" : "Association for the Advancement of Artificial Intelligence .",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "International Conference on Learning Representation .",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Fully character-level neural machine translation without explicit segmentation",
      "author" : [ "Jason Lee", "Kyunghyun Cho", "Thomas Hofmann." ],
      "venue" : "arXiv preprint arXiv:1610.03017 .",
      "citeRegEx" : "Lee et al\\.,? 2016",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2016
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Wang Ling", "Tiago Luı́s", "Luı́s Marujo", "Ramón Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso" ],
      "venue" : null,
      "citeRegEx" : "Ling et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Character-based neural machine translation",
      "author" : [ "Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W Black." ],
      "venue" : "arXiv preprint arXiv:1511.04586 .",
      "citeRegEx" : "Ling et al\\.,? 2015b",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Achieving open vocabulary neural machine translation with hybrid word-character models",
      "author" : [ "Minh-Thang Luong", "Christopher D Manning." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics .",
      "citeRegEx" : "Luong and Manning.,? 2016",
      "shortCiteRegEx" : "Luong and Manning.",
      "year" : 2016
    }, {
      "title" : "Addressing the rare word problem in neural machine translation",
      "author" : [ "Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics .",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Association for Computational Linguistics, pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "Mike Schuster", "Kuldip K Paliwal." ],
      "venue" : "Signal Processing, IEEE Transactions on 45(11):2673–2681.",
      "citeRegEx" : "Schuster and Paliwal.,? 1997",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics .",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Hierarchical neural network generative models for movie dialogues",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "arXiv preprint arXiv:1507.04808 .",
      "citeRegEx" : "Serban et al\\.,? 2015",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Blocks and fuel: Frameworks for deep learning",
      "author" : [ "Bart van Merriënboer", "Dzmitry Bahdanau", "Vincent Dumoulin", "Dmitriy Serdyuk", "David Warde-Farley", "Jan Chorowski", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1506.00619 .",
      "citeRegEx" : "Merriënboer et al\\.,? 2015",
      "shortCiteRegEx" : "Merriënboer et al\\.",
      "year" : 2015
    }, {
      "title" : "Eye movements when reading transposed text: the importance of word-beginning letters",
      "author" : [ "Sarah J White", "Rebecca L Johnson", "Simon P Liversedge", "Keith Rayner." ],
      "venue" : "Journal of Experimental Psychology: Human Perception and Performance",
      "citeRegEx" : "White et al\\.,? 2008",
      "shortCiteRegEx" : "White et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "Neural machine translation (NMT) attempts to build a single large neural network that reads a sentence and outputs a translation (Sutskever et al., 2014).",
      "startOffset" : 129,
      "endOffset" : 153
    }, {
      "referenceID" : 22,
      "context" : "Most of the extant neural machine translations models belong to a family of word-level encoder-decoders (Sutskever et al., 2014; Cho et al., 2014).",
      "startOffset" : 104,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : "Most of the extant neural machine translations models belong to a family of word-level encoder-decoders (Sutskever et al., 2014; Cho et al., 2014).",
      "startOffset" : 104,
      "endOffset" : 146
    }, {
      "referenceID" : 22,
      "context" : "However, the use of a large vocabulary seems necessary for the word-level neural machine translation models to improve performance (Sutskever et al., 2014; Cho et al., 2015).",
      "startOffset" : 131,
      "endOffset" : 173
    }, {
      "referenceID" : 4,
      "context" : "However, the use of a large vocabulary seems necessary for the word-level neural machine translation models to improve performance (Sutskever et al., 2014; Cho et al., 2015).",
      "startOffset" : 131,
      "endOffset" : 173
    }, {
      "referenceID" : 22,
      "context" : "In particular, using larger vocabulary does improve performance (Sutskever et al., 2014; Cho et al., 2015).",
      "startOffset" : 64,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "In particular, using larger vocabulary does improve performance (Sutskever et al., 2014; Cho et al., 2015).",
      "startOffset" : 64,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "Recently, Bahdanau et al. (2015) proposed a model with attention mechanism which automatically searches the alignments and greatly improves the performance.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "Recently, Bahdanau et al. (2015) proposed a model with attention mechanism which automatically searches the alignments and greatly improves the performance. However, the use of a large vocabulary seems necessary for the word-level neural machine translation models to improve performance (Sutskever et al., 2014; Cho et al., 2015). Chung et al. (2016b) listed three reasons behind the wide adoption of word-level modeling: (i) word is a basic unit of a language, (ii) data sparsity, (iii) vanishing gradient of character-level modeling.",
      "startOffset" : 10,
      "endOffset" : 353
    }, {
      "referenceID" : 0,
      "context" : "Recently, Bahdanau et al. (2015) proposed a model with attention mechanism which automatically searches the alignments and greatly improves the performance. However, the use of a large vocabulary seems necessary for the word-level neural machine translation models to improve performance (Sutskever et al., 2014; Cho et al., 2015). Chung et al. (2016b) listed three reasons behind the wide adoption of word-level modeling: (i) word is a basic unit of a language, (ii) data sparsity, (iii) vanishing gradient of character-level modeling. Consider that a language itself is an evolving system. So it is impossible to cover all words in the language. The problem of rare words that are out of vocabulary (OOV) is a critical issue which can effect the performance of neural machine translation. In particular, using larger vocabulary does improve performance (Sutskever et al., 2014; Cho et al., 2015). However, the training becomes much harder and the vocabulary is often filled with many similar words that share a lexeme but have different morphology. There are many approaches to dealing with the out-of-vocabulary issue. For example, Gulcehre et al. (2016), Luong et al.",
      "startOffset" : 10,
      "endOffset" : 1158
    }, {
      "referenceID" : 0,
      "context" : "Recently, Bahdanau et al. (2015) proposed a model with attention mechanism which automatically searches the alignments and greatly improves the performance. However, the use of a large vocabulary seems necessary for the word-level neural machine translation models to improve performance (Sutskever et al., 2014; Cho et al., 2015). Chung et al. (2016b) listed three reasons behind the wide adoption of word-level modeling: (i) word is a basic unit of a language, (ii) data sparsity, (iii) vanishing gradient of character-level modeling. Consider that a language itself is an evolving system. So it is impossible to cover all words in the language. The problem of rare words that are out of vocabulary (OOV) is a critical issue which can effect the performance of neural machine translation. In particular, using larger vocabulary does improve performance (Sutskever et al., 2014; Cho et al., 2015). However, the training becomes much harder and the vocabulary is often filled with many similar words that share a lexeme but have different morphology. There are many approaches to dealing with the out-of-vocabulary issue. For example, Gulcehre et al. (2016), Luong et al. (2015) and Cho et al.",
      "startOffset" : 10,
      "endOffset" : 1179
    }, {
      "referenceID" : 0,
      "context" : "Recently, Bahdanau et al. (2015) proposed a model with attention mechanism which automatically searches the alignments and greatly improves the performance. However, the use of a large vocabulary seems necessary for the word-level neural machine translation models to improve performance (Sutskever et al., 2014; Cho et al., 2015). Chung et al. (2016b) listed three reasons behind the wide adoption of word-level modeling: (i) word is a basic unit of a language, (ii) data sparsity, (iii) vanishing gradient of character-level modeling. Consider that a language itself is an evolving system. So it is impossible to cover all words in the language. The problem of rare words that are out of vocabulary (OOV) is a critical issue which can effect the performance of neural machine translation. In particular, using larger vocabulary does improve performance (Sutskever et al., 2014; Cho et al., 2015). However, the training becomes much harder and the vocabulary is often filled with many similar words that share a lexeme but have different morphology. There are many approaches to dealing with the out-of-vocabulary issue. For example, Gulcehre et al. (2016), Luong et al. (2015) and Cho et al. (2015) proposed to obtain the alignment information of target unknown words, after which simple word dictionary lookup or identity copy can be performed to replace the unknown words in translation.",
      "startOffset" : 10,
      "endOffset" : 1201
    }, {
      "referenceID" : 0,
      "context" : "Recently, Bahdanau et al. (2015) proposed a model with attention mechanism which automatically searches the alignments and greatly improves the performance. However, the use of a large vocabulary seems necessary for the word-level neural machine translation models to improve performance (Sutskever et al., 2014; Cho et al., 2015). Chung et al. (2016b) listed three reasons behind the wide adoption of word-level modeling: (i) word is a basic unit of a language, (ii) data sparsity, (iii) vanishing gradient of character-level modeling. Consider that a language itself is an evolving system. So it is impossible to cover all words in the language. The problem of rare words that are out of vocabulary (OOV) is a critical issue which can effect the performance of neural machine translation. In particular, using larger vocabulary does improve performance (Sutskever et al., 2014; Cho et al., 2015). However, the training becomes much harder and the vocabulary is often filled with many similar words that share a lexeme but have different morphology. There are many approaches to dealing with the out-of-vocabulary issue. For example, Gulcehre et al. (2016), Luong et al. (2015) and Cho et al. (2015) proposed to obtain the alignment information of target unknown words, after which simple word dictionary lookup or identity copy can be performed to replace the unknown words in translation. However, these approaches ignore several important properties of languages such as monolinguality and crosslinguality as pointed out by Luong and Manning (2016). Thus, Luong and Manning (2016) proposed a hybrid neural machine translation model which leverages the power of both words and characters to achieve the goal of open vocabulary neural machine translation.",
      "startOffset" : 10,
      "endOffset" : 1553
    }, {
      "referenceID" : 0,
      "context" : "Recently, Bahdanau et al. (2015) proposed a model with attention mechanism which automatically searches the alignments and greatly improves the performance. However, the use of a large vocabulary seems necessary for the word-level neural machine translation models to improve performance (Sutskever et al., 2014; Cho et al., 2015). Chung et al. (2016b) listed three reasons behind the wide adoption of word-level modeling: (i) word is a basic unit of a language, (ii) data sparsity, (iii) vanishing gradient of character-level modeling. Consider that a language itself is an evolving system. So it is impossible to cover all words in the language. The problem of rare words that are out of vocabulary (OOV) is a critical issue which can effect the performance of neural machine translation. In particular, using larger vocabulary does improve performance (Sutskever et al., 2014; Cho et al., 2015). However, the training becomes much harder and the vocabulary is often filled with many similar words that share a lexeme but have different morphology. There are many approaches to dealing with the out-of-vocabulary issue. For example, Gulcehre et al. (2016), Luong et al. (2015) and Cho et al. (2015) proposed to obtain the alignment information of target unknown words, after which simple word dictionary lookup or identity copy can be performed to replace the unknown words in translation. However, these approaches ignore several important properties of languages such as monolinguality and crosslinguality as pointed out by Luong and Manning (2016). Thus, Luong and Manning (2016) proposed a hybrid neural machine translation model which leverages the power of both words and characters to achieve the goal of open vocabulary neural machine translation.",
      "startOffset" : 10,
      "endOffset" : 1585
    }, {
      "referenceID" : 15,
      "context" : "(2015a) proposed a compositional character to word (C2W) model and applied it to machine translation (Ling et al., 2015b).",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 21,
      "context" : "They also used a hierarchical decoder which has been explored before in other context (Serban et al., 2015).",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "Unlike subword-level model based on the byte pair encoding (BPE) algorithm (Sennrich et al., 2016), we learn the subword unit automatically.",
      "startOffset" : 75,
      "endOffset" : 98
    }, {
      "referenceID" : 11,
      "context" : "Compared with the CNN word encoder (Kim et al., 2016; Lee et al., 2016), our model is able to generate a meaningful representation of the word.",
      "startOffset" : 35,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "Compared with the CNN word encoder (Kim et al., 2016; Lee et al., 2016), our model is able to generate a meaningful representation of the word.",
      "startOffset" : 35,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "For example, Ling et al. (2015a) proposed a compositional character to word (C2W) model and applied it to machine translation (Ling et al.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 19,
      "context" : "The encoder usually uses a recurrent neural network (RNN) or a bidirectional recurrent neural network (BiRNN) (Schuster and Paliwal, 1997) to encode the input sentence x = {x1, .",
      "startOffset" : 110,
      "endOffset" : 138
    }, {
      "referenceID" : 21,
      "context" : "Sutskever et al. (2014) encoded all information in the source sentence into a fixedlength vector, i.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "Bahdanau et al. (2015) computed ct by the alignment model which handles the bottleneck that the former approach meets.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 22,
      "context" : "For the detailed description of the implementation, we refer the reader to the papers (Sutskever et al., 2014; Bahdanau et al., 2015).",
      "startOffset" : 86,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : "For the detailed description of the implementation, we refer the reader to the papers (Sutskever et al., 2014; Bahdanau et al., 2015).",
      "startOffset" : 86,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : "After obtaining the representation of the word, we could encode the sentence using a bidirectional RNN as RNNsearch (Bahdanau et al., 2015).",
      "startOffset" : 116,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : "In order to decode efficiently, we proposed a variant of the gate recurrent unit (GRU) (Cho et al., 2014; Chung et al., 2014) that used in the secondlevel decoder and we denote it as HGRU (It is possible to use the LSTM (Hochreiter and Schmidhuber, 1997) units instead of the GRU described here).",
      "startOffset" : 87,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "In order to decode efficiently, we proposed a variant of the gate recurrent unit (GRU) (Cho et al., 2014; Chung et al., 2014) that used in the secondlevel decoder and we denote it as HGRU (It is possible to use the LSTM (Hochreiter and Schmidhuber, 1997) units instead of the GRU described here).",
      "startOffset" : 87,
      "endOffset" : 125
    }, {
      "referenceID" : 10,
      "context" : ", 2014) that used in the secondlevel decoder and we denote it as HGRU (It is possible to use the LSTM (Hochreiter and Schmidhuber, 1997) units instead of the GRU described here).",
      "startOffset" : 102,
      "endOffset" : 136
    }, {
      "referenceID" : 10,
      "context" : "For example, Luong and Manning (2016) uses two forward passes (one for word-level and another for character-level) in batch training which is less efficient.",
      "startOffset" : 13,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "The second layer is a bidirectional RNN sentence encoder which is identical to that of Bahdanau et al. (2015). The third layer is the first-level decoder.",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "We implement the model using Theano (Bergstra et al., 2010; Bastien et al., 2012) and Blocks (van Merriënboer et al.",
      "startOffset" : 36,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "We implement the model using Theano (Bergstra et al., 2010; Bastien et al., 2012) and Blocks (van Merriënboer et al.",
      "startOffset" : 36,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "(2016b) and Lee et al. (2016) which is provided by ACL WMT’151.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "We follow Bahdanau et al. (2015) to use similar hyperparameters.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 12,
      "context" : "We use the ADAM optimizer (Kingma and Ba, 2015) with minibatch of 56 sentences to train each model (for En-Fr we use a minibatch of 72 examples).",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 22,
      "context" : "We use a beam search to find a translation that approximately maximizes the conditional logprobability which is a commonly used approach in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015).",
      "startOffset" : 167,
      "endOffset" : 214
    }, {
      "referenceID" : 0,
      "context" : "We use a beam search to find a translation that approximately maximizes the conditional logprobability which is a commonly used approach in neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015).",
      "startOffset" : 167,
      "endOffset" : 214
    }, {
      "referenceID" : 18,
      "context" : "We measure the performance by BLEU score (Papineni et al., 2002).",
      "startOffset" : 41,
      "endOffset" : 64
    }, {
      "referenceID" : 16,
      "context" : "Note that, the purely character model of (5) (Luong and Manning, 2016) took 3 months to train and yielded +0.",
      "startOffset" : 45,
      "endOffset" : 70
    }, {
      "referenceID" : 11,
      "context" : "Compared with the CNN word encoder (Kim et al., 2016; Lee et al., 2016), the RNN word encoder in our model is able to generate a meaningful representation of the word.",
      "startOffset" : 35,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "Compared with the CNN word encoder (Kim et al., 2016; Lee et al., 2016), the RNN word encoder in our model is able to generate a meaningful representation of the word.",
      "startOffset" : 35,
      "endOffset" : 71
    }, {
      "referenceID" : 5,
      "context" : "The results of other models are taken from (1) Firat et al. (2016), (3) Chung et al.",
      "startOffset" : 47,
      "endOffset" : 67
    }, {
      "referenceID" : 5,
      "context" : "(2016), (3) Chung et al. (2016b), (4) Lee et al.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "(2016), (3) Chung et al. (2016b), (4) Lee et al. (2016) and (5) Luong and Manning (2016) respectively, except (2) is trained according to Ling et al.",
      "startOffset" : 12,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "(2016), (3) Chung et al. (2016b), (4) Lee et al. (2016) and (5) Luong and Manning (2016) respectively, except (2) is trained according to Ling et al.",
      "startOffset" : 12,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "(2016), (3) Chung et al. (2016b), (4) Lee et al. (2016) and (5) Luong and Manning (2016) respectively, except (2) is trained according to Ling et al. (2015b). The training efficiency of each model is compared in terms of training epochs and training days.",
      "startOffset" : 12,
      "endOffset" : 158
    }, {
      "referenceID" : 24,
      "context" : "It matches the behavior of human perception (White et al., 2008).",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "Unlike Chung et al. (2016a), we are able to detect the boundary of the subword units.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 9,
      "context" : "Thus, it will produce an <unk> token or just take the word from source sentence (Gulcehre et al., 2016; Luong et al., 2015).",
      "startOffset" : 80,
      "endOffset" : 123
    }, {
      "referenceID" : 17,
      "context" : "Thus, it will produce an <unk> token or just take the word from source sentence (Gulcehre et al., 2016; Luong et al., 2015).",
      "startOffset" : 80,
      "endOffset" : 123
    } ],
    "year" : 2017,
    "abstractText" : "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models and conventional character-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Moreover, the final BLEU score of our model is comparable to the state-of-the-art systems. Further analyses show that our model is able to learn morphology.",
    "creator" : "LaTeX with hyperref package"
  }
}