{
  "name" : "433.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Languages evolve temporally and geographically, both in vocabulary as well as in syntactic structures. When major languages such as English or French are adopted in another culture as the primary language, they often mix with existing languages or dialects in that culture and evolve into a stable language called a creole. Examples of creoles include the French-based Haitian Creole, and Colloquial Singaporean English (Singlish) (MianLian and Platt, 1993), an English-based creole. While the majority of the natural language processing (NLP) research attention has been focused on the major languages, little work has been done on adapting the components to creoles. One notable body of work originated from the featured translation task of the EMNLP 2011 Workshop on Statistical Machine Translation (WMT11) to translate Haitian Creole SMS messages sent during the 2010 Haitian earthquake. This work high-\nlights the importance of NLP tools on creoles in crisis situations for emergency relief (Hu et al., 2011; Hewavitharana et al., 2011).\nSinglish is one of the major languages in Singapore, with borrowed vocabulary and grammars from a number of languages including Malay, Tamil, and Chinese dialects such as Hokkien, Cantonese and Teochew (Leimgruber, 2009, 2011), and it has been increasingly used in written forms on web media. Fluent English speakers unfamiliar with Singlish would find the creole hard to comprehend (Harada, 2009). Correspondingly, fundamental English NLP components such as POS taggers and dependency parsers perform poorly on such Singlish texts based on our observations. For example, Seah et al. (2015) adapted the Socher et al. (2013) sentiment analysis engine to the Singlish vocabulary, but failed to adapt the parser. Since dependency parsers are important for tasks such as information extraction (Miwa and Bansal, 2016) and discourse parsing (Li et al., 2015), this hinders the development of such downstream applications for Singlish in written forms and thus makes it crucial to build a dependency parser that can perform well natively on Singlish.\nTo address this issue, we start with investigating the linguistic characteristics of Singlish and specifically the causes of difficulties for understanding Singlish with English syntax. We found that, despite the obvious attribute of inheriting a large portion of basic vocabularies and grammars from English, Singlish not only imports terms from regional languages and dialects, its lexical semantics and syntax also deviate significantly from English (Leimgruber, 2009, 2011). We categorize the challenges and formalize their interpretation using Universal Dependencies (Nivre et al., 2016), which extends to the creation of a Singlish dependency treebank with 1,200 sentences.\nBased on the intricate relationship between\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nSinglish dependency\nparser trained with small\nSinglish treebank\nEnglish syntactic and\nsemantic knowledge learnt\nfrom large treebank\nSinglish sentences\nSinglish dependency trees\nFigure 1: Overall model diagram\nSinglish and English, we build a Singlish parser by leveraging knowledge of English syntax as a basis. This overall approach is illustrated in Figure 1. In particular, we train a basic Singlish parser with the best off-the-shelf neural dependency parsing model using biaffine attention (Dozat and Manning, 2016), and improve it with knowledge transfer by adopting neural stacking (Chen et al., 2016; Zhang and Weiss, 2016) to integrate the English syntax. Since POS tags are important features for dependency parsing (Chen and Manning, 2014; Dyer et al., 2015), we train a POS tagger for Singlish following the same idea by integrating English POS knowledge using neural stacking (Chen et al., 2016; Zhang and Weiss, 2016).\nResults show that English syntax knowledge brings 31.58% and 36.51% relative error reduction on POS tagging and dependency parsing respectively, resulting in a Singlish dependency parser with 84.21% unlabeled attachment score (UAS).\nWe make our Singlish dependency treebank, the source code for training a dependency parser and the trained model for the parser with the best performance freely available online1."
    }, {
      "heading" : "2 Related Work",
      "text" : "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). In particular, the biaffine attention method of Dozat and Manning (2016) uses deep bi-directional long\n1https://github.com/ANONYMIZED/ ANONYMIZED\nshort-term memory (bi-LSTM) networks for highorder non-linear feature extraction, producing the state-of-the-art performance for English dependency parsing. We adopt this model as the basis for our Singlish parser.\nOur work belongs to a line of work on transfer learning for parsing, which leverages English resources in Universal Dependencies to improve the parsing accuracies of low-resource languages (Hwa et al., 2005; Cohen and Smith, 2009; Ganchev et al., 2009). Seminal work employed statistical models. McDonald et al. (2011) investigated delexicalized transfer, where word-based features are removed from a statistical model for English, so that POS and dependency label knowledge can be utilized for training a model for lowresource language. Subsequent work considered syntactic similarities between languages for better feature transfer (Täckström et al., 2012; Naseem et al., 2012; Zhang and Barzilay, 2015).\nRecently, a line of work leverages neural network models for multi-lingual parsing (Guo et al., 2015; Duong et al., 2015; Ammar et al., 2016). The basic idea is to map the word embedding spaces between different languages into the same vector space, by using sentence-aligned bilingual data. This gives consistency in tokens, POS and dependency labels thanks to the availability of Universal Dependencies (Nivre et al., 2016). Our work is similar to these methods in using a neural network model for knowledge sharing between different languages. However, ours is different in the use of a neural stacking model, which respects the distributional differences between Singlish and English words. This empirically gives higher accuracies for Singlish."
    }, {
      "heading" : "3 Singlish Dependency Treebank",
      "text" : ""
    }, {
      "heading" : "3.1 Universal Dependencies for Singlish",
      "text" : "Since English is the major genesis of Singlish, we choose English as the source of lexical feature transfer to assist Singlish dependency parsing. Universal Dependencies provides a set of multilingual treebanks with cross-lingually consistent dependency-based lexicalist annotations, designed to aid development and evaluation for cross-lingual systems, such as multilingual parsers (Nivre et al., 2016). The current version of Universal Dependencies comprises not only major treebanks for 47 languages but also their siblings for domain-specific corpora and di-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nalects. With the aligned initiatives for creating transfer-learning-friendly treebanks, we adopt the Universal Dependencies protocol for constructing the Singlish dependency treebank, both as a new resource for the low-resource languages and to facilitate knowledge transfer from English.\nOn top of the general Universal Dependencies guidelines, English-specific dependency relation definitions including additional subtypes are employed as the default standards for annotating the Singlish dependency treebank, unless augmented or redefined when necessary. The latest English corpus in Universal Dependencies v1.42 collection is constructed from the English Web Treebank (Bies et al., 2012), comprising of web media texts, which potentially smooths the knowledge transfer to our target Singlish texts in similar domains. The statistics of this dataset, from which we obtain English syntactic knowledge, is shown in Table 1 and we refer to this corpus as UD-Eng. This corpus uses 47 dependency relations and we show below how to conform to the same standard while adapting to unique Singlish grammars."
    }, {
      "heading" : "3.2 Challenges and Solutions for Annotating Singlish",
      "text" : "The deviations of Singlish from English come from both the lexical and the grammatical levels (Leimgruber, 2009, 2011), which bring challenges for analysis on Singlish using English NLP tools. The former involves imported vocabularies from the first languages of the local people and the latter can be represented by a set of relatively localized features which collectively form 5 unique grammars of Singlish according to Leimgruber (2011). We find empirically that all these deviations can be accommodated by applying the existing English dependency relation definitions, which are explained with examples as follows.\nImported vocabulary: Singlish borrows a number of words and expressions from its nonEnglish origins (Leimgruber, 2009, 2011), such as “Kiasu”, which originates from Hokkien meaning “very anxious not to miss an opportunity”.3 These imported terms often constitute out-of-vocabulary (OOV) words with respect to a standard English treebank and result in difficulties for using English-trained tools on Singlish. All borrowed words are annotated according to their original\n2Only guidelines for Universal Dependencies v2 but not the English corpus is available when this work is completed.\n3Definition by the Oxford living Dictionaries for English.\n(1) Drive this car sure draw looks .\nroot\ndet dobj\ncsubj\nadvmod dobj punct\n(2) SG where got attap chu ?\nroot nsubj\nadvmod dobj compound\npunct\n(3) Inside tent can not see leh !\nrootnmod aux\nneg discourse\npunct\ncase\n(4) U betting more downside from here ?\nroot\nnsubj dobj amod case\nnmod punct\n(5) Hope can close 22 today .\nroot ccomp\naux dobj nmod:tmod\npunct\n(6) Best to makan all , tio boh ?\nroot\nmark xcomp\ndobj\npunct\nneg\ndiscourse\npunct\n(7) I never get it free one !\nroot\nadvmod nsubj dobj xcomp\ndiscourse punct\nFigure 2: Unique Singlish grammars.\nmeanings using existing UD-Eng POS tags and dependency relations. Table A3 in Appendix A summarizes all borrowed terms in our treebank.\nTopic-prominence: This type of sentences start with establishing its topic, which often serves as the default one that the rest of the sentence refers to, and they typically employ an object-subjectverb sentence structure (Leimgruber, 2009, 2011). In particular, three subtypes of topic-prominence are observed in the Singlish dependency treebank and their annotations are addressed as follows:\nFirst, topics framed as clausal arguments at the beginning of the sentence are labeled as “csubj” (clausal subject), as shown by “Drive this car” of (1) in Figure 2. This type can be regarded as a variant of the English it-extraposition sentence structure with the extraposition moved to the front and the clause marker removed.\nSecond, noun phrases used to modify the predicate with the presence of a preposition is regarded as a “nsubj” (nominal subject). This is a common order of words used in Chinese and one example is the “SG” of (2) in Figure 2.\nThird, prepositional phrases moved in front are\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nstill treated as “nmod” (nominal modifier) of their intended heads, following the exact definition but as a Singlish-specific form of exemplification, as shown by the “Inside tent” of (3) in Figure 2.\nCopula deletion: Imported from the corresponding Chinese sentence structure, this copula verb is often optional and deleted in Singlish, which is one of its diagnostic characteristics (Leimgruber, 2009, 2011). In UD-Eng standards, predicative “be” is the only verb used as a copula, which often depends on its complement to avoid copular head. Thus the deleted copula and its “cop” (copula) arc are simply ignored to preserve the intactness of the dependency tree, as shown by (4) in Figure 2. The only possibility of a copular root is when the copula has a clausal argument or adjunct. However, such copula is typically not deleted in either Singlish or Chinese, and we have not encountered such cases in our treebank. If they appear by any chance, feasible solutions can be either recovering the copula with a special symbol or promoting the head of the clausal argument or adjunct as the sentence root.\nNP deletion: Noun-phrase (NP) deletion often results in null subjects or objects. It may be regarded as a branch of “Topic-prominence” but is a distinctive feature of Singlish with relatively high frequency of usage (Leimgruber, 2011). Again, we do not recover such relations since the deleted NP imposes negligible alteration to the dependency tree, as exemplified by (5) in Figure 2.\nInversion: Inversion in Singlish involves either reversed order of subject and verb in interrogative sentences, or tag questions in polar interrogatives (Leimgruber, 2011). The former simply involves a change of word orders and thus requires no special treatments. On the other hand, tag questions should be carefully analyzed in two scenarios. One type is in the form of “isn’t it ?” or “haven’t you ?”, which are dependents of the sentence root with the “parataxis” relation.4 The other type is exemplified as “right ?”, and its Singlish equivalent “tio boh ?” (a transliteration from Hokkien) are labeled with the “discourse” (discourse element) relation with respect to the sentence root. See example (6) in Figure 2.\nDiscourse particles: Usage of clausal-final discourse particles, which originates from Hokkien\n4Relation between the main verb of a clause and other sentential elements, such as sentential parenthetical clause, or adjacent sentences without any explicit coordination or subordination.\nand Cantonese, is one of the most typical feature of Singlish (Leimgruber, 2009, 2011; Lim, 2007). All discourse particles that appear in our treebank are summarized in Table A3 in Appendix A with the imported vocabulary:. These words express the tone of the sentence and thus have the “INTJ” (interjection) POS tag and depend on the root of the sentence or clause labeled with “discourse”, as is shown by the “leh” of (3) in Figure 2. The word “one” is a special instance of this type with the sole purpose being a tone marker in Singlish but not English, as shown by (7) in Figure 2."
    }, {
      "heading" : "3.3 Data Selection and Annotation",
      "text" : "Data Source: Singlish is used in written form mainly in social media and local Internet forums. After comparison, we chose the SG Talk Forum5 as our data source due to its relative abundance in Singlish contents. We crawled 84,459 posts using the Scrapy framework6 from pages dated up to 25th December 2016, retaining sentences of length between 5 and 50, which total 58,310. Sentences are reversely sorted according to the log likelihood of the sentence given by an English language model trained using the KenLM toolkit (Heafield et al., 2013)7 normalized by the sentence length, so that those most different from standard English can be chosen. Among the top 10,000 sentences, 1,977 sentences contain unique Singlish vocabularies defined by The Coxford Singlish Dictionary8, A Dictionary of Singlish and Singapore English9, and the Singlish Vocabulary Wikipedia page10. The average normalized log likelihood of these 10,000 sentences is -5.81, and the same measure for all sentences in UD-Eng is -4.81. This means these sentences with Singlish contents are 10 times less probable expressed as standard English than the UD-Eng contents in the web domain. This contrast indicates the degree of lexical deviation of Singlish from English. We chose 1,200 sentences from the first 10,000.\nAnnotation: The chosen texts are divided by random selection into training, development, and testing sets according to the proportion of sentences in the training, development, and test di-\n5http://sgTalk.com 6https://scrapy.org/ 7Trained using the afp eng and xin eng sources of English\nGigaword Fifth Edition (Gigaword). 8http://72.5.72.93/html/lexec.php 9http://www.singlishdictionary.com\n10https://en.wikipedia.org/wiki/ Singlish_vocabulary\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nUD English Singlish Sentences Words Sentences Words\nTrain 12,543 204,586 900 8,221 Dev 2,002 25,148 150 1,384 Test 2,077 25,096 150 1,381\nTable 1: Division of training, development, and test sets for Singlish Treebank\nvision for UD-Eng, as summarized in Table 1. The sentences are tokenized using the NLTK Tokenizer,11 and then annotated using the Dependency Viewer.12 In total, all 17 UD-Eng POS tags and 41 out of the 47 UD-Eng dependency labels are present in the Singlish dependency treebank. Besides, 100 sentences are randomly selected and double annotated by one of the coauthors, and the inter-annotator agreement has an unlabeled attachment score (UAS) of 85.30% and a labeled attachment score (LAS) of 75.72%. A full summary of the number of occurences of each POS tag and dependency labels is included in Appendix A."
    }, {
      "heading" : "4 Part-of-Speech Tagging",
      "text" : "In order to obtain automatically predicted POS tags as features for a base English dependency parser, we train a POS tagger for UD-Eng using the baseline model of Chen et al. (2016), depicted in Figure 3. The bi-LSTM networks with a CRF layer (bi-LSTM-CRF) have shown state-of-the-art performance by globally optimizing the tag sequence (Huang et al., 2015; Chen et al., 2016). Based on this English POS tagging model, we train a POS tagger for Singlish using the featurelevel neural stacking model of Chen et al. (2016). Both the English and Singlish models consist of an input layer, a feature layer, and an output layer."
    }, {
      "heading" : "4.1 Base Bi-LSTM-CRF POS Tagger",
      "text" : "Input Layer: Each token is represented as a vector by concatenating a word embedding from a lookup table with a weighted average of its character embeddings given by the attention model of Bahdanau et al. (2014). Following Chen et al. (2016), the input layer produces a dense representation for the current input token by concatenating its word vector and the ones for its surrounding context tokens in a window of finite size.\n11http://www.nltk.org/api/nltk. tokenize.html\n12http://nlp.nju.edu.cn/tanggc/tools/ DependencyViewer.exe\nx2 x1\n…\nh1\nh1\nh2\nh2\nxn\nhn\nhn\nTanh Tanh Tanh\nLinear Linear Linear\nCRF\n…\n…\n…\n…\n…\nt1 t2 tn …\nOutput layer\nFeature layer\nInput layer\nFigure 3: Base POS tagger\nFeature Layer: This layer employs a bi-LSTM network to encode the input into a sequence of hidden vectors that embody global contextual information. Following Chen et al. (2016), we adopt bi-LSTM with peephole connections (Graves and Schmidhuber, 2005).\nOutput layer: This is a CRF layer to predict the POS tags for the input words by maximizing the conditional probability of the sequence of tags given input sentence."
    }, {
      "heading" : "4.2 POS Tagger with Neural Stacking",
      "text" : "We adopt the deep integration neural stacking structure presented in Chen et al. (2016). As shown in Figure 4, the distributed vector representation for the target word at the input layer of the Singlish Tagger is augmented by concatenating the emission vector produced by the English Tagger with the original word and character-based embeddings, before applying the concatenation within a context window in section 4.1. During training, loss is back-propagated to all trainable parameters in both the Singlish Tagger and the pre-trained feature layer of the base English Tagger. At test time, the input sentence is fed to the integrated tagger model as a whole for inference."
    }, {
      "heading" : "4.3 Results",
      "text" : "We use the publicly available source code13 by Chen et al. (2016) to train a 1-layer biLSTM-CRF based POS tagger on UD-Eng, using 50-dimension pre-trained SENNA word embeddings (Collobert et al., 2011). We set the hidden layer size to 300, the initial learning rate for Adagrad (Duchi et al., 2011) to 0.01, the regularization parameter λ to 10−6, and the dropout rate to 15%. The tagger gives 94.84% accuracy on the UD-Eng\n13https://github.com/chenhongshen/ NNHetSeq\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n…\nh1\nh1\nh2\nh2\nhn\nhn\nTanh Tanh Tanh\nSinglish Tagger output layer\n…\n…\n…\nEnglish Tagger feature layer\n…\nx2 Linear x1 Linear\nxn Linear\nx1 x2 xn\nOutput\nlayer\nFeature\nlayer\nInput layer\nBase English Tagger\nFigure 4: POS tagger with neural stacking\nSystem Accuracy ENG-on-SIN 80.67% Base-ICE-SIN 77.77% Stack-ICE-SIN 84.79%\nTable 2: POS tagging accuracies\ntest set after 24 epochs, chosen according to development tests, which is comparable to the stateof-the-art accuracy of 95.17% reported by Plank et al. (2016). We use these settings to perform 10- fold jackknifing of POS tagging on the UD-Eng training set, with an average accuracy of 95.60%.\nSimilarly, we trained a POS tagger using the Singlish dependency treebank alone with pretrained word embeddings on The Singapore Component of the International Corpus of English (ICE-SIN) (Nihilani, 1992; Ooi, 1997), which consists of both spoken and written texts. However, due to limited amount of training data, the tagging accuracy is not satisfactory even with a large dropout rate to avoid overfitting. In contrast, the neural stacking structure on top of the English base model trained on UD-Eng achieves a POS tagging accuracy of 84.79%14, which corresponds to a 31.58% relative error reduction over the baseline Singlish model, as shown in Table 2. We use this for 10-fold jackknifing on Singlish parsing training data, and tagging the Singlish development and test data.\n14We empirically find that using ICE-SIN embeddings in neural stacking model performs better than using English SENNA embeddings. Similar findings are found for the parser, of which more details are given in section 6.\nOutput\nlayer\nInput layer\nFeature\nlayer\nx2x1\n…\nxn\n…\n…\n…\n…\n…\n……\n…\nℎ1 1\nℎ1 1\nℎ\uD835\uDC5A 1\nℎ\uD835\uDC5A 1\nℎ\uD835\uDC5A 2\nℎ\uD835\uDC5A 2\nℎ1 2\nℎ1 2\nℎ\uD835\uDC5A \uD835\uDC5B\nℎ\uD835\uDC5A \uD835\uDC5B\nℎ1 \uD835\uDC5B\nℎ1 \uD835\uDC5B\nMLPd MLPh MLPd MLPh MLPd MLPh\n1\n…\n1\n1\n… … …\n=\n…\nHd + HhU +1 w S\nFigure 5: Base parser"
    }, {
      "heading" : "5 Dependency Parsing",
      "text" : "We adopt the Dozat and Manning (2016) parser15 as our base model, as displayed in Figure 5, and apply neural stacking to achieve improvements over the baseline parser. Both the base and neural stacking models consist of an input layer, a feature layer, and an output layer."
    }, {
      "heading" : "5.1 Base Parser with Bi-affine Attentions",
      "text" : "Input Layer: This layer encodes the current input word by concatenating a pre-trained word embedding with a trainable word embedding and POS tag embedding from the respective lookup tables.\nFeature Layer: The two recurrent vectors produced by the multi-layer bi-LSTM network from each input vector are concatenated and mapped to multiple feature vectors in lower-dimension space by a set of parallel multilayer perceptron (MLP) layers. Following Dozat and Manning (2016), we adopt Cif-LSTM cells (Greff et al., 2016).\nOutput Layer: This layer applies biaffine transformation on the feature vectors to calculate the score of the directed arcs between every pair of words. The inferred trees for input sentence are formed by choosing the head with the highest score for each word and a cross-entropy loss is calculated to update the model parameters."
    }, {
      "heading" : "5.2 Parser with Neural Stacking",
      "text" : "Inspired by the idea of feature-level neural stacking (Chen et al., 2016; Zhang and Weiss, 2016), we concatenate the pre-trained word embedding,\n15https://github.com/tdozat/Parser\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n654\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n…\n…\n…\n…\n…\n…\n…\nℎ\uD835\uDC5A \uD835\uDC56\nℎ\uD835\uDC5A \uD835\uDC56\nℎ1 \uD835\uDC56\nℎ1 \uD835\uDC56\nℎ\uD835\uDC5A \uD835\uDC57\nℎ\uD835\uDC5A \uD835\uDC57\nℎ1 \uD835\uDC57\nℎ1 \uD835\uDC57\nMLPd MLPh MLPd MLPh\nEnglish Parser Bi-LSTM\nxi\nℎ\uD835\uDC5A \uD835\uDC56\nℎ\uD835\uDC5A \uD835\uDC56\nxi\n…\nMLPd MLPh MLPd MLPh\n…\n…\n…\n…\n…\n…\n…\n…\nSinglish Parser output layer\n+ +… …\n…… …\nxj\nℎ\uD835\uDC5A \uD835\uDC57\nℎ\uD835\uDC5A \uD835\uDC57\n+ +\n……\nxj\n…… …\nOutput\nlayer\nInput layer\nFeature\nlayer\nBase English Parser\nFigure 6: Parser with neural stacking\ntrainable word and tag embeddings, with the two recurrent state vectors at the last bi-LSTM layer of the English Tagger as the input vector for each target word. In order to further preserve syntactic knowledge retained by the English Tagger, the feature vectors from its MLP layer is added to the ones produced by the Singlish Parser, as illustrated in Figure 6, and the scoring tensor of the Singlish Parser is initialized with the one from the trained English Tagger. Loss is back-propagated by reversely traversing all forward paths to all trainable parameter for training and the whole model is used collectively for inference."
    }, {
      "heading" : "6 Experiments and Discussion",
      "text" : ""
    }, {
      "heading" : "6.1 Experimental Settings",
      "text" : "We train an English parser on UD-Eng with the default model settings in Dozat and Manning (2016). It achieves a UAS of 88.88% and a LAS of 85.00%, which are close to the state-of-the-art 85.90% LAS on UD-Eng reported by Ammar et al. (2016), and the main difference is caused by us not using fine-grained POS tags. We apply the same settings for a baseline Singlish parser. We attempt to choose a better configuration of the number of bi-LSTM layers and the hidden dimension based on the development set performance, but the default settings turn out to perform the best. Thus we stick to all default hyperparameters in Dozat and Manning (2016) for training the Singlish parsers.\nWe experimented with different word embeddings, as with the raw text sources summarized in Table 3 and further described in 6.2. When using\nthe neural stacking model, we fix the model configuration for the base English parser model and choose the size of the hidden vector and the number of bi-LSTM layers stacked on top based on the development set. It turns out that a 1-layer biLSTM with 750 hidden dimension performs the best, where the bigger hidden layer accommodates the elongated input vector to the stacked bi-LSTM and the fewer number of recurrent layers avoids overfitting on the small Singlish dependency treebank, given the deep bi-LSTM English parser network at the bottom."
    }, {
      "heading" : "6.2 Investigating Distributed Lexical Characteristics",
      "text" : "In order to learn characteristics of distributed lexical semantics for Singlish, we compare performances of the Singlish dependency parser using several sets of pre-rained word embeddings: GloVe6B, large-scale English word embeddings16; ICE-SIN, Singlish word embeddings trained using GloVe (Pennington et al., 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al., 2014) with the same settings on a comparable size of English data randomly selected from the English Gigaword Fifth Edition for a fair comparison with ICE-SIN embeddings.\nFirst, the English Giga100M embeddings marginally improve the Singlish parser from the baseline without pre-trained embeddings, giving a performance on par with using UD-Eng parser directly on Singlish, represented as “ENG-on-SIN”\n16Trained with Wikipedia 2014 the Gigaword. Downloadable from http://nlp.stanford.edu/data/ glove.6B.zip\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nTopic Prominence Copula Deletion NP Deletion Discourse Particles Others Sentences 15 19 21 51 67\nUAS LAS UAS LAS UAS LAS UAS LAS UAS LAS ENG-on-SIN 73.15 56.48 61.87 51.08 70.29 60.57 70.75 60.00 81.90 71.83 Base-Giga100M 77.78 62.96 79.86 66.91 76.00 65.14 83.75 74.75 70.34 58.40 Base-ICE 74.07 64.81 71.94 61.87 79.43 70.86 87.25 77.50 77.61 63.99 Stack-ICE 79.63 69.44 79.86 71.22 78.86 71.43 88.00 82.75 85.07 76.87\nTable 5: Error analysis with respect to grammar types\nin Table 4. With much more English lexical semantics being fed to the Singlish parser using the English GloVe6B embeddings, the enhancement is still marginal. On the contrary, the Singlish ICESIN embeddings lead to more improvement, with 18.09% relative error reduction compared with 5.11% using the English Giga100M embeddings. It is even better than using the English GloVe6B embeddings despite the huge difference in sizes. This demonstrate the distributional differences between Singlish and English tokens, even though they share a large vocabulary. More detailed comparison is described in section 6.4."
    }, {
      "heading" : "6.3 Knowledge Transfer Using Neural Stacking",
      "text" : "We train a parser with neural stacking and Singlish ICE-SIN embeddings, which achieves the best performance among all the models, with a UAS of 84.21% as shown in Table 4, which corresponds to 36.51% relative error reduction compared to the baseline. This demonstrates that knowledge from English can be successfully incorporated to boost the Singlish parser. This significant improvement is further explained below."
    }, {
      "heading" : "6.4 Improvements over Grammar Types",
      "text" : "To analyze the sources of improvements for Singlish parsing using different model configurations, we conduct error analysis over 5 syntactic categories17, including 4 types of grammars mentioned in section 3.218, and 1 for all other cases, including sentences containing imported vocabularies but expressed in basic English syntax. The number of sentences and the results in each group of the test set are shown in Table 5.\nThe neural stacking model leads to the biggest improvement over nearly all categories except for a slightly lower yet competitive performance on “NP Deletion” cases, which explains the significant overall improvement.\n17Multiple labels are allowed for one sentence. 18The “Inversion” type of grammar is not analyzed since\nthere is only 1 such sentence in the test set.\nComparing the base model with ICE-SIN embeddings with the base parser trained on UD-Eng, which contain syntactic and semantic knowledge in Singlish and English, respectively, the former outperforms the latter on all 4 types of Singlish grammars but not for the remaining samples. This suggests that the base English parser mainly contributes to analyzing basic English syntax, while the Singlish dependency treebank models unique Singlish grammars better.\nUsing the English Giga100M embeddings in the base model only helps to improve parsing for “Topic Prominence” and “Copula Deletion” compared to the Singlish ICE-SIN embeddings. The main reason can be that these two types of grammars preserve a high percentage of English syntax, with only alterations in word orders and deletions of one word. However, “NP Deletion” results in more radical sentence incompleteness and “Discourse Particles” are unique in Singlish. An interesting finding is that feeding English distributed lexical semantic information to the base Singlish parser undermines the performance even over basic English syntax, which again suggests the differences in distributed lexical semantics."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have investigated dependency parsing for Singlish, an important English-based creole language, through annotations of a Singlish dependency treebank with 10,986 words, and building an enhanced parser by leveraging on knowledge transferred from a 20-times-bigger English treebank of Universal Dependencies. We demonstrate the effectiveness of using neural stacking for feature transfer by boosting the Singlish dependency parsing performance to 84.21% UAS, with a 36.51% error reduction. We release the annotated Singlish dependency treebank, the trained model and the source code for the parser with free public access. Possible future work include expanding the investigation to other regional languages such as Malay and Indonesian.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    }, {
      "heading" : "A Statistics of Singlish Dependency Treebank",
      "text" : "POS Tags ADJ 767 INTJ 510 PUNCT 623 ADP 487 NOUN 1717 SCONJ 126 ADV 912 NUM 147 SYM 11 AUX 429 PART 354 VERB 1678 CONJ 167 PRON 675 X 10 DET 386 PROPN 787\nTable A1: Statistics of POS tags\nDependency labels acl 33 dobj 590 acl:relcl 29 expl 10 advcl 186 iobj 15 advmod 829 list 10 appos 18 mwe 100 amod 422 name 117 aux 377 neg 260 auxpass 47 nmod 364 case 463 nmod:npmod 26 cc 167 nmod:poss 153 ccomp 135 nmod:tmod 77 compound 420 nsubj 1005 compound:prt 27 nsubjpass 34 conj 229 nummod 94 cop 152 mark 275 csubj 30 parataxis 223 det 304 punct 626 det:predet 7 remnant 16 discourse 507 vocative 38 dislocated 2 xcomp 187\nTable A2: Statistics of dependency labels\nA-B act blur ah ah beng ah ne angpow arrowed aiyah ang ku kueh angmoh/ang moh ahpek / ah peks atas ba boh/bo boho jiak boh pian buay lin chu buen kuey C chai tow kway chao ah beng chap chye png char kway teow chee cheong fun / che cheong fen cheesepie cheong / chiong chiam / cham chiak liao bee / jiao liao bee chio ching chong chio bu / chiobu chui chop chop chow-angmoh chwee kueh D-F dey diam diam die kock standing die pain pain dun eat grass flip prata fried beehoon G gahmen / garment gam geylang gone case gong kia goreng pisang gui H-J hah / har / huh hai si lang heng hiak hiak hiak hiong hoot hor Hosay / ho say how lian huat jepun kia / jepun kias jialat / jia lak / jia lat K ka kaki kong kaki song kancheong kateks kautim kay kiang kayu kee chia kee siao kelong kena / kana kiam kiasu ki seow kkj kong si mi kopi kopi lui kopi-o kosong koyok ku ku bird L lagi lai liao laksa la / lah lao jio kong lao sai lau chwee nua lau leh liao / ler like dat / like that lim peh lobang loh / lor M mahjong kaki makan ma / mah masak masak mati mee mee pok mee rebus mee siam mee sua mei mei N-S nasi lemak pang sai piak sabo sai same same sia sianz / sian sia suay sibeh siew dai siew siew dai simi taisee soon kuey sotong suay / suey swee T tahan tak pakai te te kee talk cock / talk cock sing song tikopeh tio tio pian/dio pian tong tua U-Z umm zai up lorry / up one’s lorry wahlow / wah lau walaneh / wah lan eh wa / wah xiao ya ya zhun / buay zhun\nTable A3: Imported vocabulary and discourse particles in the Singlish dependency treebank"
    } ],
    "references" : [ {
      "title" : "Many languages, one parser",
      "author" : [ "Waleed Ammar", "George Mulcaire", "Miguel Ballesteros", "Chris Dyer", "Noah Smith." ],
      "venue" : "Transactions of the Association of Computational Linguistics 4:431–444. http://aclweb.org/anthology/Q16-1031.",
      "citeRegEx" : "Ammar et al\\.,? 2016",
      "shortCiteRegEx" : "Ammar et al\\.",
      "year" : 2016
    }, {
      "title" : "Globally normalized transition-based neural networks",
      "author" : [ "Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins." ],
      "venue" : "Proceedings of the 54th ACL. Association",
      "citeRegEx" : "Andor et al\\.,? 2016",
      "shortCiteRegEx" : "Andor et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Improved transition-based parsing by modeling characters instead of words with lstms",
      "author" : [ "Miguel Ballesteros", "Chris Dyer", "A. Noah Smith." ],
      "venue" : "Proceedings of the EMNLP 2015. Association for Computational Linguistics, pages 349–359.",
      "citeRegEx" : "Ballesteros et al\\.,? 2015",
      "shortCiteRegEx" : "Ballesteros et al\\.",
      "year" : 2015
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "Danqi Chen", "Christopher Manning." ],
      "venue" : "Proceedings of the EMNLP 2014. Association for Computational Linguistics, pages 740–750. https://doi.org/10.3115/v1/D14-1082.",
      "citeRegEx" : "Chen and Manning.,? 2014",
      "shortCiteRegEx" : "Chen and Manning.",
      "year" : 2014
    }, {
      "title" : "Neural network for heterogeneous annotations",
      "author" : [ "Hongshen Chen", "Yue Zhang", "Qun Liu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 731–741.",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction",
      "author" : [ "Shay Cohen", "A. Noah Smith." ],
      "venue" : "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the",
      "citeRegEx" : "Cohen and Smith.,? 2009",
      "shortCiteRegEx" : "Cohen and Smith.",
      "year" : 2009
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of Machine Learning Research 12(Aug):2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep biaffine attention for neural dependency parsing",
      "author" : [ "Timothy Dozat", "Christopher D. Manning." ],
      "venue" : "CoRR abs/1611.01734. http://arxiv.org/abs/1611.01734.",
      "citeRegEx" : "Dozat and Manning.,? 2016",
      "shortCiteRegEx" : "Dozat and Manning.",
      "year" : 2016
    }, {
      "title" : "Bidirectional LSTM-CRF models for sequence tagging",
      "author" : [ "Zhiheng Huang", "Wei Xu", "Kai Yu." ],
      "venue" : "CoRR abs/1508.01991. http://arxiv.org/abs/1508.01991.",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Bootstrapping parsers via syntactic projection across parallel texts",
      "author" : [ "Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak." ],
      "venue" : "Nat. Lang. Eng. 11(3):311–325. https://doi.org/10.1017/S1351324905003840.",
      "citeRegEx" : "Hwa et al\\.,? 2005",
      "shortCiteRegEx" : "Hwa et al\\.",
      "year" : 2005
    }, {
      "title" : "Simple and accurate dependency parsing using bidirectional lstm feature representations",
      "author" : [ "Eliyahu Kiperwasser", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association of Computational Linguistics 4:313– 327. http://aclweb.org/anthology/Q16-1023.",
      "citeRegEx" : "Kiperwasser and Goldberg.,? 2016",
      "shortCiteRegEx" : "Kiperwasser and Goldberg.",
      "year" : 2016
    }, {
      "title" : "Modelling variation in Singapore English",
      "author" : [ "Jakob R.E. Leimgruber." ],
      "venue" : "Ph.D. thesis, Oxford University.",
      "citeRegEx" : "Leimgruber.,? 2009",
      "shortCiteRegEx" : "Leimgruber.",
      "year" : 2009
    }, {
      "title" : "Singapore english",
      "author" : [ "Jakob R.E. Leimgruber." ],
      "venue" : "Language and Linguistics Compass 5(1):47–62. https://doi.org/10.1111/j.1749-818X.2010.00262.x.",
      "citeRegEx" : "Leimgruber.,? 2011",
      "shortCiteRegEx" : "Leimgruber.",
      "year" : 2011
    }, {
      "title" : "When are tree structures necessary for deep learning of representations? In Proceedings of the EMNLP 2015",
      "author" : [ "Jiwei Li", "Thang Luong", "Dan Jurafsky", "Eduard Hovy." ],
      "venue" : "Association for Computational Linguistics, pages 2304–2314.",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Mergers and acquisitions: on the ages and origins of singapore english particles",
      "author" : [ "Lisa Lim." ],
      "venue" : "World Englishes 26(4):446–473.",
      "citeRegEx" : "Lim.,? 2007",
      "shortCiteRegEx" : "Lim.",
      "year" : 2007
    }, {
      "title" : "Multi-source transfer of delexicalized dependency parsers",
      "author" : [ "Ryan McDonald", "Slav Petrov", "Keith Hall." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguis-",
      "citeRegEx" : "McDonald et al\\.,? 2011",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2011
    }, {
      "title" : "Dynamics of a contact continuum: Singaporean English",
      "author" : [ "Ho Mian-Lian", "John T. Platt." ],
      "venue" : "Oxford University Press, USA.",
      "citeRegEx" : "Mian.Lian and Platt.,? 1993",
      "shortCiteRegEx" : "Mian.Lian and Platt.",
      "year" : 1993
    }, {
      "title" : "End-to-end relation extraction using lstms on sequences and tree structures",
      "author" : [ "Makoto Miwa", "Mohit Bansal." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association",
      "citeRegEx" : "Miwa and Bansal.,? 2016",
      "shortCiteRegEx" : "Miwa and Bansal.",
      "year" : 2016
    }, {
      "title" : "Selective sharing for multilingual dependency parsing",
      "author" : [ "Tahira Naseem", "Regina Barzilay", "Amir Globerson." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Associ-",
      "citeRegEx" : "Naseem et al\\.,? 2012",
      "shortCiteRegEx" : "Naseem et al\\.",
      "year" : 2012
    }, {
      "title" : "The international computerized corpus of english",
      "author" : [ "Paroo Nihilani." ],
      "venue" : "Words in a cultural context. Singapore: UniPress pages 84–88.",
      "citeRegEx" : "Nihilani.,? 1992",
      "shortCiteRegEx" : "Nihilani.",
      "year" : 1992
    }, {
      "title" : "Universal dependencies v1: A multilingual",
      "author" : [ "Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajic", "Christopher D. Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira", "Reut Tsarfaty", "Daniel Zeman" ],
      "venue" : null,
      "citeRegEx" : "Nivre et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2016
    }, {
      "title" : "Analysing the singapore ice corpus for lexicographic evidence",
      "author" : [ "Vincent BY Ooi" ],
      "venue" : null,
      "citeRegEx" : "Ooi.,? \\Q1997\\E",
      "shortCiteRegEx" : "Ooi.",
      "year" : 1997
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the EMNLP 2014. Association for Computational Linguistics, pages 1532–1543. https://doi.org/10.3115/v1/D14-1162.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss",
      "author" : [ "Barbara Plank", "Anders Søgaard", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 54th ACL (Short Papers). Associa-",
      "citeRegEx" : "Plank et al\\.,? 2016",
      "shortCiteRegEx" : "Plank et al\\.",
      "year" : 2016
    }, {
      "title" : "Troll detection by domain-adapting sentiment analysis",
      "author" : [ "Chun-Wei Seah", "Hai Leong Chieu", "Kian Ming Adam Chai", "Loo-Nin Teow", "Lee Wei Yeong." ],
      "venue" : "18th International Conference on Information Fusion, FUSION 2015, Washington, DC, USA,",
      "citeRegEx" : "Seah et al\\.,? 2015",
      "shortCiteRegEx" : "Seah et al\\.",
      "year" : 2015
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "D. Christopher Manning", "Andrew Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the EMNLP 2013. Asso-",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Cross-lingual word clusters for direct transfer of linguistic structure",
      "author" : [ "Oscar Täckström", "Ryan McDonald", "Jakob Uszkoreit." ],
      "venue" : "Proceedings of the NAACL-HLT 2012. Association for Computational Linguistics, pages 477–487.",
      "citeRegEx" : "Täckström et al\\.,? 2012",
      "shortCiteRegEx" : "Täckström et al\\.",
      "year" : 2012
    }, {
      "title" : "Structured training for neural network transition-based parsing",
      "author" : [ "David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov." ],
      "venue" : "Proceedings of the 53rd ACL and the 7th IJCNLP. Association for Computational Linguistics, pages 323–333.",
      "citeRegEx" : "Weiss et al\\.,? 2015",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical low-rank tensors for multilingual transfer parsing",
      "author" : [ "Yuan Zhang", "Regina Barzilay." ],
      "venue" : "Proceedings of the EMNLP 2015. Association for Computational Linguistics, pages 1857– 1867. https://doi.org/10.18653/v1/D15-1213.",
      "citeRegEx" : "Zhang and Barzilay.,? 2015",
      "shortCiteRegEx" : "Zhang and Barzilay.",
      "year" : 2015
    }, {
      "title" : "Stackpropagation: Improved representation learning for syntax",
      "author" : [ "Yuan Zhang", "David Weiss." ],
      "venue" : "Proceedings of the 54th ACL. Association for Computational Linguistics, pages 1557– 1566. https://doi.org/10.18653/v1/P16-1147.",
      "citeRegEx" : "Zhang and Weiss.,? 2016",
      "shortCiteRegEx" : "Zhang and Weiss.",
      "year" : 2016
    }, {
      "title" : "A neural probabilistic structuredprediction model for transition-based dependency parsing",
      "author" : [ "Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen." ],
      "venue" : "Proceedings of the 53rd ACL and the 7th IJCNLP. Association for",
      "citeRegEx" : "Zhou et al\\.,? 2015",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Since dependency parsers are important for tasks such as information extraction (Miwa and Bansal, 2016) and discourse parsing (Li et al.",
      "startOffset" : 80,
      "endOffset" : 103
    }, {
      "referenceID" : 14,
      "context" : "Since dependency parsers are important for tasks such as information extraction (Miwa and Bansal, 2016) and discourse parsing (Li et al., 2015), this hinders the development of such downstream applications for Singlish in written forms and thus makes it crucial to build a dependency parser that can perform well natively on Singlish.",
      "startOffset" : 126,
      "endOffset" : 143
    }, {
      "referenceID" : 21,
      "context" : "We categorize the challenges and formalize their interpretation using Universal Dependencies (Nivre et al., 2016), which extends to the creation of a Singlish dependency treebank with 1,200 sentences.",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 12,
      "context" : "Singlish is one of the major languages in Singapore, with borrowed vocabulary and grammars from a number of languages including Malay, Tamil, and Chinese dialects such as Hokkien, Cantonese and Teochew (Leimgruber, 2009, 2011), and it has been increasingly used in written forms on web media. Fluent English speakers unfamiliar with Singlish would find the creole hard to comprehend (Harada, 2009). Correspondingly, fundamental English NLP components such as POS taggers and dependency parsers perform poorly on such Singlish texts based on our observations. For example, Seah et al. (2015) adapted the Socher et al.",
      "startOffset" : 203,
      "endOffset" : 591
    }, {
      "referenceID" : 12,
      "context" : "Singlish is one of the major languages in Singapore, with borrowed vocabulary and grammars from a number of languages including Malay, Tamil, and Chinese dialects such as Hokkien, Cantonese and Teochew (Leimgruber, 2009, 2011), and it has been increasingly used in written forms on web media. Fluent English speakers unfamiliar with Singlish would find the creole hard to comprehend (Harada, 2009). Correspondingly, fundamental English NLP components such as POS taggers and dependency parsers perform poorly on such Singlish texts based on our observations. For example, Seah et al. (2015) adapted the Socher et al. (2013) sentiment analysis engine to the Singlish vocabulary, but failed to adapt the parser.",
      "startOffset" : 203,
      "endOffset" : 624
    }, {
      "referenceID" : 8,
      "context" : "In particular, we train a basic Singlish parser with the best off-the-shelf neural dependency parsing model using biaffine attention (Dozat and Manning, 2016), and improve it with knowledge transfer by adopting neural stacking (Chen et al.",
      "startOffset" : 133,
      "endOffset" : 158
    }, {
      "referenceID" : 5,
      "context" : "In particular, we train a basic Singlish parser with the best off-the-shelf neural dependency parsing model using biaffine attention (Dozat and Manning, 2016), and improve it with knowledge transfer by adopting neural stacking (Chen et al., 2016; Zhang and Weiss, 2016) to integrate the English syntax.",
      "startOffset" : 227,
      "endOffset" : 269
    }, {
      "referenceID" : 30,
      "context" : "In particular, we train a basic Singlish parser with the best off-the-shelf neural dependency parsing model using biaffine attention (Dozat and Manning, 2016), and improve it with knowledge transfer by adopting neural stacking (Chen et al., 2016; Zhang and Weiss, 2016) to integrate the English syntax.",
      "startOffset" : 227,
      "endOffset" : 269
    }, {
      "referenceID" : 4,
      "context" : "Since POS tags are important features for dependency parsing (Chen and Manning, 2014; Dyer et al., 2015), we train a POS tagger for Singlish following the same idea by integrating English POS knowledge using neural stacking (Chen et al.",
      "startOffset" : 61,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : ", 2015), we train a POS tagger for Singlish following the same idea by integrating English POS knowledge using neural stacking (Chen et al., 2016; Zhang and Weiss, 2016).",
      "startOffset" : 127,
      "endOffset" : 169
    }, {
      "referenceID" : 30,
      "context" : ", 2015), we train a POS tagger for Singlish following the same idea by integrating English POS knowledge using neural stacking (Chen et al., 2016; Zhang and Weiss, 2016).",
      "startOffset" : 127,
      "endOffset" : 169
    }, {
      "referenceID" : 4,
      "context" : "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016).",
      "startOffset" : 126,
      "endOffset" : 254
    }, {
      "referenceID" : 31,
      "context" : "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016).",
      "startOffset" : 126,
      "endOffset" : 254
    }, {
      "referenceID" : 28,
      "context" : "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016).",
      "startOffset" : 126,
      "endOffset" : 254
    }, {
      "referenceID" : 3,
      "context" : "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016).",
      "startOffset" : 126,
      "endOffset" : 254
    }, {
      "referenceID" : 1,
      "context" : "Neural networks have led to significant advance in the performance for dependency parsing, including transition-based parsing (Chen and Manning, 2014; Zhou et al., 2015; Weiss et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016).",
      "startOffset" : 126,
      "endOffset" : 254
    }, {
      "referenceID" : 11,
      "context" : ", 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016).",
      "startOffset" : 33,
      "endOffset" : 90
    }, {
      "referenceID" : 8,
      "context" : ", 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016).",
      "startOffset" : 33,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : ", 2015; Andor et al., 2016), and graph-based parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016). In particular, the biaffine attention method of Dozat and Manning (2016) uses deep bi-directional long",
      "startOffset" : 8,
      "endOffset" : 185
    }, {
      "referenceID" : 10,
      "context" : "Our work belongs to a line of work on transfer learning for parsing, which leverages English resources in Universal Dependencies to improve the parsing accuracies of low-resource languages (Hwa et al., 2005; Cohen and Smith, 2009; Ganchev et al., 2009).",
      "startOffset" : 189,
      "endOffset" : 252
    }, {
      "referenceID" : 6,
      "context" : "Our work belongs to a line of work on transfer learning for parsing, which leverages English resources in Universal Dependencies to improve the parsing accuracies of low-resource languages (Hwa et al., 2005; Cohen and Smith, 2009; Ganchev et al., 2009).",
      "startOffset" : 189,
      "endOffset" : 252
    }, {
      "referenceID" : 27,
      "context" : "Subsequent work considered syntactic similarities between languages for better feature transfer (Täckström et al., 2012; Naseem et al., 2012; Zhang and Barzilay, 2015).",
      "startOffset" : 96,
      "endOffset" : 167
    }, {
      "referenceID" : 19,
      "context" : "Subsequent work considered syntactic similarities between languages for better feature transfer (Täckström et al., 2012; Naseem et al., 2012; Zhang and Barzilay, 2015).",
      "startOffset" : 96,
      "endOffset" : 167
    }, {
      "referenceID" : 29,
      "context" : "Subsequent work considered syntactic similarities between languages for better feature transfer (Täckström et al., 2012; Naseem et al., 2012; Zhang and Barzilay, 2015).",
      "startOffset" : 96,
      "endOffset" : 167
    }, {
      "referenceID" : 0,
      "context" : "Recently, a line of work leverages neural network models for multi-lingual parsing (Guo et al., 2015; Duong et al., 2015; Ammar et al., 2016).",
      "startOffset" : 83,
      "endOffset" : 141
    }, {
      "referenceID" : 21,
      "context" : "This gives consistency in tokens, POS and dependency labels thanks to the availability of Universal Dependencies (Nivre et al., 2016).",
      "startOffset" : 113,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : ", 2005; Cohen and Smith, 2009; Ganchev et al., 2009). Seminal work employed statistical models. McDonald et al. (2011) investigated delexicalized transfer, where word-based features are removed from a statistical model for English, so that POS and dependency label knowledge can be utilized for training a model for lowresource language.",
      "startOffset" : 8,
      "endOffset" : 119
    }, {
      "referenceID" : 21,
      "context" : "Universal Dependencies provides a set of multilingual treebanks with cross-lingually consistent dependency-based lexicalist annotations, designed to aid development and evaluation for cross-lingual systems, such as multilingual parsers (Nivre et al., 2016).",
      "startOffset" : 236,
      "endOffset" : 256
    }, {
      "referenceID" : 12,
      "context" : "2 Challenges and Solutions for Annotating Singlish The deviations of Singlish from English come from both the lexical and the grammatical levels (Leimgruber, 2009, 2011), which bring challenges for analysis on Singlish using English NLP tools. The former involves imported vocabularies from the first languages of the local people and the latter can be represented by a set of relatively localized features which collectively form 5 unique grammars of Singlish according to Leimgruber (2011). We find empirically that all these deviations can be accommodated by applying the existing English dependency relation definitions, which are explained with examples as follows.",
      "startOffset" : 146,
      "endOffset" : 492
    }, {
      "referenceID" : 13,
      "context" : "It may be regarded as a branch of “Topic-prominence” but is a distinctive feature of Singlish with relatively high frequency of usage (Leimgruber, 2011).",
      "startOffset" : 134,
      "endOffset" : 152
    }, {
      "referenceID" : 13,
      "context" : "Inversion: Inversion in Singlish involves either reversed order of subject and verb in interrogative sentences, or tag questions in polar interrogatives (Leimgruber, 2011).",
      "startOffset" : 153,
      "endOffset" : 171
    }, {
      "referenceID" : 15,
      "context" : "and Cantonese, is one of the most typical feature of Singlish (Leimgruber, 2009, 2011; Lim, 2007).",
      "startOffset" : 62,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "The bi-LSTM networks with a CRF layer (bi-LSTM-CRF) have shown state-of-the-art performance by globally optimizing the tag sequence (Huang et al., 2015; Chen et al., 2016).",
      "startOffset" : 132,
      "endOffset" : 171
    }, {
      "referenceID" : 5,
      "context" : "The bi-LSTM networks with a CRF layer (bi-LSTM-CRF) have shown state-of-the-art performance by globally optimizing the tag sequence (Huang et al., 2015; Chen et al., 2016).",
      "startOffset" : 132,
      "endOffset" : 171
    }, {
      "referenceID" : 5,
      "context" : "In order to obtain automatically predicted POS tags as features for a base English dependency parser, we train a POS tagger for UD-Eng using the baseline model of Chen et al. (2016), depicted in Figure 3.",
      "startOffset" : 163,
      "endOffset" : 182
    }, {
      "referenceID" : 5,
      "context" : "In order to obtain automatically predicted POS tags as features for a base English dependency parser, we train a POS tagger for UD-Eng using the baseline model of Chen et al. (2016), depicted in Figure 3. The bi-LSTM networks with a CRF layer (bi-LSTM-CRF) have shown state-of-the-art performance by globally optimizing the tag sequence (Huang et al., 2015; Chen et al., 2016). Based on this English POS tagging model, we train a POS tagger for Singlish using the featurelevel neural stacking model of Chen et al. (2016). Both the English and Singlish models consist of an input layer, a feature layer, and an output layer.",
      "startOffset" : 163,
      "endOffset" : 521
    }, {
      "referenceID" : 2,
      "context" : "1 Base Bi-LSTM-CRF POS Tagger Input Layer: Each token is represented as a vector by concatenating a word embedding from a lookup table with a weighted average of its character embeddings given by the attention model of Bahdanau et al. (2014). Following Chen et al.",
      "startOffset" : 219,
      "endOffset" : 242
    }, {
      "referenceID" : 2,
      "context" : "1 Base Bi-LSTM-CRF POS Tagger Input Layer: Each token is represented as a vector by concatenating a word embedding from a lookup table with a weighted average of its character embeddings given by the attention model of Bahdanau et al. (2014). Following Chen et al. (2016), the input layer produces a dense representation for the current input token by concatenating its word vector and the ones for its surrounding context tokens in a window of finite size.",
      "startOffset" : 219,
      "endOffset" : 272
    }, {
      "referenceID" : 5,
      "context" : "Following Chen et al. (2016), we adopt bi-LSTM with peephole connections (Graves and Schmidhuber, 2005).",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "2 POS Tagger with Neural Stacking We adopt the deep integration neural stacking structure presented in Chen et al. (2016). As shown in Figure 4, the distributed vector representation for the target word at the input layer of the Singlish Tagger is augmented by concatenating the emission vector produced by the English Tagger with the original word and character-based embeddings, before applying the concatenation within a context window in section 4.",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 7,
      "context" : "(2016) to train a 1-layer biLSTM-CRF based POS tagger on UD-Eng, using 50-dimension pre-trained SENNA word embeddings (Collobert et al., 2011).",
      "startOffset" : 118,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "3 Results We use the publicly available source code13 by Chen et al. (2016) to train a 1-layer biLSTM-CRF based POS tagger on UD-Eng, using 50-dimension pre-trained SENNA word embeddings (Collobert et al.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 24,
      "context" : "17% reported by Plank et al. (2016). We use these settings to perform 10fold jackknifing of POS tagging on the UD-Eng training set, with an average accuracy of 95.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 20,
      "context" : "Similarly, we trained a POS tagger using the Singlish dependency treebank alone with pretrained word embeddings on The Singapore Component of the International Corpus of English (ICE-SIN) (Nihilani, 1992; Ooi, 1997), which consists of both spoken and written texts.",
      "startOffset" : 188,
      "endOffset" : 215
    }, {
      "referenceID" : 22,
      "context" : "Similarly, we trained a POS tagger using the Singlish dependency treebank alone with pretrained word embeddings on The Singapore Component of the International Corpus of English (ICE-SIN) (Nihilani, 1992; Ooi, 1997), which consists of both spoken and written texts.",
      "startOffset" : 188,
      "endOffset" : 215
    }, {
      "referenceID" : 8,
      "context" : "5 Dependency Parsing We adopt the Dozat and Manning (2016) parser15 as our base model, as displayed in Figure 5, and apply neural stacking to achieve improvements over the baseline parser.",
      "startOffset" : 34,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "Following Dozat and Manning (2016), we adopt Cif-LSTM cells (Greff et al.",
      "startOffset" : 10,
      "endOffset" : 35
    }, {
      "referenceID" : 5,
      "context" : "2 Parser with Neural Stacking Inspired by the idea of feature-level neural stacking (Chen et al., 2016; Zhang and Weiss, 2016), we concatenate the pre-trained word embedding, https://github.",
      "startOffset" : 84,
      "endOffset" : 126
    }, {
      "referenceID" : 30,
      "context" : "2 Parser with Neural Stacking Inspired by the idea of feature-level neural stacking (Chen et al., 2016; Zhang and Weiss, 2016), we concatenate the pre-trained word embedding, https://github.",
      "startOffset" : 84,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "1 Experimental Settings We train an English parser on UD-Eng with the default model settings in Dozat and Manning (2016). It achieves a UAS of 88.",
      "startOffset" : 96,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "90% LAS on UD-Eng reported by Ammar et al. (2016), and the main difference is caused by us not using fine-grained POS tags.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "90% LAS on UD-Eng reported by Ammar et al. (2016), and the main difference is caused by us not using fine-grained POS tags. We apply the same settings for a baseline Singlish parser. We attempt to choose a better configuration of the number of bi-LSTM layers and the hidden dimension based on the development set performance, but the default settings turn out to perform the best. Thus we stick to all default hyperparameters in Dozat and Manning (2016) for training the Singlish parsers.",
      "startOffset" : 30,
      "endOffset" : 454
    }, {
      "referenceID" : 23,
      "context" : "2 Investigating Distributed Lexical Characteristics In order to learn characteristics of distributed lexical semantics for Singlish, we compare performances of the Singlish dependency parser using several sets of pre-rained word embeddings: GloVe6B, large-scale English word embeddings16; ICE-SIN, Singlish word embeddings trained using GloVe (Pennington et al., 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al.",
      "startOffset" : 343,
      "endOffset" : 368
    }, {
      "referenceID" : 20,
      "context" : ", 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al.",
      "startOffset" : 23,
      "endOffset" : 50
    }, {
      "referenceID" : 22,
      "context" : ", 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al.",
      "startOffset" : 23,
      "endOffset" : 50
    }, {
      "referenceID" : 23,
      "context" : ", 2014) on the ICE-SIN (Nihilani, 1992; Ooi, 1997) corpus; Giga100M, a small-scale English word embeddings trained using GloVe (Pennington et al., 2014) with the same settings on a comparable size of English data randomly selected from the English Gigaword Fifth Edition for a fair comparison with ICE-SIN embeddings.",
      "startOffset" : 127,
      "endOffset" : 152
    } ],
    "year" : 2017,
    "abstractText" : "Singlish can be interesting to the ACL community both linguistically as a major creole based on English, and computationally for information extraction and sentiment analysis of regional social media. We investigate dependency parsing of Singlish by constructing a dependency treebank under the Universal Dependencies scheme, and then training a neural network model by integrating English syntactic knowledge into a state-ofthe-art parser trained on the Singlish treebank. Results show that English knowledge can lead to 36% relative error reduction, resulting in a parser of around 85% accuracies. We make both our annotation and parser available for further research.",
    "creator" : "LaTeX with hyperref package"
  }
}