{
  "name" : "768.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Detecting Lexical Entailment in Context",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Many NLP applications require detecting relations between word meanings beyond synonymy and paraphrasing. For instance, given “Carlsen plays chess.” and the question “Which game does Carlsen play?”, successfully answering the question requires knowing that chess is a kind of game, or more generally, that chess entails game.\nWhile prior work has defined lexical entailment as a relation between word types (Turney and Mohammad, 2013), we argue entailment relations are better defined when illustrating word meaning with an example in context. Ignoring context is problematic since entailment might hold between some senses of the words, but not others. Consider the word game in two distinct contexts:\n1. The championship game was played in NYC.\n2. The hunters were interested in the big game.\nGiven the sentence, Carlsen is the world chess champion, chess =⇒ game as used in the first context, while chess 6=⇒ game in the second context.\nIn this paper, we investigate how to represent and compare the meaning of words in context for lexical entailment. Since distributional representations for word types have proved useful to detect lexical entailment out of context in supervised settings (Baroni et al., 2012; Roller et al., 2014; Turney and Mohammad, 2013), we propose to transform context-agnostic word type representations into contextualized representations that highlight salient properties of the context (Section 3), and use these contextualized representations with a range of semantic similarity features (Section 4) to successfully detect entailment.\nAs we will see, these context representations significantly improve performance over contextagnostic baselines not only in English, but also between English and French words (Section 7) on two novel datasets (Section 5). We also show that our features are sensitive to word sense changes indicated by context, and adequately capture the direction of entailment relation (Section 8). Moreover, we establish a new state-of-the-art on an existing dataset that captures a broader range of semantic relations in context (Shwartz and Dagan, 2015), and show that the proposed features, induced solely from large amounts of raw text, yield systems that perform as well, or better than existing systems that require additional human annotation."
    }, {
      "heading" : "2 Defining Lexical Entailment in Context",
      "text" : "We frame the task of lexical entailment in context as a binary classification task on examples consisting of a 4-tuple (wl, wr, cl, cr), where wl and wr are two words, and cl and cr are sentences which\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nWords (wl, wr) Exemplars (cl,cr) Does wl =⇒ wr ?\nstaff , stick cl = He walked with the help of a wooden staff . Yes cr = The kid had a candied apple on a stick. staff , body cl = The hospital has an excellent nursing staff . Yes cr = The whole body filed out of the auditorium. staff , stick cl = The hospital has an excellent nursing staff . No cr = The kid had a candied apple on a stick.\nTable 1: Examples of the context-aware lexical entailment task\nillustrate each word usage. The example is treated as positive if wl =⇒ wr, given the meaning of each word exemplified by the contexts, and negative otherwise. Table 1 provides examples.\nWe say that entailment holds if the meaning of wl in the context of cl is more specific than the meaning of wr in the context of cr. The nature of entailment relations captured out-of-context can be broader depending on the test beds considered1. Zhitomirsky-Geffet and Dagan (2009) formalize lexical entailment as a substitutional relationship, which encompasses synonymy, hypernymy, some meronymy relations, and also cause-effect relations. However, we limit entailment to the specificity relation in this work to better understand the impact of context.\nNote that lexical entailment in context is not textual entailment. Recognizing textual entailment (Dagan et al., 2013) and natural language inference (Bowman et al., 2015) involves detecting entailment relations between sentences, while lexical entailment is a relation between words."
    }, {
      "heading" : "3 Representing Words and their Contexts for Entailment",
      "text" : "How can we construct vector representations of the meaning of target words wl and wr in their respective exemplar contexts cl and cr? We start from existing representations for word types which have proven useful for detecting lexical entailment and other semantic relations out of context (Baroni et al., 2012; Kruszewski and Baroni, 2015; Vylomova et al., 2016; Turney and Mohammad, 2013).\nGiven an example (wl, wr, cl, cr), let ~wl and ~wr refer to the context-agnostic representations of wl and wr, and let Cl and Cr represent the matrices obtained by row-wise stacking of the context-\n1We refer the reader to Turney and Mohammad (2013) and Shwartz et al (2017) for comprehensive surveys of supervised and unsupervised methods for the out-of-context task.\nagnostic representations of words in cl and cr respectively."
    }, {
      "heading" : "3.1 Contextualized Word Representations",
      "text" : "Following Thater et al. (2011); Erk and Padó (2008), our first approach is to apply a filter to word type representations to highlight the salient dimensions of the exemplar context, emphasizing relevant dimensions of and downplaying unimportant ones. However, while prior work represents context by averaging word vectors, we propose richer representations that better capture the salient geometrical properties of the exemplar context that might get lost by averaging (Figure 1).\nFirst, we construct fixed length representations for the contexts cl and cr by running convolutional filters over Cl and Cr. Specifically, we calculate the column-wise maximum, minimum and the mean over the matrices Cl and Cr, as done by (Tang et al., 2014) for supervised sentiment classification. This yields three ddimensional vectors for cl (~cl,max, ~cl,min, ~cl,mean), and three d-dimensional vectors for cr (~cr,max, ~cr,min, ~cr,mean). Computing the maximum and minimum across all vector dimensions captures the exterior surface of the “instance manifold”\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n(the volume in embedding space within which all words in the instance reside), while the mean summarizes the density per-dimension within the manifold (Hovy, 2015).\nSecond, we transform initial context-agnostic representations for target word types by taking an element-wise product of the word type vectors ( ~w∗) with vectors representing salient dimensions of the exemplar context (~c∗,max, ~c∗,min, ~c∗,mean) where ∗ ∈ {l, r} . This yields three d-dimensional vectors for wl (~wl,max, ~wl,min, ~wl,mean), and three for wr (~wr,max, ~wr,min, ~wr,mean). We refer to our final word-in-context representations for wl and wr as ~wl,mask and ~wr,mask respectively, where ~wl,mask is the concatenation of ~wl,max, ~wl,min, ~wl,mean, and ~wr,mask is also similarly constructed."
    }, {
      "heading" : "3.2 A Shared Vector Space for Words and Contexts: Context2vec",
      "text" : "An alternative approach to contextualizing word representations is to directly compare the representations of words with representations of contexts. In a single language, this can be done using Context2Vec (Melamud et al., 2016), a neural model that, given a target word and its sentential context, embeds both the word and the context in the same low-dimensional space, with the objective of having the context predict the target word via a log linear model. This model approaches the state-of-the-art on lexical substitution, sentence completion, and supervised word sense disambiguation."
    }, {
      "heading" : "4 Comparing Words and Contexts for Entailment",
      "text" : "Given words and context representations described above, how can we predict entailment?"
    }, {
      "heading" : "4.1 Supervised Logistic Regression model",
      "text" : "Prior work on lexical entailment out of context suggests that the entailment relationship between words is a learnable function of the concatenation of their individual representations (Baroni et al., 2012; Turney and Mohammad, 2013). After concatenation, they are used as features for a logistic regression (Roller et al., 2014) or an SVM classifier (Baroni et al., 2012; Turney and Mohammad, 2013). We follow the same practice with our context-aware word representations. Intuitively, the classifier learns to weight the importance of each dimension of a word representation to detect\nentailment. By learning different weights for the same features describing wl or wr, the classifier can detect asymmetric behavior too, as we will see in Section 8."
    }, {
      "heading" : "4.2 Similarity Features",
      "text" : "We hypothesize that entailment relations hold between related words and introduce similarity features to capture this non-directional relation between words and contexts.\nGiven a pair of vectors (~l, ~r), we use three similarity measures: the cosine similarity cosine(~l, ~r), the dot product ~l · ~r, and the euclidean distance ‖~l − ~r‖. The cosine similarity captures the difference in the two vectors in terms of the angle between then, the euclidean distance measures the difference in magnitude, and the dot product captures both magnitude and angle.\nWe apply these measures to three types of representations. Our first set of similarities is intended to directly capture the similarities between contextualized word representations (Section 3.1). We calculate pairwise similarities between (~wl,max, ~wr,max), (~wl,min, ~wr,min), (~wl,mean, ~wr,mean), as well as between the context-agnostic representations (~wl, ~wr). Second, we add similarities based on Context2Vec representations, i.e. between each pair of ~wl,c2v, ~wr,c2v, ~cl,c2v, and ~cr,c2v. Finally, following Shwartz and Dagan (2015), we capture the similarity of the most relevant word to wl in cr, that to wr in cl, as well as between cl and cr. We therefore add the following three similarities, as well as their pairwise products:\nmax w∈cr ~wl · ~w, max w∈cl ~wr · ~w, max w∈cl,w′∈cr ~w · ~w′\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nWord : Study Example : he knocked on the door of the study\nWord : Study Example : he made several studies before the final painting\nWord : Room Example : the rooms were small but comfortable\nWord : Drawing Example : he did complicated pen-and-ink drawings\nFigure 2: Sample dataset creation process based on two synsets of the word study. The green/solid lines indicate positive examples, while the red/dashed lines indicate negative examples"
    }, {
      "heading" : "5 Evaluation Framework",
      "text" : "We evaluate our models of lexical entailment in context on three complementary datasets."
    }, {
      "heading" : "5.1 CONTEXT-PPDB",
      "text" : "The first dataset, CONTEXT-PPDB, is a finegrained lexical inference dataset created using 375 word pairs from a subset of the English Paraphrase Database (Ganitkevitch et al., 2013; Pavlick et al., 2015b). These word pairs are semiautomatically labeled with semantic relations outof-context. Shwartz and Dagan (2015) augmented them with examples of word usage in context, and re-annotating word pairs given the extra contextual information. The final dataset consists of 3750 words/contexts tuples with a corresponding label from Table 2, one of which is entailment."
    }, {
      "heading" : "5.2 CONTEXT-WN",
      "text" : "In addition to CONTEXT-PPDB, we would like a test set for controlled analysis of lexical entailment in context. We would like to directly assess the sensitivity of our models to contexts that signal different word senses, as well as quantify the extent to which our models detect asymmetric entailment relations rather than semantic similarity.\nBased on the above criteria, we introduce a large-scale dataset automatically extracted from WordNet (Fellbaum, 1998a). We call this dataset CONTEXT-WN. WordNet groups synonyms into synsets. Most synsets are further accompanied by one or more short sentences illustrating the use of the members of the synset. The idea behind CONTEXT-WN is to use these example sentences as context for the words, and hypernymy relations to draw examples of lexical entailment relations. The entire process starts from a seed list of words W and proceeds as follows (see Figure 2 for an illustration) :\n1. For each word type w ∈ W with multiple synsets, obtain all synsets Sw.\n2. For each synset i ∈ Sw, pick a hypernym synset sih, with a corresponding word form wih. Also obtain c\ni and cih which are example sentences corresponding to wi and wih respectively - (wi, wih, c\ni, cih) serves as a positive example.\n3. Permute the positive examples to get negative examples. From (wi, wih, c\ni, cih) and (wj , wjh, c\nj , cjh), generate negative examples (wi, wjh, c i, cjh) and (w j , wih, c j , cih).\n4. Flip the positive examples to generate more negative examples. From (wi, wih, c\ni, cih) generate the negative example (wih, w i, cih, c i).\nWe run this process using the 9000 most frequent words from Wikipedia as W (after filtering the top 1000 as stopwords). This yields a total of 5239 positive examples. We sample an equal number of negative examples from Step 3, with another 5239 examples being generated in Step 4. The final dataset consists of 10478 negative examples.\nCONTEXT-WN satisfies our desiderata. The dataset has a very specific focus since we only pick hypernym-hyponym pairs. The negative examples generated in Steps 3 and 4 require discriminating between different word senses and entailment directions. Finally, with over 15000 examples distributed over 6000 word pairs, the dataset is almost five times as large as CONTEXT-PPDB.\nWe use a 70/5/25 train/dev/test split as in CONTEXT-PPDB. We also ensure that each set contains different word pairs, to avoid memorization and overfitting (Levy et al., 2015)."
    }, {
      "heading" : "5.3 Crosslingual dataset",
      "text" : "Our final dataset takes a cross-lingual view of lexical entailment in context. To create this dataset, we follow the same methodology that was used to create CONTEXT-PPDB, replacing word pairs from PPDB with a set of 100 English-French\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nword pairs labeled as positive out-of-context in the cross-lingual lexical entailment dataset of Vyas and Carpuat (2016). Each word pair (wl, wr) consists of an English word and a French word. To add context, we extract 10 sentence pairs (cl, cr) per word pair from the Wikipedia in the corresponding language. These thousand examples are then given to annotators on a crowdsourcing platform 2. For each example (wl, wr, cl, cr), the annotators were asked to judge whether wl =⇒ wr given the contexts cl and cr. Each example was annotated by 5 workers3.\nFor each example, we assign the majority judgment as the correct label. 4 or more annotators agree on the label on 540 examples, and we retain only these examples as our test set. This yields a dataset of 374 positive examples and 166 negative examples."
    }, {
      "heading" : "6 Experimental Set-up",
      "text" : ""
    }, {
      "heading" : "6.1 Monolingual",
      "text" : "All experiments on CONTEXT-PPDB and CONTEXT-WN are with the default train/dev/test splits. We use 50 dimensional off-the-shelf GloVe embeddings (Pennington et al., 2014) to create ~wl, ~wr, Cl, and Cr. To obtain the Context2Vec representations ~wl,c2v, ~wl,c2v, ~cl,c2v, and ~cr,c2v, we use an existing model trained on the ukWaC corpus (Ferraresi et al., 2006) .\nWe use Logistic Regression as our classifier for both tasks, to allow for direct comparisons with previous work. We do not tune parameters of the classifier, except for adding class weights in the CONTEXT-WN experiments to account for the unbalanced data. As evaluation metric, we use weighted F1 score to compare against previous results, as well as to account for label imbalance in CONTEXT-WN."
    }, {
      "heading" : "6.2 Crosslingual",
      "text" : "We evaluate our models on cross-lingual test set in a transfer setting by training on the training set of CONTEXT-PPDB. To represent ~wl, ~wr, we experiment with two different pre-trained, 200- dimensional, bilingual embeddings - BiVec (Luong et al., 2015) and BiCVM (Hermann and Blunsom, 2014), both of which have been shown to be\n2www.crowdflower.com 3We will release further details of our annotation task including guidelines and the specific task after the review period.\nsuitable for cross-lingual semantic tasks such as dictionary induction and document classification (Upadhyay et al., 2016). The classifier is again a Logistic Regression classifier with class weights."
    }, {
      "heading" : "7 Results",
      "text" : ""
    }, {
      "heading" : "7.1 CONTEXT-PPDB",
      "text" : "Representation-based features only A context-agnostic baseline (Baroni et al., 2012) based on the concatenation of the word type features for (wl, wr) reaches F1 scores of 53.5 using Glove and 54 with Context2Vec (Table 4). The contextualized word representations and similarity features yield the best result and improves performance by 14 points. The similarity features are also effective when used in combination with the Glove or Context2Vec representations, yielding improvements of 12 to 13 points on the baseline. We also note that using ~cl,c2v and ~cr,c2v to add contextual information performs very poorly (F = ~60), regardless of the word type feature, indicating the superiority of our masked representations.\nAdding PPDB-specific features CONTEXTPPDB comes with rich information about word pairs drawn from PPDB (Pavlick et al., 2015a,b). These features include scores for likelihood of context-agnostic entailment labels, distributional similarities, and probabilities of the word pair being paraphrases, among other scores. Shwartz and Dagan (2015) establish a strong baseline of 67 F1 using these features and the most salient word/context similarities described in Section 4.2. Augmenting this system with our proposed context-aware similarities, we observe an improvement of ~2 F1 points. The contextualized representations further gives a ~3 point boost. Overall, our new context aware features help in improving upon the previous state-of-the-art by 4.8 F1 points."
    }, {
      "heading" : "7.2 CONTEXT-WN",
      "text" : "We repeat all experiments on CONTEXT-WN (Table 4), except for those that require PPDB-specific features. The trends on CONTEXT-WN are similar to those on CONTEXT-PPDB. Using similarity features with the context-agnostic baseline, improves performance by ~4 F1 points. Adding the context-aware representations, further gives a tiny boost, enabling us to score 5 F1 points more than\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nWord Type Features Context-aware Features Scores Similarities Representations P R F\n7 All [~wl,mask ; ~wr,mask] 0.674 0.694 0.677 [~wl ; ~wr] 7 7 0.531 0.539 0.535 [~wl ; ~wr] 7 [~wl,mask ; ~wr,mask] 0.512 0.521 0.516 [~wl ; ~wr] All 7 0.642 0.642 0.642 [~wl ; ~wr] All [~wl,mask ; ~wr,mask] 0.638 0.637 0.637 [~wl ; ~wr] All [~cl,c2v ; ~cr,c2v] 0.605 0.604 0.603\n[~wl,c2v ; ~wr,c2v] 7 7 0.533 0.569 0.540 [~wl,c2v ; ~wr,c2v] 7 [~wl,mask ; ~wr,mask] 0.518 0.545 0.526 [~wl,c2v ; ~wr,c2v] All 7 0.655 0.674 0.659 [~wl,c2v ; ~wr,c2v] All [~wl,mask ; ~wr,mask] 0.652 0.670 0.656 [~wl,c2v ; ~wr,c2v] All [~cl,c2v ; ~cr,c2v] 0.601 0.601 0.600 PPDB(wl, wr) Most salient only 7 0.677 0.685 0.670 PPDB(wl, wr) All 7 0.695 0.701 0.692 PPDB(wl, wr) All [~wl,mask ; ~wr,mask] 0.721 0.720 0.718\nTable 3: Experimental results on CONTEXT-PPDB. The evaluation metric is weighted F score.\nthe baseline. Again, we observe that using ~cl,c2v and ~cr,c2v does worse than the alternatives.\nThe absolute value of gains on CONTEXT-WN is smaller than on CONTEXT-PPDB, as can be expected, given that CONTEXT-WN was designed to be challenging (Section 5.2)."
    }, {
      "heading" : "7.3 Cross-lingual dataset",
      "text" : "Results on the cross-lingual dataset are similar to monolingual results (Table 5). Context-aware features outperform context-agnostic baselines - similarities boost the score by 3 and 6 points in the BiVec and BiCVM settings respectively. The contextualized word representations, when used instead of the context-agnostic representations, give a boost of almost 3 points, allowing us to score 6 points and 10 points above the context-agnostic baseline in the BiVec and BiCVM cases, respectively. The strength of these results attest to the generalizable nature of our features, which helps capture correspondences beyond a single language."
    }, {
      "heading" : "8 Analysis",
      "text" : "The experiments in Section 7 attest to the overall strength of our features. In this section, we aim to further test the assumptions underlying our proposed context representations."
    }, {
      "heading" : "8.1 Sensitivity to context",
      "text" : "How sensitive are our models to changes in contexts? To answer this, we focus on the subset of\nCONTEXT-WN that comprises of positive examples, and the equivalent number of negative examples from Step 3 of the dataset creation. These examples are created by permuting the contexts of the positive examples, and thus, can directly help in answering our question. We analyze the predictions made on this subset by our model using a metric we call ‘Macro-F1’, defined as the weighted F1 calculated over each (wl,wr) word pair, and then averaged over all word pairs.\nModels using context-aware features do consistently better on Macro-F1 than those without (Table 4). Interestingly though, the model that uses [~cl,c2v ; ~cr,c2v] does the best on this metric, indicating that while directly using the Context2Vec representations might not be ideal, these representation do capture some useful contextual knowledge."
    }, {
      "heading" : "8.2 Sensitivity to Entailment Direction",
      "text" : "To quantitatively evaluate our claim of our features learning to discriminate directionality 4, we report results on the subset of CONTEXT-WN that consists of all positive examples, and the equivalent number of flipped negative examples generated in Step 4. We measure directionality by looking at the number of example pairs ((wl, wr, cl, cr), (wr, wl, cr, cl)) where both examples are correctly labeled, i.e. the former is labeled as =⇒ and the latter as 6=⇒ .\nContext agnostic baselines detect direction significantly above chance (Table 4). Adding our\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nWord Type Feat.s Context-aware Features Scores Similarities Representations P R F Macro F1 Pairwise Acc.\nRandom 0.5 0.5 0.5 0.5 0.25 7 All [~wl,mask ; ~wr,mask] 0.700 0.661 0.670 0.677 0.623\n[~wl ; ~wr] 7 7 0.653 0.615 0.625 0.594 0.582 [~wl ; ~wr] 7 [~wl,mask ; ~wr,mask] 0.656 0.618 0.628 0.608 0.592 [~wl ; ~wr] All 7 0.700 0.663 0.672 0.651 0.597 [~wl ; ~wr] All [~wl,mask ; ~wr,mask] 0.704 0.666 0.675 0.673 0.617 [~wl ; ~wr] All [~cl,c2v ; ~cr,c2v] 0.692 0.658 0.667 0.682 0.592\n[~wl,c2v ; ~wr,c2v] 7 7 0.695 0.655 0.664 0.663 0.667 [~wl,c2v ; ~wr,c2v] 7 [~wl,mask ; ~wr,mask] 0.694 0.655 0.665 0.668 0.660 [~wl,c2v ; ~wr,c2v] All 7 0.728 0.689 0.697 0.704 0.700 [~wl,c2v ; ~wr,c2v] All [~wl,mask ; ~wr,mask] 0.724 0.685 0.694 0.704 0.691 [~wl,c2v ; ~wr,c2v] All [~cl,c2v ; ~cr,c2v] 0.713 0.678 0.687 0.708 0.638\nTable 4: Results on CONTEXT-WN. Macro-F1 and Pairwise accuracy, are intended to capture contextawareness and directionality-discrimination abilities of our features, repsectively.\nWord Type Features Context-aware Features Scores Similarities Representations P R F Random 0.570 0.481 0.501\n[~wl,bivec ; ~wr,bivec] 7 7 0.618 0.537 0.560 [~wl,bivec ; ~wr,bivec] 7 [~wl,mask ; ~wr,mask] 0.618 0.537 0.560 [~wl,bivec ; ~wr,bivec] All 7 0.632 0.576 0.595 [~wl,bivec ; ~wr,bivec] All [~wl,mask ; ~wr,mask] 0.620 0.554 0.575\n7 All [~wl,mask ; ~wr,mask] 0.650 0.607 0.622 [~wl,bicvm ; ~wr,bicvm] 7 7 0.651 0.517 0.530 [~wl,bicvm ; ~wr,bicvm] 7 [~wl,mask ; ~wr,mask] 0.655 0.528 0.543 [~wl,bicvm ; ~wr,bicvm] All 7 0.681 0.574 0.590 [~wl,bicvm ; ~wr,bicvm] All [~wl,mask ; ~wr,mask] 0.682 0.566 0.581\n7 All [~wl,mask ; ~wr,mask] 0.676 0.614 0.629\nTable 5: Results on cross-lingual dataset with training on CONTEXT-PPDB using bilingual embeddings.\ncontext features improves by ~3, with the overall best performing model again scoring highest on this metric. It is reassuring to observe that improved detection of context with our features does not come at the cost of detecting entailment direction. Unlike in the previous section, the model that uses [~cl,c2v ; ~cr,c2v] is consistently poor, scoring lower than the corresponding context-agnostic baseline."
    }, {
      "heading" : "8.3 Contextualized Masks",
      "text" : "We also hypothesized that masked contextualized representations based on the full volume of the context using min and max operations (Section 3.1) better capture salient context dimensions than the more usual vector averaging approach. We test this hypothesis empirically by replacing\nmasked word-in-context representations ~wl,mask and ~wr,mask by two other ways to capture context. In the first method, we use the mean of the contexts (~cl,mean,~cr,mean). In the second method, we use (~wl,mean, ~wr,mean), i.e. the masked representations calculated by using only the mean. On both CONTEXT-PPDB and CONTEXT-WN, our preferred method outperforms the two alternatives by 2-3 F1 points. In fact, on CONTEXT-WN we can see that our method also captures directionality best."
    }, {
      "heading" : "9 Related Work",
      "text" : "WordNet and lexical entailment The is-a hierarchy of WordNet (Fellbaum, 1998b) is a prominent source of information for unsupervised detection of hypernymy and entailment (Harabagiu\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nDataset Context aware features Scores Similarities Representations P R F Macro F1 Pairwise Acc.\nCONTEXT-PPDB All [~wl,mean ; ~wr,mean] 0.646 0.672 0.643\nNA NAAll [~cl,mean ; ~cr,mean] 0.655 0.678 0.652 All [~wl,mask ; ~wr,mask] 0.674 0.694 0.677\nCONTEXT-WN All [~wl,mean ; ~wr,mean] 0.674 0.635 0.645 0.641 0.577 All [~cl,mean ; ~cr,mean] 0.688 0.648 0.657 0.665 0.608 All [~wl,mask ; ~wr,mask] 0.700 0.661 0.670 0.677 0.623\nTable 6: Impact of masks on CONTEXT-PPDB and CONTEXT-WN.\nand Moldovan, 1998; Shwartz et al., 2015), as well as a source of various datasets (Baroni and Lenci, 2011; Baroni et al., 2012). The dataset we introduce in this work is inspired by the latter line of work, but instead of just extracting word pairs we also obtain exemplar contexts from WordNet.\nModeling word meaning in context Several approaches have been proposed to model the meaning of a word in a given context to capture semantic equivalence in tasks such as lexical substitution, word sense disambiguation or paraphrase ranking (but not entailment).\nOne line of work (Dinu and Lapata, 2010; Reisinger and Mooney, 2010) treats each word as a set of latent word senses. These models methods start with token representations for individual occurrences of a word and then choose a set of token vectors based on the current context. An alternate set of models (Erk and Padó, 2008; Thater et al., 2011; Dinu et al., 2012) avoids defining a fixed set of word senses, and instead contextualizes word type vectors as we do in this paper. These models share the idea of using an element-wise multiplication to apply a context mask to word type representations. The nature of the context representation varies: Erk and Pado (2008) use inverse selectional preferences; Thater et al (2010) combine a first order representation for the context with a second order representation for the target, Thater et al. (2011) rely on syntactic dependencies to define context. Apidianaki (2016) shows that bag-ofword context representation within a small context window works as well as syntactic definitions of context for ranking paraphrases in context.\nThe use of convolution is motivated by success of similar models on sentence classification tasks. Tang et al (2014) uses convolution over embedding matrices for unigrams, bigrams, and trigrams, while Hovy (2015) uses just unigrams. However, all these works only use convolved representations\nto predict properties of the sentence (e.g., sentiment). We use them, instead, to contextualize our target word representations.\nIn-context lexical semantic tasks Besides entailment, other lexical semantic tasks studied in the presence of context include lexical substitution (McCarthy and Navigli, 2007), cross-lingual lexical substitution (Mihalcea et al., 2010) and paraphrase ranking (Apidianaki, 2016). The last work is also notable because of their successful use of models of word-meaning in context from Thater et al (2011), which is closely related to our methods."
    }, {
      "heading" : "10 Conclusion",
      "text" : "We proposed to address lexical entailment in context, providing exemplar sentences to ground the meaning of words being considered for entailment. We show that contextualized word representations constructed by transforming contextagnostic representations, combined with wordcontext similarity features, lead to large improvements over context-agnostic model, not only in English, but also between English and French words on two novel datasets. We also improve the state-of-the-art on a related task of detecting semantic relations in context. Our features are sensitive to changes in entailment based on context, and also capture the directionality of entailment.\nIn future work, we aim to further improve performance on both monolingual and cross-lingual datasets. We anticipate that richer features, capturing second-order comparisons used to detect lexical contrast (Mohammad et al., 2013) and entailment out of context (Turney and Mohammad, 2013), might be useful for this purpose, perhaps in combination with non-linear classifiers. Given the usefulness of similarities, we also plan to investigate asymmetric scoring functions(Kotlerman et al., 2010; Shwartz et al., 2017).\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Vector-space models for PPDB paraphrase ranking in context",
      "author" : [ "Marianna Apidianaki." ],
      "venue" : "Proceedings of EMNLP 2016. Austin, TX, USA, pages 2028–2034.",
      "citeRegEx" : "Apidianaki.,? 2016",
      "shortCiteRegEx" : "Apidianaki.",
      "year" : 2016
    }, {
      "title" : "Linear Algebraic Structure of Word Meanings , with Applications to Polysemy",
      "author" : [ "Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski." ],
      "venue" : "arXiv pages 1–25. http://arxiv.org/pdf/1601.03764v1.pdf.",
      "citeRegEx" : "Arora et al\\.,? 2016",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2016
    }, {
      "title" : "Entailment above the word level in distributional semantics",
      "author" : [ "Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan." ],
      "venue" : "Proceedings of EACL 2012. pages 23–32. http://dl.acm.org/citation.cfm?id=2380822.",
      "citeRegEx" : "Baroni et al\\.,? 2012",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2012
    }, {
      "title" : "How we BLESSed distributional semantic evaluation",
      "author" : [ "Marco Baroni", "Alessandro Lenci." ],
      "venue" : "Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics. pages 1–10.",
      "citeRegEx" : "Baroni and Lenci.,? 2011",
      "shortCiteRegEx" : "Baroni and Lenci.",
      "year" : 2011
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Recognizing Textual Entailment: Models and Applications",
      "author" : [ "Ido Dagan", "Dan Roth", "Mark Sammons", "Fabio Massimo Zanzotto" ],
      "venue" : null,
      "citeRegEx" : "Dagan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2013
    }, {
      "title" : "Measuring Distributional Similarity in Context",
      "author" : [ "Georgiana Dinu", "Mirella Lapata." ],
      "venue" : "Proceedings of EMNLP 2010. Cambridge, MA, USA, pages 1162–1172. http://eprints.pascalnetwork.org/archive/00008156/.",
      "citeRegEx" : "Dinu and Lapata.,? 2010",
      "shortCiteRegEx" : "Dinu and Lapata.",
      "year" : 2010
    }, {
      "title" : "A comparison of models of word meaning in context",
      "author" : [ "Georgiana Dinu", "Stefan Thater", "Sören Laue." ],
      "venue" : "Proceedings of NAACL-HLT 2012. pages 611–615.",
      "citeRegEx" : "Dinu et al\\.,? 2012",
      "shortCiteRegEx" : "Dinu et al\\.",
      "year" : 2012
    }, {
      "title" : "A structured vector space model for word meaning in context",
      "author" : [ "Katrin Erk", "Sebastian Padó." ],
      "venue" : "Proceedings of EMNLP 2010. Honolulu, HA, USA, October, pages 897–906. https://doi.org/10.3115/1613715.1613831.",
      "citeRegEx" : "Erk and Padó.,? 2008",
      "shortCiteRegEx" : "Erk and Padó.",
      "year" : 2008
    }, {
      "title" : "WordNet",
      "author" : [ "Christiane Fellbaum." ],
      "venue" : "Wiley Online Library.",
      "citeRegEx" : "Fellbaum.,? 1998a",
      "shortCiteRegEx" : "Fellbaum.",
      "year" : 1998
    }, {
      "title" : "WordNet: An Electronic Lexical Database",
      "author" : [ "Christiane Fellbaum", "editor" ],
      "venue" : null,
      "citeRegEx" : "Fellbaum and editor.,? \\Q1998\\E",
      "shortCiteRegEx" : "Fellbaum and editor.",
      "year" : 1998
    }, {
      "title" : "Introducing and evaluating ukWaC , a very large web-derived corpus of English",
      "author" : [ "Adriano Ferraresi", "Eros Zanchetta", "Marco Baroni", "Silvia Bernardini." ],
      "venue" : "Proceedings of the 4th Web as Corpus Workshop.",
      "citeRegEx" : "Ferraresi et al\\.,? 2006",
      "shortCiteRegEx" : "Ferraresi et al\\.",
      "year" : 2006
    }, {
      "title" : "PPDB : The Paraphrase Database",
      "author" : [ "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch." ],
      "venue" : "Proceedings of NAACL-HLT 2013 (June):758—-764. http://cs.jhu.edu/ ccb/publications/ppdb.pdf.",
      "citeRegEx" : "Ganitkevitch et al\\.,? 2013",
      "shortCiteRegEx" : "Ganitkevitch et al\\.",
      "year" : 2013
    }, {
      "title" : "Knowledge processing on an extended wordnet",
      "author" : [ "Sanda Harabagiu", "Dan Moldovan." ],
      "venue" : "WordNet: An electronic lexical database 305:381–405.",
      "citeRegEx" : "Harabagiu and Moldovan.,? 1998",
      "shortCiteRegEx" : "Harabagiu and Moldovan.",
      "year" : 1998
    }, {
      "title" : "Multilingual Models for Compositional Distributed Semantics",
      "author" : [ "Karl Moritz Hermann", "Phil Blunsom." ],
      "venue" : "Proceedings of ACL 2014. Baltimore, MD, USA, pages 58–68.",
      "citeRegEx" : "Hermann and Blunsom.,? 2014",
      "shortCiteRegEx" : "Hermann and Blunsom.",
      "year" : 2014
    }, {
      "title" : "Demographic Factors Improve Classification Performance",
      "author" : [ "Dirk Hovy." ],
      "venue" : "Proceedings of ACLIJCNLP 2015. Beijing, China, pages 752–762.",
      "citeRegEx" : "Hovy.,? 2015",
      "shortCiteRegEx" : "Hovy.",
      "year" : 2015
    }, {
      "title" : "Directional Distributional Similarity for Lexical Inference",
      "author" : [ "Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet." ],
      "venue" : "Natural Language Engineering 16(4):359–389. https://doi.org/10.1017/S1351324910000124.",
      "citeRegEx" : "Kotlerman et al\\.,? 2010",
      "shortCiteRegEx" : "Kotlerman et al\\.",
      "year" : 2010
    }, {
      "title" : "Deriving Boolean structures from distributional vectors",
      "author" : [ "German Kruszewski", "Marco Baroni." ],
      "venue" : "Transactions of ACL 3:375–388.",
      "citeRegEx" : "Kruszewski and Baroni.,? 2015",
      "shortCiteRegEx" : "Kruszewski and Baroni.",
      "year" : 2015
    }, {
      "title" : "Do Supervised Distributional Methods Really Learn Lexical Inference Relations? In NAACL HLT 2015",
      "author" : [ "Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan." ],
      "venue" : "pages 970–976.",
      "citeRegEx" : "Levy et al\\.,? 2015",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2015
    }, {
      "title" : "Bilingual Word Representations with Monolingual Quality in Mind",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings Workshop on Vector Modeling for NLP, NAACL 2015. Denver, CO, USA, pages 151–159.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "SemEval2007 Task 10: English Lexical Substitution Task",
      "author" : [ "Diana McCarthy", "Roberto Navigli." ],
      "venue" : "Proceedings of SEMEVAL 2007. pages 48–53. https://doi.org/10.1007/s10579-009-9084-1.",
      "citeRegEx" : "McCarthy and Navigli.,? 2007",
      "shortCiteRegEx" : "McCarthy and Navigli.",
      "year" : 2007
    }, {
      "title" : "context2vec: Learning Generic Context Embedding with Bidirectional LSTM",
      "author" : [ "Oren Melamud", "Jacob Goldberger", "Ido Dagan." ],
      "venue" : "Proceedings of CoNLL 2016. Berlin, Germany, pages 51–61.",
      "citeRegEx" : "Melamud et al\\.,? 2016",
      "shortCiteRegEx" : "Melamud et al\\.",
      "year" : 2016
    }, {
      "title" : "SemEval-2010 Task 2: Cross-Lingual Lexical Substitution",
      "author" : [ "Rada Mihalcea", "Ravi Sinha", "Diana McCarthy." ],
      "venue" : "Proceedings of SemEval 2010 (ACL 2010). Uppsala, Sweden, July, pages 9–14.",
      "citeRegEx" : "Mihalcea et al\\.,? 2010",
      "shortCiteRegEx" : "Mihalcea et al\\.",
      "year" : 2010
    }, {
      "title" : "Computing Lexical Contrast",
      "author" : [ "Saif M. Mohammad", "Bonnie J. Dorr", "Graeme Hirst", "Peter D. Turney." ],
      "venue" : "Computational Linguistics 39(3):555–590.",
      "citeRegEx" : "Mohammad et al\\.,? 2013",
      "shortCiteRegEx" : "Mohammad et al\\.",
      "year" : 2013
    }, {
      "title" : "Adding Semantics to Data-Driven Paraphrasing",
      "author" : [ "Ellie Pavlick", "Johan Bos", "Malvina Nissim", "Charley Beller", "Benjamin Van Durme." ],
      "venue" : "Proceedings of ACL 2015. Beijing, China, pages 1512– 1522.",
      "citeRegEx" : "Pavlick et al\\.,? 2015a",
      "shortCiteRegEx" : "Pavlick et al\\.",
      "year" : 2015
    }, {
      "title" : "2015b. PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification",
      "author" : [ "Ellie Pavlick", "Pushpendre Rastogi", "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch" ],
      "venue" : "Proceedings of ACL-IJCNLP",
      "citeRegEx" : "Pavlick et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pavlick et al\\.",
      "year" : 2015
    }, {
      "title" : "GloVe: Global Vectors for Word Representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of EMNLP 2014. Doha, Qatar, pages 1532–1543. https://doi.org/10.3115/v1/D14-1162.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-Prototype Vector-Space Models of Word Meaning",
      "author" : [ "Joseph Reisinger", "Raymond J Mooney." ],
      "venue" : "Proceedings of NAACL 2010. Los Angeles, CA, June, pages 109–117.",
      "citeRegEx" : "Reisinger and Mooney.,? 2010",
      "shortCiteRegEx" : "Reisinger and Mooney.",
      "year" : 2010
    }, {
      "title" : "Inclusive yet Selective: Supervised Distributional Hypernymy Detection",
      "author" : [ "Stephen Roller", "Katrin Erk", "Gemma Boleda." ],
      "venue" : "Proceedings of COLING 2014 pages 1025–1036.",
      "citeRegEx" : "Roller et al\\.,? 2014",
      "shortCiteRegEx" : "Roller et al\\.",
      "year" : 2014
    }, {
      "title" : "Adding Context to Semantic Data-Driven Paraphrasing",
      "author" : [ "Vered Shwartz", "Ido Dagan." ],
      "venue" : "Proceedings of *SEM 2016. Berlin, Germany, 2015, pages 108–113.",
      "citeRegEx" : "Shwartz and Dagan.,? 2015",
      "shortCiteRegEx" : "Shwartz and Dagan.",
      "year" : 2015
    }, {
      "title" : "Learning to Exploit Structured Resources for Lexical Inference",
      "author" : [ "Vered Shwartz", "Omer Levy", "Ido Dagan", "Jacob Goldberger." ],
      "venue" : "Proceedings of CoNLL 2015. Beijing, China, pages 175–184. http://www.aclweb.org/anthology/K15-1018.",
      "citeRegEx" : "Shwartz et al\\.,? 2015",
      "shortCiteRegEx" : "Shwartz et al\\.",
      "year" : 2015
    }, {
      "title" : "Hypernyms under Siege: Linguistically-motivated Artillery for Hypernymy Detection",
      "author" : [ "Vered Shwartz", "Enrico Santus", "Dominik Schlechtweg." ],
      "venue" : "Proceedings of EACL 2017. Valencia, Spain.",
      "citeRegEx" : "Shwartz et al\\.,? 2017",
      "shortCiteRegEx" : "Shwartz et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning SentimentSpecific Word Embedding",
      "author" : [ "Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin." ],
      "venue" : "Proceedings of ACL 2014. Baltimore, MD, USA, pages 1555–1565. https://doi.org/10.3115/1220575.1220648.",
      "citeRegEx" : "Tang et al\\.,? 2014",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2014
    }, {
      "title" : "Contextualizing Semantic Representations Using Syntactically Enriched Vector Models",
      "author" : [ "Stefan Thater", "Hagen Fuerstenau", "Manfred Pinkal." ],
      "venue" : "Proceedings of ACL 2010. Uppsala, Sweden, July, pages 948–957. http://eprints.pascal-",
      "citeRegEx" : "Thater et al\\.,? 2010",
      "shortCiteRegEx" : "Thater et al\\.",
      "year" : 2010
    }, {
      "title" : "Word Meaning in Context : A Simple and Effective Vector Model",
      "author" : [ "Stefan Thater", "Hagen Fürstenau", "Manfred Pinkal." ],
      "venue" : "Proceedings of IJCNLP 2011. Chiang Mai, Thailand, pages 1134–1143.",
      "citeRegEx" : "Thater et al\\.,? 2011",
      "shortCiteRegEx" : "Thater et al\\.",
      "year" : 2011
    }, {
      "title" : "Experiments with three approaches to recognizing lexical entailment",
      "author" : [ "Peter Turney", "Saif Mohammad." ],
      "venue" : "Natural Language Engineering 1(1):1–42. https://doi.org/10.1017/S1351324913000387.",
      "citeRegEx" : "Turney and Mohammad.,? 2013",
      "shortCiteRegEx" : "Turney and Mohammad.",
      "year" : 2013
    }, {
      "title" : "Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment",
      "author" : [ "Yogarshi Vyas", "Marine Carpuat." ],
      "venue" : "Proceedings of NAACL-HLT 2016 pages 1187–1197.",
      "citeRegEx" : "Vyas and Carpuat.,? 2016",
      "shortCiteRegEx" : "Vyas and Carpuat.",
      "year" : 2016
    }, {
      "title" : "Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning",
      "author" : [ "Ekaterina Vylomova", "Laura Rimell", "Trevor Cohn", "Timothy Baldwin." ],
      "venue" : "Proceedings of ACL 2016. Berlin, Germany, pages",
      "citeRegEx" : "Vylomova et al\\.,? 2016",
      "shortCiteRegEx" : "Vylomova et al\\.",
      "year" : 2016
    }, {
      "title" : "Bootstrapping Distributional Feature Vector Quality",
      "author" : [ "Maayan Zhitomirsky-Geffet", "Ido Dagan." ],
      "venue" : "Computational Linguistics (November 2008).",
      "citeRegEx" : "Zhitomirsky.Geffet and Dagan.,? 2009",
      "shortCiteRegEx" : "Zhitomirsky.Geffet and Dagan.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "We show that contextualized word representations constructed from existing word embeddings, and word-context similarity features lead to significant improvements over context-agnostic models on two novel entailment test sets, and also improve the state-of-the-art on the related task of detecting semantic relations in context (Shwartz and Dagan, 2015).",
      "startOffset" : 327,
      "endOffset" : 352
    }, {
      "referenceID" : 35,
      "context" : "While prior work has defined lexical entailment as a relation between word types (Turney and Mohammad, 2013), we argue entailment relations are better defined when illustrating word meaning with an example in context.",
      "startOffset" : 81,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "Since distributional representations for word types have proved useful to detect lexical entailment out of context in supervised settings (Baroni et al., 2012; Roller et al., 2014; Turney and Mohammad, 2013), we propose to transform context-agnostic word type representations into contextualized representations that highlight salient properties of the context (Section 3), and use these contextualized representations with a range of semantic similarity features (Section 4) to successfully detect entailment.",
      "startOffset" : 138,
      "endOffset" : 207
    }, {
      "referenceID" : 28,
      "context" : "Since distributional representations for word types have proved useful to detect lexical entailment out of context in supervised settings (Baroni et al., 2012; Roller et al., 2014; Turney and Mohammad, 2013), we propose to transform context-agnostic word type representations into contextualized representations that highlight salient properties of the context (Section 3), and use these contextualized representations with a range of semantic similarity features (Section 4) to successfully detect entailment.",
      "startOffset" : 138,
      "endOffset" : 207
    }, {
      "referenceID" : 35,
      "context" : "Since distributional representations for word types have proved useful to detect lexical entailment out of context in supervised settings (Baroni et al., 2012; Roller et al., 2014; Turney and Mohammad, 2013), we propose to transform context-agnostic word type representations into contextualized representations that highlight salient properties of the context (Section 3), and use these contextualized representations with a range of semantic similarity features (Section 4) to successfully detect entailment.",
      "startOffset" : 138,
      "endOffset" : 207
    }, {
      "referenceID" : 29,
      "context" : "Moreover, we establish a new state-of-the-art on an existing dataset that captures a broader range of semantic relations in context (Shwartz and Dagan, 2015), and show that the proposed features, induced solely from large amounts of raw text, yield systems that perform as well, or better than existing systems that require additional human annotation.",
      "startOffset" : 132,
      "endOffset" : 157
    }, {
      "referenceID" : 5,
      "context" : "Recognizing textual entailment (Dagan et al., 2013) and natural language inference (Bowman et al.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 4,
      "context" : ", 2013) and natural language inference (Bowman et al., 2015) involves detecting entailment relations between sentences, while lexical entailment is a relation between words.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 36,
      "context" : "Zhitomirsky-Geffet and Dagan (2009) formalize lexical entailment as a substitutional relationship, which encompasses synonymy, hypernymy, some meronymy relations, and also cause-effect relations.",
      "startOffset" : 0,
      "endOffset" : 36
    }, {
      "referenceID" : 2,
      "context" : "How can we construct vector representations of the meaning of target words wl and wr in their respective exemplar contexts cl and cr? We start from existing representations for word types which have proven useful for detecting lexical entailment and other semantic relations out of context (Baroni et al., 2012; Kruszewski and Baroni, 2015; Vylomova et al., 2016; Turney and Mohammad, 2013).",
      "startOffset" : 290,
      "endOffset" : 390
    }, {
      "referenceID" : 17,
      "context" : "How can we construct vector representations of the meaning of target words wl and wr in their respective exemplar contexts cl and cr? We start from existing representations for word types which have proven useful for detecting lexical entailment and other semantic relations out of context (Baroni et al., 2012; Kruszewski and Baroni, 2015; Vylomova et al., 2016; Turney and Mohammad, 2013).",
      "startOffset" : 290,
      "endOffset" : 390
    }, {
      "referenceID" : 37,
      "context" : "How can we construct vector representations of the meaning of target words wl and wr in their respective exemplar contexts cl and cr? We start from existing representations for word types which have proven useful for detecting lexical entailment and other semantic relations out of context (Baroni et al., 2012; Kruszewski and Baroni, 2015; Vylomova et al., 2016; Turney and Mohammad, 2013).",
      "startOffset" : 290,
      "endOffset" : 390
    }, {
      "referenceID" : 35,
      "context" : "How can we construct vector representations of the meaning of target words wl and wr in their respective exemplar contexts cl and cr? We start from existing representations for word types which have proven useful for detecting lexical entailment and other semantic relations out of context (Baroni et al., 2012; Kruszewski and Baroni, 2015; Vylomova et al., 2016; Turney and Mohammad, 2013).",
      "startOffset" : 290,
      "endOffset" : 390
    }, {
      "referenceID" : 35,
      "context" : "We refer the reader to Turney and Mohammad (2013) and Shwartz et al (2017) for comprehensive surveys of supervised and unsupervised methods for the out-of-context task.",
      "startOffset" : 23,
      "endOffset" : 50
    }, {
      "referenceID" : 35,
      "context" : "We refer the reader to Turney and Mohammad (2013) and Shwartz et al (2017) for comprehensive surveys of supervised and unsupervised methods for the out-of-context task.",
      "startOffset" : 23,
      "endOffset" : 75
    }, {
      "referenceID" : 32,
      "context" : "Specifically, we calculate the column-wise maximum, minimum and the mean over the matrices Cl and Cr, as done by (Tang et al., 2014) for supervised sentiment classification.",
      "startOffset" : 113,
      "endOffset" : 132
    }, {
      "referenceID" : 31,
      "context" : "Following Thater et al. (2011); Erk and Padó (2008), our first approach is to apply a filter to word type representations to highlight the salient dimensions of the exemplar context, emphasizing relevant dimensions of and downplaying unimportant ones.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : "(2011); Erk and Padó (2008), our first approach is to apply a filter to word type representations to highlight the salient dimensions of the exemplar context, emphasizing relevant dimensions of and downplaying unimportant ones.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 15,
      "context" : "(the volume in embedding space within which all words in the instance reside), while the mean summarizes the density per-dimension within the manifold (Hovy, 2015).",
      "startOffset" : 151,
      "endOffset" : 163
    }, {
      "referenceID" : 21,
      "context" : "In a single language, this can be done using Context2Vec (Melamud et al., 2016), a neural model that, given a target word and its sentential context, embeds both the word and the context in the same low-dimensional space, with the objective of having the context predict the target word via a log linear model.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "1 Supervised Logistic Regression model Prior work on lexical entailment out of context suggests that the entailment relationship between words is a learnable function of the concatenation of their individual representations (Baroni et al., 2012; Turney and Mohammad, 2013).",
      "startOffset" : 224,
      "endOffset" : 272
    }, {
      "referenceID" : 35,
      "context" : "1 Supervised Logistic Regression model Prior work on lexical entailment out of context suggests that the entailment relationship between words is a learnable function of the concatenation of their individual representations (Baroni et al., 2012; Turney and Mohammad, 2013).",
      "startOffset" : 224,
      "endOffset" : 272
    }, {
      "referenceID" : 28,
      "context" : "After concatenation, they are used as features for a logistic regression (Roller et al., 2014) or an SVM classifier (Baroni et al.",
      "startOffset" : 73,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : ", 2014) or an SVM classifier (Baroni et al., 2012; Turney and Mohammad, 2013).",
      "startOffset" : 29,
      "endOffset" : 77
    }, {
      "referenceID" : 35,
      "context" : ", 2014) or an SVM classifier (Baroni et al., 2012; Turney and Mohammad, 2013).",
      "startOffset" : 29,
      "endOffset" : 77
    }, {
      "referenceID" : 29,
      "context" : "Finally, following Shwartz and Dagan (2015), we capture the similarity of the most relevant word to wl in cr, that to wr in cl, as well as between cl and cr.",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "1 CONTEXT-PPDB The first dataset, CONTEXT-PPDB, is a finegrained lexical inference dataset created using 375 word pairs from a subset of the English Paraphrase Database (Ganitkevitch et al., 2013; Pavlick et al., 2015b).",
      "startOffset" : 169,
      "endOffset" : 219
    }, {
      "referenceID" : 12,
      "context" : "1 CONTEXT-PPDB The first dataset, CONTEXT-PPDB, is a finegrained lexical inference dataset created using 375 word pairs from a subset of the English Paraphrase Database (Ganitkevitch et al., 2013; Pavlick et al., 2015b). These word pairs are semiautomatically labeled with semantic relations outof-context. Shwartz and Dagan (2015) augmented them with examples of word usage in context, and re-annotating word pairs given the extra contextual information.",
      "startOffset" : 170,
      "endOffset" : 332
    }, {
      "referenceID" : 9,
      "context" : "Based on the above criteria, we introduce a large-scale dataset automatically extracted from WordNet (Fellbaum, 1998a).",
      "startOffset" : 101,
      "endOffset" : 118
    }, {
      "referenceID" : 18,
      "context" : "We also ensure that each set contains different word pairs, to avoid memorization and overfitting (Levy et al., 2015).",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 36,
      "context" : "word pairs labeled as positive out-of-context in the cross-lingual lexical entailment dataset of Vyas and Carpuat (2016). Each word pair (wl, wr) consists of an English word and a French word.",
      "startOffset" : 97,
      "endOffset" : 121
    }, {
      "referenceID" : 26,
      "context" : "We use 50 dimensional off-the-shelf GloVe embeddings (Pennington et al., 2014) to create ~ wl, ~ wr, Cl, and Cr.",
      "startOffset" : 53,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "To obtain the Context2Vec representations ~ wl,c2v, ~ wl,c2v, ~cl,c2v, and ~cr,c2v, we use an existing model trained on the ukWaC corpus (Ferraresi et al., 2006) .",
      "startOffset" : 137,
      "endOffset" : 161
    }, {
      "referenceID" : 19,
      "context" : "To represent ~ wl, ~ wr, we experiment with two different pre-trained, 200dimensional, bilingual embeddings - BiVec (Luong et al., 2015) and BiCVM (Hermann and Blunsom, 2014), both of which have been shown to be",
      "startOffset" : 116,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : ", 2015) and BiCVM (Hermann and Blunsom, 2014), both of which have been shown to be",
      "startOffset" : 18,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "1 CONTEXT-PPDB Representation-based features only A context-agnostic baseline (Baroni et al., 2012) based on the concatenation of the word type features for (wl, wr) reaches F1 scores of 53.",
      "startOffset" : 78,
      "endOffset" : 99
    }, {
      "referenceID" : 24,
      "context" : "Adding PPDB-specific features CONTEXTPPDB comes with rich information about word pairs drawn from PPDB (Pavlick et al., 2015a,b). These features include scores for likelihood of context-agnostic entailment labels, distributional similarities, and probabilities of the word pair being paraphrases, among other scores. Shwartz and Dagan (2015) establish a strong baseline of 67 F1 using these features and the most salient word/context similarities described in Section 4.",
      "startOffset" : 104,
      "endOffset" : 342
    }, {
      "referenceID" : 3,
      "context" : ", 2015), as well as a source of various datasets (Baroni and Lenci, 2011; Baroni et al., 2012).",
      "startOffset" : 49,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : ", 2015), as well as a source of various datasets (Baroni and Lenci, 2011; Baroni et al., 2012).",
      "startOffset" : 49,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "One line of work (Dinu and Lapata, 2010; Reisinger and Mooney, 2010) treats each word as a set of latent word senses.",
      "startOffset" : 17,
      "endOffset" : 68
    }, {
      "referenceID" : 27,
      "context" : "One line of work (Dinu and Lapata, 2010; Reisinger and Mooney, 2010) treats each word as a set of latent word senses.",
      "startOffset" : 17,
      "endOffset" : 68
    }, {
      "referenceID" : 8,
      "context" : "An alternate set of models (Erk and Padó, 2008; Thater et al., 2011; Dinu et al., 2012) avoids defining a fixed set of word senses, and instead contextualizes word type vectors as we do in this paper.",
      "startOffset" : 27,
      "endOffset" : 87
    }, {
      "referenceID" : 34,
      "context" : "An alternate set of models (Erk and Padó, 2008; Thater et al., 2011; Dinu et al., 2012) avoids defining a fixed set of word senses, and instead contextualizes word type vectors as we do in this paper.",
      "startOffset" : 27,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : "An alternate set of models (Erk and Padó, 2008; Thater et al., 2011; Dinu et al., 2012) avoids defining a fixed set of word senses, and instead contextualizes word type vectors as we do in this paper.",
      "startOffset" : 27,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : "One line of work (Dinu and Lapata, 2010; Reisinger and Mooney, 2010) treats each word as a set of latent word senses. These models methods start with token representations for individual occurrences of a word and then choose a set of token vectors based on the current context. An alternate set of models (Erk and Padó, 2008; Thater et al., 2011; Dinu et al., 2012) avoids defining a fixed set of word senses, and instead contextualizes word type vectors as we do in this paper. These models share the idea of using an element-wise multiplication to apply a context mask to word type representations. The nature of the context representation varies: Erk and Pado (2008) use inverse selectional preferences; Thater et al (2010) combine a first order representation for the context with a second order representation for the target, Thater et al.",
      "startOffset" : 18,
      "endOffset" : 670
    }, {
      "referenceID" : 5,
      "context" : "One line of work (Dinu and Lapata, 2010; Reisinger and Mooney, 2010) treats each word as a set of latent word senses. These models methods start with token representations for individual occurrences of a word and then choose a set of token vectors based on the current context. An alternate set of models (Erk and Padó, 2008; Thater et al., 2011; Dinu et al., 2012) avoids defining a fixed set of word senses, and instead contextualizes word type vectors as we do in this paper. These models share the idea of using an element-wise multiplication to apply a context mask to word type representations. The nature of the context representation varies: Erk and Pado (2008) use inverse selectional preferences; Thater et al (2010) combine a first order representation for the context with a second order representation for the target, Thater et al.",
      "startOffset" : 18,
      "endOffset" : 727
    }, {
      "referenceID" : 5,
      "context" : "One line of work (Dinu and Lapata, 2010; Reisinger and Mooney, 2010) treats each word as a set of latent word senses. These models methods start with token representations for individual occurrences of a word and then choose a set of token vectors based on the current context. An alternate set of models (Erk and Padó, 2008; Thater et al., 2011; Dinu et al., 2012) avoids defining a fixed set of word senses, and instead contextualizes word type vectors as we do in this paper. These models share the idea of using an element-wise multiplication to apply a context mask to word type representations. The nature of the context representation varies: Erk and Pado (2008) use inverse selectional preferences; Thater et al (2010) combine a first order representation for the context with a second order representation for the target, Thater et al. (2011) rely on syntactic dependencies to define context.",
      "startOffset" : 18,
      "endOffset" : 852
    }, {
      "referenceID" : 0,
      "context" : "Apidianaki (2016) shows that bag-ofword context representation within a small context window works as well as syntactic definitions of context for ranking paraphrases in context.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "Apidianaki (2016) shows that bag-ofword context representation within a small context window works as well as syntactic definitions of context for ranking paraphrases in context. The use of convolution is motivated by success of similar models on sentence classification tasks. Tang et al (2014) uses convolution over embedding matrices for unigrams, bigrams, and trigrams, while Hovy (2015) uses just unigrams.",
      "startOffset" : 0,
      "endOffset" : 296
    }, {
      "referenceID" : 0,
      "context" : "Apidianaki (2016) shows that bag-ofword context representation within a small context window works as well as syntactic definitions of context for ranking paraphrases in context. The use of convolution is motivated by success of similar models on sentence classification tasks. Tang et al (2014) uses convolution over embedding matrices for unigrams, bigrams, and trigrams, while Hovy (2015) uses just unigrams.",
      "startOffset" : 0,
      "endOffset" : 392
    }, {
      "referenceID" : 20,
      "context" : "In-context lexical semantic tasks Besides entailment, other lexical semantic tasks studied in the presence of context include lexical substitution (McCarthy and Navigli, 2007), cross-lingual lexical substitution (Mihalcea et al.",
      "startOffset" : 147,
      "endOffset" : 175
    }, {
      "referenceID" : 22,
      "context" : "In-context lexical semantic tasks Besides entailment, other lexical semantic tasks studied in the presence of context include lexical substitution (McCarthy and Navigli, 2007), cross-lingual lexical substitution (Mihalcea et al., 2010) and paraphrase ranking (Apidianaki, 2016).",
      "startOffset" : 212,
      "endOffset" : 235
    }, {
      "referenceID" : 0,
      "context" : ", 2010) and paraphrase ranking (Apidianaki, 2016).",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : ", 2010) and paraphrase ranking (Apidianaki, 2016). The last work is also notable because of their successful use of models of word-meaning in context from Thater et al (2011), which is closely related to our methods.",
      "startOffset" : 32,
      "endOffset" : 175
    }, {
      "referenceID" : 23,
      "context" : "We anticipate that richer features, capturing second-order comparisons used to detect lexical contrast (Mohammad et al., 2013) and entailment out of context (Turney and Mohammad, 2013), might be useful for this purpose, perhaps in combination with non-linear classifiers.",
      "startOffset" : 103,
      "endOffset" : 126
    }, {
      "referenceID" : 35,
      "context" : ", 2013) and entailment out of context (Turney and Mohammad, 2013), might be useful for this purpose, perhaps in combination with non-linear classifiers.",
      "startOffset" : 38,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : "Given the usefulness of similarities, we also plan to investigate asymmetric scoring functions(Kotlerman et al., 2010; Shwartz et al., 2017).",
      "startOffset" : 94,
      "endOffset" : 140
    }, {
      "referenceID" : 31,
      "context" : "Given the usefulness of similarities, we also plan to investigate asymmetric scoring functions(Kotlerman et al., 2010; Shwartz et al., 2017).",
      "startOffset" : 94,
      "endOffset" : 140
    } ],
    "year" : 2017,
    "abstractText" : "Detecting entailment between words is a key task for several NLP applications. Previous work has largely focused on entailment between words out of context. We propose, instead, to address lexical entailment in context, providing exemplar sentences to ground the meaning of words considered in the entailment relation. We show that contextualized word representations constructed from existing word embeddings, and word-context similarity features lead to significant improvements over context-agnostic models on two novel entailment test sets, and also improve the state-of-the-art on the related task of detecting semantic relations in context (Shwartz and Dagan, 2015).",
    "creator" : "LaTeX with hyperref package"
  }
}