{
  "name" : "818.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Verb Physics: Relative Physical Knowledge of Actions and Objects",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\nIn this paper, we present an approach to infer relative physical knowledge of actions and objects along six dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different knowledge types improves performance."
    }, {
      "heading" : "1 Introduction",
      "text" : "Reading and reasoning about natural language text often requires trivial knowledge about everyday physical actions and objects. For example, given a sentence “Martin could fit the trophy into the suitcase”, we can trivially infer that the trophy must be smaller than the suitcase, even though it’s not stated explicitly. This reasoning requires knowledge about the action “fit”, in particular, typical preconditions that need to be satisfied in order to perform the action. In addition, reasoning about\nNatural language clues\nRelative physical knowledge about objects\nPhysical implications of actions\n“She barged into the stable.”\nHUMAN STABLE\nsize: smaller\nweight: lighter speed: faster\nstrength: n/a\nrigidness: less rigid\nx barged into y\n⇒ x is smaller than y\n⇒ x is lighter than y\n⇒ x is faster than y\n⇒ x is less rigid than y\nthe applicability of various physical actions in a given situation often requires background knowledge about objects in the world, for example, that people are usually smaller than houses, that cars generally move faster than humans walk, or that a brick probably is heavier than a feather.\nIn fact, the potential use of such knowledge about everyday actions and objects can go beyond language understanding and reasoning. Many open challenges in computer vision and robotics may also benefit from such knowledge, as shown\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nin recent work that requires visual reasoning and entailment (Izadinia et al., 2015; Zhu et al., 2014). Ideally, an AI system should acquire such knowledge through direct physical interactions with the world. However, such a physically interactive system does not seem feasible in a foreseeable future.\nIn this paper, we present an approach to acquire trivial physical knowledge from unstructured natural language text as an alternative knowledge source. In particular, we focus on acquiring relative physical knowledge of actions and objects organized along five dimensions: size, weight, strength, rigidness, and speed. Figure 1 illustrates example knowledge of (1) relative physical relations on object pairs and (2) physical implications of actions when applied to those object pairs.\nWhile natural language text is a rich source to obtain broad knowledge about the world, compiling trivial commonsense knowledge from unstructured text is a nontrivial feat. The central challenge lies in reporting bias: people rarely states the obvious (Gordon and Van Durme, 2013; Sorower et al., 2011), since it goes against Grice’s conversational maxim on the quantity (Grice, 1975) of information.\nIn this work, we demonstrate that it is possible to overcome reporting bias and still extract the unspoken knowledge from language. The key insight is this: there is consistency in the way people describe how they interact with the world, which provides vital clues to reverse engineer the common knowledge shared among people. More concretely, we frame knowledge acquisition as joint inference over two closely related puzzles: inferring relative physical knowledge about object pairs while simultaneously reasoning about physical implications of actions.\nImportantly, four of five dimensions of knowledge in our study — weight, strength, rigidness, speed — are either not visual or not easily recognizable from image recognition using currently available computer vision techniques. Thus, our work provides unique values to complement recent attempts to acquire commonsense knowledge from web images (Izadinia et al., 2015; Bagherinezhad et al., 2016; Sadeghi et al., 2015).\nIn sum, our contributions are threefold:\n• We introduce a new task of commonsense knowledge extraction from language, focusing on physical implications of actions and relative physical relations among objects, or-\nganized along five dimensions. • We propose a model that can infer relations\nover grounded object pairs together with first order relations implied by physical verbs. • We develop a new dataset VERBPHYSICS\nthat compiles crowd-sourced knowledge of actions and objects.1\nThe rest of the paper is organized as follows. We first provide the formal definition of knowledge we aim to learn in Section 2. We then describe our data collection in Section 3 and present our inference model in Section 4. Empirical results and related work follow in Sections 5 and Section 6. We conclude at Section 7."
    }, {
      "heading" : "2 Representation of Relative Physical Knowledge",
      "text" : ""
    }, {
      "heading" : "2.1 Knowledge Dimensions",
      "text" : "We consider five dimensions of relative physical knowledge in this work: size, weight, strength, rigidness, and speed. “Strength” in our work refers to the physical durability of an object (e.g., “diamond” is stronger than “glass”), while “rigidness” refers to the physical flexibility of an object (e.g., “glass” is more rigid than a “wire”). When considered in verb implications, size, weight, strength, rigidness generally concerns pre-conditions of the action, while speed concerns the post-condition of the action."
    }, {
      "heading" : "2.2 Relative physical knowledge",
      "text" : "Let us first consider the problem of representing relative physical knowledge between two objects. We can write a single piece of knowledge like “A person is larger than a basketball” as\nperson >size basketball\nAny propositional statement can have exceptions and counterexamples. Moreover, we need to cope with uncertainties involved in knowledge acquisition. Therefore, we assume each knowledge piece is associated with a probability distribution. More formally, given objects x and y, we define a random variable Oax,y whose range is {>, <,'} with respect to a knowledge dimension a ∈ {SIZE,WEIGHT,STRENGTH,RIGIDNESS,SPEED} so that:\nP (Oax,y = r), r ∈ {>, <,'}. 1Will be publicly shared at Anonymized.URL.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\napprox max width\naction\ntheme Iagent,theme agent goal Itheme,goal\nIagent,goal\n“He threw the ball”\nIagent,theme\nx threw y\n⇒ x is larger than y ⇒ x is heavier than y ⇒ x is slower than y\n“We walked into the house”\nx walked into y\n⇒ x is smaller than y ⇒ x is lighter than y ⇒ x is faster than y\nIagent,goal “I squashed the bug with my boot”\nsquashed x with y\n⇒ x is smaller than y ⇒ x is lighter than y\n⇒ x is weaker than y\nItheme,goal\n⇒ x is less rigid than y\n⇒ x is slower than y\nFigure 2: Example physical implications represented as frame relations between a pair of arguments.\nThis immediately provides two simple properties:\nP (Ox,y = >) = P (Oy,x = <) P (Ox,x = ') = 1"
    }, {
      "heading" : "2.3 Physical Implications of Verbs",
      "text" : "Next we consider representing relative physical implications of actions applied over two objects. For example, consider an action frame “x threw y.” In general, following implications are likely to be true:\n“x threw y” =⇒ x >size y “x threw y” =⇒ x >weight y\nAgain, in order to cope with exceptions and uncertainties, we assume a probability distribution associated with each implication. More formally, we define a random variable F av to denote the implication of the action verb v when applied over its arguments x and y with respect to a knowledge dimension a so that:\nP (F sizethrew = >) := P (“x threw y”⇒ x >size y) P (Fwgtthrew = >) := P (“x threw y”⇒ x >wgt y)\nwhere the range of F sizethrew is {>, <,'}. Intuitively, F sizethrew represents the likely first order relation implied by “throw” over ungrounded (i.e., variable) object pairs.\nAbove definition assumes that there is only a single implication relation for any given verb with\nrespect to a specific knowledge dimension. This is generally not true, since a verb, especially a common action verb, can often invoke a number of different frames according to frame semantics (Fillmore, 1976). Thus, given a number of different frame relations v1...vT associated with a verb v, we define random variables F with respect to a specific frame relation vt, i.e., F avt . We use this notation going forward.\nFrame Perspective on Verb Implications: Figure 2 illustrates the frame centric view on physical implication knowledge we aim to learn. Importantly, the key insight of our work is inspired by Fillmore’s original manuscript on frame semantics (Fillmore, 1976). Fillmore has argued that “frames”—the contexts in which utterances are situated—should be considered as a third primitive of describing a language, along with a grammar and lexicon. While existing frame annotations such as FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and VerbNet (Schuler, 2005) provide a rich frame knowledge associated with a predicate, none of them provide the exact kind of physical implications we consider in our paper. Thus, our work can potentially contribute to these resources by investigating new approaches to automatically recover richer frame knowledge from language."
    }, {
      "heading" : "3 Data and Crowd-sourced Knowledge",
      "text" : "Action Verbs: We pick 50 classes of Levin verbs from both “alternation classes” and “verb\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nclasses” (Levin, 1993), which corresponds to about 1100 unique verbs. We sort this list by frequency of occurrence in our frame patterns in the Google Syntax Ngrams corpus (Goldberg and Orwant, 2013) and pick the top 100 verbs.\nAction Frames: Figure 2 illustrates examples of action frame relations. Because we consider implications over pairwise argument relations for each frame, there are sometimes multiple frame relations we consider for a single frame. To enumerate action frame relations for each verb, we use syntactic patterns based on dependency parse by extracting the core components (subject, verb, direct object, prepositional object) of an action, then map the subject to an agent, the direct object to a theme, and the prepositional object to a goal.2 For those frames that involve an argument in a prepositional phrase, we create a separate frame for each preposition based on the statistics observed in the Google Syntax Ngram corpus.\nBecause the syntax ngram corpus provides only tree snippets without context, this way of enumerating potential frame patterns tend to overgenerate. Thus we refine our prepositions for each frame by taking either the intersection or union with the top 5 Google Surface Ngrams (Michel et al., 2011), depending on whether the frame was under- or over-generating. We also add an additional crowdsourcing step where we ask crowd workers to judge whether a frame pattern with a particular verb and preposition could plausibly be found in a sentence. This process results in 813 frame templates, an average of 8.13 per verb.\nObject Pairs: To provide a source of ground truth relations between objects, we select the object pairs that occur in the 813 frame templates with positive pointwise mutual information (PMI) across the Google Syntax Ngram corpus. After replacing a small set of “human” nouns with a generic HUMAN object, filtering out nouns labeled as abstract by WordNet (Miller, 1995), and distilling all surface forms to their lemmas (also with WordNet), the result is 3656 object pairs."
    }, {
      "heading" : "3.1 Crowdsourcing Knowledge",
      "text" : "We collect human judgements of the frame knowledge implications to use as a small set of seed\n2Future research could use an SRL parser instead. We use dependency parse to benefit from the Google Syntax Ngram dataset that provides language statistics over an extremely large corpus, which does not exist for SRL.\nknowledge (5%), a development set (45%), and a test set (50%). Crowd workers are given with a frame template such as “x threw y,” and then asked to list a few plausible objects (including people and animals) for the missing slots (e.g., x and y).3 We then ask them to rate general relationship that the arguments of the frame exhibit with respect to the knowledge dimension we choose (size, weight, etc.). For each knowledge dimension, or attribute, a, workers select an answer from (1) x >a y, (2) x <a y, (3) x 'a y, or (4) no general relation.\nWe conduct a similar crowdsourcing step for the set of object pairs. We ask crowd workers to compare each of the 3656 object pairs along the five knowledge dimensions we consider, selecting an answer from the same options above as with frames. We reserve 50% of the data as a test set, and split the remainder up either 5% / 45% or 20% / 30% (seed / development) to investigate the effects of different seed knowledge sizes on the model.\nStatistics for the dataset are provided in Table 1. About 90% of the frames as well as object pairs had 2/3 agreement between workers. After removing frame/attribute combinations and object pairs that received less than 2/3 agreement, or were selected by at least 2/3 workers to have no relation,\n3This step is to prime them for thinking about the particular template; we do not use the objects they provided.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nwe end up with roughly 400–600 usable frames and 2100–2500 usable object pairs per attribute."
    }, {
      "heading" : "4 Model",
      "text" : "We model knowledge acquisition as probabilistic inference using a factor graph. As shown in Figure 3, the graph consists of multiple substrates (page-wide boxes) corresponding to different knowledge dimensions (shown only three of them —strength, size, weight—for brevity). Each substrate consists of two types of sub-graphs: verb subgraphs and object subgraphs, which are connected through factors that quantify action–object compatibilities. Connecting across substrates are factors that model inter-dependencies across different knowledge dimensions. In what follows, we describe each graph component."
    }, {
      "heading" : "4.1 Nodes",
      "text" : "In the factor graph, we have two types of nodes in order to capture both classes of knowledge. The object first type of nodes are object pair nodes. Each object pair node is a random variable Oax,y which captures the relative strength of attribute a between objects x and y.\nThe second type of nodes are frame nodes. Each frame node is a random variable F avt . This corresponds to the verb v used in a particular type of frame t, and captures the implied knowledge the frame vt holds along an attribute a.\nAll random variables take on the values {>, <,'}. For an object pair node Oax,y, the value represents the belief about the relation between x and y along attribute a. For a frame node F avt , the value represents the belief about the relation along attribute a between any two objects that might be used in the frame vt.\nWe denote the sets of all object pair and frame random variables O and F , respectively."
    }, {
      "heading" : "4.2 Action–Object Compatibility",
      "text" : "The key aspect of our work is to reason about two types of knowledge simultaneously: relative knowledge on grounded object pairs, and implications of actions related to those objects. Thus we connect the verb subgraphs and object subgraphs through selectional preference factors ψs between two such nodes Oax,y and F a vt if we find evidence from text that suggests objects x and y are used in the frame vt. These factors encourage both random variables to agree on the same value.\nAs an example, consider a node Osizep,b which represents the relative size of a person and a basketball, and a node F sizethrewdobj which represents the relative size implied by an “x threw y” frame. If we find significant evidence in text that “[person] threw [basketball]” occurs, we would add a selectional preference factor to connect Osizep,b with F sizethrewdobj and encourage them towards the same value. This means that if it is discovered that people are larger than basketballs (>), then we would expect the frame “x threw y” to entail x >size y (also >)."
    }, {
      "heading" : "4.3 Semantic Similarities",
      "text" : "Some frames have relatively sparse text evidences to support their corresponding knowledge acquisition. Thus, we include several types of factors based on semantic similarities as described below.\nCross-Verb Frame Similarity We add a group of factors ψv between two verbs v and u (to connect a specific frame of v with a corresponding frame of u) based on the verb-level similarities.\nWithin-Verb Frame Similarity Within each verb v, which consists of a set of frame relations v1, ...vT , we also include frame-level similarity factors ψf between vi and vj . This gives us more evidence over a broader range of frames when textual evidence might be sparse.\nObject Similarity As with verbs, we add factors ψo that encourage similar pairs of objects to take the same value. Given that each node represents a pair of objects, finding that x and y are similar yields two main cases in how to add factors (aside from the trivial case where the variable Oax,y is given a unary factor to encourage the value ').\n1. If nodes Ox,z and Oy,z exist, we expect objects x and y to both have a similar relation to z. We add a factor that encourages Ox,z and Oy,z to take the same value. The same is true if nodes Oz,x and Oz,y exist.\n2. On the other hand, if nodesOx,z andOz,y exist, we expect these two nodes to reach the opposite decision. In this case, we add a factor that encourages one node to take the value > if the other prefers the value <, and vice versa. (For the case of ', if one prefers that value, then both should.)\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nvsizesquish\nvsizethrow\nvsizewalk\nvweightthrow\nvweightwalk … … … F sizethrow1 F sizethrow2 F sizethrow3 F sizethrow4\nf a o v\ns\nf\nv v\na\ns\ns v\na\na\nframes for vsizethrow\no\no\ns\nh a rd\nn e ss\nrandom variable (RV)\ngroup of RVs\nfactor connects RV\nfactors connect subset of RVs\nverb similarity\nframe similarity\nobject similarity\nattribute similarity\nselectional preference\nf\na\no\nv\ns attribute subgraphs\nsubgraphs object\nverb subgraphs\nsi z e\nw e ig\nh t\nOsizes,t\nOsizep,q\nOsizeq,r\nOsizep,s\nst re\nn g th\nvstrengthsquish\nOstrengthp,t Ostrengthp,q\nFigure 3: High level view of the factor graph model. Performance on both learning relative knowledge about objects (right), as well as entailed knowledge from verbs (center) via realized frames (left), is improved by modeling their interplay (orange). Unary seed (ψseed) and embedding (ψemb) factors are omitted for clarity."
    }, {
      "heading" : "4.4 Cross-Knowledge Correlation",
      "text" : "Some knowledge dimensions, such as size and weight, have a significant correlation in their implied relations. For two such attributes a and b, if the same frame F avi and F b vi exists in both graphs, we add a factor ψa between them to push them towards taking the same value."
    }, {
      "heading" : "4.5 Seed Knowledge",
      "text" : "In order to kick off learning, we provide a small set of seed knowledge among the random variables in {O,F} with seed factors ψseed. These unary seed factors push the belief for its associated random variable strongly towards the seed label."
    }, {
      "heading" : "4.6 Potential Functions",
      "text" : "Unary Factors For all frame and object pair random variables in the training set, we train a maximum entropy classifier to predict the value of the variable. We then use the probabilities of the classifier as potentials for seed factors given to all random variables in their class (frame or object pair). Each log-linear classifier is trained separately per attribute on a featurized vector of the variable:\nP (r|Xa) ∝ ewa·f(Xa)\nThe feature function is defined differently according to the node type:\nf(Oap,q) := 〈g(p), g(q)〉 f(F avt) := 〈h(t), g(v), g(t)〉\nHere g(x) is the GloVe word embedding for the word x (t is the frame relation’s preposition, and g(t) is simply set to the zero vector if there is no preposition) and h(t) is a one-hot vector of the frame relation type. We use GloVe vectors of 100 dimensions for verbs and 50 dimensions for objects and prepositions (the dimensions picked based on development set).\nBinary Factors In the case of all other factors, we use a “soft 1” agreement matrix with strong signal down the diagonals: > ' <> 0.7 0.1 0.2\n' 0.15 0.7 0.15 < 0.2 0.1 0.7 "
    }, {
      "heading" : "4.7 Inference",
      "text" : "After our full graph is constructed, we use belief propagation to infer the assignments of frames and object pairs not in our training data. Each message µ is a vector x with probabilities for each value\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nx ∈ {>, <,'}. A message passed from a random variable v to a neighboring factor f is the product of the messages from its other neighboring factors:\nµv→f (x) ∝ ∏\nf ′∈N(v)\\{f}\nµf ′→v(x)\nA message passed from a factor f with potential ψ to a random variable v is a marginalized belief about the value of v of the other neighboring random variables combined with its potential:\nµf→v(x) ∝ ∑\nx′∈x′\\x\nψ(x′) ∏\nv′∈N(f)\\{v}\nµv′→f (x ′ v′)"
    }, {
      "heading" : "5 Experimental Results",
      "text" : "Factor Graph Construction We first need to pick a set of frames and objects to determine our set of random variables. The frames are simply the subset of the frames that were crowdsourced in the given configuration (e.g., seed + dev), with “soft 1” unary seed factors (the gold label indexed row of the binary factor matrix) given only to those in the seed set. The same selection criteria and seed factors are applied to the crowdsourced object pairs.\nFor lexical similarity factors (ψv, ψo), we pick connections based on the cosine similarity scores of GloVe vectors (Pennington et al., 2014) thresholded above a value (0.4) chosen based on development set performance. Attribute similarity factors (ψa) are chosen based on sets of frames that reach largely the same decisions on the seed data (95%). Frame similarity factors (ψf ) are added to pairs of frames with linguistically similar constructions. (Both attribute and frame similarity factors did not help on development set performance and thus were omitted from the final models; their effects are shown in the ablations in Table 3.)\nFinally, selectional preference factors (ψs) are picked by using positive pointwise mutual information (PMI) between the frames and the object pairs’ occurrences in the Google Syntax Ngram corpus.\nBaselines Baselines include making a RANDOM choice, picking between >, <, and '), picking the MAJORITY label, and a maximum entropy classifier based on the embedding representation (EMB-MAXENT)."
    }, {
      "heading" : "5.1 Inferring Knowledge on Actions",
      "text" : "Our first experiment is to predict knowledge implied by new frames. We experiment with two different seed knowledge sets: OUR MODEL (A) is based on more skimp seed knowledge, taking only 5% of the object pair data as seed. In contrast, OUR MODEL (B) is based on 20% of the object pairs as seed knowledge.\nThe full results for the baseline methods and our model are given in Table 4.7. Though the speed attribute has a skewed label distribution, giving the majority baseline high performance, our model outperforms the baselines on other attributes as well as overall.\nMetaphorical Language: While our frame patterns are intended to capture action verbs, our templates also match senses of those verbs that can be used with abstract or metaphorical arguments, rather than directly physical ones. One example from the development set is “x contained y.” While x and y can be real objects, more abstract senses of “contained” could involve y as a “forest fire” or even a “revolution.” In these instances, x >size y is plausible as an abstract notion: if some entity can contain a revolution, we might think that entity as “larger” or “stronger” than the revolution."
    }, {
      "heading" : "5.2 Inferring Knowledge on Objects",
      "text" : "Our second experiment is to predict the correct relations of new object pairs. Results for this task are also given in Table 4.7."
    }, {
      "heading" : "6 Related work",
      "text" : "Several works straddle the gap between IE, knowledge base completion, and learning commonsense knowledge from text. Earlier works in these areas use large amounts of text to try to extract general statements like “A THING CAN BE READABLE” (Gordon et al., 2010) and frequencies of events (Gordon and Schubert, 2012). Our work focuses on specific domains of knowledge rather than general statements or occurrence statistics, and develops a frame- centric approach to circumvent reporting bias. Other work uses a knowledge base and scores unseen tuples based on similarity to existing ones (Angeli and Manning, 2013; Li et al., 2016). Relatedly, previous work uses natural language inference to infer new facts from a dataset of commonsense facts that can be ex-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nDevelopment Test Algorithm size weight stren rigid speed overall size weight stren rigid speed overall RANDOM 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33\nMAJORITY 0.38 0.41 0.42 0.18 0.83 0.43 0.35 0.35 0.43 0.20 0.88 0.44 EMB-MAXENT 0.62 0.64 0.60 0.83 0.83 0.69 0.55 0.55 0.59 0.79 0.88 0.66 OUR MODEL (A) 0.71 0.63 0.61 0.82 0.83 0.71 0.55 0.55 0.55 0.79 0.89 0.65 OUR MODEL (B) 0.74 0.69 0.67 0.82 0.78 0.74 0.76 0.59 0.66 0.80 0.87 0.73\nDevelopment Test Algorithm size weight stren rigid speed overall size weight stren rigid speed overall RANDOM 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33 0.33\nMAJORITY 0.50 0.54 0.51 0.50 0.53 0.51 0.51 0.55 0.52 0.49 0.50 0.51 EMB-MAXENT 0.68 0.66 0.64 0.67 0.65 0.66 0.71 0.67 0.64 0.65 0.63 0.66 OUR MODEL (A) 0.74 0.69 0.67 0.68 0.66 0.69 0.68 0.70 0.66 0.66 0.60 0.66\nTable 2: Accuracy of baselines and our model on both tasks. Top: frame prediction task, OUR MODEL (A) uses 5% object pairs as seed, OUR MODEL (B) uses 20% object pairs as seed. Bottom: object pair prediction task (5% object pairs as seed).\nAblated (or added) component Accuracy – Verb similarity 0.69 + Frame similarity 0.62 – Action-object compatibility 0.62 – Object similarity 0.70 + Attribute similarity 0.62 – Frame embeddings 0.63 – Frame seeds 0.62 – Object embeddings 0.62 – Object seeds 0.62 OUR MODEL (A) 0.71\nTable 3: Ablation results on size attribute for the frame prediction task on the development dataset for OUR MODEL (A) (5% of the object pairs as seed data). Our final model (OUR MODEL (B), using 20% object pairs as seed data), employs only unary embeddings and selctional preference factors.\ntracted from unstructured text (Angeli and Manning, 2014). While our work also focuses on commonsense knowledge, we attempt to directly learn a small number of specific types of knowledge from text without reasoning from an existing database or dataset of facts.\nA handful of works have attempted to learn the types of knowledge we address in this work. One recent work tried to directly predict several binary attributes (such “is large” and “is yellow”) from on off-the-shelf word embeddings, noting that accuracy was very low (Rubinstein et al., 2015). Another line of work addressed grounding verbs in the context of robotic tasks. One paper in this line acquires verb meanings by observing state changes in the environment (She and Chai, 2016). While they have data on 165 verb frames, they only have enough to report statistical signifi-\ncance on four of them. Another work in this line does a deep investigation of eleven verbs, modeling their physical effect via annotated images along eighteen attributes (Gao et al., 2016). These works are encouraging investigations into multimodal groundings of a small set of verbs. Our work instead grounds into a fixed set of attributes but leverages language on a broader scale to learn about more verbs in more diverse set of frames."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We presented a novel take on verb-centric frame semantics to learn implied physical knowledge latent in verbs. We showed that by modeling changes in physical attributes entailed by verbs together with objects that exhibit these properties, we are able to better solve both tasks. Experiments on our novel dataset confirm that a model which takes advantage of physical relations as they arise from verbs and between objects outperforms baselines lacking such information. Our dataset and code will be made publicly available upon publication."
    } ],
    "references" : [ {
      "title" : "Philosophers are mortal: Inferring the truth of unseen facts",
      "author" : [ "Gabor Angeli", "Christopher D Manning." ],
      "venue" : "CoNLL. pages 133–142.",
      "citeRegEx" : "Angeli and Manning.,? 2013",
      "shortCiteRegEx" : "Angeli and Manning.",
      "year" : 2013
    }, {
      "title" : "Naturalli: Natural logic inference for common sense reasoning",
      "author" : [ "Gabor Angeli", "Christopher D Manning." ],
      "venue" : "EMNLP. pages 534–545.",
      "citeRegEx" : "Angeli and Manning.,? 2014",
      "shortCiteRegEx" : "Angeli and Manning.",
      "year" : 2014
    }, {
      "title" : "Are elephants bigger than butterflies? reasoning about sizes of objects",
      "author" : [ "Hessam Bagherinezhad", "Hannaneh Hajishirzi", "Yejin Choi", "Ali Farhadi." ],
      "venue" : "arXiv preprint arXiv:1602.00753 .",
      "citeRegEx" : "Bagherinezhad et al\\.,? 2016",
      "shortCiteRegEx" : "Bagherinezhad et al\\.",
      "year" : 2016
    }, {
      "title" : "The berkeley framenet project",
      "author" : [ "Collin F Baker", "Charles J Fillmore", "John B Lowe." ],
      "venue" : "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics-",
      "citeRegEx" : "Baker et al\\.,? 1998",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 1998
    }, {
      "title" : "Frame semantics and the nature of language",
      "author" : [ "Charles J Fillmore." ],
      "venue" : "Annals of the New York Academy of Sciences 280(1):20–32.",
      "citeRegEx" : "Fillmore.,? 1976",
      "shortCiteRegEx" : "Fillmore.",
      "year" : 1976
    }, {
      "title" : "Physical causality of action verbs in grounded language understanding",
      "author" : [ "Qiaozi Gao", "Malcolm Doering", "Shaohua Yang", "Joyce Y Chai." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL). volume 1,",
      "citeRegEx" : "Gao et al\\.,? 2016",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2016
    }, {
      "title" : "A dataset of syntactic-ngrams over time from a very large corpus of english books",
      "author" : [ "Yoav Goldberg", "Jon Orwant." ],
      "venue" : "Second Joint Conference on Lexical and Computational Semantics (* SEM). volume 1, pages 241–247.",
      "citeRegEx" : "Goldberg and Orwant.,? 2013",
      "shortCiteRegEx" : "Goldberg and Orwant.",
      "year" : 2013
    }, {
      "title" : "Using textual patterns to learn expected event frequencies",
      "author" : [ "Jonathan Gordon", "Lenhart K Schubert." ],
      "venue" : "Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction. Association for Computa-",
      "citeRegEx" : "Gordon and Schubert.,? 2012",
      "shortCiteRegEx" : "Gordon and Schubert.",
      "year" : 2012
    }, {
      "title" : "Reporting bias and knowledge acquisition",
      "author" : [ "Jonathan Gordon", "Benjamin Van Durme." ],
      "venue" : "Proceedings of the 2013 workshop on Automated knowledge base construction. ACM, pages 25–30.",
      "citeRegEx" : "Gordon and Durme.,? 2013",
      "shortCiteRegEx" : "Gordon and Durme.",
      "year" : 2013
    }, {
      "title" : "Learning from the web: Extracting general world knowledge from noisy text",
      "author" : [ "Jonathan Gordon", "Benjamin Van Durme", "Lenhart K Schubert." ],
      "venue" : "Collaboratively-Built Knowledge Sources and AI.",
      "citeRegEx" : "Gordon et al\\.,? 2010",
      "shortCiteRegEx" : "Gordon et al\\.",
      "year" : 2010
    }, {
      "title" : "Logic and conversationin p",
      "author" : [ "HP Grice." ],
      "venue" : "cole and j. morgan (eds.) syntax and semantics volume 3: Speech acts.",
      "citeRegEx" : "Grice.,? 1975",
      "shortCiteRegEx" : "Grice.",
      "year" : 1975
    }, {
      "title" : "Segment-phrase table for semantic segmentation, visual entailment and paraphrasing",
      "author" : [ "Hamid Izadinia", "Fereshteh Sadeghi", "Santosh K Divvala", "Hannaneh Hajishirzi", "Yejin Choi", "Ali Farhadi." ],
      "venue" : "Proceedings of the IEEE International Confer-",
      "citeRegEx" : "Izadinia et al\\.,? 2015",
      "shortCiteRegEx" : "Izadinia et al\\.",
      "year" : 2015
    }, {
      "title" : "English verb classes and alternations: A preliminary investigation",
      "author" : [ "Beth Levin." ],
      "venue" : "University of Chicago press.",
      "citeRegEx" : "Levin.,? 1993",
      "shortCiteRegEx" : "Levin.",
      "year" : 1993
    }, {
      "title" : "Commonsense knowledge base completion",
      "author" : [ "Xiang Li", "Aynaz Taheri", "Lifu Tu", "Kevin Gimpel." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), Berlin, Germany, August. Association for Computa-",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Quantitative analysis of culture using millions of digitized books. science",
      "author" : [ "Jean-Baptiste Michel", "Yuan Kui Shen", "Aviva Presser Aiden", "Adrian Veres", "Matthew K Gray", "Joseph P Pickett", "Dale Hoiberg", "Dan Clancy", "Peter Norvig", "Jon Orwant" ],
      "venue" : null,
      "citeRegEx" : "Michel et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Michel et al\\.",
      "year" : 2011
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A Miller." ],
      "venue" : "Communications of the ACM 38(11):39–",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "The proposition bank: An annotated corpus of semantic roles",
      "author" : [ "Martha Palmer", "Daniel Gildea", "Paul Kingsbury." ],
      "venue" : "Computational linguistics 31(1):71– 106.",
      "citeRegEx" : "Palmer et al\\.,? 2005",
      "shortCiteRegEx" : "Palmer et al\\.",
      "year" : 2005
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP). pages 1532– 1543. http://www.aclweb.org/anthology/D14-1162.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "How well do distributional models capture different types of semantic knowledge? In ACL (2)",
      "author" : [ "Dana Rubinstein", "Effi Levi", "Roy Schwartz", "Ari Rappoport." ],
      "venue" : "pages 726–730.",
      "citeRegEx" : "Rubinstein et al\\.,? 2015",
      "shortCiteRegEx" : "Rubinstein et al\\.",
      "year" : 2015
    }, {
      "title" : "Viske: Visual knowledge extraction and question answering by visual verification of relation phrases",
      "author" : [ "Fereshteh Sadeghi", "Santosh K Kumar Divvala", "Ali Farhadi." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.",
      "citeRegEx" : "Sadeghi et al\\.,? 2015",
      "shortCiteRegEx" : "Sadeghi et al\\.",
      "year" : 2015
    }, {
      "title" : "Verbnet: A broadcoverage, comprehensive verb lexicon",
      "author" : [ "Karin Kipper Schuler" ],
      "venue" : null,
      "citeRegEx" : "Schuler.,? \\Q2005\\E",
      "shortCiteRegEx" : "Schuler.",
      "year" : 2005
    }, {
      "title" : "Incremental acquisition of verb hypothesis space towards physical world interaction",
      "author" : [ "Lanbo She", "Joyce Y Chai." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "She and Chai.,? 2016",
      "shortCiteRegEx" : "She and Chai.",
      "year" : 2016
    }, {
      "title" : "Inverting grice’s maxims to learn rules from natural language extractions",
      "author" : [ "Mohammad S Sorower", "Janardhan R Doppa", "Walker Orr", "Prasad Tadepalli", "Thomas G Dietterich", "Xiaoli Z Fern." ],
      "venue" : "Advances in neural information processing systems.",
      "citeRegEx" : "Sorower et al\\.,? 2011",
      "shortCiteRegEx" : "Sorower et al\\.",
      "year" : 2011
    }, {
      "title" : "Reasoning about object affordances in a knowledge base representation",
      "author" : [ "Yuke Zhu", "Alireza Fathi", "Li Fei-Fei." ],
      "venue" : "European conference on computer vision. Springer, pages 408–424.",
      "citeRegEx" : "Zhu et al\\.,? 2014",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "in recent work that requires visual reasoning and entailment (Izadinia et al., 2015; Zhu et al., 2014).",
      "startOffset" : 61,
      "endOffset" : 102
    }, {
      "referenceID" : 23,
      "context" : "in recent work that requires visual reasoning and entailment (Izadinia et al., 2015; Zhu et al., 2014).",
      "startOffset" : 61,
      "endOffset" : 102
    }, {
      "referenceID" : 22,
      "context" : "The central challenge lies in reporting bias: people rarely states the obvious (Gordon and Van Durme, 2013; Sorower et al., 2011), since it goes against Grice’s conversational maxim on the quantity (Grice, 1975) of information.",
      "startOffset" : 79,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : ", 2011), since it goes against Grice’s conversational maxim on the quantity (Grice, 1975) of information.",
      "startOffset" : 76,
      "endOffset" : 89
    }, {
      "referenceID" : 11,
      "context" : "Thus, our work provides unique values to complement recent attempts to acquire commonsense knowledge from web images (Izadinia et al., 2015; Bagherinezhad et al., 2016; Sadeghi et al., 2015).",
      "startOffset" : 117,
      "endOffset" : 190
    }, {
      "referenceID" : 2,
      "context" : "Thus, our work provides unique values to complement recent attempts to acquire commonsense knowledge from web images (Izadinia et al., 2015; Bagherinezhad et al., 2016; Sadeghi et al., 2015).",
      "startOffset" : 117,
      "endOffset" : 190
    }, {
      "referenceID" : 19,
      "context" : "Thus, our work provides unique values to complement recent attempts to acquire commonsense knowledge from web images (Izadinia et al., 2015; Bagherinezhad et al., 2016; Sadeghi et al., 2015).",
      "startOffset" : 117,
      "endOffset" : 190
    }, {
      "referenceID" : 4,
      "context" : "This is generally not true, since a verb, especially a common action verb, can often invoke a number of different frames according to frame semantics (Fillmore, 1976).",
      "startOffset" : 150,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : "Importantly, the key insight of our work is inspired by Fillmore’s original manuscript on frame semantics (Fillmore, 1976).",
      "startOffset" : 106,
      "endOffset" : 122
    }, {
      "referenceID" : 3,
      "context" : "While existing frame annotations such as FrameNet (Baker et al., 1998), PropBank (Palmer et al.",
      "startOffset" : 50,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : ", 1998), PropBank (Palmer et al., 2005), and VerbNet (Schuler, 2005) provide a rich frame knowledge associated with a predicate, none of them provide the exact kind of physical implications we consider in our paper.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 20,
      "context" : ", 2005), and VerbNet (Schuler, 2005) provide a rich frame knowledge associated with a predicate, none of them provide the exact kind of physical implications we consider in our paper.",
      "startOffset" : 21,
      "endOffset" : 36
    }, {
      "referenceID" : 12,
      "context" : "classes” (Levin, 1993), which corresponds to about 1100 unique verbs.",
      "startOffset" : 9,
      "endOffset" : 22
    }, {
      "referenceID" : 6,
      "context" : "We sort this list by frequency of occurrence in our frame patterns in the Google Syntax Ngrams corpus (Goldberg and Orwant, 2013) and pick the top 100 verbs.",
      "startOffset" : 102,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "Thus we refine our prepositions for each frame by taking either the intersection or union with the top 5 Google Surface Ngrams (Michel et al., 2011), depending on whether the frame was under- or over-generating.",
      "startOffset" : 127,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "After replacing a small set of “human” nouns with a generic HUMAN object, filtering out nouns labeled as abstract by WordNet (Miller, 1995), and distilling all surface forms to their lemmas (also with WordNet), the result is 3656 object pairs.",
      "startOffset" : 125,
      "endOffset" : 139
    }, {
      "referenceID" : 17,
      "context" : "For lexical similarity factors (ψv, ψo), we pick connections based on the cosine similarity scores of GloVe vectors (Pennington et al., 2014) thresholded above a value (0.",
      "startOffset" : 116,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "Earlier works in these areas use large amounts of text to try to extract general statements like “A THING CAN BE READABLE” (Gordon et al., 2010) and frequencies of events (Gordon and Schubert, 2012).",
      "startOffset" : 123,
      "endOffset" : 144
    }, {
      "referenceID" : 7,
      "context" : ", 2010) and frequencies of events (Gordon and Schubert, 2012).",
      "startOffset" : 34,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "Other work uses a knowledge base and scores unseen tuples based on similarity to existing ones (Angeli and Manning, 2013; Li et al., 2016).",
      "startOffset" : 95,
      "endOffset" : 138
    }, {
      "referenceID" : 13,
      "context" : "Other work uses a knowledge base and scores unseen tuples based on similarity to existing ones (Angeli and Manning, 2013; Li et al., 2016).",
      "startOffset" : 95,
      "endOffset" : 138
    }, {
      "referenceID" : 1,
      "context" : "tracted from unstructured text (Angeli and Manning, 2014).",
      "startOffset" : 31,
      "endOffset" : 57
    }, {
      "referenceID" : 18,
      "context" : "One recent work tried to directly predict several binary attributes (such “is large” and “is yellow”) from on off-the-shelf word embeddings, noting that accuracy was very low (Rubinstein et al., 2015).",
      "startOffset" : 175,
      "endOffset" : 200
    }, {
      "referenceID" : 21,
      "context" : "One paper in this line acquires verb meanings by observing state changes in the environment (She and Chai, 2016).",
      "startOffset" : 92,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "Another work in this line does a deep investigation of eleven verbs, modeling their physical effect via annotated images along eighteen attributes (Gao et al., 2016).",
      "startOffset" : 147,
      "endOffset" : 165
    } ],
    "year" : 2017,
    "abstractText" : "Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., “my house is bigger than me”. However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like “John entered his house” implies that his house is bigger than John. In this paper, we present an approach to infer relative physical knowledge of actions and objects along six dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different knowledge types improves performance.",
    "creator" : "LaTeX with hyperref package"
  }
}