{
  "name" : "481.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Most human language understanding is grounded in perception. There is thus growing interest in combining information from language and vision in the NLP and AI communities.\nSo far, the primary testbeds of Language and Vision (LaVi) models have been ‘Visual Question Answering’ (VQA) (e.g. Antol et al. (2015); Malinowski and Fritz (2014); Malinowski et al. (2015); Gao et al. (2015); Ren et al. (2015)) and ‘Image Captioning’ (IC) (e.g. Hodosh et al. (2013); Fang et al. (2015); Chen and Lawrence Zitnick (2015); Donahue et al. (2015); Karpathy and Fei-Fei (2015); Vinyals et al. (2015)). Whilst some models have seemed extremely successful\non those tasks, it remains unclear how the reported results should be interpreted and what those models are actually learning. There is an emerging feeling in the LaVi community that the current VQA task should be revised, especially as it has been shown that it can be handled well by ‘blind’ models which use only language input or by simple concatenation of language and vision features (Agarwal et al., 2016; Jabri et al., 2016; Zhang et al., 2016; Goyal et al., 2016). In IC too, Hodosh and Hockenmaier (2016) showed that, contrarily to what prior research had suggested, the task is far from been solved, since IC models are not able to distinguish between a correct and incorrect caption.\nSuch results indicate that in current datasets, language provides priors that make it possible for a LaVi model to be successful without truly understanding and integrating language and vision. But problems do not stop at biasing. Johnson et al. (2016) highlight a second weakness of such datasets, pointing out that “they conflate multiple sources of error, making it hard to pinpoint model weaknesses”. The authors thus suggest the need for a diagnostic dataset. Thirdly, it has been observed that existing IC evaluation metrics are sen-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nsitive to n-gram overlap and that there is a need for measures that better substitute human judgments (Hodosh et al., 2013; Elliott and Keller, 2014; Anderson et al., 2016).\nOur paper contributes to investigate these three weaknesses of current VQA and IC research by proposing an automatic method for creating a big dataset of real images with minimal language bias and some diagnostic abilities. Our dataset, called FOIL (Find One mismatch between Image and Language caption), consists of images associated with incorrect captions (Figure 1). We only introduce one error (or ‘foil’) per caption (i.e. the caption is ‘nearly’ correct). Specifically, we replace one word in the original caption with an incorrect one. This allows us to produce challenging error-detection/correction tasks while simultaneously providing ground truth (we know where the error is) that can be used to objectively measure the performance of current models. Given this data, we propose three tasks based on widely accepted evaluation measures. We evaluate a) the ability of the system to compute whether a caption is compatible (correct) or incompatible (foil) with the image (T1); b) when it is incompatible, what makes it so: the system has to highlight the mismatch in the caption (T2); c) which word should be used to correct the caption by replacing the foil word (T3). T1 is related to IC, but instead of generating a caption from scratch, we investigate the ability of the system to understand that the caption does not describe the image correctly. T2 and T3 are more related to VQA, however require a deep understanding of an image, as the aim is to spot a single mistake in an otherwise correct caption, with foils potentially close to the correct words.\nThe dataset presented in this paper (Section 3) is built on top of MS-COCO (Lin et al., 2014), and contains 297,268 datapoints and 97,847 images. We will refer to it as FOIL-COCO. We evaluate two state-of-the-art models: the popular VQA system proposed by Antol et al. (2015); Lu et al. (2015), and the attention-based model by Lu et al. (2016). We show that those models perform close to chance level, while humans are capable of doing the task accurately (Section 4). Section 5 provides an analysis of our results, allowing us to diagnose three failures of LaVi models. First, their coarse representations of language and visual input do not encode suitably structured information to spot mismatches between an utterance and the\ncorresponding scene (tested by T1). Second, their language representation is not fine-grained enough to identify the part of an utterance that causes a mismatch with the image as it is (T2). Third, their visual representation is also too poor to spot and name the visual area that corresponds to a marked mistake in the text (T3)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Image captioning (IC) and visual question answering (VQA) tasks are the most relevant to our work. In IC (Fang et al., 2015; Chen and Lawrence Zitnick, 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015), the goal is to generate a caption for a given image, such that it is both semantically and syntactically correct, and properly describes the content of that image. In VQA (Antol et al., 2015; Malinowski and Fritz, 2014; Malinowski et al., 2015; Gao et al., 2015; Ren et al., 2015), the system attempts to answer open-ended questions related to the content of a given image. There is a wealth of literature on both tasks, but we only discuss here the ones most related to our work and refer the reader to the recent surveys by (Bernardi et al., 2016; Wu et al., 2016).\nDespite the successes of the state-of-the-art LaVi models, it is unclear whether they capture vision and language in a truly integrative fashion. We could identify three types of arguments surrounding the high performance of LaVi models:\n(i) Triviality of the LaVi tasks: Recent work has shown that the LaVi models are mainly based on language priors (Ren et al., 2015; Agarwal et al., 2016; Kafle and Kanan, 2016) and even simple correlation and memorization (Zhou et al., 2015; Jabri et al., 2016; Hodosh and Hockenmaier, 2016) can result in a good superficial performance, without the underlying models truly understanding the visual content. Zhang et al. (2016) first unveiled that there exists a huge dataset bias in the popular VQA dataset by Antol et al. (2015): they showed that almost half of all the questions in this dataset can be answered correctly by using the question alone and ignoring the image completely. In the same vein, Zhou et al. (2015) proposed a simple baseline for the task of VQA. This baseline simply concatenates the Bag of Words (BoW) features from the question and Convolutional Neural Networks (CNN) features from the image to predict the answer. They showed that such a simple\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nmethod can achieve comparable performance to complex and deep architectures. Jabri et al. (2016) proposed a similar model for the task of multiple choice VQA, and suggested a cross-dataset generalization scheme as an evaluation criterion for VQA systems. We complement this research by introducing three new tasks with different levels of difficulty, on which LaVi models can be evaluated sequentially.\n(ii) Need for diagnostics: To overcome the bias uncovered in previous datasets, several research groups have started proposing tasks which involve distinguishing distractors from a groundtruth caption for an image. Zhang et al. (2016) introduced a binary VQA task along with a dataset composed of sets of similar artificial images, allowing for more precise diagnostics of a system’s errors. Goyal et al. (2016a) balanced the dataset of Antol et al. (2015), collecting a new set of complementary natural images which are similar to existing items in the original dataset, but result in different answers to a common question. Hodosh and Hockenmaier (2016) also proposed to evaluate a number of state-of-the-art LaVi algorithms in the presence of distractors. Their evaluation was however limited to a small dataset (namely, Flickr30K (Young et al., 2014)) and the caption generation was based on a hand-crafted scheme using only inter-dataset distractors.\nMost related to our paper is the work by Ding et al. (2016).1 Like us, they propose to extend the MS-COCO dataset by generating decoys from human-created image captions. They also suggest an evaluation apparently similar to our T1, requiring the LaVi system to detect the true target caption amongst the decoys. Our efforts, however, differ in some substantial ways. First, their technique to create incorrect captions (using BLEU to set an upper similarity threshold) is so that many of those captions will differ from the gold description in more than one respect. For instance, the caption two elephants standing next to each other in a grass field is associated with the decoy a herd of giraffes standing next to each other in a dirt field (errors: herd, giraffe, dirt) or with animals are gathering next to each other in a dirt field (error: dirt; infelicities: animals and gathering, which are both pragmatically odd). Clearly, the more the caption changes in the decoy, the eas-\n1Please note that this paper was only published on arxiv a few weeks ago, at the end of December 2016. We have not yet evaluated their system against our data.\nier the task becomes. In contrast, the foil captions we propose only differ from the gold description by one word and are thus more challenging. Secondly, their automatic caption generation means that ‘correct’ descriptions can be produced, resulting in some confusion in human responses to the task. We made sure to prevent such cases, and human performance on our dataset is thus close to 100%. We note as well that our task does not require any complex instructions for the annotation, indicating that it is intuitive to human beings (see §4). Thirdly, their evaluation is a multiple-choice task, where the system has to compare all captions to understand which one is closest to the image. This is arguably a simpler task than the one we propose, where a caption is given and the system is asked to classify it as correct or foil: as we show in §4, detecting a correct caption is much easier than detecting foils. So evaluating precision on both gold and foil items is crucial.\nFinally, (Johnson et al., 2016) proposed CLEVR, a dataset to aid in diagnostic evaluation of VQA systems. This dataset has been designed with the explicit goal of enabling detailed analysis of different aspects of visual reasoning, by minimizing dataset biases and providing rich groundtruth representations for both images and questions.\n(iii) Lack of objective evaluation metrics: The evaluation of Natural Language Generation (NLG) systems is known to be a hard problem. It is further unclear whether the quality of LaVi models should be measured using the same metrics designed for language-only tasks. Elliott and Keller (2014) performed a sentence-level correlation analysis of NLG evaluation measures against expert human judgements in the context of IC. Their study revealed that most of those metrics were only weakly correlated with human judgements. In the same line of research, Anderson et al. (2016) showed that the most widely-used metrics for IC fail to capture semantic propositional content, which is an essential component of human caption evaluation. They proposed a semantic evaluation metric called SPICE, that measures how effectively image captions recover objects, attributes and the relations between them. In this paper, we tackle this problem by proposing tasks which can be evaluated based on objective metrics for classification/detection error.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399"
    }, {
      "heading" : "3 Dataset",
      "text" : "In this section, we describe how we automatically generate FOIL-COCO datapoints, viz., image, original and foil caption triples. We used the training and validation Microsoft’s Common Objects in Context (MS-COCO) dataset (Lin et al., 2014) (2014 version) as our starting point. In MS-COCO, each image is described by at least five descriptions written by humans via Amazon Mechanical Turk (AMT). The images contains 91 common object categories (e.g. dog, elephant, bird, . . . and car, bicycle, ariplane, . . . ), from 11 supercategories (Animal, Vehicle, resp.), with 82 of them having more than 5K labeled instances. In total there are 123,287 images for captions (82,783 training and 40,504 validation sets).2\nOur data generation process consists of four main steps: 1. Generation of candidate word-pairs (target::foil) replacement lists; 2. Splitting of replacement word pairs into training and testing pairs; 3. Generation of foil captions; 4. Mining of the hardest foil caption for each image-original caption pair. The last two steps are illustrated in\nFigure 2.3\nGeneration of replacement word pairs (step 1) We want to replace one word in the original caption with a wrong word, we refer to them as target and foil word, respectively. We take target and foil words to be nouns, specifically the labels of MSCOCO categories, and we couple together words belonging to the same supercategory (e.g., bicycle::motorcycle, bicycle::car, bird::dog). In this step, we have used as our vocabulary 73 out of the 91 MS-COCO categories by leaving out those categories that were names with multi-word expressions (e.g. traffic light). We have obtained 472 target::foil word pairs.\nSplitting of replacement word pairs into training and testing (step 2) To avoid the models learning trivial correlations due to replacement frequency, we randomly split, within each supercategory, the candidate target::foil pairs which are used to generate the captions of the training vs. test sets. We have obtained 256 pairs, built out of 72 target and 70 foil words, to generate the captions for the training set and 216 pairs, containing 73 target and 71 foil words, to generate the captions\n2MS-COCO test set is not available for downloading. 3We will make our dataset available with also a validation\nset.\nfor the test set.\nGeneration of foil captions (step 3) We would like to generate foil captions by replacing only target words which refer to visually salient objects. To this end, given an image, we replaced only those target words that occur in more than one MS-COCO caption associated with that image. Moreover, we want to use foils which are visually not present, viz. that refer to visual content not present in the image. Hence, given an image, an original caption and a target word, we replace the latter only with foils that are not among the labels (objects) annotated in MS-COCO for that image. We have used the images from MS-COCO training and validation sets to generate our training and test sets, respectively. We obtained 2,229,899 and 1,097,012 captions for our training and test sets, respectively.\nMining of the hardest foil caption for each image-original caption pair (step 4) To eliminate possible visual-language dataset bias, for each image and original caption, out of all foil captions generated in step 3, we selected only the hardest one. For this purpose, we need to model the visual-language bias of the dataset. To this end, we use Neuraltalk4 proposed in (Fei-Fei, 2015), one of the state-of-the-art image captioning systems, pre-trained on MS-COCO. Neuraltalk is based on an LSTM which takes as input an image and generates a sentence describing its content. We obtain a neural network N that implicitly represents the visual-language bias in its weights. We use N to approximate the conditional probability of a caption C given a dataset T and and an image I (P (C|I, T )). This is obtained simply using the loss l(C,N (I)) i.e., the error obtained by comparing the pseudo-ground truth C with the sentence predicted by N : P (C|I, T ) = 1 − l(C,N (I)). We refer to (Fei-Fei, 2015) for more details on how l() is computed. Finally, P (C|I, T ) is used to select, for an image and an original caption, the hardest foil among all the possible foil captions, viz., the one with the highest probability according to the dataset bias learned by N . Through this process, we obtained 197,788 and 99,480 original::foil caption pairs for the training and test sets, respectively. None of the target::foil word pairs has been filtered out by this mining process.\n4https://github.com/karpathy/ neuraltalk\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nnr. of datapoints nr. unique images nr. of tot. captions nr. target::foil pairs Train 197,788 65,697 395,576 256 Test 99,480 32,150 198,960 216\nTable 1: FOIL-COCO: a dataset consisting for each image of both a correct (original MS-COCO) caption and a wrong caption. The latter is created by replacing one of the nouns in the original caption (target word) with a foil noun.\nFigure 2: The main aspects of the foil caption generation process. Left column: some of the original COCO captions associated with an image. In bold we highlight one of the target words (bicycle), chosen because it is mentioned by more than one annotator. Middle column: For each original caption and each chosen target word, different foil captions are generated by replacing the target word with all possible candidate foil replacements. Right column: A single caption is selected amongst all foil candidates. We select the ”hardest” caption, according to Neuraltalk model, trained using only the original captions.\nThe final FOIL-COCO dataset consists of 297,268 datapoints (197,788 in training and 99,480 in test set). All the 11 MS-COCO supercategories are represented in our dataset and contain 73 categories from the 91 MS-COCO ones (4.8 categories per supercategory on average.) Further details are reported in Table 1."
    }, {
      "heading" : "4 Experiments and Results",
      "text" : "We have conducted three tasks aiming at understanding whether LaVi models can spot if there are mismatches between their coarse representations of language vs. visual inputs (T1); if their language representation is fine-grained enough to detect where the mismatch is (T2); if their visual representation is fine-grained enough so to be able to extract from it the information necessary to correct the foil (T3).\nTask 1 (T1): Correct vs. foil classification Given an image and a caption, the model is asked to mark whether the caption is correct or wrong.\nTask 2 (T2): Foil word detection Given an image and a foil caption, the model has to detect the foil word. In order to systematically check the system’s performance with different prior information, for this task, we have tested two different settings (a) the foil has to be selected among only the nouns in the caption, or (b) among all the content words in it.\nTask 3 (T3): Foil word correction Given an image, a foil caption and the foil word, the model has to detect the foil one and provide its correction. For computational reasons, we have instantiated this task by asking models to correct the foil word by selecting the correct word from the target words, instead of from the whole dataset vocabulary (viz. more that 10K words)."
    }, {
      "heading" : "4.1 Models",
      "text" : "There are two possible approaches to solve the proposed tasks: by using IC models and classify the top ranked captions in the generation process;\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nor by using VQA models. State-of-the-art IC models are trained to generate captions using Recursive Neural Network (RNN), this make them not suitable for our tasks since our foil captions contain only one wrong word. These models are instead useful for selecting over n-captions the best one (Hodosh and Hockenmaier, 2016; Ding et al., 2016). Our classification task could be seen as a multiple choice question, where there are only two choices “correct” and “foil”. Therefore we have evaluated VQA models. In particular, we have used two of the three models evaluated in (Goyal et al., 2016) against a balanced VQA dataset.5\nLSTM + norm I: We use the VQA best performing model in (Antol et al., 2015), namely deeper LSTM + norm I (Lu et al., 2015). This model uses a two stack Long-Short Term Memory (LSTM) to encode the questions and the last fully connected layer of VGGNet to encode the images. Both image embedding and caption embedding are projected into a 1024-dimensional feature space. Following (Goyal et al., 2016), we have normalized the image feature before projecting it into a new feature space. The combination of these two projected embeddings is performed by a point-wise multiplication. The multi-model representation so obtained is used for the classification, which is performed by multi-layer perceptron (MLP) classifier. The original VQA model is trained on 1000 most frequent answers, while in our case it is trained on only 2 possible answers i.e. “correct” or “foil” captions.\nHieCoAtt We use the Hierarchical Co-Attention model proposed by (Lu et al., 2016) that coattends to both the image and the question to solve the task. In particular, we evaluated the alternate version, viz. the model that sequentially alternates between generating image and question attention, and does so in a hierarchical way by starting from the word-level, then going to the phrase and then to the entire question-level. These levels are combined recursively to produce the distribution over the foil vs. correct captions.\nFor the foil word detection task, we have applied the occlusion method to the models above. Following (Goyal et al., 2016b), we systematically\n5We have not evaluated the Multimodal Compact Bilinear Pooling (Fukui et al., 2016) against our tasks, however in (Goyal et al., 2016) this model accuracy is higher than the HieCoAtt of 4.3% – which in the context of our results is a rather little difference.\nocclude subsets of the language input, forward propagate the masked input through the model, and compute the change in the probability of the answer predicted with the unmasked original input. For the error correction task, we applied the linear regression method over all the target words and selected the target word which has the highest probability of making that wrong caption correct with respect to the given image.\nBaselines We compare the SoA models above against the following baselines. For the classification task, we used a Blind model, viz. MLP+BoW (Jabri et al., 2016). This model only accepts captions as input to predict the answer. Differently from (Jabri et al., 2016) instead of concatenating BoW representations of caption and corresponding answer as input to MLP, we only input BoW representation of caption to the MLP network and use it as classifier. Apart from that, we have used the same number of parameters, viz. MLP has 2 hidden layers having 8,192 hidden units and dropout is used after the first layer. For the error detection and error classification tasks, we compare the models’ results against chance.6\nUpper-bound Using Crowdflower, we collected human answers from 256 subjects for 352 image and caption tuples randomly selected from the test set. We collected 1056 judgements (av. 3.9 per raters) and for each tuple, we took the answer provided by at least 2/3 annotators. Subjects were given an image and a caption and had to decide whether it was correct or wrong (T1) and if they thought it was wrong, they had to write which was the foil word (T2). Annotators are nearly perfect in classifying captions and detecting foil words. Hence, though, we have collected human answers only on a rather small subset of the test set, we believe their results are representative of how easy are the tasks for humans."
    }, {
      "heading" : "4.2 Results",
      "text" : "As shown in Table 2, FOIL-COCO dataset is challenging. The ‘blind’, language-only model does badly on T1 with an accuracy of 43.95% (19.53% on foil captions), demonstrating that language bias is minimal. The two state-of-the-art systems do significantly worse than humans on both T1 and\n6The average number of nouns per caption is 4.3 and average number of content words (viz., after removing the stop words) is 6.3. Similary for T3, there are 72 possible target words for a given foil word.\n7 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649\n658\n659\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE.\nT2. Systems show a strong bias towards correct captions and poor overall performance. They respectively only identify 34.51% (LSTM +norm I) and 36.38% (HieCoAtt) of the incorrect captions (T1). On the foil word detection task, they only reach 24.25% and 33.69% accuracy (T2). Their accuracy on foil word correction (T3) is extremely low, at 4.7% and 4.21% respectively. The result on T3 makes it clear that systems are unable to extract from the image representation the information needed to correct the foil: despite being told which element in the caption is wrong, they are not able to zoom into the correct part of the image to correct the foil word, or if they are, cannot name the object in that region."
    }, {
      "heading" : "5 Analysis",
      "text" : "We performed a mixed-effect logistic regression analysis in order to check whether the performance of the models in T1 can be predicted by various linguistic variables (see results in Figure 3). We included: 1) semantic similarity between the original word and the foil (computed as the cosine between the two corresponding Word2Vec embeddings); 2) frequency of original word in FOILCOCO captions; 3) frequency of the foil word in FOIL-COCO captions; 4) length of the caption (number of words). The mixed-effect model was performed to get rid of possible effects due to either object supercategory (indoor, food, vehicle, etc.) or target::foil pair (e.g., zebra::giraffe, boat::airplane, etc.). For both LSTM + norm I and HieCoAtt, Word2Vec similarity, frequency of the original word, and frequency of the foil word turned out to be highly reliable predictors of the model’s response. The higher the values of these variables, the more the models tend to provide the wrong output. That is, when the foil word (e.g. cat) is semantically very similar to the original one (e.g. dog), the models tend to wrongly classify the caption as ‘correct’. The same holds for frequency values. In particular, the higher the frequency of both the original word and the foil one, the more the models fail. This indicates that systems find it difficult to distinguish related concepts at the textvision interface, and also that they may tend to be biased towards frequently occurring concepts, ‘seeing them everywhere’ even when they are not present in the image. Caption length turned out to be only a partially reliable predictor in the LSTM + norm I model, whereas it is a reliable predictor\n600 601 602 603 604 605 606 607 608 609 610 611 612 613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650 651 652 653 654 655 656 657 658 659 660 661 662 663\n664\n665\n672\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n95\n696\n697\n698\n699\nACL 2017 Submission ***. Confidential Review Copy. DO NOT DISTRIBUTE. T1: Classification task Overall Correct Foil Blind 43.95 68.36 19.53 LSTM + norm I 63.26 92.02 34.51 HieCoAtt 64.14 91.89 36.38 Human 91.48 91.67 91.28 T2: Error detection task only nouns all content words Chance 23.25 15.87 LSTM + norm I 26.32 24.25 HieCoAtt 38.79 33.69 Human 100 T3: Error correction task all target words Chance 1.38 LSTM + norm I 4.7 HieCoAtt 4.21\nTable 2: T1: Accuracy results on the classification task, relativelly to all the datapoints (overall) and by type of caption (correct vs. foil); T2: Accuracy results on the error detection task, when the foil is known to be among the nouns only or when it is know to be among all the content words;7 T3: Accuracy results\non the error correction task when the correct word has to be choosen among any of the target words\nobject super-categ ry (indoor, food, vehicle, etc.) or original word/foil word. For both LSTM Q + norm I and HieCoAtt, Word2Vec similarity, frequency of the original word, a d frequ ncy of the foil word turned out to be highly reliable predictors of the model’s res onse. The higher the values of these variables, the more the models tend to provide the wrong output. That is, when the foil word (e.g. cat) is semantically very similar to the original one (e.g. dog), the models tend to wrongly classify the caption as ‘correct’. The same holds for frequency values. In particular, the higher the frequency of both the original word and the foil one, the more the odels fail. This indicates that systems find it difficult to distinguish related concepts at the text-vision interface, and also that they may tend to be biased towards frequently occurring concepts, ‘seeing them everywhere’ even when they are not present in th image.\nCaption length turned out to be only a partially reliable predictor in the LSTM + norm I mod l, whereas it is a reliable predictor in HieCoAtt. In particular, the longer the foil caption, the higher the pr bability that the model will wrongly label the caption as correct. Intuitively, the longer the caption, the harder for the model to spot that there is a foil word that makes the caption wrong.\nWe also checked the average precision of several object d t ction models on the categories in our dataset. For each category (e.g. ‘dog’), we calculated how likely the model was to localize that object in the images that contained it. We obtained between 24% and 36% average precision\nAH: Correct numbers, showing that detection on our dataset is not an easy task. However, we cannot conclude that the systems performed badly for that reason only. Indeed, there is low correlation between object detection precision and model accuracy (Pearson 0.0363 for the LSTM + norm I, 0.179 for HieCoAtt on Task 1). While this is only an indicative result (because the models used for object detection are different from the endto-end VQA models tested here), we can assume that other challenges lower the performance of the tested systems.\nFinally, we checked whether there was any correlation between results and the position of the foil in the sentence, to ensure the models did not profit from any artefact of the data. We did not find any such correlation.\nRB: add comments on the example with accuracy per pairs.\n6 Conclusions\nTo trust students truely understand a task, teachers check whether they can correct mistakes. This is what we have done with the FOIL-COCO dataset. The mistakes have been carefully thought, but au-\nin HieCoAtt. In particular, the longer the foil caption, the higher the probability that the model will wrongly label the caption as correct. Intuitively, the longer the caption, the harder for the model to spot th th re is a foil word hat makes th caption wrong. As revealed by the fairly high variance explained by the random effect related to target::foil pairs in the regression analysis, both models perform very well on some target::foil pairs, but fail on some others. (See Table 3 for same examples of easy/hard target::foil pairs.)\nWe also checked the average precision of several object detection models on the categories in our dataset. For each category (e.g. ‘dog’), we calculated how likely the model was to localize that object in th images that contained it. W obtained between 25.12% (Accessory) and 57.58% (Animal) average precision, showing that detection on our dataset is not an easy task. However, we cannot conclude that the systems performed badly for that reason only. Indeed, there is low correlation between object detection precision and model accuracy (Pearson −0.0363 for the LSTM + norm I, 0.179 for HieCoAtt on T1). While this is only an indicative result (because the models used for object detection are different from the endto-end VQA models tested here), we can assume that other challenges lower the performance of the tested syst ms.\nFinally, we checked whether there was any correlation between results and the position of the foil in the sentence, to ensure the models did not profit from any artefact of the data. We did not find any such correlation."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have introduced FOIL-COCO, a large dataset of images associated with both correct and foil captions. The error production is automatically\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nT1: Classification task Overall Correct Foil\nBlind 43.95 68.36 19.53 LSTM + norm I 63.26 92.02 34.51 HieCoAtt 64.14 91.89 36.38 Human 91.48 91.67 91.28\nT2: Foil word detection task only nouns all content words\nChance 23.25 15.87 LSTM + norm I 26.32 24.25 HieCoAtt 38.79 33.69 Human 100\nT3: Foil word correction task all target words\nChance 1.38 LSTM + norm I 4.7 HieCoAtt 4.21\nTop-5 Bottom-5 T1: LSTM + norm I\nracket::glove 100 motorcycle::ariplane 0 racket::kite 97.29 bicycle::ariplane 0 couch::toilet 97.11 drier::scissors 0 racket::skis 95.23 bus::ariplane 0.35 giraffe::sheep 95.09 zebra::giraffe 0.43 T1: HieCoAtt tie::handbag 100 drier::scissor 0 snowboard::glove 100 fork::glass 0 racket::skis 100 handbag::tie 0 racket::glove 100 motorcycle::airplane 0 backpack::handbag 100 train::airplane 0\nTop-5 Bottom-5 T2: LSTM + norm I\ndrier::scissor 100 glove::skis 0 zebra::giraffe 88.98 snowboard::racket 0 boat::airplane 87.87 donut::apple 0 truck::airplane 85.71 glove::surfboard 0 train::airplane 81.93 spoon::bottle 0 T2: HieCoAtt zebra::elephant 94.92 direr::scissors 0 backpack::handbag 94.44 handbag::tie 0 cow::zebra 93.33 broccolli:orange 1.47 bird::sheep 93.11 zebra::giraffe 1.96 orange::carrot 92.37 boat::airplane 2.09\nTable 3: Easiest and hardest target::foil pairs: T1 (caption classification) and T2 (foil word detection).\ngenerated, but carefully thought out, making the task of spotting foils particularly challenging. By associating the dataset with a series of tasks, we allow for diagnosing various failures of current LaVi systems, from their coarse understanding of the correspondences between text and vision to their grasp of language and image structure.\nOur hypothesis is that systems which, like humans, deeply integrate the language and vision modalities, should spot foil captions quite easily. The state-of-the art LaVi models we have tested fall through that test, implying that they fail to integrate the two modalities. To complete the analysis of these results, we plan to carry out a further task, namely ask the system to detect in the image the area that produces the mismatch with the foil word (the red box around the bird in Figure 1.) This extra step would allow us to fully diagnose the failure of the tested systems and confirm what is implicit in our results from task 3:\nthat the algorithms are unable to map particular elements of the text to their visual counterparts. We note that the addition of this extra step will move this work closer to the textual/visual explanation research (e.g., (Park et al., 2016; Selvaraju et al., 2016)). We will then have a pipeline able to not only test whether a mistake can be detected, but also whether the system can explain its decision: ‘the wrong word is dog because the cyclists are in fact approaching a bird, there, in the image’.\nLaVi models are a great success of recent research, and we are impressed by the amount of ideas, data and models produced in this stimulating and attractive area. With our work, we would like to push the community to think of ways the models can better merge language and vision rather than merely use one as a supplement to the other.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Analyzing the behavior of visual question answering models",
      "author" : [ "A. Agarwal", "D. Batra", "D. Parikh." ],
      "venue" : "Proceedings of Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Agarwal et al\\.,? 2016",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2016
    }, {
      "title" : "Spice: Semantic propositional image caption evaluation",
      "author" : [ "Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould." ],
      "venue" : "In Proceedings of the European Conference on Computer Vision (ECCV).",
      "citeRegEx" : "Anderson et al\\.,? 2016",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2016
    }, {
      "title" : "VQA: Visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "International Conference on Computer Vision (ICCV).",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic description generation from images: A survey of models, datasets, and evaluation",
      "author" : [ "Raffaella Bernardi", "Ruket Cakici", "Desmond Elliott", "Aykut Erdem", "Erkut Erdem", "Nazli Ikizler-Cinbis", "Frank Keller", "Adrian Muscat", "Barbara Plank" ],
      "venue" : null,
      "citeRegEx" : "Bernardi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bernardi et al\\.",
      "year" : 2016
    }, {
      "title" : "Mind’s eye: A recurrent visual representation for image caption generation",
      "author" : [ "Xinlei Chen", "C Lawrence Zitnick." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 2422–2431.",
      "citeRegEx" : "Chen and Zitnick.,? 2015",
      "shortCiteRegEx" : "Chen and Zitnick.",
      "year" : 2015
    }, {
      "title" : "Understanding image and text simultaneously: a dual vision-language machine comprehension task",
      "author" : [ "Nan Ding", "Sebastian Goodman", "Fei Sha", "Radu Soricut." ],
      "venue" : "arXiv preprint arXiv:1612.07833 .",
      "citeRegEx" : "Ding et al\\.,? 2016",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2016
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "Jeffrey Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell." ],
      "venue" : "Proceedings of the IEEE",
      "citeRegEx" : "Donahue et al\\.,? 2015",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2015
    }, {
      "title" : "Comparing automatic evaluation measures for image description",
      "author" : [ "Desmond Elliott", "Frank Keller." ],
      "venue" : "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Short Papers. pages 452–457.",
      "citeRegEx" : "Elliott and Keller.,? 2014",
      "shortCiteRegEx" : "Elliott and Keller.",
      "year" : 2014
    }, {
      "title" : "From captions to visual concepts and back",
      "author" : [ "Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh K Srivastava", "Li Deng", "Piotr Dollár", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C Platt" ],
      "venue" : "In Proceedings of the IEEE Conference on Computer",
      "citeRegEx" : "Fang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep visualsemantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy Li Fei-Fei." ],
      "venue" : "In Proceedings of CVPR.",
      "citeRegEx" : "Fei.Fei.,? 2015",
      "shortCiteRegEx" : "Fei.Fei.",
      "year" : 2015
    }, {
      "title" : "Multimodal compact bilinear pooling for visual question answering and visual grounding",
      "author" : [ "A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Fukui et al\\.,? 2016",
      "shortCiteRegEx" : "Fukui et al\\.",
      "year" : 2016
    }, {
      "title" : "Are you talking to a machine? dataset and methods for multilingual image question",
      "author" : [ "Haoyuan Gao", "Junhua Mao", "Jie Zhou", "Zhiheng Huang", "Lei Wang", "Wei Xu." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 2296–2304.",
      "citeRegEx" : "Gao et al\\.,? 2015",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2015
    }, {
      "title" : "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
      "author" : [ "Y. Goyal", "T. Khot", "D. Summers-Stay", "D. Batra", "D. Parikh." ],
      "venue" : "ArXiv e-prints .",
      "citeRegEx" : "Goyal et al\\.,? 2016",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2016
    }, {
      "title" : "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
      "author" : [ "Yash Goyal", "Tejas Khot", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "arXiv preprint arXiv:1612.00837 .",
      "citeRegEx" : "Goyal et al\\.,? 2016a",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards transparent ai systems: Interpreting visual question answering models",
      "author" : [ "Yash Goyal", "Akrit Mohapatra", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "In Proceedings of ICML Visualization Workshop.",
      "citeRegEx" : "Goyal et al\\.,? 2016b",
      "shortCiteRegEx" : "Goyal et al\\.",
      "year" : 2016
    }, {
      "title" : "Focused evaluation for image description with binary forcedchoice tasks",
      "author" : [ "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Proceedings of the 5th Workshop on Vision and Language (VL’16).",
      "citeRegEx" : "Hodosh and Hockenmaier.,? 2016",
      "shortCiteRegEx" : "Hodosh and Hockenmaier.",
      "year" : 2016
    }, {
      "title" : "Framing image description as a ranking task: Data, models and evaluation metrics",
      "author" : [ "Micah Hodosh", "Peter Young", "Julia Hockenmaier." ],
      "venue" : "Journal of Artificial Intelligence Research 47:853–899.",
      "citeRegEx" : "Hodosh et al\\.,? 2013",
      "shortCiteRegEx" : "Hodosh et al\\.",
      "year" : 2013
    }, {
      "title" : "Revisiting visual question answering baselines",
      "author" : [ "A. Jabri", "A. Joulin", "L.J.P. van der Maaten." ],
      "venue" : "Proceedings of the European Conference on Computer Vision (ECCV). pages 727–739.",
      "citeRegEx" : "Jabri et al\\.,? 2016",
      "shortCiteRegEx" : "Jabri et al\\.",
      "year" : 2016
    }, {
      "title" : "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning",
      "author" : [ "Justin Johnson", "Bharath Hariharan", "Laurens van der Maaten", "Li Fei-Fei", "C. Lawrence Zitnick", "Ross Girshick." ],
      "venue" : "ArXiv:1612.06890.",
      "citeRegEx" : "Johnson et al\\.,? 2016",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "Visual question answering: Datasets, algorithms, and future challenges",
      "author" : [ "Kushal Kafle", "Christopher Kanan." ],
      "venue" : "arXiv preprint arXiv:1610.01465 .",
      "citeRegEx" : "Kafle and Kanan.,? 2016",
      "shortCiteRegEx" : "Kafle and Kanan.",
      "year" : 2016
    }, {
      "title" : "Deep visualsemantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 3128–3137.",
      "citeRegEx" : "Karpathy and Fei.Fei.,? 2015",
      "shortCiteRegEx" : "Karpathy and Fei.Fei.",
      "year" : 2015
    }, {
      "title" : "Microsoft coco: Common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C Lawrence Zitnick." ],
      "venue" : "European Conference on Computer Vision. Springer, pages 740–755.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Deeper lstm and normalized cnn visual question answering model",
      "author" : [ "Jiasen Lu", "Xiao Lin", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "https://github.com/ VT-vision-lab/VQA_LSTM_CNN.",
      "citeRegEx" : "Lu et al\\.,? 2015",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical question-image coattention for visual question answering",
      "author" : [ "Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "In Proceedings of NIPS 2016. https://github. com/jiasenlu/HieCoAttenVQA.",
      "citeRegEx" : "Lu et al\\.,? 2016",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "A multiworld approach to question answering about realworld scenes based on uncertain input",
      "author" : [ "Mateusz Malinowski", "Mario Fritz." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 1682–1690.",
      "citeRegEx" : "Malinowski and Fritz.,? 2014",
      "shortCiteRegEx" : "Malinowski and Fritz.",
      "year" : 2014
    }, {
      "title" : "Ask your neurons: A neural-based approach to answering questions about images",
      "author" : [ "Mateusz Malinowski", "Marcus Rohrbach", "Mario Fritz." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision. pages 1–9.",
      "citeRegEx" : "Malinowski et al\\.,? 2015",
      "shortCiteRegEx" : "Malinowski et al\\.",
      "year" : 2015
    }, {
      "title" : "Attentive explanations: Justifying decisions and pointing to the evidence",
      "author" : [ "Dong Huk Park", "Lisa Anne Hendricks", "Zeynep Akata", "Trevor Darrell Bernt Schiele", "Marcus Rohrbach." ],
      "venue" : "ArXiv:1612.04757.",
      "citeRegEx" : "Park et al\\.,? 2016",
      "shortCiteRegEx" : "Park et al\\.",
      "year" : 2016
    }, {
      "title" : "Exploring models and data for image question answering",
      "author" : [ "M. Ren", "R. Kiros", "R. Zemel." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS 2015).",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization",
      "author" : [ "Ramprasaath R. Selvaraju", "Abhishek Das", "Ramakrishna Vedantam", "Michael Cogswell", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "ArXiv:1610.02391v2.",
      "citeRegEx" : "Selvaraju et al\\.,? 2016",
      "shortCiteRegEx" : "Selvaraju et al\\.",
      "year" : 2016
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 3156–3164.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Visual question answering: A survey of methods and datasets",
      "author" : [ "Qi Wu", "Damien Teney", "Peng Wang", "Chunhua Shen", "Anthony Dick", "Anton van den Hengel." ],
      "venue" : "arXiv preprint arXiv:1607.05910 .",
      "citeRegEx" : "Wu et al\\.,? 2016",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
      "author" : [ "Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Transactions of the Association for Computational Linguis-",
      "citeRegEx" : "Young et al\\.,? 2014",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2014
    }, {
      "title" : "Yin and yang: Balancing and answering binary visual questions",
      "author" : [ "Peng Zhang", "Yash Goyal", "Douglas Summers-Stay", "Dhruv Batra", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 5014–5022.",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Simple baseline for visual question answering",
      "author" : [ "Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus." ],
      "venue" : "arXiv preprint arXiv:1512.02167 .",
      "citeRegEx" : "Zhou et al\\.,? 2015",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Antol et al. (2015); Malinowski and Fritz (2014); Malinowski et al.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "Antol et al. (2015); Malinowski and Fritz (2014); Malinowski et al.",
      "startOffset" : 0,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "Antol et al. (2015); Malinowski and Fritz (2014); Malinowski et al. (2015); Gao et al.",
      "startOffset" : 0,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "Antol et al. (2015); Malinowski and Fritz (2014); Malinowski et al. (2015); Gao et al. (2015); Ren et al.",
      "startOffset" : 0,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "Antol et al. (2015); Malinowski and Fritz (2014); Malinowski et al. (2015); Gao et al. (2015); Ren et al. (2015)) and ‘Image Captioning’ (IC) (e.",
      "startOffset" : 0,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : "Antol et al. (2015); Malinowski and Fritz (2014); Malinowski et al. (2015); Gao et al. (2015); Ren et al. (2015)) and ‘Image Captioning’ (IC) (e.g. Hodosh et al. (2013); Fang et al.",
      "startOffset" : 0,
      "endOffset" : 169
    }, {
      "referenceID" : 2,
      "context" : "Antol et al. (2015); Malinowski and Fritz (2014); Malinowski et al. (2015); Gao et al. (2015); Ren et al. (2015)) and ‘Image Captioning’ (IC) (e.g. Hodosh et al. (2013); Fang et al. (2015); Chen and Lawrence Zitnick (2015); Donahue et al.",
      "startOffset" : 0,
      "endOffset" : 189
    }, {
      "referenceID" : 2,
      "context" : "Antol et al. (2015); Malinowski and Fritz (2014); Malinowski et al. (2015); Gao et al. (2015); Ren et al. (2015)) and ‘Image Captioning’ (IC) (e.g. Hodosh et al. (2013); Fang et al. (2015); Chen and Lawrence Zitnick (2015); Donahue et al.",
      "startOffset" : 0,
      "endOffset" : 223
    }, {
      "referenceID" : 2,
      "context" : "Antol et al. (2015); Malinowski and Fritz (2014); Malinowski et al. (2015); Gao et al. (2015); Ren et al. (2015)) and ‘Image Captioning’ (IC) (e.g. Hodosh et al. (2013); Fang et al. (2015); Chen and Lawrence Zitnick (2015); Donahue et al. (2015); Karpathy and Fei-Fei (2015); Vinyals et al.",
      "startOffset" : 0,
      "endOffset" : 246
    }, {
      "referenceID" : 2,
      "context" : "Antol et al. (2015); Malinowski and Fritz (2014); Malinowski et al. (2015); Gao et al. (2015); Ren et al. (2015)) and ‘Image Captioning’ (IC) (e.g. Hodosh et al. (2013); Fang et al. (2015); Chen and Lawrence Zitnick (2015); Donahue et al. (2015); Karpathy and Fei-Fei (2015); Vinyals et al.",
      "startOffset" : 0,
      "endOffset" : 275
    }, {
      "referenceID" : 2,
      "context" : "Antol et al. (2015); Malinowski and Fritz (2014); Malinowski et al. (2015); Gao et al. (2015); Ren et al. (2015)) and ‘Image Captioning’ (IC) (e.g. Hodosh et al. (2013); Fang et al. (2015); Chen and Lawrence Zitnick (2015); Donahue et al. (2015); Karpathy and Fei-Fei (2015); Vinyals et al. (2015)).",
      "startOffset" : 0,
      "endOffset" : 298
    }, {
      "referenceID" : 0,
      "context" : "There is an emerging feeling in the LaVi community that the current VQA task should be revised, especially as it has been shown that it can be handled well by ‘blind’ models which use only language input or by simple concatenation of language and vision features (Agarwal et al., 2016; Jabri et al., 2016; Zhang et al., 2016; Goyal et al., 2016).",
      "startOffset" : 263,
      "endOffset" : 345
    }, {
      "referenceID" : 17,
      "context" : "There is an emerging feeling in the LaVi community that the current VQA task should be revised, especially as it has been shown that it can be handled well by ‘blind’ models which use only language input or by simple concatenation of language and vision features (Agarwal et al., 2016; Jabri et al., 2016; Zhang et al., 2016; Goyal et al., 2016).",
      "startOffset" : 263,
      "endOffset" : 345
    }, {
      "referenceID" : 32,
      "context" : "There is an emerging feeling in the LaVi community that the current VQA task should be revised, especially as it has been shown that it can be handled well by ‘blind’ models which use only language input or by simple concatenation of language and vision features (Agarwal et al., 2016; Jabri et al., 2016; Zhang et al., 2016; Goyal et al., 2016).",
      "startOffset" : 263,
      "endOffset" : 345
    }, {
      "referenceID" : 12,
      "context" : "There is an emerging feeling in the LaVi community that the current VQA task should be revised, especially as it has been shown that it can be handled well by ‘blind’ models which use only language input or by simple concatenation of language and vision features (Agarwal et al., 2016; Jabri et al., 2016; Zhang et al., 2016; Goyal et al., 2016).",
      "startOffset" : 263,
      "endOffset" : 345
    }, {
      "referenceID" : 0,
      "context" : "There is an emerging feeling in the LaVi community that the current VQA task should be revised, especially as it has been shown that it can be handled well by ‘blind’ models which use only language input or by simple concatenation of language and vision features (Agarwal et al., 2016; Jabri et al., 2016; Zhang et al., 2016; Goyal et al., 2016). In IC too, Hodosh and Hockenmaier (2016) showed that, contrarily to what prior research had suggested, the task is far from been solved, since IC models are not able to distinguish between a correct and incorrect caption.",
      "startOffset" : 264,
      "endOffset" : 388
    }, {
      "referenceID" : 0,
      "context" : "There is an emerging feeling in the LaVi community that the current VQA task should be revised, especially as it has been shown that it can be handled well by ‘blind’ models which use only language input or by simple concatenation of language and vision features (Agarwal et al., 2016; Jabri et al., 2016; Zhang et al., 2016; Goyal et al., 2016). In IC too, Hodosh and Hockenmaier (2016) showed that, contrarily to what prior research had suggested, the task is far from been solved, since IC models are not able to distinguish between a correct and incorrect caption. Such results indicate that in current datasets, language provides priors that make it possible for a LaVi model to be successful without truly understanding and integrating language and vision. But problems do not stop at biasing. Johnson et al. (2016) highlight a second weakness of such datasets, pointing out that “they conflate multiple sources of error, making it hard to pinpoint model weaknesses”.",
      "startOffset" : 264,
      "endOffset" : 822
    }, {
      "referenceID" : 16,
      "context" : "sitive to n-gram overlap and that there is a need for measures that better substitute human judgments (Hodosh et al., 2013; Elliott and Keller, 2014; Anderson et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 172
    }, {
      "referenceID" : 7,
      "context" : "sitive to n-gram overlap and that there is a need for measures that better substitute human judgments (Hodosh et al., 2013; Elliott and Keller, 2014; Anderson et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 172
    }, {
      "referenceID" : 1,
      "context" : "sitive to n-gram overlap and that there is a need for measures that better substitute human judgments (Hodosh et al., 2013; Elliott and Keller, 2014; Anderson et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 172
    }, {
      "referenceID" : 21,
      "context" : "The dataset presented in this paper (Section 3) is built on top of MS-COCO (Lin et al., 2014), and contains 297,268 datapoints and 97,847 images.",
      "startOffset" : 75,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "We evaluate two state-of-the-art models: the popular VQA system proposed by Antol et al. (2015); Lu et al.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "We evaluate two state-of-the-art models: the popular VQA system proposed by Antol et al. (2015); Lu et al. (2015), and the attention-based model by Lu et al.",
      "startOffset" : 76,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "We evaluate two state-of-the-art models: the popular VQA system proposed by Antol et al. (2015); Lu et al. (2015), and the attention-based model by Lu et al. (2016). We show that those models perform close to chance level, while humans are capable of doing the task accurately (Section 4).",
      "startOffset" : 76,
      "endOffset" : 165
    }, {
      "referenceID" : 8,
      "context" : "In IC (Fang et al., 2015; Chen and Lawrence Zitnick, 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015), the goal is to generate a caption for a given image, such that it is both semantically and syntactically correct, and properly describes the content of that image.",
      "startOffset" : 6,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "In IC (Fang et al., 2015; Chen and Lawrence Zitnick, 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015), the goal is to generate a caption for a given image, such that it is both semantically and syntactically correct, and properly describes the content of that image.",
      "startOffset" : 6,
      "endOffset" : 130
    }, {
      "referenceID" : 20,
      "context" : "In IC (Fang et al., 2015; Chen and Lawrence Zitnick, 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015), the goal is to generate a caption for a given image, such that it is both semantically and syntactically correct, and properly describes the content of that image.",
      "startOffset" : 6,
      "endOffset" : 130
    }, {
      "referenceID" : 29,
      "context" : "In IC (Fang et al., 2015; Chen and Lawrence Zitnick, 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015), the goal is to generate a caption for a given image, such that it is both semantically and syntactically correct, and properly describes the content of that image.",
      "startOffset" : 6,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "In VQA (Antol et al., 2015; Malinowski and Fritz, 2014; Malinowski et al., 2015; Gao et al., 2015; Ren et al., 2015), the system attempts to answer open-ended questions related to the content of a given image.",
      "startOffset" : 7,
      "endOffset" : 116
    }, {
      "referenceID" : 24,
      "context" : "In VQA (Antol et al., 2015; Malinowski and Fritz, 2014; Malinowski et al., 2015; Gao et al., 2015; Ren et al., 2015), the system attempts to answer open-ended questions related to the content of a given image.",
      "startOffset" : 7,
      "endOffset" : 116
    }, {
      "referenceID" : 25,
      "context" : "In VQA (Antol et al., 2015; Malinowski and Fritz, 2014; Malinowski et al., 2015; Gao et al., 2015; Ren et al., 2015), the system attempts to answer open-ended questions related to the content of a given image.",
      "startOffset" : 7,
      "endOffset" : 116
    }, {
      "referenceID" : 11,
      "context" : "In VQA (Antol et al., 2015; Malinowski and Fritz, 2014; Malinowski et al., 2015; Gao et al., 2015; Ren et al., 2015), the system attempts to answer open-ended questions related to the content of a given image.",
      "startOffset" : 7,
      "endOffset" : 116
    }, {
      "referenceID" : 27,
      "context" : "In VQA (Antol et al., 2015; Malinowski and Fritz, 2014; Malinowski et al., 2015; Gao et al., 2015; Ren et al., 2015), the system attempts to answer open-ended questions related to the content of a given image.",
      "startOffset" : 7,
      "endOffset" : 116
    }, {
      "referenceID" : 3,
      "context" : "There is a wealth of literature on both tasks, but we only discuss here the ones most related to our work and refer the reader to the recent surveys by (Bernardi et al., 2016; Wu et al., 2016).",
      "startOffset" : 152,
      "endOffset" : 192
    }, {
      "referenceID" : 30,
      "context" : "There is a wealth of literature on both tasks, but we only discuss here the ones most related to our work and refer the reader to the recent surveys by (Bernardi et al., 2016; Wu et al., 2016).",
      "startOffset" : 152,
      "endOffset" : 192
    }, {
      "referenceID" : 27,
      "context" : "We could identify three types of arguments surrounding the high performance of LaVi models: (i) Triviality of the LaVi tasks: Recent work has shown that the LaVi models are mainly based on language priors (Ren et al., 2015; Agarwal et al., 2016; Kafle and Kanan, 2016) and even simple correlation and memorization (Zhou et al.",
      "startOffset" : 205,
      "endOffset" : 268
    }, {
      "referenceID" : 0,
      "context" : "We could identify three types of arguments surrounding the high performance of LaVi models: (i) Triviality of the LaVi tasks: Recent work has shown that the LaVi models are mainly based on language priors (Ren et al., 2015; Agarwal et al., 2016; Kafle and Kanan, 2016) and even simple correlation and memorization (Zhou et al.",
      "startOffset" : 205,
      "endOffset" : 268
    }, {
      "referenceID" : 19,
      "context" : "We could identify three types of arguments surrounding the high performance of LaVi models: (i) Triviality of the LaVi tasks: Recent work has shown that the LaVi models are mainly based on language priors (Ren et al., 2015; Agarwal et al., 2016; Kafle and Kanan, 2016) and even simple correlation and memorization (Zhou et al.",
      "startOffset" : 205,
      "endOffset" : 268
    }, {
      "referenceID" : 33,
      "context" : ", 2016; Kafle and Kanan, 2016) and even simple correlation and memorization (Zhou et al., 2015; Jabri et al., 2016; Hodosh and Hockenmaier, 2016) can result in a good superficial performance, without the underlying models truly understanding the visual content.",
      "startOffset" : 76,
      "endOffset" : 145
    }, {
      "referenceID" : 17,
      "context" : ", 2016; Kafle and Kanan, 2016) and even simple correlation and memorization (Zhou et al., 2015; Jabri et al., 2016; Hodosh and Hockenmaier, 2016) can result in a good superficial performance, without the underlying models truly understanding the visual content.",
      "startOffset" : 76,
      "endOffset" : 145
    }, {
      "referenceID" : 15,
      "context" : ", 2016; Kafle and Kanan, 2016) and even simple correlation and memorization (Zhou et al., 2015; Jabri et al., 2016; Hodosh and Hockenmaier, 2016) can result in a good superficial performance, without the underlying models truly understanding the visual content.",
      "startOffset" : 76,
      "endOffset" : 145
    }, {
      "referenceID" : 0,
      "context" : ", 2015; Agarwal et al., 2016; Kafle and Kanan, 2016) and even simple correlation and memorization (Zhou et al., 2015; Jabri et al., 2016; Hodosh and Hockenmaier, 2016) can result in a good superficial performance, without the underlying models truly understanding the visual content. Zhang et al. (2016) first unveiled that there exists a huge dataset bias in the popular VQA dataset by Antol et al.",
      "startOffset" : 8,
      "endOffset" : 304
    }, {
      "referenceID" : 0,
      "context" : ", 2015; Agarwal et al., 2016; Kafle and Kanan, 2016) and even simple correlation and memorization (Zhou et al., 2015; Jabri et al., 2016; Hodosh and Hockenmaier, 2016) can result in a good superficial performance, without the underlying models truly understanding the visual content. Zhang et al. (2016) first unveiled that there exists a huge dataset bias in the popular VQA dataset by Antol et al. (2015): they showed that almost half of all the questions in this dataset can be answered correctly by using the question alone and ignoring the image completely.",
      "startOffset" : 8,
      "endOffset" : 407
    }, {
      "referenceID" : 0,
      "context" : ", 2015; Agarwal et al., 2016; Kafle and Kanan, 2016) and even simple correlation and memorization (Zhou et al., 2015; Jabri et al., 2016; Hodosh and Hockenmaier, 2016) can result in a good superficial performance, without the underlying models truly understanding the visual content. Zhang et al. (2016) first unveiled that there exists a huge dataset bias in the popular VQA dataset by Antol et al. (2015): they showed that almost half of all the questions in this dataset can be answered correctly by using the question alone and ignoring the image completely. In the same vein, Zhou et al. (2015) proposed a simple baseline for the task of VQA.",
      "startOffset" : 8,
      "endOffset" : 600
    }, {
      "referenceID" : 31,
      "context" : "Their evaluation was however limited to a small dataset (namely, Flickr30K (Young et al., 2014)) and the caption generation was based on a hand-crafted scheme using only inter-dataset distractors.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 11,
      "context" : "Jabri et al. (2016) proposed a similar model for the task of multiple choice VQA, and suggested a cross-dataset generalization scheme as an evaluation criterion for VQA systems.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "Jabri et al. (2016) proposed a similar model for the task of multiple choice VQA, and suggested a cross-dataset generalization scheme as an evaluation criterion for VQA systems. We complement this research by introducing three new tasks with different levels of difficulty, on which LaVi models can be evaluated sequentially. (ii) Need for diagnostics: To overcome the bias uncovered in previous datasets, several research groups have started proposing tasks which involve distinguishing distractors from a groundtruth caption for an image. Zhang et al. (2016) introduced a binary VQA task along with a dataset composed of sets of similar artificial images, allowing for more precise diagnostics of a system’s errors.",
      "startOffset" : 0,
      "endOffset" : 561
    }, {
      "referenceID" : 10,
      "context" : "Goyal et al. (2016a) balanced the dataset of Antol et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "(2016a) balanced the dataset of Antol et al. (2015), collecting a new set of complementary natural images which are similar to existing items in the original dataset, but result in different answers to a common question.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "(2016a) balanced the dataset of Antol et al. (2015), collecting a new set of complementary natural images which are similar to existing items in the original dataset, but result in different answers to a common question. Hodosh and Hockenmaier (2016) also proposed to evaluate a number of state-of-the-art LaVi algorithms in the presence of distractors.",
      "startOffset" : 32,
      "endOffset" : 251
    }, {
      "referenceID" : 2,
      "context" : "(2016a) balanced the dataset of Antol et al. (2015), collecting a new set of complementary natural images which are similar to existing items in the original dataset, but result in different answers to a common question. Hodosh and Hockenmaier (2016) also proposed to evaluate a number of state-of-the-art LaVi algorithms in the presence of distractors. Their evaluation was however limited to a small dataset (namely, Flickr30K (Young et al., 2014)) and the caption generation was based on a hand-crafted scheme using only inter-dataset distractors. Most related to our paper is the work by Ding et al. (2016).1 Like us, they propose to extend the MS-COCO dataset by generating decoys from human-created image captions.",
      "startOffset" : 32,
      "endOffset" : 611
    }, {
      "referenceID" : 18,
      "context" : "Finally, (Johnson et al., 2016) proposed CLEVR, a dataset to aid in diagnostic evaluation of VQA systems.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 6,
      "context" : "Elliott and Keller (2014) performed a sentence-level correlation analysis of NLG evaluation measures against expert human judgements in the context of IC.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "In the same line of research, Anderson et al. (2016) showed that the most widely-used metrics for IC fail to capture semantic propositional content, which is an essential component of human caption evaluation.",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 21,
      "context" : "We used the training and validation Microsoft’s Common Objects in Context (MS-COCO) dataset (Lin et al., 2014) (2014 version) as our starting point.",
      "startOffset" : 92,
      "endOffset" : 110
    }, {
      "referenceID" : 9,
      "context" : "To this end, we use Neuraltalk4 proposed in (Fei-Fei, 2015), one of the state-of-the-art image captioning systems, pre-trained on MS-COCO.",
      "startOffset" : 44,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : "We refer to (Fei-Fei, 2015) for more details on how l() is computed.",
      "startOffset" : 12,
      "endOffset" : 27
    }, {
      "referenceID" : 15,
      "context" : "These models are instead useful for selecting over n-captions the best one (Hodosh and Hockenmaier, 2016; Ding et al., 2016).",
      "startOffset" : 75,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "These models are instead useful for selecting over n-captions the best one (Hodosh and Hockenmaier, 2016; Ding et al., 2016).",
      "startOffset" : 75,
      "endOffset" : 124
    }, {
      "referenceID" : 12,
      "context" : "In particular, we have used two of the three models evaluated in (Goyal et al., 2016) against a balanced VQA dataset.",
      "startOffset" : 65,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "LSTM + norm I: We use the VQA best performing model in (Antol et al., 2015), namely deeper LSTM + norm I (Lu et al.",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 22,
      "context" : ", 2015), namely deeper LSTM + norm I (Lu et al., 2015).",
      "startOffset" : 37,
      "endOffset" : 54
    }, {
      "referenceID" : 12,
      "context" : "Following (Goyal et al., 2016), we have normalized the image feature before projecting it into a new feature space.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 23,
      "context" : "HieCoAtt We use the Hierarchical Co-Attention model proposed by (Lu et al., 2016) that coattends to both the image and the question to solve the task.",
      "startOffset" : 64,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "Following (Goyal et al., 2016b), we systematically",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "We have not evaluated the Multimodal Compact Bilinear Pooling (Fukui et al., 2016) against our tasks, however in (Goyal et al.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : ", 2016) against our tasks, however in (Goyal et al., 2016) this model accuracy is higher than the HieCoAtt of 4.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : "MLP+BoW (Jabri et al., 2016).",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 17,
      "context" : "Differently from (Jabri et al., 2016) instead of concatenating BoW representations of caption and corresponding answer as input to MLP, we only input BoW representation of caption to the MLP network and use it as classifier.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 26,
      "context" : ", (Park et al., 2016; Selvaraju et al., 2016)).",
      "startOffset" : 2,
      "endOffset" : 45
    }, {
      "referenceID" : 28,
      "context" : ", (Park et al., 2016; Selvaraju et al., 2016)).",
      "startOffset" : 2,
      "endOffset" : 45
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we aim to understand whether current language and vision (LaVi) models truly grasp the interaction between the two modalities. To this end, we propose an extension of the MS-COCO dataset, FOIL-COCO, which associates images with both correct and ‘foil’ captions, that is, descriptions of the image that are highly similar to the original ones, but contain one single mistake (“foil word”). We show that current LaVi models fall into the traps of this data and perform badly on three tasks: a) caption classification (correct vs. foil); b) foil word detection; c) foil word correction. Humans, in contrast, have near-perfect performance on those tasks. We demonstrate that using language cues only is not enough to deal with FOILCOCO and that it challenges the state-ofthe-art by requiring a fine-grained understanding of the relation between text and image.",
    "creator" : "LaTeX with hyperref package"
  }
}