{
  "name" : "178.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Weakly-Supervised Method for Jointly Embedding Concepts, Phrases, and Words",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recent vector space models of semantics are primarily built on words. Word embeddings are either used directly as features in downstream modeling tasks (Chiu et al., 2016b), or are composed to produce representations of more complex linguistic units, such as phrases, sentences (Socher et al., 2011; Le and Mikolov, 2014), or even documents (Yang et al., 2016). However, there are many cases in which words are not atomic semantic units, and multi-word expressions cannot be expressed as a composition of their member words: for example, “the Big Apple” is not a fruit, and “Lou Gehrig’s disease” has little to do with baseball. Furthermore, such concepts often multiple textual forms: “the Big Apple” and “New York City” both refer to the same location, and “Lou Gehrig’s disease” and “amyotrophic lateral scle-\nrosis” are the same medical condition. Despite the lack of lexical overlap between these phrases, we would like a semantic model that can represent the underlying concept, regardless of the specific textual form used.\nOur novel approach to this task combines structured knowledge with proven techniques for learning word embeddings to train context-based representations for contexts, phrases, and words. The model requires no human annotations: we use known phrases as distant supervision in distributional similarity training over an unannotated corpus. As in the formulation of Mintz et al. (2009) in extracting textual relationships, we assume that any occurrence of a known phrase signifies an occurrence of each concept1 it can refer to, and train our concept embeddings using the contexts of their phrase forms. We experimentally validate our approach by learning embeddings for biomedical concepts and real-world entities. An evaluation on concept similarity and relatedness tasks shows that our embeddings are competitive with prior models that required human annotations for concepts. We also present a novel dataset of similarity and relatedness of real-world entities, identified both by Wikipedia page and text phrase.\nFurthermore, analysis suggests the source ontology structure is reflected in the organization of our embeddings: concepts and their representative phrases are embedded close to one another, and embedded concepts of the same semantic type cluster together with some regularity."
    }, {
      "heading" : "2 Related Work",
      "text" : "Single word embeddings have been directly used as entity/concept models in prior work, includ-\n1The term “concept” has been used in prior literature to mean both a specific concept in an ontology and a specific sense of a word. In this study, “concept” refers to a canonical abstract concept or real-world entity.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ning in analogy completion tasks (Mikolov et al., 2013b; Linzen, 2016; Gladkova et al., 2016) and in similarity/relatedness tasks (Muneeb et al., 2015; Chiu et al., 2016a). Multi-word concepts have also been explored by approaches such as lexicalization and word averaging (Mikolov et al., 2013a; Socher et al., 2013). Fan et al. (2015), Yu et al. (2016), and Hill and Korhonen (2014) modify embedding training with additional information, such as named entity mentions and associated perceptions.\nAdditionally, a number of models have been proposed for the related task of separately embedding different word senses. Camacho-Collados et al. (2015) use a semantic network to train word sense representations using relevant subsets of Wikipedia. Nieta Pina and Johansson (2016) use WordNet graph structure as context for embedding word senses; a similar approach was taken by Hu et al. (2015) for embedding entities, using the hierarchy of the containing ontology augmented with textual information.\nIn all of these cases, however, words are used as atomic units, and can be composed in order to model concepts. However, there is also a rich literature on learning embeddings of knowledge base entities and ontology concepts directly from the structure, relationships, and attributes encoded in the knowledge base (Bordes et al., 2011, 2013; Wang et al., 2014b; Lin et al., 2015). Toutanova et al. (2015) decomposed textual relations into single words to improve generalization, but still relied on knowledge graph structure for embedding entities; Socher et al. (2013) and Yang et al. (2015), however, incorporated pre-trained word embeddings as an initialization for entity representations, using word averaging.\nSeveral alternative methods for embedding concepts atomically have been proposed in the biomedical domain. De Vine et al. (2014) automatically tag a corpus for biomedical concepts and use the sequence of concept identifiers as training context for word2vec, while Mencia et al. (2016) use the text content of documents tagged with MeSH headers to learn representations for the headers. Choi et al. (2016a) in contrast, use sequences of human-annotated medical codes to learn embeddings; Choi et al. (2016b) take a similar approach, and also learn concept embeddings from concept identifier cooccurrence statistics.\nWang et al. (2014a) is the closest approach to ours, as they propose methods for jointly embedding KB entities and words into the same space. However, their embedding method relies on the binary relations between entities in the knowledge base, which have been shown to be highly incomplete (Min et al., 2013). Their best results also rely on human annotation via Wikipedia links."
    }, {
      "heading" : "3 Method",
      "text" : "We propose a method to jointly embed concepts, phrases, and words into a real-valued space, using structured knowledge from an ontology and an unannotated training corpus.\nLet C denote the set of canonical concepts in an ontology. For any specific concept c ∈ C, let Pc denote the set of phrases that can be used to represent it; P denotes the union of all of these sets of phrases. Figure 1 illustrates this many-tomany relationship between concepts and phrases.\nAdditionally, let T denote the sequence of tokens in a training corpus, and W be the word vocabulary used in it."
    }, {
      "heading" : "3.1 Preprocessing",
      "text" : "After extracting the mapping between concepts and phrases from the ontology, each phrase is assigned a unique identifier. Then, all occurrences of the mapped phrases in the training corpus are replaced with the unique identifiers, converting them to unigrams. For example, in the sentence\nPatient was diagnosed with chronic\nobstructive lung disease in December.\nthe phrase “chronic obstructive lung disease” will be replaced with its unique identifer, yielding\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nPatient was diagnosed with PHRASE 1337\nin December.\nThis replacement is done greedily over the number of tokens matched, producing two parallel versions of the training corpus: the original, untagged text (TU ), and the text tagged for mapped phrases (TT ). One token in TT may correspond to multiple tokens in TU ."
    }, {
      "heading" : "3.2 Proposed model",
      "text" : "Our model adapts the skip-gram with negative sampling variant of word2vec for joint embedding of concepts, phrases, and words. Embeddings of dimensionality d utilize three matrices:\n• EW = |W | × d; word embeddings\n• EP = |P | × d; phrase embeddings\n• EC = |C| × d; concept embeddings\nAdditionally, we use a single negative sampling matrix ENS = |W | × k. The relationships between these matrices are illustrated in Figure 2.\nTo train the embeddings, we iterate over the tagged and untagged versions of the training corpus in parallel, using a sliding context window of k untagged words. Word embeddings are trained using all words around them, and phrase embeddings are trained using the word contexts around the complete phrase. Concept embeddings are updated using the contexts of phrases that can represent them; for a mention of phrase p, these updates are normalized by the number of concepts p can represent. In all cases, the embeddings are trained to maximize the log-likelihood of their observed contexts, and to minimize the log-likelihood of randomly-selected negative samples."
    }, {
      "heading" : "3.2.1 Training objective",
      "text" : "To calculate the loss for a single (w, p, o) observation, then, where w is the observed word, p is\nEC\nEP\nEW\nENS\nη\nβ\nFigure 2: Schematic of proposed model; the dashed ENS box is discarded after training.\nthe observed phrase just completed, and o is the observed context word, we do as follows. Let Cp be the set of concepts that p can represent, and N be the set of negative samples for this observation. We follow the notation of (Levy and Goldberg, 2014) in representing the probability of the pair (w, o) coming from the true data distribution:\nP (D = 1|w, c) = σ(~w · ~c) = 1 1 + e−~w·~c (1)\nThen, the word embedding loss `w, phrase embedding loss `p, and concept embedding loss `c can be calculated as follows:\n`w = logσ(~w · ~o)− ∑ n∈N logσ(~w · ~n) (2)\n`p = logσ(~p · ~o)− ∑ n∈N logσ(~p · ~n) (3)\n`c∈Cs = [ logσ(~c · ~o)− ∑ n∈N logσ(~c · ~n) ] 1 |Cs|\n(4)"
    }, {
      "heading" : "3.2.2 Enforcing embedding similarity",
      "text" : "The training algorithm described in Section 3.2 shares information between words, phrases, and concepts only via the shared negative sample embedding matrix. This can be beneficial, as some phrases are not easily decomposable; in the case of “Lou Gehrig’s disease”, knowing that Lou Gehrig played baseball says little about ALS. However, many phrases have some compositionality (e.g., “high blood pressure”), and many phrases directly encode useful information about the concept (e.g., “amyotrophic lateral sclerosis”).\nTo enable a tradeoff between independence and compositionality in our training, we introduce two hyperparameters to our model: β controls the compositionality of phrases by words, and η controls the compositionality of concepts by phrases.\nTo construct the updated loss function, let Wp be the list of words in phrase p, andWp be the average of their embeddings. For a given concept c, let Pc denote the set of phrases that can represent it, with Pc being the average of their embeddings.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nNow, the updated objective functions look like:\n`w =logσ(~w · ~o)− ∑ n∈N logσ(~w · ~n) (5)\n`p = ( 1− β )( logσ(~p · ~o)− ∑ n∈N logσ(~p · ~n) )\n+ β ( logσ(~p · Wp) ) (6)\n`c∈Cs = [( 1− η )( logσ(~c · ~o)− ∑ n∈N logσ(~c · ~n) )\n+ η ( logσ(~c · Pc) )] 1 |Cs|\n(7)"
    }, {
      "heading" : "4 Materials",
      "text" : ""
    }, {
      "heading" : "4.1 Training corpora",
      "text" : "We train our embeddings on four corpora. Prior work has noted the effects of training corpus choice on a variety of tasks in the biomedical domain (Pakhomov et al., 2016; Griffis et al., 2016; Garla and Brandt, 2012), so we experiment with literature abstracts, clinical narratives, and their combination. Table 1 lists statistics about concepts and phrases embedded from each corpus.\nPUBMED All abstracts from the 2016 PubMed baseline of biomedical literaure (i.e., all abstracts present in PubMed at the beginning of 2016).\nCLINICAL A set of clinical notes from the Ohio State Wexner Medical Center, regarding patients diagnosed with a variety of chronic conditions.\nPM+CL Concatenation of the above corpora. WIKINYT Combination of all articles in a 2015 dump of English Wikipedia and the New York Times portions of Gigaword (Parker et al., 2011)."
    }, {
      "heading" : "4.2 Ontologies",
      "text" : "Our canonical concepts and their representative strings are sourced from two ontologies: the Unified Medical Language System (UMLS) (Bodenreider, 2004), and YAGO (Suchanek et al., 2007; Mahdisoltani et al., 2015). The UMLS Metathesaurus provides mappings between more than 100 biomedical vocabularies, enumerates the medical concepts described, and includes canonical forms (Concept Unique Identifiers, or CUIs) for each concept. CUIs are further mapped to one or more textual forms, along with semantic type information. YAGO is a knowledge base encoding information about real-world entities and the relationships between them, including information from\nWordNet (Fellbaum, 1998) and Wikipedia. As with the UMLS, YAGO maps between canonical entity identifiers and one or more textual forms.\nIn both cases, we restrict ourselves to text forms in English, and entities with at least one text form that appears ≥ 25 times in our corpora."
    }, {
      "heading" : "4.3 Evaluation datasets",
      "text" : ""
    }, {
      "heading" : "4.3.1 Biomedical similarity and relatedness",
      "text" : "To evaluate our biomedical concepts, we used the MayoSRS and Mini Mayo SRS datasets from (Pedersen et al., 2007) and the UMNSRS Similarity and Relatedness datasets of (Pakhomov et al., 2010). MayoSRS consists of 101 pairs of concepts, identified by both UMLS CUI and text form, ranked on a scale of 1-4; the Mini Mayo SRS set includes 29 of these pairs, selected for high inter-rater agreement. The UMNSRS Similarity and Relatedness datasets are composed of 566 and 587 concept pairs, respectively, ranked on a scale of 1-1600 for similarity or relatedness; CUI and text forms are provided for all concepts."
    }, {
      "heading" : "4.3.2 General-domain similarity and relatedness",
      "text" : "While there are well-studied datasets of word similarity and relatedness available, such as WordSim-353 (Finkelstein et al., 2001), SimLex999 (Hill et al., 2015), as well as datasets for evaluating compositionality (Marelli et al., 2014) and typed relations between nouns (Baroni and Lenci, 2011), we are not aware of comparable similarity or relatedness datasets for real-world entities. We therefore present two novel datasets composed of human judgments of similarity and relatedness between pairs of people, places, and organizations.\nWe used Amazon’s Mechanical Turk crowdsourcing platform for our data collection. Participants were asked to assign a similarity or relatedness score from 0-100 to each of 34 pairs of entities, with a higher score indicating higher similarity/relatedness. We collected judgments for 24 such sets of entity pairs, getting separate judgments for similarity and for relatedness. 4 entity pairs in each set were used as validation questions, as they were determined to tend strongly towards either high or low similarity/relatedness. These pairs were not included in the final dataset; filtering for participants who self-reported high English reading proficiency and who gave reasonable responses to the validation questions pro-\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nCorpus Onto # Docs # Tokens # Concepts Max Phrases Avg\nPhrases PUBMED\nUMLS 14.7m 2.7b 188,486 390 5.4\nCLINICAL 377k 160.3m 37,219 160 7.2 PM+CL 15.1m 2.8b 193,064 390 5.4 WIKINYT YAGO 6.2m 3.5b 748,752 7,191 3.2\nTable 1: Number of documents and tokens in each of the corpora used for training concept embeddings, along with the number of concepts (from ontology Onto) found in each. Max Phrases is the maximum number of representative phrases for a single concept that appear at least 25 times in the corpus; Avg Phrases is the average number of mapped phrases over all concepts.\nduced a final dataset of 688 pairs for each task (658 pairs present in both filtered datasets). Participants were paid at least $0.75 for each survey, with the rate increased as needed to guarantee state minimum wage for the time spent on the survey. Overall Fleiss’ κ for our annotations was 0.24 for both datasets, indicating fair agreement between annotators. For further details about our data collection and analysis of response, please see the supplemental material."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Model comparisons",
      "text" : "To evaluate the benefit of embedding concepts directly, we compare with several approximations using embeddings learned on our corpora:\n• APPROXPHRco We approximate a concept’s embedding as the average of the embeddings of its representative strings.\n• APPROXWORD We approximate a concept’s embedding as the average embedding of the words used in it representative strings.\n• PHRASEco We use the learned embedding of a particular string directly.\n• WORD We approximate a particular string as the average embedding of its words.\nDomain Dataset # of pairs\nMedical\nMayo SRS 101 Mini Mayo SRS 29\nUMNSRS Relatedness 587 UMNSRS Similarity 566\nGeneral Relatedness 688\nSimilarity 688\nTable 2: Similarity and relatedness datasets used\nIn all settings, we use subscript co to denote jointly-trained embeddings; WORDind indicates word embeddings trained alone using the skipgram model of word2vec.\nIn addition, we compare with a variety of pretrained embeddings from prior research:\n• DEVINE Embeddings of 52k UMLS CUIs trained over sequences of CUIs automatically tagged from EHR data and medical abstracts (De Vine et al., 2014).\n• CHOI Embeddings of 15k UMLS CUIs trained via hierarchical sampling over sequences of manually annotated claims codes from EHR data (Choi et al., 2016b).\n• MENCIA Embeddings of 26k MESH headings trained over manually-tagged PubMed abstracts to maximize similarity of headings to relevant documents (Mencia et al., 2016).\n• CHIU-2 Word embeddings trained on PubMed abstracts with a context window size of 2 (Chiu et al., 2016a).\n• CHIU-30 Word embeddings trained on PubMed abstracts with a context window size of 30 (Chiu et al., 2016a)."
    }, {
      "heading" : "5.2 Hyperparameter tuning",
      "text" : "Recent findings highlight the importance of hyperparameter selection for performance of distributed representations on downstream tasks (Levy et al., 2015; Chiu et al., 2016a). We therefore evaluate the impact of various hyperparameters on the similarity and relatedness task, and use the results to tune optimal settings for further evaluations.2 In\n2(Chiu et al., 2016a) point out that in the biomedical domain, hyperparameter settings that improve performance on intrinsic evaluations of the vector space may degrade perfor-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nCorpus β η NS WS Dim PUBMED 0.75 0.0 5 30 400 CLINICAL 0.25 0.25 10 5 600 PM+CL 0.0 0.0 5 30 100 WIKINYT 0.0 0.0 5 5 300\nTable 3: Tuned hyperparameters for each corpus; NS is the number of negative samples used, WS is the size of the context window, and Dim is the dimensionality of the vector space.\naddition to the β and η tradeoff parameters of our model, we experiment with number of negative samples, vector dimensionality, and context window size. In all cases, we trained for 10 iterations. Table 3 lists the final tuned settings for each corpus; for more details on our hyperparameter tuning, please see the supplementary material."
    }, {
      "heading" : "5.3 Similarity and relatedness",
      "text" : "Given a pair of ontology entities 〈e1, e2〉, we calculate both their similarity and relatedness via the cosine similarity of their embeddings. We then rank each list of entity pairs in order of decreasing cosine similarity, and compare our ranking against the ranked list of human similarity or relatedness judgments. We report Spearman’s rank correlation coefficient (Spearman’s ρ), which ranges from -1 (reverse ranking) to 1 (same ranking); Table 4 shows our results. On the two smaller biomedical datasets, our method is outperformed by CHOI; however, on the larger datasets, our concepts embeddings have the highest correlation with human judgments on the similarity task, and competitive performance on the relatedness task (jointly embedded phrases are negligibly better than CHIU-2). On our real-world entity datasets, the phrase-based concept approximation outperforms the direct concept embeddings."
    }, {
      "heading" : "5.4 Concepts, phrases, and words",
      "text" : "Since our model jointly embeds concepts, phrases, and words into the same space, we assess how well the ontological links between concepts and phrases and between phrases and words are preserved. We first approximate each concept and phrase by averaging the embeddings of their representative phrases and words, and calculate co-\nmance in extrinsic downstream applications. Our analysis is restricted to evaluations relying on the affine organization of the vector space; thus, we note that these optimized hyperparameters will likely change for downstream tasks.\nsine similarity with the learned concept or phrase embedding. Figure 3a shows that almost every concept has very high similarity to its mean phrasal embedding, but we see higher variance when comparing phrases to the average of their words (Figure 3b). This is further reflected in Figure 3c, where we see that most concepts has cosine similarity near 0.5 to the average of the words of its representative phrases.\nTo assess if this pattern holds for individual cases, we examine the phrases and words in the neighborhood of a parent concept or phrase. In particular, given a parent concept or phrase p, we calculate the mean average precision (MAP) of Cp, the child phrases or words connected to it, within the full phrase or word vocabulary V .\nWe first rank all v ∈ V in order of descending cosine similarity to p; this produces the ranked list V̂ . Then, for each child term c in the “correct” subset Cp ⊂ V , we calculate its precision as:\nPr(c|Cp, V ) = 1V̂<c\nV̂c (8)\nwhere V̂c denotes the index of c in V̂ , and 1V̂<c is an indicator function over the elements of V̂ up to c that is 1 if the element is in Cp and 0 otherwise. The MAP calculation is then\nMAP(p|V ) = 1 |P | ∑ p∈P\n∑ c∈Cp Pr(c|Cp, V )\n|Cp| (9)\nFigure 4a shows that the resulting MAP scores for concepts and their phrases are skewed towards 0, with clear peaks at 0.5 and 1. This suggests that most concepts are not especially near their individual representative phrases in the vector space, in comparison to the full phrase vocabulary. In contrast, the MAP scores for phrases and their words are strikingly bi-modal near 0 and 1, indicating that most phrases are either quite close or very distant from their component words in the embedding space. Taken together, these results indicate that our model seems to be distributing component phrases or words fairly equally around their parents, though at varying distance."
    }, {
      "heading" : "5.5 Semantic type clustering",
      "text" : "Choi et al. (2016b) propose the Medical Conceptual Similarity Measure, which evaluates how well embedded concepts of the same semantic\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nCorpus Setting Mayo SRS Mini Mayo SRS\nUMNSRS Relatedness\nUMNSRS Similarity\nPM+CL\nWORDind 0.237 0.400 0.546 0.567 WORDco 0.370 0.505 0.579 0.615\nPHRASEco 0.618 0.477 0.591 0.628 APPROXWORDind 0.376 0.606 0.375 0.445 APPROXWORDco 0.493 0.600 0.400 0.495\nAPPROXPHRco 0.609 0.623 0.543 0.637 CONCEPTco 0.593 0.591 0.559 0.653\nConcept baselines DEVINE(CUIs only) 0.559 0.434 0.422 0.455\nCHOI(+ICD9,etc.) 0.817 0.726 0.384 0.552 MENCIA(MeSH headers) 0.646 0.572 0.534 0.565\nWord baselines CHIU-2(words only) 0.368 0.565 0.496 0.560CHIU-30(words only) 0.472 0.630 0.590 0.653\nTable 4: Similarity/relatedness results for tuned embeddings on biomedical datasets. Reported scores are Spearman’s ρ, which ranges [−1, 1]. We only report PM+CL results here; PUBMED and CLINICAL are similar. Best-performing settings for each dataset are marked in bold.\nFigure 3: Compositionality evaluation\nSetting Relatedness Similarity WORDind 0.602 0.589 WORDco 0.645 0.636\nPHRASEco 0.784 0.743 APPROXWORDind 0.578 0.588 APPROXWORDco 0.615 0.624\nAPPROXPHRco 0.810 0.766 CONCEPTco 0.789 0.750\nTable 5: Similarity/relatedness results for tuned embeddings on real-world entity datasets.\ntype cluster in a fixed neighborhood of k nearest neighbors. To obtain a more complete picture of the space, we adapt their methodology to find all concepts of a given semantic type within the entire space; as this is a ranking problem, we report mean average precision. We follow Choi et al. (2016b) in selecting 6 common semantic types to evaluate: disease or syndrome, pharmacologic substance, neoplastic process, finding, injury and poisoning, and sign or symptom (this replaces clinical drug, as not all concept embedding sets contained multiple concepts of that type).\nWe evaluated our three biomedical concept embeddings, and compared against the four medical concept embeddings from prior work; the\nresults are shown in Table 6. The medical code embeddings of Choi et al. (2016b) demonstrate superior clustering in aggregate,3 but clustering performance is mixed among the semantic types, with our embeddings clustering competitively with prior work. In particular, “sign or symptom,” which has many exemplars in each embedding set, is uniformly poorly clustered, suggesting that its semantics are difficult to capture."
    }, {
      "heading" : "6 Discussion",
      "text" : "Despite the noisiness of our distantly-supervised training method, our results indicate that we capture certain semantic characteristics of ontology concepts nearly as well as prior methods that require significantly more annotation. Furthermore, we can scale to a much larger vocabulary without additional expense. However, there are a number of points that bear further examination.\nOur results comparing APPROXWORDco to APPROXWORDind align with prior work indicating that including structured knowledge about concepts in training word embeddings improves\n3Choi et al. (2016b) propose another embedding technique that has superior MAP (0.20) on finding, which we omit for brevity.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nFigure 4: Mean average precision distributions for concept-phrase and phrase-word comparisons.\nSTY PM CL PM\n+CL DV Choi ME\npharmacologic\nsubstance 0.26 0.14 0.20 0.17 0.21 0.13\ndisease or syndrome 0.20 0.12 0.15 0.21 0.33 0.16\ninjury or poisoning 0.04 0.02 0.03 0.04 0.36 0.05\nsign or\nsymptom 0.09 0.06 0.05 0.06 0.02 0.02\nneoplastic\nprocess 0.20 0.04 0.18 0.14 0.25 0.10\nfinding 0.12 0.12 0.11 0.09 0.12 0.01 Overall 0.15 0.08 0.12 0.12 0.22 0.08\nTable 6: MAP for semantic type clustering with selected biomedical semantic types (STY). PM=PUBMED, CL=CLINICAL, DV=DeVine, ME=Mencia. Highest MAP per semantic type is marked in bold.\ntheir performance in some semantic tasks (Wang et al., 2014a; Faruqui et al., 2015). However, the extreme similarity observed in our compositionality analysis, and the high performance of concept approximations, suggests that some non-lexical information about concepts is not being captured in our method. Leveraging an approach similar to Wang et al. (2014a) and directly including ontology structure as a training criterion may be one way to model some of this additional information.\nChiu et al. (2016b,a); Faruqui et al. (2016) describe several issues with evaluating linguistic embeddings with similarity and relatedness tasks, and poor generalization of results to downstream applications. While we address some of their concerns (e.g., our concept embeddings rule out polysemy), several of the issues they describe remain. One of these is the lack of train/test splits on the similarity and relatedness tasks; though we do not\ntrain our model directly on these data, we are still tuning on the test set. We attempt to mitigate this issue by an adapted 5-fold cross-validation, and our other analyses of the space indicate that our embeddings are capturing some semantic properties, but applications to other intrinsic tasks such as analogy completion or downstream NLP tasks would provide additional strong evidence of these properties. While analogies at the entity/concept level are limited, we suggest that applications such as entity linking be pursued in future work.\nFinally, we note a slight mismatch between the ontologies we used. The UMLS encodes information about abstract biomedical concepts, while YAGO is focused on concrete, real-world entities. We demonstrate that our method is able to model both kinds of information, but it would benefit from an application to more abstract data in the general domain, similar to previous work with BabelNet (Camacho-Collados et al., 2015)."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We propose a novel model to jointly embed canonical concepts, the phrases that represent them, and words into a shared vector space. Our method only uses distant supervision from phrases linked to concepts in an ontology, but we experimentally demonstrate that our embeddings preserve several semantic properties comparably to recent methods that require human-annotated data. In particular, our concept embeddings maintain similarity and relatedness, as evaluated by cosine similarity, and preserve the links between concepts of the same type and between concepts and their representative phrases.\nWith this paper, we include cui2vec, a software implementation of our method, as well as our novel dataset of similarity and relatedness of realworld entities.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    }, {
      "heading" : "A Supplemental Material",
      "text" : "A.1 Hyperparameter tuning We used an adapted 5-fold cross-validation for our hyperparameter tuning. First, each dataset was partitioned into 5 portions. Then, each hyperparameter setting was evaluated on each of the other 4 partitions, and the average of those results was considered to be that setting’s error for the target partition. The overall error of each setting is then the average of its 5 per-fold errors.4\nWe evaluated the following settings: β, η ∈ {0, 0.25, 0.5, 0.75, 1}, number of negative samples ∈ {5, 10, 15}, vector dimension ∈ {50, 100, 200, 300, 400, 500, 600}, and context window size ∈ {2, 5, 10, 30}. Default hyperparameter settings were 5 negative samples, window size of 2, and vector dimension of 100. Chiu et al. (2016a) found that a learning rate of α = 0.05 performed better than the standard 0.025 with biomedical tasks; accordingly, we use α = 0.05 in our biomedical corpora. However, the traditional learning rate of α = 0.025 gave better performance on our general-domain corpus. In all cases, we restricted ourselves to a minimum frequency threshold for words and phrases of 255.\nIn general, we find that while enforcing some small amount of concept-phrase or phrase-word with the β and η parameters can be helpful, similarity and relatedness performance trend negative as β and η increase. This negative trend is more pronounced with the concept-phrase similarity, controlled by the η parameter; a non-zero η is only optimal in a single corpus setting.\nWith regards to the other hyperparameters we evaluated, no clear trends emerged, as each hyperparameter behaved differently in each corpus. However, the differences between best and worstperforming hyperparameter settings were relatively small (the largest was a difference in Spearman’s ρ of .075, from vector dimensionality in\n4Since the similarity and relatedness evaluation is a ranking task, evaluated by Spearman’s ρ, this is not equivalent to simply ranking the full dataset; in fact, the validation numbers skew worse than the overall numbers, as the smaller dataset size in the cross-fold evaluations penalizes ranking errors more highly.\n5This was done primarily for memory constraints. However, given the small size of our Clinical corpus, we experimented with smaller thresholds; we found overall decreases in performance and only small gains in the number of concepts modeled.\n1100\n1101\n1102\n1103\n1104\n1105\n1106\n1107\n1108\n1109\n1110\n1111\n1112\n1113\n1114\n1115\n1116\n1117\n1118\n1119\n1120\n1121\n1122\n1123\n1124\n1125\n1126\n1127\n1128\n1129\n1130\n1131\n1132\n1133\n1134\n1135\n1136\n1137\n1138\n1139\n1140\n1141\n1142\n1143\n1144\n1145\n1146\n1147\n1148\n1149\n1150\n1151\n1152\n1153\n1154\n1155\n1156\n1157\n1158\n1159\n1160\n1161\n1162\n1163\n1164\n1165\n1166\n1167\n1168\n1169\n1170\n1171\n1172\n1173\n1174\n1175\n1176\n1177\n1178\n1179\n1180\n1181\n1182\n1183\n1184\n1185\n1186\n1187\n1188\n1189\n1190\n1191\n1192\n1193\n1194\n1195\n1196\n1197\n1198\n1199\nthe CLINICAL corpus). The most consistentlyperforming hyperparameter settings across the corpora were negative samples=5, vector dimensionality=100, and context window size=5.\nA.2 Similarity/Relatedness dataset\nWe followed a similar process to Pakhomov et al. (2010) in selecting the entity pairs to be used in our dataset. We first filtered all entities in YAGO to the subset that we learned embeddings for (≈ 700k); we then filtered to only entities labeled with WordNet types organization or person, or with the YAGO type geoEntity. For each pairing of these categories (Organization-Organization, Organization-Place, Organization-Person, PlacePlace, Place-Person, and Person-Person), we manually selected 30 pairs of entities for each of the following relatedness categories: Completely Unrelated, Somewhat Unrelated, Somewhat Related, and Highly Related. These produced the list of 720 entity pairs we used for our Mechanical Turk surveys.\nWe augmented each survey of 30 questions with 4 manually-created validation pairs using common entities (e.g., London, New York), each of which was categorized as Highly Related or Completely Unrelated. We included these validation questions at random indices in our surveys. To evaluate if participants were reading the questions, we binned their ratings on these validation questions into 0-25 (Completely Unrelated), 26- 50 (Somewhat Unrelated), 51-75 (Somewhat Related), and 76-100 (Highly Related). If a participant’s ratings disagreed with ours on multiple validation questions, we discarded their data (we allowed disagreement on a single question, as some validation questions had high variance in responses among reliable annotators).\nWe recruited 6 participants for each survey, for a total of 34 unique participants across the 48 HITs. The majority responded to a single HIT, while 3 completed more than 20. We discarded all submissions from 3 participants, as they did not report English reading proficiency (1) or did not satisfy the validation questions (2). All participants were paid, regardless of if we used their data or not.\nTo generate the final dataset, we assessed each participant’s responses to the validation questions in each survey. We kept surveys for which we had at least 4 participants with satisfactory answers to\nFigure 5: Distribution of Fleiss’ kappa for the HITs in our similarity and relatedness datasets.\nthe validation questions; this resulted in discarding 1 of the 24 HITs for each task. Due to 2 repeated pairs, this gave us final dataset sizes of 688 pairs for each of similarity and relatedness, 658 of which were shared between the tasks.\nIn assessing inter-annotator agreement (IAA), we considered each HIT individually, as we had neither the same participants nor the same number of accepted responses for all HITs. Figure 5 shows the distribution of Fleiss’ κ for the HITs in each dataset; the average for each case was 0.24."
    } ],
    "references" : [ {
      "title" : "How we BLESSed distributional semantic evaluation",
      "author" : [ "Marco Baroni", "Alessandro Lenci." ],
      "venue" : "GEMS ’11 Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics pages 1–10.",
      "citeRegEx" : "Baroni and Lenci.,? 2011",
      "shortCiteRegEx" : "Baroni and Lenci.",
      "year" : 2011
    }, {
      "title" : "The Unified Medical Language System (UMLS): integrating biomedical terminology",
      "author" : [ "Olivier Bodenreider." ],
      "venue" : "Nucleic Acids Research 32(90001):D267–D270. https://doi.org/10.1093/nar/gkh061.",
      "citeRegEx" : "Bodenreider.,? 2004",
      "shortCiteRegEx" : "Bodenreider.",
      "year" : 2004
    }, {
      "title" : "Translating Embeddings for Modeling MultiRelational Data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko." ],
      "venue" : "Advances in Neural Information Processing Systems. Curran Associates, Inc., vol-",
      "citeRegEx" : "Bordes et al\\.,? 2013",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning Structured Embeddings of Knowledge Bases",
      "author" : [ "Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence. pages 301–306.",
      "citeRegEx" : "Bordes et al\\.,? 2011",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2011
    }, {
      "title" : "A Unified Multilingual Semantic Representation of Concepts",
      "author" : [ "José Camacho-Collados", "Mohammad Taher Pilehvar", "Roberto Navigli." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-",
      "citeRegEx" : "Camacho.Collados et al\\.,? 2015",
      "shortCiteRegEx" : "Camacho.Collados et al\\.",
      "year" : 2015
    }, {
      "title" : "How to Train Good Word Embeddings for Biomedical NLP",
      "author" : [ "Billy Chiu", "Gamal Crichton", "Anna Korhonen", "Sampo Pyysalo." ],
      "venue" : "Proceedings of the 15th Workshop on Biomedical Natural Language Processing pages 166–174.",
      "citeRegEx" : "Chiu et al\\.,? 2016a",
      "shortCiteRegEx" : "Chiu et al\\.",
      "year" : 2016
    }, {
      "title" : "Intrinsic Evaluation of Word Vectors Fails to Predict Extrinsic Performance",
      "author" : [ "Billy Chiu", "Anna Korhonen", "Sampo Pyysalo." ],
      "venue" : "Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP pages 1–6.",
      "citeRegEx" : "Chiu et al\\.,? 2016b",
      "shortCiteRegEx" : "Chiu et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-layer Representation Learning for Medical Concepts",
      "author" : [ "Edward Choi", "Mohammad Taha Bahadori", "Elizabeth Searles", "Catherine Coffey", "Jimeng Sun." ],
      "venue" : "SIGKDD 2016 .",
      "citeRegEx" : "Choi et al\\.,? 2016a",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning Low-Dimensional Representations of Medical Concepts Methods Background",
      "author" : [ "Youngduck Choi", "Chill Yi-I Chiu", "David Sontag." ],
      "venue" : "AMIA Summit on Clinical Research Informatics. pages 41–50.",
      "citeRegEx" : "Choi et al\\.,? 2016b",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2016
    }, {
      "title" : "Medical semantic similarity with a neural language model",
      "author" : [ "Lance De Vine", "Guido Zuccon", "Bevan Koopman", "Laurianne Sitbon", "Peter Bruza." ],
      "venue" : "Proceedings of the 23rd ACM International Conference on Information",
      "citeRegEx" : "Vine et al\\.,? 2014",
      "shortCiteRegEx" : "Vine et al\\.",
      "year" : 2014
    }, {
      "title" : "Jointly Embedding Relations and Mentions for Knowledge Population",
      "author" : [ "Miao Fan", "Kai Cao", "Yifan He", "Ralph Grishman." ],
      "venue" : "Proceedings of the International Conference Recent Advances in Natural Language Processing. INCOMA Ltd. Shoumen,",
      "citeRegEx" : "Fan et al\\.,? 2015",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2015
    }, {
      "title" : "Retrofitting Word Vectors to Semantic Lexicons",
      "author" : [ "Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Associa-",
      "citeRegEx" : "Faruqui et al\\.,? 2015",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2015
    }, {
      "title" : "Problems With Evaluation of Word Embeddings Using Word Similarity Tasks",
      "author" : [ "Manaal Faruqui", "Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer." ],
      "venue" : "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP. Asso-",
      "citeRegEx" : "Faruqui et al\\.,? 2016",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2016
    }, {
      "title" : "WordNet",
      "author" : [ "Christiane Fellbaum." ],
      "venue" : "Wiley Online Library.",
      "citeRegEx" : "Fellbaum.,? 1998",
      "shortCiteRegEx" : "Fellbaum.",
      "year" : 1998
    }, {
      "title" : "Placing Search in Context: The Concept Revisited",
      "author" : [ "Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin." ],
      "venue" : "Proceedings of the 10th International Conference on World Wide Web.",
      "citeRegEx" : "Finkelstein et al\\.,? 2001",
      "shortCiteRegEx" : "Finkelstein et al\\.",
      "year" : 2001
    }, {
      "title" : "Knowledgebased biomedical word sense disambiguation: an evaluation and application to clinical document classification",
      "author" : [ "V.N. Garla", "C. Brandt." ],
      "venue" : "Journal of the American Medical Informatics Association pages 882–886.",
      "citeRegEx" : "Garla and Brandt.,? 2012",
      "shortCiteRegEx" : "Garla and Brandt.",
      "year" : 2012
    }, {
      "title" : "Analogy-based Detection of Morphological and Semantic Relations With Word Embeddings: What Works and What Doesn’t",
      "author" : [ "Anna Gladkova", "Aleksandr Drozd", "Satoshi Matsuoka." ],
      "venue" : "Proceedings of NAACL-HLT 2016, San Diego, California, June",
      "citeRegEx" : "Gladkova et al\\.,? 2016",
      "shortCiteRegEx" : "Gladkova et al\\.",
      "year" : 2016
    }, {
      "title" : "A Quantitative and Qualitative Evaluation of Sentence Boundary Detection for the Clinical Domain Department of Computer Science and Engineering",
      "author" : [ "Denis Griffis", "Chaitanya Shivade", "Eric Fosler-lussier", "Albert M Lai" ],
      "venue" : "Biomedi-",
      "citeRegEx" : "Griffis et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Griffis et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning Abstract Concept Embeddings from Multi-Modal Data: Since You Probably Can’t See What I Mean",
      "author" : [ "Felix Hill", "Anna Korhonen." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Hill and Korhonen.,? 2014",
      "shortCiteRegEx" : "Hill and Korhonen.",
      "year" : 2014
    }, {
      "title" : "SimLex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation",
      "author" : [ "Felix Hill", "Roi Reichart", "Anna Korhonen." ],
      "venue" : "Computational Linguistics 41(4):665–695.",
      "citeRegEx" : "Hill et al\\.,? 2015",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2015
    }, {
      "title" : "Entity Hierarchy Embedding",
      "author" : [ "Zhiting Hu", "Poyao Huang", "Yuntian Deng", "Yingkai Gao", "Eric Xing." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natu-",
      "citeRegEx" : "Hu et al\\.,? 2015",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed Representations of Sentences and Documents",
      "author" : [ "Qv Le", "Tomas Mikolov." ],
      "venue" : "International Conference on Machine Learning - ICML 2014 32:1188–1196.",
      "citeRegEx" : "Le and Mikolov.,? 2014",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    }, {
      "title" : "Neural Word Embedding as Implicit Matrix Factorization",
      "author" : [ "Omer Levy", "Yoav Goldberg." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) pages 2177–2185.",
      "citeRegEx" : "Levy and Goldberg.,? 2014",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Improving Distributional Similarity with Lessons Learned from Word Embeddings",
      "author" : [ "Omer Levy", "Yoav Goldberg", "Ido Dagan." ],
      "venue" : "Transactions of the Association for Computational Linguistics 3:211–225.",
      "citeRegEx" : "Levy et al\\.,? 2015",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning Entity and Relation Embeddings for Knowledge Graph Completion",
      "author" : [ "Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu." ],
      "venue" : "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence. AAAI Press,",
      "citeRegEx" : "Lin et al\\.,? 2015",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "Issues in evaluating semantic spaces using word analogies",
      "author" : [ "Tal Linzen." ],
      "venue" : "Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP. pages 13–18.",
      "citeRegEx" : "Linzen.,? 2016",
      "shortCiteRegEx" : "Linzen.",
      "year" : 2016
    }, {
      "title" : "YAGO3: A Knowledge Base from Multilingual Wikipedias",
      "author" : [ "Farzaneh Mahdisoltani", "Joanna Biega", "Fabian M Suchanek." ],
      "venue" : "Conference on Innovative Data Systems Research.",
      "citeRegEx" : "Mahdisoltani et al\\.,? 2015",
      "shortCiteRegEx" : "Mahdisoltani et al\\.",
      "year" : 2015
    }, {
      "title" : "A SICK cure for the evaluation of compositional distributional semantic models",
      "author" : [ "Marco Marelli", "S Menini", "Marco Baroni", "L Bentivogli", "R Bernardi", "R Zamparelli." ],
      "venue" : "LREC. May.",
      "citeRegEx" : "Marelli et al\\.,? 2014",
      "shortCiteRegEx" : "Marelli et al\\.",
      "year" : 2014
    }, {
      "title" : "Medical Concept Embeddings via Labeled Background Corpora",
      "author" : [ "Eneldo Loza Mencia", "Gerard de Melo", "Jinseok Nam." ],
      "venue" : "Proceedings of the 10th Language Resources and Evaluation Conference pages 4629–4636.",
      "citeRegEx" : "Mencia et al\\.,? 2016",
      "shortCiteRegEx" : "Mencia et al\\.",
      "year" : 2016
    }, {
      "title" : "Distributed Representations of Words and Phrases and Their Compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems 26. Curran Associates, Inc., NIPS ’13,",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic Regularities in Continuous Space Word Representations",
      "author" : [ "Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distant Supervision for Relation Extraction with an Incomplete Knowledge Base",
      "author" : [ "Bonan Min", "Ralph Grishman", "Li Wan", "Chang Wang", "David Gondek." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for",
      "citeRegEx" : "Min et al\\.,? 2013",
      "shortCiteRegEx" : "Min et al\\.",
      "year" : 2013
    }, {
      "title" : "Distant supervision for relation extraction without labeled data",
      "author" : [ "Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on",
      "citeRegEx" : "Mintz et al\\.,? 2009",
      "shortCiteRegEx" : "Mintz et al\\.",
      "year" : 2009
    }, {
      "title" : "Evaluating distributed word representations for capturing semantics of biomedical concepts",
      "author" : [ "T H Muneeb", "Sunil Kumar Sahu", "Ashish Anand." ],
      "venue" : "Proceedings of the 2015 Workshop on Biomedical Natural Language Processing (BioNLP 2015)",
      "citeRegEx" : "Muneeb et al\\.,? 2015",
      "shortCiteRegEx" : "Muneeb et al\\.",
      "year" : 2015
    }, {
      "title" : "Embedding Senses for Efficient Graph-based Word Sense Disambiguation",
      "author" : [ "Luis Nieta Pina", "Richard Johansson." ],
      "venue" : "Proceedings of TextGraphs-10: the Workshop on Graph-based Methods for Natural Language Processing. Association for Compu-",
      "citeRegEx" : "Pina and Johansson.,? 2016",
      "shortCiteRegEx" : "Pina and Johansson.",
      "year" : 2016
    }, {
      "title" : "Semantic Similarity and Relatedness between Clinical Terms: An Experimental Study",
      "author" : [ "Serguei Pakhomov", "Bridget McInnes", "Terrence Adam", "Ying Liu", "Ted Pedersen", "Genevieve B Melton." ],
      "venue" : "AMIA Annual Symposium Proceedings 2010:572–576.",
      "citeRegEx" : "Pakhomov et al\\.,? 2010",
      "shortCiteRegEx" : "Pakhomov et al\\.",
      "year" : 2010
    }, {
      "title" : "Corpus domain effects on distributional semantic modeling of medical terms",
      "author" : [ "Serguei V S Pakhomov", "Greg Finley", "Reed McEwan", "Yan Wang", "Genevieve B Melton." ],
      "venue" : "Bioinformatics 32(August):btw529.",
      "citeRegEx" : "Pakhomov et al\\.,? 2016",
      "shortCiteRegEx" : "Pakhomov et al\\.",
      "year" : 2016
    }, {
      "title" : "English Gigaword",
      "author" : [ "Robert Parker", "David Graff", "Junbo Kong", "Ke Chen", "Kazuaki Maeda." ],
      "venue" : "Linguistic Data Consortium .",
      "citeRegEx" : "Parker et al\\.,? 2011",
      "shortCiteRegEx" : "Parker et al\\.",
      "year" : 2011
    }, {
      "title" : "Measures of semantic similarity and relatedness in the biomedical domain",
      "author" : [ "Ted Pedersen", "Serguei V.S. Pakhomov", "Siddharth Patwardhan", "Christopher G. Chute." ],
      "venue" : "Journal of Biomedical Informatics 40(3):288–299.",
      "citeRegEx" : "Pedersen et al\\.,? 2007",
      "shortCiteRegEx" : "Pedersen et al\\.",
      "year" : 2007
    }, {
      "title" : "Reasoning With Neural Tensor Networks for Knowledge Base Completion",
      "author" : [ "Richard Socher", "Danqi Chen", "Christopher D. Manning", "Andrew Y. Ng." ],
      "venue" : "Advances in Neural Information Processing Systems 26:926–934.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions",
      "author" : [ "Richard Socher", "Jeffrey Pennington", "Eric H Huang", "Andrew Y Ng", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Socher et al\\.,? 2011",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "YAGO: A Core of Semantic Knowledge Unifying WordNet and Wikipedia",
      "author" : [ "Fabian M Suchanek", "Gjergji Kasneci", "Gerhard Weikum." ],
      "venue" : "Proceedings of the 16th International Conference on World Wide Web pages 697–706.",
      "citeRegEx" : "Suchanek et al\\.,? 2007",
      "shortCiteRegEx" : "Suchanek et al\\.",
      "year" : 2007
    }, {
      "title" : "Representing Text for Joint Embedding of Text and Knowledge Bases",
      "author" : [ "Kristina Toutanova", "Danqi Chen", "Patrick Pantel", "Hoifung Poon", "Pallavi Choudhury", "Michael Gamon." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Toutanova et al\\.,? 2015",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2015
    }, {
      "title" : "Knowledge Graph and Text Jointly Embedding",
      "author" : [ "Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational",
      "citeRegEx" : "Wang et al\\.,? 2014a",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Knowledge Graph Embedding by Translating on Hyperplanes",
      "author" : [ "Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen." ],
      "venue" : "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence. AAAI Press, Quebec City, Quebec, Canada,",
      "citeRegEx" : "Wang et al\\.,? 2014b",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2014
    }, {
      "title" : "Embedding Entities and Relations for Learning and Inference in Knowledge Bases",
      "author" : [ "Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng." ],
      "venue" : "Proceedings of the International Conference on Learning Representations. ICLR, page 12.",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical Attention Networks for Document Classification",
      "author" : [ "Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Associa-",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Retrofitting Word Vectors of MeSH Terms to Improve Semantic Similarity Measures",
      "author" : [ "Zhiguo Yu", "Trevor Cohen", "Wallace Byron", "Elmer V Bernstam", "Todd R Johnson." ],
      "venue" : "Proceedings of the Seventh International Workshop on Health Text Mining",
      "citeRegEx" : "Yu et al\\.,? 2016",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    }, {
      "title" : "2016a) found that a learning rate",
      "author" : [ "Chiu" ],
      "venue" : null,
      "citeRegEx" : "Chiu,? \\Q2016\\E",
      "shortCiteRegEx" : "Chiu",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Word embeddings are either used directly as features in downstream modeling tasks (Chiu et al., 2016b), or are composed to produce representations of more complex linguistic units, such as phrases, sentences (Socher et al.",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 40,
      "context" : ", 2016b), or are composed to produce representations of more complex linguistic units, such as phrases, sentences (Socher et al., 2011; Le and Mikolov, 2014), or even documents (Yang et al.",
      "startOffset" : 114,
      "endOffset" : 157
    }, {
      "referenceID" : 21,
      "context" : ", 2016b), or are composed to produce representations of more complex linguistic units, such as phrases, sentences (Socher et al., 2011; Le and Mikolov, 2014), or even documents (Yang et al.",
      "startOffset" : 114,
      "endOffset" : 157
    }, {
      "referenceID" : 46,
      "context" : ", 2011; Le and Mikolov, 2014), or even documents (Yang et al., 2016).",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "Word embeddings are either used directly as features in downstream modeling tasks (Chiu et al., 2016b), or are composed to produce representations of more complex linguistic units, such as phrases, sentences (Socher et al., 2011; Le and Mikolov, 2014), or even documents (Yang et al., 2016). However, there are many cases in which words are not atomic semantic units, and multi-word expressions cannot be expressed as a composition of their member words: for example, “the Big Apple” is not a fruit, and “Lou Gehrig’s disease” has little to do with baseball. Furthermore, such concepts often multiple textual forms: “the Big Apple” and “New York City” both refer to the same location, and “Lou Gehrig’s disease” and “amyotrophic lateral sclerosis” are the same medical condition. Despite the lack of lexical overlap between these phrases, we would like a semantic model that can represent the underlying concept, regardless of the specific textual form used. Our novel approach to this task combines structured knowledge with proven techniques for learning word embeddings to train context-based representations for contexts, phrases, and words. The model requires no human annotations: we use known phrases as distant supervision in distributional similarity training over an unannotated corpus. As in the formulation of Mintz et al. (2009) in extracting textual relationships, we assume that any occurrence of a known phrase signifies an occurrence of each concept1 it can refer to, and train our concept embeddings using the contexts of their phrase forms.",
      "startOffset" : 83,
      "endOffset" : 1342
    }, {
      "referenceID" : 30,
      "context" : "ing in analogy completion tasks (Mikolov et al., 2013b; Linzen, 2016; Gladkova et al., 2016) and in similarity/relatedness tasks (Muneeb et al.",
      "startOffset" : 32,
      "endOffset" : 92
    }, {
      "referenceID" : 25,
      "context" : "ing in analogy completion tasks (Mikolov et al., 2013b; Linzen, 2016; Gladkova et al., 2016) and in similarity/relatedness tasks (Muneeb et al.",
      "startOffset" : 32,
      "endOffset" : 92
    }, {
      "referenceID" : 16,
      "context" : "ing in analogy completion tasks (Mikolov et al., 2013b; Linzen, 2016; Gladkova et al., 2016) and in similarity/relatedness tasks (Muneeb et al.",
      "startOffset" : 32,
      "endOffset" : 92
    }, {
      "referenceID" : 33,
      "context" : ", 2016) and in similarity/relatedness tasks (Muneeb et al., 2015; Chiu et al., 2016a).",
      "startOffset" : 44,
      "endOffset" : 85
    }, {
      "referenceID" : 5,
      "context" : ", 2016) and in similarity/relatedness tasks (Muneeb et al., 2015; Chiu et al., 2016a).",
      "startOffset" : 44,
      "endOffset" : 85
    }, {
      "referenceID" : 29,
      "context" : "Multi-word concepts have also been explored by approaches such as lexicalization and word averaging (Mikolov et al., 2013a; Socher et al., 2013).",
      "startOffset" : 100,
      "endOffset" : 144
    }, {
      "referenceID" : 39,
      "context" : "Multi-word concepts have also been explored by approaches such as lexicalization and word averaging (Mikolov et al., 2013a; Socher et al., 2013).",
      "startOffset" : 100,
      "endOffset" : 144
    }, {
      "referenceID" : 44,
      "context" : "However, there is also a rich literature on learning embeddings of knowledge base entities and ontology concepts directly from the structure, relationships, and attributes encoded in the knowledge base (Bordes et al., 2011, 2013; Wang et al., 2014b; Lin et al., 2015).",
      "startOffset" : 202,
      "endOffset" : 267
    }, {
      "referenceID" : 24,
      "context" : "However, there is also a rich literature on learning embeddings of knowledge base entities and ontology concepts directly from the structure, relationships, and attributes encoded in the knowledge base (Bordes et al., 2011, 2013; Wang et al., 2014b; Lin et al., 2015).",
      "startOffset" : 202,
      "endOffset" : 267
    }, {
      "referenceID" : 2,
      "context" : ", 2015; Chiu et al., 2016a). Multi-word concepts have also been explored by approaches such as lexicalization and word averaging (Mikolov et al., 2013a; Socher et al., 2013). Fan et al. (2015), Yu et al.",
      "startOffset" : 8,
      "endOffset" : 193
    }, {
      "referenceID" : 2,
      "context" : ", 2015; Chiu et al., 2016a). Multi-word concepts have also been explored by approaches such as lexicalization and word averaging (Mikolov et al., 2013a; Socher et al., 2013). Fan et al. (2015), Yu et al. (2016), and Hill and Korhonen (2014) modify embedding training with additional information, such as named entity mentions and associated perceptions.",
      "startOffset" : 8,
      "endOffset" : 211
    }, {
      "referenceID" : 2,
      "context" : ", 2015; Chiu et al., 2016a). Multi-word concepts have also been explored by approaches such as lexicalization and word averaging (Mikolov et al., 2013a; Socher et al., 2013). Fan et al. (2015), Yu et al. (2016), and Hill and Korhonen (2014) modify embedding training with additional information, such as named entity mentions and associated perceptions.",
      "startOffset" : 8,
      "endOffset" : 241
    }, {
      "referenceID" : 2,
      "context" : "Camacho-Collados et al. (2015) use a semantic network to train word sense representations using relevant subsets of Wikipedia.",
      "startOffset" : 0,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "Camacho-Collados et al. (2015) use a semantic network to train word sense representations using relevant subsets of Wikipedia. Nieta Pina and Johansson (2016) use WordNet graph structure as context for embedding word senses; a similar approach was taken by Hu et al.",
      "startOffset" : 0,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "Camacho-Collados et al. (2015) use a semantic network to train word sense representations using relevant subsets of Wikipedia. Nieta Pina and Johansson (2016) use WordNet graph structure as context for embedding word senses; a similar approach was taken by Hu et al. (2015) for embedding entities, using the hierarchy of the containing ontology augmented with textual information.",
      "startOffset" : 0,
      "endOffset" : 274
    }, {
      "referenceID" : 2,
      "context" : "However, there is also a rich literature on learning embeddings of knowledge base entities and ontology concepts directly from the structure, relationships, and attributes encoded in the knowledge base (Bordes et al., 2011, 2013; Wang et al., 2014b; Lin et al., 2015). Toutanova et al. (2015) decomposed textual relations into single words to improve generalization, but still relied on knowledge graph structure for embedding entities; Socher et al.",
      "startOffset" : 203,
      "endOffset" : 293
    }, {
      "referenceID" : 2,
      "context" : "However, there is also a rich literature on learning embeddings of knowledge base entities and ontology concepts directly from the structure, relationships, and attributes encoded in the knowledge base (Bordes et al., 2011, 2013; Wang et al., 2014b; Lin et al., 2015). Toutanova et al. (2015) decomposed textual relations into single words to improve generalization, but still relied on knowledge graph structure for embedding entities; Socher et al. (2013) and Yang et al.",
      "startOffset" : 203,
      "endOffset" : 458
    }, {
      "referenceID" : 2,
      "context" : "However, there is also a rich literature on learning embeddings of knowledge base entities and ontology concepts directly from the structure, relationships, and attributes encoded in the knowledge base (Bordes et al., 2011, 2013; Wang et al., 2014b; Lin et al., 2015). Toutanova et al. (2015) decomposed textual relations into single words to improve generalization, but still relied on knowledge graph structure for embedding entities; Socher et al. (2013) and Yang et al. (2015), however, incorporated pre-trained word embeddings as an initialization for entity representations, using word averaging.",
      "startOffset" : 203,
      "endOffset" : 481
    }, {
      "referenceID" : 2,
      "context" : "However, there is also a rich literature on learning embeddings of knowledge base entities and ontology concepts directly from the structure, relationships, and attributes encoded in the knowledge base (Bordes et al., 2011, 2013; Wang et al., 2014b; Lin et al., 2015). Toutanova et al. (2015) decomposed textual relations into single words to improve generalization, but still relied on knowledge graph structure for embedding entities; Socher et al. (2013) and Yang et al. (2015), however, incorporated pre-trained word embeddings as an initialization for entity representations, using word averaging. Several alternative methods for embedding concepts atomically have been proposed in the biomedical domain. De Vine et al. (2014) automatically tag a corpus for biomedical concepts and use the sequence of concept identifiers as training context for word2vec, while Mencia et al.",
      "startOffset" : 203,
      "endOffset" : 732
    }, {
      "referenceID" : 2,
      "context" : "However, there is also a rich literature on learning embeddings of knowledge base entities and ontology concepts directly from the structure, relationships, and attributes encoded in the knowledge base (Bordes et al., 2011, 2013; Wang et al., 2014b; Lin et al., 2015). Toutanova et al. (2015) decomposed textual relations into single words to improve generalization, but still relied on knowledge graph structure for embedding entities; Socher et al. (2013) and Yang et al. (2015), however, incorporated pre-trained word embeddings as an initialization for entity representations, using word averaging. Several alternative methods for embedding concepts atomically have been proposed in the biomedical domain. De Vine et al. (2014) automatically tag a corpus for biomedical concepts and use the sequence of concept identifiers as training context for word2vec, while Mencia et al. (2016) use the text content of documents tagged with MeSH headers to learn representations for the headers.",
      "startOffset" : 203,
      "endOffset" : 888
    }, {
      "referenceID" : 2,
      "context" : "However, there is also a rich literature on learning embeddings of knowledge base entities and ontology concepts directly from the structure, relationships, and attributes encoded in the knowledge base (Bordes et al., 2011, 2013; Wang et al., 2014b; Lin et al., 2015). Toutanova et al. (2015) decomposed textual relations into single words to improve generalization, but still relied on knowledge graph structure for embedding entities; Socher et al. (2013) and Yang et al. (2015), however, incorporated pre-trained word embeddings as an initialization for entity representations, using word averaging. Several alternative methods for embedding concepts atomically have been proposed in the biomedical domain. De Vine et al. (2014) automatically tag a corpus for biomedical concepts and use the sequence of concept identifiers as training context for word2vec, while Mencia et al. (2016) use the text content of documents tagged with MeSH headers to learn representations for the headers. Choi et al. (2016a) in contrast, use sequences of human-annotated medical codes to learn embeddings; Choi et al.",
      "startOffset" : 203,
      "endOffset" : 1009
    }, {
      "referenceID" : 2,
      "context" : "However, there is also a rich literature on learning embeddings of knowledge base entities and ontology concepts directly from the structure, relationships, and attributes encoded in the knowledge base (Bordes et al., 2011, 2013; Wang et al., 2014b; Lin et al., 2015). Toutanova et al. (2015) decomposed textual relations into single words to improve generalization, but still relied on knowledge graph structure for embedding entities; Socher et al. (2013) and Yang et al. (2015), however, incorporated pre-trained word embeddings as an initialization for entity representations, using word averaging. Several alternative methods for embedding concepts atomically have been proposed in the biomedical domain. De Vine et al. (2014) automatically tag a corpus for biomedical concepts and use the sequence of concept identifiers as training context for word2vec, while Mencia et al. (2016) use the text content of documents tagged with MeSH headers to learn representations for the headers. Choi et al. (2016a) in contrast, use sequences of human-annotated medical codes to learn embeddings; Choi et al. (2016b) take a similar approach, and also learn concept embeddings from concept identifier cooccurrence statistics.",
      "startOffset" : 203,
      "endOffset" : 1110
    }, {
      "referenceID" : 31,
      "context" : "However, their embedding method relies on the binary relations between entities in the knowledge base, which have been shown to be highly incomplete (Min et al., 2013).",
      "startOffset" : 149,
      "endOffset" : 167
    }, {
      "referenceID" : 22,
      "context" : "We follow the notation of (Levy and Goldberg, 2014) in representing the probability of the pair (w, o) coming from the true data distribution:",
      "startOffset" : 26,
      "endOffset" : 51
    }, {
      "referenceID" : 36,
      "context" : "Prior work has noted the effects of training corpus choice on a variety of tasks in the biomedical domain (Pakhomov et al., 2016; Griffis et al., 2016; Garla and Brandt, 2012), so we experiment with literature abstracts, clinical narratives, and their combination.",
      "startOffset" : 106,
      "endOffset" : 175
    }, {
      "referenceID" : 17,
      "context" : "Prior work has noted the effects of training corpus choice on a variety of tasks in the biomedical domain (Pakhomov et al., 2016; Griffis et al., 2016; Garla and Brandt, 2012), so we experiment with literature abstracts, clinical narratives, and their combination.",
      "startOffset" : 106,
      "endOffset" : 175
    }, {
      "referenceID" : 15,
      "context" : "Prior work has noted the effects of training corpus choice on a variety of tasks in the biomedical domain (Pakhomov et al., 2016; Griffis et al., 2016; Garla and Brandt, 2012), so we experiment with literature abstracts, clinical narratives, and their combination.",
      "startOffset" : 106,
      "endOffset" : 175
    }, {
      "referenceID" : 37,
      "context" : "WIKINYT Combination of all articles in a 2015 dump of English Wikipedia and the New York Times portions of Gigaword (Parker et al., 2011).",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 1,
      "context" : "2 Ontologies Our canonical concepts and their representative strings are sourced from two ontologies: the Unified Medical Language System (UMLS) (Bodenreider, 2004), and YAGO (Suchanek et al.",
      "startOffset" : 145,
      "endOffset" : 164
    }, {
      "referenceID" : 41,
      "context" : "2 Ontologies Our canonical concepts and their representative strings are sourced from two ontologies: the Unified Medical Language System (UMLS) (Bodenreider, 2004), and YAGO (Suchanek et al., 2007; Mahdisoltani et al., 2015).",
      "startOffset" : 175,
      "endOffset" : 225
    }, {
      "referenceID" : 26,
      "context" : "2 Ontologies Our canonical concepts and their representative strings are sourced from two ontologies: the Unified Medical Language System (UMLS) (Bodenreider, 2004), and YAGO (Suchanek et al., 2007; Mahdisoltani et al., 2015).",
      "startOffset" : 175,
      "endOffset" : 225
    }, {
      "referenceID" : 13,
      "context" : "YAGO is a knowledge base encoding information about real-world entities and the relationships between them, including information from WordNet (Fellbaum, 1998) and Wikipedia.",
      "startOffset" : 143,
      "endOffset" : 159
    }, {
      "referenceID" : 38,
      "context" : "1 Biomedical similarity and relatedness To evaluate our biomedical concepts, we used the MayoSRS and Mini Mayo SRS datasets from (Pedersen et al., 2007) and the UMNSRS Similarity and Relatedness datasets of (Pakhomov et al.",
      "startOffset" : 129,
      "endOffset" : 152
    }, {
      "referenceID" : 35,
      "context" : ", 2007) and the UMNSRS Similarity and Relatedness datasets of (Pakhomov et al., 2010).",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : "2 General-domain similarity and relatedness While there are well-studied datasets of word similarity and relatedness available, such as WordSim-353 (Finkelstein et al., 2001), SimLex999 (Hill et al.",
      "startOffset" : 148,
      "endOffset" : 174
    }, {
      "referenceID" : 19,
      "context" : ", 2001), SimLex999 (Hill et al., 2015), as well as datasets for evaluating compositionality (Marelli et al.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : ", 2015), as well as datasets for evaluating compositionality (Marelli et al., 2014) and typed relations between nouns (Baroni and Lenci, 2011), we are not aware of comparable similarity or relatedness datasets for real-world entities.",
      "startOffset" : 61,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : ", 2014) and typed relations between nouns (Baroni and Lenci, 2011), we are not aware of comparable similarity or relatedness datasets for real-world entities.",
      "startOffset" : 42,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "• CHOI Embeddings of 15k UMLS CUIs trained via hierarchical sampling over sequences of manually annotated claims codes from EHR data (Choi et al., 2016b).",
      "startOffset" : 133,
      "endOffset" : 153
    }, {
      "referenceID" : 28,
      "context" : "• MENCIA Embeddings of 26k MESH headings trained over manually-tagged PubMed abstracts to maximize similarity of headings to relevant documents (Mencia et al., 2016).",
      "startOffset" : 144,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "• CHIU-2 Word embeddings trained on PubMed abstracts with a context window size of 2 (Chiu et al., 2016a).",
      "startOffset" : 85,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "• CHIU-30 Word embeddings trained on PubMed abstracts with a context window size of 30 (Chiu et al., 2016a).",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 23,
      "context" : "2 Hyperparameter tuning Recent findings highlight the importance of hyperparameter selection for performance of distributed representations on downstream tasks (Levy et al., 2015; Chiu et al., 2016a).",
      "startOffset" : 160,
      "endOffset" : 199
    }, {
      "referenceID" : 5,
      "context" : "2 Hyperparameter tuning Recent findings highlight the importance of hyperparameter selection for performance of distributed representations on downstream tasks (Levy et al., 2015; Chiu et al., 2016a).",
      "startOffset" : 160,
      "endOffset" : 199
    }, {
      "referenceID" : 5,
      "context" : "2 In (Chiu et al., 2016a) point out that in the biomedical domain, hyperparameter settings that improve performance on intrinsic evaluations of the vector space may degrade perfor-",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 7,
      "context" : "We follow Choi et al. (2016b) in selecting 6 common semantic types to evaluate: disease or syndrome, pharmacologic substance, neoplastic process, finding, injury and poisoning, and sign or symptom (this replaces clinical drug, as not all concept embedding sets contained multiple concepts of that type).",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "We follow Choi et al. (2016b) in selecting 6 common semantic types to evaluate: disease or syndrome, pharmacologic substance, neoplastic process, finding, injury and poisoning, and sign or symptom (this replaces clinical drug, as not all concept embedding sets contained multiple concepts of that type). We evaluated our three biomedical concept embeddings, and compared against the four medical concept embeddings from prior work; the results are shown in Table 6. The medical code embeddings of Choi et al. (2016b) demonstrate superior clustering in aggregate,3 but clustering performance is mixed among the semantic types, with our embeddings clustering competitively with prior work.",
      "startOffset" : 10,
      "endOffset" : 517
    }, {
      "referenceID" : 7,
      "context" : "Our results comparing APPROXWORDco to APPROXWORDind align with prior work indicating that including structured knowledge about concepts in training word embeddings improves Choi et al. (2016b) propose another embedding technique that has superior MAP (0.",
      "startOffset" : 173,
      "endOffset" : 193
    }, {
      "referenceID" : 43,
      "context" : "their performance in some semantic tasks (Wang et al., 2014a; Faruqui et al., 2015).",
      "startOffset" : 41,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "their performance in some semantic tasks (Wang et al., 2014a; Faruqui et al., 2015).",
      "startOffset" : 41,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "We demonstrate that our method is able to model both kinds of information, but it would benefit from an application to more abstract data in the general domain, similar to previous work with BabelNet (Camacho-Collados et al., 2015).",
      "startOffset" : 200,
      "endOffset" : 231
    }, {
      "referenceID" : 8,
      "context" : ", 2014a; Faruqui et al., 2015). However, the extreme similarity observed in our compositionality analysis, and the high performance of concept approximations, suggests that some non-lexical information about concepts is not being captured in our method. Leveraging an approach similar to Wang et al. (2014a) and directly including ontology structure as a training criterion may be one way to model some of this additional information.",
      "startOffset" : 9,
      "endOffset" : 308
    }, {
      "referenceID" : 4,
      "context" : "Chiu et al. (2016b,a); Faruqui et al. (2016) describe several issues with evaluating linguistic embeddings with similarity and relatedness tasks, and poor generalization of results to downstream applications.",
      "startOffset" : 0,
      "endOffset" : 45
    } ],
    "year" : 2017,
    "abstractText" : "Recent work on embedding ontology concepts has relied on either expensive manual annotation or automated concept tagging methods that ignore the textual contexts around concepts. We propose a novel method for jointly learning concept, phrase, and word embeddings from an unlabeled text corpus, by using the representative phrases for ontology concepts as distant supervision. We learn embeddings for medical concepts in the Unified Medical Language System and generaldomain concepts in YAGO, using a variety of corpora. Our embeddings show performance competitive with existing methods on concept similarity and relatedness tasks, while requiring no human corpus annotation and demonstrating more than 3x coverage in the vocabulary size.",
    "creator" : "TeX"
  }
}