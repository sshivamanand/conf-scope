{
  "name" : "331.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Connecting the dots: Summarizing and Structuring Large Document Collections Using Concept Maps",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Multi-document summarization (MDS), the transformation of a set of documents into a short text containing their most important aspects, is a longstudied problem in NLP. Generated summaries have been shown to support humans dealing with large document collections in information seeking tasks (McKeown et al., 2005; Maña-López et al., 2004; Roussinov and Chen, 2001). However, when exploring a set of documents manually, humans rarely write a fully-formulated summary for themselves. Instead, user studies (Chin et al., 2009; Kang et al., 2011) show that they note down important keywords and phrases, try to identify relationships between them and organize them accordingly. Therefore, we believe that the study of summarization with similarly structured outputs is an important extension of the traditional task.\nACL 2017\nlong papers\nsubstantial research invites submissions of must describe\nConcept Relation Proposition\nFigure 1: Elements of a concept map.\nA representation more in line with observed user behavior is a concept map (Novak and Gowin, 1984). Concept maps are labeled graphs showing concepts as nodes and relationships between them as edges (Figure 1). Labels can be freely defined. A concept can be an entity, abstract idea, event or activity, designated by a unique label. Good maps should be propositionally coherent, meaning that every relation together with the two connected concepts form a meaningful proposition.\nIntroduced in 1972 as a teaching tool (Novak and Cañas, 2007), applications in education (Edwards and Fraser, 1983; Roy, 2008), for writing assistance (Villalon, 2012) or to structure information repositories (Briggs et al., 2004; Richardson and Fox, 2005) have been reported. For summarization, concept maps allow to represent a summary concisely and clearly reveal relations. Moreover, we see a second interesting use case beyond the capabilities of textual summaries: When concepts and relations are linked to corresponding locations in the documents, the graph can be used to navigate in a document collection, similar to how a table-of-contents is used in a single document.\nThe corresponding task that we propose is concept-map-based MDS: Given a set of related documents, create a concept map that represents its most important content, satisfies a specified size limit and is connected. Further, to ease the use of automatic evaluations, we focus on the extractive variant of the task in this work, requiring that all labels have to be taken from the documents.\nThe proposed task is complex, consisting of several interdependent subtasks. One has to ex-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nstudents\nstudent loans\ncost of their college education\na credit-worthy cosigner sufficient credit history a steady income\nprivate lending institutions\nfederal student loans\ncredit check U.S. Department\nof Schooling\nare offered to\ndo not have\nrequire a borrower to have\nregularly offer\npay for\nhelp students cover\nwill need to apply with\nnot requiring\nprovide a significant financial aid to\noffered by\nFigure 2: Excerpt from a summary concept map on the topic “students loans without credit history”.\ntract appropriate labels for concepts and relations and recognize different expressions that refer to the same concept across multiple documents. Further, one has to select the most important concepts and relations for the summary and finally organize them in a graph satisfying the connectedness and size constraints. In order to develop and evaluate methods for the task, gold-standard corpora are necessary, but no suitable corpus is available. The manual creation of such a dataset is very timeconsuming, as annotators need to perform all subtasks described above. In particular, an annotator would need to manually identify all potential concepts in the documents, while only a few of them will eventually end up in the summary.\nTo overcome these issues, we developed a corpus creation method that effectively combines automatic preprocessing, scalable crowdsourcing and high-quality expert annotations. Using it, we can avoid the high effort for single annotators, allowing us to scale to document clusters that are 15 times larger than in traditional summarization corpora. In addition, the method includes a novel crowdsourcing scheme, low-context importance annotation, that resolves quality issues observed in previous work (Lloret et al., 2013).\nFor concept-map-based MDS, we created a new corpus of 30 topics, each with around 40 source documents and a summarizing concept map that is the consensus of many crowdworkers. The document clusters were built from a web crawl, comprising a variety of different genres, text styles and document lengths. Figure 2 shows one example.\nTo summarize, we make the following contributions: (1) We propose a novel summarization task, concept-map-based MDS, (2) present a new crowdsourcing scheme to create reference summaries, (3) publish a new dataset for the proposed task and (4) provide an evaluation protocol and baseline. These resources are publicly available\nunder a permissive license. 1"
    }, {
      "heading" : "2 Related Work",
      "text" : "Some attempts have been made to automatically construct concept maps from text, working with either single documents (Zubrinic et al., 2015; Villalon, 2012; Valerio and Leake, 2006; Kowata et al., 2010) or document clusters (Qasim et al., 2013; Zouaq and Nkambou, 2009; Rajaraman and Tan, 2002). These approaches extract concept and relation labels from syntactic structures and connect them to build a concept map. However, common task definitions and comparable evaluations are missing. In addition, only a few of them, namely Villalon (2012) and Valerio and Leake (2006), define summarization as their goal and try to compress the input to a substantially smaller size. Our newly proposed task and the created large-cluster dataset fill these gaps as they emphasize the summarization aspect of the task.\nFor the subtask of selecting summary-worthy concepts and relations, techniques developed for traditional summarization (Nenkova and McKeown, 2011) and keyphrase extraction (Hasan and Ng, 2014) are related and applicable. Approaches that build graphs of propositions to create a summary (Fang et al., 2016; Li et al., 2016; Liu et al., 2015; Li, 2015) seem to be particularly related, however, there is one important difference: While they use graphs as an intermediate representation from which a textual summary is then generated, the goal of the proposed task is to create a graph that is directly interpretable and useful for a user.\nFor traditional summarization, the most wellknown datasets emerged out of the DUC and TAC competitions.2 They provide clusters of news articles with gold-standard summaries that serve as a benchmark to compare different approaches.\n1URL hidden, anon. resources are attached for review. 2duc.nist.gov, tac.nist.gov\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nImagine you want to learn something about students loans without credit history. How useful would the following statements be for you?\n(P1) students with bad credit history - apply for - federal loans with the FAFSA 2 Extremely Important 2 Very Important 2 Moderately Important 2 Slightly Important 2 Not at all Important\n(P2) students - encounter - unforeseen financial emergencies 2 Extremely Important 2 Very Important 2 Moderately Important 2 Slightly Important 2 Not at all Important\nFigure 3: Likert-scale crowdsourcing task with topic description and two example propositions.\nExtending these efforts, several more specialized corpora have been created: With regard to size, Nakano et al. (2010) present a corpus of summaries for large-scale collections of web pages. Recently, corpora with more heterogeneous documents have been suggested, e.g. (Zopf et al., 2016) and (Benikova et al., 2016). The corpus we present combines these aspects, as it has large clusters of heterogeneous documents, and provides a necessary benchmark to evaluate the proposed task. For concept map generation, one corpus with humancreated summary concept maps for student essays has been created (Villalon et al., 2010). In contrast to our corpus, it only deals with single documents, requires a two orders of magnitude smaller amount of compression and is not publicly available .\nOther types of information representation that also model concepts and their relationships are knowledge bases, such as Freebase (Bollacker et al., 2009), and ontologies. However, they both differ in important aspects: Whereas concept maps follow an open label paradigm and are meant to be interpretable by humans, knowledge bases and ontologies are usually more strictly typed and made to be machine-readable. Moreover, approaches to automatically construct them from text typically try to extract as much information as possible, while we want to summarize a document. As a result, approaches from these fields cannot be directly applied. Vice versa, Zouaq et al. (2011) showed that concept maps can be a useful first step to create a domain ontology."
    }, {
      "heading" : "3 Low-Context Importance Annotation",
      "text" : "Lloret et al. (2013) describe several experiments to crowdsource reference summaries. Workers are asked to read 10 documents and then select 10 summary sentences from them for a reward of $0.05. They discovered several challenges, including poor work quality and the subjectiveness of the annotation task, indicating that crowdsourcing is not useful for this purpose. To overcome these is-\nsues, we introduce a new task design, low-context importance annotation, to determine summaryworthy parts of documents. Compared to Lloret et al.’s approach, it is more in line with crowdsourcing best practices, as the tasks are simple, intuitive and small (Sabou et al., 2014) and workers receive reasonable payment (Fort et al., 2011)."
    }, {
      "heading" : "3.1 Task Design",
      "text" : "We break the importance annotation down to single propositions. The goal of our crowdsourcing scheme is to obtain a score for each proposition indicating its importance in a document cluster, such that a ranking according to the score would reveal what is most important and should be included in a summary. In contrast to other work, we do not show the documents to the workers at all, but provide only a description of the document cluster’s topic. This ensures that tasks are small, simple and can be done quickly (see Figure 3). In preliminary tests, we found that this design, despite the minimal context, works reasonably well as long as the topic is something the worker can relate to. For instance, consider Figure 3: One can easily say that P1 is more important than P2 without reading the documents. We distinguish two task variants:\nLikert-scale Tasks Instead of enforcing binary importance decisions, we use a 5-point Likertscale to allow more fine-grained annotations. The obtained labels are translated into scores (5..1) and the average of all scores for a proposition is used as an estimate for its importance. This follows the idea that while single workers might find the task subjective, the consensus of multiple workers, represented in the average score, tends to be less subjective due to the “wisdom of the crowd”. We randomly group five propositions into a task.\nComparison Tasks As an alternative, we use a second task design based on pairwise comparisons. Comparisons are known to be easier to make and more consistent (Belz and Kow, 2010),\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nbut also more expensive, as the number of pairs grows quadratically with the number of objects.3 To reduce the cost, we group five propositions into a task and ask workers to order them by importance per drag-and-drop. From the results, we derive pairwise comparisons and use TrueSkill (Herbrich et al., 2007), a powerful Bayesian rank induction model (Zhang et al., 2016), to obtain importance estimates for each proposition."
    }, {
      "heading" : "3.2 Pilot Study",
      "text" : "To verify the proposed approach, we conducted a pilot study on Amazon Mechanical Turk using data from TAC2008 (Dang and Owczarzak, 2008). We collected importance estimates for 474 propositions extracted from the first three clusters4 using both task designs. Each Likert-scale task was assigned to 5 different workers and awarded $0.06. For comparison tasks, we also collected 5 labels each, paid $0.05 and sampled around 7% of all possible pairs. We submitted them in batches of 100 pairs and selected pairs for subsequent batches based on the confidence of the TrueSkill model.\nQuality Control Following the observations of Lloret et al. (2013), we established several measures for quality control. First, we restricted our tasks to workers from the US with an approval rate of at least 95%. Second, we identified low quality workers by measuring the correlation of each worker’s Likert-scores with the average of the other four scores. The worst workers (at most 5% of all labels) were removed. In addition, we included trap sentences, similar as in (Lloret et al., 2013), in around 80 of the tasks. In contrast to Lloret et al.’s findings, both an obvious trap sentence (This sentence is not important) and a less obvious but unimportant one (Barack Obama graduated from Harvard Law) were consistently labeled as unimportant (1.08 and 1.14), indicating that the workers paid attention to the task.\nAgreement and Reliability For Likert-scale tasks, we follow Snow et al. (2008) and calculate agreement as the average Pearson correlation of a worker’s Likert-score with the average score of the remaining workers.5 This measure is less\n3Even with intelligent sampling strategies, such as the active learning in CrowdBT (Chen et al., 2013), the number of pairs is only reduced by a constant factor (Zhang et al., 2016).\n4D0801A-A, D0802A-A, D0803A-A 5As workers are not consistent across all items, we create\nfive meta-workers by sorting the labels per proposition.\nstrict than exact label agreement and can account for close labels and high- or low-scoring workers. We observe a correlation of 0.81, indicating substantial agreement. For comparisons, the majority agreement is 0.73. To further examine the reliability of the collected data, we followed the approach of Kiritchenko and Mohammed (2016) and simply repeated the crowdsourcing for one of the three topics. Between the importance estimates calculated from the first and second run, we found a Pearson correlation of 0.82 (Spearman 0.78) for Likert-scale tasks and 0.69 (Spearman 0.66) for comparison tasks. This shows that the approach, despite the high subjectiveness of the task, allows us to collect reliable annotations.\nPeer Evaluation In addition to the reliability studies, we extrinsically evaluated the annotations in the task of summary evaluation. For each of the 58 peer summaries in TAC2008, we calculated a score as the sum of the importance estimates of the propositions it contains. Table 1 shows how these peer scores, averaged over the three topics, correlate with the manual responsiveness scores assigned during TAC in comparison to ROUGE-2 and Pyramid scores.6 The results demonstrate that with both task designs, we obtain importance annotations that are similarly useful for summary evaluation as pyramid annotations or gold-standard summaries (used for ROUGE).\nConclusion Based on the pilot study results, we conclude that the proposed crowdsourcing scheme allows us to obtain proper annotations for the importance of propositions. Thus, the approach can be used to construct gold-standard summaries, evaluate systems or generate training data."
    }, {
      "heading" : "4 Corpus Creation",
      "text" : "To carry out comparable evaluations, the proposed task requires a gold-standard corpus. We\n6Correlations for ROUGE and Pyramid are lower than reported in TAC since we only use 3 topics instead of all 48.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nProposition Extraction\nProposition Filtering\nImportance Annotation\nProposition Revision\nConcept Map Construction\n§4.2 §4.3 §4.4 §4.5 §4.6\nFigure 4: Steps of the corpus creation (with references to the corresponding sections).\npresent our corpus construction process, outlined in Figure 4, that combines automatic preprocessing, scalable crowdsourcing and high-quality expert annotations to efficiently create such a dataset."
    }, {
      "heading" : "4.1 Source Data",
      "text" : "As a starting point, we used the DIP corpus (Habernal et al., 2016), a collection of 49 clusters of 100 web pages on educational topics (e.g. bullying, homeschooling, drugs). It was created from a large web crawl using state-of-the-art information retrieval. Web pages are boilerplate-cleaned, segmented and all sentences have binary labels for topic relevance. For our corpus, we selected 30 of the topics. From the list of 100 ranked web pages per topic, we choose the top-ranked ones that were from distinct websites and had at least one relevant sentence, stopping at 50 documents."
    }, {
      "heading" : "4.2 Proposition Extraction",
      "text" : "As concept maps consist of propositions expressing the relation between concepts (see Figure 1), we need to impose such a structure upon the plain text in the document clusters. This could be done by manually annotating spans representing concepts and relations in every sentence, however, the size of our clusters makes this a huge effort: Even if we restrict the annotation to sentences known to be relevant, 2288 sentences per topic (69k in total) need to be processed. Therefore, we resort to an automatic approach for this step.\nThe Open Information Extraction paradigm (Banko et al., 2007) offers a representation very similar to the desired one. For instance, from\nSpecifically, eligible students may borrow the Direct Stafford Loans and eligible parents may borrow the Direct PLUS Loan on behalf of their dependent students.\nOpen IE systems extract tuples of two arguments and a relation phrase representing propositions:\n(eligible students, may borrow, Dir. Stafford Loan) (eligible parents, may borrow, Direct PLUS Loan)\nWhile the relation phrase is similar to a relation\nin a concept map, many arguments in these tuples represent useful concepts. We used Open IE 47, a state-of-the-art Open IE system (Stanovsky and Dagan, 2016), to process the relevant sentences of all topics. After removing duplicates, we obtained 4137 tuples per topic.\nSince we want to create a gold-standard corpus, we have to ensure that we produce high-quality data. We therefore made use of the confidence assigned to every extracted tuple to filter out low quality ones. To ensure that we do not filter too aggressively (and miss important aspects in the final summary), we manually annotated 500 tuples sampled from all topics for correctness. On the first 250 of them, we tuned the filter threshold to 0.5, which keeps 98.7% of the correct extractions in the unseen second half. After filtering, a topic had on average 2850 propositions (85k in total)."
    }, {
      "heading" : "4.3 Proposition Filtering",
      "text" : "Despite the similarity of the Open IE paradigm, not every extracted tuple is a suitable proposition for a concept map. To reduce the effort in the subsequent steps, we therefore want to filter out unsuitable ones. A tuple is suitable if it (1) is a correct extraction, (2) is meaningful without any context and (3) has arguments that represent proper concepts. To better understand these criteria, we performed a small annotation study.\nBased on a set of examples, we created a guideline explaining when to label a tuple as suitable for a concept map. We found tuples to be unsuitable mostly because they had unresolvable pronouns, conflicting with (2), or arguments that were full clauses or propositions, conflicting with (3), while (1) was mostly taken care of by the confidence filtering in §4.2. Three annotators independently labeled a set of 500 randomly sampled tuples. We observed an agreement of 82% (κ = 0.60).\nDue to the high number of tuples we decided to automate the filtering step. We trained a linear SVM on the majority voted annotations. As features, we used the extraction confidence, length of arguments and relations as well as part-of-speech\n7github.com/knowitall/openie\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\ntags, among others. To ensure that the automatic classification does not remove suitable propositions, we tuned the classifier to avoid false negatives. In particular, we introduced class weights, improving precision on the negative class at the cost of a higher fraction of positive classifications. Further, we assumed that we can manually verify a certain number of the most uncertain negative classifications. Under these constraints, a model trained on 350 instances achieves 93% precision on negative classifications on the unseen 150 instances, assuming that 20% are manually verified. We found this to be a reasonable trade-off of automation and data quality and applied the model to the full dataset.\nThe classifier filtered out 43% of the propositions, leaving 1622 per topic. We manually examined the 17k least confident negative classifications and corrected 955 of them. We also corrected positive classifications for certain types of tuples for which we knew the classifier to be imprecise. Finally, each topic was left with an average of 1554 propositions (47k in total)."
    }, {
      "heading" : "4.4 Importance Annotation",
      "text" : "Given the propositions identified in the previous step, we now applied our crowdsourcing scheme as described in §3 to determine their importance. To cope with the large number of propositions, we combine the two task designs: First, we collect 5 Likert-scores for each proposition, clean the data and calculate averages. Then, using only the top 100 propositions8, we crowdsource 10% of the pairwise comparisons among them. Using TrueSkill, we obtain a fine-grained ranking of the 100 most important propositions. We use the topic descriptions from the underlying DIP corpus.\nFor Likert-scores, the average agreement over all topics is 0.80, while the majority agreement for comparisons is 0.78. We repeated the data collection for three randomly selected topics and found the Pearson correlation between both runs to be 0.73 (Spearman 0.73) for Likert-scores and 0.72 (Spearman 0.71) for comparisons. These figures show that the crowdsourcing approach works on this dataset as reliably as on the TAC documents.\nIn total, we uploaded 53k scoring and 12k comparison tasks to Mechanical Turk, spending $4425.45 including fees. From the fine-grained\n8We also add all propositions with the same score as the 100th, yielding 112 propositions on average.\nranking of the 100 most important propositions, we select the top 50 per topic to construct a summary concept map in the subsequent steps."
    }, {
      "heading" : "4.5 Proposition Revision",
      "text" : "Having a manageable number of propositions per topic, an expert annotator then applied a few straightforward transformations that correct common errors of the Open IE system. First, we break down propositions with conjunctions in either of the arguments into separate propositions per conjunct, which the Open IE system sometimes fails to do. And second, we correct span errors that might occur in the arguments or relation phrase, especially when sentences were not properly segmented. As a result, we have a set of high quality propositions for our concept map, consisting of, due to the first transformation, 56.1 propositions per topic on average."
    }, {
      "heading" : "4.6 Concept Map Construction",
      "text" : "In this final step, we connect the set of important propositions to form a graph. For instance, given the following two propositions\n(student, may borrow, Stafford Loan) (the student, does not have, a credit history)\none can easily see, although the first arguments differ slightly, that both labels describe the concept student, allowing us to build a concept map with three concepts. The annotation task thus involves deciding which of the available propositions to include in the map, which of their concepts to join and which of the available labels to use. As these decisions highly depend upon each other and require context, we decided to use expert annotators rather than crowdsource the subtasks.\nAnnotators were given the topic description and the most important, ranked propositions. They could connect them step by step in a simple annotation tool that visualized the map constructed so far. They were instructed to reach a size of 25 concepts, the recommended maximum size for a concept map (Novak and Cañas, 2007). Further, they should prefer more important propositions and ensure connectedness. When connecting two propositions, they were asked to keep the concept label that was appropriate for both propositions. To support the annotator, the tool used ADW (Pilehvar et al., 2013), a state-of-the-art approach for semantic similarity, to suggest possible connections. If\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nCorpus Cluster Cluster Size Docs Doc. Size Rel. Std. This work 30 97,880 ± 50,086.2 40.5 ± 6.8 2,412.8 ± 3,764.1 1.56 DUC 2006 50 17,461 ± 6,627.8 25.0 ± 0.0 729.2 ± 542.3 0.74 DUC 2004 50 6,721 ± 3,017.9 10.0 ± 0.0 672.1 ± 506.3 0.75 TAC 2008A 48 5,892 ± 2,832.4 10.0 ± 0.0 589.2 ± 480.3 0.82\nTable 2: Topic clusters in comparison to classic corpora (size in token, mean with standard deviation).\nan annotator was not able to connect 25 concepts, she was allowed to create up to three synthetic relations with freely defined labels, making the maps slightly abstractive. On average, the constructed maps have 0.77 synthetic relations, mostly connecting concepts whose relation is too obvious to be explicitly stated in text (e.g. between Montessori teacher and Montessori education).\nTo assess the reliability of this annotation step, we had the first three maps created by two annotators. We casted the task of selecting propositions to be included in the map as a binary decision task and observed an agreement of 84% (κ = 0.66). Second, we modeled the decision which concepts to join as a binary decision on all pairs of common concepts, observing an agreement of 95% (κ = 0.70). And finally, we compared which concept labels the annotators decided to include in the final map, observing 85% (κ = 0.69) agreement. Hence, the annotation shows substantial agreement (Landis and Koch, 1977)."
    }, {
      "heading" : "5 Corpus Analysis",
      "text" : "In this section, we describe our newly created corpus, which, in addition to having summaries in the form of concept maps, differs from traditional summarization corpora in several aspects."
    }, {
      "heading" : "5.1 Document Clusters",
      "text" : "Size The corpus consists of document clusters for 30 different topics. Each of them contains around 40 documents with on average 2413 tokens, which leads to an average cluster size of 97,880 token. With these characteristics, the document clusters are 15 times larger than typical DUC clusters of ten documents and five times larger than the 25-document-clusters (Table 2). In addition, the documents are also more variable in terms of length, as the (length-adjusted) standard deviation is twice as high as in the other corpora.\nGenres Because we used a large web crawl as the source for our corpus, it contains docu-\nper Map Token Character Concepts 25.0± 0.0 3.2± 0.5 22.0± 4.1 Relations 25.2± 1.3 3.2± 0.5 17.1± 2.6\nTable 3: Size of concept maps (mean with std).\nments from a variety of genres. To further analyze this property, we categorized a sample of 50 documents from the corpus. Among them, we found professionally written articles and blog posts (28%), educational material for parents and kids (26%), personal blog posts (16%), forum discussions and comments (12%), commented link collections (12%) and scientific articles (6%). Systems summarizing the corpus can therefore not rely on genre-specific characteristics and have to deal with the full variety of text types.\nTextual Heterogeneity In addition to the variety of genres, the documents also differ in terms of language use. To capture this property, we follow Zopf et al. (2016) and compute, for every topic, the average Jensen-Shannon divergence between the word distribution of one document and the word distribution in the remaining documents. The higher this value is, the more the language differs between documents. We found the average divergence over all topics to be 0.3490, whereas it is 0.3019 in DUC 2004 and 0.3188 in TAC 2008A."
    }, {
      "heading" : "5.2 Concept Maps",
      "text" : "As Table 3 shows, each of the 30 reference concept maps has exactly 25 concepts and between 24 and 28 relations. Labels for both concepts and relations consist on average of 3.2 tokens, whereas the latter are a bit shorter in characters.\nTo obtain a better picture of what kind of text spans have been used as labels, we automatically tagged them with their part-of-speech and determined their head with a dependency parser. Concept labels tend to be headed by nouns (82%) or verbs (15%), while they also contain adjectives, prepositions and determiners. Relation labels, on\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nthe other hand, are almost always headed by a verb (94%) and contain prepositions, nouns and particles in addition. These distributions are very similar to those reported by Villalon et al. (2010) for their (single-document) concept map corpus.\nAnalyzing the graph structure of the maps, we found that all of them are connected. They have on average 7.2 central concepts with more than one relation, while the remaining ones occur in only one proposition. During the annotation, we found that achieving a higher number of connections would mean compromising importance, i.e. including less important propositions, and we decided against it. For experiments, we provide a split into 15 topics each for training and testing."
    }, {
      "heading" : "6 Baseline Experiments",
      "text" : "In this section, we briefly describe a baseline and evaluation scripts that we release, with a detailed documentation, along with the corpus.\nBaseline Method We implemented a simple approach inspired by previous work on concept map generation and keyphrase extraction. For a document cluster, it performs the following steps:\n1. Extract all NPs as potential concepts.\n2. Merge potential concepts whose labels match after stemming into a single concept.\n3. For each pair of concepts co-occurring in a sentence, select the tokens in between as a potential relation if they contain a verb.\n4. If a pair of concepts has more than one relation, select the one with the shortest label.\n5. Rank all concepts by importance.\n6. Connect concepts to a graph of 25 concepts.\nFor (5), we trained a binary classifier to identify the important concepts in the set of all potential concepts. We used common features for keyphrase extraction, including position, frequency and length, and Weka’s Random Forest (Hall et al., 2009). At inference time, the classifier provides an importance score for each concept. In (6), we start with the full graph of all extracted concepts and relations and use a heuristic to find a subgraph that is connected, satisfies the size limit of 25 concepts and has many high-scoring concepts: We iteratively remove the weakest concept until only one connected component of 25 concepts or less remains, which is the final concept map.\nEvaluation Metrics In order to automatically compare generated concept maps with reference maps, we propose three metrics.9 As a concept map is fully defined by the set of its propositions, we can compute precision, recall and F1scores between the two sets, with propositions represented as the concatenation of concept and relation labels. Strict Match compares them after stemming and only counts complete matches. Using METEOR (Denkowski and Lavie, 2014), we offer a second metric that takes synonyms and paraphrases into account and also scores partial matches. And finally, we compute ROUGE-2 (Lin, 2004) between the concatenation of all propositions from the maps. These automatic measures might be complemented with a human evaluation.\nResults Table 4 shows the performance of the baseline. An analysis of the single pipeline steps revealed major bottlenecks of the method and challenges of the task. First, we observed that around 76% of gold concepts are covered with the extraction (step 1+2), while the top 25 concepts (step 5) only contain 17%. Hence, content selection is a major challenge, stemming from the large cluster sizes. Second, while also 17% of gold concepts are contained in the final maps (step 6), scores for strict proposition matching are low, indicating a poor performance of the relation extraction (step 3). The propagation of these errors along the pipeline contributes to overall low scores."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we presented low-context importance annotation, a novel crowdsourcing scheme that we used to create a new benchmark corpus for concept-map-based MDS. The corpus has largescale document clusters of heterogeneous web documents, posing a challenging summarization task. Together with the corpus, we provide implementations of a baseline method and evaluation scripts and hope that our efforts facilitate future research on this variant of summarization.\n9For exact definitions, see the published scripts and docs.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Open Information Extraction from the Web",
      "author" : [ "Michele Banko", "Michael J. Cafarella", "Stephen Soderland", "Matt Broadhead", "Oren Etzioni." ],
      "venue" : "Proceedings of the 20th International Joint Conference on Artifical Intelligence. Hyderabad, India, pages",
      "citeRegEx" : "Banko et al\\.,? 2007",
      "shortCiteRegEx" : "Banko et al\\.",
      "year" : 2007
    }, {
      "title" : "Comparing Rating Scales and Preference Judgements in Language Evaluation",
      "author" : [ "Anja Belz", "Eric Kow." ],
      "venue" : "Proceedings of the 6th International Natural Language Generation Conference. Trim, Co. Meath, Ireland, pages 7–16.",
      "citeRegEx" : "Belz and Kow.,? 2010",
      "shortCiteRegEx" : "Belz and Kow.",
      "year" : 2010
    }, {
      "title" : "Bridging the gap between extractive and abstractive summaries: Creation and evaluation of coherent extracts from heterogeneous sources",
      "author" : [ "Darina Benikova", "Margot Mieskes", "Christian M. Meyer", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 26th In-",
      "citeRegEx" : "Benikova et al\\.,? 2016",
      "shortCiteRegEx" : "Benikova et al\\.",
      "year" : 2016
    }, {
      "title" : "Freebase",
      "author" : [ "Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor." ],
      "venue" : "Compilation Proceedings of the International Conference on Management Data & 27th Symposium on Principles of Database",
      "citeRegEx" : "Bollacker et al\\.,? 2009",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2009
    }, {
      "title" : "Concept Maps Applied to Mars Exploration Public Outreach",
      "author" : [ "Geoffrey Briggs", "David A. Shamma", "Alberto J. Cañas", "Roger Carff", "Jeffrey Scargle", "Joseph D. Novak." ],
      "venue" : "Concept Maps: Theory, Methodology, Technology. Proceedings of the",
      "citeRegEx" : "Briggs et al\\.,? 2004",
      "shortCiteRegEx" : "Briggs et al\\.",
      "year" : 2004
    }, {
      "title" : "Pairwise Ranking Aggregation in a Crowdsourced Setting",
      "author" : [ "Xi Chen", "Paul N. Bennett", "Kevyn Collins-Thompson", "Eric Horvitz." ],
      "venue" : "Proceedings of the Sixth ACM International Conference on Web Search and Data Mining. Rome, Italy, pages 193–",
      "citeRegEx" : "Chen et al\\.,? 2013",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2013
    }, {
      "title" : "Exploring the analytical processes of intelligence analysts",
      "author" : [ "George Chin", "Olga A. Kuchar", "Katherine E. Wolf." ],
      "venue" : "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. Boston, MA, USA, pages 11–20.",
      "citeRegEx" : "Chin et al\\.,? 2009",
      "shortCiteRegEx" : "Chin et al\\.",
      "year" : 2009
    }, {
      "title" : "Overview of the TAC 2008 Update Summarization Task",
      "author" : [ "Hoa Trang Dang", "Karolina Owczarzak." ],
      "venue" : "Proceedings of the First Text Analysis Conference. Gaithersburg, MD, USA, pages 1–16.",
      "citeRegEx" : "Dang and Owczarzak.,? 2008",
      "shortCiteRegEx" : "Dang and Owczarzak.",
      "year" : 2008
    }, {
      "title" : "Meteor Universal: Language Specific Translation Evaluation for Any Target Language",
      "author" : [ "Michael Denkowski", "Alon Lavie." ],
      "venue" : "Proceedings of the Ninth Workshop on Statistical Machine Translation. Baltimore, Maryland, USA, pages 376–380.",
      "citeRegEx" : "Denkowski and Lavie.,? 2014",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2014
    }, {
      "title" : "2016. A Proposition",
      "author" : [ "Kuhnle", "Simone Teufel" ],
      "venue" : null,
      "citeRegEx" : "Kuhnle and Teufel.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kuhnle and Teufel.",
      "year" : 2016
    }, {
      "title" : "Concept Maps Core Ele",
      "author" : [ "dia Silva Boeres" ],
      "venue" : null,
      "citeRegEx" : "Boeres.,? \\Q2010\\E",
      "shortCiteRegEx" : "Boeres.",
      "year" : 2010
    }, {
      "title" : "The Measurement of Observer Agreement for Categorical Data",
      "author" : [ "J. Richard Landis", "Gary G. Koch." ],
      "venue" : "Biometrics 33(1):159–174. https://doi.org/10.2307/2529310.",
      "citeRegEx" : "Landis and Koch.,? 1977",
      "shortCiteRegEx" : "Landis and Koch.",
      "year" : 1977
    }, {
      "title" : "Abstractive Multi-document Summarization with Semantic Information Extraction",
      "author" : [ "Wei Li." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, pages 1908–1913.",
      "citeRegEx" : "Li.,? 2015",
      "shortCiteRegEx" : "Li.",
      "year" : 2015
    }, {
      "title" : "Abstractive News Summarization based on Event Semantic Link Network",
      "author" : [ "Wei Li", "Lei He", "Hai Zhuge." ],
      "venue" : "Proceedings of the 26th International Conference on Computational Linguistics (COLING). Osaka, Japan, pages 236–246.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "ROUGE: A Package for Automatic Evaluation of Summaries",
      "author" : [ "Chin-Yew Lin." ],
      "venue" : "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop. pages 74–81. http://aclweb.org/anthology/W04-1013.",
      "citeRegEx" : "Lin.,? 2004",
      "shortCiteRegEx" : "Lin.",
      "year" : 2004
    }, {
      "title" : "Toward Abstractive Summarization Using Semantic Representations",
      "author" : [ "Fei Liu", "Jeffrey Flanigan", "Sam Thomson", "Norman Sadeh", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Analyzing the capabilities of crowdsourcing services for text summarization",
      "author" : [ "Elena Lloret", "Laura Plaza", "Ahmet Aker." ],
      "venue" : "Language Resources and Evaluation 47(2):337–369. https://doi.org/10.1007/s10579-012-9198-8.",
      "citeRegEx" : "Lloret et al\\.,? 2013",
      "shortCiteRegEx" : "Lloret et al\\.",
      "year" : 2013
    }, {
      "title" : "Multidocument summarization: An added value to clustering in interactive retrieval",
      "author" : [ "Manuel J. Maña-López", "Manuel de Buenaga", "José M. Gómez-Hidalgo." ],
      "venue" : "ACM Transactions on Information Systems 22(2):215–241.",
      "citeRegEx" : "Maña.López et al\\.,? 2004",
      "shortCiteRegEx" : "Maña.López et al\\.",
      "year" : 2004
    }, {
      "title" : "Do summaries help? A Task-Based Evaluation of Multi-Document Summarization",
      "author" : [ "Kathleen McKeown", "Rebecca J. Passonneau", "David K. Elson", "Ani Nenkova", "Julia Hirschberg." ],
      "venue" : "Proceedings of the 28th Annual International ACM SIGIR Con-",
      "citeRegEx" : "McKeown et al\\.,? 2005",
      "shortCiteRegEx" : "McKeown et al\\.",
      "year" : 2005
    }, {
      "title" : "Construction of Text Summarization Corpus for the Credibility of Information on the Web",
      "author" : [ "Masahiro Nakano", "Hideyuki Shibuki", "Rintaro Miyazaki", "Madoka Ishioroshi", "Koichi Kaneko", "Tatsunori Mori." ],
      "venue" : "Proceedings of the Seventh",
      "citeRegEx" : "Nakano et al\\.,? 2010",
      "shortCiteRegEx" : "Nakano et al\\.",
      "year" : 2010
    }, {
      "title" : "Automatic Summarization",
      "author" : [ "Ani Nenkova", "Kathleen R. McKeown." ],
      "venue" : "Foundations and Trends in Information Retrieval 5(2):103–233. https://doi.org/10.1561/1500000015.",
      "citeRegEx" : "Nenkova and McKeown.,? 2011",
      "shortCiteRegEx" : "Nenkova and McKeown.",
      "year" : 2011
    }, {
      "title" : "Theoretical Origins of Concept Maps, How to Construct Them, and Uses in Education",
      "author" : [ "Joseph D. Novak", "Alberto J. Cañas." ],
      "venue" : "Reflecting Education 3(1):29–42.",
      "citeRegEx" : "Novak and Cañas.,? 2007",
      "shortCiteRegEx" : "Novak and Cañas.",
      "year" : 2007
    }, {
      "title" : "Learning How to Learn",
      "author" : [ "Joseph D. Novak", "D. Bob Gowin." ],
      "venue" : "Cambridge University Press, Cambridge. https://doi.org/10.1017/CBO9781139173469.",
      "citeRegEx" : "Novak and Gowin.,? 1984",
      "shortCiteRegEx" : "Novak and Gowin.",
      "year" : 1984
    }, {
      "title" : "Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity",
      "author" : [ "Mohammad Taher Pilehvar", "David Jurgens", "Roberto Navigli." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Pilehvar et al\\.,? 2013",
      "shortCiteRegEx" : "Pilehvar et al\\.",
      "year" : 2013
    }, {
      "title" : "Concept map construction from text documents using affinity propagation",
      "author" : [ "Iqbal Qasim", "Jin-Woo Jeong", "Jee-Uk Heu", "Dong-Ho Lee." ],
      "venue" : "Journal of Information Science 39(6):719– 736. https://doi.org/10.1177/0165551513494645.",
      "citeRegEx" : "Qasim et al\\.,? 2013",
      "shortCiteRegEx" : "Qasim et al\\.",
      "year" : 2013
    }, {
      "title" : "Knowledge discovery from texts: A Concept Frame Graph Approach",
      "author" : [ "Kanagasabai Rajaraman", "Ah-Hwee Tan." ],
      "venue" : "Proceedings of the Eleventh International Conference on Information and Knowledge Management. McLean, VA, USA, pages 669–",
      "citeRegEx" : "Rajaraman and Tan.,? 2002",
      "shortCiteRegEx" : "Rajaraman and Tan.",
      "year" : 2002
    }, {
      "title" : "Using concept maps as a cross-language resource discovery tool for large documents in digital libraries",
      "author" : [ "Ryan Richardson", "Edward A. Fox." ],
      "venue" : "Proceedings of the 5th ACM/IEEE-CS Joint Conference on Digital Libraries. Denver, CO, USA, page",
      "citeRegEx" : "Richardson and Fox.,? 2005",
      "shortCiteRegEx" : "Richardson and Fox.",
      "year" : 2005
    }, {
      "title" : "Information navigation on the web by clustering and summarizing query results",
      "author" : [ "Dmitri G. Roussinov", "Hsinchun Chen." ],
      "venue" : "Information Processing & Management 37(6):789–816. https://doi.org/10.1016/S0306-4573(00)00062-5.",
      "citeRegEx" : "Roussinov and Chen.,? 2001",
      "shortCiteRegEx" : "Roussinov and Chen.",
      "year" : 2001
    }, {
      "title" : "Using Concept Maps for Information Conceptualization and Schematization in Technical Reading and Writing Courses: A Case Study for Computer Science Majors in Japan",
      "author" : [ "Debopriyo Roy." ],
      "venue" : "IEEE International Professional Communication",
      "citeRegEx" : "Roy.,? 2008",
      "shortCiteRegEx" : "Roy.",
      "year" : 2008
    }, {
      "title" : "Corpus Annotation through Crowdsourcing: Towards Best Practice Guidelines",
      "author" : [ "Marta Sabou", "Kalina Bontcheva", "Leon Derczynski", "Arno Scharl." ],
      "venue" : "Proceedings of the 9th International Conference on Language Resources and Evaluation. Reykjavik,",
      "citeRegEx" : "Sabou et al\\.,? 2014",
      "shortCiteRegEx" : "Sabou et al\\.",
      "year" : 2014
    }, {
      "title" : "Cheap and Fast – But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks",
      "author" : [ "Rion Snow", "Brendan O’Connor", "Daniel Jurafsky", "Andrew Ng" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Snow et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Snow et al\\.",
      "year" : 2008
    }, {
      "title" : "Creating a Large Benchmark for Open Information Extraction",
      "author" : [ "Gabriel Stanovsky", "Ido Dagan." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas, pages 2300–2305.",
      "citeRegEx" : "Stanovsky and Dagan.,? 2016",
      "shortCiteRegEx" : "Stanovsky and Dagan.",
      "year" : 2016
    }, {
      "title" : "JumpStarting Concept Map Construction with Knowledge Extracted from Documents",
      "author" : [ "Alejandro Valerio", "David B. Leake." ],
      "venue" : "Proceedings of the 2nd International Conference on Concept Mapping. San José, Costa Rica, pages 296–303.",
      "citeRegEx" : "Valerio and Leake.,? 2006",
      "shortCiteRegEx" : "Valerio and Leake.",
      "year" : 2006
    }, {
      "title" : "Automated Generation of Concept Maps to Support Writing",
      "author" : [ "Jorge J. Villalon." ],
      "venue" : "PhD Thesis, University of Sydney, Australia.",
      "citeRegEx" : "Villalon.,? 2012",
      "shortCiteRegEx" : "Villalon.",
      "year" : 2012
    }, {
      "title" : "Analysis of a Gold Standard for Concept Map Mining - How Humans Summarize Text Using Concept Maps",
      "author" : [ "Jorge J. Villalon", "Rafael A. Calvo", "Rodrigo Montenegro." ],
      "venue" : "Proceedings of the 4th International Conference on Concept Mapping. Vina",
      "citeRegEx" : "Villalon et al\\.,? 2010",
      "shortCiteRegEx" : "Villalon et al\\.",
      "year" : 2010
    }, {
      "title" : "Crowdsourced Top-k Algorithms: An Experimental Evaluation",
      "author" : [ "Xiaohang Zhang", "Guoliang Li", "Jianhua Feng." ],
      "venue" : "Proceedings of the Very Large Databases Endowment 9(8):612–623.",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "The Next Step for Multi-Document Summarization: A Heterogeneous Multi-Genre Corpus Built with a Novel Construction Approach",
      "author" : [ "Markus Zopf", "Maxime Peyrard", "Judith EckleKohler." ],
      "venue" : "Proceedings of the 26th Inter-",
      "citeRegEx" : "Zopf et al\\.,? 2016",
      "shortCiteRegEx" : "Zopf et al\\.",
      "year" : 2016
    }, {
      "title" : "Ontologizing concept maps using graph theory",
      "author" : [ "Amal Zouaq", "Dragan Gasevic", "Marek Hatala." ],
      "venue" : "Proceedings of the 2011 ACM Symposium on Applied Computing. TaiChung, Taiwan, pages 1687–1692.",
      "citeRegEx" : "Zouaq et al\\.,? 2011",
      "shortCiteRegEx" : "Zouaq et al\\.",
      "year" : 2011
    }, {
      "title" : "Evaluating the Generation of Domain Ontologies in the Knowledge Puzzle Project",
      "author" : [ "Amal Zouaq", "Roger Nkambou." ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering 21(11):1559– 1572. https://doi.org/10.1109/TKDE.2009.25.",
      "citeRegEx" : "Zouaq and Nkambou.,? 2009",
      "shortCiteRegEx" : "Zouaq and Nkambou.",
      "year" : 2009
    }, {
      "title" : "Implementation of method for generating concept map from unstructured text in the Croatian language",
      "author" : [ "Krunoslav Zubrinic", "Ines Obradovic", "Tomo Sjekavica." ],
      "venue" : "23rd International Conference on Software,",
      "citeRegEx" : "Zubrinic et al\\.,? 2015",
      "shortCiteRegEx" : "Zubrinic et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Generated summaries have been shown to support humans dealing with large document collections in information seeking tasks (McKeown et al., 2005; Maña-López et al., 2004; Roussinov and Chen, 2001).",
      "startOffset" : 123,
      "endOffset" : 196
    }, {
      "referenceID" : 17,
      "context" : "Generated summaries have been shown to support humans dealing with large document collections in information seeking tasks (McKeown et al., 2005; Maña-López et al., 2004; Roussinov and Chen, 2001).",
      "startOffset" : 123,
      "endOffset" : 196
    }, {
      "referenceID" : 27,
      "context" : "Generated summaries have been shown to support humans dealing with large document collections in information seeking tasks (McKeown et al., 2005; Maña-López et al., 2004; Roussinov and Chen, 2001).",
      "startOffset" : 123,
      "endOffset" : 196
    }, {
      "referenceID" : 6,
      "context" : "Instead, user studies (Chin et al., 2009; Kang et al., 2011) show that they note down important keywords and phrases, try to identify relationships between them and organize them accordingly.",
      "startOffset" : 22,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : "A representation more in line with observed user behavior is a concept map (Novak and Gowin, 1984).",
      "startOffset" : 75,
      "endOffset" : 98
    }, {
      "referenceID" : 21,
      "context" : "Introduced in 1972 as a teaching tool (Novak and Cañas, 2007), applications in education (Edwards and Fraser, 1983; Roy, 2008), for writing assistance (Villalon, 2012) or to structure information repositories (Briggs et al.",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 28,
      "context" : "Introduced in 1972 as a teaching tool (Novak and Cañas, 2007), applications in education (Edwards and Fraser, 1983; Roy, 2008), for writing assistance (Villalon, 2012) or to structure information repositories (Briggs et al.",
      "startOffset" : 89,
      "endOffset" : 126
    }, {
      "referenceID" : 33,
      "context" : "Introduced in 1972 as a teaching tool (Novak and Cañas, 2007), applications in education (Edwards and Fraser, 1983; Roy, 2008), for writing assistance (Villalon, 2012) or to structure information repositories (Briggs et al.",
      "startOffset" : 151,
      "endOffset" : 167
    }, {
      "referenceID" : 4,
      "context" : "Introduced in 1972 as a teaching tool (Novak and Cañas, 2007), applications in education (Edwards and Fraser, 1983; Roy, 2008), for writing assistance (Villalon, 2012) or to structure information repositories (Briggs et al., 2004; Richardson and Fox, 2005) have been reported.",
      "startOffset" : 209,
      "endOffset" : 256
    }, {
      "referenceID" : 26,
      "context" : "Introduced in 1972 as a teaching tool (Novak and Cañas, 2007), applications in education (Edwards and Fraser, 1983; Roy, 2008), for writing assistance (Villalon, 2012) or to structure information repositories (Briggs et al., 2004; Richardson and Fox, 2005) have been reported.",
      "startOffset" : 209,
      "endOffset" : 256
    }, {
      "referenceID" : 16,
      "context" : "In addition, the method includes a novel crowdsourcing scheme, low-context importance annotation, that resolves quality issues observed in previous work (Lloret et al., 2013).",
      "startOffset" : 153,
      "endOffset" : 174
    }, {
      "referenceID" : 39,
      "context" : "Some attempts have been made to automatically construct concept maps from text, working with either single documents (Zubrinic et al., 2015; Villalon, 2012; Valerio and Leake, 2006; Kowata et al., 2010) or document clusters (Qasim et al.",
      "startOffset" : 117,
      "endOffset" : 202
    }, {
      "referenceID" : 33,
      "context" : "Some attempts have been made to automatically construct concept maps from text, working with either single documents (Zubrinic et al., 2015; Villalon, 2012; Valerio and Leake, 2006; Kowata et al., 2010) or document clusters (Qasim et al.",
      "startOffset" : 117,
      "endOffset" : 202
    }, {
      "referenceID" : 32,
      "context" : "Some attempts have been made to automatically construct concept maps from text, working with either single documents (Zubrinic et al., 2015; Villalon, 2012; Valerio and Leake, 2006; Kowata et al., 2010) or document clusters (Qasim et al.",
      "startOffset" : 117,
      "endOffset" : 202
    }, {
      "referenceID" : 24,
      "context" : ", 2010) or document clusters (Qasim et al., 2013; Zouaq and Nkambou, 2009; Rajaraman and Tan, 2002).",
      "startOffset" : 29,
      "endOffset" : 99
    }, {
      "referenceID" : 38,
      "context" : ", 2010) or document clusters (Qasim et al., 2013; Zouaq and Nkambou, 2009; Rajaraman and Tan, 2002).",
      "startOffset" : 29,
      "endOffset" : 99
    }, {
      "referenceID" : 25,
      "context" : ", 2010) or document clusters (Qasim et al., 2013; Zouaq and Nkambou, 2009; Rajaraman and Tan, 2002).",
      "startOffset" : 29,
      "endOffset" : 99
    }, {
      "referenceID" : 20,
      "context" : "For the subtask of selecting summary-worthy concepts and relations, techniques developed for traditional summarization (Nenkova and McKeown, 2011) and keyphrase extraction (Hasan and Ng, 2014) are related and applicable.",
      "startOffset" : 119,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : "Approaches that build graphs of propositions to create a summary (Fang et al., 2016; Li et al., 2016; Liu et al., 2015; Li, 2015) seem to be particularly related, however, there is one important difference: While they use graphs as an intermediate representation from which a textual summary is then generated, the goal of the proposed task is to create a graph that is directly interpretable and useful for a user.",
      "startOffset" : 65,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "Approaches that build graphs of propositions to create a summary (Fang et al., 2016; Li et al., 2016; Liu et al., 2015; Li, 2015) seem to be particularly related, however, there is one important difference: While they use graphs as an intermediate representation from which a textual summary is then generated, the goal of the proposed task is to create a graph that is directly interpretable and useful for a user.",
      "startOffset" : 65,
      "endOffset" : 129
    }, {
      "referenceID" : 12,
      "context" : "Approaches that build graphs of propositions to create a summary (Fang et al., 2016; Li et al., 2016; Liu et al., 2015; Li, 2015) seem to be particularly related, however, there is one important difference: While they use graphs as an intermediate representation from which a textual summary is then generated, the goal of the proposed task is to create a graph that is directly interpretable and useful for a user.",
      "startOffset" : 65,
      "endOffset" : 129
    }, {
      "referenceID" : 20,
      "context" : ", 2010) or document clusters (Qasim et al., 2013; Zouaq and Nkambou, 2009; Rajaraman and Tan, 2002). These approaches extract concept and relation labels from syntactic structures and connect them to build a concept map. However, common task definitions and comparable evaluations are missing. In addition, only a few of them, namely Villalon (2012) and Valerio and Leake (2006), define summarization as their goal and try to compress the input to a substantially smaller size.",
      "startOffset" : 30,
      "endOffset" : 350
    }, {
      "referenceID" : 20,
      "context" : ", 2010) or document clusters (Qasim et al., 2013; Zouaq and Nkambou, 2009; Rajaraman and Tan, 2002). These approaches extract concept and relation labels from syntactic structures and connect them to build a concept map. However, common task definitions and comparable evaluations are missing. In addition, only a few of them, namely Villalon (2012) and Valerio and Leake (2006), define summarization as their goal and try to compress the input to a substantially smaller size.",
      "startOffset" : 30,
      "endOffset" : 379
    }, {
      "referenceID" : 36,
      "context" : "(Zopf et al., 2016) and (Benikova et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : ", 2016) and (Benikova et al., 2016).",
      "startOffset" : 12,
      "endOffset" : 35
    }, {
      "referenceID" : 34,
      "context" : "For concept map generation, one corpus with humancreated summary concept maps for student essays has been created (Villalon et al., 2010).",
      "startOffset" : 114,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "Other types of information representation that also model concepts and their relationships are knowledge bases, such as Freebase (Bollacker et al., 2009), and ontologies.",
      "startOffset" : 129,
      "endOffset" : 153
    }, {
      "referenceID" : 10,
      "context" : "Extending these efforts, several more specialized corpora have been created: With regard to size, Nakano et al. (2010) present a corpus of summaries for large-scale collections of web pages.",
      "startOffset" : 44,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : ", 2016) and (Benikova et al., 2016). The corpus we present combines these aspects, as it has large clusters of heterogeneous documents, and provides a necessary benchmark to evaluate the proposed task. For concept map generation, one corpus with humancreated summary concept maps for student essays has been created (Villalon et al., 2010). In contrast to our corpus, it only deals with single documents, requires a two orders of magnitude smaller amount of compression and is not publicly available . Other types of information representation that also model concepts and their relationships are knowledge bases, such as Freebase (Bollacker et al., 2009), and ontologies. However, they both differ in important aspects: Whereas concept maps follow an open label paradigm and are meant to be interpretable by humans, knowledge bases and ontologies are usually more strictly typed and made to be machine-readable. Moreover, approaches to automatically construct them from text typically try to extract as much information as possible, while we want to summarize a document. As a result, approaches from these fields cannot be directly applied. Vice versa, Zouaq et al. (2011) showed that concept maps can be a useful first step to create a domain ontology.",
      "startOffset" : 13,
      "endOffset" : 1175
    }, {
      "referenceID" : 29,
      "context" : "’s approach, it is more in line with crowdsourcing best practices, as the tasks are simple, intuitive and small (Sabou et al., 2014) and workers receive reasonable payment (Fort et al.",
      "startOffset" : 112,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "Comparisons are known to be easier to make and more consistent (Belz and Kow, 2010),",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 35,
      "context" : ", 2007), a powerful Bayesian rank induction model (Zhang et al., 2016), to obtain importance estimates for each proposition.",
      "startOffset" : 50,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : "2 Pilot Study To verify the proposed approach, we conducted a pilot study on Amazon Mechanical Turk using data from TAC2008 (Dang and Owczarzak, 2008).",
      "startOffset" : 124,
      "endOffset" : 150
    }, {
      "referenceID" : 16,
      "context" : "In addition, we included trap sentences, similar as in (Lloret et al., 2013), in around 80 of the tasks.",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : "Quality Control Following the observations of Lloret et al. (2013), we established several measures for quality control.",
      "startOffset" : 3,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "Agreement and Reliability For Likert-scale tasks, we follow Snow et al. (2008) and calculate agreement as the average Pearson correlation of a worker’s Likert-score with the average score of the remaining workers.",
      "startOffset" : 16,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : "Even with intelligent sampling strategies, such as the active learning in CrowdBT (Chen et al., 2013), the number of pairs is only reduced by a constant factor (Zhang et al.",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 35,
      "context" : ", 2013), the number of pairs is only reduced by a constant factor (Zhang et al., 2016).",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "To further examine the reliability of the collected data, we followed the approach of Kiritchenko and Mohammed (2016) and simply repeated the crowdsourcing for one of the three topics.",
      "startOffset" : 25,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "The Open Information Extraction paradigm (Banko et al., 2007) offers a representation very similar to the desired one.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 31,
      "context" : "We used Open IE 47, a state-of-the-art Open IE system (Stanovsky and Dagan, 2016), to process the relevant sentences of all topics.",
      "startOffset" : 54,
      "endOffset" : 81
    }, {
      "referenceID" : 21,
      "context" : "They were instructed to reach a size of 25 concepts, the recommended maximum size for a concept map (Novak and Cañas, 2007).",
      "startOffset" : 100,
      "endOffset" : 123
    }, {
      "referenceID" : 23,
      "context" : "To support the annotator, the tool used ADW (Pilehvar et al., 2013), a state-of-the-art approach for semantic similarity, to suggest possible connections.",
      "startOffset" : 44,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "Hence, the annotation shows substantial agreement (Landis and Koch, 1977).",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 36,
      "context" : "To capture this property, we follow Zopf et al. (2016) and compute, for every topic, the average Jensen-Shannon divergence between the word distribution of one document and the word distribution in the remaining documents.",
      "startOffset" : 36,
      "endOffset" : 55
    }, {
      "referenceID" : 32,
      "context" : "These distributions are very similar to those reported by Villalon et al. (2010) for their (single-document) concept map corpus.",
      "startOffset" : 58,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : "Using METEOR (Denkowski and Lavie, 2014), we offer a second metric that takes synonyms and paraphrases into account and also scores partial matches.",
      "startOffset" : 13,
      "endOffset" : 40
    }, {
      "referenceID" : 14,
      "context" : "And finally, we compute ROUGE-2 (Lin, 2004) between the concatenation of all propositions from the maps.",
      "startOffset" : 32,
      "endOffset" : 43
    } ],
    "year" : 2017,
    "abstractText" : "Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multidocument summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization.",
    "creator" : "LaTeX with hyperref package"
  }
}