{
  "name" : "251.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\nIn this work we provide a mathematical formalism for compositionality, and a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, we show that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, we explain the success of vector calculus for solving word analogies. When these assumptions do not hold, we show that compositionality is no longer additive, and provide the correct composition operator.\nFinally, we establish a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby. SDR models provide information-theoretically optimal symbol-embeddings for problems where the training data takes the form of co-occurrence statistics. We prove that the parameters of Skip-Gram models can be readily modified to obtain the parameters of SDR models by simply adding informa-\ntion on symbol frequencies. This shows that the Skip-Gram model is essentially learning optimal word embeddings in the sense of Globerson and Tishby. Further, it implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models."
    }, {
      "heading" : "1 Introduction",
      "text" : "The idea of representing words as vectors has a long history in computational linguistics and machine learning. The general idea is to find a map from words to vectors such that wordsimilarity and vector-similarity are in correspondence. Whilst vector-similarity can be readily quantified in terms of distances and angles, quantifying word-similarity is a more ambiguous task. A key insight in that regard is to posit that the meaning of a word is captured by “the company it keeps” (Firth, 1957) and, therefore, that two words that keep company with similar words are likely to be similar themselves. To break the cyclicality of this definition it is common to consider models that only attempt to capture pairwise cooccurrence statistics1.\nIn the simplest case, one seeks vectors whose similarity approximates the co-occurrence frequencies. In more sophisticated methods cooccurrences are reweighed to suppress the effect of more frequent words (Rohde et al., 2006) and/or to emphasize pairs of words whose co-occurrence frequency maximally deviates from the independence assumption (Church and Hanks, 1990).\nAn alternative to seeking word-embeddings that reflect co-occurrence statistics is to extract the\n1Normally, if a is similar to b and b is similar to c, it should be that a is more similar to c than it is to a random word. But pairwise-similarity models do not explicitly require that, i.e., do not penalize model parameters under which a and c are not particularly similar.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nvectorial representation of words from non-linear statistical language models, specifically neural networks. (Bengio et al., 2003) already proposed (i) associating with each vocabulary word a feature vector, (ii) expressing the probability function of word sequences in terms of the feature vectors of the words in the sequence, and (iii) learning simultaneously the vectors and the parameters of the probability function. This approach came into prominence recently through works of Mikolov et al. (see below) whose main departure from (Bengio et al., 2003) was to follow the suggestion of (Mnih and Hinton, 2007) and tradeaway the expressive capacity of general neuralnetwork models for the scalability to very large corpora afforded by the more restricted class of log-linear models.\nAn unexpected side effect of deriving wordembeddings via neural networks is that the wordvectors produced appear to enjoy (approximate) additive compositionality: adding two wordvectors often results in a vector whose nearest word-vector is of the word capturing the composition of the added words, e.g., “man” + “royal” = “king” (Mikolov et al., 2013c). This unexpected property allows one to use these vectors to answer word-analogy questions algebraically, e.g., answering the question “Man is to king as woman is to ?” by returning the word whose word-vector is nearest to the vector\nv(king) - v(man) + v(woman).\nIn this work we focus on explaining the source of this phenomenon for the most prominent such model, namely the Skip-Gram (SG) model introduced in (Mikolov et al., 2013a). The SG model learns vector representations of words based on their patterns of co-occurrence in the training corpus as follows: it assigns to each word c in the vocabulary V , a “context” and a “target” vector, respectively uc and vc, which are to be used in order to predict the words that appear around each occurrence of c within a window of ∆ tokens. Specifically, the log probability of any target word w to occur at any position within distance ∆ of a context word c is taken to be proportional to the inner product between uc and vw, i.e., letting n = |V |,\np(w|c) = e uTc vw∑n\ni=1 e uTc vi\n. (1)\nFurther, SG assumes that the conditional proba-\nbility of each possible set of words in a window around a context word c factorizes as the product of the respective conditional probabilities:\np(w−∆, . . . , w∆|c) = ∆∏\nδ=−∆ δ 6=0\np(wδ|c). (2)\n(Mikolov et al., 2013a) proposed learning the SG parameters on a training corpus by using maximum likelihood estimation under (1) and (2). Thus, if wi denotes the i-th word in the training corpus and T the length of the corpus, we seek the word vectors that maximize\n1\nT T∑ i=1 ∆∑ δ=−∆ δ 6=0 log p(wi+δ|wi) . (3)\nAs mentioned, the normalized context vectors obtained from maximizing (3) under (1) and (2) exhibit additive compositionality. For example, the cosine distance between the sum of the context vectors of the words “Vietnam” and “capital” and the context vector of the word “Hanoi” is small.\nWhile there has been much interest in using algebraic operations on word vectors to carry out semantic operations like composition, the only published work which attempts a rigorous theoretical understanding of this phenomenon is (Arora et al., 2016). This work guarantees that word vectors can be recovered by factorizing the so-called PMI matrix, and that algebraic operations on these word vectors can be used to solve analogies, under certain conditions on the process that generated the training corpus. Specifically, the word vectors must be known, a priori before their recovery, to have been generated by randomly scaling uniformly sampled vectors from the unit sphere2. Further, the ith word in the corpus must have been selected with probability proportional to eu T wci , where the “discourse” vector ci governs the topic of the corpus at the ith word. The discourse vector is assumed to evolve according to a random walk on the unit sphere that has a uniform stationary distribution.\nBy way of contrast, our results assume nothing about the properties of the word vectors a priori. In fact, the connection we establish between the Skip-gram and Sufficient Dimensionality Reduction model of (Globerson and Tishby, 2003)\n2More generally, it suffices that the word vectors have certain properties consistent with this sampling process.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nshows that the word vectors learned by Skip-Gram are information-theoretically optimal. Further, the context word c in the Skip-Gram model essentially serves the role that the discourse vector does in the PMI model of (Arora et al., 2016): the words neighboring c are selected with probability proportional to eu T c vw . We find the exact non-linear composition operator when no assumptions are made on the context word. When an analogous assumption to that of (Arora et al., 2016) is made, that the context words are uniformly distributed, we prove that the composition operator reduces to vector addition.\nWhile our primary motivation has been to provide a better theoretical understanding of the popular SG model, our connection with the SDR method opens up the possibility of practical applicability of our approach more generally. In particular, there is the question of whether, for a given corpus, fitting an SG model will give good embeddings—while we are making reasonable linguistic assumptions about how to model words and the interdependencies of words in a corpus, it’s not clear that these have to hold universally on all corpuses to which we apply SG. However, the fact that when we fit an SG model we are fitting an SDR model (up to frequency information), and the fact that SDR models are informationtheoretically optimal in a certain sense, argues that regardless of whether the SG assumptions hold, SG always gives us optimal features in the following sense: the learned context embeddings and target embeddings preserve the maximal amount of mutual information between any pair of random variables X and Y consistent with the observed co-occurence matrix, where Y is the target word and X is the predictor word (in a min-max sense, since there are many ways of coupling X and Y , each of which may have different amounts of mutual information). Importantly, this statement requires no assumptions on the distribution P (X,Y )."
    }, {
      "heading" : "2 Compositionality of Skip-Gram",
      "text" : "In this section, we first give a mathematical formulation of the intuitive notion of compositionality of words. We then prove that the composition operator for the Skip-Gram model in full generality is a non-linear function of the vectors of the words being composed. Under a single simplifying assumption, the operator linearizes and reduces to\nthe addition of the word vectors. Finally, we explain how linear compositionality allows for solving word analogies with vector algebra.\nA natural way of capturing the compositionality of words is to say that the set of context words c1, . . . , cm has the same meaning as the single word c if for every other word w,\np(w|c1, . . . , cm) = p(w|c) .\nAlthough this is an intuitively satisfying definition, we never expect it to hold exactly; instead, we replace exact equality with the minimization of KL-divergence. That is, we state that the best candidate for having the same meaning as the set of context words C is the word\narg min c∈V\nDKL(p(·|C) | p(·|c)) . (4)\nWe refer to any vector that minimizes (4) as a synonym of the set of words C.\nThere are two natural concerns with (4). The first is that, in general, it is not clear how to define p(·|C). The second is that KL-divergence minimization is a hard problem, as it involves optimization over many high dimensional probability distributions. Our main result shows that both of these problems go away for any language model that satisfies two assumptions:\nA1. For every word c, there exists Zc such that for every word w,\np(w|c) = 1 Zc exp(uTc vw) . (5)\nA2. For every set of words C = {c1, c2, . . . , cm}, there exists ZC such that for every word w,\np(w|C) = p(w) 1−m\nZC\nm∏ i=1 p(w|ci) . (6)\nClearly, the SG model satisfies A1 by definition. We prove that it also satisfies A2 when m ≤ ∆ (Lemma 1).\nTheorem 1. In every word model that satisfies A1 and A2, for every set of words C = {c1, . . . , cm}, the synonyms of C satisfy∑\nw∈V p(w|c)vw = ∑ w∈V p(w|C)vw . (7)\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nTheorem 1 characterizes the composition operator for any language model which satisfies our two assumptions; in general, this operator is not addition. Instead, a synonym c is a vector such that the average word vector under p(·|c) matches that under p(·|C). When the expectations in (7) can be computed, the composition operator can be implemented by solving a non-linear system of equations to find a vector u for which the left-hand side of (7) equals the right-hand side.\nOur next result proves that although the composition operator is nontrivial in the general case, to recover vector addition as the composition operator, it suffices to assume that the word frequency is uniform.\nTheorem 2. In every word model that satisfies A1, A2, and where p(w) = 1/|V | for everyw ∈ V , the synonym of C = {c1, . . . , cm} is\nu1 + . . .+ um .\nAs word frequencies are typically much closer to a Zipf distribution (Piantadosi, 2014), the uniformity assumption of Theorem 2 is not realistic. That said, we feel it is important to point out that, as reported in (Mikolov et al., 2013b), additivity captures compositionality more accurately when the training set is manipulated so that the prior distribution of the words is made closer to uniform.\nUsing composition to solve analogies It has been observed that word vectors trained using nonlinear models like SG tend to encode semantic relationships between words as linear relationships between the word vectors (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014). In particular, analogies of the form “man:woman::king:?” can often be solved by taking ? to be the word in the vocabulary whose context vector has the smallest angle with uwoman + (uking−uman). Theorems 1 and 2 offer insight into the solution such analogy questions.\nWe first consider solving an analogy of the form “m:w::k:?”” in the case where the composition operator is nonlinear. The fact that m and w share a relationship means m is a synonym for the set of words {w,R}, where R is a set of words encoding the relationship between m and w. Similarly, the fact that k and ? share the same relationship means k is a synonym for the set of words {?, R}.\nBy Theorem 1, we have that R and ? must satisfy∑ `∈V p(`|m)v` = ∑ `∈V\np(`|w,R)v` and∑ `∈V p(`|k)v` = ∑ `∈V p(`|?, R)v`.\nWe see that solving analogies when the composition operator is nonlinear requires the solution of two highly nonlinear systems of equations. In sharp contrast, when the composition operator is linear, the solution of analogies delightfully reduces to elementary vector algebra. To see this, we again begin with the assertion that the fact that m and w share a relationship means m is a synonym for the set of words {w,R}; Similarly, k is a synonym for {?, R}. By Theorem 2,\num = uw + ur and\nuk = u? + ur,\nwhich gives the expected relationship\nu? = uk + (uw − um).\nNote that because this expression for u? is in terms of k, w, andm, there is actually no need to assume that R is a set of actual words in V ."
    }, {
      "heading" : "2.1 Proofs",
      "text" : "Proof of Theorem 1. Note that p(w|C) equals\np(w)1−m\nZC\nm∏ i=1 p(w|ci)\n= p(w)1−m\nZC exp ( m∑ i=1 uTcivw − m∑ i=1 logZci )\n= 1\nZ p(w)1−m exp(uTCvw) ,\nwhere Z = ZC ∏m i=1 Zi, and uC = ∑m i=1 ui.\nMinimizing the KL-divergence\nDKL(p(·|c1, . . . , cm)‖p(·|c))\nas a function of c is equivalent to maximizing the negative cross-entropy as a function of uc, i.e., as maximizing\nQ(uc) = Z ∑ w exp(uTCvw) p(w)m−1 (uTc vw − logZc) .\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nSince Q is concave, the maximizers occur where its gradient vanishes. As∇ucQ equals\nZ ∑ w exp(uTCvw) p(w)m−1 [ vw − ∑n `=1 exp(u T c v`)v`∑n k=1 exp(u T c vk) ] = ∑n `=1 exp(u T c v`)v`∑n\nk=1 exp(u T c vk) − Z ∑ w exp(uTCvw)vw p(w)m−1\n= ∑ w∈V p(w|c)vw − ∑ w∈V p(w|c1, . . . , cm)vw ,\nwe see that (7) follows. Proof of Theorem 2. Recall that uC = ∑m\ni=1 ui. When p(w) = 1/|V | for all w ∈ V , the negative cross-entropy simplifies to\nQ(uc) = Z ∑ w exp ( uTCvw ) (uTc vw − logZc) ,\nand its gradient∇ucQ to\nZ ∑ w exp(uC Tvw) [ vw − ∑n `=1 exp(u T c v`)v`∑n k=1 exp(u T c vk) ] = Z\n∑ w exp(uC Tvw)vw − ∑ w exp(uTc vw)vw .\nThus,∇Q(uC) = 0 and since Q is concave, uC is its unique maximizer.\nLemma 1. The SG model satisfies assumption A2 when m ≤ ∆.\nProof of Lemma 1. First, assume that m = ∆. In the SG model target words are conditionally independent given a context word, i.e.,\np(c1, . . . , cm|w) = m∏ i=1 p(ci|w).\nApplying Baye’s rule,\np(w|c1, . . . , cm) = p(c1, . . . , cm|w)p(w)\np(c1, . . . , cm)\n= p(w)\np(c1, . . . , cm) m∏ i=1 p(ci|w)\n= p(w)\np(c1, . . . , cm) m∏ i=1 p(w|ci)p(ci) p(w)\n= p(w)1−m\nZC\nm∏ i=1 p(w|ci) , (8)\nwhere ZC = 1/ ( ∏m i=1 p(ci)). This establishes the result when m = ∆. The cases m < ∆ follow by marginalizing out ∆ −m context words in the equality 8.\nProjection of synonyms onto the vocabulary Theorem 2 states that if there is a word c in the vocabulary V whose context vector equals the sum of the context vectors of the words c1, . . . , cm, then c has the same “meaning”, in the sense of (4), as the composition of the words c1, . . . , cm. For any given set of words C = {c1, . . . , cm}, it is unlikely that there exists a word c ∈ V whose context vector is exactly equal to the sum of the context vectors of the words c1, . . . , cm. Similarly, in Theorem 1, the solution(s) to (7) will most likely not equal the context vector of any word in V . In both cases, we thus need to project the vector(s) onto words in our vocabulary in some manner.\nSince Theorem 1 holds for any prior over V , in theory, we could enumerate all words in V and find the word(s) that minimize the difference of the left hand side of (7) from the right hand side. In practice, it turns out that the angle between the context vector of a word w ∈ V and solutionvector(s) is a good proxy and one gets very good experimental results by selecting as the synonym of a collection of words, the word that minimizes the angle to the synonym vector.\nMinimizing the angle has been empirically successful at capturing composition in multiple loglinear word models. One way to understand the success of this approach is to recall that each word c is characterized by a categorical distribution over all other words w, as stated in (1). The peaks of this categorical distribution are precisely the words with which c co-occurs most often. These words characterize c more than all the other words in the vocabulary, so it is reasonable to expect that a word c′ whose categorical distribution has similar peaks as the categorical distribution of c is similar in meaning to c. Note that the location of the peaks of p(·|c) are immune to the scaling of uc (athough the values of p(·|c) may change); thus, the wordsw which best characterize c are those for which vw has a high inner product with uc/‖uc‖2. Since ∣∣∣∣uTc vw‖uc‖2 − u T c′vw ‖uc′‖2 ∣∣∣∣≤ √ 2 ( 1− u T c uc′ ‖uc‖2‖uc′‖2 ) ‖vw‖2,\nit is clear that if the angle between the context representations of c and c′ is small, the distributions p(w|c) and p(w|c′) will tend to have similar peaks.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599"
    }, {
      "heading" : "3 Skip-Gram learns a Sufficient Dimensionality Reduction Model",
      "text" : "The Skip-Gram model assumes that the distribution of the neighbors of a word follows a specific exponential parametrization of a categorical distribution. There is empirical evidence that this model generates features that are useful for NLP tasks, but there is no a priori guarantee that the training corpus was generated in this manner. In this section, we provide theoretical support for the usefulness of the features learned even when the SkipGram model is misspecified.\nTo do so, we draw a connection between SkipGram and the Sufficient Dimensionality Reduction (SDR) factorization of Globerson and Tishby (Globerson and Tishby, 2003). The SDR model learns optimal3 embeddings for discrete random variables X and Y without assuming any parametric form on the distributions of X and Y , and it is useful in a variety of applications, including information retrieval, document classification, and association analysis (Globerson and Tishby, 2003). As it turns out, these embeddings, like Skip-Gram, are obtained by learning the parameters of an exponentially parameterized distribution. In Theorem 3 below, we show that if a SkipGram model is fit to the cooccurence statistics of X and Y , then the output can be trivially modified (by adding readily-available information on word frequencies) to obtain the parameters of an SDR model.\nThis connection is significant for two reasons: first, the original algorithm of (Globerson and Tishby, 2003) for learning SDR embeddings is expensive, as it involves information projections. Theorem 3 shows that if one can efficiently fit a Skip-Gram model, then one can efficiently fit an SDR model. This implies that Skip-Gram specific approximation heuristics like negativesampling, hierarchical softmas, and Glove, which are believed to return high-quality approximations to Skip-Gram parameters (Mikolov et al., 2013b; Pennington et al., 2014), can be used to efficiently approximate SDR model parameters. Second, (Globerson and Tishby, 2003) argues for the optimality of the SDR embedding in any domain where the training information on X and Y\n3Optimal in an information-theoretic sense: they preserve the maximal mutual information between any pair of random variables with the observed coocurrence statistics, without regard to the underlying joint distribution.\nconsists of their coocurrence statistics; this optimality and the Skip-Gram/SDR connection argues for the use of Skip-Gram approximations in such domains, and supports the positive experimental results that have been observed in applications in network science (Grover and Leskovec, 2016), proteinomics (Asgari and Mofrad, 2015), and other fields.\nAs stated above, the SDR factorization solves the problem of finding information-theoretically optimal features, given co-occurrence statistics for a pair of discrete random variables X and Y . Associate a vector wi to the ith state of X , a vector hj to the jth state of Y , and let W = [wT1 · · ·wT|X|]\nT and H be defined similarly. Globerson and Tishby show that such optimal features can be obtained from a low-rank factorization of the matrix G of co-occurence measurements: Gij counts the number of times state i of X has been observed to co-occur with state j of Y. The loss of this factorization is measured using the KL-divergence, and so the optimal features are obtained from solving the problem\narg min W,H DKL\n( G\nZG ∥∥∥∥ 1ZW,H eWHT ) .\nHere, ZG = ∑\nij Gij normalizes G into an estimate of the joint pmf of X and Y , and similarly ZW,H is the constant that normalizes eWH T into a joint pmf. The expression eWH T\ndenotes entrywise exponentiation of WHT .\nNow we revisit the Skip-Gram training objective, and show that it differs from the SDR objective only slightly. Whereas the SDR objective measures the distance between the pmfs given by (normalized versions of) G and eWH T , the SkipGram objective measures the distance between the pmfs given by (normalized versions of) the rows of G and eWH T . That is, SDR emphasizes fitting the entire pmfs, while Skip-Gram emphasizes fitting conditional distributions.\nBefore presenting our main result, we state and prove the following lemma, which is of independent interest and is used in the proof of our main theorem. Recall that Skip-Gram represents each word c as a multinomial distribution over all other words w, and it learns the parameters for these distributions by a maximum likelihood estimation. It is known that learning model parameters by maximum likelihood estimation is equivalent to minimizing the KL-divergence of the learned\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nmodel from the empirical distribution; the following lemma establishes the KL-divergence that Skip-Gram minimizes. Lemma 2. Let G be the word co-occurrence matrix constructed from the corpus on which a SkipGram model is trained, in which case Gcw is the number of times word w occurs as a neighboring word of c in the corpus. For each word c, let gc denote the empirical frequency of the word in the corpus, so that\ngc = ∑ w Gcw/ ∑ t,w Gt,w.\nGiven a positive vector x, let x̂ = x/‖x‖1. Then, the Skip-Gram model model parameters U =[ u1 · · · u|V | ]T and V = [v1 · · · u|V |]T minimize the objective∑\nc\ngcDKL(ĝc ‖ êuTc VT ),\nwhere gc is the cth row of G.\nProof. Recall that Skip-Gram chooses U and V to maximize\nQ = 1\nT T∑ i=1 C∑ δ=−C δ 6=0 log p(wi+δ|wi) ,\nwhere\np(w|c) = e uTc vw∑n\ni=1 e uTc vi\n.\nThis objective can be rewritten using the pairwise cooccurence statistics as\nQ= 1\nT ∑ c,w Gcw log p(w|c)\n= 1\nT ∑ c [(∑ t Gct )∑ w Gcw∑ tGct log p(w|c) ]\n∝ 1 T ∑ c\n[ ( ∑\ntGct) ( ∑\ntwGtw) ∑ w Gcw∑ tGct log p(w|c)\n]\n= ∑ c gc (∑ w ( ĝc ) w log p(w|c) ) = ∑ c gc ( −DKL(ĝc ‖ p(·|c))−H(ĝc) ) ,\nwhere H(·) denotes the entropy of a distribution. It follows that since Skip-Gram maximizes Q, it minimizes∑ c gcDKL(ĝc ‖ p(·|c))= ∑ c gcDKL(ĝc ‖ êuTc VT ).\nWe now prove our main theorem of this section, which states that SDR parameters can be obtained by augmenting the Skip-Gram embeddings to account for word frequencies.\nTheorem 3. Let U,V be the results of fitting a Skip-Gram model to G, and consider the augmented matrices\nŨ = [U |α] and Ṽ = [V |1],\nwhere\nαc = log\n( gc∑\nw e uTc vw\n) and gc = ∑ wGc,w∑ t,wGt,w .\nThen, the features (Ũ, Ṽ) constitute a sufficient dimensionality reduction of G.\nProof. For convenience, let G denote the joint pdf matrix G/ZG, and let Ĝ denote the matrix obtained by normalizing each row of G to be a probability distribution. Then, it suffices to show that DKL(G ‖ qW,H) is minimized over the set of probability distributions{\nqW,H ∣∣∣∣ qW,H(w, c) = 1Z (eWHT )cw } ,\nwhen W = Ũ and H = Ṽ. To establish this result, we use a chain rule for the KL-divergence. Recall that if we denote the expected KL-divergence between two marginal pmfs by\nDKL(p(·|c)‖q(·|c))\n= ∑ c p(c) (∑ w p(w|c) log ( p(w|c) q(w|c) )) ,\nthen the KL-divergence satisfies the chain rule:\nDKL(p(w, c)‖q(w, c)) = DKL(p(c)‖q(c)) +DKL(p(w|c)‖q(w|c)).\nUsing this chain rule, we get\nDKL(G ‖ qW,H(w, c)) (9) =DKL(g ‖ qW,H(c))+DKL(Ĝ‖qW,H(w|c)).\nNote that the second term in this sum is, in the notation of Lemma 2, DKL(Ĝ‖qW,H(w|c)) = ∑ c gcDKL(ĝc ‖ êwTc HT ),\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nso the matrices U and V that are returned by fitting the Skip-Gram model minimize the second term in this sum. We now show that the augmented matrices W = Ũ and H = Ṽ also minimize this second term, and in addition they make the first term vanish.\nTo see that the first of these claims holds, i.e., that the augmented matrices make the second term in (9) vanish, note that\nqŨ,Ṽ(w|c) ∝ e ũTc ṽw = eu T c vw+αc ∝ qU,V(w|c),\nand the constant of proportionality is independent of w. It follows that qŨ,Ṽ(w|c) = qU,V(w|c) and\nDKL(Ĝ ‖ qŨ,Ṽ(w|c)) = DKL(Ĝ ‖ qU,V(w|c)).\nThus, the choice W = Ũ and H = Ṽ minimizes the second term in (9).\nTo see that the augmented matrices make the first term in (9) vanish, observe that when W = Ũ and H = Ṽ, we have that qŨ,Ṽ(c) = g by construction. This can be verified by calculation:\nqŨ,Ṽ(c) = ∑ w qŨ,Ṽ(w, c)∑ w,t qŨ,Ṽ(w, t) = ∑ w e uTc vw+αc∑ w,t e uTt vw+αt\n=\n(∑ w e uTc vw )\neαc∑ t (∑ w e uTt vw ) eαt\n=\n[ (eUV T 1) eα ] c\n1T [ (eUV T 1) eα ] . Here, the notation x y denotes entry-wise multiplication of vectors.\nSince αc = log(gc) − log (( eUV T 1 ) c ) , we\nhave\nqŨ,Ṽ(c) =\n[ (eUV T 1) eα ] c\n1T [ (eUV T 1) eα ] = gc∑ t gt = gc.\nThe choice W = Ũ and H = Ṽ makes the first term in (9) vanish, and it also minimizes the second term in (9). Thus, it follows that the features (Ũ, Ṽ) constitute a sufficient dimensionality reduction of G."
    } ],
    "references" : [ {
      "title" : "A latent variable model approach to PMI-based word embeddings",
      "author" : [ "Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski." ],
      "venue" : "Transactions of the Association for Computational Linguistics 4:385–399.",
      "citeRegEx" : "Arora et al\\.,? 2016",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2016
    }, {
      "title" : "Continuous distributed representation of biological sequences for deep proteomics and genomics",
      "author" : [ "Ehsaneddin Asgari", "Mohammad R.K. Mofrad." ],
      "venue" : "PloS One 10(11).",
      "citeRegEx" : "Asgari and Mofrad.,? 2015",
      "shortCiteRegEx" : "Asgari and Mofrad.",
      "year" : 2015
    }, {
      "title" : "A Neural Probabilistic Language Model",
      "author" : [ "Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent", "Christian Jauvin." ],
      "venue" : "Journal Of Machine Learning Research 3:1137–1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Word Association Norms, Mutual Information, and Lexicography",
      "author" : [ "Kenneth Ward Church", "Patrick Hanks." ],
      "venue" : "Comput. Linguist. 16(1):22–29.",
      "citeRegEx" : "Church and Hanks.,? 1990",
      "shortCiteRegEx" : "Church and Hanks.",
      "year" : 1990
    }, {
      "title" : "A synopsis of linguistic theory 19301955",
      "author" : [ "J.R. Firth." ],
      "venue" : "Studies in linguistic analysis pages 1–32.",
      "citeRegEx" : "Firth.,? 1957",
      "shortCiteRegEx" : "Firth.",
      "year" : 1957
    }, {
      "title" : "Sufficient Dimensionality Reduction",
      "author" : [ "Amir Globerson", "Naftali Tishby." ],
      "venue" : "Journal of Machine Learning Research 3:1307–1331.",
      "citeRegEx" : "Globerson and Tishby.,? 2003",
      "shortCiteRegEx" : "Globerson and Tishby.",
      "year" : 2003
    }, {
      "title" : "node2vec: Scalable Feature Learning for Networks",
      "author" : [ "Aditya Grover", "Jure Leskovec." ],
      "venue" : "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pages 855–864.",
      "citeRegEx" : "Grover and Leskovec.,? 2016",
      "shortCiteRegEx" : "Grover and Leskovec.",
      "year" : 2016
    }, {
      "title" : "Linguistic Regularities in Sparse and Explicit Word Representations",
      "author" : [ "Omer Levy", "Yoav Goldberg." ],
      "venue" : "Proceedings of the Eighteenth Conference on Computational Natural Language Learning. pages 171–180.",
      "citeRegEx" : "Levy and Goldberg.,? 2014",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Efficient Estimation of Word Representations in Vector Space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed Representations of Words and Phrases and their Compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "Advances in Neural Information Processing Systems 26: 27th Annual Conference",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig." ],
      "venue" : "Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics,",
      "citeRegEx" : "Mikolov et al\\.,? 2013c",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Three New Graphical Models for Statistical Language Modelling",
      "author" : [ "Andriy Mnih", "Geoffrey Hinton." ],
      "venue" : "Proceedings of the 24th International Conference on Machine Learning. ACM, pages 641–648.",
      "citeRegEx" : "Mnih and Hinton.,? 2007",
      "shortCiteRegEx" : "Mnih and Hinton.",
      "year" : 2007
    }, {
      "title" : "GloVe: Global Vectors for Word Representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP). pages 1532– 1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Zipf’s word frequency law in natural language: A critical review and future directions",
      "author" : [ "Steven T. Piantadosi." ],
      "venue" : "Psychonomic bulletin & review 21(5):1112–1130.",
      "citeRegEx" : "Piantadosi.,? 2014",
      "shortCiteRegEx" : "Piantadosi.",
      "year" : 2014
    }, {
      "title" : "An improved model of semantic similarity based on lexical co-occurence",
      "author" : [ "Douglas L.T. Rohde", "Laura M. Gonnerman", "David C. Plaut." ],
      "venue" : "Communications of the ACM 8:627–633.",
      "citeRegEx" : "Rohde et al\\.,? 2006",
      "shortCiteRegEx" : "Rohde et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "A key insight in that regard is to posit that the meaning of a word is captured by “the company it keeps” (Firth, 1957) and, therefore, that two words that keep company with similar words are likely to be similar themselves.",
      "startOffset" : 106,
      "endOffset" : 119
    }, {
      "referenceID" : 14,
      "context" : "In more sophisticated methods cooccurrences are reweighed to suppress the effect of more frequent words (Rohde et al., 2006) and/or to emphasize pairs of words whose co-occurrence frequency maximally deviates from the independence assumption (Church and Hanks, 1990).",
      "startOffset" : 104,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : ", 2006) and/or to emphasize pairs of words whose co-occurrence frequency maximally deviates from the independence assumption (Church and Hanks, 1990).",
      "startOffset" : 125,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "(Bengio et al., 2003) already proposed (i) associating with each vocabulary word a feature vector, (ii) expressing the probability function of word sequences in terms of the feature vectors of the words in the sequence, and (iii) learning simultaneously the vectors and the parameters of the probability function.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "(see below) whose main departure from (Bengio et al., 2003) was to follow the suggestion of (Mnih and Hinton, 2007) and tradeaway the expressive capacity of general neuralnetwork models for the scalability to very large corpora afforded by the more restricted class of log-linear models.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 11,
      "context" : ", 2003) was to follow the suggestion of (Mnih and Hinton, 2007) and tradeaway the expressive capacity of general neuralnetwork models for the scalability to very large corpora afforded by the more restricted class of log-linear models.",
      "startOffset" : 40,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : ", “man” + “royal” = “king” (Mikolov et al., 2013c).",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "While there has been much interest in using algebraic operations on word vectors to carry out semantic operations like composition, the only published work which attempts a rigorous theoretical understanding of this phenomenon is (Arora et al., 2016).",
      "startOffset" : 230,
      "endOffset" : 250
    }, {
      "referenceID" : 5,
      "context" : "In fact, the connection we establish between the Skip-gram and Sufficient Dimensionality Reduction model of (Globerson and Tishby, 2003) More generally, it suffices that the word vectors have certain properties consistent with this sampling process.",
      "startOffset" : 108,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "Further, the context word c in the Skip-Gram model essentially serves the role that the discourse vector does in the PMI model of (Arora et al., 2016): the words neighboring c are selected with probability proportional to eu T c vw .",
      "startOffset" : 130,
      "endOffset" : 150
    }, {
      "referenceID" : 0,
      "context" : "When an analogous assumption to that of (Arora et al., 2016) is made, that the context words are uniformly distributed, we prove that the composition operator reduces to vector addition.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "As word frequencies are typically much closer to a Zipf distribution (Piantadosi, 2014), the uniformity assumption of Theorem 2 is not realistic.",
      "startOffset" : 69,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : "That said, we feel it is important to point out that, as reported in (Mikolov et al., 2013b), additivity captures compositionality more accurately when the training set is manipulated so that the prior distribution of the words is made closer to uniform.",
      "startOffset" : 69,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : "Using composition to solve analogies It has been observed that word vectors trained using nonlinear models like SG tend to encode semantic relationships between words as linear relationships between the word vectors (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014).",
      "startOffset" : 216,
      "endOffset" : 289
    }, {
      "referenceID" : 12,
      "context" : "Using composition to solve analogies It has been observed that word vectors trained using nonlinear models like SG tend to encode semantic relationships between words as linear relationships between the word vectors (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014).",
      "startOffset" : 216,
      "endOffset" : 289
    }, {
      "referenceID" : 7,
      "context" : "Using composition to solve analogies It has been observed that word vectors trained using nonlinear models like SG tend to encode semantic relationships between words as linear relationships between the word vectors (Mikolov et al., 2013b; Pennington et al., 2014; Levy and Goldberg, 2014).",
      "startOffset" : 216,
      "endOffset" : 289
    }, {
      "referenceID" : 5,
      "context" : "To do so, we draw a connection between SkipGram and the Sufficient Dimensionality Reduction (SDR) factorization of Globerson and Tishby (Globerson and Tishby, 2003).",
      "startOffset" : 136,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "The SDR model learns optimal3 embeddings for discrete random variables X and Y without assuming any parametric form on the distributions of X and Y , and it is useful in a variety of applications, including information retrieval, document classification, and association analysis (Globerson and Tishby, 2003).",
      "startOffset" : 280,
      "endOffset" : 308
    }, {
      "referenceID" : 5,
      "context" : "This connection is significant for two reasons: first, the original algorithm of (Globerson and Tishby, 2003) for learning SDR embeddings is expensive, as it involves information projections.",
      "startOffset" : 81,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "This implies that Skip-Gram specific approximation heuristics like negativesampling, hierarchical softmas, and Glove, which are believed to return high-quality approximations to Skip-Gram parameters (Mikolov et al., 2013b; Pennington et al., 2014), can be used to efficiently approximate SDR model parameters.",
      "startOffset" : 199,
      "endOffset" : 247
    }, {
      "referenceID" : 12,
      "context" : "This implies that Skip-Gram specific approximation heuristics like negativesampling, hierarchical softmas, and Glove, which are believed to return high-quality approximations to Skip-Gram parameters (Mikolov et al., 2013b; Pennington et al., 2014), can be used to efficiently approximate SDR model parameters.",
      "startOffset" : 199,
      "endOffset" : 247
    }, {
      "referenceID" : 5,
      "context" : "Second, (Globerson and Tishby, 2003) argues for the optimality of the SDR embedding in any domain where the training information on X and Y",
      "startOffset" : 8,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "consists of their coocurrence statistics; this optimality and the Skip-Gram/SDR connection argues for the use of Skip-Gram approximations in such domains, and supports the positive experimental results that have been observed in applications in network science (Grover and Leskovec, 2016), proteinomics (Asgari and Mofrad, 2015), and other fields.",
      "startOffset" : 261,
      "endOffset" : 288
    }, {
      "referenceID" : 1,
      "context" : "consists of their coocurrence statistics; this optimality and the Skip-Gram/SDR connection argues for the use of Skip-Gram approximations in such domains, and supports the positive experimental results that have been observed in applications in network science (Grover and Leskovec, 2016), proteinomics (Asgari and Mofrad, 2015), and other fields.",
      "startOffset" : 303,
      "endOffset" : 328
    } ],
    "year" : 2017,
    "abstractText" : "In recent years word-embedding models have gained great popularity due to their remarkable performance on several tasks, including word analogy questions and caption generation. An unexpected “sideeffect” of such models is that their vectors often exhibit compositionality, i.e., adding two word-vectors results in a vector that is only a small angle away from the vector of a word representing the semantic composite of the original words, e.g., “man” + “royal” = “king”. In this work we provide a mathematical formalism for compositionality, and a theoretical justification for the presence of additive compositionality in word vectors learned using the Skip-Gram model. In particular, we show that additive compositionality holds in an even stricter sense (small distance rather than small angle) under certain assumptions on the process generating the corpus. As a corollary, we explain the success of vector calculus for solving word analogies. When these assumptions do not hold, we show that compositionality is no longer additive, and provide the correct composition operator. Finally, we establish a connection between the Skip-Gram model and the Sufficient Dimensionality Reduction (SDR) framework of Globerson and Tishby. SDR models provide information-theoretically optimal symbol-embeddings for problems where the training data takes the form of co-occurrence statistics. We prove that the parameters of Skip-Gram models can be readily modified to obtain the parameters of SDR models by simply adding information on symbol frequencies. This shows that the Skip-Gram model is essentially learning optimal word embeddings in the sense of Globerson and Tishby. Further, it implies that the heuristics commonly used to approximately fit Skip-Gram models can be used to fit SDR models.",
    "creator" : "LaTeX with hyperref package"
  }
}