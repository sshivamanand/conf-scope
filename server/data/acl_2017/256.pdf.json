{
  "name" : "256.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "The dialog manager is one of the key components of dialog systems, which is responsible for modeling the decision-making process. Specifically, it typically takes a new utterance and the dialog context as input, and generates discourse-level decisions (Bohus and Rudnicky, 2003; Williams and Young, 2007). Advanced dialog managers usually have a list of potential actions that enable them to have diverse behavior during a conversation, e.g. different strategies to recover from non-understanding (Yu et al., 2016). However, the conventional approach of designing a dialog manager (Williams and Young, 2007) does not\nscale well to open-domain conversation models because of the vast quantity of possible decisions. Thus, there has been a growing interest in applying encoder-decoder models (Sutskever et al., 2014) for modeling open-domain conversation (Vinyals and Le, 2015; Serban et al., 2016a). The basic approach treats a conversation as a transduction task, in which the dialog history is the source sequence and the next response is the target sequence. The model is then trained end-to-end on large conversation corpora using the maximum-likelihood estimation (MLE) objective without the need for manual crafting.\nHowever recent research has found that encoder-decoder models tend to generate generic and dull responses (e.g., I don’t know), rather than meaningful and specific answers (Li et al., 2015; Serban et al., 2016b). There have been many attempts to explain and solve this limitation, and they can be broadly divided into two categories (see Section 2 for details): (1) the first category argues that the dialog history is only one of the factors that decide the next response. Other features should be extracted and provided to the models as conditionals in order to generate more specific responses (Xing et al., 2016; Li et al., 2016a); (2) the second category aims to improve the encoder-decoder model itself, including decoding with beam search and its variations (Wiseman and Rush, 2016), encouraging responses that have long-term payoff (Li et al., 2016b), etc.\nBuilding upon the past work in dialog managers and encoder-decoder models, the key idea of this paper is to model dialogs as a one-to-many problem at the discourse level. Previous studies indicate that there are many factors in open-domain dialogs that decide the next response, and it is nontrivial to extract all of them. Intuitively, given a similar dialog history (and other observed inputs), there may exist many valid responses (at the\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ndiscourse-level), each corresponding to a certain configuration of the latent variables that are not presented in the input. To uncover the potential responses, we strive to model a probabilistic distribution over the distributed utterance embeddings of the potential responses using a latent variable (Figure 1). This allows us to generate diverse responses by drawing samples from the learned distribution and reconstruct their words via a decoder neural network.\nFigure 1: Given A’s question, there exists many valid responses from B for different assumptions of the latent variables, e.g., B’s hobby.\nSpecifically, our contributions are three-fold: 1. We present a novel neural dialog model adapted from conditional variational autoencoders (CVAE) (Yan et al., 2015; Sohn et al., 2015), which introduces a latent variable that can capture discourse-level variations as described above 2. We propose Knowledge-Guided CVAE (kgCVAE), which enables easy integration of expert knowledge and results in performance improvement and model interpretability. 3. We develop a training method in addressing the difficulty of optimizing CVAE for natural language generation (Bowman et al., 2015). We evaluate our models on human-human conversation data and yield promising results in: (a) generating appropriate and discourse-level diverse responses, and (b) showing that the proposed training method is more effective than the previous techniques."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our work is related to both recent advancement in encoder-decoder dialog models and generative models based on CVAE."
    }, {
      "heading" : "2.1 Encoder-decoder Dialog Models",
      "text" : "Since the emergence of the neural dialog model, the problem of output diversity has received much attention in the research community. Ideal output responses should be both coherent and diverse. However, most models end up with generic and dull responses. To tackle this problem, one line of research has focused on augmenting the input of encoder-decoder models with richer context information, in order to generate more spe-\ncific responses. Li et al., (2016a) captured speakers’ characteristics by encoding background information and speaking style into the distributed embeddings, which are used to re-rank the generated response from an encoder-decoder model. Xing et al., (2016) maintain topic encoding based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003) of the conversation to encourage the model to output more topic coherent responses.\nOn the other hand, many attempts have also been made to improve the architecture of encoderdecoder models. Li et al,. (2015) proposed to optimize the standard encoder-decoder by maximizing the mutual information between input and output, which in turn reduces generic responses. This approach penalized unconditionally high frequency responses, and favored responses that have high conditional probability given the input. Wiseman and Rush (2016) focused on improving the decoder network by alleviating the biases between training and testing. They introduced a searchbased loss that directly optimizes the networks for beam search decoding. The resulting model achieves better performance on word ordering, parsing and machine translation. Besides improving beam search, Li et al., (2016b) pointed out that the MLE objective of an encoder-decoder model is unable to approximate the real-world goal of the conversation. Thus, they initialized a encoderdecoder model with MLE objective and leveraged reinforcement learning to fine tune the model by optimizing three heuristic rewards functions: informativity, coherence, and ease of answering."
    }, {
      "heading" : "2.2 Conditional Variational Autoencoder",
      "text" : "The variational autoencoder (VAE) (Kingma and Welling, 2013; Rezende et al., 2014) is one of the most popular frameworks for image generation. The basic idea of VAE is to encode the input x into a probability distribution z instead of a point encoding in the autoencoder. Then VAE applies a decoder network to reconstruct the original input using samples from z. To generate images, VAE first obtains a sample of z from the prior distribution, e.g. N (0, I), and then produces an image via the decoder network. A more advanced model, the conditional VAE (CVAE), is a recent modification of VAE to generate diverse images conditioned on certain attributes, e.g. generating different human faces given skin color (Yan et al., 2015; Sohn et al., 2015). Inspired by CVAE, we view the dialog contexts as the conditional attributes and adapt CVAE\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nto generate diverse responses instead of images. Although VAE/CVAE has achieved impressive results in image generation, adapting this to natural language generators is non-trivial. Bowman et al., (2015) have used VAE with Long-Short Term Memory (LSTM)-based recognition and decoder networks to generate sentences from a latent Gaussian variable. They showed that their model is able to generate diverse sentences with even a greedy LSTM decoder. They also reported the difficulty of training because the LSTM decoder tends to ignore the latent variable. We refer to this issue as the vanishing latent variable problem. Serban et al., (2016b) have applied a latent variable hierarchical encoder-decoder dialog model to introduce utterance-level variations and facilitate longer responses. To improve upon the past models, we firstly introduce a novel mechanism to leverage linguistic knowledge in training end-to-end neural dialog models, and we also propose a novel training technique that mitigates the vanishing latent variable problem."
    }, {
      "heading" : "3 Proposed Models",
      "text" : "Figure 2: Graphical models of CVAE (a) and kgCVAE (b)"
    }, {
      "heading" : "3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation",
      "text" : "Each dyadic conversation can be represented via three random variables: the dialog context c (context window size k − 1), the response utterance x (the kth utterance) and a latent variable z, which is used to capture the latent distribution over the valid responses. Further, c is composed of the dialog history: the preceding k-1 utterances; conversational floor (1 if the utterance is from the same speaker of x, otherwise 0) and meta features m (e.g. the topic). We then define the conditional distribution p(x, z|c) = p(x|z, c)p(z|c) and our goal is to use deep neural networks (parametrized by θ) to approximate p(z|c) and p(x|z, c). We refer to pθ(z|c) as the prior network and pθ(x, |z, c) as the\nresponse decoder. Then the generative process of x is (Figure 2 (a)):\n1. Sample a latent variable z from the prior network pθ(z|c).\n2. Generate x through the response decoder pθ(x|z, c).\nCVAE is trained to maximize the conditional log likelihood of x given c, which involves an intractable marginalization over the latent variable z. As proposed in (Sohn et al., 2015; Yan et al., 2015), CVAE can be efficiently trained with the Stochastic Gradient Variational Bayes (SGVB) framework (Kingma and Welling, 2013) by maximizing the variational lower bound of the conditional log likelihood. We assume the z follows multivariate Gaussian distribution with a diagonal covariance matrix and introduce a recognition network qφ(z|x, c) to approximate the true posterior distribution p(z|x, c). Sohn and et al,. (2015) have shown that the variational lower bound can be written as:\nL(θ, φ;x, c) = −KL(qφ(z|x, c)‖pθ(z|c)) +Eqφ(z|c,x)[log pθ(x|z, c)] (1) ≤ log p(x|c)\nFigure 3 demonstrates an overview of our model. The utterance encoder is a bidirectional recurrent neural network (BRNN) (Schuster and Paliwal, 1997) with a gated recurrent unit (GRU) (Chung et al., 2014) to encode each utterance into fixedsize vectors by concatenating the last hidden states of the forward and backward RNN ui = [~hi, ~hi]. x is simply uk. The context encoder is a 1-layer GRU network that encodes the preceding k-1 utterances by taking u1:k−1 and the corresponding conversation floor as inputs. The last hidden state hc of the context encoder is concatenated with meta features and c = [hc,m]. Since we assume z follows isotropic Gaussian distribution, the recognition network qφ(z|x, c) ∼ N (µ, σ2I) and the prior network pθ(z|c) ∼ N (µ′, σ′2I), and then we have: [\nµ log(σ2)\n] =Wr [ x c ] + br (2)[\nµ′\nlog(σ′2)\n] = MLPp(c) (3)\nWe then use the reparametrization trick (Kingma and Welling, 2013) to obtain samples of z either\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nFigure 3: The neural network architectures for the baseline and the proposed CVAE/kgCVAE models.⊕ denotes the concatenation of the input vectors. The dashed blue connections only appear in kgCVAE.\nfrom N (z;µ, σ2I) predicted by the recognition network (training) or N (z;µ′, σ′2I) predicted by the prior network (testing). Finally, the response decoder is a 1-layer GRU network with initial state s0 =Wi[z, c]+bi. The response decoder then predicts the words in x sequentially."
    }, {
      "heading" : "3.2 Knowledge-Guided CVAE (kgCVAE)",
      "text" : "In practice, training CVAE is a challenging optimization problem and often requires large amount of data. On the other hand, past research in spoken dialog systems and discourse analysis has suggested that many linguistic cues capture crucial features in representing natural conversation. For example, dialog acts (Poesio and Traum, 1998) have been widely used in the dialog managers (Litman and Allen, 1987; Raux et al., 2005; Zhao and Eskenazi, 2016) to represent the propositional function of the system. Therefore, we conjecture that it will be beneficial for the model to learn meaningful latent z if it is provided with explicitly extracted discourse features during the training.\nIn order to incorporate the linguistic features into the basic CVAE model, we first denote the set of linguistic features as y. Then we assume that the generation of x depends on c, z and y. y relies on z and c as shown in Figure 2. Specifically, during training the initial state of the response decoder is s0 = Wi[z, c, y] + bi and the input at every step is [et, y] where et is the word embedding of tth word in x. In addition, there is an MLP to predict y′ = MLPy(z, c) based on z and c. In the testing stage, the predicted y′ is used by the response decoder instead of the oracle decoders. We denote the modified model as knowledge-guided\nCVAE (kgCVAE) and developers can add desired discourse features that they wish the latent variable z to capture. KgCVAE model is trained by maximizing:\nL(θ, φ;x, c, y) = −KL(qφ(z|x, c, y)‖Pθ(z|c)) +Eqφ(z|c,x,y)[log p(x|z, c, y)] +Eqφ(z|c,x,y)[log p(y|z, c)] (4)\nSince now the reconstruction of y is a part of the loss function, kgCVAE can more efficiently encode y-related information into z than discovering it only based on the surface-level x and c. Another advantage of kgCVAE is that it can output a highlevel label (e.g. dialog act) along with the wordlevel responses, which allows easier interpretation of the model’s outputs."
    }, {
      "heading" : "3.3 Optimization Challenges",
      "text" : "A straightforward VAE with RNN decoder fails to encode meaningful information in z due to the vanishing latent variable problem (Bowman et al., 2015). Bowman et al., (2015) proposed two solutions: (1) KL annealing: gradually increasing the weight of the KL term from 0 to 1 during training; (2) word drop decoding: setting a certain percentage of the target words to 0. We found that CVAE suffers from the same issue when the decoder is an RNN. Also we did not consider word drop decoding because Bowman et al,. (2015) have shown that it may hurt the performance when the drop rate is too high.\nAs a result, we propose a simple yet novel technique to tackle the vanishing latent variable problem: bag-of-word loss. The idea is to introduce\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nan auxiliary loss that requires the decoder network to predict the bag-of-words in the response x as shown in Figure 3(b). We decompose x into two variables: xo with word order and xbow without order, and assume that xo and xbow are conditionally independent given z and c: p(x, z|c) = p(xo|z, c)p(xbow|z, c)p(z|c). Due to the conditional independence assumption, the latent variable is forced to capture global information about the target response. Let f = MLPb(z, x) ∈ RV where V is vocabulary size, and we have:\nlog p(xbow|z, c) = log |x|∏ t=1 efxt∑V j e fj (5)\nwhere |x| is the length of x and xt is the word index of tth word in x. The modified variational lower bound for CVAE with bag-of-word loss is (see Appendix A for kgCVAE):\nL′(θ, φ;x, c) = L(θ, φ;x, c) +Eqφ(z|c,x,y)[log p(xbow|z, c)] (6)\nWe will show that the bag-of-word loss in Equation 6 is very effective against the vanishing latent variable and it is also complementary to the KL annealing technique."
    }, {
      "heading" : "4 Experiment Setup",
      "text" : ""
    }, {
      "heading" : "4.1 Dataset",
      "text" : "We chose the Switchboard (SW) 1 Release 2 Corpus (Godfrey and Holliman, 1997) to evaluate the proposed models. SW has 2400 two-sided telephone conversations with manually transcribed speech and alignment. In the beginning of the call, a computer operator gave the callers recorded prompts that define the desired topic of discussion. There are 70 available topics. We randomly split the data into 2316/60/62 dialogs for train/validate/test. The pre-processing includes (1) tokenize using the NLTK tokenizer (Bird et al., 2009); (2) remove non-verbal symbols and repeated words due to false starts; (3) keep the top 10K frequent word types as the vocabulary. The final data have 207, 833/5, 225/5, 481 (c, x) pairs for train/validate/test. Furthermore, a subset of SW was manually labeled with dialog acts (Stolcke et al., 2000). We extracted dialog act labels based on the dialog act recognizer proposed in (Ribeiro et al., 2015). The features include the uni-gram and bi-gram of the utterance, and the contextual features of the last 3 utterances. We trained a Support Vector Machine\n(SVM) (Suykens and Vandewalle, 1999) with linear kernel on the subset of SW with human annotations. There are 42 types of dialog acts and the SVM achieved 77.3% accuracy on held-out data. Then the rest of SW data are labelled with dialog acts using the trained SVM dialog act recognizer."
    }, {
      "heading" : "4.2 Training",
      "text" : "We trained with the following hyperparameters (according to the loss on the validate dataset): word embedding has size 200 and is shared across everywhere. We initialize the word embedding from Glove embedding pre-trained on Twitter (Pennington et al., 2014). The utterance encoder has a hidden size of 300 for each direction. The context encoder has a hidden size of 600 and the response decoder has a hidden size of 400. The prior network and the MLP for predicting y both have 1 hidden layer of size 400 and tanh non-linearity. The latent variable z has a size of 200. The context window k is 10. All the initial weights are sampled from a uniform distribution [-0.08, 0.08]. The mini-batch size is 30. The models are trained end-to-end using the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.001 and gradient clipping at 5. We selected the best models based on the variational lower bound on the validate data. Finally, we use the BOW loss along with KL annealing of 10,000 batches to achieve the best performance. Section 5.4 gives a detailed argument for the importance of the BOW loss."
    }, {
      "heading" : "5 Results",
      "text" : ""
    }, {
      "heading" : "5.1 Experiments Setup",
      "text" : "We compared three neural dialog models: a strong baseline model, CVAE, and kgCVAE. The baseline model is an encoder-decoder neural dialog model without latent variables similar to (Serban et al., 2016a). The baseline model’s encoder uses the same context encoder to encode the dialog history and the meta features as shown in Figure 3. The encoded context c is directly fed into the decoder networks as the initial state. The hyperparameters of the baseline are the same as the ones reported in Section 4.2 and the baseline is trained to minimize the standard cross entropy loss of the decoder RNN model without any auxiliary loss.\nAlso, to compare the diversity introduced by the stochasticity in the proposed latent variable versus the softmax of RNN at each decoding step, we generate N responses from the baseline by sam-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\npling from the softmax. For CVAE/kgCVAE, we sample N times from the latent z and only use greedy decoders so that the randomness comes entirely from the latent variable z."
    }, {
      "heading" : "5.2 Quantitative Analysis",
      "text" : "Automatically evaluating an open-domain generative dialog model is an open research challenge (Liu et al., 2016). Following our one-tomany hypothesis, we propose the following metrics. We assume that for a given dialog context c, there existMc reference responses rj , j ∈ [1,Mc]. Meanwhile a model can generateN hypothesis responses hi, i ∈ [1, N ]. The generalized responselevel precision/recall for a given dialog context is:\nprecision(c) =\n∑N i=1maxj∈[1,Mc]d(rj , hi)\nN\nrecall(c) =\n∑Mc j=1maxi∈[1,N ]d(rj , hi))\nMc\nwhere d(rj , hi) is a distance function which lies between 0 to 1 and measures the similarities between rj and hi. The final score is averaged over the entire test dataset and we report the performance with 3 types of distance functions in order to evaluate the systems from various linguistic points of view:\n1. Smoothed Sentence-level BLEU (Chen and Cherry, 2014): BLEU is a popular metric that measures the geometric mean of modified ngram precision with a length penalty (Papineni et al., 2002; Li et al., 2015). We use BLEU-1 to 4 as our lexical similarity metric.\n2. Cosine Distance of Bag-of-word Embedding: a simple method to obtain sentence embeddings is to take the average or extrema of all the word embeddings in the sentences (Forgues et al., 2014; Adi et al., 2016). The d(rj , hi) is the cosine distance of the two embedding vectors. We used Glove embedding described in Section 4 and denote the average method as A-bow and extrema method as E-bow.\n3. Dialog Act Match: to measure the similarity at the discourse level, the same dialogact tagger from 4.1 is applied to label all the generated responses of each model. We set d(rj , hi) = 1 if rj and hi have the same dialog acts, otherwise d(rj , hi) = 0.\nOne challenge of using the above metrics is that there is only one, rather than multiple reference responses/contexts. This impacts reliability of our measures. Inspired by (Sordoni et al., 2015), we utilized information retrieval techniques (see Appendix A) to gather 10 extra candidate reference responses/context from other conversations with the same topics. Then the 10 candidate references are filtered by two experts, which serve as the ground truth to train the reference response classifier. The result is 6.69 references in average per context. The average number of distinct reference dialog acts is 4.2. Table 1 shows the results.\nBoth CVAE and kgCVAE outperform the baseline in terms of recall in all the metrics. This confirms our hypothesis that generating responses with discourse-level diversity can lead to a more comprehensive coverage of the potential responses than promoting only word-level diversity. As for precision, we observed that the baseline has higher or similar scores than CVAE in all metrics, which is expected since the baseline tends to generate the mostly likely and safe responses repeatedly in the N hypotheses. However, kgCVAE is able to achieve the highest precision and recall in the 4 metrics at the same time (BLEU1/3/4, E-BOW). One reason for kgCVAE’s good performance is that the predicted dialog act label in kgCVAE can regularize the generation process of its RNN decoder by forcing it to generate more coherent\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nand precise words. We further analyze the precision/recall of BLEU-4 by looking at the average score versus the number of distinct reference dialog acts. A low number of distinct dialog acts represents the situation where the dialog context has a strong constraint on the range of the next response (low entropy), while a high number indicates the opposite (high-entropy). Figure 4 shows that CVAE/kgCVAE achieves significantly higher recall than the baseline in higher entropy contexts. Also it shows that CVAE suffers from lower precision, especially in low entropy contexts. Finally, kgCVAE gets higher precision than both the baseline and CVAE in the full spectrum of context entropy.\nFigure 4: BLEU-4 precision/recall vs. the number of distinct reference dialog acts."
    }, {
      "heading" : "5.3 Qualitative Analysis",
      "text" : "Table 2 shows the outputs generated from the baseline and kgCVAE. In example 1, caller A begins with an open-ended question. The kgCVAE model generated highly diverse answers that cover multiple plausible dialog acts. Further, we notice that the generated text exhibits similar dialog acts compared to the ones predicted separately by the model, implying the consistency of natural language generation based on y. On the contrary, the responses from the baseline model are limited to local n-gram variations and share a similar prefix, i.e. ”I’m”. Example 2 is a situation where caller A is telling B stories. The ground truth response is a back-channel and the range of valid answers is more constrained than example 1 since B is playing the role of a listener. The baseline successfully predicts ”uh-huh”. The kgCVAE model is also able to generate various ways of back-channeling. This implies that the latent z is able to capture context-sensitive variations, i.e. in low-entropy dialog contexts modeling lexical diversity while in high-entropy ones modeling discourse-level diversity. Moreover, kgCVAE is occasionally able to generate more sophisticated grounding (sample 4) beyond a simple back-channel, which is also an acceptable response given the dialog context.\nIn addition, past work (Kingma and Welling, 2013) has shown that the recognition network is able to learn to cluster high-dimension data, so we conjecture that posterior z outputted from the recognition network should cluster the responses into meaningful groups. Figure 5 visualizes the posterior z of responses in the test dataset in 2D space using t-SNE (Maaten and Hinton, 2008). We found that the learned latent space is highly correlated with the dialog act and length of responses, which confirms our assumption.\nFigure 5: t-SNE visualization of the posterior z for test responses with top 8 frequent dialog acts. The size of circle represents the response length."
    }, {
      "heading" : "5.4 Results for Bag-of-Word Loss",
      "text" : "Finally, we evaluate the effectiveness of bag-ofword (BOW) loss for training VAE/CVAE with the RNN decoder. To compare with past work (Bowman et al., 2015), we conducted the same language modelling (LM) task on Penn Treebank using VAE. The network architecture is same except we use GRU instead of LSTM. We compared four different training setups: (1) standard VAE without any heuristics; (2) VAE with KL annealing (KLA); (3) VAE with BOW loss; (4) VAE with both BOW loss and KLA. Intuitively, a well trained model should lead to a low reconstruction loss and small but non-trivial KL cost. For all models with KLA, the KL weight increases linearly from 0 to 1 in the first 5000 batches.\nTable 3 shows the reconstruction perplexity and the KL cost on the test dataset. The standard VAE fails to learn a meaningful latent variable by having a KL cost close to 0 and a reconstruction perplexity similar to a small LSTM LM (Zaremba et al., 2014). KLA helps to improve the recon-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nExample 1-Topic: Recycling Context: A: are they doing a lot of recycling out in Georgia? Target-B (statement): well at my workplace we have palaces for aluminium cans and we have a separate trash can for recyclable paper Baseline+Sampling kgCVAE+Greedy 1. well I’m a graduate student and have two kids 1. (non-understand) pardon 2. well I was in last year and so we’ve had lots of recycling 2. (statement) oh you’re not going to have a curbside pick up here 3. I’m not sure 3. (statement) okay I am sure about a recycling center 4. well I don’t know I just moved here in new york 4. (yes-answer) yeah so\nExample 2-Topic: Child Care Context: A: you know a private home to take their children to when they’re young until they hit the preschool age and they Target-B (backchannel): uh-huh Baseline+Sampling kgCVAE+Greedy 1. um - hum 1. (backchannel) uh-huh 2. yeah 2. (turn-exit) um-hum 3. um - hum 3. (backchannel) yeah 4. uh-huh 4. (statement) oh yeah I think that’s part of the problem\nTable 2: Generated responses from the baselines and kgCVAE in two examples. KgCVAE also provides the predicted dialog act for each response. The context only shows the last utterance due to space limit (the actual context window size is 10).\nstruction loss, but it requires early stopping since the models will fall back to the standard VAE after the KL weight becomes 1. At last, the models with BOW loss achieved significantly lower perplexity and larger KL cost.\nModel Perplexity KL cost Standard 122.0 0.05 KLA 111.5 2.02 BOW 97.72 7.41 BOW+KLA 73.04 15.94\nTable 3: The reconstruction perplexity and KL terms on Penn Treebank test set.\nFigure 6 visualizes the evolution of the KL cost. We can see that for the standard model, the KL cost crashes to 0 at the beginning of training and never recovers. On the contrary, the model with only KLA learns to encode substantial information in latent z when the KL cost weight is small. However, after the KL weight is increased to 1 (after 5000 batch), the model once again decides to ignore the latent z and falls back to the naive implementation. The model with BOW loss, however, consistently converges to a non-trivial KL cost even without KLA, which confirms the importance of BOW loss for training latent variable models with the RNN decoder. Last but not least, our experiments showed that the conclusions drawn from LM using VAE also apply to training CVAE/kgCVAE, so we used BOW loss together with KLA for all previous experiments."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In conclusion, we identified the one-to-many nature of open-domain conversation and proposed two novel models that show superior performance in generating diverse and appropriate responses at the discourse level. While the current paper addresses diversifying responses in respect to dialogue acts, this work is part of a larger research direction that targets leveraging both past linguistic findings and the learning power of deep neural networks to learn better representation of the latent factors in dialog. In turn, the output of this novel neural dialog model will be easier to explain and control by humans. In addition to dialog acts, we plan to apply our kgCVAE model to capture other different linguistic phenomena including sentiment, named entities,etc. Last but not least, the recognition network in our model will serve as the foundation for designing a datadriven dialog manager, which automatically discovers useful high-level intents. All of the above suggest a promising research direction.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    }, {
      "heading" : "A Supplemental Material",
      "text" : "Variational Lower Bound for kgCVAE We assume that even with the presence of linguistic feature y regarding x, the prediction of xbow still only depends on the z and c. Therefore, we have:\nL(θ, φ;x, c, y) = −KL(qφ(z|x, c, y)‖Pθ(z|c)) +Eqφ(z|c,x,y)[log p(x|z, c, y)] +Eqφ(z|c,x,y)[log p(y|z, c)] +Eqφ(z|c,x,y)[log p(xbow|z, c)]\n(7)\nCollection of Multiple Reference Responses We collected multiple reference responses for each dialog context in the test set by information retrieval techniques combining with traditional a machine learning method. First, we encode the dialog history using Term Frequency-Inverse Document Frequency (TFIDF) (Salton and Buckley, 1988) weighted bag-of-words into vector representation h. Then we denote the topic of the conversation as t and denote f as the conversation floor, i.e. if the speakers of the last utterance in the dialog history and response utterance are the same f = 1 otherwise f = 0. Then we computed the similarity d(ci, cj) between two dialog contexts using:\nd(ci, cj) = 1(ti = tj)1(ti = tj) hi · hj ||hi||||hj || (8)\nUnlike past work (Sordoni et al., 2015), this similarity function only cares about the distance in the context and imposes no constraints on the response, therefore is suitbale for finding diverse responses regarding to the same dialog context. Secondly, for each dialog context in the test set, we retrieved the 10 nearest neighbors from the training set and treated the responses from the training set as candidate reference responses. Thirdly, we further sampled 240 context-responses pairs from 5481 pairs in the total test set and post-processed the selected candidate responses by two human computational linguistic experts who were told to give a binary label for each candidate response about whether the response is appropriate regarding its dialog context. The filtered lists then served as the ground truth to train our reference response classifier. For the next step, we extracted bigrams, part-of-speech bigrams and word part-of-speech\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\npairs from both dialogue contexts and candidate reference responses with rare threshold for feature extraction being set to 20. Then L2-regularized logistic regression with 10-fold cross validation was applied as the machine learning algorithm. Cross validation accuracy on the human-labelled data was 71%. Finally, we automatically annotated the rest of test set with this trained classifier and the resulting data were used for model evaluation."
    } ],
    "references" : [ {
      "title" : "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks",
      "author" : [ "Yossi Adi", "Einat Kermany", "Yonatan Belinkov", "Ofer Lavi", "Yoav Goldberg." ],
      "venue" : "arXiv preprint arXiv:1608.04207 .",
      "citeRegEx" : "Adi et al\\.,? 2016",
      "shortCiteRegEx" : "Adi et al\\.",
      "year" : 2016
    }, {
      "title" : "Natural language processing with Python",
      "author" : [ "Steven Bird", "Ewan Klein", "Edward Loper." ],
      "venue" : "” O’Reilly Media, Inc.”.",
      "citeRegEx" : "Bird et al\\.,? 2009",
      "shortCiteRegEx" : "Bird et al\\.",
      "year" : 2009
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M Blei", "Andrew Y Ng", "Michael I Jordan." ],
      "venue" : "Journal of machine Learning research 3(Jan):993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Ravenclaw: Dialog management using hierarchical task decomposition and an expectation agenda",
      "author" : [ "Dan Bohus", "Alexander I Rudnicky" ],
      "venue" : null,
      "citeRegEx" : "Bohus and Rudnicky.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bohus and Rudnicky.",
      "year" : 2003
    }, {
      "title" : "Generating sentences from a continuous space",
      "author" : [ "Samuel R Bowman", "Luke Vilnis", "Oriol Vinyals", "Andrew M Dai", "Rafal Jozefowicz", "Samy Bengio." ],
      "venue" : "arXiv preprint arXiv:1511.06349 .",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "A systematic comparison of smoothing techniques for sentencelevel bleu",
      "author" : [ "Boxing Chen", "Colin Cherry." ],
      "venue" : "ACL 2014 page 362.",
      "citeRegEx" : "Chen and Cherry.,? 2014",
      "shortCiteRegEx" : "Chen and Cherry.",
      "year" : 2014
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1412.3555 .",
      "citeRegEx" : "Chung et al\\.,? 2014",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Bootstrapping dialog systems with word embeddings",
      "author" : [ "Gabriel Forgues", "Joelle Pineau", "Jean-Marie Larchevêque", "Réal Tremblay." ],
      "venue" : "NIPS, Modern Machine Learning and Natural Language Processing Workshop.",
      "citeRegEx" : "Forgues et al\\.,? 2014",
      "shortCiteRegEx" : "Forgues et al\\.",
      "year" : 2014
    }, {
      "title" : "Switchboard-1 release 2",
      "author" : [ "John J Godfrey", "Edward Holliman." ],
      "venue" : "Linguistic Data Consortium, Philadelphia .",
      "citeRegEx" : "Godfrey and Holliman.,? 1997",
      "shortCiteRegEx" : "Godfrey and Holliman.",
      "year" : 1997
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980 .",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Autoencoding variational bayes",
      "author" : [ "Diederik P Kingma", "Max Welling." ],
      "venue" : "arXiv preprint arXiv:1312.6114 .",
      "citeRegEx" : "Kingma and Welling.,? 2013",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "arXiv preprint arXiv:1510.03055 .",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "A persona-based neural conversation model",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "arXiv preprint arXiv:1603.06155 .",
      "citeRegEx" : "Li et al\\.,? 2016a",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Deep reinforcement learning for dialogue generation",
      "author" : [ "Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky." ],
      "venue" : "arXiv preprint arXiv:1606.01541 .",
      "citeRegEx" : "Li et al\\.,? 2016b",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "A plan recognition model for subdialogues in conversations",
      "author" : [ "Diane J Litman", "James F Allen." ],
      "venue" : "Cognitive science 11(2):163–200.",
      "citeRegEx" : "Litman and Allen.,? 1987",
      "shortCiteRegEx" : "Litman and Allen.",
      "year" : 1987
    }, {
      "title" : "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "author" : [ "Chia-Wei Liu", "Ryan Lowe", "Iulian V Serban", "Michael Noseworthy", "Laurent Charlin", "Joelle Pineau." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of Machine Learning Research 9(Nov):2579–2605.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "EMNLP. volume 14, pages 1532–",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards an axiomatization of dialogue acts",
      "author" : [ "Massimo Poesio", "David Traum." ],
      "venue" : "Proceedings of the Twente Workshop on the Formal Semantics and Pragmatics of Dialogues (13th Twente Workshop on Language Technology. Citeseer.",
      "citeRegEx" : "Poesio and Traum.,? 1998",
      "shortCiteRegEx" : "Poesio and Traum.",
      "year" : 1998
    }, {
      "title" : "Lets go public! taking a spoken dialog system to the real world",
      "author" : [ "Antoine Raux", "Brian Langner", "Dan Bohus", "Alan W Black", "Maxine Eskenazi." ],
      "venue" : "in Proc. of Interspeech 2005. Citeseer.",
      "citeRegEx" : "Raux et al\\.,? 2005",
      "shortCiteRegEx" : "Raux et al\\.",
      "year" : 2005
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra." ],
      "venue" : "arXiv preprint arXiv:1401.4082 .",
      "citeRegEx" : "Rezende et al\\.,? 2014",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "The influence of context on dialogue act recognition",
      "author" : [ "Eugénio Ribeiro", "Ricardo Ribeiro", "David Martins de Matos." ],
      "venue" : "arXiv preprint arXiv:1506.00839 .",
      "citeRegEx" : "Ribeiro et al\\.,? 2015",
      "shortCiteRegEx" : "Ribeiro et al\\.",
      "year" : 2015
    }, {
      "title" : "Termweighting approaches in automatic text retrieval",
      "author" : [ "Gerard Salton", "Christopher Buckley." ],
      "venue" : "Information processing & management 24(5):513– 523.",
      "citeRegEx" : "Salton and Buckley.,? 1988",
      "shortCiteRegEx" : "Salton and Buckley.",
      "year" : 1988
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "Mike Schuster", "Kuldip K Paliwal." ],
      "venue" : "IEEE Transactions on Signal Processing 45(11):2673–2681.",
      "citeRegEx" : "Schuster and Paliwal.,? 1997",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "Proceedings of the 30th AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Serban et al\\.,? 2016a",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1605.06069 .",
      "citeRegEx" : "Serban et al\\.,? 2016b",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning structured output representation using deep conditional generative models",
      "author" : [ "Kihyuk Sohn", "Honglak Lee", "Xinchen Yan." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 3483–3491.",
      "citeRegEx" : "Sohn et al\\.,? 2015",
      "shortCiteRegEx" : "Sohn et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural network approach to context-sensitive generation of conversational responses",
      "author" : [ "Alessandro Sordoni", "Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Sordoni et al\\.,? 2015",
      "shortCiteRegEx" : "Sordoni et al\\.",
      "year" : 2015
    }, {
      "title" : "Dialogue act modeling for automatic tagging and recognition",
      "author" : [ "Andreas Stolcke", "Noah Coccaro", "Rebecca Bates", "Paul Taylor", "Carol Van Ess-Dykema", "Klaus Ries", "Elizabeth Shriberg", "Daniel Jurafsky", "Rachel Martin", "Marie Meteer" ],
      "venue" : null,
      "citeRegEx" : "Stolcke et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Stolcke et al\\.",
      "year" : 2000
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems. pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Least squares support vector machine classifiers",
      "author" : [ "Johan AK Suykens", "Joos Vandewalle." ],
      "venue" : "Neural processing letters 9(3):293–300.",
      "citeRegEx" : "Suykens and Vandewalle.,? 1999",
      "shortCiteRegEx" : "Suykens and Vandewalle.",
      "year" : 1999
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le." ],
      "venue" : "arXiv preprint arXiv:1506.05869 .",
      "citeRegEx" : "Vinyals and Le.,? 2015",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    }, {
      "title" : "Partially observable markov decision processes for spoken dialog systems",
      "author" : [ "Jason D Williams", "Steve Young." ],
      "venue" : "Computer Speech & Language 21(2):393–422.",
      "citeRegEx" : "Williams and Young.,? 2007",
      "shortCiteRegEx" : "Williams and Young.",
      "year" : 2007
    }, {
      "title" : "Sequence-to-sequence learning as beam-search optimization",
      "author" : [ "Sam Wiseman", "Alexander M Rush." ],
      "venue" : "arXiv preprint arXiv:1606.02960 .",
      "citeRegEx" : "Wiseman and Rush.,? 2016",
      "shortCiteRegEx" : "Wiseman and Rush.",
      "year" : 2016
    }, {
      "title" : "Topic augmented neural response generation with a joint attention mechanism",
      "author" : [ "Chen Xing", "Wei Wu", "Yu Wu", "Jie Liu", "Yalou Huang", "Ming Zhou", "Wei-Ying Ma." ],
      "venue" : "arXiv preprint arXiv:1606.08340 .",
      "citeRegEx" : "Xing et al\\.,? 2016",
      "shortCiteRegEx" : "Xing et al\\.",
      "year" : 2016
    }, {
      "title" : "Attribute2image: Conditional image generation from visual attributes",
      "author" : [ "Xinchen Yan", "Jimei Yang", "Kihyuk Sohn", "Honglak Lee." ],
      "venue" : "arXiv preprint arXiv:1512.00570 .",
      "citeRegEx" : "Yan et al\\.,? 2015",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2015
    }, {
      "title" : "Strategy and policy learning for nontask-oriented conversational systems",
      "author" : [ "Zhou Yu", "Ziyu Xu", "Alan W Black", "Alex I Rudnicky." ],
      "venue" : "17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. volume 2, page 7.",
      "citeRegEx" : "Yu et al\\.,? 2016",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1409.2329 .",
      "citeRegEx" : "Zaremba et al\\.,? 2014",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards end-to-end learning for dialog state tracking and management using deep reinforcement learning",
      "author" : [ "Tiancheng Zhao", "Maxine Eskenazi." ],
      "venue" : "arXiv preprint arXiv:1606.02560 .",
      "citeRegEx" : "Zhao and Eskenazi.,? 2016",
      "shortCiteRegEx" : "Zhao and Eskenazi.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Specifically, it typically takes a new utterance and the dialog context as input, and generates discourse-level decisions (Bohus and Rudnicky, 2003; Williams and Young, 2007).",
      "startOffset" : 122,
      "endOffset" : 174
    }, {
      "referenceID" : 33,
      "context" : "Specifically, it typically takes a new utterance and the dialog context as input, and generates discourse-level decisions (Bohus and Rudnicky, 2003; Williams and Young, 2007).",
      "startOffset" : 122,
      "endOffset" : 174
    }, {
      "referenceID" : 37,
      "context" : "different strategies to recover from non-understanding (Yu et al., 2016).",
      "startOffset" : 55,
      "endOffset" : 72
    }, {
      "referenceID" : 33,
      "context" : "However, the conventional approach of designing a dialog manager (Williams and Young, 2007) does not scale well to open-domain conversation models because of the vast quantity of possible decisions.",
      "startOffset" : 65,
      "endOffset" : 91
    }, {
      "referenceID" : 30,
      "context" : "Thus, there has been a growing interest in applying encoder-decoder models (Sutskever et al., 2014) for modeling open-domain conversation (Vinyals and Le, 2015; Serban et al.",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 32,
      "context" : ", 2014) for modeling open-domain conversation (Vinyals and Le, 2015; Serban et al., 2016a).",
      "startOffset" : 46,
      "endOffset" : 90
    }, {
      "referenceID" : 25,
      "context" : ", 2014) for modeling open-domain conversation (Vinyals and Le, 2015; Serban et al., 2016a).",
      "startOffset" : 46,
      "endOffset" : 90
    }, {
      "referenceID" : 11,
      "context" : ", I don’t know), rather than meaningful and specific answers (Li et al., 2015; Serban et al., 2016b).",
      "startOffset" : 61,
      "endOffset" : 100
    }, {
      "referenceID" : 26,
      "context" : ", I don’t know), rather than meaningful and specific answers (Li et al., 2015; Serban et al., 2016b).",
      "startOffset" : 61,
      "endOffset" : 100
    }, {
      "referenceID" : 35,
      "context" : "Other features should be extracted and provided to the models as conditionals in order to generate more specific responses (Xing et al., 2016; Li et al., 2016a); (2) the second category aims to improve the encoder-decoder model itself, including decoding with beam search and its variations (Wiseman and Rush, 2016), encouraging responses that have long-term payoff (Li et al.",
      "startOffset" : 123,
      "endOffset" : 160
    }, {
      "referenceID" : 12,
      "context" : "Other features should be extracted and provided to the models as conditionals in order to generate more specific responses (Xing et al., 2016; Li et al., 2016a); (2) the second category aims to improve the encoder-decoder model itself, including decoding with beam search and its variations (Wiseman and Rush, 2016), encouraging responses that have long-term payoff (Li et al.",
      "startOffset" : 123,
      "endOffset" : 160
    }, {
      "referenceID" : 34,
      "context" : ", 2016a); (2) the second category aims to improve the encoder-decoder model itself, including decoding with beam search and its variations (Wiseman and Rush, 2016), encouraging responses that have long-term payoff (Li et al.",
      "startOffset" : 139,
      "endOffset" : 163
    }, {
      "referenceID" : 13,
      "context" : ", 2016a); (2) the second category aims to improve the encoder-decoder model itself, including decoding with beam search and its variations (Wiseman and Rush, 2016), encouraging responses that have long-term payoff (Li et al., 2016b), etc.",
      "startOffset" : 214,
      "endOffset" : 232
    }, {
      "referenceID" : 36,
      "context" : "We present a novel neural dialog model adapted from conditional variational autoencoders (CVAE) (Yan et al., 2015; Sohn et al., 2015), which introduces a latent variable that can capture discourse-level variations as described above 2.",
      "startOffset" : 96,
      "endOffset" : 133
    }, {
      "referenceID" : 27,
      "context" : "We present a novel neural dialog model adapted from conditional variational autoencoders (CVAE) (Yan et al., 2015; Sohn et al., 2015), which introduces a latent variable that can capture discourse-level variations as described above 2.",
      "startOffset" : 96,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "We develop a training method in addressing the difficulty of optimizing CVAE for natural language generation (Bowman et al., 2015).",
      "startOffset" : 109,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : ", (2016) maintain topic encoding based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003) of the conversation to encourage the model to output more topic coherent responses.",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "Li et al., (2016a) captured speakers’ characteristics by encoding background information and speaking style into the distributed embeddings, which are used to re-rank the generated response from an encoder-decoder model.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 10,
      "context" : "Li et al., (2016a) captured speakers’ characteristics by encoding background information and speaking style into the distributed embeddings, which are used to re-rank the generated response from an encoder-decoder model. Xing et al., (2016) maintain topic encoding based on Latent Dirichlet Allocation (LDA) (Blei et al.",
      "startOffset" : 0,
      "endOffset" : 241
    }, {
      "referenceID" : 2,
      "context" : ", (2016) maintain topic encoding based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003) of the conversation to encourage the model to output more topic coherent responses. On the other hand, many attempts have also been made to improve the architecture of encoderdecoder models. Li et al,. (2015) proposed to optimize the standard encoder-decoder by maximizing the mutual information between input and output, which in turn reduces generic responses.",
      "startOffset" : 77,
      "endOffset" : 305
    }, {
      "referenceID" : 2,
      "context" : ", (2016) maintain topic encoding based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003) of the conversation to encourage the model to output more topic coherent responses. On the other hand, many attempts have also been made to improve the architecture of encoderdecoder models. Li et al,. (2015) proposed to optimize the standard encoder-decoder by maximizing the mutual information between input and output, which in turn reduces generic responses. This approach penalized unconditionally high frequency responses, and favored responses that have high conditional probability given the input. Wiseman and Rush (2016) focused on improving the decoder network by alleviating the biases between training and testing.",
      "startOffset" : 77,
      "endOffset" : 627
    }, {
      "referenceID" : 2,
      "context" : ", (2016) maintain topic encoding based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003) of the conversation to encourage the model to output more topic coherent responses. On the other hand, many attempts have also been made to improve the architecture of encoderdecoder models. Li et al,. (2015) proposed to optimize the standard encoder-decoder by maximizing the mutual information between input and output, which in turn reduces generic responses. This approach penalized unconditionally high frequency responses, and favored responses that have high conditional probability given the input. Wiseman and Rush (2016) focused on improving the decoder network by alleviating the biases between training and testing. They introduced a searchbased loss that directly optimizes the networks for beam search decoding. The resulting model achieves better performance on word ordering, parsing and machine translation. Besides improving beam search, Li et al., (2016b) pointed out that the MLE objective of an encoder-decoder model is unable to approximate the real-world goal of the conversation.",
      "startOffset" : 77,
      "endOffset" : 971
    }, {
      "referenceID" : 10,
      "context" : "2 Conditional Variational Autoencoder The variational autoencoder (VAE) (Kingma and Welling, 2013; Rezende et al., 2014) is one of the most popular frameworks for image generation.",
      "startOffset" : 72,
      "endOffset" : 120
    }, {
      "referenceID" : 21,
      "context" : "2 Conditional Variational Autoencoder The variational autoencoder (VAE) (Kingma and Welling, 2013; Rezende et al., 2014) is one of the most popular frameworks for image generation.",
      "startOffset" : 72,
      "endOffset" : 120
    }, {
      "referenceID" : 36,
      "context" : "generating different human faces given skin color (Yan et al., 2015; Sohn et al., 2015).",
      "startOffset" : 50,
      "endOffset" : 87
    }, {
      "referenceID" : 27,
      "context" : "generating different human faces given skin color (Yan et al., 2015; Sohn et al., 2015).",
      "startOffset" : 50,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "Bowman et al., (2015) have used VAE with Long-Short Term Memory (LSTM)-based recognition and decoder networks to generate sentences from a latent Gaussian variable.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : "Bowman et al., (2015) have used VAE with Long-Short Term Memory (LSTM)-based recognition and decoder networks to generate sentences from a latent Gaussian variable. They showed that their model is able to generate diverse sentences with even a greedy LSTM decoder. They also reported the difficulty of training because the LSTM decoder tends to ignore the latent variable. We refer to this issue as the vanishing latent variable problem. Serban et al., (2016b) have applied a latent variable hierarchical encoder-decoder dialog model to introduce utterance-level variations and facilitate longer responses.",
      "startOffset" : 0,
      "endOffset" : 461
    }, {
      "referenceID" : 27,
      "context" : "As proposed in (Sohn et al., 2015; Yan et al., 2015), CVAE can be efficiently trained with the Stochastic Gradient Variational Bayes (SGVB) framework (Kingma and Welling, 2013) by maximizing the variational lower bound of the conditional log likelihood.",
      "startOffset" : 15,
      "endOffset" : 52
    }, {
      "referenceID" : 36,
      "context" : "As proposed in (Sohn et al., 2015; Yan et al., 2015), CVAE can be efficiently trained with the Stochastic Gradient Variational Bayes (SGVB) framework (Kingma and Welling, 2013) by maximizing the variational lower bound of the conditional log likelihood.",
      "startOffset" : 15,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : ", 2015), CVAE can be efficiently trained with the Stochastic Gradient Variational Bayes (SGVB) framework (Kingma and Welling, 2013) by maximizing the variational lower bound of the conditional log likelihood.",
      "startOffset" : 105,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : ", 2015), CVAE can be efficiently trained with the Stochastic Gradient Variational Bayes (SGVB) framework (Kingma and Welling, 2013) by maximizing the variational lower bound of the conditional log likelihood. We assume the z follows multivariate Gaussian distribution with a diagonal covariance matrix and introduce a recognition network qφ(z|x, c) to approximate the true posterior distribution p(z|x, c). Sohn and et al,. (2015) have shown that the variational lower bound can be written as:",
      "startOffset" : 106,
      "endOffset" : 431
    }, {
      "referenceID" : 24,
      "context" : "The utterance encoder is a bidirectional recurrent neural network (BRNN) (Schuster and Paliwal, 1997) with a gated recurrent unit (GRU) (Chung et al.",
      "startOffset" : 73,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "The utterance encoder is a bidirectional recurrent neural network (BRNN) (Schuster and Paliwal, 1997) with a gated recurrent unit (GRU) (Chung et al., 2014) to encode each utterance into fixedsize vectors by concatenating the last hidden states of the forward and backward RNN ui = [~ hi, ~ hi].",
      "startOffset" : 136,
      "endOffset" : 156
    }, {
      "referenceID" : 10,
      "context" : "We then use the reparametrization trick (Kingma and Welling, 2013) to obtain samples of z either",
      "startOffset" : 40,
      "endOffset" : 66
    }, {
      "referenceID" : 19,
      "context" : "For example, dialog acts (Poesio and Traum, 1998) have been widely used in the dialog managers (Litman and Allen, 1987; Raux et al.",
      "startOffset" : 25,
      "endOffset" : 49
    }, {
      "referenceID" : 14,
      "context" : "For example, dialog acts (Poesio and Traum, 1998) have been widely used in the dialog managers (Litman and Allen, 1987; Raux et al., 2005; Zhao and Eskenazi, 2016) to represent the propositional function of the system.",
      "startOffset" : 95,
      "endOffset" : 163
    }, {
      "referenceID" : 20,
      "context" : "For example, dialog acts (Poesio and Traum, 1998) have been widely used in the dialog managers (Litman and Allen, 1987; Raux et al., 2005; Zhao and Eskenazi, 2016) to represent the propositional function of the system.",
      "startOffset" : 95,
      "endOffset" : 163
    }, {
      "referenceID" : 39,
      "context" : "For example, dialog acts (Poesio and Traum, 1998) have been widely used in the dialog managers (Litman and Allen, 1987; Raux et al., 2005; Zhao and Eskenazi, 2016) to represent the propositional function of the system.",
      "startOffset" : 95,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "3 Optimization Challenges A straightforward VAE with RNN decoder fails to encode meaningful information in z due to the vanishing latent variable problem (Bowman et al., 2015).",
      "startOffset" : 154,
      "endOffset" : 175
    }, {
      "referenceID" : 4,
      "context" : "3 Optimization Challenges A straightforward VAE with RNN decoder fails to encode meaningful information in z due to the vanishing latent variable problem (Bowman et al., 2015). Bowman et al., (2015) proposed two solutions: (1) KL annealing: gradually increasing the weight of the KL term from 0 to 1 during training; (2) word drop decoding: setting a certain percentage of the target words to 0.",
      "startOffset" : 155,
      "endOffset" : 199
    }, {
      "referenceID" : 4,
      "context" : "3 Optimization Challenges A straightforward VAE with RNN decoder fails to encode meaningful information in z due to the vanishing latent variable problem (Bowman et al., 2015). Bowman et al., (2015) proposed two solutions: (1) KL annealing: gradually increasing the weight of the KL term from 0 to 1 during training; (2) word drop decoding: setting a certain percentage of the target words to 0. We found that CVAE suffers from the same issue when the decoder is an RNN. Also we did not consider word drop decoding because Bowman et al,. (2015) have shown that it may hurt the performance when the drop rate is too high.",
      "startOffset" : 155,
      "endOffset" : 545
    }, {
      "referenceID" : 8,
      "context" : "1 Dataset We chose the Switchboard (SW) 1 Release 2 Corpus (Godfrey and Holliman, 1997) to evaluate the proposed models.",
      "startOffset" : 59,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "The pre-processing includes (1) tokenize using the NLTK tokenizer (Bird et al., 2009); (2) remove non-verbal symbols and repeated words due to false starts; (3) keep the top 10K frequent word types as the vocabulary.",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 29,
      "context" : "Furthermore, a subset of SW was manually labeled with dialog acts (Stolcke et al., 2000).",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 22,
      "context" : "We extracted dialog act labels based on the dialog act recognizer proposed in (Ribeiro et al., 2015).",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 31,
      "context" : "We trained a Support Vector Machine (SVM) (Suykens and Vandewalle, 1999) with linear kernel on the subset of SW with human annotations.",
      "startOffset" : 42,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : "We initialize the word embedding from Glove embedding pre-trained on Twitter (Pennington et al., 2014).",
      "startOffset" : 77,
      "endOffset" : 102
    }, {
      "referenceID" : 9,
      "context" : "The models are trained end-to-end using the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 25,
      "context" : "The baseline model is an encoder-decoder neural dialog model without latent variables similar to (Serban et al., 2016a).",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 15,
      "context" : "2 Quantitative Analysis Automatically evaluating an open-domain generative dialog model is an open research challenge (Liu et al., 2016).",
      "startOffset" : 118,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "Smoothed Sentence-level BLEU (Chen and Cherry, 2014): BLEU is a popular metric that measures the geometric mean of modified ngram precision with a length penalty (Papineni et al.",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 17,
      "context" : "Smoothed Sentence-level BLEU (Chen and Cherry, 2014): BLEU is a popular metric that measures the geometric mean of modified ngram precision with a length penalty (Papineni et al., 2002; Li et al., 2015).",
      "startOffset" : 162,
      "endOffset" : 202
    }, {
      "referenceID" : 11,
      "context" : "Smoothed Sentence-level BLEU (Chen and Cherry, 2014): BLEU is a popular metric that measures the geometric mean of modified ngram precision with a length penalty (Papineni et al., 2002; Li et al., 2015).",
      "startOffset" : 162,
      "endOffset" : 202
    }, {
      "referenceID" : 7,
      "context" : "Cosine Distance of Bag-of-word Embedding: a simple method to obtain sentence embeddings is to take the average or extrema of all the word embeddings in the sentences (Forgues et al., 2014; Adi et al., 2016).",
      "startOffset" : 166,
      "endOffset" : 206
    }, {
      "referenceID" : 0,
      "context" : "Cosine Distance of Bag-of-word Embedding: a simple method to obtain sentence embeddings is to take the average or extrema of all the word embeddings in the sentences (Forgues et al., 2014; Adi et al., 2016).",
      "startOffset" : 166,
      "endOffset" : 206
    }, {
      "referenceID" : 28,
      "context" : "Inspired by (Sordoni et al., 2015), we utilized information retrieval techniques (see Appendix A) to gather 10 extra candidate reference responses/context from other conversations with the same topics.",
      "startOffset" : 12,
      "endOffset" : 34
    }, {
      "referenceID" : 10,
      "context" : "In addition, past work (Kingma and Welling, 2013) has shown that the recognition network is able to learn to cluster high-dimension data, so we conjecture that posterior z outputted from the recognition network should cluster the responses into meaningful groups.",
      "startOffset" : 23,
      "endOffset" : 49
    }, {
      "referenceID" : 16,
      "context" : "Figure 5 visualizes the posterior z of responses in the test dataset in 2D space using t-SNE (Maaten and Hinton, 2008).",
      "startOffset" : 93,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "To compare with past work (Bowman et al., 2015), we conducted the same language modelling (LM) task on Penn Treebank using VAE.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 38,
      "context" : "The standard VAE fails to learn a meaningful latent variable by having a KL cost close to 0 and a reconstruction perplexity similar to a small LSTM LM (Zaremba et al., 2014).",
      "startOffset" : 151,
      "endOffset" : 173
    } ],
    "year" : 2017,
    "abstractText" : "While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses. Unlike past work that has focused on diversifying the output of the decoder at word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that captures the discourse-level diversity in the encoder. Our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance. Finally, the training procedure is improved by introducing a bag-of-word loss. Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence in discourse-level decision-making.",
    "creator" : "LaTeX with hyperref package"
  }
}