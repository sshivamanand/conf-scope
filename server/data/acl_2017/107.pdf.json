{
  "name" : "107.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Weakly Supervised Cross-Lingual Named Entity Recognition via Effective Annotation and Representation Projection",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n010\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n060\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Named entity recognition (NER) is a fundamental information extraction task that automatically detects named entities in text and classifies them into pre-defined entity types such as PERSON, ORGANIZATION, GPE (GeoPolitical Entities), EVENT,\nLOCATION, DATE, etc. NER provides essential inputs for many information extraction applications, including relation extraction, entity linking, question answering and text mining. Building fast and accurate NER systems is a crucial step towards enabling large-scale automated information extraction and knowledge discovery on the huge volumes of electronic documents existing today.\nThe state-of-the-art NER systems are statistical machine learning models including maximum entropy Markov models (MEMMs) (McCallum et al., 2000), conditional random fields (CRFs) (Lafferty et al., 2001) and neural networks (Collobert et al., 2011; Lample et al., 2016). To achieve high accuracy, an NER system needs to be trained with a large amount of human-annotated data, and is often supplied with language-specific resources (gazetteers, word clusters). Annotating NER data by human is rather expensive and time-consuming, and can be quite difficult for a new language. This places a big challenge in building NER systems of multiple languages for supporting multilingual information extraction applications.\nThe difficulty of acquiring human-annotated multilingual NER data raises the following question: given a well-trained NER system in a source language (e.g., English), how can one go about extending it to a new language with decent performance and no human annotation in the target language? There are mainly two types of approaches for building weakly supervised cross-lingual NER systems.\nThe first type of approaches are based on annotation projection, e.g., (Zitouni and Florian, 2008; Ehrmann et al., 2011). These approaches require parallel corpora between a source language (usually English) and a target language with alignment information. The source-language sentences are labeled with NER tags, either by human as in (Ehrmann et al., 2011), or by a source-language NER system as in (Zitouni and Florian, 2008). The\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nNER tags are then projected to the target-language sentences via the alignment information. Note that the quality of the projection-labeled data heavily depends on the alignment accuracy of the parallel sentences, while this factor was not considered in the previous work when determining which sentence pairs should be selected for projection.\nThe second type of approaches are based on direct model transfer, e.g., (Täckström et al., 2012; Tsai et al., 2016). The basic idea is to train a single NER system in the source language with language-independent features, so the system can be applied to other languages using those universal features. Täckström et al. (2012) uses monolingual data in source/target languages and aligned parallel data between source and target languages to build cross-lingual word clusters. The crosslingual word clusters (together with part-of-speech tags) are then used to generate universal features. Tsai et al. (2016) applies the cross-lingual wikifier developed in (Tsai and Roth, 2016) and multilingual Wikipedia dump to generate languageindependent labels (FreeBase types and Wikipedia categories) for n-grams in a document, and those labels are used as universal features.\nIn this paper, we make the following contributions to weakly supervised cross-lingual NER with no human annotation in the target languages. First, for the annotation projection approach, we develop a heuristic data selection scheme that seeks to select good-quality projection-labeled NER data from comparable corpora. Experimental results show that the data selection scheme can significantly improve the accuracy of the targetlanguage NER system (compared with no data selection) when the alignment accuracy is low and the projection-labeled data are noisy.\nSecond, we propose a new approach for direct NER model transfer based on representation projection. It projects word representations in vector space (word embeddings) from a target language to a source language, to create a universal representation of the words in different languages. Under this approach, the NER system trained for the source language can be directly applied to the target language without the need for re-training.\nFinally, we design two co-decoding schemes that combine the outputs (views) of the two projection-based systems to produce an output that is more accurate than the outputs of individual systems. We evaluate the performance of the pro-\nposed approaches on both in-house and open NER data sets for a number of target languages. The results show that the combined systems outperform the state-of-the-art cross-lingual NER approaches proposed in Täckström et al. (2012) and Tsai et al. (2016) on the CoNLL NER test data (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003).\nWe organize the paper as follows. In Section 2 we introduce three NER models that are used in the paper. In Section 3 we present an annotation projection approach with effective data selection. In Section 4 we propose a representation projection approach for direct NER model transfer. In Section 5 we describe two co-decoding schemes that combine the outputs of two projection-based approaches in a smart way. We evaluate the performance of the proposed approaches in Section 6 and conclude the paper in Section 7."
    }, {
      "heading" : "2 NER Models",
      "text" : "The NER task can be formulated as a sequence labeling problem: given a sequence of words x1, ..., xn, we want to infer the NER label li for each word xi, 1 ≤ i ≤ n. In this section we introduce three NER models that are used in the paper."
    }, {
      "heading" : "2.1 CRFs and MEMMs",
      "text" : "Conditional random fields (CRFs) are a class of discriminative probabilistic graphical models that provide powerful tools for labeling sequential data (Lafferty et al., 2001). CRFs learn a conditional probability model pλ(l|x) from a set of labeled training data, where x = (x1, ...,xn) is a random sequence of input words, l = (l1, ..., ln) is the sequence of label variables (NER tags) for x, and l has certain Markov properties conditioned on x. Specifically, a general-order CRF with order o assumes that label variable li is dependent on a fixed number o of previous label variables li−1, ..., li−o, with the following conditional distribution:\npλ(l|x) = e ∑n i=1 ∑K k=1 λkfk(li,li−1,...,li−o,x)\nZ(x) (1)\nwhere fk’s are the feature functions, λk’s are the weights (parameters) of the feature functions, and Z(x) is the normalization constant. The exponential parametric form can be derived using the maximum entropy principle (Berger et al., 1996). When o = 1, we have a first-order CRF which is also known as a linear-chain CRF.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nGiven a set of labeled training data D = (x(j), l(j))j=1,...,N , the training of a CRF model is to find an optimal set of parameters λ∗ that maximize the conditional log-likelihood of the data:\nλ∗ = arg max λ ΣNj=1 log pλ(l (j)|x(j)) (2)\nOnce we obtain λ∗, we can use the trained model pλ∗(l|x) to decode the most likely label sequence l∗ for any new input sequence of words x (via the Viterbi algorithm for example):\nl∗ = arg max l pλ∗(l|x) (3)\nA related conditional probability model, called maximum entropy Markov model (MEMM) (McCallum et al., 2000), assumes that l is a Markov chain conditioned on x:\npλ(l|x) = n∏ i=1 pλ(li|li−1, ..., li−o,x)\n= n∏ i=1 e ∑K k=1 λkfk(li,li−1,...,li−o,x) Z(li−1, ..., li−o,x) (4)\nThe main difference between CRFs and MEMMs is that CRFs normalize the conditional distribution over the whole sequence as in (1), while MEMMs normalize the conditional distribution per token as in (4). As a result, CRFs can better handle the label bias problem (Lafferty et al., 2001). This benefit, however, comes at a price. The training time of order-o CRFs grows exponentially (O(Mo+1)) with the number of output labels M , which is typically slow even for moderate-size training data ifM is large. In contrast, the training time of order-o MEMMs is linear (O(M)) with respect to M independent of o, so it can handle larger training data with higher order of dependency. We have implemented a linear-chain CRF model and a general-order MEMM model."
    }, {
      "heading" : "2.2 Neural Networks",
      "text" : "With the increasing popularity of distributed (vector) representations of words, neural network models have been recently applied to tackle many NLP tasks including NER (Collobert et al., 2011).\nWe have implemented a feedforward neural network model which maximizes the log-likelihood of the tag sequence similar to that of (Collobert et al., 2011). We use a locally normalized model (the conditional distribution is normalized per token as in MEMMs) and introduce context dependency by conditioning on the previously assigned\ntags. We use the target word and surrounding context as features. We do not employ other common features such as gazetteers or character-level representations as such features might not be readily available or might not transfer to other languages.\nWe have deployed two neural network architectures. The first one (called NN1) uses the word embedding of a word as the input. We also devise a second architecture (called NN2) which adds a smoothing layer that computes the cosine similarity between a word embedding and a fixed set of prototype vectors (learned during training) and returns a weighted average of these prototype vectors as the input. In our experiments we find that with the smoothing layer, NN2 tends to have a more balanced precision and recall than NN1. Both networks have one hidden layer, with sigmoid and softmax activation functions on the hidden and output layers respectively. The two neural network models are depicted in Figure 1."
    }, {
      "heading" : "3 Annotation Projection Approach",
      "text" : "Traditional annotation projection approaches require parallel corpora between a source language and a target language with alignment information. In this paper, we develop a heuristic scheme that can effectively select good-quality projectionlabeled NER data from noisy comparable corpora (not necessarily parallel corpora). We use English as the source language.\nSuppose we have sentence pairs (X,Y) between English and a target language, where X includes N English sentences x(1), ...,x(N), Y includes N target-language sentences y(1), ...,y(N), and y(i) is aligned to x(i) via an alignment model, 1 ≤ i ≤ N . We use a sentence pair (x,y) as an example to illustrate how the annotation projection procedure works, where x = (x1, x2, ..., xs) is an English sentence, and y = (y1, y2, ..., yt) is a target-language sentence that is aligned to x.\nAnnotation Projection Procedure\n1. Apply the English NER system on the En-\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nglish sentence x to generate the NER tags l = (l1, l2, ..., ls) for x.\n2. Project the NER tags to the target-language sentence y using the alignment information. Specifically, if a sequence of English words (xi, ..., xi+p) is aligned to a sequence of target-language words (yj , ..., yj+q), and (xi, ..., xi+p) is recognized (by the English NER system) as an entity with NER tag l, then (yj , ..., yj+q) is labeled with l1. Let l′ = (l′1, l ′ 2, ..., l ′ t) be the projected NER\ntags for the target-language sentence y.\nWe can apply the annotation projection procedure on all the sentence pairs (X,Y), to generate projected NER tags L′ for the target-language sentences Y. (Y, L′) are automatically labeled NER data with no human annotation in the target language. One can use those projection-labeled data to train an NER system in the target language. The quality of such weakly labeled NER data, and consequently the accuracy of the target-language NER system, depend on both 1) the accuracy of the English NER system, and 2) the alignment accuracy of the comparable corpora.\nIf some of the projection-labeled data have bad quality and we use all the data for weakly supervised learning, the accuracy of the target-language NER system will be adversely affected by those bad-quality data. We would like to design effective data selection schemes that can select goodquality projection-labeled data from noisy data, to improve the accuracy of the annotation projection approach for cross-lingual NER."
    }, {
      "heading" : "3.1 Data Selection Scheme",
      "text" : "We first design a metric to measure the annotation quality of a projection-labeled sentence in the target language. We construct a frequency table T which includes all the entities in the projectionlabeled target-language sentences. For each entity e, T also includes the projected NER tags for e and the frequency T(e, l) that entity e is labeled with tag l. Table 1 shows a snapshot of the frequency table where the target language is Portuguese.\nWe use the frequency T(e, l) to measure the reliability of labeling entity e with tag l in the target language. The intuition is that if an entity e is labeled by a tag l with higher frequency than other\n1If the IOB (Inside, Outside, Beginning) tagging format is used, then (yj , yj+1, ..., yj+q) is labeled with (B-l, I-l,...,I-l).\nEntity Name NER Tag Frequency Estados Unidos GPE 0.853 Estados Unidos ORGANIZATION 0.143 Estados Unidos PEOPLE 0.001 Estados Unidos PRODUCT 0.001 Estados Unidos TITLEWORK 0.001 Estados Unidos EVENT 0.001\nTable 1: A snapshot of the frequency table where the target language is Portuguese. Estados Unidos means United States. The correct NER tag for Estados Unidos is GPE which has the highest frequency in the weakly labeled data.\ntags in the projection-labeled data, it is more likely that the annotation is correct, because the source (English) NER system and the alignment system are independent statistical models and it is less likely that both systems make consistent errors simultaneously generating the same wrong tag.\nBased on the frequency scores, we calculate the quality score of a projection-labeled targetlanguage sentence y by averaging the frequency scores of the projected entities in the sentence:\nq(y) = Σe∈yT(e, l\n′(e))\nn(y) (5)\nwhere l′(e) is the projected NER tag for e, and n(y) is the total number of entities in sentence y.\nWe use q(y) to measure the annotation quality of sentence y. In addition, we use n(y) to measure the amount of annotation information contained in sentence y. Based on these two metrics, we design a heuristic data selection scheme which selects projection-labeled sentences in the target language that satisfy the following condition:\nq(y) ≥ q; n(y) ≥ n (6)\nwhere q is a quality score threshold and n is an entity number threshold. We can tune the two parameters to make tradeoffs among the annotation quality of the selected sentences, the annotation information contained in the selected sentences, and the total number of sentence selected."
    }, {
      "heading" : "3.2 Improvements",
      "text" : "We evaluate the effectiveness of the data selection scheme via experiments for 4 target languages: Japanese, Korean, German and Portuguese. We have comparable corpora between English and each target language with alignment information (ranging from 2.0M to 5.5M tokens). For each\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nLanguage (q, n) Training Size F1 Score Japanese (0, 0) 5.10M 41.1\n(0.8, 4) 691K 55.0 Korean (0, 0) 4.72M 18.7 (0.9, 3) 246K 38.7 German (0, 0) 5.47M 65.7\n(0.5, 3) 3.37M 66.4 Portuguese (0, 0) 2.17M 61.7\n(0.5, 4) 1.46M 62.9\nTable 2: Performance comparison of NER systems trained without data selection ((q, n) = (0, 0)) and with data selection (F1 score shown in bold).\ntarget language, we also have a set of humanannotated NER data (ranging from 30K to 45K tokens) which are served as the test data for evaluating the target-language NER system.\nThe source (English) NER system is a linearchain CRF model which achieves an accuracy of 88.9 F1 score on an independent NER test set. The alignment systems between English and the target languages are maximum entropy models (Ittycheriah and Roukos, 2005), with an accuracy of 69.4 F1 score for Japanese, 62.0 F1 score for Korean, 76.1 F1 score for German and 88.0 F1 score for Portuguese, respectively, on independent alignment test sets.\nFor each target language, we compare an NER system trained with all the projection-labeled data with no data selection (i.e., q = 0 and n = 0 in the data selection scheme) and an NER system trained with projection-labeled data selected by the data selection scheme. For the data selection scheme, we have tried different threshold parameters q and n and reported the best numbers in Table 2. Both NER systems are 2nd-order MEMM models2 which use the same template of features.\nThe results are shown in Table 2. For different target languages, we use the same source (English) NER system for annotation projection, so the differences in the improvements are mainly due to the alignment accuracy of the comparable corpora between English and different target languages. When the alignment accuracy is low (e.g., as for Japanese and Korean) and hence the projection-labeled NER data are quite noisy, the proposed data selection scheme is very effective in selecting good-quality projection-labeled data and the improvement is significant: +13.9 F1 score for\n2CRFs cannot handle training data with a few millions of words, since our NER system has over 50 entity types, and the training time of CRFs grows at least quadratically in the number of entity types.\nJapanese and +20.0 F1 score for Korean."
    }, {
      "heading" : "4 Representation Projection Approach",
      "text" : "In this section, we present a new approach for direct NER model transfer based on representation projection. Under this approach, we train a single English NER system that uses only word embeddings as input representations. We create mapping functions which can map words in any language into English and we simply use the English NER system to decode. In particular, by mapping all languages into English, we are using one universal NER system and we do not need to re-train when a new language is added."
    }, {
      "heading" : "4.1 Monolingual Word Embeddings",
      "text" : "We first build vector representations of words (word embeddings) for a language from monolingual data. We have developed a variant of the CBOW word2vec model (Mikolov et al., 2013a), which concatenates the context words surrounding a target word x using a weight ( 1dist(x,xc) ) that decays with the distance of a context word xc to x.\nWe train 300-dimensional word embeddings for English. Following (Mikolov et al., 2013b), we use larger dimensional embeddings for the target languages, namely 800. We train word2vec for 1 epoch for English/Spanish and 5 epochs for the rest of the languages for which we have less data."
    }, {
      "heading" : "4.2 Cross-Lingual Embedding Mappings",
      "text" : "We learn cross-lingual word embedding mappings similarly to (Mikolov et al., 2013b). For a target language f , we first extract a small training dictionary from a phrase table that includes word-to-word alignments between English and the target language. The dictionary contains English and target-language word pairs with weights: (xi, yi, wi)i=1,...,n, where xi is an English word, yi is a target-language word, and the weight wi = P (xi|yi) is the empirical probability of xi given yi as extracted from the phrase table.\nGiven monolingual word embeddings for English and the target language f , where ui ∈ Rd1 is the vector representation for English word xi, vi ∈ Rd2 is the vector representation for target word yi, we find a linear mapping M f e by solving the following weighted least squares problem using the dictionary as the training data:\nMfe = arg min M Σni=1wi||ui −Mvi||2 (7)\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nwhere we generalize the formulation in (Mikolov et al., 2013b) by adding weights to the word pairs, so that more frequent translation pairs are of higher importance. Using Mfe , for any new word in the target language with vector representation v, we can project it into English as the vector Mfev.\nThe training dictionary plays a key role in finding an effective cross-lingual embedding mapping. To control the size of the dictionary, we only include word pairs with a minimum frequency threshold. We set the threshold to obtain approximately 5K to 6K unique word pairs for a target language, as experiments show that larger dictionaries might harm the performance of representation projection for direct NER model transfer."
    }, {
      "heading" : "4.3 Direct NER Model Transfer",
      "text" : "The source (English) NER system is a neural network model (with architecture NN1 or NN2) that uses only word embedding features (embeddings of a word and its surrounding context) in the English vector space. Model transfer is achieved simply by projecting the target language word embeddings into the English vector space and decoding these using the English NER system.\nMore specifically, given the word embeddings of a sequence of words in a target language f , (v1, ...,vt), we project them into the English vector space by applying the cross-lingual linear mapping Mfe : (M f ev1, ...,M f evt). The English NER system is then applied on the projected input to produce the NER tags. Words which are not in the target vocabulary are projected into their English embeddings if found in the English vocabulary, or into an NER-trained unk vector otherwise."
    }, {
      "heading" : "4.4 Related Work",
      "text" : "Many ways of obtaining cross-lingual embeddings have been proposed in the literature, following one of two main approaches. On approach builds monolingual representations separately and then brings them to the same space typically using a seed dictionary (Mikolov et al., 2013b; Faruqui and Dyer, 2014). Another line of work builds inter-lingual representations simultaneously, often by generating mixed language corpora using the supervision at hand (aligned sentences, documents, etc.) (Vulić and Moens, 2015; Gouws et al., 2015). We opt for the first solution in this paper because of its flexibility: we can map all languages to English rather than requiring separate embeddings for each language pair. Additionally we are\nable to easily add a new language without any constraints on the type of data needed. Note that although we do not specifically create inter-lingual representations, by training mappings to the common language, English, we are able to map words in different languages to a common space.\nSimilar approaches for cross-lingual model transfer have been applied to other tasks such as document classification, syntactic parsing and part-of-speech tagging (Klementiev et al., 2012; Guo et al., 2015; Gouws and Søgaard, 2015). We expect these ideas to carry over to the NER task, in particular due to the semantic nature of the task which may allow for more natural embeddingbased cross-lingual transfer."
    }, {
      "heading" : "5 Co-Decoding",
      "text" : "We have developed two weakly supervised systems for cross-lingual NER, which are trained with different data using different models (MEMM model for annotation projection and neural network model for representation projection). We would like to design a co-decoding scheme that can combine the outputs (views) of the two systems to produce an output that is more accurate than the outputs of individual systems.\nSince both systems are statistical models and can produce confidence scores (probabilities), a natural co-decoding scheme is to compare the confidence scores of the NER tags generated by the two systems and select the tags with higher confidences scores. However, confidence scores of two weakly trained systems may not be directly comparable, especially when comparing O tags with non-O tags. We consider an exclude-O confidence-based co-decoding scheme which we find to be more effective empirically. It is similar to the pure confidence-based scheme, with the only difference that it prefers a non-O tag to an O tag independent of the confidence scores.\nIn experiments we find that the annotation projection system tends to have a high precision and low recall, i.e., it detects few entities, but for the detected entities the accuracy is high. The representation projection system tends to have a more balanced precision and recall. Based on this observation, we develop a rank-based co-decoding scheme that gives higher priority to the annotation projection system. The scheme will first include all the entities detected by the annotation projection system. Then, it will add all the entities de-\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nJapanese Approach P R F1\nAnnotation-Projection (AP) 71.3 44.7 55.0 Representation-Projection (NN1) 71.5 36.6 48.4 Representation-Projection (NN2) 59.9 42.4 49.7 Co-Decoding (Conf): AP+NN1 66.2 49.7 56.7 Co-Decoding (Rank): AP+NN1 69.4 52.1 59.6 Co-Decoding (Conf): AP+NN2 60.5 53.8 56.9 Co-Decoding (Rank): AP+NN2 63.2 55.9 59.3\nSupervised (272K) 84.5 80.9 82.7 Korean\nApproach P R F1 Annotation-Projection (AP) 69.3 26.9 38.7 Representation-Projection (NN1) 61.1 19.3 29.3 Representation-Projection (NN2) 59.3 37.1 45.6 Co-Decoding (Conf): AP+NN1 62.0 32.7 42.8 Co-Decoding (Rank): AP+NN1 66.9 35.1 46.0 Co-Decoding (Conf): AP+NN2 59.1 44.6 50.8 Co-Decoding (Rank): AP+NN2 60.7 45.2 51.8\nSupervised (90K) 72.5 44.0 54.8 German\nApproach P R F1 Annotation-Projection (AP) 77.6 58.0 66.4 Representation-Projection (NN1) 69.0 48.8 57.2 Representation-Projection (NN2) 63.7 66.1 64.9 Co-Decoding (Conf): AP+NN1 69.1 60.3 64.4 Co-Decoding (Rank): AP+NN1 73.3 63.6 68.1 Co-Decoding (Conf): AP+NN2 64.9 71.1 67.8 Co-Decoding (Rank): AP+NN2 66.9 72.0 69.3\nSupervised (125K) 77.8 68.1 72.6 Portuguese\nApproach P R F1 Annotation-Projection (AP) 84.4 50.1 62.9 Representation-Projection (NN1) 70.5 47.6 56.8 Representation-Projection (NN2) 66.0 63.4 64.7 Co-Decoding (Conf): AP+NN1 72.0 55.8 62.8 Co-Decoding (Rank): AP+NN1 77.7 60.0 67.7 Co-Decoding (Conf): AP+NN2 68.5 67.4 68.0 Co-Decoding (Rank): AP+NN2 71.3 68.6 69.9\nSupervised (173K) 79.8 71.9 75.6\nTable 3: In-house NER test data: Precision, recall, F1 score on exact phrasal matches. The highest F1 score among all weakly supervised approaches is shown in bold. Same for Tables 4 and 5.\ntected by the representation projection system that do not conflict3 with entities detected by the annotation projection system (to improve recall)."
    }, {
      "heading" : "6 Experiments",
      "text" : "In this section, we evaluate the performance of the proposed approaches for cross-lingual NER, including the 2 projection-based approaches and the 2 co-decoding schemes for combining them: (1) The annotation projection approach (AP) with heuristic data selection;\n3Two entities detected by two different systems conflict with each other if either 1) the two entities have different boundaries but overlap with each other; or 2) the two entities have the same boundary but different NER tags.\n(2) The representation projection approach (with two neural network architectures NN1 and NN2); (3) The exclude-O confidence-based co-decoding scheme; (4) The rank-based co-decoding scheme."
    }, {
      "heading" : "6.1 NER Data Sets",
      "text" : "We have used various NER data sets for evaluation. The first group includes in-house humanannotated NER data for four languages: Japanese, Korean, German and Portuguese. The data have over 50 entity types. The main motivation of deploying such a fine-grained entity type set is to build cognitive question answering applications on top of the NER systems. The sizes of test data sets are ranging from 30K to 45K tokens.\nThe second group includes open humanannotated NER data for Spanish, Dutch and German from the CoNLL 02/03 development/test data sets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The CoNLL data have 4 entity types: PER, ORG, LOC and MISC. The sizes of the development/test data sets are ranging from 35K to 70K tokens. The development data are used for tuning the parameters of the models."
    }, {
      "heading" : "6.2 Evaluation for In-House NER Data",
      "text" : "In Table 3 we show the results of different approaches for the in-house NER data. For the annotation projection approach, the source (English) NER system is a linear-chain CRF model trained with 320K tokens of human-annotated English newswire data, and the target-language NER system is a 2nd-order MEMM model trained with 691K, 246K, 3.37M and 1.46M tokens of projection-labeled data for Japanese, Korean, German and Portuguese, respectively. For representation projection, the source (English) NER systems are neural network models with architectures NN1 and NN2 (Figure 1), both trained with 320K tokens of human-annotated English newswire data.\nThe results show that the annotation projection approach (AP) has a relatively high precision and low recall. For representation projection, neural network model NN2 (with a smoothing layer) is better than NN1, and NN2 tends to have a more balanced precision and recall. The rank-based codecoding scheme is more effective for combining the two projection-based approaches. In particular, the rank-based co-decoding scheme that combines AP and NN2 achieves the highest F1 score among all the weakly supervised approaches for\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nSpanish Approach P R F1\nAnnotation-Projection (AP) 64.4 55.9 59.8 Representation-Projection (NN1) 63.9 52.2 57.4 Representation-Projection (NN2) 55.3 51.8 53.5 Co-Decoding (Conf): AP+NN1 62.6 64.1 63.4 Co-Decoding (Rank): AP+NN1 61.9 62.9 62.4 Co-Decoding (Conf): AP+NN2 56.8 61.4 59.0 Co-Decoding (Rank): AP+NN2 58.7 61.6 60.2\nSupervised (264K) 81.3 79.8 80.6 Dutch\nApproach P R F1 Annotation-Projection (AP) 71.1 60.6 65.4 Representation-Projection (NN1) 82.6 47.4 60.3 Representation-Projection (NN2) 66.3 43.5 52.5 Co-Decoding (Conf): AP+NN1 69.9 63.8 66.7 Co-Decoding (Rank): AP+NN1 70.7 62.8 66.5 Co-Decoding (Conf): AP+NN2 63.3 61.8 62.5 Co-Decoding (Rank): AP+NN2 67.7 63.3 65.4\nSupervised (199K) 82.9 81.7 82.3 German\nApproach P R F1 Annotation-Projection (AP) 73.7 52.9 61.6 Representation-Projection (NN1) 79.4 41.4 54.4 Representation-Projection (NN2) 64.6 42.7 51.4 Co-Decoding (Conf): AP+NN1 71.1 57.6 63.7 Co-Decoding (Rank): AP+NN1 72.4 57.7 64.2 Co-Decoding (Conf): AP+NN2 64.6 58.1 61.2 Co-Decoding (Rank): AP+NN2 68.0 59.1 63.2\nSupervised (206K) 81.2 64.3 71.8\nTable 4: CoNLL NER development data.\nKorean, German and Portuguese (second highest F1 score for Japanese), and it improves over the best of the two projection-based systems by 2.9 to 6.2 F1 score.\nWe also provide the performance of supervised learning where the NER system is trained with human-annotated NER data in the target language (with size shown in the bracket). While in general the performance of the weakly supervised approaches is not as good as supervised learning, the best combined system achieves an F1 score close to the supervised Korean/German system (trained with 90K/125K tokens of human-annotated data)."
    }, {
      "heading" : "6.3 Evaluation for CoNLL NER Data",
      "text" : "For the CoNLL data, the source (English) NER system for annotation projection is a linear-chain CRF model trained with the CoNLL English training data (∼200K tokens), and the targetlanguage NER system is a 2nd-order MEMM model trained with 2.26M, 3.02M and 1.17M tokens of projection-labeled data for Spanish, Dutch and German, respectively. For representation projection, the source (English) NER systems are neural network models (NN1 and NN2) both\ntrained with the CoNLL English training data. In Table 4 we show the results for the CoNLL development data. For representation projection, neural network model NN1 is better than NN2. Both the annotation projection approach and NN1 tend to have a high precision. In this case, the confidence-based co-decoding scheme that combines AP and NN1 achieves the highest F1 score for Spanish and Dutch (second highest F1 score for German), and improves over the best of the two projection-labeled systems by 1.3 to 3.6 F1 score.\nIn Table 5 we compare our top systems (confidence or rank-based co-decoding of AP and NN1, determined by the development data) with the best results of the cross-lingual NER approaches proposed in Täckström et al. (2012) and Tsai et al. (2016) on the CoNLL test data. Our systems outperform the previous two approaches, especially for German, and are closer to supervised learning."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper we developed two weakly supervised approaches for cross-lingual NER based on effective annotation and representation projection. We also designed two co-decoding schemes that combine the two projection-based systems in a smart way. Experimental results show that the combined system achieves higher accuracy than both systems and outperforms two state-of-the-art crosslingual NER approaches, providing a strong baseline for building cross-lingual NER systems with no human annotation in the target languages.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "The state-of-the-art named entity recognition (NER) systems are supervised machine learning models that require large amounts of human-annotated data to achieve high accuracy. However, annotating NER data by human is expensive and time-consuming, and can be quite difficult for a new language. In this paper, we present two approaches for weakly supervised cross-lingual NER with no human annotation in a target language. The first approach is to create weakly labeled NER data for a target language via annotation projection on comparable corpora, where we develop a heuristic scheme that effectively selects good-quality weakly labeled data from noisy data. The second approach is to project distributed representations of words (word embeddings) from a target language to a source language, so that the source-language NER system can be applied to the target language without re-training. We also design two co-decoding schemes that combine the outputs of the two projection-based approaches in a smart way. We evaluate the performance of the proposed approaches on both in-house and open NER data sets for a number of target languages. The results show that the combined systems outperform two state-of-the-art cross-lingual NER approaches on the CoNLL test data.",
    "creator" : "LaTeX with hyperref package"
  }
}