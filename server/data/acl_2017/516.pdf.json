{
  "name" : "516.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Tandem Anchoring: a Multiword Anchor Approach for Interactive Topic Modeling",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\nTopic models distill large collections of text into topics, giving a high-level summary of the thematic structure of the data without manual annotation. In addition to facilitating discovery of topical trends (Gardner et al., 2010), topic modeling is used for a wide variety of problems including document classification (Rubin et al., 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al., 2004), and sentiment analysis (Titov and McDonald, 2008). However, the most compelling use of topic models is to help users understand large datasets (Chuang et al., 2012).\nInteractive topic modeling (Hu et al., 2014) allows non-experts to refine automatically generated\ntopics, making topic models less of a “take it or leave it” proposition. Including humans input during training improves the quality of the model and allows users to guide topics in a specific way, custom tailoring the model for a specific downstream task or analysis.\nThe downside is that interactive topic modeling is slow—algorithms typically scale with the size of the corpus—and requires non-intuitive information from the user in the form of must-link and cannot-link constraints (Andrzejewski et al., 2009). We address these shortcomings of interactive topic modeling by using an interactive version of the anchor words algorithm for topic models.\nThe anchor algorithm (Arora et al., 2013) is an alternative topic modeling algorithm which scales with the number of unique word types in the data rather than the number of documents or tokens (Section 1). This makes the anchor algorithm fast enough for interactive use, even in web-scale document collections.\nA drawback of the anchor method is that anchor words—words that have high probability of being in a single topic—are not intuitive. We extend the anchor algorithm to use multiple anchor words in tandem (Section 2). Tandem anchors not only improve interactive refinement, but also make the underlying anchor-based method more intuitive.\nFor interactive topic modeling, tandem anchors produce higher quality topics than single word anchors (Section 3). Tandem anchors provide a framework for fast interactive topic modeling: users improve and refine an existing model through multiword anchors (Section 4). Compared to existing methods such as Interactive Topic Models (Hu et al., 2014), our method is much faster.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199"
    }, {
      "heading" : "1 Vanilla Anchor Algorithm",
      "text" : "The anchor algorithm computes the topic matrix A, where Av,k is the conditional probability of observing word v given topic k, e.g., the probability of seeing the word “lens” given the camera topic in a corpus of Amazon product reviews. Arora et al. (2012) finds these probabilities by assuming that every topic contains at least one ‘anchor’ word which has a non-zero probability only in that topic. Anchor words make computing the topic matrix A tractable because the occurrence pattern of the anchor word mirrors the occurrence pattern of the topic itself.\nTo recover the topic matrix A using anchor words, we first compute a V × V cooccurrence matrix Q, where Qi,j is the conditional probability p(wj |wi) of seeing word type wj after having seen wi in the same document. A form of the Gram-Schmidt process on Q finds anchor words {g1 . . . gk} (Arora et al., 2013).\nOnce we have the set of anchor words, we can compute the probability of a topic given a word (the inverse of the conditioning in A). This coefficient matrix C is defined row-wise for each word i\nC∗i,· = argmin Ci,· DKL\n( Qi,· ∥∥∥∥ K∑ k=1 Ci,kQgk,· ) ,\n(1) which gives the best reconstruction (based on Kullback-Leibler divergence DKL) of non-anchor words given anchor words’ conditional probabilities. For example, in our product review data, a word such as “battery” is a convex combination of the anchor words’ contexts (Qgk,·) such as “camera”, “phone”, and “car”. Solving each row of C is fast and is embarrassingly parallel. Finally, we apply Bayes’ rule to recover the topic matrix A from the coefficient matrix C.\nThe anchor algorithm can be orders of magnitude faster than probabilistic inference. The construction of Q requires only a single pass over the data and can be pre-computed for interactive use-cases. Once Q is constructed, topic inference scales with the size of Q which, in turn, depends on the square of the vocabulary size V . In contrast, traditional topic model inference typically requires multiple passes over the entire data. Techniques such as Online LDA (Hoffman et al., 2010) or Stochastic Variation Inference (Hoffman et al., 2013) could improve this to a single pass over the\nAnchor Top Words in Topics backpack backpack camera lens bag room carry fit cameras equipment comfortable camera camera lens pictures canon digital lenses batteries filter mm photos bag bag camera diaper lens bags genie smell\nroom diapers odor\nTable 1: Three separate attempts to construct a topic concerning camera bags in Amazon product reviews with single word anchors. The term “backpack” is a good anchor because it uniquely identifies the topic. However, both “camera” and “bag” are poor anchors for this topic.\nentire data. However, even if such techniques were to be adapted to incorporate human guidance, a single pass is not tractable for interactive use."
    }, {
      "heading" : "2 Tandem Anchor Extension",
      "text" : "Single word anchors can be opaque to users. For an example of bewildering anchor words, consider a camera bag topic from a collection of Amazon product reviews (Table 1). The anchor word “backpack” may seem strange. However, this dataset contains nothing about regular backpacks; thus, “backpack” is unique to camera bags. Bizarre, low-to-mid frequency words are often anchors because anchor words must be unique to a topic; intuitive or high-frequency words cannot be anchors if they have probability in any other topic.\nIf we instead ask users to give us representative words for this topic, we would expect combinations of words like “camera” and “bag.” However, with single word anchors we must choose a single word to anchor each topic. Unfortunately, because these words might appear in multiple topics, individually they are not suitable as anchor words. The anchor word “camera” generates a general camera topic instead of camera bags, and the topic anchored by “bag” includes bags for diaper pails (Table 1).\nInstead, we need to use sets of representative terms as an interpretable, parsimonious description of a topic. This section discusses strategies to build anchors from multiple words and the implications of using multiword anchors to recover topics. This extension not only makes anchors more interpretable but also enables users to manually construct effective anchors in interactive topic modeling settings.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299"
    }, {
      "heading" : "2.1 Anchor Facets",
      "text" : "We first need to turn words into an anchor. If we interpret the anchor algorithm geometrically, each row of Q represents a word as a point in V -dimensional space. We then model each point as a convex combination of anchor words to reconstruct the topic matrix A (Equation 1). Instead of individual anchor words (one anchor word per topic), we use anchor facets, or sets of words that describe a topic. The facets for each anchor form a new pseudoword, or an invented point in V -dimensional space (described in more detail in Section 2.2).\nWhile these new points do not correspond to words in the vocabulary, we can express nonanchor words as convex combinations of pseudowords. To construct these pseudowords from their facets, we combine the co-occurrence profiles of the facets. These pseudowords then augment the original cooccurrence matrix Q with K additional rows corresponding to synthetic pseudowords forming each of K multiword anchors. We refer to this augmented matrix as S. The rest of the anchor algorithm proceeds unmodified.\nOur augmented matrix S is therefore a (V + K) × V matrix. As before, V is the number of token types in the data and K is the number of topics. The first V rows of S correspond to the V token types observed in the data, while the additionalK rows correspond to the pseudowords constructed from anchor facets. Each entry of S encodes conditional probabilities so that Si,j is equal to p(wi |wj). For the additionalK rows, we invent a cooccurrence pattern that can effectively explain the other words’ conditional probabilities."
    }, {
      "heading" : "2.2 Combining Facets into Pseudowords",
      "text" : "We now describe more concretely how to combine an anchor facets to describe the cooccurrence pattern of our new pseudoword anchor. In tandem anchors, we create vector representations that combine the information from anchor facets. Our anchor facets are G1 . . .GK , where Gk is a set of anchor facets which will form the kth pseudoword anchor. The pseudowords are g1 . . . gK , where gk is the pseudoword from Gk. These pseudowords form the new rows of S. We give several candidates for combining anchors facets into a single multiword anchor; we compare their performance in Section 3.\nVector Average An obvious function for com-\nputing the central tendency is the vector average. For each anchor facet,\nSgk,j = ∑ i∈Gk Si,j |Gk| , (2)\nwhere |Gk| is the cardinality of Gk. Vector average makes the pseudoword Sgk,j more central, which is intuitive but inconsistent with the interpretation from Arora et al. (2013) that anchors should be extreme points whose linear combinations explain more central words.\nOr-operator An alternative approach is to consider a cooccurrence with any anchor facet in Gk. For word j, we use De Morgan’s laws to set\nSgk,j = 1− ∏ i∈Gk (1− Si,j). (3)\nUnlike the average, which pulls the pseudoword inward, this or-operator pushes the word outward, increasing each of the dimensions. Increasing the volume of the simplex spanned by the anchors explains more words.\nElement-wise Min Vector average and oroperator are both sensitive to outliers and cannot account for polysemous anchor facets. Returning to our previous example, both “camera” and “bag” are bad anchors for camera bags because they appear in documents discussing other products. However, if both “camera” and “bag” are anchor facets, we can look at an intersection of their contexts: words that appear with both. Using the intersection, the cooccurrence pattern of our anchor facet will only include terms relevant to camera bags.\nMathematically, this is an element-wise min operator,\nSgk,j = min i∈Gk Si,j . (4)\nThis construction, while perhaps not as simple as the previous two, is robust to words which have cooccurrences which are not unique to a single topic.\nHarmonic Mean Leveraging the intuition that we should use a combination function which is both centralizing (like vector average) and ignores large outliers (like element-wise min), the final combination function is the element-wise harmonic mean. Thus, for each anchor facet\nSgk,j = ∑ i∈Gk ( S−1i,j |Gk| )−1 . (5)\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nSince the harmonic mean tends towards the lowest values in the set, it is not sensitive to large outliers, giving us robustness to polysemous words."
    }, {
      "heading" : "2.3 Finding Topics",
      "text" : "After constructing the pseudowords of S we then need to find the coefficients Ci,k which describe each word in our vocabulary as a convex combination of the multiword anchors. Like standard anchor methods, we solve the following for each token type:\nC∗i,· = argmin Ci,· DKL\n( Si,· ∥∥∥∥ K∑ k=1 Ci,kSgk,· ) .\n(6) Finally, we appeal to Bayes’ rule, we recover the topic-word matrix A from the coefficients of C."
    }, {
      "heading" : "3 High Water Mark for Tandem Anchors",
      "text" : "Before addressing interactivity, we apply tandem anchors to real world data, but with anchors gleaned from metadata. Our purpose is twofold. First, we determine which combiner from Section 2.2 to use in our interactive experiments in Section 4 and second, we confirm that well-chosen tandem anchors can improve topics. In addition, we examine running time of tandem anchors and compare to traditional model-based interactive topic modeling techniques. We cannot assume that we will have metadata available to build tandem anchors, but we use them here because they provide a high water mark without the variance introduced by study participants."
    }, {
      "heading" : "3.1 Experimental Setup",
      "text" : "We use the well-known 20 Newsgroups dataset (20NEWS) used in previous interactive topic modeling work: 18,846 usenet postings from 20 different newgroups in the early 1990s. We remove stopwords and words which appear in fewer than 100 documents or more than 1,500 documents.\nTo seed the tandem anchors, we use the titles of newsgroups. To build each multiword anchor facet, we split the title on word boundaries and expand any abbreviations or acronyms. For example, the newsgroup title ‘comp.os.mswindows.misc’ becomes {“computer”, “operating”, “system”, “microsoft”, “windows”, “miscellaneous”}. We do not fully specify the topic; the title gives some intuition, but the topic modeling algorithm must still recover the complete\ntopic-word distributions. This is akin to knowing the names of the categories used but nothing else. Critically, the topic modeling algorithm has no knowledge of document-label relationships."
    }, {
      "heading" : "3.2 Experimental Results",
      "text" : "Our first evaluation is a classification task to predict documents’ newsgroup membership. We do not aim for state-of-the-art accuracy, but the experiment shows title-based tandem anchors yield topics closer to the underlying classes than GramSchmidt anchors. After randomly splitting the data into test and training sets we learn topics from the test data using both the title-based tandem anchors and the Gram-Schmidt single word anchors.1 For multiword anchors, we use each of the proposed combiner functions from Section 2.2. The anchor algorithm only gives the topic-word distributions and not word-level topic assignments, we infer token-level topic assignments using Latent Dirichlet Allocation (Blei et al., 2003) with fixed topics discovered by the anchor method. We use our own implementation of Gibbs sampling with fixed topics and a symmetric document-topic Dirichlet prior with concentration α = .01. Since the topics are fixed, this inference is very fast and can be parallelized on a perdocument basis. We then train a hinge-loss linear classifier on the newsgroup labels using Vowpal Wabbit2 with topic-word pairs as features. Finally, we infer topic assignments in the test data and evaluate the classification using those topicword features. For both training and test, we exclude words outside the LDA vocabulary.\nThe topics created from multiword anchor facets are more accurate than Gram-Schmidt topics (Figure 1). This is true regardless of the combiner function. However, harmonic mean is more accurate than the other functions.3\nSince 20NEWS has twenty classes, accuracy alone does not capture confusion between closely related newsgroups. For example, accuracy penalizes a classifier just as much for label-\n1With fixed anchors and data the anchor algorithm is deterministic, so we use random splits instead of the standard train/test splits so that we can compute variance.\n2http://hunch.net/˜vw/ 3Significant at p < 0.01/4 when using two-tailed t-tests with a Bonferroni correction. For each of our evaluations, we verify the normality of our data (D’Agostino and Pearson, 1973) and use two-tailed t-tests with Bonferroni correction to determine whether the differences between the different methods are significant.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n●●\n● ●● ●\n● ●\n●\n● ● ●\n●●\n● ●\n●\n● ●● ●\n● ●●\nAccuracy ARI F−Measure VI Coherence\nGram−Schmidt\nTitle+Average\nTitle+Or\nTitle+Min\nTitle+HMean\n0. 55\n0. 60\n0. 65\n0. 70\n0. 30 0. 35 0. 40 0. 45 0. 50\n0. 56 0. 60 0. 64 0. 68 0. 72 2. 4 2. 7 3. 0 3. 3 3. 6 − 23 0 − 22 5 − 22 0 − 21 5 − 21 0\nFigure 1: Using metadata can improve anchor-based topic models. For all metrics, the unsupervised Gram-Schmidt anchors do worse than creating anchors based on Newsgroup titles (for all metrics except VI, higher is better). For coherence, Gram-Schmidt does better than two functions for combining anchor words, but not the element-wise min or harmonic mean.\ning a document from ‘rec.sport.baseball’ with ‘rec.sport.hockey’ as with ‘alt.atheism’ despite the similarity between sports newsgroups. Consequently, after building a confusion matrix between the predicted and true classes, external clustering metrics reveal confusion between classes.\nThe first clustering metric is the adjusted Rand index (Yeung and Ruzzo, 2001), which is akin to accuracy for clustering, as it gives the percentage of correct pairing decisions from a reference clustering. Adjusted Rand index also accounts for chance groupings of documents. Next we use Fmeasure, which also considers pairwise groups, balancing the contribution of false negatives, but without the true negatives. Finally, we use variation of information. This metric measures the amount of information lost by switching from the gold standard labels to the predicted labels (Meilă, 2003). Since we are measuring the amount of information lost, lower variation of information is better.\nBased on these clustering metrics, tandem anchors can yield superior topics to those created using single word anchors (Figure 1). As with accuracy, this is true regardless of which combination function we use. Furthermore, harmonic mean produces the least confusion between classes.4\nThe final evaluation is topic coherence by Newman et al. (2010), which measures whether the topics make sense, and correlates with human judgments of topic quality. Given V , the set of\n4Significant at p < 0.01/4 when using two-tailed t-tests with a Bonferroni correction. For each of our evaluations, we verify the normality of our data (D’Agostino and Pearson, 1973) and use two-tailed t-tests with Bonferroni correction to determine whether the differences between the different methods are significant.\nthe n most probable words of a topic, coherence is\n∑ v1,v2∈V log D(v1, v2) + D(v2) (7)\nwhere D(v1, v2) is the co-document frequency of word types v1 and v2, and D(v2) is the document frequency of word type v2. A smoothing parameter prevents zero logarithms.\nFigure 1 also shows topic coherence. Although title-based anchor facets produce better classification features, topics from Gram-Schmidt anchors have better coherence than title-based anchors with the vector average or the or-operator. However, when using the harmonic mean combiner, title-based anchors produce the most human interpretable topics.4\nHarmonic mean beats other combiner functions because it is robust to ambiguous or irrelevant term cooccurrences an anchor facet. Both the vector average and the or-operator are swayed by large outliers, making them sensitive to ambiguous terms in an anchor facet. Element-wise min also has this robustness, but harmonic mean is also able to better characterize anchor facets as it has more centralizing tendency than the min."
    }, {
      "heading" : "3.3 Runtime Considerations",
      "text" : "Tandem anchors will enable users to direct topic inference to improve topic quality. However, for the algorithm to be interactive we must also consider runtime. Cook and Thomas (2005) argue that for interactive applications with user-initiated actions like ours the response time should be less than ten seconds. Longer waits can increase the cognitive load on the user and harm the user interaction.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nAlgorithm #Doc Runtime (seconds) Tandem Anchors 18828 2.13\nFast ITM 13284 24.8 Utopian 515 48.0\nTable 2: Reported run times for various interactive topic modeling algorithms. Only tandem anchors is fast enough to be interactive. For Tandem Anchors, we ran on a single core of an AMD Phemon II X6 1090T processor. For Fast ITM, we use the time reported by Hu and Boyd-Graber (2012). For Utopian, we use the time published by Choo et al. (2013).\nFortunately, the runtime of tandem anchors is amenable to interactive topic modeling. Our run time results are summarized in Table 2. On 20NEWS, interactive updates take roughly two seconds. Furthermore, larger datasets typically have a sublinear increase in distinct word types, so we can expect to see similar run times, even on much larger datasets.\nCompared to other interactive topic modeling algorithms, tandem anchors has a very attractive run time. For example, using an optimized version of the sampler for the Interactive Topic Model described by Hu and Boyd-Graber (2012), and the recommended 30 iterations of sampling, the Interactive Topic Model is well beyond our desired update time for interactive use and an order of magnitude slower than tandem anchors.\nAnother promising interactive topic modeling approach is Utopian (Choo et al., 2013), which uses non-negative factorization, albeit without the benefit of anchor words. Utopian is much slower than tandem anchors. Even on the small InfoVisVAST dataset, Utopian takes nearly a minute to converge. Furthermore, since Utopian scales linearly with the size of the data, for moderately sized datasets such as 20NEWS, Utopian is infeasible for interactive topic modeling due to run time.\nWhile each of these interactive topic modeling algorithms do achieve reasonable topics, only our algorithm fits the run time requirements for interactivity. Furthermore, since tandem anchors scales with the size of the vocabulary rather than the size of the data, this trend will only become more pronounced as we increase the amount of data."
    }, {
      "heading" : "4 Interactive Anchor Words",
      "text" : "Given high quality anchor facets, the tandem anchor algorithm can produce high quality topic\nmodels (particularly when the harmonic mean combiner is used). Furthermore, the tandem anchor algorithm is fast enough to be interactive (as opposed to model-based approaches such as the Interactive Topic Model). We now turn our attention to our main experiment: tandem anchors applied to the problem of interactive topic modeling. We compare both single word and tandem anchors in our study. We do not include the Interactive Topic Model or Utopian, as their run times are too slow for our users."
    }, {
      "heading" : "4.1 Interface and User Study",
      "text" : "To show that interactive tandem anchor words are fast, effective, and intuitive, we ask users to understand a dataset using the anchor word algorithm. For this user study, we recruit twenty participants drawn from a university student body. The student median age is twenty-two. Seven are female, and thirteen are male. None of the students had any prior familiarity with topic modeling or the 20NEWS dataset.\nEach participant sees a simple user interface (Figure 2) with topic given as a row with two columns. The left column allows users to view and edit topics’ anchor words; the right column lists the most probable words in each topic.5 The user can remove an anchor word or drag words from the topic word lists (right column) to become an anchor word. Users can also add additional topics by clicking the “Add Anchor” to create additional anchors. If the user wants to add a word to a tandem anchor set that does not appear in the interface, they manually type the word (restricted to the model’s vocabulary). When the user wants to see the updated topics for their newly refined anchors,\n5While we use topics generated using harmonic mean for our final analysis, users were shown topics generated using the min combiner. However, this does not change our result.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nthey click “Update Topics”. We give each a participant a high level overview of topic modeling. We also describe common problems with topic models including intruding topic words, duplicate topics, and ambiguous topics. Users are instructed to use their best judgement to determine if topics are useful. The task is to edit the anchor words to improve the topics. We asked that users spend at least twenty minutes, but no more than thirty minutes. We repeat the task twice: once with tandem anchors, and once with single word anchors.6"
    }, {
      "heading" : "4.2 Quantitative Results",
      "text" : "We now validate our main result that for interactive topic modeling, tandem anchors yields better topics than single word anchors. Like our titlebased experiments in Section 3, topics generated from users become features to train and test a classifier for the 20NEWS dataset. Based on our results with title-based anchors, we use the harmonic mean combiner in our analysis. As before, we report not only accuracy, but also multiple clustering metrics using the confusion matrix from the classification task. Finally, we report topic coherence.\nFigure 3 summarizes the results of our quantitative evaluation. While we only compare user generated anchors in our analysis, we include the unsupervised Gram-Schmidt anchors as a baseline. Some of the data violate assumptions of normality. Therefore, we use Wilcoxon’s signed-rank test (Wilcoxon, 1945) to determine if the differences between multiword anchors and single word anchors are significant.\nTopics from user generated multiword anchors yield higher classification accuracy (Figure 3). Not only is our approach more scalable than the Interactive Topic Model, but we also achieve higher classification accuracy than Hu et al. (2014).7 Tandem anchors also improve clustering metrics.8\nWhile user selected tandem anchors produce better classification features than single word anchors, users selected single word anchors produce topics with similar topic coherence scores.9\n6The order in which users complete these tasks is counterbalanced.\n7However, the values are not strictly comparable, as Hu et al. (2014) use the standard chronological test/train fold, and we use random splits.\n8Significant at p < 0.01 when using Wilcoxon’s signedrank test.\n9The difference between coherence scores was not statis-\nTo understand this phenomenon, we use quality metrics (AlSumait et al., 2009) for ranking topics by their correspondence to genuine themes in the data. Significant topics are likely skewed towards a few related words, so we measure the distance of each topic-word distribution from the uniform distribution over words. Topics which are close to the underlying word distribution of the entire data are likely to be vacuous, so we also measure the distance of each topic-word distribution from the underlying word distribution. Finally, background topics are likely to appear in a wide range of documents, while meaningful topics will appear in a smaller subset of the data.\nFigure 4 reports our topic significance findings. For all three significance metrics, multiword anchors produce more significant topics than single word anchors.8Topic coherence is based solely on the top n words of a topic, while both accuracy and topic significance depend on the entire topicword distributions. With single word anchors, topics with good coherence may still be too general. Tandem anchors enables users to produce topics with more specific word distributions which are better features for classification."
    }, {
      "heading" : "4.3 Qualitative Results",
      "text" : "We examine the qualitative differences between how users select multiword anchor facets versus single word anchors. Table 3 gives examples of topics generated using different anchor strategies. In a follow-up survey with our users, 75% find it easier to affect individual changes in the topics using tandem anchors compared to single word anchors. Users who prefer editing multiword anchors over single word anchors often report that multiword anchors make it easier to merge similar topics into a single focused topic by combining anchors. For example, by combining multiple words related to Christianity, users were able to create a topic which is highly specific, and differentiated from general religion themes which included terms about Atheism and Judaism.\nWhile users find that use tandem anchors is easier, only 55% of our users say that they prefer the final topics produced by tandem anchors compared to single word anchors. This is in harmony with our quantitative measurements of topic coherence, and may be the result of our stopping criteria: when users judged the topics to be useful.\ntically significant using Wilcoxon’s signed-rank test.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n● ●● ● ●●\n●\nAccuracy ARI F−Measure VI Coherence\nTandem\nSingleword\nGram−Schmidt\n0. 55\n0. 60\n0. 65\n0. 70 0. 30 0. 35 0. 40 0. 45 0. 50 0. 55\n0. 60\n0. 65\n0. 70 2. 7 3. 0 3. 3 3. 6 − 24 0 − 23 0 − 22 0 − 21 0 − 20 0 − 19 0\nFigure 3: Classification accuracy and coherence using topic features gleaned from user provided multiword and single word anchors. Grahm-Schmidt anchors are provided as a baseline. For all metrics except VI, higher is better. Except for coherence, multiword anchors are best.\n●\n●●●\n●\n●●●\n●\n●●●\nuniform vacuous background\nTandem\nSingleword\nGram−Schmidt\n1 2 3\n0. 5\n1. 0\n1. 5\n2. 0\n0. 5\n1. 0\n1. 5\n2. 0\nFigure 4: Topic significance for both single word and multiword anchors. In all cases higher is better. Multiword anchors produce topics which are more significant than single word anchors.\nAnchor Top Words in Topic Automatic Gram Schmidt love love god evolution romans heard car game game games team hockey baseball heard Interactive Single-word evolution evolution theory science faith quote facts religion religion god government state jesus israel baseball baseball games players word teams car hockey hockey team play games season players Interactive Tandem atheism god exists prove god science evidence reason faith objective christian jesus jesus christian christ church bible christians jew israel israel jews jewish israeli state religion baseball bat ball hit baseball ball player games call hockey nhl team hockey player nhl win play\nTable 3: Comparison of topics generated for 20NEWS using various types of anchor words. Users are able to combine words to create more specific topics with tandem anchors.\nHowever, 100% of our users feel that the topics created through interaction were better than those generated from Gram-Schmidt anchors. This was true regardless of whether we used tandem anchors or single word anchors.\nOur participants also produce fewer topics when using multiword anchors. The mean difference between topics under single word anchors and multiple word anchors is 9.35. In follow up interviews, participants indicate that the easiest way to resolve an ambiguous topic with single word anchors was to create a new anchor for each of the ambiguous terms, thus explaining the proliferation of topics for single word anchors. In contrast, fixing an ambiguous tandem anchor is simple: users just add more terms to the anchor facet."
    }, {
      "heading" : "5 Conclusion",
      "text" : "Tandem anchors extend the anchor words algorithm to allow multiple words to be combined into anchor facets. For interactive topic modeling, using anchor facets in place of single word anchors produces higher quality topic models and are more intuitive to use. Furthermore, our approach scales much better than existing interactive topic modeling techniques, allowing interactivity on large datasets for which interactivity was previous impossible.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Topic significance ranking of lda generative models",
      "author" : [ "Loulwah AlSumait", "Daniel Barbará", "James Gentle", "Carlotta Domeniconi." ],
      "venue" : "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, pages 67–82.",
      "citeRegEx" : "AlSumait et al\\.,? 2009",
      "shortCiteRegEx" : "AlSumait et al\\.",
      "year" : 2009
    }, {
      "title" : "Incorporating domain knowledge into topic modeling via Dirichlet forest priors",
      "author" : [ "David Andrzejewski", "Xiaojin Zhu", "Mark Craven" ],
      "venue" : null,
      "citeRegEx" : "Andrzejewski et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Andrzejewski et al\\.",
      "year" : 2009
    }, {
      "title" : "A practical algorithm for topic modeling with provable guarantees",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Yonatan Halpern", "David Mimno", "Ankur Moitra", "David Sontag", "Yichen Wu", "Michael Zhu." ],
      "venue" : "Proceedings of The 30th International Conference on",
      "citeRegEx" : "Arora et al\\.,? 2013",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2013
    }, {
      "title" : "Computing a nonnegative matrix factorization–provably",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Ravindran Kannan", "Ankur Moitra." ],
      "venue" : "Proceedings of the forty-fourth annual ACM symposium on Theory of computing. ACM, pages 145–162.",
      "citeRegEx" : "Arora et al\\.,? 2012",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M. Blei", "Andrew Ng", "Michael Jordan." ],
      "venue" : "Journal of Machine Learning Research 3:993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Utopian: User-driven topic modeling based on interactive nonnegative matrix factorization",
      "author" : [ "Jaegul Choo", "Changhyun Lee", "Chandan K Reddy", "Heejung Park." ],
      "venue" : "Visualization and Computer Graphics, IEEE Transactions on 19(12):1992–2001.",
      "citeRegEx" : "Choo et al\\.,? 2013",
      "shortCiteRegEx" : "Choo et al\\.",
      "year" : 2013
    }, {
      "title" : "Termite: Visualization techniques for assessing textual topic models",
      "author" : [ "Jason Chuang", "Christopher D. Manning", "Jeffrey Heer." ],
      "venue" : "Advanced Visual Interfaces. http://vis.stanford.edu/papers/termite.",
      "citeRegEx" : "Chuang et al\\.,? 2012",
      "shortCiteRegEx" : "Chuang et al\\.",
      "year" : 2012
    }, {
      "title" : "Illuminating the path: The research and development agenda for visual analytics",
      "author" : [ "Kristin A. Cook", "James J. Thomas." ],
      "venue" : "Technical report, Pacific Northwest National Laboratory (PNNL), Richland, WA (US).",
      "citeRegEx" : "Cook and Thomas.,? 2005",
      "shortCiteRegEx" : "Cook and Thomas.",
      "year" : 2005
    }, {
      "title" : "Tests for departure from normality. empirical results for the distributions of b2 and b1",
      "author" : [ "Ralph D’Agostino", "Egon S Pearson" ],
      "venue" : null,
      "citeRegEx" : "D.Agostino and Pearson.,? \\Q1973\\E",
      "shortCiteRegEx" : "D.Agostino and Pearson.",
      "year" : 1973
    }, {
      "title" : "The topic browser: An interactive tool for browsing topic models",
      "author" : [ "Matthew J Gardner", "Joshua Lutes", "Jeff Lund", "Josh Hansen", "Dan Walker", "Eric Ringger", "Kevin Seppi." ],
      "venue" : "NIPS Workshop on Challenges of Data Visualization. volume 2.",
      "citeRegEx" : "Gardner et al\\.,? 2010",
      "shortCiteRegEx" : "Gardner et al\\.",
      "year" : 2010
    }, {
      "title" : "Online learning for latent dirichlet allocation",
      "author" : [ "Matthew Hoffman", "Francis R Bach", "David M Blei." ],
      "venue" : "advances in neural information processing systems. pages 856–864.",
      "citeRegEx" : "Hoffman et al\\.,? 2010",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2010
    }, {
      "title" : "Stochastic variational inference",
      "author" : [ "Matthew D Hoffman", "David M Blei", "Chong Wang", "John William Paisley." ],
      "venue" : "Journal of Machine Learning Research 14(1):1303–1347.",
      "citeRegEx" : "Hoffman et al\\.,? 2013",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient tree-based topic modeling",
      "author" : [ "Yuening Hu", "Jordan Boyd-Graber." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2. Association for Computational Linguistics, pages 275–",
      "citeRegEx" : "Hu and Boyd.Graber.,? 2012",
      "shortCiteRegEx" : "Hu and Boyd.Graber.",
      "year" : 2012
    }, {
      "title" : "Interactive topic modeling",
      "author" : [ "Yuening Hu", "Jordan Boyd-Graber", "Brianna Satinoff", "Alison Smith." ],
      "venue" : "Mach. Learn. 95(3):423–469. http://dx.doi.org/10.1007/s10994-013-5413-0.",
      "citeRegEx" : "Hu et al\\.,? 2014",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2014
    }, {
      "title" : "Comparing clusterings by the variation of information",
      "author" : [ "Marina Meilă." ],
      "venue" : "Learning theory and kernel machines, Springer, pages 173–187.",
      "citeRegEx" : "Meilă.,? 2003",
      "shortCiteRegEx" : "Meilă.",
      "year" : 2003
    }, {
      "title" : "Automatic evaluation of topic coherence",
      "author" : [ "David Newman", "Jey Han Lau", "Karl Grieser", "Timothy Baldwin." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Newman et al\\.,? 2010",
      "shortCiteRegEx" : "Newman et al\\.",
      "year" : 2010
    }, {
      "title" : "The author-topic model for authors and documents",
      "author" : [ "Michal Rosen-Zvi", "Thomas Griffiths", "Mark Steyvers", "Padhraic Smyth." ],
      "venue" : "Proceedings of the 20th conference on Uncertainty in artificial intelligence. AUAI Press, pages 487–494.",
      "citeRegEx" : "Rosen.Zvi et al\\.,? 2004",
      "shortCiteRegEx" : "Rosen.Zvi et al\\.",
      "year" : 2004
    }, {
      "title" : "Statistical topic models for multi-label document classification",
      "author" : [ "Timothy Rubin", "America Chambers", "Padhraic Smyth", "Mark Steyvers." ],
      "venue" : "Machine Learning 1(88):157–208.",
      "citeRegEx" : "Rubin et al\\.,? 2012",
      "shortCiteRegEx" : "Rubin et al\\.",
      "year" : 2012
    }, {
      "title" : "A joint model of text and aspect ratings for sentiment summarization",
      "author" : [ "Ivan Titov", "Ryan T McDonald." ],
      "venue" : "ACL. Citeseer, volume 8, pages 308–316.",
      "citeRegEx" : "Titov and McDonald.,? 2008",
      "shortCiteRegEx" : "Titov and McDonald.",
      "year" : 2008
    }, {
      "title" : "Lda-based document models for ad-hoc retrieval",
      "author" : [ "Xing Wei", "W Bruce Croft." ],
      "venue" : "Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, pages 178–185.",
      "citeRegEx" : "Wei and Croft.,? 2006",
      "shortCiteRegEx" : "Wei and Croft.",
      "year" : 2006
    }, {
      "title" : "Individual comparisons by ranking methods",
      "author" : [ "Frank Wilcoxon." ],
      "venue" : "Biometrics bulletin 1(6):80–83.",
      "citeRegEx" : "Wilcoxon.,? 1945",
      "shortCiteRegEx" : "Wilcoxon.",
      "year" : 1945
    }, {
      "title" : "Details of the adjusted rand index and clustering algorithms, supplement to the paper an empirical study on principal component analysis for clustering gene expression data",
      "author" : [ "Ka Yee Yeung", "Walter L Ruzzo." ],
      "venue" : "Bioinformatics 17(9):763–774.",
      "citeRegEx" : "Yeung and Ruzzo.,? 2001",
      "shortCiteRegEx" : "Yeung and Ruzzo.",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "In addition to facilitating discovery of topical trends (Gardner et al., 2010), topic modeling is used for a wide variety of problems including document classification (Rubin et al.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 17,
      "context" : ", 2010), topic modeling is used for a wide variety of problems including document classification (Rubin et al., 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al.",
      "startOffset" : 97,
      "endOffset" : 117
    }, {
      "referenceID" : 19,
      "context" : ", 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 16,
      "context" : ", 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al., 2004), and sentiment analysis (Titov and McDonald, 2008).",
      "startOffset" : 76,
      "endOffset" : 100
    }, {
      "referenceID" : 18,
      "context" : ", 2004), and sentiment analysis (Titov and McDonald, 2008).",
      "startOffset" : 32,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "However, the most compelling use of topic models is to help users understand large datasets (Chuang et al., 2012).",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : "Interactive topic modeling (Hu et al., 2014) allows non-experts to refine automatically generated topics, making topic models less of a “take it or leave it” proposition.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "The downside is that interactive topic modeling is slow—algorithms typically scale with the size of the corpus—and requires non-intuitive information from the user in the form of must-link and cannot-link constraints (Andrzejewski et al., 2009).",
      "startOffset" : 217,
      "endOffset" : 244
    }, {
      "referenceID" : 2,
      "context" : "The anchor algorithm (Arora et al., 2013) is an alternative topic modeling algorithm which scales with the number of unique word types in the data rather than the number of documents or tokens (Section 1).",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 13,
      "context" : "Compared to existing methods such as Interactive Topic Models (Hu et al., 2014), our method is much faster.",
      "startOffset" : 62,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "gk} (Arora et al., 2013).",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "Arora et al. (2012) finds these probabilities by assuming that every topic contains at least one ‘anchor’ word which has a non-zero probability only in that topic.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "Techniques such as Online LDA (Hoffman et al., 2010) or Stochastic Variation Inference (Hoffman et al.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : ", 2010) or Stochastic Variation Inference (Hoffman et al., 2013) could improve this to a single pass over the Anchor Top Words in Topics backpack backpack camera lens bag room carry fit cameras equipment comfortable camera camera lens pictures canon digital lenses batteries filter mm photos bag bag camera diaper lens bags genie smell room diapers odor",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "Vector average makes the pseudoword Sgk,j more central, which is intuitive but inconsistent with the interpretation from Arora et al. (2013) that anchors should be extreme points whose linear combinations explain more central words.",
      "startOffset" : 121,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "The anchor algorithm only gives the topic-word distributions and not word-level topic assignments, we infer token-level topic assignments using Latent Dirichlet Allocation (Blei et al., 2003) with fixed topics discovered by the anchor method.",
      "startOffset" : 172,
      "endOffset" : 191
    }, {
      "referenceID" : 8,
      "context" : "For each of our evaluations, we verify the normality of our data (D’Agostino and Pearson, 1973) and use two-tailed t-tests with Bonferroni correction to determine whether the differences between the different methods are significant.",
      "startOffset" : 65,
      "endOffset" : 95
    }, {
      "referenceID" : 21,
      "context" : "The first clustering metric is the adjusted Rand index (Yeung and Ruzzo, 2001), which is akin to accuracy for clustering, as it gives the percentage of correct pairing decisions from a reference clustering.",
      "startOffset" : 55,
      "endOffset" : 78
    }, {
      "referenceID" : 14,
      "context" : "This metric measures the amount of information lost by switching from the gold standard labels to the predicted labels (Meilă, 2003).",
      "startOffset" : 119,
      "endOffset" : 132
    }, {
      "referenceID" : 14,
      "context" : "This metric measures the amount of information lost by switching from the gold standard labels to the predicted labels (Meilă, 2003). Since we are measuring the amount of information lost, lower variation of information is better. Based on these clustering metrics, tandem anchors can yield superior topics to those created using single word anchors (Figure 1). As with accuracy, this is true regardless of which combination function we use. Furthermore, harmonic mean produces the least confusion between classes.4 The final evaluation is topic coherence by Newman et al. (2010), which measures whether the topics make sense, and correlates with human judgments of topic quality.",
      "startOffset" : 120,
      "endOffset" : 580
    }, {
      "referenceID" : 8,
      "context" : "For each of our evaluations, we verify the normality of our data (D’Agostino and Pearson, 1973) and use two-tailed t-tests with Bonferroni correction to determine whether the differences between the different methods are significant.",
      "startOffset" : 65,
      "endOffset" : 95
    }, {
      "referenceID" : 7,
      "context" : "Cook and Thomas (2005) argue that for interactive applications with user-initiated actions like ours the response time should be less than ten seconds.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 11,
      "context" : "For Fast ITM, we use the time reported by Hu and Boyd-Graber (2012). For Utopian, we use the time published by Choo et al.",
      "startOffset" : 42,
      "endOffset" : 68
    }, {
      "referenceID" : 5,
      "context" : "For Utopian, we use the time published by Choo et al. (2013).",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "Another promising interactive topic modeling approach is Utopian (Choo et al., 2013), which uses non-negative factorization, albeit without the benefit of anchor words.",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "For example, using an optimized version of the sampler for the Interactive Topic Model described by Hu and Boyd-Graber (2012), and the recommended 30 iterations of sampling, the Interactive Topic Model is well beyond our desired update time for interactive use and an order of magnitude slower than tandem anchors.",
      "startOffset" : 100,
      "endOffset" : 126
    }, {
      "referenceID" : 20,
      "context" : "Therefore, we use Wilcoxon’s signed-rank test (Wilcoxon, 1945) to determine if the differences between multiword anchors and single word anchors are significant.",
      "startOffset" : 46,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "The difference between coherence scores was not statisTo understand this phenomenon, we use quality metrics (AlSumait et al., 2009) for ranking topics by their correspondence to genuine themes in the data.",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : "Not only is our approach more scalable than the Interactive Topic Model, but we also achieve higher classification accuracy than Hu et al. (2014).7 Tandem anchors also improve clustering metrics.",
      "startOffset" : 129,
      "endOffset" : 146
    }, {
      "referenceID" : 12,
      "context" : "Not only is our approach more scalable than the Interactive Topic Model, but we also achieve higher classification accuracy than Hu et al. (2014).7 Tandem anchors also improve clustering metrics.8 While user selected tandem anchors produce better classification features than single word anchors, users selected single word anchors produce topics with similar topic coherence scores.9 The order in which users complete these tasks is counterbalanced. However, the values are not strictly comparable, as Hu et al. (2014) use the standard chronological test/train fold, and we use random splits.",
      "startOffset" : 129,
      "endOffset" : 520
    } ],
    "year" : 2017,
    "abstractText" : "Interactive topic models are powerful tools for those seeking to understand large collections of text. However, existing sampling-based interactive topic modeling approaches scale poorly to large data sets. Anchor methods, which use a single word to uniquely identify a topic, offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for user-facing applications. We propose combinations of words as anchors, going beyond existing single word anchor algorithms—an approach we call “Tandem Anchors”. We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and non-interactive approaches. Tandem anchors are faster and more intuitive than existing interactive approaches. Topic models distill large collections of text into topics, giving a high-level summary of the thematic structure of the data without manual annotation. In addition to facilitating discovery of topical trends (Gardner et al., 2010), topic modeling is used for a wide variety of problems including document classification (Rubin et al., 2012), information retrieval (Wei and Croft, 2006), author identification (Rosen-Zvi et al., 2004), and sentiment analysis (Titov and McDonald, 2008). However, the most compelling use of topic models is to help users understand large datasets (Chuang et al., 2012). Interactive topic modeling (Hu et al., 2014) allows non-experts to refine automatically generated topics, making topic models less of a “take it or leave it” proposition. Including humans input during training improves the quality of the model and allows users to guide topics in a specific way, custom tailoring the model for a specific downstream task or analysis. The downside is that interactive topic modeling is slow—algorithms typically scale with the size of the corpus—and requires non-intuitive information from the user in the form of must-link and cannot-link constraints (Andrzejewski et al., 2009). We address these shortcomings of interactive topic modeling by using an interactive version of the anchor words algorithm for topic models. The anchor algorithm (Arora et al., 2013) is an alternative topic modeling algorithm which scales with the number of unique word types in the data rather than the number of documents or tokens (Section 1). This makes the anchor algorithm fast enough for interactive use, even in web-scale document collections. A drawback of the anchor method is that anchor words—words that have high probability of being in a single topic—are not intuitive. We extend the anchor algorithm to use multiple anchor words in tandem (Section 2). Tandem anchors not only improve interactive refinement, but also make the underlying anchor-based method more intuitive. For interactive topic modeling, tandem anchors produce higher quality topics than single word anchors (Section 3). Tandem anchors provide a framework for fast interactive topic modeling: users improve and refine an existing model through multiword anchors (Section 4). Compared to existing methods such as Interactive Topic Models (Hu et al., 2014), our method is much faster.",
    "creator" : "LaTeX with hyperref package"
  }
}