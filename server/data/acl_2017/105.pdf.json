{
  "name" : "105.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Morphological Inflection Generation with Hard Monotonic Attention",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\nWe present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and nonneural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft attention (Bahdanau et al., 2014) models for the task, shedding some light on the features such models extract."
    }, {
      "heading" : "1 Introduction",
      "text" : "Morphological inflection generation involves generating a target word (e.g. “härtestem”, the German word for “hardest”), given a source word (e.g. “hart”, the German word for “hard”) and the morpho-syntactic attributes of the target (POS=adjective, gender=masculine, type=superlative, etc.).\nThe task is important for many down-stream NLP tasks such as machine translation, especially for dealing with data sparsity in morphologically rich languages where a lemma can be inflected into many different word forms. Several studies have shown that translating into lemmas in the target language and then applying inflection generation as a post-processing step is beneficial for phrase-based machine translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013)\nand more recently for neural machine translation (Garcı́a-Martı́nez et al., 2016).\nThe task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning. Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence.\nMore recently, the task was tackled using neural sequence to sequence learning over character sequences with impressive results (Faruqui et al., 2016). The vanilla encoder-decoder models as used by Faruqui et al. compress the input sequence to a single, fixed-sized continuous representation. Instead, soft-attention based sequence to sequence learning paradigm (Bahdanau et al., 2014) allows directly conditioning on the entire input sequence, and was utilized for morphological inflection generation with great success (Kann and Schütze, 2016b,a).\nHowever, the sequence-to-sequence models require large training sets in order to perform well: their performance on the relatively small CELEX dataset is inferior to the latent variable WFST model of Dreyer et al. (2008). Interestingly, the neural WFST model by Rastogi et al. (2016) also suffered from the same issue on the CELEX dataset, and surpassed the latent variable model only when given twice as much data to train on.\nWe propose a model which handles the above issues by directly modeling an almost monotonic alignment between the input and output character\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nsequences, which is commonly found in the morphological inflection generation task (e.g. in languages with suffixing morphology). The model consists of an encoder-decoder neural network with a dedicated control mechanism: in each step, the model attends to a single input state and either writes a symbol to the output sequence or advances the attention pointer to the next input state from the bi-directionally encoded sequence, as described visually in Figure 1.\nThis modeling suits the natural monotonic alignment between the input and output, as the network learns to attend to the relevant inputs before writing the output which they are aligned to. The encoder is a bi-directional RNN, where each character in the input word is represented using a concatenation of a forward RNN and a backward RNN states over the word’s characters. The combination of the bi-directional encoder and the controllable hard attention mechanism enables to condition the output on the entire input sequence. Moreover, since each character representation is aware of the neighboring characters, non-monotone relations are also captured, which is important in cases where segments in the output word are a result of long range dependencies in the input word. The recurrent nature of the decoder, together with a dedicated feedback connection that passes the last prediction to the next decoder step explicitly, enables the model to also condition the current output on all the previous outputs at each prediction step. The hard attention mechanism allows the network to jointly align and transduce while using a focused representation at each step, rather then the weighted sum of representations used in the soft attention model. In contrast to previous sequence-to-sequence work, we do not require the training procedure to also learn the alignment. Instead, we use a simple training procedure which relies on independently learned character-level alignments, from which we derive gold transduction+control sequences. The network can then be trained using straightforward cross-entropy loss.\nTo evaluate our model, we perform extensive experiments on three previously studied morphological inflection generation datasets: the CELEX dataset (Baayen et al., 1993), the Wiktionary dataset (Durrett and DeNero, 2013) and the SIGMORPHON2016 dataset (Cotterell et al., 2016). We show that while our model is on par with\nor better than the previous neural and non-neural state-of-the-art approaches, it also performs significantly better with very small training sets, being the first neural model to surpass the performance of the weighted FST model with latent variables specifically tailored for the task by Dreyer et al. (2008). Finally, we analyze and compare our model and the soft attention model, showing how they function very similarly with respect to the alignments and representations they learn, in spite of our model being much simpler. This analysis also sheds light on the representations such models learn for the morphological inflection generation task, showing how they encode specific features like a symbol’s type and the symbol’s location in a sequence.\nTo summarize, our contributions in this paper are three-fold:\n1. We present a hard attention model for nearlymonotonic sequence to sequence learning, as common in the morphological inflection setting.\n2. We evaluate the model on the task of morphological inflection generation, establishing a new state of the art on three previouslystudied datasets for the task.\n3. We perform an analysis and comparison of our model and the soft-attention model, shedding light on the features such models extract for the inflection generation task."
    }, {
      "heading" : "2 The Hard Attention Model",
      "text" : "We would like to transduce an input sequence, x1:n ∈ Σ∗x into an output sequence, y1:m ∈ Σ∗y, where Σx and Σy are the input and output vocabularies, respectively. Imagine a machine with read-only random access to the encoding of the input sequence, and a single pointer that determines the current read location. We can then model sequence transduction as a series of pointer movement and write operations. If we assume the alignment is monotone, the machine can be simplified: the memory can be read in sequential order, where the pointer movement is controlled by a single “move forward” operation (step) which we add to the output vocabulary. We implement this behavior using an encoder-decoder neural network, with a control mechanism which determines in each step of the decoder whether to write an\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\noutput symbol or promote the attention pointer the next element of the encoded input."
    }, {
      "heading" : "2.1 Model Definition",
      "text" : "In prediction time, we seek the output sequence y1:m ∈ Σ∗y, for which:\ny1:m = arg max y′∈Σ∗y\np(y′|x1:n, f) (1)\nWhere x ∈ Σ∗x is the input sequence and f = {f1, ..., fl} is a set of features influencing the transduction task (in the inflection generation task these would be the desired morpho-syntactic features of the output sequence). Given a nearlymonotonic alignment between the input and the output, we replace the search for a sequence of letters with a sequence of write and step actions s1:q ∈ Σ∗s, where Σs = Σy ∪ {step}. This sequence is a series of step and write actions required to go from x1:n to y1:m according to the monotonic alignment between them. In this case we define: 1\ns1:q = arg max s′∈Σ∗s\np(s′|x1:n, f)\n= arg max s′∈Σ∗s ∏ s′i∈s′ p(s′i|s′1...s′i−1, x1:n, f) (2)\nwe can then estimate this using a neural network:\ns1:q = arg max s′∈Σ∗s NN(x1:n, f,Θ) (3)\nWhere the network’s parameters Θ are learned using a set of training examples. We will now describe the network architecture."
    }, {
      "heading" : "2.2 Network Architecture",
      "text" : "Notation We use bold letters for vectors and matrices. We treat LSTM as a parameterized function LSTMθ(x1...xn) mapping a sequence of input vectors x1...xn to a an output vector hn. The equations for the LSTM variant we use are detailed in the supplementary material of this paper.\nEncoder For every element in the input sequence: x1:n = x1...xn, we take the corresponding embedding: ex1 ...exn , where: exi ∈ RE .\n1We note that our model (Eq 2) solves a different objective than (Eq 1), as it searches for the best derivation and not the best sequence. In order to accurately solve (1) we would need to marginalize over the different derivations leading to the same sequence, which is computationally challenging. However, as we see in the experiments section, the best-derivation approximation is effective in practice.\nThese embeddings are parameters of the model which will be learned during training. We then feed the embeddings into a bi-directional LSTM encoder (Graves and Schmidhuber, 2005) which results in a sequence of vectors: x1:n = x1...xn, where each vector xi ∈ R2H is a concatenation of: LSTMforward(ex1 , ex2 , ...exi) and LSTMbackward(exn , exn−1 ...exi), the forward LSTM and the backward LSTM outputs when fed with exi .\nDecoder Once the input sequence is encoded, we feed the decoder RNN, LSTMdec, with three inputs at each step:\n1. The current attended input, xa ∈ R2H , initialized with the first element of the encoded sequence, x1.\n2. A set of feature embeddings that influence the generation process, concatenated to a single vector: f = [f1...fl] ∈ RF ·l.\n3. si−1 ∈ RE , which is an embedding for the predicted output symbol in the previous decoder step.\nThose three inputs are concatenated into a single vector zi = [xa, f , si−1] ∈ R2H+F ·l+E , which is fed into the decoder, providing the decoder output vector: LSTMdec(z1...zi) ∈ RH . Finally, to model the distribution over the possible actions, we project the decoder output to a vector of |Σs| elements, followed by a softmax layer:\np(si = c)\n= softmax c(W · LSTMdec(z1...zi) + b) (4)\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nControl Mechanism When the most probable action is step, the attention is promoted so xa contains the next encoded input representation to be used in the next step of the decoder. The process is demonstrated visually in Figure 1."
    }, {
      "heading" : "2.3 Training the Model",
      "text" : "For every example: (x1:n, y1:m, f) in the training data, we should produce a sequence of step and write actions s1:q to be predicted by the decoder. The sequence is dependent on the alignment between the input and the output: ideally, the network will attend to all the input characters aligned to an output character before writing it. While recent work in sequence transduction advocate jointly training the alignment and the decoding mechanisms (Bahdanau et al., 2014; Yu et al., 2016), we instead show that in our case it is worthwhile to decouple these stages and learn a hard alignment beforehand, using it to guide the training of the encoder-decoder network and enabling the use of correct alignments for the attention mechanism from the beginning of the network training phase. For this purpose, we first run a character level alignment process on the training data. We use the character alignment model of Sudoh et al. (2013) which is based on a Chinese Restaurant Process which weights single alignments (character-to-character) in proportion to how many times such an alignment has been seen elsewhere out of all possible alignments. Once we have the character level alignment per input-output sequence pair in the training set, we deterministically infer the sequence of actions s1:q from it by requiring to read all the input elements aligned to an output element (using the step action) before writing it. We then train the network to predict this sequence of actions by using a conventional cross entropy loss function per example: L(x1:n, y1:m, f,Θ) = − ∑\nsj∈s1:q\nlog softmax sj (d),\nd = W · LSTMdec(z1...zi) + b (5)\nAn alternative view of our encoder-decoder architecture is that of a transition system with ADVANCE and WRITE(CH) actions, where the oracle is derived from a given character-level alignment, the input is encoded using a biRNN, and the next action is determined by an RNN over the previous state/action combinations."
    }, {
      "heading" : "3 Experiments",
      "text" : "We perform extensive experiments with three previously studied morphological inflection generation datasets to evaluate our hard attention model in various settings. In all experiments we report the results of the best performing neural and nonneural baselines which were previously published on those datasets. The implementation details for our models are described in the supplementary material section of this paper. The source code for our models is available on github.2\nCELEX Our first evaluation is on a very small dataset, to see if our model indeed avoids the tendency to overfit with small training sets. We report exact match accuracy on the German inflection generation dataset compiled by Dreyer et al. (2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA→13SKE, 2PIE→13PKE, 2PKE→z, and rP→pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive models that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann and Schütze (2016a) which is based on the soft-attention model of Bahdanau et al. (2014), the neural-weighted FST of Rastogi et al. (2016) which uses stacked bi-directional LSTM’s to weigh its arcs (NWFST), and the model of Dreyer et al. (2008) which uses a weighted FST with latent-variables structured particularly for morphological string transduction tasks (LAT). Following previous reports on this dataset, we use the same data splits as Dreyer et al. (2008), dividing the data for each inflection type into five folds, each consisting of 500 training, 1000 development and 1000 test examples. We train a separate model for each fold and report exact match accuracy, averaged over the five folds.\nWiktionary To neutralize the negative effect of very small training sets on the performance of the different learning approaches, we also evaluate our model on the dataset created by Durrett and DeN-\n2the link is removed in this version to keep the submission anonymized\n3The acronyms stand for: 13SIA=1st/3rd person, singular, indefinite, past;13SKE=1st/3rd person, subjunctive, present; 2PIE=2nd person, plural, indefinite, present;13PKE=1st/3rd person, plural, subjunctive, present; 2PKE=2nd person, plural, subjunctive, present; z=infinitive; rP=imperative, plural; pA=past participle.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n13SIA 2PIE 2PKE rP Avg. MED 83.9 95 87.6 84 87.62 NWFST 86.8 94.8 87.9 81.1 87.65 LAT 87.5 93.4 87.4 84.9 88.3 Hard 85.8 95.1 89.5 87.2 89.44\nTable 1: Results on the CELEX dataset\nDE-N DE-V ES-V FI-NA FI-V FR-V NL-V Avg. DDN13 88.31 94.76 99.61 92.14 97.23 98.80 90.50 94.47 NCK15 88.6 97.50 99.80 93.00 98.10 99.20 96.10 96.04 FTND16 88.12 97.72 99.81 95.44 97.81 98.82 96.71 96.34 YBB16 87.5 92.11 99.52 95.48 98.10 98.65 95.90 95.32 Hard 88.87 97.35 99.79 95.75 98.07 99.04 97.03 96.55\nTable 2: Results on the Wiktionary datasets\nero (2013), which contains up to 360k training examples per language. It was built by extracting Finnish, German and Spanish inflection tables from Wiktionary, used in order to evaluate their system based on string alignments and a semiCRF sequence classifier with linguistically inspired features. We also used the expansion made by Nicolai et al. (2015) to include French and Dutch inflections as well. Their system also performs an align-and-transduce approach, extracting rules from the aligned training set and applying them in inference time with a proprietary character sequence classifier. In addition to those systems we also compare to the results of the recent neural approaches of Faruqui et al. (2016), which did not use an attention mechanism, and Yu et al. (2016), which coupled the alignment and transduction tasks.\nSIGMORPHON As different languages show different morphological phenomena, we also experiment with how our model copes with this various phenomena using the morphological inflection dataset from the SIGMORPHON2016 shared task (Cotterell et al., 2016). Here the training data consists of ten languages, with five morphological system types (detailed in Table 3): Russian (RU), German (DE), Spanish (ES), Georgian (GE), Finnish (FI), Turkish (TU), Arabic (AR), Navajo (NA), Hungarian (HU) and Maltese (MA) with roughly 12,800 training and 1600 development examples per language. We compare our model to two soft attention baselines on this dataset: MED (Kann and Schütze, 2016b), which was the best participating system in the shared\ntask, and our implementation of the global (soft) attention model presented by Luong et al. (2015)."
    }, {
      "heading" : "4 Results",
      "text" : "On the low resource setting (CELEX), our model significantly outperforms both the recent neural models of Kann and Schütze (2016a) and Rastogi et al. (2016) and the morphologically aware latent variable model of Dreyer et al. (2008), as detailed in Table 1. It is also, to our knowledge, the first model that surpassed in overall accuracy the latent variable model on this dataset. We attribute our advantage over the soft attention model to the ability of the hard attention control mechanism to harness the monotonic alignments found in the data, while also conditioning on the entire output history which wasn’t available in the FST models. Figure 2 plots the train-set and dev-set accuracies of the soft and hard attention models as a function of the training epoch. While both models perform similarly on the train-set (with the soft attention model fitting it slightly faster), the hard attention model performs significantly better on the dev-set. This shows the soft attention model’s tendency to overfit on the small dataset, as it is not enforcing the monotonic assumption of the hard attention model.\nOn the large training set experiments (Wiktionary), our model is the best performing model on German verbs, Finnish nouns/adjectives and Dutch verbs, resulting in the highest reported average accuracy across all the inflection types when compared to the four previous neural and nonneural state of the art baselines, as detailed in Ta-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n595\n596\n597\n598\n599\nble 2. This shows the robustness of our model also with large amounts of training examples, and the advantage the hard attention mechanism provides over the encoder-decoder approach of Faruqui et al. (2016) which does not employ an attention mechanism. Our model is also significantly more accurate than the model of Yu et al. (2016), which shows the advantage of using independently learned alignments to guide the network’s attention from the beginning of the training process. As can be seen in Table 3, on the SIGMORPHON 2016 dataset our model performs better than both soft-attention baselines for the suffixing+stem-change languages (Russian, German and Spanish) and is slightly less accurate than our implementation of the soft attention model on the rest of the languages, which is now the best performing model on this dataset to our knowledge. We explain this by looking at the languages from a linguistic typology point of view, as detailed in Cotterell et al. (2016). Since Russian, German and Spanish employ a suffixing morphol-\nogy with internal stem changes, they are more suitable for monotonic alignment as the transformations they need to model are the addition of suffixes and changing characters in the stem. The rest of the languages in the dataset employ more context sensitive morphological phenomena like vowel harmony and consonant harmony, which require to model long range dependencies in the input sequence which better suits the soft attention mechanism. While our implementation of the soft attention model and MED are very similar modelwise, we hypothesize that our soft attention model results are better due to the fact that we trained the model for 100 epochs and picked the best performing model on the development set, while the MED system was trained for a fixed amount of 20 epochs (although trained on more data – both train and development sets)."
    }, {
      "heading" : "5 Analysis",
      "text" : "The Learned Alignments In order to see if the alignments predicted by our model fit the mono-\nsuffixing+stem changes circ. suffixing+agg.+v.h. c.h. templatic RU DE ES GE FI TU HU NA AR MA Avg.\nMED 91.46 95.8 98.84 98.5 95.47 98.93 96.8 91.48 99.3 88.99 95.56 Soft 92.18 96.51 98.88 98.88 96.99 99.37 97.01 95.41 99.3 88.86 96.34 Hard 92.21 96.58 98.92 98.12 95.91 97.99 96.25 93.01 98.77 88.32 95.61\nTable 3: Results on the SIGMORPHON 2016 morphological inflection dataset. The text above each language lists the morphological phenomena it includes: circ.=circumfixing, agg.=agglutinative, v.h.=vowel harmony, c.h.=consonant harmony\n0 10 20 30 40\n0\n0.5\n1\nepoch\nac cu\nra cy\nsoft-train hard-train soft-dev hard-dev\nFigure 2: Learning curves for the soft and hard attention models on the first fold of the CELEX dataset\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\ntonic alignment structure found in the data, and whether are they more suitable for the task when compared to the alignments found by the soft attention model, we examined alignment predictions of the two models on examples from the development portion of the CELEX dataset, as depicted in Figure 3. First, we notice the alignments found by the soft attention model are also monotonic, supporting our modeling approach for the task. Figure 3 (bottom-right) also shows how the hardattention model performs deletion (legte→lege) by predicting a sequence of two step operations. Another notable morphological transformation is the one-to-many alignment, found in the top example: flog→fliege, where the model needs to transform a character in the input, o, to two characters in the output, ie. This is performed by two consecutive write operations after the step operation of the relevant character to be replaced. Notice that in this case, the soft attention model performs a different alignment by aligning the character i to o and the character g to the sequence eg, which\nis not the expected alignment in this case from a linguistic point of view.\nThe Learned Representations How does the soft-attention model manage to learn monotonic alignments? Perhaps the the network learns to encode the sequential position as part of its encoding of an input element? More generally, what information is encoded by the soft and hard alignment encoders? We selected 500 random encoded characters-in-context from input words in the CELEX development set, where every encoded representation is a vector in R200. Since those vectors are outputs from the bi-LSTM encoders of the models, every vector of this form carries information of the specific character with its entire context. We project these encodings into 2-D using SVD and plot them twice, each time using a different coloring scheme. We first color each point according to the character it represents (Figures 4a, 4b). In the second coloring scheme (Figures 4c, 4d), each point is colored according\n(a) Colors indicate which character is encoded.\n(b) Colors indicate which character is encoded.\n(c) Colors indicate the character’s position.\n(d) Colors indicate the character’s position.\nFigure 4: SVD dimension reduction to 2D of 500 character representations in context from the encoder, for both the soft attention (top) and hard attention (bottom) models.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nto the character’s sequential position in the word it came from, blue indicating positions near the beginning of the word, and red positions near its end.\nWhile both models tend to cluster representations for similar characters together (Figures 4a, 4b), the hard attention model tends to have much more isolated character clusters. Figures 4c, 4d show that both models also tend to learn representations which are sensitive to the position of the character, although it seems that here the soft attention model is more sensitive to this information as its coloring forms a nearly-perfect red-to-blue transition on the X axis. This may be explained by the soft-attention mechanism encouraging the encoder to encode positional information in the input representations, which may help it to predict better attention scores, and to avoid collisions when computing the weighted sum of representations for the context vector. In contrast, our hardattention model has other means of obtaining the position information in the decoder using the step actions, and for that reason it does not encode it as strongly in the representations of the inputs. This behavior may allow it to perform well even with fewer examples, as the location information is represented more explicitly in the model using the step actions."
    }, {
      "heading" : "6 Related Work",
      "text" : "Many previous works on inflection generation used machine learning methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) with assumptions about the set of possible processes needed to create the output word. Our work was mainly inspired by Faruqui et al. (2016) which trained an independent encoder-decoder neural network for every inflection type in the training data, alleviating the need for feature engineering. Kann and Schütze (2016b,a) tackled the task with a single soft attention model (Bahdanau et al., 2014) for all inflection types, which resulted in the best submission at the SIGMORPHON 2016 shared task (Cotterell et al., 2016). Another closely related work (Rastogi et al., 2016) modeled the task with a WFST in which the arcs are learned by optimizing a global loss function over all the possible paths in the state graph while modeling contextual features with bi-directional LSTM’s. Another recent work (Kann et al., 2016)\nproposed performing neural multi-source morphological reinflection, generating an inflection from several source forms of a word.\nPrevious works on neural sequence transduction include the RNN Transducer (Graves, 2012) which uses two independent RNN’s over monotonically aligned sequences to predict a distribution over the possible output symbols in each step, including a null symbol to model the alignment. Yu et al. (2016) improved this by replacing the null symbol with a dedicated learned transition probability. Both models are trained using a forwardbackward approach, marginalizing over all possible alignments. Our model differs from the above by learning the alignments independently, thus enabling a dependency between the encoder and decoder. While providing better results than Yu et al. (2016), this also simplifies the model training using a simple cross-entropy loss. Jaitly et al. (2015) proposed the Neural Transducer model, which is also trained on external alignments. They divide the input into blocks of a constant size and perform soft attention separately on each block. Lu et al. (2016) used a combination of an RNN encoder with a CRF layer to model the dependencies in the output sequence. A line of work on attention-based speech recognition (Chorowski et al., 2015; Bahdanau et al., 2016) proposed improvements to the attention mechanism: adding location awareness by using the previous attention weights when computing the next ones, and preventing the model from attending on too many or too few inputs using “sharpening” and “smoothing” techniques on the attention weight distributions."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We presented a hard attention model for morphological inflection generation. The model employs an explicit alignment which is used to train a neural network to perform transduction by decoding with a hard attention mechanism. Our model performs on par or better than previous neural and non-neural approaches on various morphological inflection generation datasets, while staying competitive with dedicated models even with very few training examples. Future work may include applying the model to other nearly-monotonic alignand-transduce tasks like abstractive summarization, transliteration or machine translation.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Paradigm classification in supervised learning of morphology",
      "author" : [ "Malin Ahlberg", "Markus Forsberg", "Mans Hulden." ],
      "venue" : "NAACL HLT 2015. pages 1024– 1029.",
      "citeRegEx" : "Ahlberg et al\\.,? 2015",
      "shortCiteRegEx" : "Ahlberg et al\\.",
      "year" : 2015
    }, {
      "title" : "The {CELEX} lexical data base on {CDROM",
      "author" : [ "R Harald Baayen", "Richard Piepenbrock", "Rijn van H" ],
      "venue" : null,
      "citeRegEx" : "Baayen et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Baayen et al\\.",
      "year" : 1993
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "CoRR abs/1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "End-to-end attentionbased large vocabulary speech recognition",
      "author" : [ "Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Yoshua Bengio" ],
      "venue" : "In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2016
    }, {
      "title" : "Translating into morphologically rich languages with synthetic phrases",
      "author" : [ "Victor Chahuneau", "Eva Schlinger", "Noah A. Smith", "Chris Dyer." ],
      "venue" : "EMNLP. pages 1677–1687.",
      "citeRegEx" : "Chahuneau et al\\.,? 2013",
      "shortCiteRegEx" : "Chahuneau et al\\.",
      "year" : 2013
    }, {
      "title" : "Attention-based models for speech recognition",
      "author" : [ "Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Advances in Neural Information Processing Systems 28, pages 577–585.",
      "citeRegEx" : "Chorowski et al\\.,? 2015",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2015
    }, {
      "title" : "Combining morpheme-based machine translation with postprocessing morpheme prediction",
      "author" : [ "Ann Clifton", "Anoop Sarkar." ],
      "venue" : "ACL. pages 32–42.",
      "citeRegEx" : "Clifton and Sarkar.,? 2011",
      "shortCiteRegEx" : "Clifton and Sarkar.",
      "year" : 2011
    }, {
      "title" : "The SIGMORPHON 2016 shared task— morphological reinflection",
      "author" : [ "Ryan Cotterell", "Christo Kirov", "John Sylak-Glassman", "David Yarowsky", "Jason Eisner", "Mans Hulden." ],
      "venue" : "Proceedings of the 2016 Meeting of SIGMORPHON.",
      "citeRegEx" : "Cotterell et al\\.,? 2016",
      "shortCiteRegEx" : "Cotterell et al\\.",
      "year" : 2016
    }, {
      "title" : "Discovering morphological paradigms from plain text using a dirichlet process mixture model",
      "author" : [ "Markus Dreyer", "Jason Eisner." ],
      "venue" : "EMNLP. pages 616–627.",
      "citeRegEx" : "Dreyer and Eisner.,? 2011",
      "shortCiteRegEx" : "Dreyer and Eisner.",
      "year" : 2011
    }, {
      "title" : "Latent-variable modeling of string transductions with finite-state methods",
      "author" : [ "Markus Dreyer", "Jason R Smith", "Jason Eisner." ],
      "venue" : "Proceedings of the conference on empirical methods in natural language processing. pages 1080–1089.",
      "citeRegEx" : "Dreyer et al\\.,? 2008",
      "shortCiteRegEx" : "Dreyer et al\\.",
      "year" : 2008
    }, {
      "title" : "Supervised learning of complete morphological paradigms",
      "author" : [ "Greg Durrett", "John DeNero." ],
      "venue" : "NAACL HLT 2013. pages 1185–1195.",
      "citeRegEx" : "Durrett and DeNero.,? 2013",
      "shortCiteRegEx" : "Durrett and DeNero.",
      "year" : 2013
    }, {
      "title" : "Parameter estimation for probabilistic finite-state transducers",
      "author" : [ "Jason Eisner." ],
      "venue" : "Proceedings of the 40th annual meeting on Association for Computational Linguistics. pages 1–8.",
      "citeRegEx" : "Eisner.,? 2002",
      "shortCiteRegEx" : "Eisner.",
      "year" : 2002
    }, {
      "title" : "Morphological inflection generation using character sequence to sequence learning",
      "author" : [ "Manaal Faruqui", "Yulia Tsvetkov", "Graham Neubig", "Chris Dyer." ],
      "venue" : "NAACL HLT 2016.",
      "citeRegEx" : "Faruqui et al\\.,? 2016",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2016
    }, {
      "title" : "Modeling inflection and wordformation in smt",
      "author" : [ "Alexander M. Fraser", "Marion Weller", "Aoife Cahill", "Fabienne Cap." ],
      "venue" : "EACL. pages 664–674.",
      "citeRegEx" : "Fraser et al\\.,? 2012",
      "shortCiteRegEx" : "Fraser et al\\.",
      "year" : 2012
    }, {
      "title" : "Factored neural machine translation",
      "author" : [ "Mercedes Garcı́a-Martı́nez", "Loı̈c Barrault", "Fethi Bougares" ],
      "venue" : null,
      "citeRegEx" : "Garcı́a.Martı́nez et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Garcı́a.Martı́nez et al\\.",
      "year" : 2016
    }, {
      "title" : "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
      "author" : [ "A. Graves", "J. Schmidhuber." ],
      "venue" : "Neural Networks 18(5-6):602–610.",
      "citeRegEx" : "Graves and Schmidhuber.,? 2005",
      "shortCiteRegEx" : "Graves and Schmidhuber.",
      "year" : 2005
    }, {
      "title" : "Sequence transduction with recurrent neural networks",
      "author" : [ "Alex Graves." ],
      "venue" : "arXiv preprint arXiv:1211.3711 .",
      "citeRegEx" : "Graves.,? 2012",
      "shortCiteRegEx" : "Graves.",
      "year" : 2012
    }, {
      "title" : "Semi-supervised learning of morphological paradigms and lexicons",
      "author" : [ "Mans Hulden", "Markus Forsberg", "Malin Ahlberg." ],
      "venue" : "EACL. pages 569–578.",
      "citeRegEx" : "Hulden et al\\.,? 2014",
      "shortCiteRegEx" : "Hulden et al\\.",
      "year" : 2014
    }, {
      "title" : "A neural transducer",
      "author" : [ "Navdeep Jaitly", "David Sussillo", "Quoc V Le", "Oriol Vinyals", "Ilya Sutskever", "Samy Bengio." ],
      "venue" : "arXiv preprint arXiv:1511.04868 .",
      "citeRegEx" : "Jaitly et al\\.,? 2015",
      "shortCiteRegEx" : "Jaitly et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural multi-source morphological reinflection",
      "author" : [ "Katharina Kann", "Ryan Cotterell", "Hinrich Schütze." ],
      "venue" : "EACL 2017 .",
      "citeRegEx" : "Kann et al\\.,? 2016",
      "shortCiteRegEx" : "Kann et al\\.",
      "year" : 2016
    }, {
      "title" : "Med: The lmu system for the sigmorphon 2016 shared task on morphological reinflection",
      "author" : [ "Katharina Kann", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "Kann and Schütze.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kann and Schütze.",
      "year" : 2016
    }, {
      "title" : "Singlemodel encoder-decoder with explicit morphological representation for reinflection",
      "author" : [ "Katharina Kann", "Hinrich Schütze." ],
      "venue" : "ACL.",
      "citeRegEx" : "Kann and Schütze.,? 2016b",
      "shortCiteRegEx" : "Kann and Schütze.",
      "year" : 2016
    }, {
      "title" : "Regular models of phonological rule systems",
      "author" : [ "Ronald M. Kaplan", "Martin Kay." ],
      "venue" : "Computational Linguistics 20(3):331–378.",
      "citeRegEx" : "Kaplan and Kay.,? 1994",
      "shortCiteRegEx" : "Kaplan and Kay.",
      "year" : 1994
    }, {
      "title" : "Two-level morphology: A general computational model of word-form recognition and production",
      "author" : [ "Kimmo Koskenniemi." ],
      "venue" : "Technical report.",
      "citeRegEx" : "Koskenniemi.,? 1983",
      "shortCiteRegEx" : "Koskenniemi.",
      "year" : 1983
    }, {
      "title" : "Segmental recurrent neural networks for end-to-end speech recognition",
      "author" : [ "Liang Lu", "Lingpeng Kong", "Chris Dyer", "Noah A Smith", "Steve Renals." ],
      "venue" : "arXiv preprint arXiv:1603.00223 .",
      "citeRegEx" : "Lu et al\\.,? 2016",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1508.04025 .",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Generating complex morphology for machine translation",
      "author" : [ "Einat Minkov", "Kristina Toutanova", "Hisami Suzuki." ],
      "venue" : "ACL.",
      "citeRegEx" : "Minkov et al\\.,? 2007",
      "shortCiteRegEx" : "Minkov et al\\.",
      "year" : 2007
    }, {
      "title" : "A rational design for a weighted finite-state transducer library",
      "author" : [ "Mehryar Mohri", "Fernando Pereira", "Michael Riley." ],
      "venue" : "International Workshop on Implementing Automata. pages 144–158.",
      "citeRegEx" : "Mohri et al\\.,? 1997",
      "shortCiteRegEx" : "Mohri et al\\.",
      "year" : 1997
    }, {
      "title" : "Inflection generation as discriminative string transduction",
      "author" : [ "Garrett Nicolai", "Colin Cherry", "Grzegorz Kondrak." ],
      "venue" : "NAACL HLT 2015. pages 922–931.",
      "citeRegEx" : "Nicolai et al\\.,? 2015",
      "shortCiteRegEx" : "Nicolai et al\\.",
      "year" : 2015
    }, {
      "title" : "Weighting finite-state transductions with neural context",
      "author" : [ "Pushpendre Rastogi", "Ryan Cotterell", "Jason Eisner." ],
      "venue" : "Proc. of NAACL.",
      "citeRegEx" : "Rastogi et al\\.,? 2016",
      "shortCiteRegEx" : "Rastogi et al\\.",
      "year" : 2016
    }, {
      "title" : "Noise-aware character alignment for bootstrapping statistical machine transliteration from bilingual corpora",
      "author" : [ "Katsuhito Sudoh", "Shinsuke Mori", "Masaaki Nagata." ],
      "venue" : "EMNLP 2013. pages 204–209.",
      "citeRegEx" : "Sudoh et al\\.,? 2013",
      "shortCiteRegEx" : "Sudoh et al\\.",
      "year" : 2013
    }, {
      "title" : "Applying morphology generation models to machine translation",
      "author" : [ "Kristina Toutanova", "Hisami Suzuki", "Achim Ruopp." ],
      "venue" : "ACL. pages 514–522.",
      "citeRegEx" : "Toutanova et al\\.,? 2008",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2008
    }, {
      "title" : "Minimally supervised morphological analysis by multimodal alignment",
      "author" : [ "David Yarowsky", "Richard Wicentowski." ],
      "venue" : "ACL.",
      "citeRegEx" : "Yarowsky and Wicentowski.,? 2000",
      "shortCiteRegEx" : "Yarowsky and Wicentowski.",
      "year" : 2000
    }, {
      "title" : "Online segment to segment neural transduction",
      "author" : [ "Lei Yu", "Jan Buys", "Phil Blunsom." ],
      "venue" : "arXiv preprint arXiv:1609.08194 .",
      "citeRegEx" : "Yu et al\\.,? 2016",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "Matthew D Zeiler." ],
      "venue" : "arXiv preprint arXiv:1212.5701 .",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    }, {
      "title" : "2016) which also performed a similar ensembling technique. For the character level alignment process we use the implementation provided by the organizers of the SIGMORPHON2016 shared task.5",
      "author" : [ "Kann", "Schütze (2016a", "Faruqui" ],
      "venue" : null,
      "citeRegEx" : "Kann et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kann et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Finally we present an analysis of the continuous representations learned by both the hard and soft attention (Bahdanau et al., 2014) models for the task, shedding some light on the features such models extract.",
      "startOffset" : 109,
      "endOffset" : 132
    }, {
      "referenceID" : 26,
      "context" : "Several studies have shown that translating into lemmas in the target language and then applying inflection generation as a post-processing step is beneficial for phrase-based machine translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garcı́a-Martı́nez et al.",
      "startOffset" : 196,
      "endOffset" : 312
    }, {
      "referenceID" : 31,
      "context" : "Several studies have shown that translating into lemmas in the target language and then applying inflection generation as a post-processing step is beneficial for phrase-based machine translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garcı́a-Martı́nez et al.",
      "startOffset" : 196,
      "endOffset" : 312
    }, {
      "referenceID" : 6,
      "context" : "Several studies have shown that translating into lemmas in the target language and then applying inflection generation as a post-processing step is beneficial for phrase-based machine translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garcı́a-Martı́nez et al.",
      "startOffset" : 196,
      "endOffset" : 312
    }, {
      "referenceID" : 13,
      "context" : "Several studies have shown that translating into lemmas in the target language and then applying inflection generation as a post-processing step is beneficial for phrase-based machine translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garcı́a-Martı́nez et al.",
      "startOffset" : 196,
      "endOffset" : 312
    }, {
      "referenceID" : 4,
      "context" : "Several studies have shown that translating into lemmas in the target language and then applying inflection generation as a post-processing step is beneficial for phrase-based machine translation (Minkov et al., 2007; Toutanova et al., 2008; Clifton and Sarkar, 2011; Fraser et al., 2012; Chahuneau et al., 2013) and more recently for neural machine translation (Garcı́a-Martı́nez et al.",
      "startOffset" : 196,
      "endOffset" : 312
    }, {
      "referenceID" : 14,
      "context" : ", 2013) and more recently for neural machine translation (Garcı́a-Martı́nez et al., 2016).",
      "startOffset" : 57,
      "endOffset" : 89
    }, {
      "referenceID" : 23,
      "context" : "The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al.",
      "startOffset" : 87,
      "endOffset" : 128
    }, {
      "referenceID" : 22,
      "context" : "The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al.",
      "startOffset" : 87,
      "endOffset" : 128
    }, {
      "referenceID" : 27,
      "context" : "The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning.",
      "startOffset" : 214,
      "endOffset" : 248
    }, {
      "referenceID" : 11,
      "context" : "The task was traditionally tackled with hand engineered finite state transducers (FST) (Koskenniemi, 1983; Kaplan and Kay, 1994) which rely on expert knowledge, or using trainable weighted finite state transducers (Mohri et al., 1997; Eisner, 2002) which combine expert knowledge with datadriven parameter tuning.",
      "startOffset" : 214,
      "endOffset" : 248
    }, {
      "referenceID" : 32,
      "context" : "Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence.",
      "startOffset" : 41,
      "endOffset" : 189
    }, {
      "referenceID" : 8,
      "context" : "Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence.",
      "startOffset" : 41,
      "endOffset" : 189
    }, {
      "referenceID" : 10,
      "context" : "Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence.",
      "startOffset" : 41,
      "endOffset" : 189
    }, {
      "referenceID" : 17,
      "context" : "Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence.",
      "startOffset" : 41,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence.",
      "startOffset" : 41,
      "endOffset" : 189
    }, {
      "referenceID" : 28,
      "context" : "Many other machinelearning based methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence.",
      "startOffset" : 41,
      "endOffset" : 189
    }, {
      "referenceID" : 12,
      "context" : "More recently, the task was tackled using neural sequence to sequence learning over character sequences with impressive results (Faruqui et al., 2016).",
      "startOffset" : 128,
      "endOffset" : 150
    }, {
      "referenceID" : 2,
      "context" : "Instead, soft-attention based sequence to sequence learning paradigm (Bahdanau et al., 2014) allows directly conditioning on the entire input sequence, and was utilized for morphological inflection generation with great success (Kann and Schütze, 2016b,a).",
      "startOffset" : 69,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence. More recently, the task was tackled using neural sequence to sequence learning over character sequences with impressive results (Faruqui et al., 2016). The vanilla encoder-decoder models as used by Faruqui et al. compress the input sequence to a single, fixed-sized continuous representation. Instead, soft-attention based sequence to sequence learning paradigm (Bahdanau et al., 2014) allows directly conditioning on the entire input sequence, and was utilized for morphological inflection generation with great success (Kann and Schütze, 2016b,a). However, the sequence-to-sequence models require large training sets in order to perform well: their performance on the relatively small CELEX dataset is inferior to the latent variable WFST model of Dreyer et al. (2008). Interestingly, the neural WFST model by Rastogi et al.",
      "startOffset" : 8,
      "endOffset" : 969
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ahlberg et al., 2015; Nicolai et al., 2015) were proposed for the task, although with specific assumptions about the set of possible processes that are needed to create the output sequence. More recently, the task was tackled using neural sequence to sequence learning over character sequences with impressive results (Faruqui et al., 2016). The vanilla encoder-decoder models as used by Faruqui et al. compress the input sequence to a single, fixed-sized continuous representation. Instead, soft-attention based sequence to sequence learning paradigm (Bahdanau et al., 2014) allows directly conditioning on the entire input sequence, and was utilized for morphological inflection generation with great success (Kann and Schütze, 2016b,a). However, the sequence-to-sequence models require large training sets in order to perform well: their performance on the relatively small CELEX dataset is inferior to the latent variable WFST model of Dreyer et al. (2008). Interestingly, the neural WFST model by Rastogi et al. (2016) also suffered from the same issue on the CELEX dataset, and surpassed the latent variable model only when given twice as much data to train on.",
      "startOffset" : 8,
      "endOffset" : 1032
    }, {
      "referenceID" : 1,
      "context" : "To evaluate our model, we perform extensive experiments on three previously studied morphological inflection generation datasets: the CELEX dataset (Baayen et al., 1993), the Wiktionary dataset (Durrett and DeNero, 2013) and the SIGMORPHON2016 dataset (Cotterell et al.",
      "startOffset" : 148,
      "endOffset" : 169
    }, {
      "referenceID" : 10,
      "context" : ", 1993), the Wiktionary dataset (Durrett and DeNero, 2013) and the SIGMORPHON2016 dataset (Cotterell et al.",
      "startOffset" : 32,
      "endOffset" : 58
    }, {
      "referenceID" : 7,
      "context" : ", 1993), the Wiktionary dataset (Durrett and DeNero, 2013) and the SIGMORPHON2016 dataset (Cotterell et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "To evaluate our model, we perform extensive experiments on three previously studied morphological inflection generation datasets: the CELEX dataset (Baayen et al., 1993), the Wiktionary dataset (Durrett and DeNero, 2013) and the SIGMORPHON2016 dataset (Cotterell et al., 2016). We show that while our model is on par with or better than the previous neural and non-neural state-of-the-art approaches, it also performs significantly better with very small training sets, being the first neural model to surpass the performance of the weighted FST model with latent variables specifically tailored for the task by Dreyer et al. (2008). Finally, we analyze and compare our model and the soft attention model, showing how they function very similarly with respect to the alignments and representations they learn, in spite of our model being much simpler.",
      "startOffset" : 149,
      "endOffset" : 633
    }, {
      "referenceID" : 15,
      "context" : "We then feed the embeddings into a bi-directional LSTM encoder (Graves and Schmidhuber, 2005) which results in a sequence of vectors: x1:n = x1.",
      "startOffset" : 63,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "While recent work in sequence transduction advocate jointly training the alignment and the decoding mechanisms (Bahdanau et al., 2014; Yu et al., 2016), we instead show that in our case it is worthwhile to decouple these stages and learn a hard alignment beforehand, using it to guide the training of the encoder-decoder network and enabling the use of correct alignments for the attention mechanism from the beginning of the network training phase.",
      "startOffset" : 111,
      "endOffset" : 151
    }, {
      "referenceID" : 33,
      "context" : "While recent work in sequence transduction advocate jointly training the alignment and the decoding mechanisms (Bahdanau et al., 2014; Yu et al., 2016), we instead show that in our case it is worthwhile to decouple these stages and learn a hard alignment beforehand, using it to guide the training of the encoder-decoder network and enabling the use of correct alignments for the attention mechanism from the beginning of the network training phase.",
      "startOffset" : 111,
      "endOffset" : 151
    }, {
      "referenceID" : 2,
      "context" : "While recent work in sequence transduction advocate jointly training the alignment and the decoding mechanisms (Bahdanau et al., 2014; Yu et al., 2016), we instead show that in our case it is worthwhile to decouple these stages and learn a hard alignment beforehand, using it to guide the training of the encoder-decoder network and enabling the use of correct alignments for the attention mechanism from the beginning of the network training phase. For this purpose, we first run a character level alignment process on the training data. We use the character alignment model of Sudoh et al. (2013) which is based on a Chinese Restaurant Process which weights single alignments (character-to-character) in proportion to how many times such an alignment has been seen elsewhere out of all possible alignments.",
      "startOffset" : 112,
      "endOffset" : 599
    }, {
      "referenceID" : 1,
      "context" : "(2008) from the CELEX database (Baayen et al., 1993).",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : "We report exact match accuracy on the German inflection generation dataset compiled by Dreyer et al. (2008) from the CELEX database (Baayen et al.",
      "startOffset" : 87,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA→13SKE, 2PIE→13PKE, 2PKE→z, and rP→pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive models that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann and Schütze (2016a) which is based on the soft-attention model of Bahdanau et al.",
      "startOffset" : 32,
      "endOffset" : 402
    }, {
      "referenceID" : 1,
      "context" : "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA→13SKE, 2PIE→13PKE, 2PKE→z, and rP→pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive models that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann and Schütze (2016a) which is based on the soft-attention model of Bahdanau et al. (2014), the neural-weighted FST of Rastogi et al.",
      "startOffset" : 32,
      "endOffset" : 471
    }, {
      "referenceID" : 1,
      "context" : "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA→13SKE, 2PIE→13PKE, 2PKE→z, and rP→pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive models that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann and Schütze (2016a) which is based on the soft-attention model of Bahdanau et al. (2014), the neural-weighted FST of Rastogi et al. (2016) which uses stacked bi-directional LSTM’s to weigh its arcs (NWFST), and the model of Dreyer et al.",
      "startOffset" : 32,
      "endOffset" : 521
    }, {
      "referenceID" : 1,
      "context" : "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA→13SKE, 2PIE→13PKE, 2PKE→z, and rP→pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive models that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann and Schütze (2016a) which is based on the soft-attention model of Bahdanau et al. (2014), the neural-weighted FST of Rastogi et al. (2016) which uses stacked bi-directional LSTM’s to weigh its arcs (NWFST), and the model of Dreyer et al. (2008) which uses a weighted FST with latent-variables structured particularly for morphological string transduction tasks (LAT).",
      "startOffset" : 32,
      "endOffset" : 627
    }, {
      "referenceID" : 1,
      "context" : "(2008) from the CELEX database (Baayen et al., 1993). The dataset includes only 500 training examples for each of the four inflection types: 13SIA→13SKE, 2PIE→13PKE, 2PKE→z, and rP→pA which we refer to as 13SIA, 2PIE, 2PKE and rP, respectively.3 We compare our model to three competitive models that reported results on this dataset: the Morphological Encoder-Decoder (MED) of Kann and Schütze (2016a) which is based on the soft-attention model of Bahdanau et al. (2014), the neural-weighted FST of Rastogi et al. (2016) which uses stacked bi-directional LSTM’s to weigh its arcs (NWFST), and the model of Dreyer et al. (2008) which uses a weighted FST with latent-variables structured particularly for morphological string transduction tasks (LAT). Following previous reports on this dataset, we use the same data splits as Dreyer et al. (2008), dividing the data for each inflection type into five folds, each consisting of 500 training, 1000 development and 1000 test examples.",
      "startOffset" : 32,
      "endOffset" : 846
    }, {
      "referenceID" : 27,
      "context" : "We also used the expansion made by Nicolai et al. (2015) to include French and Dutch inflections as well.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 12,
      "context" : "In addition to those systems we also compare to the results of the recent neural approaches of Faruqui et al. (2016), which did not use an attention mechanism, and Yu et al.",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "In addition to those systems we also compare to the results of the recent neural approaches of Faruqui et al. (2016), which did not use an attention mechanism, and Yu et al. (2016), which coupled the alignment and transduction tasks.",
      "startOffset" : 95,
      "endOffset" : 181
    }, {
      "referenceID" : 7,
      "context" : "SIGMORPHON As different languages show different morphological phenomena, we also experiment with how our model copes with this various phenomena using the morphological inflection dataset from the SIGMORPHON2016 shared task (Cotterell et al., 2016).",
      "startOffset" : 225,
      "endOffset" : 249
    }, {
      "referenceID" : 21,
      "context" : "We compare our model to two soft attention baselines on this dataset: MED (Kann and Schütze, 2016b), which was the best participating system in the shared task, and our implementation of the global (soft) attention model presented by Luong et al.",
      "startOffset" : 74,
      "endOffset" : 99
    }, {
      "referenceID" : 7,
      "context" : "SIGMORPHON As different languages show different morphological phenomena, we also experiment with how our model copes with this various phenomena using the morphological inflection dataset from the SIGMORPHON2016 shared task (Cotterell et al., 2016). Here the training data consists of ten languages, with five morphological system types (detailed in Table 3): Russian (RU), German (DE), Spanish (ES), Georgian (GE), Finnish (FI), Turkish (TU), Arabic (AR), Navajo (NA), Hungarian (HU) and Maltese (MA) with roughly 12,800 training and 1600 development examples per language. We compare our model to two soft attention baselines on this dataset: MED (Kann and Schütze, 2016b), which was the best participating system in the shared task, and our implementation of the global (soft) attention model presented by Luong et al. (2015).",
      "startOffset" : 226,
      "endOffset" : 830
    }, {
      "referenceID" : 19,
      "context" : "On the low resource setting (CELEX), our model significantly outperforms both the recent neural models of Kann and Schütze (2016a) and Rastogi et al.",
      "startOffset" : 106,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : "On the low resource setting (CELEX), our model significantly outperforms both the recent neural models of Kann and Schütze (2016a) and Rastogi et al. (2016) and the morphologically aware latent variable model of Dreyer et al.",
      "startOffset" : 106,
      "endOffset" : 157
    }, {
      "referenceID" : 9,
      "context" : "(2016) and the morphologically aware latent variable model of Dreyer et al. (2008), as detailed in Table 1.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "This shows the robustness of our model also with large amounts of training examples, and the advantage the hard attention mechanism provides over the encoder-decoder approach of Faruqui et al. (2016) which does not employ an attention mechanism.",
      "startOffset" : 178,
      "endOffset" : 200
    }, {
      "referenceID" : 11,
      "context" : "This shows the robustness of our model also with large amounts of training examples, and the advantage the hard attention mechanism provides over the encoder-decoder approach of Faruqui et al. (2016) which does not employ an attention mechanism. Our model is also significantly more accurate than the model of Yu et al. (2016), which shows the advantage of using independently learned alignments to guide the network’s attention from the beginning of the training process.",
      "startOffset" : 178,
      "endOffset" : 327
    }, {
      "referenceID" : 7,
      "context" : "We explain this by looking at the languages from a linguistic typology point of view, as detailed in Cotterell et al. (2016). Since Russian, German and Spanish employ a suffixing morphology with internal stem changes, they are more suitable for monotonic alignment as the transformations they need to model are the addition of suffixes and changing characters in the stem.",
      "startOffset" : 101,
      "endOffset" : 125
    }, {
      "referenceID" : 32,
      "context" : "Many previous works on inflection generation used machine learning methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) with assumptions about the set of possible processes needed to create the output word.",
      "startOffset" : 75,
      "endOffset" : 223
    }, {
      "referenceID" : 8,
      "context" : "Many previous works on inflection generation used machine learning methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) with assumptions about the set of possible processes needed to create the output word.",
      "startOffset" : 75,
      "endOffset" : 223
    }, {
      "referenceID" : 10,
      "context" : "Many previous works on inflection generation used machine learning methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) with assumptions about the set of possible processes needed to create the output word.",
      "startOffset" : 75,
      "endOffset" : 223
    }, {
      "referenceID" : 17,
      "context" : "Many previous works on inflection generation used machine learning methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) with assumptions about the set of possible processes needed to create the output word.",
      "startOffset" : 75,
      "endOffset" : 223
    }, {
      "referenceID" : 0,
      "context" : "Many previous works on inflection generation used machine learning methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) with assumptions about the set of possible processes needed to create the output word.",
      "startOffset" : 75,
      "endOffset" : 223
    }, {
      "referenceID" : 28,
      "context" : "Many previous works on inflection generation used machine learning methods (Yarowsky and Wicentowski, 2000; Dreyer and Eisner, 2011; Durrett and DeNero, 2013; Hulden et al., 2014; Ahlberg et al., 2015; Nicolai et al., 2015) with assumptions about the set of possible processes needed to create the output word.",
      "startOffset" : 75,
      "endOffset" : 223
    }, {
      "referenceID" : 2,
      "context" : "Kann and Schütze (2016b,a) tackled the task with a single soft attention model (Bahdanau et al., 2014) for all inflection types, which resulted in the best submission at the SIGMORPHON 2016 shared task (Cotterell et al.",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : ", 2014) for all inflection types, which resulted in the best submission at the SIGMORPHON 2016 shared task (Cotterell et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 131
    }, {
      "referenceID" : 29,
      "context" : "Another closely related work (Rastogi et al., 2016) modeled the task with a WFST in which the arcs are learned by optimizing a global loss function over all the possible paths in the state graph while modeling contextual features with bi-directional LSTM’s.",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 19,
      "context" : "Another recent work (Kann et al., 2016) proposed performing neural multi-source morphological reinflection, generating an inflection from several source forms of a word.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 16,
      "context" : "Previous works on neural sequence transduction include the RNN Transducer (Graves, 2012) which uses two independent RNN’s over monotonically aligned sequences to predict a distribution over the possible output symbols in each step, including a null symbol to model the alignment.",
      "startOffset" : 74,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "A line of work on attention-based speech recognition (Chorowski et al., 2015; Bahdanau et al., 2016) proposed improvements to the attention mechanism: adding location awareness by using the previous attention weights when computing the next ones, and preventing the model from attending on too many or too few inputs using “sharpening” and “smoothing” techniques on the attention weight distributions.",
      "startOffset" : 53,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "A line of work on attention-based speech recognition (Chorowski et al., 2015; Bahdanau et al., 2016) proposed improvements to the attention mechanism: adding location awareness by using the previous attention weights when computing the next ones, and preventing the model from attending on too many or too few inputs using “sharpening” and “smoothing” techniques on the attention weight distributions.",
      "startOffset" : 53,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ahlberg et al., 2015; Nicolai et al., 2015) with assumptions about the set of possible processes needed to create the output word. Our work was mainly inspired by Faruqui et al. (2016) which trained an independent encoder-decoder neural network for every inflection type in the training data, alleviating the need for feature engineering.",
      "startOffset" : 8,
      "endOffset" : 193
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ahlberg et al., 2015; Nicolai et al., 2015) with assumptions about the set of possible processes needed to create the output word. Our work was mainly inspired by Faruqui et al. (2016) which trained an independent encoder-decoder neural network for every inflection type in the training data, alleviating the need for feature engineering. Kann and Schütze (2016b,a) tackled the task with a single soft attention model (Bahdanau et al., 2014) for all inflection types, which resulted in the best submission at the SIGMORPHON 2016 shared task (Cotterell et al., 2016). Another closely related work (Rastogi et al., 2016) modeled the task with a WFST in which the arcs are learned by optimizing a global loss function over all the possible paths in the state graph while modeling contextual features with bi-directional LSTM’s. Another recent work (Kann et al., 2016) proposed performing neural multi-source morphological reinflection, generating an inflection from several source forms of a word. Previous works on neural sequence transduction include the RNN Transducer (Graves, 2012) which uses two independent RNN’s over monotonically aligned sequences to predict a distribution over the possible output symbols in each step, including a null symbol to model the alignment. Yu et al. (2016) improved this by replacing the null symbol with a dedicated learned transition probability.",
      "startOffset" : 8,
      "endOffset" : 1300
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ahlberg et al., 2015; Nicolai et al., 2015) with assumptions about the set of possible processes needed to create the output word. Our work was mainly inspired by Faruqui et al. (2016) which trained an independent encoder-decoder neural network for every inflection type in the training data, alleviating the need for feature engineering. Kann and Schütze (2016b,a) tackled the task with a single soft attention model (Bahdanau et al., 2014) for all inflection types, which resulted in the best submission at the SIGMORPHON 2016 shared task (Cotterell et al., 2016). Another closely related work (Rastogi et al., 2016) modeled the task with a WFST in which the arcs are learned by optimizing a global loss function over all the possible paths in the state graph while modeling contextual features with bi-directional LSTM’s. Another recent work (Kann et al., 2016) proposed performing neural multi-source morphological reinflection, generating an inflection from several source forms of a word. Previous works on neural sequence transduction include the RNN Transducer (Graves, 2012) which uses two independent RNN’s over monotonically aligned sequences to predict a distribution over the possible output symbols in each step, including a null symbol to model the alignment. Yu et al. (2016) improved this by replacing the null symbol with a dedicated learned transition probability. Both models are trained using a forwardbackward approach, marginalizing over all possible alignments. Our model differs from the above by learning the alignments independently, thus enabling a dependency between the encoder and decoder. While providing better results than Yu et al. (2016), this also simplifies the model training using a simple cross-entropy loss.",
      "startOffset" : 8,
      "endOffset" : 1682
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ahlberg et al., 2015; Nicolai et al., 2015) with assumptions about the set of possible processes needed to create the output word. Our work was mainly inspired by Faruqui et al. (2016) which trained an independent encoder-decoder neural network for every inflection type in the training data, alleviating the need for feature engineering. Kann and Schütze (2016b,a) tackled the task with a single soft attention model (Bahdanau et al., 2014) for all inflection types, which resulted in the best submission at the SIGMORPHON 2016 shared task (Cotterell et al., 2016). Another closely related work (Rastogi et al., 2016) modeled the task with a WFST in which the arcs are learned by optimizing a global loss function over all the possible paths in the state graph while modeling contextual features with bi-directional LSTM’s. Another recent work (Kann et al., 2016) proposed performing neural multi-source morphological reinflection, generating an inflection from several source forms of a word. Previous works on neural sequence transduction include the RNN Transducer (Graves, 2012) which uses two independent RNN’s over monotonically aligned sequences to predict a distribution over the possible output symbols in each step, including a null symbol to model the alignment. Yu et al. (2016) improved this by replacing the null symbol with a dedicated learned transition probability. Both models are trained using a forwardbackward approach, marginalizing over all possible alignments. Our model differs from the above by learning the alignments independently, thus enabling a dependency between the encoder and decoder. While providing better results than Yu et al. (2016), this also simplifies the model training using a simple cross-entropy loss. Jaitly et al. (2015) proposed the Neural Transducer model, which is also trained on external alignments.",
      "startOffset" : 8,
      "endOffset" : 1779
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Ahlberg et al., 2015; Nicolai et al., 2015) with assumptions about the set of possible processes needed to create the output word. Our work was mainly inspired by Faruqui et al. (2016) which trained an independent encoder-decoder neural network for every inflection type in the training data, alleviating the need for feature engineering. Kann and Schütze (2016b,a) tackled the task with a single soft attention model (Bahdanau et al., 2014) for all inflection types, which resulted in the best submission at the SIGMORPHON 2016 shared task (Cotterell et al., 2016). Another closely related work (Rastogi et al., 2016) modeled the task with a WFST in which the arcs are learned by optimizing a global loss function over all the possible paths in the state graph while modeling contextual features with bi-directional LSTM’s. Another recent work (Kann et al., 2016) proposed performing neural multi-source morphological reinflection, generating an inflection from several source forms of a word. Previous works on neural sequence transduction include the RNN Transducer (Graves, 2012) which uses two independent RNN’s over monotonically aligned sequences to predict a distribution over the possible output symbols in each step, including a null symbol to model the alignment. Yu et al. (2016) improved this by replacing the null symbol with a dedicated learned transition probability. Both models are trained using a forwardbackward approach, marginalizing over all possible alignments. Our model differs from the above by learning the alignments independently, thus enabling a dependency between the encoder and decoder. While providing better results than Yu et al. (2016), this also simplifies the model training using a simple cross-entropy loss. Jaitly et al. (2015) proposed the Neural Transducer model, which is also trained on external alignments. They divide the input into blocks of a constant size and perform soft attention separately on each block. Lu et al. (2016) used a combination of an RNN encoder with a CRF layer to model the dependencies in the output sequence.",
      "startOffset" : 8,
      "endOffset" : 1986
    } ],
    "year" : 2017,
    "abstractText" : "We present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and nonneural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft attention (Bahdanau et al., 2014) models for the task, shedding some light on the features such models extract.",
    "creator" : "LaTeX with hyperref package"
  }
}