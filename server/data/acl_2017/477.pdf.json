{
  "name" : "477.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "From Characters to Words to in Between: Do We Capture Morphology?",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Continuous representations of words learned by neural networks are central to many NLP tasks (Cho et al., 2014; Chen and Manning, 2014; Dyer et al., 2015). However, directly mapping a finite set of word types to a continuous representation has well-known limitations. First, it makes a closed vocabulary assumption, enabling only generic out-of-vocabulary handling. Second, it cannot exploit systematic functional relationships in learning. For example, cat and cats stand in the same relationship as dog and dogs. While this re-\nlationship might be discovered for these specific frequent words, it does not help us learn that the same relationship also holds for the much rarer words tarsier and tarsiers.\nThese functional relationships reflect the fact that words are composed from smaller units of meaning, or morphemes. For instance, cats consists of two morphemes, cat and -s, with the latter shared by the words dogs and tarsiers. Modeling this effect is crucial for languages with rich morphology, where vocabulary sizes are larger, many more words are rare, and many more such functional relationships exist. Hence, some models produce word representations as a function of subword units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Schütze, 2015). However, a downside of these models is that they introduce a dependence on morphological segmentation or analysis.\nBut morphemes typically have similar orthographic representations across words. For example, the morpheme -s is realized as -es in finches, but this variation is limited, and the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al., 2013; ?; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (dos Santos and Zadrozny, 2014; Qiu et al., 2014). These models can also represent rare and unknown words, and they produce compact parameterizations. They have the added appeal that they do not depend on morphological analysis, and they raise a provocative question: does NLP benefit from explicit modeling of morphology, or can this be replaced entirely by modeling of characters?\nOur understanding of these models is incom-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nplete because they have been tested on different tasks, often compared only with direct word embeddings. A number of questions remain open:\n1. How do representations based on morphemes compare with those based on characters? 2. What is the best way to compose subword representations? 3. Do character representations adequately substitute for morphological analysis? 4. How do different representations interact with languages of different morphological typologies?\nThe last question is relevant, since, as Bender (2013) note, languages are typologically diverse, and behaviors exhibited by a model on one language may differ radically on others. Most models implicitly assume concatenative morphology, but typology of many widely-spoken languages is primarily non-concatenative, and it is unclear such models will behave on these languages.\nTo answer these questions, we performed a systematic comparison across different models for the simple and ubiquitous task of language modeling. We present experiments that vary (1) the type of subword unit; (2) the composition function; and (3) morphological typology. To understand the extent to which character-level models capture true morphological regularities, we present oracle experiments using human morphological annotations instead of automatic morphological segments. Our results show that:\n1. For most languages, character-level representations outperform the standard word representations. Most interestingly, a previously unstudied combination of character trigrams composed with bi-LSTMs performs best on the majority of languages. 2. Bi-LSTMs and CNNs are more effective composition functions than addition. 3. Character representations learn functional relationships between orthographically similar words, but are not as accurate as models with explicit knowledge of morphology. 4. Character-level models are effective across a range of morphological typologies, but orthography influences their effectiveness."
    }, {
      "heading" : "2 Morphological Typology",
      "text" : "A morpheme is the smallest unit of meaning in a word. Some morphemes express core meaning (roots), while others express one or more depen-\ndent features of the core meaning, such as person, gender, or aspect. A morphological analysis identifies the lemma and features of a word. A morph is the surface realization of a morpheme (Morley, 2000), which may vary from word to word. These distinctions are shown in Table 1.\nMorphological typology classifies languages based on the processes by which morphemes are composed to form words. While most languages will exhibit a variety of such processes, for any given language, some processes are much more frequent than others, and we will broadly identify our experimental languages with these processes.\nWhen morphemes are combined sequentially, the morphology is concatenative. However, morphemes can also be composed by nonconcatenative processes. We consider four broad categories of both concatenative and nonconcatenative processes in our experiments.\nFusional languages realize multiple features in a single concatenated morpheme. For example, English verbs can express number, person, and tense in a single morpheme:\nwanted (English) want + ed\nwant + VB+1st+SG+Past Agglutinative languages assign one feature per morpheme. Morphemes are concatenated to form a word and the morpheme boundaries are clear. For example (Haspelmath, 2010):\nokursam (Turkish) oku+r+sa+m\n“read”+AOR+COND+1SG Root and Pattern Morphology forms words by inserting consonants and vowels of dependent morphemes into a consonantal root based on a given pattern. For example, the Arabic root ktb (“write”) produces (Roark and Sproat, 2007):\nkatab “wrote” (Arabic) takaatab “wrote to each other” (Arabic)\nReduplication is a process where a word form is produced by repeating part or all of the root to express new features. For example:\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nanak “child” (Indonesian) anak-anak “children” (Indonesian)\nbuah “fruit” (Indonesian) buah-buahan “various fruits” (Indonesian)"
    }, {
      "heading" : "3 Representation Models",
      "text" : "We compare ten different models, varying subword units and composition functions commonly used in recent work (Table 2). Given word w, we compute its representation w as:\nw = f(Ws, σ(w)) (1)\nwhere σ is a deterministic function that returns a sequence of subword units; Ws is a parameter matrix of representations for the vocabulary of subword units; and f is a composition function which takes σ(w) and Ws as input and returns w. All of the representations that we consider take this form, varying only in f and σ."
    }, {
      "heading" : "3.1 Subword Units",
      "text" : "We consider four variants of σ in Equation 1, each returning a different type of subword unit: character, character trigram, or morph. Morphs are obtained from Morfessor (Smit et al., 2014) or a word segmentation based on Byte Pair Encoding (BPE; Gage (1994)), which has been shown to be effective for handling rare words in neural machine translation (Sennrich et al., 2016). BPE works by iteratively replacing frequent pairs of bytes with a single unused byte. For Morfessor, we use default parameters while for BPE we set the number of merge operations to 10,000.1 When we segment into character trigrams, we consider all trigrams in the word, including those covering notional beginning and end of word characters, as in Sperr et al. (2013). Example output of σ is shown in Table 3."
    }, {
      "heading" : "3.2 Composition Functions",
      "text" : "We use three variants of f in Eq. 1. The first constructs the representation w of word w by adding the representations of subwords s1, . . . , sn = σ(w), where the representation of si is vector si.\nw = n∑\ni=1\nsi (2)\n1BPE takes a single parameter: the number of merge operations. We tried different parameter values (1k, 10k, 100k) and manually examined the resulting segmentation on the English dataset. Qualitatively, 10k gave the most plausible segmentation and we used this setting across all languages.\nThe only subword unit that we don’t compose by addition is characters, since this will produce the same representation for many different words.\nOur second composition function is a bidirectional long-short-term memory (bi-LSTM), which we adapt based on its use in the characterlevel model of Ling et al. (2015) and its widespread use in NLP generally. Given si and the previous LSTM hidden state hi−1, an LSTM (Hochreiter and Schmidhuber, 1997) computes the following outputs for the subword at position i:\nhi = LSTM(si,hi−1) (3)\nŝi+1 = g(VT · hi) (4)\nwhere ŝi+1 is the predicted target subword, g is the softmax function and V is a weight matrix.\nA bi-LSTM (Graves et al., 2005) combines the final state of an LSTM over the input sequence with one over the reversed input sequence. Given the hidden state produced from the final input of the forward LSTM, hfwn and the hidden state produced from the final input of the backward LSTM hbw0 , we compute the word representation as:\nwt = Wf · hfwn + Wb · hbw0 + b (5)\nwhere Wf and Wb are parameter matrices and hfwn and hbw0 are forward and backward LSTM states.\nThe third composition function is a convolutional neural network (CNN) with highway layers, as in Kim et al. (2016). Let c1, . . . , ck be the sequence of characters of word w. The character embedding matrix is C ∈ Rd×k, where the i-th column corresponds to the embeddings of ci. We first apply a narrow convolution between C and a filter F ∈ Rd×n of width n to obtain a feature map f ∈ Rk−n+1. In particular, the computation of the j-th element of f is defined as\nf [j] = tanh(〈C[∗, j : j + n− 1],F〉+ b) (6)\nwhere 〈A,B〉 = Tr(ABT ) is the Frobenius inner product and b is a bias. The CNN model applies filters of varying width, representing features of character n-grams. We then calculate the maxover-time of each feature map.\nyj = max j f [j] (7)\nand concatenate them to derive the word representation wt = [y1, . . . , ym], where m is the number of filters applied. Highway layers allow some dimensions of wt to be carried or transformed. Since it can learn character n-grams directly, we only use the CNN with character input.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nModels Subword Unit(s) Composition Function Sperr et al. (2013) words, character n-grams addition Luong et al. (2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition dos Santos and Zadrozny (2014) words, characters CNN Cotterell and Schütze (2015) words, morphological analyses addition Sennrich et al. (2016) morphs (BPE) none Kim et al. (2016) characters CNN Ling et al. (2015) characters bi-LSTM Wieting et al. (2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Lee et al. (2016) characters CNN\nTable 2: Summary of previous work on representing words through compositions of subword units.\nUnit Output of σ(wants) Morfessor ˆwant, s$ BPE ˆw, ants$ char-trigram ˆwa, wan, ant, ts$ character ˆ, w, a, n, t, s, $ analysis want+VB, +3rd, +SG, +Pres\nTable 3: Input representations for wants\nFigure 1: Our LSTM-LM architecture."
    }, {
      "heading" : "3.3 Language Model",
      "text" : "We use language models (LM) because they are simple and fundamental to many NLP applications. Given a sequence of text s = w1, . . . , wT , our LM computes the probability of s as:\nP (w1, . . . , wT ) = T∏ t=1 P (yt|w1, . . . , wt−1) (8)\nwhere yt = wt if wt is in the output vocabulary and yt = UNK otherwise.\nOur language model is an LSTM variant of recurrent neural network language (RNN) LM (Mikolov et al., 2010). At time step t, it receives input wt and predicts yt+1. Using Eq. 1, it first computes representation wt of wt. Given this representation and previous state ht−1, it produces a new state ht and predicts yt+1:\nht = LSTM(wt,ht−1) (9)\nŷt+1 = g(VT · ht) (10)\nwhere g is a softmax function over the vocabulary yielding the probability in Equation 8. Note that this design means that we can predict only words from a fixed output vocabulary, so our models differ only in their representation of context words. This makes design it possible to compare language models using perplexity on a fixed output space, though open vocabulary word prediction is an interesting direction for future work.\nThe complete architecture of our system is shown in Figure 1, showing segmentation function σ and composition function f from Equation 1."
    }, {
      "heading" : "4 Experiments",
      "text" : "We perform experiments on ten languages (Table 4). We use datasets from Ling et al. (2015) for English and Turkish. For Czech and Russian we use Universal Dependencies (UD) v1.3 (Nivre et al., 2015). For other languages, we use prepro-\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nTypology Languages #tokens #types Fusional Czech 1.2M 125.4K English 1.2M 81.1K Russian 0.8M 103.5K Agglutinative Finnish 1.2M 188.4K Japanese 1.2M 59.2K Turkish 0.6M 126.2K Root&Pattern Arabic 1.4M 137.5K Hebrew 1.1M 104.9K Reduplication Indonesian 1.2M 76.5K Malaysian 1.2M 77.7K\nTable 4: Statistics of our datasets.\ncessed Wikipedia data (Al-Rfou et al., 2013).2 For each dataset, we use approximately 1.2M tokens to train, and approximately 150K tokens each for development and testing. Preprocessing involves lowercasing (except for character models) and removing hyperlinks.\nTo ensure that we compared models and not implementations, we reimplemented all models in a single framework using Tensorflow (Abadi et al., 2015). We use a common setup for all experiments based on that of (Ling et al., 2015; Kim et al., 2016; Miyamoto and Cho, 2016). In preliminary experiments, we confirmed that our models produced similar patterns of perplexities for the reimplemented word and character LSTM models of Ling et al. (2015). Even following detailed discussion with Ling (p.c.), we were unable to reproduce their perplexities exactly—our English reimplementation gives lower perplexities; our Turkish higher—but we do reproduce their general result that character bi-LSTMs outperform word models. We suspect that different preprocessing and the stochastic learning explains differences in perplexities. Our final model with bi-LSTMs composition follows Miyamoto and Cho (2016) as it gives us the same results reported in their paper."
    }, {
      "heading" : "4.1 Training and Evaluation",
      "text" : "Our LSTM-LM uses two hidden layers with 200 hidden units and representation vectors for words, characters, and morphs all have dimension 200. All parameters are initialized uniformly at random from -0.1 to 0.1, trained by stochastic gradient de-\n2The Arabic and Hebrew dataset are unvocalized. Japanese mixes Kanji, Katakana, Hiragana, and Latin characters (for foreign words). Hence, a Japanese character can correspond to a character, syllable, or word. The preprocessed dataset is already word-segmented.\nscent with mini-batch size of 32, time steps of 20, for 50 epochs. To avoid overfitting, we apply dropout with probability 0.5. For all models which do not use bi-LSTM composition, we start with a learning rate of 1.0 and decrease it by half if the validation perplexity does not decrease by 0.1 after 3 epochs. For models with bi-LSTMs composition, we use a constant learning rate of 0.2 and stop training when validation perplexity does not improve after 3 epochs. For the character CNN model, we use the same settings as the small model of Kim et al. (2016).\nTo make our results comparable to Ling et al. (2015), for each language we limit the output vocabulary to the most frequent 5,000 training words plus an unknown word token. To learn to predict unknown words, we follow Ling et al. (2015): in training, words that occur only once are stochastically replaced with the unknown token with probability 0.5. To evaluate the models, we compute perplexity on the test data."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : "Table 5 presents our main results. In six of ten languages, character-trigram representations composed with bi-LSTMs achieve the lowest perplexities. As far as we know, this particular model has not been tested before, though it is similar (but more general) than the model of Sperr et al. (2013). We can see that the performance of character, character trigrams, and BPE are very competitive. Composition by bi-LSTMs or CNN is more effective than addition, except for Turkish. We also observe that BPE always outperforms Morfessor, even for the agglutinative languages. We now turn to a more detailed analysis by morphological typology.\nFusional languages. For these languages, character trigrams composed with bi-LSTMs outperformed all other models, particularly for Czech and Russian (up to 20%), which is unsurprising since both are morphologically richer than English.\nAgglutinative languages. We observe different results for each language. For Finnish, character trigrams composed with bi-LSTMs achieves the best perplexity. Surprisingly, for Turkish character trigrams composed via addition is best and addition also performs quite well for other representations, potentially useful since the addition function is simpler and faster than bi-LSTMs. We\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nLanguage word character char trigrams BPE Morfessor\n%imp bi-lstm CNN add bi-lstm add bi-lstm add bi-lstm\nCzech 41.46 34.25 36.60 42.73 33.59 49.96 33.74 47.74 36.87 18.98 English 46.40 43.53 44.67 45.41 42.97 47.51 43.30 49.72 49.72 7.39 Russian 34.93 28.44 29.47 35.15 27.72 40.10 28.52 39.60 31.31 20.64 Finnish 24.21 20.05 20.29 24.89 18.62 26.77 19.08 27.79 22.45 23.09 Japanese 98.14 98.14 91.63 101.99 101.09 126.53 96.80 111.97 99.23 6.63 Turkish 66.97 54.46 55.07 50.07 54.23 59.49 57.32 62.20 62.70 25.24 Arabic 48.20 42.02 43.17 50.85 39.87 50.85 42.79 52.88 45.46 17.28 Hebrew 38.23 31.63 33.19 39.67 30.40 44.15 32.91 44.94 34.28 20.48 Indonesian 46.07 45.47 46.60 58.51 45.96 59.17 43.37 59.33 44.86 5.86 Malay 54.67 53.01 50.56 68.51 50.74 68.99 51.21 68.20 52.50 7.52\nTable 5: Language model perplexities on test, showing improvement of the best system over words.\nsuspect that this is due to the fact that Turkish morphemes are reasonably short, hence wellapproximated by character trigrams. For Japanese, we observe that the improvements from character representation more modest than in other language.\nRoot and Pattern. For these languages, character trigrams composed with bi-LSTMs also achieve the best perplexity. We had wondered with CNNs would be more effective for root-andpatter morphology, but since these data are unvocalized, it is more likely that non-concatenative effects are minimized, though we do still find morphological variants with consonantal inflections that behave more like concatenation. For example, maktab (root:ktb) is written as mktb. We suspect this makes character trigrams quite effective since they match the tri-consonantal root patterns among words which share the same root.\nReduplication. For Indonesian, BPE morphs composed with bi-LSTMs model obtain the best perplexity. For Malay, the character CNN outperforms other models. However, these improvements are small compared to other languages. This likely reflects that Indonesian and Malay are only moderately inflected, where inflection involves both concatenative and non-concatenative processes."
    }, {
      "heading" : "5.1 Effects of Morphological Analysis",
      "text" : "In the experiments above, we used unsupervised morphological segmentation as a proxy for morphological analysis (Table 3). However, as discussed in Section 2, this is quite approximate, so it is natural to wonder what would happen if we had an ideal morphological analysis. If character-\nLanguages Addition bi-LSTM Czech 51.8 30.07\nRussian 41.82 26.44\nTable 6: Perplexity results using hand-annotated morphological analyses (cf. Table 5).\nlevel models were adequate to model the effects of morphology, then they would have similar predictive accuracy. To answer this question, we used the human-annotated morphological analyses provided in the UD datasets for Czech and Russian, the only languages in our set for which these analyses were available. In these experiments we treat the lemma and each morphological feature as a subword unit.\nThe results (Table 6) show that bi-LSTM composition of these representations outperforms all other models for both languages. These results demonstrate that neither character representations nor unsupervised segmentation is a perfect replacement for manual morphological analysis, at least in terms of predictive accuracy. Especially in light of character-level results, they also imply that current methods of unsupervised morphological analysis are inadequate substitutes for morphological analysis."
    }, {
      "heading" : "5.2 Automatic Morphological Analysis",
      "text" : "The oracle experiments show promising results if we have annotated data. But these annotations are expensive, so investigated the effects of automatic morphological analysis. We obtained analyses for Arabic with the MADAMIRA (Pasha et al., 2014). As in the experiment using annotations, we treated each morphological feature as a sub-\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n666\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nword unit. The resulting perplexities of 71.94 and 42.85 for addition and bi-LSTMs, respectively, are worse than those obtained with character trigrams (39.87), though it approaches the best perplexities."
    }, {
      "heading" : "5.3 Targeted Perplexity Results",
      "text" : "A difficulty in interpreting the results of Table 5 with respect to specific morphological processes is that perplexity is measured for all words. But these processes do not apply to all words, so it may be that the effects of specific morphological processes are washed out. To get a clearer picture, we measured perplexity for only specific subsets of words in our test data: specifically, given target word wi, we measure perplexity of word wi+1. In other words, we analyze the perplexities when the inflected words of interest are in the most recent history, exploiting the recency bias of our LSTM-LM. This is the perplexity most likely to be strongly affected by different representations, since we do not vary representations of the predicted word itself.\nWe look at several cases: nouns and verbs in Czech and Russian, where word classes can be identified from annotations, and reduplication in Indonesian, which we can identify mostly automatically. For each analysis, we also distinguish between frequent cases, where the inflected word occurs more than ten times in the training data, and rare cases, where it occurs fewer than ten times. We compare only bi-LSTM models.\nFor Czech and Russian, we again use the UD annotation to identify words of interest. The results (Table 7), show that manual morphological analysis uniformly outperforms other subword models, with an especially strong effect for Czech nouns, suggesting that other models do not capture useful predictive properties of a morphological analysis. We do however note that character trigrams achieve low perplexities in most cases, similar to overall results (Table 5). We also observe that the subword models are more effective for rare verbs.\nFor Indonesian, we exploit the fact that the hyphen symbol ‘-’ typically separates the first and second occurrence of a reduplicated morpheme, as in the examples of Section 2. We use the presence of word tokens containing hyphens to estimate the percentage of those exhibiting reduplication. As shown in Table 8, the numbers are quite low.\nTable 9 shows results for reduplication. In\nModel all frequent rare word 101.71 91.71 156.98 characters 99.21 91.35 137.42 BPE 117.2 108.86 156.81\nTable 9: Average perplexities of words that occur after reduplicated words in the test set.\ncontrast with the overall results, BPE bi-LSTMs model produce the worse perplexities, while character bi-LSTMs produce the best, suggesting that these models are more effective for reduplication.\nLooking more closely at BPE segmentation of reduplicated words, we found that only 6 of 252 reduplicated words have a correct word segmentation, with the reduplicated morpheme often being combined differently with the notional startof-word or hyphen character. One the other hand BPE correctly learns 8 out of 9 Indonesian prefixes and 4 out of 7 Indonesian suffixes.3 This analysis supports our intuition that the improvement from BPE might come from its modeling of concatenative morphology.\n3We use Indonesian preprefixes, prefixes and suffixes listed in Larasati et al. (2011)\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nModel Frequent Words Rare Words OOV wordsman including relatively unconditional hydroplane uploading foodism\nword\nperson like extremely nazi molybdenum - - anyone featuring making fairly your - - children include very joints imperial - -\nmen includes quite supreme intervene - -\nBPE ii called newly unintentional emphasize upbeat vigilantism LSTM hill involve never ungenerous heartbeat uprising pyrethrumtext like essentially unanimous hybridized handling pausanias netherlands creating least unpalatable unplatable hand-colored footway char- mak include resolutely unconstitutional selenocysteine drifted tuaregs trigrams vill includes regeneratively constitutional guerrillas affected quft LSTM cow undermining reproductively unimolecular scrofula conflicted subjectivism\nmaga under commonly medicinal seleucia convicted tune-up\nchar- mayr inclusion relates undamaged hydrolyzed musagte formulas LSTM many insularity replicate unmyelinated hydraulics mutualism formallymary includes relativity unconditionally hysterotomy mutualists fecal may include gravestones uncoordinated hydraulic meursault foreland char- mtn include legislatively unconventional hydroxyproline unloading fordism CNN mann includes lovely unintentional hydrate loading dadaismjan excluding creatively unconstitutional hydrangea upgrading popism nun included negatively untraditional hyena upholding endemism\nTable 10: Top nearest neighbours (cosine similarities) of semantically and syntactically similar words.\nQuery Top nearest neighbours kota-kota wilayah-wilayah (areas), pulau-pulau (islands), negara-negara (countries), (cities) bahasa-bahasa (languages), koloni-koloni (colonies)\nberlembah-lembah berargumentasi (argue), bercakap-cakap (converse), berkemauan (will), (have many valleys) berimplikasi (imply), berketebalan (have a thickness)\nTable 11: Top nearest neighbours (cosine similarities) of Indonesian reduplicated words."
    }, {
      "heading" : "5.4 Qualitative Analysis",
      "text" : "Table 10 presents nearest neighbors under cosine similarity for in-vocabulary, rare, and out-ofvocabulary (OOV) words.4 For frequent words, standard word embeddings are clearly superior for lexical meaning. Character and morph representations tend to find words that are orthographically similar, suggesting that they better at modeling dependent than root morphemes. The same pattern holds for rare and OOV words. We suspect that the subword models outperform words on language modeling because they exploit affixes to signal word class. We also noticed similar patterns in Japanese.\nWe analyze reduplication by querying reduplicated words to find their nearest neighbors using the BPE bi-LSTM model. If the model were sensitive to reduplication, we would expect to see the morphological variants word in the top-n nearest neighbors. However, from Table 11, this is not so. With the partially reduplicated query berlembahlembah, we do not find the lemma lembah.\n4https://radimrehurek.com/gensim/"
    }, {
      "heading" : "6 Conclusion",
      "text" : "We presented a systematic comparison of word representation models with different levels of morphological awareness, across languages with different morphological typologies. Our results confirm previous findings that character-level models are effective for many languages, but these models do not match the predictive accuracy of model with explicit knowledge of morphology, and our qualitative analysis suggests that they learn orthographic similarity of affixes, and lose the meaning of root morphemes.\nAlthough morphological analyses are available in limited quantities, our results suggest that there might be utility in semi-supervised learning from partially annotated data. Across languages with different typologies, our experiments show that the subword unit models are most effective on agglutinative languages. However, these results do not generalize to all languages, since factors such as morphology and orthography affect the utility of these representations. We plan to explore these effects in future work.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org",
      "author" : [ "sudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "sudevan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "sudevan et al\\.",
      "year" : 2015
    }, {
      "title" : "Polyglot: Distributed word representations for multilingual nlp",
      "author" : [ "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena." ],
      "venue" : "Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Association for Computa-",
      "citeRegEx" : "Al.Rfou et al\\.,? 2013",
      "shortCiteRegEx" : "Al.Rfou et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax",
      "author" : [ "Emily M. Bender." ],
      "venue" : "Morgan & Claypool Publishers.",
      "citeRegEx" : "Bender.,? 2013",
      "shortCiteRegEx" : "Bender.",
      "year" : 2013
    }, {
      "title" : "Enriching word vectors with subword information",
      "author" : [ "Piotr Bojanowski", "Edouard Grave", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "CoRR abs/1607.04606. http://arxiv.org/abs/1607.04606.",
      "citeRegEx" : "Bojanowski et al\\.,? 2016",
      "shortCiteRegEx" : "Bojanowski et al\\.",
      "year" : 2016
    }, {
      "title" : "Compositional Morphology for Word Representations and Language Modelling",
      "author" : [ "Jan A. Botha", "Phil Blunsom." ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning (ICML). Beijing, China. *Award for best application paper*.",
      "citeRegEx" : "Botha and Blunsom.,? 2014",
      "shortCiteRegEx" : "Botha and Blunsom.",
      "year" : 2014
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "Danqi Chen", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-",
      "citeRegEx" : "Chen and Manning.,? 2014",
      "shortCiteRegEx" : "Chen and Manning.",
      "year" : 2014
    }, {
      "title" : "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Morphological word-embeddings",
      "author" : [ "Ryan Cotterell", "Hinrich Schütze." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Cotterell and Schütze.,? 2015",
      "shortCiteRegEx" : "Cotterell and Schütze.",
      "year" : 2015
    }, {
      "title" : "Learning character-level representations for part-of-speech tagging",
      "author" : [ "Cı́cero Nogueira dos Santos", "Bianca Zadrozny" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "Santos and Zadrozny.,? \\Q2014\\E",
      "shortCiteRegEx" : "Santos and Zadrozny.",
      "year" : 2014
    }, {
      "title" : "Transitionbased dependency parsing with stack long shortterm memory",
      "author" : [ "Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computa-",
      "citeRegEx" : "Dyer et al\\.,? 2015",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2015
    }, {
      "title" : "A new algorithm for data compression",
      "author" : [ "Philip Gage." ],
      "venue" : "C Users J. 12(2):23–38. http://dl.acm.org/citation.cfm?id=177910.177914.",
      "citeRegEx" : "Gage.,? 1994",
      "shortCiteRegEx" : "Gage.",
      "year" : 1994
    }, {
      "title" : "Multilingual language processing from bytes",
      "author" : [ "Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Gillick et al\\.,? 2016",
      "shortCiteRegEx" : "Gillick et al\\.",
      "year" : 2016
    }, {
      "title" : "Bidirectional lstm networks for improved phoneme classification and recognition",
      "author" : [ "Alex Graves", "Santiago Fernández", "Jürgen Schmidhuber." ],
      "venue" : "Proceedings of the 15th International Conference on Artificial Neu-",
      "citeRegEx" : "Graves et al\\.,? 2005",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2005
    }, {
      "title" : "Understanding Morphology",
      "author" : [ "Martin Haspelmath." ],
      "venue" : "Understanding Language Series. Arnold, London, second edition.",
      "citeRegEx" : "Haspelmath.,? 2010",
      "shortCiteRegEx" : "Haspelmath.",
      "year" : 2010
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Comput. 9(8):1735– 1780. https://doi.org/10.1162/neco.1997.9.8.1735.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander Rush." ],
      "venue" : "Proceedings of the 2016 Conference on Artificial Intelligence (AAAI).",
      "citeRegEx" : "Kim et al\\.,? 2016",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2016
    }, {
      "title" : "Indonesian Morphology Tool (MorphInd): Towards an Indonesian",
      "author" : [ "Septina Dian Larasati", "Vladislav Kuboň", "Daniel Zeman" ],
      "venue" : null,
      "citeRegEx" : "Larasati et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Larasati et al\\.",
      "year" : 2011
    }, {
      "title" : "Fully character-level neural machine translation without explicit segmentation",
      "author" : [ "Jason Lee", "Kyunghyun Cho", "Thomas Hofmann." ],
      "venue" : "CoRR abs/1610.03017. http://arxiv.org/abs/1610.03017.",
      "citeRegEx" : "Lee et al\\.,? 2016",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2016
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Ling et al\\.,? 2015",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Better word representations with recursive neural networks for morphology",
      "author" : [ "Thang Luong", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the Seventeenth Conference on Computational Natural Language Learning. Association for",
      "citeRegEx" : "Luong et al\\.,? 2013",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2013
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafit", "Luks Burget", "Jan Cernock", "Sanjeev Khudanpur." ],
      "venue" : "Takao Kobayashi, Keikichi Hirose, and Satoshi Nakamura, editors, INTERSPEECH. ISCA, pages 1045–1048.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Gated word-character recurrent language model",
      "author" : [ "Yasumasa Miyamoto", "Kyunghyun Cho." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Lin-",
      "citeRegEx" : "Miyamoto and Cho.,? 2016",
      "shortCiteRegEx" : "Miyamoto and Cho.",
      "year" : 2016
    }, {
      "title" : "Syntax in Functional Grammar: An Introduction to Lexicogrammar in Systemic Linguistics",
      "author" : [ "G. David Morley." ],
      "venue" : "Continuum.",
      "citeRegEx" : "Morley.,? 2000",
      "shortCiteRegEx" : "Morley.",
      "year" : 2000
    }, {
      "title" : "Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of ara",
      "author" : [ "Arfath Pasha", "Mohamed Al-Badrashiny", "Mona Diab", "Ahmed El Kholy", "Ramy Eskander", "Nizar Habash", "Manoj Pooleery", "Owen Rambow", "Ryan Roth" ],
      "venue" : null,
      "citeRegEx" : "Pasha et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pasha et al\\.",
      "year" : 2014
    }, {
      "title" : "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss",
      "author" : [ "Barbara Plank", "Anders Søgaard", "Yoav Goldberg." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Plank et al\\.,? 2016",
      "shortCiteRegEx" : "Plank et al\\.",
      "year" : 2016
    }, {
      "title" : "Co-learning of word representations and morpheme representations",
      "author" : [ "Siyu Qiu", "Qing Cui", "Jiang Bian", "Bin Gao", "TieYan Liu." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers.",
      "citeRegEx" : "Qiu et al\\.,? 2014",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2014
    }, {
      "title" : "Computational Approach to Morphology and Syntax",
      "author" : [ "Brian Roark", "Richard Sproat." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Roark and Sproat.,? 2007",
      "shortCiteRegEx" : "Roark and Sproat.",
      "year" : 2007
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Morfessor 2.0: Toolkit for statistical morphological segmentation",
      "author" : [ "Peter Smit", "Sami Virpioja", "Stig-Arne Grönroos", "Mikko Kurimo" ],
      "venue" : "In Proceedings of the Demonstrations at the 14th Conference",
      "citeRegEx" : "Smit et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Smit et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Continuous representations of words learned by neural networks are central to many NLP tasks (Cho et al., 2014; Chen and Manning, 2014; Dyer et al., 2015).",
      "startOffset" : 93,
      "endOffset" : 154
    }, {
      "referenceID" : 5,
      "context" : "Continuous representations of words learned by neural networks are central to many NLP tasks (Cho et al., 2014; Chen and Manning, 2014; Dyer et al., 2015).",
      "startOffset" : 93,
      "endOffset" : 154
    }, {
      "referenceID" : 9,
      "context" : "Continuous representations of words learned by neural networks are central to many NLP tasks (Cho et al., 2014; Chen and Manning, 2014; Dyer et al., 2015).",
      "startOffset" : 93,
      "endOffset" : 154
    }, {
      "referenceID" : 19,
      "context" : "Hence, some models produce word representations as a function of subword units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Schütze, 2015).",
      "startOffset" : 132,
      "endOffset" : 206
    }, {
      "referenceID" : 4,
      "context" : "Hence, some models produce word representations as a function of subword units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Schütze, 2015).",
      "startOffset" : 132,
      "endOffset" : 206
    }, {
      "referenceID" : 7,
      "context" : "Hence, some models produce word representations as a function of subword units obtained from morphological segmentation or analysis (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Schütze, 2015).",
      "startOffset" : 132,
      "endOffset" : 206
    }, {
      "referenceID" : 18,
      "context" : "For example, the morpheme -s is realized as -es in finches, but this variation is limited, and the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al.",
      "startOffset" : 219,
      "endOffset" : 256
    }, {
      "referenceID" : 15,
      "context" : "For example, the morpheme -s is realized as -es in finches, but this variation is limited, and the general relationship between morphology and orthography can be exploited by composing the representations of characters (Ling et al., 2015; Kim et al., 2016), character n-grams (Sperr et al.",
      "startOffset" : 219,
      "endOffset" : 256
    }, {
      "referenceID" : 3,
      "context" : ", 2016), character n-grams (Sperr et al., 2013; ?; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al.",
      "startOffset" : 27,
      "endOffset" : 100
    }, {
      "referenceID" : 4,
      "context" : ", 2016), character n-grams (Sperr et al., 2013; ?; Bojanowski et al., 2016; Botha and Blunsom, 2014), bytes (Plank et al.",
      "startOffset" : 27,
      "endOffset" : 100
    }, {
      "referenceID" : 24,
      "context" : ", 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (dos Santos and Zadrozny, 2014; Qiu et al.",
      "startOffset" : 40,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : ", 2016; Botha and Blunsom, 2014), bytes (Plank et al., 2016; Gillick et al., 2016), or combinations thereof (dos Santos and Zadrozny, 2014; Qiu et al.",
      "startOffset" : 40,
      "endOffset" : 82
    }, {
      "referenceID" : 25,
      "context" : ", 2016), or combinations thereof (dos Santos and Zadrozny, 2014; Qiu et al., 2014).",
      "startOffset" : 33,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "How do different representations interact with languages of different morphological typologies? The last question is relevant, since, as Bender (2013) note, languages are typologically diverse, and behaviors exhibited by a model on one language may differ radically on others.",
      "startOffset" : 137,
      "endOffset" : 151
    }, {
      "referenceID" : 22,
      "context" : "A morph is the surface realization of a morpheme (Morley, 2000), which may vary from word to word.",
      "startOffset" : 49,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "For example (Haspelmath, 2010): okursam (Turkish) oku+r+sa+m “read”+AOR+COND+1SG Root and Pattern Morphology forms words by inserting consonants and vowels of dependent morphemes into a consonantal root based on a given pattern.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 26,
      "context" : "For example, the Arabic root ktb (“write”) produces (Roark and Sproat, 2007): katab “wrote” (Arabic) takaatab “wrote to each other” (Arabic) Reduplication is a process where a word form is produced by repeating part or all of the root to express new features.",
      "startOffset" : 52,
      "endOffset" : 76
    }, {
      "referenceID" : 28,
      "context" : "Morphs are obtained from Morfessor (Smit et al., 2014) or a word segmentation based on Byte Pair Encoding (BPE; Gage (1994)), which has been shown to be effective for handling rare words in neural machine translation (Sennrich et al.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 27,
      "context" : ", 2014) or a word segmentation based on Byte Pair Encoding (BPE; Gage (1994)), which has been shown to be effective for handling rare words in neural machine translation (Sennrich et al., 2016).",
      "startOffset" : 170,
      "endOffset" : 193
    }, {
      "referenceID" : 10,
      "context" : ", 2014) or a word segmentation based on Byte Pair Encoding (BPE; Gage (1994)), which has been shown to be effective for handling rare words in neural machine translation (Sennrich et al.",
      "startOffset" : 65,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : ", 2014) or a word segmentation based on Byte Pair Encoding (BPE; Gage (1994)), which has been shown to be effective for handling rare words in neural machine translation (Sennrich et al., 2016). BPE works by iteratively replacing frequent pairs of bytes with a single unused byte. For Morfessor, we use default parameters while for BPE we set the number of merge operations to 10,000.1 When we segment into character trigrams, we consider all trigrams in the word, including those covering notional beginning and end of word characters, as in Sperr et al. (2013). Example output of σ is shown in Table 3.",
      "startOffset" : 65,
      "endOffset" : 563
    }, {
      "referenceID" : 14,
      "context" : "Given si and the previous LSTM hidden state hi−1, an LSTM (Hochreiter and Schmidhuber, 1997) computes the following outputs for the subword at position i: hi = LSTM(si,hi−1) (3) ŝi+1 = g(V · hi) (4)",
      "startOffset" : 58,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "Our second composition function is a bidirectional long-short-term memory (bi-LSTM), which we adapt based on its use in the characterlevel model of Ling et al. (2015) and its widespread use in NLP generally.",
      "startOffset" : 148,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "A bi-LSTM (Graves et al., 2005) combines the final state of an LSTM over the input sequence with one over the reversed input sequence.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 12,
      "context" : "A bi-LSTM (Graves et al., 2005) combines the final state of an LSTM over the input sequence with one over the reversed input sequence. Given the hidden state produced from the final input of the forward LSTM, h n and the hidden state produced from the final input of the backward LSTM h 0 , we compute the word representation as: wt = Wf · h n + Wb · h 0 + b (5) where Wf and Wb are parameter matrices and h n and h 0 are forward and backward LSTM states. The third composition function is a convolutional neural network (CNN) with highway layers, as in Kim et al. (2016). Let c1, .",
      "startOffset" : 11,
      "endOffset" : 572
    }, {
      "referenceID" : 12,
      "context" : "(2013) words, character n-grams addition Luong et al. (2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al.",
      "startOffset" : 41,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al.",
      "startOffset" : 39,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition dos Santos and Zadrozny (2014) words, characters CNN Cotterell and Schütze (2015) words, morphological analyses addition Sennrich et al.",
      "startOffset" : 39,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition dos Santos and Zadrozny (2014) words, characters CNN Cotterell and Schütze (2015) words, morphological analyses addition Sennrich et al.",
      "startOffset" : 39,
      "endOffset" : 183
    }, {
      "referenceID" : 3,
      "context" : "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition dos Santos and Zadrozny (2014) words, characters CNN Cotterell and Schütze (2015) words, morphological analyses addition Sennrich et al.",
      "startOffset" : 39,
      "endOffset" : 234
    }, {
      "referenceID" : 3,
      "context" : "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition dos Santos and Zadrozny (2014) words, characters CNN Cotterell and Schütze (2015) words, morphological analyses addition Sennrich et al. (2016) morphs (BPE) none Kim et al.",
      "startOffset" : 39,
      "endOffset" : 296
    }, {
      "referenceID" : 3,
      "context" : "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition dos Santos and Zadrozny (2014) words, characters CNN Cotterell and Schütze (2015) words, morphological analyses addition Sennrich et al. (2016) morphs (BPE) none Kim et al. (2016) characters CNN Ling et al.",
      "startOffset" : 39,
      "endOffset" : 332
    }, {
      "referenceID" : 3,
      "context" : "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition dos Santos and Zadrozny (2014) words, characters CNN Cotterell and Schütze (2015) words, morphological analyses addition Sennrich et al. (2016) morphs (BPE) none Kim et al. (2016) characters CNN Ling et al. (2015) characters bi-LSTM Wieting et al.",
      "startOffset" : 39,
      "endOffset" : 366
    }, {
      "referenceID" : 3,
      "context" : "(2013) morphs (Morfessor) recursive NN Botha and Blunsom (2014) words, morphs (Morfessor) addition Qiu et al. (2014) words, morphs (Morfessor) addition dos Santos and Zadrozny (2014) words, characters CNN Cotterell and Schütze (2015) words, morphological analyses addition Sennrich et al. (2016) morphs (BPE) none Kim et al. (2016) characters CNN Ling et al. (2015) characters bi-LSTM Wieting et al. (2016) character n-grams addition Bojanowski et al.",
      "startOffset" : 39,
      "endOffset" : 407
    }, {
      "referenceID" : 3,
      "context" : "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al.",
      "startOffset" : 34,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Lee et al.",
      "startOffset" : 34,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Lee et al.",
      "startOffset" : 34,
      "endOffset" : 177
    }, {
      "referenceID" : 3,
      "context" : "(2016) character n-grams addition Bojanowski et al. (2016) character n-grams addition Vylomova et al. (2016) characters, morphs (Morfessor) bi-LSTM, CNN Miyamoto and Cho (2016) words, characters bi-LSTM Lee et al. (2016) characters CNN",
      "startOffset" : 34,
      "endOffset" : 221
    }, {
      "referenceID" : 20,
      "context" : "Our language model is an LSTM variant of recurrent neural network language (RNN) LM (Mikolov et al., 2010).",
      "startOffset" : 84,
      "endOffset" : 106
    }, {
      "referenceID" : 18,
      "context" : "We use datasets from Ling et al. (2015) for English and Turkish.",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "cessed Wikipedia data (Al-Rfou et al., 2013).",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 18,
      "context" : "We use a common setup for all experiments based on that of (Ling et al., 2015; Kim et al., 2016; Miyamoto and Cho, 2016).",
      "startOffset" : 59,
      "endOffset" : 120
    }, {
      "referenceID" : 15,
      "context" : "We use a common setup for all experiments based on that of (Ling et al., 2015; Kim et al., 2016; Miyamoto and Cho, 2016).",
      "startOffset" : 59,
      "endOffset" : 120
    }, {
      "referenceID" : 21,
      "context" : "We use a common setup for all experiments based on that of (Ling et al., 2015; Kim et al., 2016; Miyamoto and Cho, 2016).",
      "startOffset" : 59,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "cessed Wikipedia data (Al-Rfou et al., 2013).2 For each dataset, we use approximately 1.2M tokens to train, and approximately 150K tokens each for development and testing. Preprocessing involves lowercasing (except for character models) and removing hyperlinks. To ensure that we compared models and not implementations, we reimplemented all models in a single framework using Tensorflow (Abadi et al., 2015). We use a common setup for all experiments based on that of (Ling et al., 2015; Kim et al., 2016; Miyamoto and Cho, 2016). In preliminary experiments, we confirmed that our models produced similar patterns of perplexities for the reimplemented word and character LSTM models of Ling et al. (2015). Even following detailed discussion with Ling (p.",
      "startOffset" : 23,
      "endOffset" : 706
    }, {
      "referenceID" : 1,
      "context" : "cessed Wikipedia data (Al-Rfou et al., 2013).2 For each dataset, we use approximately 1.2M tokens to train, and approximately 150K tokens each for development and testing. Preprocessing involves lowercasing (except for character models) and removing hyperlinks. To ensure that we compared models and not implementations, we reimplemented all models in a single framework using Tensorflow (Abadi et al., 2015). We use a common setup for all experiments based on that of (Ling et al., 2015; Kim et al., 2016; Miyamoto and Cho, 2016). In preliminary experiments, we confirmed that our models produced similar patterns of perplexities for the reimplemented word and character LSTM models of Ling et al. (2015). Even following detailed discussion with Ling (p.c.), we were unable to reproduce their perplexities exactly—our English reimplementation gives lower perplexities; our Turkish higher—but we do reproduce their general result that character bi-LSTMs outperform word models. We suspect that different preprocessing and the stochastic learning explains differences in perplexities. Our final model with bi-LSTMs composition follows Miyamoto and Cho (2016) as it gives us the same results reported in their paper.",
      "startOffset" : 23,
      "endOffset" : 1158
    }, {
      "referenceID" : 15,
      "context" : "For the character CNN model, we use the same settings as the small model of Kim et al. (2016). To make our results comparable to Ling et al.",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "For the character CNN model, we use the same settings as the small model of Kim et al. (2016). To make our results comparable to Ling et al. (2015), for each language we limit the output vocabulary to the most frequent 5,000 training words plus an unknown word token.",
      "startOffset" : 76,
      "endOffset" : 148
    }, {
      "referenceID" : 15,
      "context" : "For the character CNN model, we use the same settings as the small model of Kim et al. (2016). To make our results comparable to Ling et al. (2015), for each language we limit the output vocabulary to the most frequent 5,000 training words plus an unknown word token. To learn to predict unknown words, we follow Ling et al. (2015): in training, words that occur only once are stochastically replaced with the unknown token with probability 0.",
      "startOffset" : 76,
      "endOffset" : 332
    }, {
      "referenceID" : 23,
      "context" : "We obtained analyses for Arabic with the MADAMIRA (Pasha et al., 2014).",
      "startOffset" : 50,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : "We use Indonesian preprefixes, prefixes and suffixes listed in Larasati et al. (2011)",
      "startOffset" : 63,
      "endOffset" : 86
    } ],
    "year" : 2017,
    "abstractText" : "Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results largely confirm previous findings that character representations are effective across many languages, though we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most other settings. However, we also find room for improvement: character models do not match the predictive accuracy of a model with access to explicit morphological analyses.",
    "creator" : "LaTeX with hyperref package"
  }
}