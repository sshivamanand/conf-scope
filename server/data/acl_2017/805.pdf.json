{
  "name" : "805.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "TextFlow: A Text Similarity Measure based on Continuous Sequences",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Background",
      "text" : "The number of pages required to print the content of the World Wide Web was estimated to 305 billion in a 2015 article1. While a big part of this content consists of visual information such as pictures and videos, texts also continue growing at a very high pace. A recent study shows that the average webpage weights 1200KB with plain text accounting for up to 16% of that size2.\nWhile efficient distribution of textual data and computations are the key to deal with the increas-\n1http://goo.gl/p9lt7V 2http://goo.gl/c41wpa\ning scale of textual search, similarity measures still play an important role in refining search results to more specific needs such as the recognition of paraphrases and textual entailment, plagiarism detection and fine-grained ranking of information. These tasks also often target dedicated document collections for domain oriented research where text similarity measures can be directly applied.\nFinding relevant approaches to compute text similarity motivated a lot of research in the last decades (Sahami and Heilman, 2006; Hatzivassiloglou et al., 1999), and more recently with deep learning methods (Socher et al., 2011; Yih et al., 2011; Severyn and Moschitti, 2015). However, most of the recent advances focused on designing high performance classification methods, trained and tested for specific tasks and did not offer a standalone similarity measure that could be applied (i) regardless of the application domain and (ii) without requiring training corpora.\nYih and Meek (2007) presented an approach to improve text similarity measures for web search queries, with a length ranging from one word to short sequences of words. The proposed method is tailored to this specific task, and the training models are not expected to perform well on different kinds of data such as sentences, questions or paragraphs. Achananuparp et al. (2008) compared several text similarity measures for paraphrase recognition, textual entailment, and the TREC 9 question variants task. In their experiments the best performance was obtained with a linear combination of semantic and lexical similarities, including a word order similarity proposed in (Li et al., 2006) which constructs two vectors of the common words between two sentences and uses their respective positions in the sentences as term weights. The word order similarity measure is then computed by subtracting the two vec-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ntors and taking the absolute value. While such representation takes into account the actual positions of the words, it does not allow detecting subsequence matches and takes into account missing words only by omission.\nMore generally, existing standalone (or traditional) text similarity measures rely on intersections between token sets, text sizes, and frequency, including measures such as the Cosine similarity, Euclidean distance, Levenshtein, Jaccard and Jaro (Jaro, 1989). The sequential nature of natural language is taken into account mostly through word n-grams and skip-grams which capture distinct slices of the analysed texts but do not preserve the order in which they appear.\nIn this paper, we use intuitions from a common representation in DNA sequence alignment to design of a new standalone similarity measure called TextFlow (XF). The proposed measure uses at the same time the full sequence of input texts in a natural sub-sequence matching approach together with individual token matches and mismatches. Our contributions can be detailed further as follows:\n• A novel standalone similarity measure which:\n– exploits the full sequence of words in the compared texts.\n– is asymmetric in a way that allows it to provide the best performance on different tasks (e.g., paraphrase detection, textual entailment and ranking).\n– when required, it can be trained with a small set of parameters controlling the impact of sub-sequence matching, position gaps and unmatched words.\n– provides consistent high performance across tasks and datasets compared to traditional similarity measures.\n• A neural network architecture to train TextFlow parameters for specific tasks.\n• An empirical study on both performance consistency and standard evaluation measures, performed with eight datasets from three different tasks.\n• A new evaluation measure, called CORE, used to better show the consistency of a system at high performance using both its rank\n2 The TextFlow Similarity\nXF is inspired from a dot matrix representation commonly used in pairwise DNA sequence alignment (cf. figure 2). We use a similar dot matrix representation for text pairs and draw a curve oscillating around the diagonal (cf. figure 2). The area under the curve is considered to be the distance between the two text pairs which is then normalized with the matrix surface. For practical computation, we transform this first intuitive representation using the delta of positions as in figure 3. In this setting, the Y axis is the delta of positions of a word occurring in the two texts being compared. If the word does not occur in the target text, the delta is considered to be a maximum reference value (l in figure 2).\nThe semantics are: the bigger the area under curve is, the lower the similarity between the compared texts. XF values are real numbers in the [0,1] interval, with 1 indicating a perfect match, and 0 indicating that the compared texts do not have any common token. With this representation, we are able to take into account all matched words and sub-sequences at the same time. The exact value for the XF similarity between two texts X = {x1, x2, .., xn} and Y = {y1, y2, .., ym} is therefore computed as:\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nFigure 2: Illustration of TextFlow Intuition\nFigure 3: Practical TextFlow Computation\nXF (X,Y ) = 1− 1 nm n∑ i=2 1 Si Ti,i−1(X,Y )\n− 1 nm n∑ i=2 1 Si Ri,i−1(X,Y ) (1)\nWith Ti,i−1(X,Y ) corresponding to the triangular area in the [i − 1, i] step (cf. figure 3) and Ri,i−1(X,Y ) corresponding to the rectangular component. They are expressed as:\nTi,i−1(X,Y ) = |∆P (xi, X, Y )−∆P (xi−1, X, Y )|\n2 (2)\nand:\nRi,i−1(X,Y ) = Min(∆P (xi, X, Y ),∆P (xi−1, X, Y )) (3)\nWith:\n• ∆P (xi, X, Y ) the minimum difference between xi positions in X and Y . If xi /∈ X ∩ Y , ∆P (xi, X, Y ) is set to the same reference value equal to m, (i.e., the cost of a\nmissing word is set by default to the length of the target text), and:\n• Si is the length of the longest matching sequence between X and Y including the word xi, if xi ∈ X ∩ Y , or 1 otherwise.\nXF computation is performed in O(nm) in the worst case where we have to check all tokens in the target text Y for all tokens in the input text X . XF is an asymmetric similarity measure. Its asymmetric aspect has interesting semantic applications as we show in the example below (cf. figure 2). The minimum value of XF provided the best differentiation between positive and negative text pairs when looking for semantic equivalence (i.e., paraphrases), the maximum value was among the the top three for the textual entailment example. We conduct this comparison at a larger scale in the evaluation section.\nWe add 3 parameters to XF in order to represent the importance that should be given to position deltas (Position factor α), missing words (sensitivity factor β), and sub-sequence matching (sequence factor γ), such as:\nXFα,β,γ(X,Y ) = 1− 1\nβnm n∑ i=2 α Sγi T βi,i−1(X,Y )\n− 1 βnm n∑ i=2 α Sγi Rβi,i−1(X,Y )\n(4)\nWith:\nT βi,i−1(X,Y ) = |∆βP (xi, X, Y )−∆βP (xi−1, X, Y )|\n2 (5)\nRβi,i−1(X,Y ) = Min(∆βP (xi, X, Y ),∆βP (xi−1, X, Y )) (6) and:\n• ∆βP (xi, X, Y ) = βm, if xi /∈ X ∩ Y\n• α < β: forces missing words to always cost more than matched words.\n• Sγi = {\n1ifSi = 1orxi /∈ X ∩ Y γ SiforSi > 1\nThe γ factor increases or decreases the impact of sub-sequence matching, α applies to individual token matches whether inside or outside a sequence, and β increases or decreases the impact of\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nPositive Entailment E1 Under a blue sky with white clouds, a child reaches up to touch the propeller of a plane\nstanding parked on a field of grass. E2 A child is reaching to touch the propeller of a plane.\nNegative Entailment E3 Two men on bicycles competing in a race. E4 Men are riding bicycles on the street. Positive Paraphrase P1 The most serious breach of royal security in recent years occurred in 1982 when 30-\nyear-old Michael Fagan broke into the queen’s bedroom at Buckingham Palace. P2 It was the most serious breach of royal security since 1982 when an intruder, Michael\nFagan, found his way into the Queen’s bedroom at Buckingham Palace.\nNegative Paraphrase P3 “Americans don’t cut and run, we have to see this misadventure through,” she said. P4 She also pledged to bring peace to Iraq: “Americans don’t cut and run, we have to see\nthis misadventure through.”\nTask Entailment Recognition Paraphrase Detection Sentence Pair (E1, E2) (E3, E4) (E1, E2) - (E3, E4) (P1, P2) (P3, P4) (P1, P2) - (P3, P4) Example class (Pos/Neg) (Pos) (Neg) (Gap) (Pos) (Neg) (Gap) Jaro-Winkler 0.629 0.712* -0.083** 0.884 0.738 0.146 Levenshtein 0.351 0.259 0.092 0.708 0.577 0.130 Jaccard 0.250* 0.143 0.107 0.571* 0.583 -0.012 Cosine 0.462 0.250 0.212 0.730 0.746** -0.016 Word Overlap 0.800 0.250 0.550 0.800 0.875* -0.075 MIN(XF (x,y), XF(y,x)) 0.260** 0.563** -0.303* 0.693** 0.497 0.196 MAX(XF(x,y), XF(y,x)) 0.707 0.563** 0.144 0.832 0.739 0.093\nFigure 4: Example sentences and similarity values. The best value per column is highlighted. The second best is underlined. Worst and second worst values are followed by one and two stars. Entailment examples are taken from SNLI (Bowman et al., 2015). Paraphrase examples are taken from MSRP 4.\nmissing tokens as well as the normalization quantity βnm in equation 4 to keep the similarity values in the [0,1] range."
    }, {
      "heading" : "2.1 Parameter Training",
      "text" : "By default XF has canonical parameters set to 1. However, when needed, α, β, and γ can be trained on learning data for a specific task. We designed a neural network to perform this task, with a hidden layer dedicated to compute the exact XF value. To do so we compute, for each input text pair, the coefficients vector that would lead exactly to the XF value when multiplied by the vector< αβ , α βγ , β >. Figure 5) presents the training neural network considering several types of sequences (or translations) of the input text pairs (e.g., lemmas, words, synsets).\nWe use identity as activation function in the dedicated XF layer in order to have a correct comparison with the other similarity measures, including canonical XF where the similarity value is provided in the input layer (cf. figure 6)."
    }, {
      "heading" : "3 Evaluation",
      "text" : "Datasets. This evaluation was performed on 8 datasets from 3 different classification tasks: Tex-\ntual Entailment Recognition, Paraphrase Detection, and ranking relevancy. The datasets are as follows:\n• RTE 1, 2, and 3: the first three datasets from the Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2006). Each dataset consists of sentence pairs which are annotated with 2 labels: entailment, and nonentailment. They contain respectively (200, 800), (800, 800), and (800, 800) (train, test) pairs.\n• Guardian: an RTE dataset collected from 78,696 Guardian articles5 published from January 2004 onwards and consisting of 32K pairs which we split randomly in 90%/10% training/test sets. Positive examples were collected from the titles and first sentences. Negative examples were collected from the same source by selecting consecutive sentences and random sentences.\n• SNLI: a recent RTE dataset consisting of 560K training sentence pairs annotated with\n5https://github.com/daoudclarke/ rte-experiment\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nFigure 5: NN architecture A1 for XF Parameter Training\n3 labels: entailment, neutral and contradiction (Bowman et al., 2015). We discarded the contradiction pairs as they do not necessarily represent dissimilar sentences and are therefore a random noise w.r.t. our similarity measure evaluation.\n• MSRP: the Microsoft Research Paraphrase corpus, consisting of 5,800 sentence pairs annotated with a binary label indicating whether the two sentences are paraphrases or not.\n• Semeval-16-3B: a dataset of questionquestion similarity collected from StackOverflow (Nakov et al., 2016). The dataset was contains 3,169 training pairs and 700 test pairs. Three labels are considered: ”Perfect Match”, ”Relevant” or ”Irrelevant”. We combined the first two into the same positive category for our evaluation.\n• Semeval-14-1: a corpus of Sentences Involving Compositional Knowledge (Marelli et al., 2014) consisting of 10,000 english sentence pairs annotated with both similarity scores and relevance labels.\nFeatures. After a preprocessing step where we removed stopwords, we computed the similarity values using 7 different types of sequences constructed, respectively, with the following value from each token:\n• Word (plain text value)\n• Lemma\n• Part-Of-Speech (POS) tag\n• WordNet Synset6 OR Lemma\n• WordNet Synset OR Lemma for Nouns\n• WordNet Synset OR Lemma for Verbs\n• WordNet Synset OR Lemma for Nouns and Verbs.\nIn the 4 last types of sequences the lemma is used when there is no corresponding WordNet synset. In a first experiment we compare different aggregation functions on top of XF (i.e., minimum, maximum and average) in table 3. We used the LibLinear7 SVM classifier for this task.\nIn the second part of the evaluation, we use neural networks to compare the efficiency of XFc, XFt and other similarity measures with in the same setting. We use the neural net described in figure 5 for the trained versionXFt and the equivalent architecture presented in figure 6 for all other similarity measures. For canonical XFc we use by default the best aggregation for the task as observed in table 3.\n6https://wordnet.princeton.edu/ 7https://www.csie.ntu.edu.tw/˜cjlin/\nliblinear/\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nTask Entailment Recognition Paraphrase Detection Ranking Relevance Datasets RTE 1 RTE 2 RTE 3 Guardian SNLI MSRP Semeval16-t3B Semeval12-t17 XF MIN 55.3 53.8 60.0 77.3 58.0 72.1 77.4 77.8 XF AVG 51.4 1 57.2 62.5 84.9 62.0 72.0 77.6 79.5 XF MAX 53.9 61.3 64.7 86.7 64.3 71.4 76.7 77.7\nTable 1: Accuracy evaluation with different aggregations of XF using an SVM classifier.\nFigure 6: NN Architecture A2 for the equivalent evaluation of other similarity measures.\nSimilarity Measures. We considered nine traditional similarity measures included in the Simmetrics distribution8: Cosine, Euclidean distance, Word Overlap, Dice coefficient (Dice, 1945), Jaccard(Jain and Dubes, 1988), Damerau, Jaro-Winkler (JW) (Porter et al., 1997), Levenshtein (LEV) (Sankoff and Kruskal, 1983), and Longest Common Subsequence (LCS) (Friedman and Sideli, 1992). Implementation. XF was implemented in Java as an extension of the Simmetrics package, made available at this address9. The neural neworks were implemented in Python with TensorFlow10. We also share the training sets used for both parameter training and evaluation. The evaluation was performed on a 4-cores laptop with 32GB of RAM. The initial parameters forXFt were chosen with a random function. Evaluation Measures. We use the standard accuracy values and F1, precision and recall for the\n8https://github.com/Simmetrics/ simmetrics\n9Code link omitted for blind review. 10https://www.tensorflow.org/\npositive class (i.e., entailment, paraphrase, and ranking relevance). We also study the relative rank in performance of each similarity measure across all datasets using the average rank, the rank variance and a new evaluation measure called cOnsistent peRformancE (CORE), computed as follows for a system m, a set of datasets D, a set of systems S, and an evaluation measure E ∈ {F1, P recision,Recall, Accuracy}:\nCORE D,S,E (m) =\nMIN p∈S ( AVG d∈D (RS(Ed(p)) + Vd∈D(RS(Ed(p))) ) AVG d∈D ( RS(Ed(m)) ) + Vd∈D ( RS(Ed(m))\n) (7) With RS(Ed(m)) the rank of m according to the evaluation measure E on dataset d w.r.t. competing systems S. Vd∈D(RS(Ed(m))) is the rank variance of m over datasets. The intuition can be read on the results tables 2, 3, and 4. Basically, CORE tells us how consistent a system/method is in having high performance, relatively to the set of competing systems S. The maximum value of CORE is 1 for the best performing system according to its rank. It also allows quantifying how consistently a system achieves high performance for the remaining systems.\nTensorflow outperformed the results obtained with a combination of word order similarity and semantic similarities tested in (Achananuparp et al., 2008), with gaps of +1.0 in F1 and +6.1 accuracy on MSRP and +4.2 F1 and +2.7% accuracy on RTE 3."
    }, {
      "heading" : "4 Discussion",
      "text" : "Canonical Text Flow. TFc had the best average and micro-average accuracy on the 8 classification datasets, with a gap of +0.4 to +6.3 when compared to the state-of-the-art measures. It also reached the best precision average with a gap of +1.8 to +6.3. On the F1 score level XFc achieved the second best performance with a gap of -1.7, mainly caused by its underperformance in recall, where it had the third best performance with a gap of -6.3. The recall table is omitted due to a lack\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nCosine Euc Overlap Dice Jaccard Damerau JW LEV LCS XFC XFT RTE 1 .561 .564 .550 .504 .511 .557 .532 .561 .568 .550 .575 RTE 2 .575 .555 .598 .566 .572 .548 .541 .551 .548 .597 .612 RTE 3 .652 .562 .636 .637 .630 .567 .538 .567 .562 .627 .647 Guardian .748 .750 .820 .778 .780 .847 .726 .847 .848 .867 .876 SNLI .621 .599 .665 .612 .608 .631 .556 .630 .619 .641 .656 MSRP .719 .689 .720 .729 .731 .687 .699 .685 .717 .724 .732 Semeval-16-3B .756 .734 .769 .781 .780 .759 .751 .759 .737 .777 .782 Semeval-14-1 .790 .756 .779 .783 .786 .749 .719 .749 .757 .783 .798 AVG .678 .651 .692 .674 .675 .668 .633 .669 .670 .696 .710 Micro Avg .699 .675 .725 .700 .700 .701 .646 .701 .701 .726 .739 RANK Avg 5.1 8.2 4.5 5.6 5.5 6.9 10.1 6.7 6.7 4.1 1.2 RANK Var. 9.0 5.9 4.3 10.0 8.6 5.3 1.6 6.2 8.2 2.7 0.2 CORE 0.104 0.103 0.167 0.094 0.104 0.121 0.125 0.113 0.098 0.215 1.000\nCosine Euc Overlap Dice Jaccard Damerau JW LEV LCS XFC XFT RTE 1 .612 .564 .636 .512 .523 .578 .513 .583 .494 .565 .599 RTE 2 .579 .590 .662 .565 .558 .549 .516 .551 .555 .616 .646 RTE 3 .705 .598 .682 .695 .682 .608 .556 .607 .603 .665 .690 Guardian .742 .749 .816 .774 .776 .849 .713 .849 .850 .862 .873 SNLI .582 .576 .641 .562 .564 .627 .479 .627 .611 .594 .585 MSRP .808 .797 .812 .814 .813 .784 .802 .783 .804 .804 .810 Semeval-16-3B .632 .462 .625 .648 .644 .544 .545 .547 .508 .633 .662 Semeval-14-1 .764 .707 .748 .753 .746 .706 .680 .706 .714 .744 .673 AVG .678 .630 .702 .665 .663 .655 .600 .656 .642 .685 .692 Micro Avg .684 .656 .716 .679 .677 .691 .608 .692 .688 .702 .687 RANK Avg 4.5 8.12 3.12 5.12 5.5 6.89 9.88 6.62 7.12 4.62 3.88 RANK Var. 9.7 4.7 4.4 14.7 6.6 8.7 1.8 9.1 8.1 2.3 11.0 CORE 0.485 0.538 0.915 0.348 0.571 0.443 0.588 0.438 0.452 1.000 0.464\nTable 3: F1 scores. The best result is highlighted, the second best is underlined.\nCosine Euc Overlap Dice Jaccard Damerau JW LEV LCS XFC XFT RTE 1 .548 .564 .534 .503 .510 .552 .535 .555 .596 .546 .566 RTE 2 .574 .547 .571 .567 .578 .547 .546 .551 .546 .588 .594 RTE 3 .624 .565 .618 .611 .610 .568 .547 .568 .564 .616 .627 Guardian .759 .753 .836 .789 .789 .839 .749 .840 .839 .891 .894 SNLI .644 .608 .690 .642 .632 .631 .577 .630 .621 .679 .735 MSRP .740 .705 .732 .749 .755 .723 .713 .722 .743 .760 .765 Semeval-16-3B .634 .708 .678 .698 .698 .732 .698 .729 .674 .700 .686 Semeval-14-1 .745 .738 .738 .743 .769 .716 .672 .716 .727 .762 .740 AVG .659 .649 .675 .663 .668 .664 .630 .664 .664 .693 .701 Micro Avg .693 .674 .721 .699 .704 .694 .645 .693 .693 .737 .752 RANK Avg. 5.6 7.5 5.9 5.9 5.1 6.1 9.6 6.1 7.1 3.2 2.5 RANK Var. 9.4 10.0 6.4 5.3 7.8 7.0 4.6 7.6 11.6 3.1 6.9 CORE 0.420 0.361 0.515 0.567 0.488 0.482 0.446 0.462 0.338 1.000 0.676\nTable 4: Precision values. The best result is highlighted, the second best is underlined.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nof space. On a rank level, XFc had the best consistent rank for accuracy F1 and precision, and the second best for recall.\nTrained Text Flow. When compared to soa measures and to canonical XF, the trained version, XFt, obtained the best accuracy with a gap ranging from +1.4 to +7.8. XFt also obtained the second best F1 average with a -1.0 gap, but with clear inconsistencies according to the dataset. XFt obtained the best precision with a gap ranging from +0.8 to +7.1. XFt did not perform well on recall with 64.5% micro-average compared to WordOverlap with 72%. Both its recall and F1 performance can be explained by the fact that the measure was trained to optimize accuracy, and not the F1 score for the positive class; which also suggests that the approach could be adapted to F1 optimization if needed. Canonical XF was more consistent than trained XF on all dimensions except accuracy, for which XFt was optimized. We argue that this consistency was made possible through the asymmetry of XF which allowed it to adapt to different kinds of similarities (i.e., equivalence/paraphrase, inference/entailment, and mutual distance/ranking). These results also show that the actual positions difference is a relevant factor for text similarity. We explain it mainly by the natural flow of language where the important entities and relations are often expressed first, in contrast with a purely logical-driven approach which has to consider, for instance, that active forms and passive forms are equivalent in meaning. The difference in positions is also not read literally, both because of the higher impact associated to missed words and to the α parameter which allows leveraging their impact in the trained version.\nIn additional experiments, we compared TFc and TFt with the other similarity measures when applied to bi-grams and tri-grams instead of individual tokens. The results were significantly lower on all datasets (between 3 and 10 points loss in accuracy) for both the soa measures and TextFlow variants. This result could be explained by the fact that n-grams are too rigid when a sub-sequence varies even slightly, e.g., the insertion of a new word inside a 3-words sequence leads to a tri-gram mismatch and reduces bi-gram overlap from 100% to 50% for the considered sub-sequence. This issue is not encountered with TextFlow as it relies on the token level, and such an insertion will\nnot cancel or reduce significantly the gains from the correct ordering of the words. We also conducted ranking correlation experiments on three test datasets provided at the semantic text similarity task at Semeval 2012, with gold score values for their text pairs. The datasets have 750 sentence pairs each, and are extracted from the Microsoft Research video descriptions corpus, MSRP and the SMTeuroparl11. When compared to the traditional similarity measures, TextFlow had the best correlation on the first two datasets with, for instance, 0.54 and 0.46 pearson correlation on the lemmas sequences and the second best on the MSRP extract where the Cosine similarity had the best performance with 0.71 vs 0.68, noting that the Cosine similarity uses word frequencies when the evaluated version of TextFlow did not use wordlevel weights.\nIncluding word weights is one of the promising perspectives in line with this work as it could be done simply by making the deltas vary according to the weight/importance of the (un)matched word. Also, in such setting, the impact of a sequence of N words will naturally increase or decrease according to the word weights (cf. figure 3). We conducted a preliminary test using the inverse document frequency of the words as extracted from Wikipedia with Gensim12, which led to an improvement of up to 2% for most datasets, with performance decreasing slightly on two of them. Other kinds of weights could also be included just as easily, such as contextual word relatedness using embeddings or other semantic relatedness factors such as WordNet distances (Pedersen et al., 2004)."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We presented a novel standalone similarity measure that takes into account continuous word sequences. Evaluation on eight datasets show promising results for textual entailment recognition, paraphrase detection and ranking. Among the potential extensions of this work are the inclusion of different kinds of weights such as TF-IDF, embedding relatedness and semantic relatedness. We also intend to test other variants around the same concept, including considering the matched words and sequences to have a negative weight to balance further the weight of missing words.\n11goo.gl/NVnybD 12https://radimrehurek.com/gensim/\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    }, {
      "heading" : "Acknowledgements",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "The evaluation of sentence similarity measures",
      "author" : [ "Palakorn Achananuparp", "Xiaohua Hu", "Xiajiong Shen." ],
      "venue" : "Data warehousing and knowledge discovery, Springer, pages 305–316.",
      "citeRegEx" : "Achananuparp et al\\.,? 2008",
      "shortCiteRegEx" : "Achananuparp et al\\.",
      "year" : 2008
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1508.05326 .",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "The pascal recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, Springer,",
      "citeRegEx" : "Dagan et al\\.,? 2006",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2006
    }, {
      "title" : "Measures of the amount of ecologic association between species",
      "author" : [ "Lee R Dice." ],
      "venue" : "Ecology 26(3):297– 302.",
      "citeRegEx" : "Dice.,? 1945",
      "shortCiteRegEx" : "Dice.",
      "year" : 1945
    }, {
      "title" : "Tolerating spelling errors during patient validation",
      "author" : [ "Carol Friedman", "Robert Sideli." ],
      "venue" : "Computers and Biomedical Research 25(5):486–509.",
      "citeRegEx" : "Friedman and Sideli.,? 1992",
      "shortCiteRegEx" : "Friedman and Sideli.",
      "year" : 1992
    }, {
      "title" : "Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning",
      "author" : [ "Vasileios Hatzivassiloglou", "Judith L Klavans", "Eleazar Eskin." ],
      "venue" : "Proceedings of the 1999 joint sigdat conference on empirical methods",
      "citeRegEx" : "Hatzivassiloglou et al\\.,? 1999",
      "shortCiteRegEx" : "Hatzivassiloglou et al\\.",
      "year" : 1999
    }, {
      "title" : "Algorithms for clustering data",
      "author" : [ "Anil K Jain", "Richard C Dubes." ],
      "venue" : "Prentice-Hall, Inc.",
      "citeRegEx" : "Jain and Dubes.,? 1988",
      "shortCiteRegEx" : "Jain and Dubes.",
      "year" : 1988
    }, {
      "title" : "Advances in record-linkage methodology as applied to matching the 1985 census of tampa, florida",
      "author" : [ "Matthew A Jaro." ],
      "venue" : "Journal of the American Statistical Association 84(406):414–420.",
      "citeRegEx" : "Jaro.,? 1989",
      "shortCiteRegEx" : "Jaro.",
      "year" : 1989
    }, {
      "title" : "Sentence similarity based on semantic nets and corpus statistics. IEEE transactions on knowledge and data engineering 18(8):1138–1150",
      "author" : [ "Yuhua Li", "David McLean", "Zuhair A Bandar", "James D O’shea", "Keeley Crockett" ],
      "venue" : null,
      "citeRegEx" : "Li et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2006
    }, {
      "title" : "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual",
      "author" : [ "Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli" ],
      "venue" : null,
      "citeRegEx" : "Marelli et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Marelli et al\\.",
      "year" : 2014
    }, {
      "title" : "Bioinformatics: sequence and genome analysis",
      "author" : [ "David W Mount." ],
      "venue" : "Cold Spring Harbor Laboratory Press.",
      "citeRegEx" : "Mount.,? 2004",
      "shortCiteRegEx" : "Mount.",
      "year" : 2004
    }, {
      "title" : "Semeval-2016 task 3: Community",
      "author" : [ "Preslav Nakov", "Lluı́s Màrquez", "Alessandro Moschitti", "Walid Magdy", "Hamdy Mubarak", "Abed Alhakim Freihat", "Jim Glass", "Bilal Randeree" ],
      "venue" : null,
      "citeRegEx" : "Nakov et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nakov et al\\.",
      "year" : 2016
    }, {
      "title" : "Wordnet:: Similarity: measuring the relatedness of concepts",
      "author" : [ "Ted Pedersen", "Siddharth Patwardhan", "Jason Michelizzi." ],
      "venue" : "Demonstration papers at HLT-NAACL 2004. Association for Computational Linguistics, pages 38–41.",
      "citeRegEx" : "Pedersen et al\\.,? 2004",
      "shortCiteRegEx" : "Pedersen et al\\.",
      "year" : 2004
    }, {
      "title" : "Approximate string comparison and its effect on an advanced record linkage system",
      "author" : [ "Edward H Porter", "William E Winkler" ],
      "venue" : "In Advanced record linkage system. US Bureau of the Census, Research Report. Citeseer",
      "citeRegEx" : "Porter and Winkler,? \\Q1997\\E",
      "shortCiteRegEx" : "Porter and Winkler",
      "year" : 1997
    }, {
      "title" : "A web-based kernel function for measuring the similarity of short text snippets",
      "author" : [ "Mehran Sahami", "Timothy D Heilman." ],
      "venue" : "Proceedings of the 15th international conference on World Wide Web. AcM, pages 377–386.",
      "citeRegEx" : "Sahami and Heilman.,? 2006",
      "shortCiteRegEx" : "Sahami and Heilman.",
      "year" : 2006
    }, {
      "title" : "Time warps, string edits, and macromolecules: the theory and practice of sequence comparison",
      "author" : [ "David Sankoff", "Joseph B Kruskal." ],
      "venue" : "Reading: Addison-Wesley Publication, 1983, edited by Sankoff, David; Kruskal, Joseph B. 1.",
      "citeRegEx" : "Sankoff and Kruskal.,? 1983",
      "shortCiteRegEx" : "Sankoff and Kruskal.",
      "year" : 1983
    }, {
      "title" : "Learning to rank short text pairs with convolutional deep neural networks",
      "author" : [ "Aliaksei Severyn", "Alessandro Moschitti." ],
      "venue" : "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM,",
      "citeRegEx" : "Severyn and Moschitti.,? 2015",
      "shortCiteRegEx" : "Severyn and Moschitti.",
      "year" : 2015
    }, {
      "title" : "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection",
      "author" : [ "Richard Socher", "Eric H Huang", "Jeffrey Pennington", "Andrew Y Ng", "Christopher D Manning." ],
      "venue" : "NIPS. volume 24, pages 801–809.",
      "citeRegEx" : "Socher et al\\.,? 2011",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Improving similarity measures for short segments of text",
      "author" : [ "Wen-Tau Yih", "Christopher Meek." ],
      "venue" : "AAAI. volume 7, pages 1489–1494.",
      "citeRegEx" : "Yih and Meek.,? 2007",
      "shortCiteRegEx" : "Yih and Meek.",
      "year" : 2007
    }, {
      "title" : "Learning discriminative projections for text similarity measures",
      "author" : [ "Wen-tau Yih", "Kristina Toutanova", "John C Platt", "Christopher Meek." ],
      "venue" : "Proceedings of the Fifteenth Conference on Computational Natural Language Learning. Association for Com-",
      "citeRegEx" : "Yih et al\\.,? 2011",
      "shortCiteRegEx" : "Yih et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Finding relevant approaches to compute text similarity motivated a lot of research in the last decades (Sahami and Heilman, 2006; Hatzivassiloglou et al., 1999), and more recently with deep learning methods (Socher et al.",
      "startOffset" : 103,
      "endOffset" : 160
    }, {
      "referenceID" : 5,
      "context" : "Finding relevant approaches to compute text similarity motivated a lot of research in the last decades (Sahami and Heilman, 2006; Hatzivassiloglou et al., 1999), and more recently with deep learning methods (Socher et al.",
      "startOffset" : 103,
      "endOffset" : 160
    }, {
      "referenceID" : 17,
      "context" : ", 1999), and more recently with deep learning methods (Socher et al., 2011; Yih et al., 2011; Severyn and Moschitti, 2015).",
      "startOffset" : 54,
      "endOffset" : 122
    }, {
      "referenceID" : 19,
      "context" : ", 1999), and more recently with deep learning methods (Socher et al., 2011; Yih et al., 2011; Severyn and Moschitti, 2015).",
      "startOffset" : 54,
      "endOffset" : 122
    }, {
      "referenceID" : 16,
      "context" : ", 1999), and more recently with deep learning methods (Socher et al., 2011; Yih et al., 2011; Severyn and Moschitti, 2015).",
      "startOffset" : 54,
      "endOffset" : 122
    }, {
      "referenceID" : 8,
      "context" : "In their experiments the best performance was obtained with a linear combination of semantic and lexical similarities, including a word order similarity proposed in (Li et al., 2006) which constructs two vectors of the common words between two sentences and uses their respective positions in the sentences as term weights.",
      "startOffset" : 165,
      "endOffset" : 182
    }, {
      "referenceID" : 4,
      "context" : "Finding relevant approaches to compute text similarity motivated a lot of research in the last decades (Sahami and Heilman, 2006; Hatzivassiloglou et al., 1999), and more recently with deep learning methods (Socher et al., 2011; Yih et al., 2011; Severyn and Moschitti, 2015). However, most of the recent advances focused on designing high performance classification methods, trained and tested for specific tasks and did not offer a standalone similarity measure that could be applied (i) regardless of the application domain and (ii) without requiring training corpora. Yih and Meek (2007) presented an approach to improve text similarity measures for web search queries, with a length ranging from one word to short sequences of words.",
      "startOffset" : 130,
      "endOffset" : 592
    }, {
      "referenceID" : 0,
      "context" : "Achananuparp et al. (2008) compared several text similarity measures for paraphrase recognition, textual entailment, and the TREC 9 question variants task.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 7,
      "context" : "More generally, existing standalone (or traditional) text similarity measures rely on intersections between token sets, text sizes, and frequency, including measures such as the Cosine similarity, Euclidean distance, Levenshtein, Jaccard and Jaro (Jaro, 1989).",
      "startOffset" : 247,
      "endOffset" : 259
    }, {
      "referenceID" : 10,
      "context" : "• A new evaluation measure, called CORE, used to better show the consistency of a system at high performance using both its rank Figure 1: Dot matrix example for 2 DNA sequences (Mount, 2004)",
      "startOffset" : 178,
      "endOffset" : 191
    }, {
      "referenceID" : 1,
      "context" : "Entailment examples are taken from SNLI (Bowman et al., 2015).",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "• RTE 1, 2, and 3: the first three datasets from the Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2006).",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : "3 labels: entailment, neutral and contradiction (Bowman et al., 2015).",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 11,
      "context" : "• Semeval-16-3B: a dataset of questionquestion similarity collected from StackOverflow (Nakov et al., 2016).",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "• Semeval-14-1: a corpus of Sentences Involving Compositional Knowledge (Marelli et al., 2014) consisting of 10,000 english sentence pairs annotated with both similarity scores and relevance labels.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "We considered nine traditional similarity measures included in the Simmetrics distribution8: Cosine, Euclidean distance, Word Overlap, Dice coefficient (Dice, 1945), Jaccard(Jain and Dubes, 1988), Damerau, Jaro-Winkler (JW) (Porter et al.",
      "startOffset" : 152,
      "endOffset" : 164
    }, {
      "referenceID" : 6,
      "context" : "We considered nine traditional similarity measures included in the Simmetrics distribution8: Cosine, Euclidean distance, Word Overlap, Dice coefficient (Dice, 1945), Jaccard(Jain and Dubes, 1988), Damerau, Jaro-Winkler (JW) (Porter et al.",
      "startOffset" : 173,
      "endOffset" : 195
    }, {
      "referenceID" : 15,
      "context" : ", 1997), Levenshtein (LEV) (Sankoff and Kruskal, 1983), and Longest Common Subsequence (LCS) (Friedman and Sideli, 1992).",
      "startOffset" : 27,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : ", 1997), Levenshtein (LEV) (Sankoff and Kruskal, 1983), and Longest Common Subsequence (LCS) (Friedman and Sideli, 1992).",
      "startOffset" : 93,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "Tensorflow outperformed the results obtained with a combination of word order similarity and semantic similarities tested in (Achananuparp et al., 2008), with gaps of +1.",
      "startOffset" : 125,
      "endOffset" : 152
    }, {
      "referenceID" : 12,
      "context" : "Other kinds of weights could also be included just as easily, such as contextual word relatedness using embeddings or other semantic relatedness factors such as WordNet distances (Pedersen et al., 2004).",
      "startOffset" : 179,
      "endOffset" : 202
    } ],
    "year" : 2017,
    "abstractText" : "Text similarity measures are used in multiple tasks such as plagiarism detection, information ranking and recognition of paraphrases and textual entailment. While recent advances in deep learning highlighted further the relevance of sequential models in natural language generation, existing similarity measures do not fully exploit the sequential nature of language. Examples of such similarity measures include ngrams and skip-grams overlap which rely on distinct slices of the input texts. In this paper we present a novel text similarity measure inspired from a common representation in DNA sequence alignment algorithms. The new measure, called TextFlow, represents input text pairs as continuous curves and uses both the actual position of the words and sequence matching to compute the similarity value. Our experiments on eight different datasets show very encouraging results in paraphrase detection, textual entailment recognition and ranking relevance.",
    "creator" : "LaTeX with hyperref package"
  }
}