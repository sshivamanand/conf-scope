{
  "name" : "26.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "As the amount of the knowledge bases (KBs) grows, people are paying more attention to seeking effective methods for accessing these precious intellectual resources. There are several tailor-made languages designed for querying KBs, such as\nSPARQL (Prudhommeaux and Seaborne, 2008). However, to handle such query languages, users are required to not only be familiar with the particular language grammars, but also be aware of the architectures of the KBs. By contrast, knowledge base-based question answering (KB-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years.\nGiven natural language questions, the goal of KB-QA is to automatically return answers from the KB. There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al., 2014a,b, 2015; Dong et al., 2015; Xu et al., 2016a,b) methods. SP-based methods usually focus on constructing a semantic parser that could convert natural language questions into structured expressions like logical forms. IR-based methods usually search answers from the KB based on the information conveyed in questions, where ranking techniques are often adopted to make correct selections from candidate answers.\nRecently, with the progress of deep learning, neural network-based (NN-based) methods have been introduced to the KB-QA task (Bordes et al., 2014b). They belong to IR-based methods. Different from previous methods, NN-based methods represent both of the questions and the answers as semantic vectors. Then the complex process of KB-QA could be converted into a similarity matching process between an input question and its candidate answers in a semantic space. The candidates with the highest similarity score will be selected as the final answers. Because they are adaptive and robust, NN-based methods have at-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ntracted more and more attention, and this paper also focuses on using end-to-end neural networks to answer questions over knowledge base.\nIn NN-based methods, the crucial step is to compute the similarity score between a question and a candidate answer, where the key is to learn their representations. Previous methods put more emphasis on learning representation of the answer end. For example, Bordes et al. (2014a) consider the importance of the subgraph of the candidate answer. Dong et al. (2015) make use of the context and the type of the answer. However, the representation of the question end is oligotrophic. Existing approaches often represent a question into a single vector using simple bag-of-words (BOW) model (Bordes et al., 2014a,b), whereas the relatedness to the answer end is neglected. We argue that a question should be represented differently according to the different focuses of various answer aspects1.\nTake the question “Who is the president of France?” and one of its candidate answers “Francois Hollande” as an example. When dealing with the answer entity Francois Holland, “president” and “France” in the question is more focused, and the question representation should bias towards the two words. While facing the answer type /business/board member, “Who” should be the most prominent word. Meantime, some questions may value answer type more than other answer aspects. While in some other questions, answer relation may be the most important information we should consider, which is dynamic and flexible corresponding to different questions and answers. Obviously, this is an attention mechanism, which reveals the mutual influences between the representation of questions and the corresponding answer aspects.\nWe believe that such kind of representation is more expressive. Dong et al. (2015) represents questions using three CNNs with different parameters when dealing with different answer aspects including answer path, answer context and answer type. We argue that simply selecting three independent CNNs is mechanical and inflexible. Thus, we go one step further, and propose a crossattention based neural network to perform KBQA. The cross-attention model, which stands for the mutual attention between the question and the\n1An answer aspect could be the answer entity itself, the answer type, the answer context, etc.\nanswer aspects, contains two parts: the answertowards-question attention part and the questiontowards-answer attention part. The former help learn flexible and adequate question representation, and the latter help adjust the question-answer weight, getting the final score. We illustrate in section 3.2 for more details. In this way, we formulate the cross-attention mechanism to model the question answering procedure. Note that our proposed model is an entire end-to-end approach which only depends on training data. Some integrated systems which use extra patterns and resources are not directly comparable to ours. Our target is to explore a better solution following the end-to-end KB-QA technical path.\nMoreover, we notice that the representations of the KB resources (entities and relations) are also limited in previous work. specifically, they are often learned barely on the QA training data, which results in two limitations. 1) The global information of the KB is deficient. For example, if question-answer pair (q, a) appears in the training data, and the global KB information implies us that a′ is similar to a2, denoted by (a∼ a′), then (q, a′) is more probable to be right. However, current QA training mechanism cannot guarantee (a ∼ a′) could be learned. 2) The problem of out-ofvocabulary (OOV) stands out. Due to the limited coverage of the training data, the OOV problem is common while testing, and many answer entities in testing candidate set have never been seen before. The attention of these resources become the same because they shared the same OOV embedding, and this will do harm to the proposed attention model. To tackle these two problems, we additionally incorporates KB itself as training data for training embeddings besides original questionanswer pairs. In this way, the global structure of the whole knowledge could be captured, and the OOV problem could be alleviated naturally.\nIn summary, the contributions are as follows. 1) We present a novel cross-attention based NN model tailored to KB-QA task, which considers the mutual influence between the representation of questions and the corresponding answer aspects. 2) We leverage the global KB information, aiming at represent the answers more precisely. It also alleviates the OOV problem, which is very helpful to the cross-attention model.\n2The complete KB is able to offer this kind of information, e.g., a and a′ share massive context.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n3) The experimental results on the open dataset WebQuestions demonstrate the effectiveness of the proposed approach."
    }, {
      "heading" : "2 Overview",
      "text" : "The goal of KB-QA task could be formulated as follows. Given a natural language question q, the system returns an entity set A as answers. The architecture of our proposed KB-QA system is shown in Figure 1, which illustrates the basic flow of our approach. First, we identify the topic entity of the question, and generate candidate answers from Freebase. Then, a cross-attention based neural network is employed to represent the question under the influence of the candidate answer aspects. Finally, the similarity score between the question and each corresponding candidate answer is calculated, and the candidates with highest score will be selected as the final answers3.\nCross-Attention based Neural Network\nq: Who is the president of France?\nTopic entity France Candidate\ngeneration\nCandidate Answers\nParis \uD835\uDC4E1\nFrench \uD835\uDC4E2\nSemi-presidential system\n⋮\nAnswer Aspects\nanswer entity:/m/05qtj answer relation: capital\nanswer type: /location/city town answer context:/m/0276jx2, /m\n/0jd4j, /m/0f3vz, …\n⋮\nS(\uD835\uDC5E, \uD835\uDC4E1) S(\uD835\uDC5E, \uD835\uDC4E2 )\n⋮ ⋮\n⋯\nA\nRanking\nFreebase\nFigure 1: The overview of the proposed KB-QA system.\nWe utilize Freebase (Bollacker et al., 2008) as our knowledge base. It has more than 3 billion facts, and is used as the supporting KB for many QA tasks. In Freebase, the facts are represented by subject-predicate-object triples (s, p, o). For clarity, we call each basic element a resource, which could be either an entity or a relation. For example, (/m/0f8l9c, location.country.capital,/m/05qtj)4 describes the fact that the capital of France is Paris, where /m/0f8l9c and /m/05qtj are entities denoting France and Paris respectively, and location.country.capital is a relation.\n3We also adopt a margin strategy to obtain multiple answers for a question and this will be explained in the next section.\n4Note that the Freebase prefixes are omitted for neatness."
    }, {
      "heading" : "3 Our Approach",
      "text" : ""
    }, {
      "heading" : "3.1 Candidate Generation",
      "text" : "All the entities in Freebase should be candidate answers ideally, but in practice, this is time consuming and not really necessary. For each question q, we use Freebase API (Bollacker et al., 2008) to identify a topic entity, which could be simply understood as the main entity of the question. For example, France is the topic entity of question “Who is the president of France?”. Freebase API method is able to resolve as many as 86% questions if we use the top1 result (Yao and Van Durme, 2014). After getting the topic entity, we collect all the entities directly connected to it and the ones connected with 2-hop5. These entities constitute a candidate set Cq ."
    }, {
      "heading" : "3.2 The Neural Cross-Attention Model",
      "text" : "We present a cross-attention based neural network, which represents the question dynamically according to different answer aspects, also considering their connections. Concretely, each aspect of the answer focuses on different words of the question and thus decides how the question is represented. Then the question pays different attention to each answer aspect to decide their weights. Figure 2 is the architecture of our model. We will illustrate how the system works as follows."
    }, {
      "heading" : "3.2.1 Question Representation",
      "text" : "First of all, we have to obtain the representation of each word in the question. These representations retain all the information of the question, and could serve the following steps. Suppose question q is expressed as q = (x1, x2, ..., xn), where xi denotes the ith word. As shown in Figure 2, we first look up a word embedding matrix Ew ∈ Rd×vw to get the word embeddings, which is randomly initialized, and updated during the training process. Here, d means the dimension of the embeddings and vw denotes the vocabulary size of natural language words.\nThen, the embeddings are fed into a long shortterm memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks. LSTM has been proven to be effective in many natural language processing (NLP) tasks such as machine translation (Sutskever et al., 2014) and dependency parsing (Dyer et al., 2015), and it is adept in harnessing long\n5For example, (/m/0f8l9c, governing officials, government.position held.office holder, /m/02qg4z) is a 2-top connection.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n\uD835\uDC651\uD835\uDC5E \uD835\uDC652 \uD835\uDC656\uD835\uDC655\uD835\uDC653 \uD835\uDC654 \uD835\uDC4E\uD835\uDC61\uD835\uDC4E\uD835\uDC5F\uD835\uDC4E\uD835\uDC52 \uD835\uDC4E\uD835\uDC50\nWord Embedding Matrix \uD835\uDC38\uD835\uDC64 KB Embedding Matrix \uD835\uDC38\uD835\uDC58\nℎ1 ℎ2 ℎ3 ℎ4 ℎ5 ℎ6 \uD835\uDC5E\n\uD835\uDEFC1 \uD835\uDEFC2 \uD835\uDEFC3 \uD835\uDEFC4 \uD835\uDEFC5 \uD835\uDEFC6\n\uD835\uDC5E1A-Q Attention Model\n\uD835\uDC52\uD835\uDC50\n+ + + = \uD835\uDC46(\uD835\uDC5E, \uD835\uDC4E)\n\uD835\uDEFD1 \uD835\uDEFD2 \uD835\uDEFD3 \uD835\uDEFD4\nQ-A Attention Model\nMean\nover time\nBidirectional LSTM\n\uD835\uDC52\uD835\uDC61\uD835\uDC52\uD835\uDC5F\uD835\uDC52\uD835\uDC52\nFigure 2: The architecture of the proposed attention-based neural network. Note that only one aspect(in orange color) is depicted for clarity. The other three aspects follow the same way.\nsentences. Note that if we use unidirectional LSTM, the outcome of a specific word contains only the information of the words before it, whereas the words after it are not taken into account. To avoid this, we employ bidirectional LSTM as Bahdanau (2015) does, which consists of both forward and backward networks. The forward LSTM handles the question from left to right, and the backward LSTM processes in the reverse order. Thus, we could acquire two hidden state sequences, one from the forward one ( −→ h1, −→ h2, ..., −→ hn) and the other from the backward one ( ←− h1, ←− h2, ..., ←− hn). We concatenate the forward hidden state and the backward hidden state of each word, resulting in hj = [ −→ hj ; ←− hj ]. The hidden unit of forward and backward LSTM is d2 , so the concatenated vector is of dimension d. In this way, we obtain the representation of each word in the question."
    }, {
      "heading" : "3.2.2 Answer aspect representation",
      "text" : "We directly use the embedding for each answer aspect through the KB embedding matrix Ek ∈ Rd×vk . Here, vk means the vocabulary size of the KB resources. The embedding matrix is randomly initialized and learned during training, and could be further enhanced with the help of global information as described in Section 3.3. Concretely, we employ four kinds of answer aspects: answer entity ae, answer relation ar, answer type at and answer context ac. Their embeddings are denoted as ee, er, et and ec, respectively. It is worth noting that the answer context consists of multiple KB resources, and we denote it as (c1, c2, ..., cm). We first acquire their KB embeddings (ec1 , ec2 , ..., ecm) throughEk, then calculate\nan average embedding by ec = 1m ∑m\ni=1 eci . 3.2.3 Cross-Attention model The most crucial part of the proposed approach is the cross-attention mechanism. The crossattention mechanism is composed of two parts: the answer-towards-question attention part and the question-towards-answer attention part. • Answer-towards-question(A-Q) attention\nBased on our assumption, each answer aspect should focus on different words of the same question. The extent of attention can be measured by the relatedness between each word representation hj and an answer aspect embedding ei. We propose the following formulas to calculate the weights.\nαij = exp(ωij) n∑ k=1 exp(ωik) (1)\nωij = f(W T [hj ; ei] + b) (2)\nHere, αij denotes the weight of attention from answer aspect ei to the jth word in the question, where ei ∈ {ee, er, et, ec}. f(·) is a non-linear activation function, such as hyperbolic tangent transformation here. Let n be the length of the question. W ∈ R2d×d is an intermediate matrix and b is the offset. Both of them are randomly initialized and updated during training. Subsequently, according to the specific answer aspect ei, the attention weights are employed to calculate a weighted sum of the hidden representations, resulting in a semantic vector that represent the question.\nqi = n∑ j=1 αijhj (3)\nThe similarity score of the question q and this particular candidate answer aspect ei (ei ∈ {ee, er, et, ec}) could be defined as follows.\nS (q, ei) = h(qi, ei) (4)\nThe scoring function h(·) is computed as the inner product between the sentence representation qi, which has already carried the attention from the answer aspect part, and the corresponding answer aspect ei, and is parametrized into the network and updated during the training process. • Question-towards-answer(Q-A) attention\nIntuitively, different question should value the four answer aspect differently. Since we have already calculated the scores of (q, ei), we define the final similarity score of the question q and each candidate answer a as follows.\nS (q, a) = ∑\nei∈{ee,er,et,ec}\nβei · S (q, ei) (5)\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nβei = exp (ωei)∑\nek∈{ee,er,et,ec} exp (ωek)\n(6)\nωei = f ( WT · [q; ei] + b ) (7)\nq = 1\nn ∑n j hj (8)\nHere βei denotes the attention of question towards answer aspects, indicating which answer aspect should be more focused in one (q, a) pair. W ∈ R2d×d is also a intermediate matrix as in the answer-towards-question attention part, and b is an offset value.6 q is calculated by averagely pooling all the bi-directional LSTM hidden state sequences, resulting a vector which represents the question to determine which answer aspect should be more focused.\nThe proposed cross-attention model could also be intuitively interpreted as a re-reading mechanism (Hermann et al., 2015). Our aim is to select correct answers from a candidate set. When we judge a candidate answer, suppose we first look at its type, and we will re-read the question to find out which part of the question should be more focused (handling attention). Then we go to next aspect and re-read the question again, until the all the aspects are utilized. After we read all the answer aspects and get all the scores, the final similarity score between question and answer should be a weighted sum of all the scores. We believe that this mechanism is beneficial for the system to better understand the question with the help of the answer aspects, and it may lead to a performance promotion."
    }, {
      "heading" : "3.2.4 Training",
      "text" : "We first construct the training data. Since we have (q, a) pairs as supervision data, candidate set Cq can be divided into two subsets, namely, correct answer set Pq and wrong answer set Nq. For each correct answer a ∈ Pq, we randomly select k wrong answers a′ ∈ Nq as negative examples. For some topic entities, there may be not enough wrong answers to acquire k wrong answers. Under this circumstance, we extend Nq from other randomly selected candidate set C ′q. With the generated training data, we are able to make use of pairwise training. The training loss is given as follows, which is a hinge loss.\nLq,a,a′ = [γ + S (q, a ′)− S (q, a)]+ (9)\n6Note that the W and b in the two attention part is different and independent.\nwhere γ is a positive real number that ensure a margin between positive and negative examples. And [z]+ means max(0, z). The intuition of this training strategy is to guarantee the score of positive question-answer pairs be higher than negative ones with a margin. The objective function is as follows.\nmin ∑ q 1 |Pq| ∑ a∈Pq ∑ a′∈Nq Lq,a,a′ (10)\nWe adopt stochastic gradient descent (SGD) to implement the learning process, shuffled minibatches are utilized."
    }, {
      "heading" : "3.2.5 Inference",
      "text" : "In testing stage, given the candidate answer setCq, we have to calculate S(q, a) for each a ∈ Cq, and find out the maximum value Smax.\nSmax = argmax a∈Cq\n{S (q, a)} (11)\nIt is worth noting that many questions have more than one answer, so it is improper to set the candidate answer which have the maximum value as the final answer. Instead, we take advantage of the margin γ. If the score of an candidate answer is within the margin compared with Smax, we put it in the final answer set.\nA = {â|Smax − S (q, â) < γ} (12)"
    }, {
      "heading" : "3.3 Combining Global Knowledge",
      "text" : "In this section, we elaborate how the global information of a KB could be leveraged. As stated before, we try to take into account the complete knowledge information of the KB. To this end, we adopt TransE model (Bordes et al., 2013) and integrate its outcome into our training process. In TransE, relations are considered as translations in the embedding space. For consistency, we denote each fact as (s, p, o). TransE utilizes pairwise training strategy as well. Randomly sampled corrupted facts (s′, p, o′) are the negative examples. The distance measure d(s + p, o) is defined as ‖s+ p− o‖22. And the training loss is given as follows. Lk =\n∑ (s,p,o)∈S ∑ (s′,p,o′)∈S′ [γk + d (s+ p, o)− d ( s′ + p, o′ ) ] +\n(13) Where S is the set of KB facts and S′ is the corrupted facts. In our QA task, we filter out the completely unrelated facts to save time. Specifically, we first collect all the topic entities of all the questions as initial set. Then, we expand the set by adding directly connected and 2-hop entities. Finally, all the facts containing these entities form\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nthe positive set, and the negative facts are randomly corrupted. This is a compromising solution due to the large scale of Freebase. To employ the global information in our training process, we adopt a multi-task training strategy. Specifically, we perform KB-QA training and TransE training in turn. After each epoch of KB-QA training, 100 epochs of TransE training are conducted, and the embeddings of the KB resources are shared and updated during both training processes. The proposed training process ensures that the global KB information act as additional supervision, and the interconnections among the resources are fully considered. In addition, as more KB resources are involved, the OOV problem is relieved. Since all the OOV resources have exactly the same attention towards a question, it will weaken the effectiveness of the attention model. So the alleviation of OOV is able to bring additional benefits to the attention model."
    }, {
      "heading" : "4 Experiments",
      "text" : "To evaluate the proposed method, we conduct experiments on WebQuestions (Berant et al., 2013) dataset that includes 3,778 question-answer pairs for training and 2,032 for testing. The questions are collected from Google Suggest API, and the answers are labeled manually by Amazon MTurk. All the answers are from Freebase. We use threequarter of the training data as training set, and the left as validate set. We use F1 score as evaluation matric, and the average result is computed by the script provided by Berant et al. (2013).\nNote that our proposed approach is an entire end-to-end method, which totally depends on training data. It is worth noting that Yih et al. (2015; 2016) achieves much higher F1 scores than other methods. Their staged system is able to address more questions with constraints and aggregations. However, their approach applies numbers of manually designed rules and features, which come from the observations on the training set questions. These particular manual efforts reduce the adaptability of their approach. Moreover, there are some integrated systems such as Xu et al. (2016a; 2016b) achieve higher F1 scores which leverage Wikipedia free text as external knowledge, so their systems are not directly comparable to ours."
    }, {
      "heading" : "4.1 Settings",
      "text" : "For KB-QA training, we use mini-batch stochastic gradient descent to minimize the pairwise train-\ning loss. The minibatch size is set to 100. The learning rate is set to 0.01. Both the word embedding matrix Ew and KB embedding matrix Ev are normalized after each epoch. The embedding size d = 512, then the hidden unit size is 256. Margin γ is set to 0.6. Negative example number k = 2000. We set the embedding dimension to 512 in TransE training process, and the minibatch size is also 100. γk is set to 1. All these hyperparameters of the proposed network is determined according to the performance on the validate set."
    }, {
      "heading" : "4.2 Results The effectiveness of the proposed approach",
      "text" : "To demonstrate the effectiveness of the proposed approach, we compare our method with state-of-the-art end-to-end NN-based methods.\nTable 1 shows the results on WebQuestions dataset. Bordes et al. (2014b) apply BOW method to obtain a single vector for both questions and answers. Bordes et al. (2014a) further improve their work by proposing the concept of subgraph embeddings. Besides the answer path, the subgraph contains all the entities and relations connected to the answer entity. The final vector is also obtained by bag-of-words strategy. Yang et al. (2014) follow the SP-based manner, but uses embeddings to map entities and relations into KB resources, then the question can be converted into logical forms. They jointly consider the two mapping processes. Dong et al. (2015) use three columns of CNNs to represent questions corresponding to three aspects of the answers, namely the answer context, the answer path and the answer type. Bordes et al. (2015) put KB-QA into the memory networks framework (Sukhbaatar et al., 2015), and achieves the state-of-the-art performance of endto-end methods. Our approach employs bidirectional LSTM, cross-attention model and global KB information.\nFrom the results, we observe that our approach achieves the best performance of all the end-to-end\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nmethods on WebQuestions. Bordes et al. (2014b; 2014a; 2015) all utilize BOW model to represent the questions, while ours takes advantage of the attention of answer aspects to dynamically represent the questions. Also note that Bordes et al. (2015) uses additional training data such as Reverb (Fader et al., 2011) and their original dataset SimpleQuestions. Dong et al. (2015) employs three fixed CNNs to represent questions, while ours is able to express the focus of each unique answer aspect to the words in the question. Besides, our approach employs the global KB information. So, we believe that the results faithfully shows that the proposed approach is more effective than the other competitive methods. Model Analysis\nIn this part, we further discuss the impacts of the components of our model. Table 2 indicates the effectiveness of different parts in the model.\nMethods F1\nLSTM 38.2\nBi LSTM 39.1\nBi LSTM+A-Q-ATT 41.6\nBi LSTM+C-ATT 41.8\nBi LSTM+GKI 40.4\nBi LSTM+A-Q-ATT+GKI 42.6\nBi LSTM+C-ATT+GKI 42.9\nTable 2: The ablation results of our models.\nLSTM employs unidirectional LSTM, and uses the last hidden state as the question representation. Bi LSTM adopts a bidirectional LSTM. A-Q-ATT denotes the answer-towards-question attention part, and C-ATT stands for our crossattention. GKI means global knowledge information. Bi LSTMS+C-ATT+GKI is our full proposed approach.From the results, we could observe the following.\n1) Bi LSTM+C-ATT dramatically improves the F1 score by 2.7 points compared with Bi LSTM, 0.2 points higher than Bi LSTM+A-Q-ATT. Similarly, Bi LSTM+C-ATT+GKI significantly outperforms Bi LSTM+GKI by 2.5 points, improving Bi LSTM+A-Q-ATT+GKI by 0.3 points. The results prove that the proposed cross-attention model is effective.\n2) Bi LSTM+GKI performs better than Bi LSTM, and achieves an improvement of 1.3 points. Similarly, Bi LSTM+C-ATT+GKI improves Bi LSTM+C-ATT by 1.1 points, which indicates that the proposed training strategy\nsuccessfully leverages the global information of the underlying KB.\n3) Bi LSTM+C-ATT+GKI achieves the best performance as we expected, and improves the original Bi LSTM dramatically by 3.8 points. This directly shows the power of the attention model and the global KB information.\nTo illustrate the effectiveness of the attention mechanism clearly, we present the attention weights of a question in the form of heat map as shown in Figure 3.\nwhere is the carpathian mountain range located\nanswer entity\nanswer type answer relation answer context\nFigure 3: The visualized attention heat map. Answer entity: /m/06npd(Slovakia), answer relation: partially containedby, answer type: /location/country, answer context: (/m/04dq9kf, /m/01mp, ...)\nFrom this example we observe that our methods is able to capture the attention properly. It is instructive to figure out the attention part of the question when dealing with different answer aspects. The heat map will help us understand which parts are most useful for selecting correct answers. For instance, from Figure 3, we can see that location.country is paying great attention to “Where”, indicating that “Where” is much more important than the other parts in the question when dealing with this type. In other words, the other parts are not that crucial since “Where” is strongly implying that the question is asking about a location. As for Q-A attention part, we see that answer type and answer relation are more important than other answer aspects in this example."
    }, {
      "heading" : "4.3 Error Analysis",
      "text" : "We randomly sample 100 imperfectly answered questions and categorize the errors into two main classes as follows. Wrong attention\nIn some occasions (17 in 100 questions, 17%), we find the generated attention weights unreasonable. For instance, for question “What are the songs that Justin Bieber wrote?”, answer type /music/composition pays the most attention on “What” rather than “songs”. We think this is due to the bias of the training data, and we believe these errors could be solved by introducing more instructive training data. Complex questions and label errors\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nAnother challenging problem is the complex questions (34%). For example, “When was the last time Knicks won the championship?” is actually to ask the last championship, but the predicted answers give all the championships. This is due to that the model cannot learn what “last” mean in the training process. In addition, the label mistakes also influence the evaluation (3%), such as, “What college did John Nash teach at?”, where the labeled answer is Princeton University, but Massachusetts Institute of Technology should also be an answer, and the proposed method is able to answer it correctly. Other errors include topic entity generation error and the multiple answers error (giving more answers than expected). We guess these errors are caused by the simple implementations of the related steps in our method, and we will not explain them in detail due to space limitation."
    }, {
      "heading" : "5 Related Work",
      "text" : ""
    }, {
      "heading" : "5.1 Neural Network-based KB-QA",
      "text" : "In recent years, deep neural networks have been applied to many NLP tasks, showing promising results. Bordes et al. (2014b) was the first to introduce NN-based method to solve KB-QA problem. The questions and KB triples were represented by vectors in a low dimensional space. Thus the cosine similarity could be used to find the most possible answer. BOW method was employed to obtain a single vector for both the questions and the answers. Pairwise training was utilized, and the negative examples were randomly selected from the KB facts. Bordes et al. (2014a) further improved their work by proposing the concept of subgraph embeddings. The key idea was to involve as much information as possible in the answer end. Besides the answer triple, the subgraph contained all the entities and relations connected to the answer entity. The final vector was also obtained by bag-of-words strategy.\nYih et al. (2014) focused on single-relation questions. The KB-QA task was divided into two steps. Firstly, they found the topic entity of the question. Then, the rest of the question was represented by CNNs and used to match relations. Yang et al. (2014) tackled entity and relation mapping as joint procedures. Actually, these two methods followed the SP-based manner, but they took advantage of neural networks to obtain intermediate mapping results.\nThe most similar work to ours is Dong et\nal. (2015). They considered the different aspects of answers, using three columns of CNNs to represent questions respectively. The difference is that our approach uses cross-attention mechanism for each unique answer aspect, so the question representation is not fixed to only three types. Moreover, we utilize the global KB information.\nXu et al. (2016a; 2016b) proposed integrated systems to address KB-QA problems incorporating Wikipedia free text, in which they used multichannel CNNs to extract relations."
    }, {
      "heading" : "5.2 Attention-based Model",
      "text" : "The attention mechanism has been widely used in different areas. Bahdanau et al. (2015) first applied attention model in NLP. They improved the encoder-decoder Neural Machine Translation (NMT) framework by jointly learning align and translation. They argued that representing source sentence by a fixed vector is unreasonable, and proposed a soft-align method, which could be understood as attention mechanism. Rush et al. (2015) implemented sentence-level summarization task. They utilized local attention-based model that generated each word of the summary conditioned on the input sentence.\nYin et al. (2016) tackled simple question answering by an attentive convolutional neural network. They stacked an attentive maxpooling above convolution layer to model the relationship between predicates and question patterns. Our approach differs from previous work in that we use attentions to help represent question dynamically, not generating current word from vocabulary as before."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we focus on KB-QA task. Firstly, we consider the impacts of the different answer aspects when representing the question, and propose a novel cross-attention model for KB-QA. Specifically, we employ the focus of the answer aspects to each question word and the attention weights of the question towards the answer aspects. This kind of dynamic representation is more precise and flexible. Secondly, we leverage the global KB information, which could take full advantage of the complete KB, and also alleviate the OOV problem for the attention model. The extensive experiments demonstrate that the proposed approach could achieve better performance compared with state-of-the-art end-to-end methods.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of ICLR,2015 .",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Semantic parsing on freebase from question-answer pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang." ],
      "venue" : "EMNLP. volume 2, page 6.",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "Freebase: a collaboratively created graph database for structuring human knowledge",
      "author" : [ "Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor." ],
      "venue" : "Proceedings of the 2008 ACM SIGMOD international conference on Management",
      "citeRegEx" : "Bollacker et al\\.,? 2008",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Question answering with subgraph embeddings",
      "author" : [ "Antoine Bordes", "Sumit Chopra", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1406.3676 .",
      "citeRegEx" : "Bordes et al\\.,? 2014a",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2014
    }, {
      "title" : "Large-scale simple question answering with memory networks",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1506.02075 .",
      "citeRegEx" : "Bordes et al\\.,? 2015",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2015
    }, {
      "title" : "Translating embeddings for modeling multirelational data",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Alberto GarciaDuran", "Jason Weston", "Oksana Yakhnenko." ],
      "venue" : "Advances in neural information processing systems. pages 2787–2795.",
      "citeRegEx" : "Bordes et al\\.,? 2013",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2013
    }, {
      "title" : "Open question answering with weakly supervised embedding models",
      "author" : [ "Antoine Bordes", "Jason Weston", "Nicolas Usunier." ],
      "venue" : "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, pages 165–180.",
      "citeRegEx" : "Bordes et al\\.,? 2014b",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2014
    }, {
      "title" : "Large-scale semantic parsing via schema matching and lexicon extension",
      "author" : [ "Qingqing Cai", "Alexander Yates." ],
      "venue" : "ACL. pages 423–433.",
      "citeRegEx" : "Cai and Yates.,? 2013",
      "shortCiteRegEx" : "Cai and Yates.",
      "year" : 2013
    }, {
      "title" : "Question answering over freebase with multicolumn convolutional neural networks",
      "author" : [ "Li Dong", "Furu Wei", "Ming Zhou", "Ke Xu." ],
      "venue" : "ACL. pages 260–269.",
      "citeRegEx" : "Dong et al\\.,? 2015",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2015
    }, {
      "title" : "Transitionbased dependency parsing with stack long shortterm memory",
      "author" : [ "Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A Smith." ],
      "venue" : "arXiv preprint arXiv:1505.08075 .",
      "citeRegEx" : "Dyer et al\\.,? 2015",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Identifying relations for open information extraction",
      "author" : [ "Anthony Fader", "Stephen Soderland", "Oren Etzioni." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages",
      "citeRegEx" : "Fader et al\\.,? 2011",
      "shortCiteRegEx" : "Fader et al\\.",
      "year" : 2011
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 1693–",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Scaling semantic parsers with on-the-fly ontology matching",
      "author" : [ "Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer." ],
      "venue" : "In Proceedings of EMNLP.. Citeseer.",
      "citeRegEx" : "Kwiatkowski et al\\.,? 2013",
      "shortCiteRegEx" : "Kwiatkowski et al\\.",
      "year" : 2013
    }, {
      "title" : "Sparql query language for rdf",
      "author" : [ "Eric Prudhommeaux", "Andy Seaborne." ],
      "venue" : "w3c recommendation, january 2008.",
      "citeRegEx" : "Prudhommeaux and Seaborne.,? 2008",
      "shortCiteRegEx" : "Prudhommeaux and Seaborne.",
      "year" : 2008
    }, {
      "title" : "Transforming dependency structures to logical forms for semantic parsing",
      "author" : [ "Siva Reddy", "Oscar Täckström", "Michael Collins", "Tom Kwiatkowski", "Dipanjan Das", "Mark Steedman", "Mirella Lapata." ],
      "venue" : "Transactions of the Association for Computational",
      "citeRegEx" : "Reddy et al\\.,? 2016",
      "shortCiteRegEx" : "Reddy et al\\.",
      "year" : 2016
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1509.00685 .",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "End-to-end memory networks. In Advances in neural information processing systems",
      "author" : [ "Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus" ],
      "venue" : null,
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems. pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "An introduction to question answering over linked data",
      "author" : [ "Christina Unger", "André Freitas", "Philipp Cimiano." ],
      "venue" : "Reasoning Web International Summer School. Springer, pages 100–140.",
      "citeRegEx" : "Unger et al\\.,? 2014",
      "shortCiteRegEx" : "Unger et al\\.",
      "year" : 2014
    }, {
      "title" : "Hybrid question answering over knowledge base and free text",
      "author" : [ "Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Techni-",
      "citeRegEx" : "Xu et al\\.,? 2016b",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Question answering on freebase via relation extraction and textual evidence",
      "author" : [ "Kun Xu", "Siva Reddy", "Yansong Feng", "Songfang Huang", "Dongyan Zhao." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Associ-",
      "citeRegEx" : "Xu et al\\.,? 2016a",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Joint relational embeddings for knowledge-based question answering",
      "author" : [ "Min-Chul Yang", "Nan Duan", "Ming Zhou", "HaeChang Rim." ],
      "venue" : "EMNLP. volume 14, pages 645–650.",
      "citeRegEx" : "Yang et al\\.,? 2014",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2014
    }, {
      "title" : "Information extraction over structured data: Question answering with freebase",
      "author" : [ "Xuchen Yao", "Benjamin Van Durme." ],
      "venue" : "ACL. Citeseer, pages 956– 966.",
      "citeRegEx" : "Yao and Durme.,? 2014",
      "shortCiteRegEx" : "Yao and Durme.",
      "year" : 2014
    }, {
      "title" : "Semantic parsing via staged query graph generation: Question answering with knowledge base",
      "author" : [ "Scott Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao" ],
      "venue" : null,
      "citeRegEx" : "Yih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yih et al\\.",
      "year" : 2015
    }, {
      "title" : "Semantic parsing for single-relation question answering",
      "author" : [ "Wen-tau Yih", "Xiaodong He", "Christopher Meek." ],
      "venue" : "ACL (2). Citeseer, pages 643–648.",
      "citeRegEx" : "Yih et al\\.,? 2014",
      "shortCiteRegEx" : "Yih et al\\.",
      "year" : 2014
    }, {
      "title" : "The value of semantic parse labeling for knowledge base question answering",
      "author" : [ "Wen-tau Yih", "Matthew Richardson", "Christopher Meek", "Ming-Wei Chang", "Jina Suh", "Matt Richardson", "Chris Meek", "Scott Wen-tau Yih." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Yih et al\\.,? 2016",
      "shortCiteRegEx" : "Yih et al\\.",
      "year" : 2016
    }, {
      "title" : "Simple question answering by attentive convolutional neural network",
      "author" : [ "Wenpeng Yin", "Mo Yu", "Bing Xiang", "Bowen Zhou", "Hinrich Schütze." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Techni-",
      "citeRegEx" : "Yin et al\\.,? 2016",
      "shortCiteRegEx" : "Yin et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning context-dependent mappings from sentences to logical form",
      "author" : [ "Luke S Zettlemoyer", "Michael Collins." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural",
      "citeRegEx" : "Zettlemoyer and Collins.,? 2009",
      "shortCiteRegEx" : "Zettlemoyer and Collins.",
      "year" : 2009
    }, {
      "title" : "Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars",
      "author" : [ "Luke S Zettlemoyer", "Michael Collins." ],
      "venue" : "arXiv preprint arXiv:1207.1420 .",
      "citeRegEx" : "Zettlemoyer and Collins.,? 2012",
      "shortCiteRegEx" : "Zettlemoyer and Collins.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "There are several tailor-made languages designed for querying KBs, such as SPARQL (Prudhommeaux and Seaborne, 2008).",
      "startOffset" : 82,
      "endOffset" : 115
    }, {
      "referenceID" : 19,
      "context" : "By contrast, knowledge base-based question answering (KB-QA) (Unger et al., 2014), which takes natural language as query language, is a more user-friendly solution, and has become a research focus in recent years.",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al.",
      "startOffset" : 93,
      "endOffset" : 242
    }, {
      "referenceID" : 7,
      "context" : "There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al.",
      "startOffset" : 93,
      "endOffset" : 242
    }, {
      "referenceID" : 1,
      "context" : "There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al.",
      "startOffset" : 93,
      "endOffset" : 242
    }, {
      "referenceID" : 15,
      "context" : "There are two mainstream research directions for this task: semantic parsing-based (SPbased) (Zettlemoyer and Collins, 2009, 2012; Kwiatkowski et al., 2013; Cai and Yates, 2013; Berant et al., 2013; Yih et al., 2015, 2016; Reddy et al., 2016) and information retrieval-based (IR-based) (Yao and Van Durme, 2014; Bordes et al.",
      "startOffset" : 93,
      "endOffset" : 242
    }, {
      "referenceID" : 6,
      "context" : "Recently, with the progress of deep learning, neural network-based (NN-based) methods have been introduced to the KB-QA task (Bordes et al., 2014b).",
      "startOffset" : 125,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "For example, Bordes et al. (2014a) consider the importance of the subgraph of the candidate answer.",
      "startOffset" : 13,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "For example, Bordes et al. (2014a) consider the importance of the subgraph of the candidate answer. Dong et al. (2015) make use of the context and the type of the answer.",
      "startOffset" : 13,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "For example, Bordes et al. (2014a) consider the importance of the subgraph of the candidate answer. Dong et al. (2015) make use of the context and the type of the answer. However, the representation of the question end is oligotrophic. Existing approaches often represent a question into a single vector using simple bag-of-words (BOW) model (Bordes et al., 2014a,b), whereas the relatedness to the answer end is neglected. We argue that a question should be represented differently according to the different focuses of various answer aspects1. Take the question “Who is the president of France?” and one of its candidate answers “Francois Hollande” as an example. When dealing with the answer entity Francois Holland, “president” and “France” in the question is more focused, and the question representation should bias towards the two words. While facing the answer type /business/board member, “Who” should be the most prominent word. Meantime, some questions may value answer type more than other answer aspects. While in some other questions, answer relation may be the most important information we should consider, which is dynamic and flexible corresponding to different questions and answers. Obviously, this is an attention mechanism, which reveals the mutual influences between the representation of questions and the corresponding answer aspects. We believe that such kind of representation is more expressive. Dong et al. (2015) represents questions using three CNNs with different parameters when dealing with different answer aspects including answer path, answer context and answer type.",
      "startOffset" : 13,
      "endOffset" : 1443
    }, {
      "referenceID" : 2,
      "context" : "We utilize Freebase (Bollacker et al., 2008) as our knowledge base.",
      "startOffset" : 20,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "For each question q, we use Freebase API (Bollacker et al., 2008) to identify a topic entity, which could be simply understood as the main entity of the question.",
      "startOffset" : 41,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "Then, the embeddings are fed into a long shortterm memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks.",
      "startOffset" : 65,
      "endOffset" : 99
    }, {
      "referenceID" : 18,
      "context" : "LSTM has been proven to be effective in many natural language processing (NLP) tasks such as machine translation (Sutskever et al., 2014) and dependency parsing (Dyer et al.",
      "startOffset" : 113,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : ", 2014) and dependency parsing (Dyer et al., 2015), and it is adept in harnessing long",
      "startOffset" : 31,
      "endOffset" : 50
    }, {
      "referenceID" : 11,
      "context" : "The proposed cross-attention model could also be intuitively interpreted as a re-reading mechanism (Hermann et al., 2015).",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 5,
      "context" : "To this end, we adopt TransE model (Bordes et al., 2013) and integrate its outcome into our training process.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "4 Experiments To evaluate the proposed method, we conduct experiments on WebQuestions (Berant et al., 2013) dataset that includes 3,778 question-answer pairs for training and 2,032 for testing.",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "4 Experiments To evaluate the proposed method, we conduct experiments on WebQuestions (Berant et al., 2013) dataset that includes 3,778 question-answer pairs for training and 2,032 for testing. The questions are collected from Google Suggest API, and the answers are labeled manually by Amazon MTurk. All the answers are from Freebase. We use threequarter of the training data as training set, and the left as validate set. We use F1 score as evaluation matric, and the average result is computed by the script provided by Berant et al. (2013). Note that our proposed approach is an entire end-to-end method, which totally depends on training data.",
      "startOffset" : 87,
      "endOffset" : 544
    }, {
      "referenceID" : 17,
      "context" : "(2015) put KB-QA into the memory networks framework (Sukhbaatar et al., 2015), and achieves the state-of-the-art performance of endto-end methods.",
      "startOffset" : 52,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : "Bordes et al. (2014b) apply BOW method to obtain a single vector for both questions and answers.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "Bordes et al. (2014b) apply BOW method to obtain a single vector for both questions and answers. Bordes et al. (2014a) further improve their work by proposing the concept of subgraph embeddings.",
      "startOffset" : 0,
      "endOffset" : 119
    }, {
      "referenceID" : 3,
      "context" : "Bordes et al. (2014b) apply BOW method to obtain a single vector for both questions and answers. Bordes et al. (2014a) further improve their work by proposing the concept of subgraph embeddings. Besides the answer path, the subgraph contains all the entities and relations connected to the answer entity. The final vector is also obtained by bag-of-words strategy. Yang et al. (2014) follow the SP-based manner, but uses embeddings to map entities and relations into KB resources, then the question can be converted into logical forms.",
      "startOffset" : 0,
      "endOffset" : 384
    }, {
      "referenceID" : 3,
      "context" : "Bordes et al. (2014b) apply BOW method to obtain a single vector for both questions and answers. Bordes et al. (2014a) further improve their work by proposing the concept of subgraph embeddings. Besides the answer path, the subgraph contains all the entities and relations connected to the answer entity. The final vector is also obtained by bag-of-words strategy. Yang et al. (2014) follow the SP-based manner, but uses embeddings to map entities and relations into KB resources, then the question can be converted into logical forms. They jointly consider the two mapping processes. Dong et al. (2015) use three columns of CNNs to represent questions corresponding to three aspects of the answers, namely the answer context, the answer path and the answer type.",
      "startOffset" : 0,
      "endOffset" : 604
    }, {
      "referenceID" : 3,
      "context" : "Bordes et al. (2014b) apply BOW method to obtain a single vector for both questions and answers. Bordes et al. (2014a) further improve their work by proposing the concept of subgraph embeddings. Besides the answer path, the subgraph contains all the entities and relations connected to the answer entity. The final vector is also obtained by bag-of-words strategy. Yang et al. (2014) follow the SP-based manner, but uses embeddings to map entities and relations into KB resources, then the question can be converted into logical forms. They jointly consider the two mapping processes. Dong et al. (2015) use three columns of CNNs to represent questions corresponding to three aspects of the answers, namely the answer context, the answer path and the answer type. Bordes et al. (2015) put KB-QA into the memory networks framework (Sukhbaatar et al.",
      "startOffset" : 0,
      "endOffset" : 785
    }, {
      "referenceID" : 10,
      "context" : "(2015) uses additional training data such as Reverb (Fader et al., 2011) and their original dataset SimpleQuestions.",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "Bordes et al. (2014b; 2014a; 2015) all utilize BOW model to represent the questions, while ours takes advantage of the attention of answer aspects to dynamically represent the questions. Also note that Bordes et al. (2015) uses additional training data such as Reverb (Fader et al.",
      "startOffset" : 0,
      "endOffset" : 223
    }, {
      "referenceID" : 3,
      "context" : "Bordes et al. (2014b; 2014a; 2015) all utilize BOW model to represent the questions, while ours takes advantage of the attention of answer aspects to dynamically represent the questions. Also note that Bordes et al. (2015) uses additional training data such as Reverb (Fader et al., 2011) and their original dataset SimpleQuestions. Dong et al. (2015) employs three fixed CNNs to represent questions, while ours is able to express the focus of each unique answer aspect to the words in the question.",
      "startOffset" : 0,
      "endOffset" : 352
    }, {
      "referenceID" : 3,
      "context" : "Bordes et al. (2014b) was the first to introduce NN-based method to solve KB-QA problem.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "Bordes et al. (2014b) was the first to introduce NN-based method to solve KB-QA problem. The questions and KB triples were represented by vectors in a low dimensional space. Thus the cosine similarity could be used to find the most possible answer. BOW method was employed to obtain a single vector for both the questions and the answers. Pairwise training was utilized, and the negative examples were randomly selected from the KB facts. Bordes et al. (2014a) further improved their work by proposing the concept of subgraph embeddings.",
      "startOffset" : 0,
      "endOffset" : 461
    }, {
      "referenceID" : 3,
      "context" : "Bordes et al. (2014b) was the first to introduce NN-based method to solve KB-QA problem. The questions and KB triples were represented by vectors in a low dimensional space. Thus the cosine similarity could be used to find the most possible answer. BOW method was employed to obtain a single vector for both the questions and the answers. Pairwise training was utilized, and the negative examples were randomly selected from the KB facts. Bordes et al. (2014a) further improved their work by proposing the concept of subgraph embeddings. The key idea was to involve as much information as possible in the answer end. Besides the answer triple, the subgraph contained all the entities and relations connected to the answer entity. The final vector was also obtained by bag-of-words strategy. Yih et al. (2014) focused on single-relation questions.",
      "startOffset" : 0,
      "endOffset" : 809
    }, {
      "referenceID" : 3,
      "context" : "Bordes et al. (2014b) was the first to introduce NN-based method to solve KB-QA problem. The questions and KB triples were represented by vectors in a low dimensional space. Thus the cosine similarity could be used to find the most possible answer. BOW method was employed to obtain a single vector for both the questions and the answers. Pairwise training was utilized, and the negative examples were randomly selected from the KB facts. Bordes et al. (2014a) further improved their work by proposing the concept of subgraph embeddings. The key idea was to involve as much information as possible in the answer end. Besides the answer triple, the subgraph contained all the entities and relations connected to the answer entity. The final vector was also obtained by bag-of-words strategy. Yih et al. (2014) focused on single-relation questions. The KB-QA task was divided into two steps. Firstly, they found the topic entity of the question. Then, the rest of the question was represented by CNNs and used to match relations. Yang et al. (2014) tackled entity and relation mapping as joint procedures.",
      "startOffset" : 0,
      "endOffset" : 1047
    }, {
      "referenceID" : 3,
      "context" : "Bordes et al. (2014b) was the first to introduce NN-based method to solve KB-QA problem. The questions and KB triples were represented by vectors in a low dimensional space. Thus the cosine similarity could be used to find the most possible answer. BOW method was employed to obtain a single vector for both the questions and the answers. Pairwise training was utilized, and the negative examples were randomly selected from the KB facts. Bordes et al. (2014a) further improved their work by proposing the concept of subgraph embeddings. The key idea was to involve as much information as possible in the answer end. Besides the answer triple, the subgraph contained all the entities and relations connected to the answer entity. The final vector was also obtained by bag-of-words strategy. Yih et al. (2014) focused on single-relation questions. The KB-QA task was divided into two steps. Firstly, they found the topic entity of the question. Then, the rest of the question was represented by CNNs and used to match relations. Yang et al. (2014) tackled entity and relation mapping as joint procedures. Actually, these two methods followed the SP-based manner, but they took advantage of neural networks to obtain intermediate mapping results. The most similar work to ours is Dong et al. (2015). They considered the different aspects of answers, using three columns of CNNs to represent questions respectively.",
      "startOffset" : 0,
      "endOffset" : 1297
    }, {
      "referenceID" : 0,
      "context" : "Bahdanau et al. (2015) first applied attention model in NLP.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "Bahdanau et al. (2015) first applied attention model in NLP. They improved the encoder-decoder Neural Machine Translation (NMT) framework by jointly learning align and translation. They argued that representing source sentence by a fixed vector is unreasonable, and proposed a soft-align method, which could be understood as attention mechanism. Rush et al. (2015) implemented sentence-level summarization task.",
      "startOffset" : 0,
      "endOffset" : 365
    }, {
      "referenceID" : 0,
      "context" : "Bahdanau et al. (2015) first applied attention model in NLP. They improved the encoder-decoder Neural Machine Translation (NMT) framework by jointly learning align and translation. They argued that representing source sentence by a fixed vector is unreasonable, and proposed a soft-align method, which could be understood as attention mechanism. Rush et al. (2015) implemented sentence-level summarization task. They utilized local attention-based model that generated each word of the summary conditioned on the input sentence. Yin et al. (2016) tackled simple question answering by an attentive convolutional neural network.",
      "startOffset" : 0,
      "endOffset" : 547
    } ],
    "year" : 2017,
    "abstractText" : "With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important. Knowledge base-based question answering (KB-QA) is one of the most promising approaches to access the substantial knowledge. Meantime, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results. However, previous work did not put emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers. This simple representation strategy is unable to express the proper information of the question. Hence, we present an end-to-end neural network model to represent the questions and their corresponding scores dynamically according to the various candidate answer aspects via cross-attention mechanism. In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers. Moreover, it also alleviates the out-ofvocabulary (OOV) problem, which helps the cross-attention model to represent the question more precisely. The experimental results on WebQuestions demonstrate the effectiveness of the proposed approach.",
    "creator" : "LaTeX with hyperref package"
  }
}