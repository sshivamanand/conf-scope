{
  "name" : "173.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Determining Gains Acquired from Word Embedding Quantitatively using Discrete Distribution Clustering",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Word embeddings, or word vectors, have been broadly adopted for document analysis (Mikolov et al., 2013b,a). A key appeal of word embedding methods is that they can be obtained from external large-scale corpus and then be easily utilized for different data. Before choosing word embeddings for data analysis, researchers must first consider how much extra gain can be brought from the “embedded” knowledge of words in comparison with that achieved by existing bag-of-words based approaches. Moreover, they must also consider how to quantify that gain. Such a preliminary evaluation is often necessary before any further decisions can be made about the data.\nAnswering such questions is important: almost every model used in practice exploits some basic\nrepresentations — bag-of-words and word embeddings — for the sake of its computational tractability. Based on word embeddings, high-level models are designed for various tasks. Examples include entity representations, similarity measures, data manifolds, hierarchical structures, language models, and neural architectures. Therefore, it is important to investigate whether the gain or loss found in practice should be credited to the extra assumptions associated with those high-level models or to the use of basic word embeddings. As our experiments demonstrate, introducing these extra assumptions will make individual methods effective only if certain constraints are met. We will address this issue from an unsupervised perspective.\nOur proposed clustering framework has several advantages. Instead of suppressing a document into a fixed-length vector feeding post-analysis, our framework uses the Wasserstein distance (or the Earth Mover’s Distance, EMD) as a metadistance to quantify the dissimilarity between two empirical nonparametric measures (or discrete distributions) over word embedding space (Wan, 2007; Kusner et al., 2015). Hence, it excludes any vector representation of the documents and sidesteps extra high-level assumptions, which is crucial and beneficial to evaluating the gain from basic word embeddings.\nOur approach is intuitive and robust. the Wasserstein distance considers the cross-term relationship between different words in a principled fashion. As defined, the distance between two documents, say A and B, are the minimum cumulative cost that words from document A need to “travel” to match exactly the point cloud of document B. Here, the travel cost of a path between two words is their (squared) Euclidean distance in the word embedding space. Therefore, how much\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nbenefit Wasserstein distance brings also depends on how the utilized word embedding space flattens document data as distributions, disentangling independent factors into different regions, subject to the interest of task.\nWhile Wasserstein distance is well suited for document analysis, a major obstacle of EMDbased approaches is the high-magnitude computation involved, especially for the original D2clustering method (Li and Wang, 2008) — an EMD-based clustering framework. The main technical hurdle is to compute the Wasserstein barycenter, which is a discrete distribution, efficiently for a given set of discrete distributions. Thanks to the recent advance of algorithms for efficient solving of Wasserstein barycenters, one can now perform document clustering in a nonparametric way by directly treating them as empirical measures over word embedding space (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017). In short, the intuition behind such a nonparametric framework is: By keeping D2-clustering to a minimal assumption set, we can achieve results of the highest possible clustering quality regardless the size and patterns of data. Obtaining highest quality clustering of unstructured text data merits consuming extra computational resources. Because it is a crucial step in many techniques and applications of natural language processing, such as cross-document co-reference resolution (Singh et al., 2011), document summarization (Radev et al., 2004; Wang and Li, 2010), retrospective events detection (Yang et al., 1998), and opinion mining (Zhai et al., 2011).\nOur contributions. Our work has two main contributions. First, we create a basic tool of document clustering with mere hyper-parameters at scale. Our tool leverages the state-of-the-art numerical toolbox developed for optimal transport to achieve computational feasibility. Meanwhile, it gives state-of-the-art clustering performances across heterogeneous text data — an advantage over other methods in the literature. Second, with our tool, one can quantitatively inspect how well a word-embedding model can fit the data and how much gain or loss will be obtained compared to traditional bag-of-words models. Acquiring insights on these questions is valuable for document analysis beyond clustering. The exploration of D2-clustering for documents will provide a win-\ndow for investigating these questions."
    }, {
      "heading" : "2 Related Work",
      "text" : "In the original D2-clustering framework (Li and Wang, 2008), calculating Wasserstein barycenter involves solving a large-scale LP problem at each inner iteration, severely limiting the scalability and robustness of the framework. Such high magnitude of computations had prohibited it from many real-world applications until recently. To accelerate the computation of Wasserstein barycenter, and ultimately to improve D2clustering, multiple numerical algorithmic efforts have been made in the recent few years (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017).\nAlthough the effectiveness of Wasserstein distance has been well recognized in the computer vision and multimedia literature, the property of Wasserstein barycenter has not been well understood. To our knowledge, there still lacks systematic study of applying Wasserstein barycenter and D2-clustering in document analysis with word embeddings.\nA closely related work to ours was proposed by authors of (Kusner et al., 2015) who recently connected the Wasserstein distance to the word embeddings for comparing documents. Our work differs from theirs in the methodology. We directly pursue a scalable clustering setting rather than construct a nearest neighbor graph based on calculated distances, because the calculation of the Wasserstein distances of all pairs is too expensive to be practical. The authors of (Kusner et al., 2015) used a lower bound that was cheaper to compute in order to prune unnecessary full distance calculation, but the scalability of this modified approach is still considered very limited, an issue to be discussed in Section 4.3. On the other hand, our approach adopts the framework similar to K-means which is of complexity O(n) per iteration, and usually converges within tens of iterations. The computation of D2-clustering, though in its original form was magnitudes heavier than other document clustering methods, can now be done with efficient parallelization and proper implementations (Ye et al., 2017)."
    }, {
      "heading" : "3 Our Approach",
      "text" : "This section introduces the distance, the D2clustering technique, the fast computation frame-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nwork, and how they are used in the proposed document clustering method."
    }, {
      "heading" : "3.1 Wasserstein Distance",
      "text" : "Suppose we represent each document d k consisting m\nk unique words by a discrete measure or a discrete distribution, where k = 1, . . . , N with N being the sample size:\nd k =\nX mk\ni=1 w(k) i x (k) i . (1)\nHere x denotes the Dirac measure with support x, and w(k)\ni 0 is the “importance weight” for the i-th word in the k-th document, with P mk i=1w (k) i = 1. And x(k) i\n2 Rd, called a support point, is the semantic embedding vector of the i-th word. The 2nd-order Wasserstein distance between two documents d1 and d2 (and likewise for any document pairs) is defined by the following LP problem: W 2(d1, d2) :=\nmin\n⇧\nP i,j ⇡ i,j kx(1) i x(2) j\nk22 s.t. P m2 j=1 ⇡i,j = wi, 8i, P m1 i=1 ⇡i,j = wj , 8j\n⇡ i,j 0, 8i, j , (2)\nwhere ⇧ = {⇡ i,j } is a m1 ⇥ m2 coupling matrix, and let {C\ni,j := kx(1) i x(2) j k22} be transportation costs between words. Wasserstein distance is a true metric (Villani, 2003) for measures, and its best exact algorithm has a complexity of O(m3 logm) (Pele and Werman, 2009; Cuturi, 2013), if m1 = m2 = m."
    }, {
      "heading" : "3.2 Discrete Distribution (D2-) Clustering",
      "text" : "D2-clustering (Li and Wang, 2008) iterates between the assignment step and centroids updating step in a similar way as the Lloyd’s K-means. Suppose we are to find K clusters. The assignment step finds each member distribution its nearest mean from K candidates. The mean of each cluster is again a discrete distribution with m support points, denoted by c\ni , i = 1, . . . ,K. Each mean is iteratively updated to minimize its total within cluster variation. We can write the D2clustering problem as follows: given sample data {d\nk }N k=1, support size of means m, and desired\nnumber of clusters K, D2-clustering solves\nmin\nc1,...,cK\nX N\nk=1 min 1iK W 2(d k , c i ) , (3)\nwhere c1, . . . , cK are Wasserstein barycenters. At the core of solving the above formulation is an optimization method that searches the Wasserstein barycenters of varying partitions. Therefore, we concentrate on the following problem. For each cluster, we reorganize the index of member distributions from 1, . . . , n. The Wasserstein barycenter (Agueh and Carlier, 2011; Cuturi and Doucet, 2014) is by definition the solution of\nmin\nc\nX n\nk=1 W 2(d k , c) , (4)\nwhere c = P m\ni=1wi xi . The above Wasserstein barycenter formulation involves two levels of optimization: the outer level finding the minimizer of total variations, and the inner level solving Wasserstein distances. We remark that in D2clustering, we need to solve multiple Wasserstein barycenters rather than a single one. This constitutes the third level of optimization."
    }, {
      "heading" : "3.3 Modified Bregman ADMM for Computing Wasserstein Barycenter",
      "text" : "The recent modified Bregman alternating direction method of multiplier (B-ADMM) algorithm (Ye et al., 2017), motivated by (Wang and Banerjee, 2014), is a practical choice for computing Wasserstein barycenters. We briefly sketch their algorithmic procedure of this optimization method here for the sake of completeness. To solve for Wasserstein barycenter defined in Eq. (4), the key procedure of the modified Bregman ADMM involves iterative updates of four block of primal variables: the support points of c — {x\ni }m i=1 (with trans-\nportation costs {C i,j }(k) for k = 1, . . . , n), the importance weights of c — {w\ni }m i=1, and two sets of\nsplit matching variables — {⇡(k,1) i,j } and {⇡(k,2) i,j }, for k = 1, . . . , n, as well as Lagrangian variables { (k)\ni,j } for k = 1, . . . , n. In the end, both {⇡(k,1) i,j } and {⇡(k,2)\ni,j } converge to the matching weight in Eq. (2) with respect to d(c, d\nk ). The iterative algorithm proceeds as follows until c converges or a maximum number of iterations are reached: given constant ⌧ 10, ⇢ / P i,j,k C(k) i,jP\nn k=1mkm and round-off\ntolerance ✏ = 10 10, those variables are updated in the following order. Update {x\ni }m i=1 and {C (k) i,j } in every ⌧ iterations:\nx i :=\n1\nnw i\nX n\nk=1\nX mk\nj=1 ⇡(k,1) i,j x(k) j , 8i, (5)\nC(k) i,j := kx i x(k) j k22, 8i, j and k, (6)\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nUpdate {⇡(k,1) i,j } and {⇡(k,2) i,j }. For each i, j and k,\n⇡(k,2) i,j := ⇡(k,2) i,j exp\nC(k)\ni,j\n(k) i,j\n⇢\n! + ✏ (7)\n⇡(k,1) i,j := w(k) j ⇡(k,2) i,j\n.⇣X m\nl=1 ⇡(k,2) l,j\n⌘ (8)\n⇡(k,1) i,j := ⇡(k,1) i,j exp ⇣ (k) i,j /⇢ ⌘ + ✏ (9)\nUpdate {w i }m i=1. For i = 1, . . . ,m ,\nw i :=\nnX\nk=1\nP mk j=1 ⇡ (k,1) i,j\nP i,j ⇡(k,1) i,j\n(10)\nw i := w i\n.⇣X m\ni=1 w i\n⌘ (11)\nUpdate {⇡(k,2) i,j } and { (k) i,j }. For each i, j and k,\n⇡(k,2) i,j := w i ⇡(k,1) i,j\n.⇣X mk\nl=1 ⇡(k,1) i,l\n⌘ (12)\n(k) i,j := (k) i,j + ⇢ ⇣ ⇡(k,1) i,l ⇡(k,2) i,l ⌘ . (13)\nEq. (5)-(13) can all be vectorized as very efficient numerical routines. In a data parallel implementation, only Eq. (5) and Eq. (10) (involving P n\nk=1) needs to be synchronized. The software package detailed in (Ye et al., 2017) was used to generate relevant experiments. We make available our codes and pre-processed datasets for reproducing all experiments of our approach."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets and Evaluation Metrics",
      "text" : "We prepare six datasets to conduct a set of experiments. Two short-text datasets are created as follows. (1) BBCNews abstract: we concatenate the title and the first sentence of news posts from BBCNews dataset1 to create an abstract version. (2) Wiki events: each cluster/class contains a set of news abstracts on the same story such as “2014 Crimean Crisis” crawled from Wikipedia current events following (Wu et al., 2015); this dataset offers more challenges because it has more finegrained classes and fewer documents (with shorter length) per class than the others have. It also shows more realistic nature of real-world applications such as news event clustering.\nWe also experiment with two long-text datasets and two domain-specific text datasets.\n1BBCNews and BBCSport are downloaded from http://mlg.ucd.ie/datasets/bbc.html\n(3) Reuters-21578: we obtain the original Reuters21578 text dataset and process as follows: remove documents with multiple categories, remove documents with empty body, remove duplicates, and select documents from the largest 10 categories. Reuters dataset is a highly unbalanced dataset (the top category has more than 3000 documents while the 10-th category has fewer than 100), this imbalance induces some extra randomness in comparing the results. (4) 20Newsgroups “bydate” version: we obtain the raw “bydate” versionand process them as follows: remove headers and footers, remove URLs and Email addresses, delete documents with less than 10 words. 20Newsgroups have roughly comparable sizes of categories. (5) BBCSports. (6) Ohsumed and Ohsumed-full: documents are medical abstracts from the MeSH categories of the year 1991. Specifically, there are 23 cardiovascular diseases categories.\nEvaluating clustering results is known to be nontrivial. We use the following three sets of quantitative metrics to assess the quality of clusters by knowing the groundtruth categorical labels of documents: (1) Homogeneity, Completeness, and V-measure (Rosenberg and Hirschberg, 2007); (2) Adjusted Mutual Information (AMI) (Vinh et al., 2010); (3) Adjusted Rand Index (ARI) (Rand, 1971). For sensitivity analysis, we use the homogeneity score (Rosenberg and Hirschberg, 2007) as a projection dimension of other metrics, creating a 2D plot to visualize the metrics of a method along different homogeneity levels. Generally speaking, more clusters leads to higher homogeneity by chance."
    }, {
      "heading" : "4.2 Methods in Comparison",
      "text" : "We examine four categories of methods that place a vector-space model over documents, and compare them to our D2-clustering framework. When needed, we use K-means++ to obtain clusters from dimension reduced vectors. To diminish the randomness brought by K-mean initialization, we ensemble the clustering results of 50 repeated runs (Strehl and Ghosh, 2003), and report the metrics for the ensembled one. The largest possible vocabulary used, excluding word embedding based approaches, is composed of words appearing in at least two documents. On each dataset, we select the same set of Ks, the number of clusters, for all methods. Typically, Ks are chosen around\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nthe number of groundtruth categories in logarithmic scale.\nWe prepare two versions of the TF-IDF vectors as the unigram model. The ensembled K-means methods are used to obtain clusters. (1) TF-IDF vector (Sparck Jones, 1972). (2) TF-IDF-N vector is found by choosing the most frequent N words in a corpus, where N 2 {500, 1000, 1500, 2000}. The difference between the two methods highlights the sensitivity issue brought by the size of chosen vocabulary.\nWe also compare our approach with the following seven additional baselines. They are (3) Spectral Clustering (Laplacian) (4) Latent Semantic Indexing (LSI) (Deerwester et al., 1990). (5) Locality Preserving Projection (LPP) (He and Niyogi, 2004; Cai et al., 2005). (6) Nonnegative Matrix Factorization (NMF) (Lee and Seung, 1999; Xu et al., 2003). (7) Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Hoffman et al., 2010; Pritchard et al., 2000). (8) Average of word vectors (AvgDoc) (9) Paragraph Vectors (PV) (Le and Mikolov, 2014). Details on their experimental setups and hyper-parameter search strategies can be found in the Appendix."
    }, {
      "heading" : "4.3 Runtime",
      "text" : "We report the runtime for our approach upon two largest datasets. The experiments regarding other smaller datasets all finish within minutes in a single machine, which we omit due to page constraints. Like K-means, the running time spent by our approach depends on the number of actual iterations before a termination criterion is met. In the Newsgroups dataset, with m = 100 and K = 45, the time per iteration is 121 seconds on 48 processors. In Reuters dataset, with m = 100 and K = 20, the time per iteration is 190 seconds on 24 processors. Each run finishes in around tens of iterations typically, upon which the percentage of label changes is less than 0.1%.\nOur approach adopts the Elkan’s algorithm pruning unnecessary computations of Wasserstein distance in assignment steps of K-means (Elkan, 2003). For the Newsgroups data (with m = 100 and K = 45), our approach terminates in 36 iterations, and totally computes 12, 162, 717 (⇡ 3.5% ⇥ 186122) distance pairs in assignment steps, saving 60% (⇡ 1 12,162,71736⇥45⇥18612 ) distance pairs to calculate in the standard D2-clustering. In comparison, the clustering approaches based on\nK nearest neighbor graph with the prefetch-andprune method of (Kusner et al., 2015) needs substantially more pairs to compute Wasserstein distance, meanwhile the speed-ups also suffer from the curse of dimensionality. Their detailed statistics are reported in Table 1. Based on the results, our approach is much more practical as a basic document clustering tool."
    }, {
      "heading" : "4.4 Results",
      "text" : "We summarize our numerical results in this section.\nRegular text datasets. The first four datasets in Table 2 cover quite general and broad topics. We consider them to be regular and representative datasets encountered more frequently in applications. We report the clustering performances of the ten methods in Fig. 1, where three different metrics are plotted against the clustering homogeneity. The higher result at the same level of homogeneity is better, and the ability to achieve higher homogeneity is also welcomed. Clearly, D2-clustering is the only method that shows ro-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nbustly superior performances among all ten methods. Specifically, it ranks first in three datasets, and second in the other one. In comparison, LDA performs competitively on the “Reuters” dataset, but is substantially unsuccessful on others. Meanwhile, LPP performs competitively on the “Wiki events” and “Newsgroups” datasets, but it underperforms on other two. Laplacian, LSI, and TfidfN can achieve comparably performance if their reduced dimensions are fine tuned, which unfortunately is not realistic in practice. NMF is a simple and effective method which always gives stable, though subpar, performance.\nShort texts vs. long texts. D2-clustering performs much more impressive on short texts (“BBC abstract” and “Wiki events”) than it does on long texts (“Reuters” and “Newsgroups”). This outcome is somewhat expected, because the bag-ofwords method suffers from high sparsity for short texts, and word-embedding based methods in theory should have an edge here. As shown in Fig. 1, D2-clustering has indeed outperformed other nonembedding approaches by a large margin on short texts (improved by about 40% and 20% respectively). Nevertheless, we find lifting from word embedding to document clustering is not a free\nlunch. Neither AvgDoc nor PV can perform as competitively as D2-clustering performs on both. Domain-specific text datasets. We are also interested in how word embedding can help group domain-specific texts into clusters. In particular, does the semantic knowledge “embedded” in words provides enough clues to discriminate fine-grained concepts? We report the best AMI achieved by each method in Table 3. Our preliminary result indicates state-of-the-art word embeddings do not provide enough gain here to exceed the performance of existing methodologies. On the easy one, aka “BBCSport” dataset, basic bagof-words approach (Tfidf and Tfidf-N) already suffices to discriminate different sport categories; and on the hard one, aka “Ohsumed” dataset, D2clustering only slightly improves over Tf-idf and others, ranking behind LPP. Meanwhile, we feel the overall quality of clustering “Ohsumed” texts is quite far from useful in practice, no matter which method to use. (See next section for more discussions.)"
    }, {
      "heading" : "4.5 Sensitivity to Word Embeddings.",
      "text" : "We validate the robustness of D2 clustering with different word embedding models, and we also\n7\n612\n613\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n662\n663\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nshow all their results in Fig. 2. As we mentioned, the effectiveness of Wasserstein document clustering depends on how relevant the utilized word embeddings are with the tasks. In those general document clustering tasks, however, word embedding models trained on general corpus perform robustly well with acceptably small variations. This outcome reveals our framework as generally effective and not dependent on a specific word embedding model. In addition, we also conduct experiments with word embeddings with a smaller dimension, aka 50 and 100. Their results are not as good as those we reported (therefore detailed numbers are not included due to space limit).\nInadequate embeddings may not be disastrous. In addition to our standard running set, we also try D2-clustering with purely random word embeddings, meaning each word vector is independently sampled from spherical Gaussian at 300 dimen-\nARI AMI V-measure BBCNews .146 .187 .190\nabstract .792 +442% .759+306% .762+301%\nWiki events .194 .369 .463 .277 +43% .545+48% .611+32%\nReuters .498 .524 .588 .515 +3% .534+2% .594+1%\nNewsgroups .194 .358 .390 .305 +57% .493+38% .499+28%\nBBCSport .755 .740 .760 .801 +6% .812+10% .817+8%\nOhsumed .080 .204 .292 .116 +45% .260+27% .349+20%\nTable 4: Comparison between random word embeddings (upper row) and meaningful pre-trained word embeddings (lower row), based on their best ARI, AMI, and V-measures. The improvements by percentiles are also shown in the subscripts.\n8\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nsion, to see how deficient it can be. From our experimental results, random word embeddings degrade the performance of D2-clustering, but it still performs much better than purely random clustering does, and is even consistently better than LDA. Its performances across different datasets is highly correlated with the bag-of-words (Tfidf and TfidfN). By comparing a pre-trained word embedding model to a randomly generated one, we find that the extra gain is significant (> 10%) in clustering four of the six datasets. Their detailed statistics are in Table 4 and Fig. 3."
    }, {
      "heading" : "5 Discussions",
      "text" : "Performance advantage. There has been one immediate observation from these studies, D2clustering always outperforms two of its degenerated cases, namely Tf-idf and AvgDoc, and three other popular methods: LDA, NMF, and PV, on all tasks. Therefore, for document clustering, users can expect to gain performance improvements by using our approach. Clustering sensitivity. From the four 2D plots in Fig. 1, we notice that the results of Laplacian, LSI and Tfidf-N are rather sensitive to their extra hyper-parameters. Once the vocabulary set, weight scheme and embeddings of words are fixed, our framework involves only two additional hyper-parameters: the number of intended clusters, K, and the selected support size of centroid distributions, m. We have chosen more than one m in all related experiments (m = {64, 100} for long documents, and m = 10, 20 for short documents). Our empirical experiments show that the effect of m on different metrics is less sensitive than the change of K. As shown in Fig. 1, results at different K are plotted for each method. The gray dots denote results of multiple runs of D2clustering. They are always contracted around the top-right region of the whole population, revealing\nthe predictive and robustly supreme performance of our approach. When bag-of-words suffices. Among the results of “BBCSport” dataset, Tfidf-N shows that by restricting the vocabulary set into a smaller one (which may be more relevant to the interest of tasks), it already can achieve highest clustering AMI without any other techniques. Other unsupervised regularization over data is likely unnecessary, or even degrades the performance slightly. Toward better word embeddings. Our experiments on the Ohsumed dataset have been limited. The result shows that it could be highly desirable to incorporate certain domain knowledge to derive more effective vector embeddings of words and phrases to encode their domainspecific knowledge, such as jargons that have knowledge dependencies and hierarchies in educational data mining, and signal words that capture multi-dimensional aspects of emotions in sentiment analysis.\nFinally, we report the best AMIs of all methods on all datasets in Table 3. By looking at each method and the average of best AMIs over six datasets, we find our proposed clustering framework often performs competitively and robustly, which is the only method reaching more than 90% of the best AMI on each dataset. Furthermore, this observation holds for varying lengths of documents and varying difficulty levels of clustering tasks. Our nonparametric framework benefits from both bag-of-words and word embeddings."
    }, {
      "heading" : "6 Conclusions",
      "text" : "This paper introduces a nonparametric clustering framework for document analysis. Its computational tractability, robustness and supreme performance, as a fundamental tool, are empirically validated. Its ease of use enables data scientists to use it for the pre-screening purpose of examining word embeddings in a specific task. Finally, the gains acquired from word embeddings are quantitatively measured from a nonparametric unsupervised perspective.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Barycenters in the wasserstein space",
      "author" : [ "Martial Agueh", "Guillaume Carlier." ],
      "venue" : "SIAM J. Math. Analysis 43(2):904–924.",
      "citeRegEx" : "Agueh and Carlier.,? 2011",
      "shortCiteRegEx" : "Agueh and Carlier.",
      "year" : 2011
    }, {
      "title" : "Laplacian eigenmaps and spectral techniques for embedding and clustering",
      "author" : [ "Mikhail Belkin", "Partha Niyogi." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS). volume 14, pages 585– 591.",
      "citeRegEx" : "Belkin and Niyogi.,? 2001",
      "shortCiteRegEx" : "Belkin and Niyogi.",
      "year" : 2001
    }, {
      "title" : "Iterative bregman projections for regularized transportation problems",
      "author" : [ "Jean-David Benamou", "Guillaume Carlier", "Marco Cuturi", "Luca Nenna", "Gabriel Peyré." ],
      "venue" : "SIAM J. Sci. Computing (SJSC) 37(2):A1111–A1138.",
      "citeRegEx" : "Benamou et al\\.,? 2015",
      "shortCiteRegEx" : "Benamou et al\\.",
      "year" : 2015
    }, {
      "title" : "Latent dirichlet allocation",
      "author" : [ "David M Blei", "Andrew Y Ng", "Michael I Jordan." ],
      "venue" : "J. Machine Learning Research (JMLR) 3:993–1022.",
      "citeRegEx" : "Blei et al\\.,? 2003",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Document clustering using locality preserving indexing",
      "author" : [ "Deng Cai", "Xiaofei He", "Jiawei Han." ],
      "venue" : "Trans. Knowledge and Data Engineering (TKDE) 17(12):1624–1637.",
      "citeRegEx" : "Cai et al\\.,? 2005",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2005
    }, {
      "title" : "Sinkhorn distances: Lightspeed computation of optimal transport",
      "author" : [ "Marco Cuturi." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS). pages 2292–2300.",
      "citeRegEx" : "Cuturi.,? 2013",
      "shortCiteRegEx" : "Cuturi.",
      "year" : 2013
    }, {
      "title" : "Fast computation of wasserstein barycenters",
      "author" : [ "Marco Cuturi", "Arnaud Doucet." ],
      "venue" : "Int. Conf. Machine Learning (ICML). pages 685–693.",
      "citeRegEx" : "Cuturi and Doucet.,? 2014",
      "shortCiteRegEx" : "Cuturi and Doucet.",
      "year" : 2014
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "Scott C. Deerwester", "Susan T Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman." ],
      "venue" : "J. American Soc. Information Science 41(6):391–407.",
      "citeRegEx" : "Deerwester et al\\.,? 1990",
      "shortCiteRegEx" : "Deerwester et al\\.",
      "year" : 1990
    }, {
      "title" : "Using the triangle inequality to accelerate k-means",
      "author" : [ "Charles Elkan." ],
      "venue" : "Int. Conf. Machine Learning (ICML). volume 3, pages 147–153.",
      "citeRegEx" : "Elkan.,? 2003",
      "shortCiteRegEx" : "Elkan.",
      "year" : 2003
    }, {
      "title" : "Locality preserving projections",
      "author" : [ "Xiaofei He", "Partha Niyogi." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS). MIT, volume 16, page 153.",
      "citeRegEx" : "He and Niyogi.,? 2004",
      "shortCiteRegEx" : "He and Niyogi.",
      "year" : 2004
    }, {
      "title" : "Online learning for latent Dirichlet allocation",
      "author" : [ "Matthew Hoffman", "Francis R Bach", "David M Blei." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS). pages 856–864.",
      "citeRegEx" : "Hoffman et al\\.,? 2010",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2010
    }, {
      "title" : "From word embeddings to document distances",
      "author" : [ "Matt J Kusner", "Yu Sun", "Nicholas N I. Kolkin", "K Q. Weinberger." ],
      "venue" : "Int. Conf. Machine Learning (ICML).",
      "citeRegEx" : "Kusner et al\\.,? 2015",
      "shortCiteRegEx" : "Kusner et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc Le", "Tomas Mikolov." ],
      "venue" : "Int. Conf. Machine Learning. pages 1188–1196.",
      "citeRegEx" : "Le and Mikolov.,? 2014",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    }, {
      "title" : "Learning the parts of objects by non-negative matrix factorization",
      "author" : [ "Daniel D Lee", "H Sebastian Seung." ],
      "venue" : "Nature 401(6755):788–791.",
      "citeRegEx" : "Lee and Seung.,? 1999",
      "shortCiteRegEx" : "Lee and Seung.",
      "year" : 1999
    }, {
      "title" : "Real-time computerized annotation of pictures",
      "author" : [ "Jia Li", "James Z Wang." ],
      "venue" : "Trans. Pattern Analysis and Machine Intelligence (PAMI) 30(6):985–1002.",
      "citeRegEx" : "Li and Wang.,? 2008",
      "shortCiteRegEx" : "Li and Wang.",
      "year" : 2008
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS). pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig." ],
      "venue" : "HLT-NAACL. pages 746– 751.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Fast and robust earth mover’s distances",
      "author" : [ "Ofir Pele", "Michael Werman." ],
      "venue" : "Int. Conf. Computer Vision (ICCV). IEEE, pages 460–467.",
      "citeRegEx" : "Pele and Werman.,? 2009",
      "shortCiteRegEx" : "Pele and Werman.",
      "year" : 2009
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Empiricial Methods in Natural Language Processing (EMNLP). volume 12, pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Inference of population structure using multilocus genotype data",
      "author" : [ "Jonathan K Pritchard", "Matthew Stephens", "Peter Donnelly." ],
      "venue" : "Genetics 155(2):945–959.",
      "citeRegEx" : "Pritchard et al\\.,? 2000",
      "shortCiteRegEx" : "Pritchard et al\\.",
      "year" : 2000
    }, {
      "title" : "Centroid-based summarization of multiple documents",
      "author" : [ "Dragomir R Radev", "Hongyan Jing", "Małgorzata Styś", "Daniel Tam." ],
      "venue" : "Information Processing & Management 40(6):919–938.",
      "citeRegEx" : "Radev et al\\.,? 2004",
      "shortCiteRegEx" : "Radev et al\\.",
      "year" : 2004
    }, {
      "title" : "Objective criteria for the evaluation of clustering methods",
      "author" : [ "William M Rand." ],
      "venue" : "J. American Statistical Association 66(336):846–850.",
      "citeRegEx" : "Rand.,? 1971",
      "shortCiteRegEx" : "Rand.",
      "year" : 1971
    }, {
      "title" : "Vmeasure: A conditional entropy-based external cluster evaluation measure",
      "author" : [ "Andrew Rosenberg", "Julia Hirschberg." ],
      "venue" : "EMNLP-CoNLL. volume 7, pages 410–420.",
      "citeRegEx" : "Rosenberg and Hirschberg.,? 2007",
      "shortCiteRegEx" : "Rosenberg and Hirschberg.",
      "year" : 2007
    }, {
      "title" : "Large-scale cross-document coreference using distributed inference and hierarchical models",
      "author" : [ "Sameer Singh", "Amarnag Subramanya", "Fernando Pereira", "Andrew McCallum." ],
      "venue" : "ACL-HLT . Association for Computational Linguistics, pages 793–803.",
      "citeRegEx" : "Singh et al\\.,? 2011",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2011
    }, {
      "title" : "A statistical interpretation of term specificity and its application in retrieval",
      "author" : [ "Karen Sparck Jones." ],
      "venue" : "J. Documentation 28(1):11–21.",
      "citeRegEx" : "Jones.,? 1972",
      "shortCiteRegEx" : "Jones.",
      "year" : 1972
    }, {
      "title" : "Cluster ensembles—a knowledge reuse framework for combining multiple partitions",
      "author" : [ "Alexander Strehl", "Joydeep Ghosh." ],
      "venue" : "J. Machine Learning Research (JMLR) 3:583–617.",
      "citeRegEx" : "Strehl and Ghosh.,? 2003",
      "shortCiteRegEx" : "Strehl and Ghosh.",
      "year" : 2003
    }, {
      "title" : "Topics in optimal transportation",
      "author" : [ "Cédric Villani" ],
      "venue" : null,
      "citeRegEx" : "Villani.,? \\Q2003\\E",
      "shortCiteRegEx" : "Villani.",
      "year" : 2003
    }, {
      "title" : "Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance",
      "author" : [ "Nguyen Xuan Vinh", "Julien Epps", "James Bailey." ],
      "venue" : "J. Machine Learning Research (JMLR) 11:2837–2854.",
      "citeRegEx" : "Vinh et al\\.,? 2010",
      "shortCiteRegEx" : "Vinh et al\\.",
      "year" : 2010
    }, {
      "title" : "A novel document similarity measure based on earth movers distance",
      "author" : [ "Xiaojun Wan." ],
      "venue" : "Information Sciences 177(18):3718–3730.",
      "citeRegEx" : "Wan.,? 2007",
      "shortCiteRegEx" : "Wan.",
      "year" : 2007
    }, {
      "title" : "Document update summarization using incremental hierarchical clustering",
      "author" : [ "Dingding Wang", "Tao Li." ],
      "venue" : "Int. Conf. on Information and knowledge Management (CIKM). ACM, pages 279–288.",
      "citeRegEx" : "Wang and Li.,? 2010",
      "shortCiteRegEx" : "Wang and Li.",
      "year" : 2010
    }, {
      "title" : "Bregman alternating direction method of multipliers",
      "author" : [ "Huahua Wang", "Arindam Banerjee." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS). pages 2816–2824.",
      "citeRegEx" : "Wang and Banerjee.,? 2014",
      "shortCiteRegEx" : "Wang and Banerjee.",
      "year" : 2014
    }, {
      "title" : "Storybase: Towards building a knowledge base for news events",
      "author" : [ "Zhaohui Wu", "Chen Liang", "C Lee Giles." ],
      "venue" : "ACL-IJCNLP 2015. pages 133–138.",
      "citeRegEx" : "Wu et al\\.,? 2015",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2015
    }, {
      "title" : "Document clustering based on non-negative matrix factorization",
      "author" : [ "Wei Xu", "Xin Liu", "Yihong Gong." ],
      "venue" : "ACM SIGIR Conf. on Research and Development in Informaion Retrieval. ACM, pages 267– 273.",
      "citeRegEx" : "Xu et al\\.,? 2003",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2003
    }, {
      "title" : "A study of retrospective and on-line event detection",
      "author" : [ "Yiming Yang", "Tom Pierce", "Jaime Carbonell." ],
      "venue" : "SIGIR Conf. on Research and Development in Information Retrieval. ACM, pages 28–36.",
      "citeRegEx" : "Yang et al\\.,? 1998",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 1998
    }, {
      "title" : "Scaling up discrete distribution clustering using admm",
      "author" : [ "Jianbo Ye", "Jia Li." ],
      "venue" : "Int. Conf. Image Processing (ICIP). IEEE, pages 5267–5271.",
      "citeRegEx" : "Ye and Li.,? 2014",
      "shortCiteRegEx" : "Ye and Li.",
      "year" : 2014
    }, {
      "title" : "Fast discrete distribution clustering using wasserstein barycenter with sparse support",
      "author" : [ "Jianbo Ye", "Panruo Wu", "James Z. Wang", "Jia Li." ],
      "venue" : "IEEE Transactions on Signal Processing To appear. https://doi.org/10.1109/TSP.2017.2659647.",
      "citeRegEx" : "Ye et al\\.,? 2017",
      "shortCiteRegEx" : "Ye et al\\.",
      "year" : 2017
    }, {
      "title" : "Clustering product features for opinion mining",
      "author" : [ "Zhongwu Zhai", "Bing Liu", "Hua Xu", "Peifa Jia." ],
      "venue" : "Int. Conf. on Web Search and Data Mining (WSDM). ACM, pages 347–354.",
      "citeRegEx" : "Zhai et al\\.,? 2011",
      "shortCiteRegEx" : "Zhai et al\\.",
      "year" : 2011
    }, {
      "title" : "A new mallows distance based metric for comparing clusterings",
      "author" : [ "Ding Zhou", "Jia Li", "Hongyuan Zha." ],
      "venue" : "Int. Conf. Machine Learning (ICML). ACM, pages 1028–1035.",
      "citeRegEx" : "Zhou et al\\.,? 2005",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Instead of suppressing a document into a fixed-length vector feeding post-analysis, our framework uses the Wasserstein distance (or the Earth Mover’s Distance, EMD) as a metadistance to quantify the dissimilarity between two empirical nonparametric measures (or discrete distributions) over word embedding space (Wan, 2007; Kusner et al., 2015).",
      "startOffset" : 312,
      "endOffset" : 344
    }, {
      "referenceID" : 11,
      "context" : "Instead of suppressing a document into a fixed-length vector feeding post-analysis, our framework uses the Wasserstein distance (or the Earth Mover’s Distance, EMD) as a metadistance to quantify the dissimilarity between two empirical nonparametric measures (or discrete distributions) over word embedding space (Wan, 2007; Kusner et al., 2015).",
      "startOffset" : 312,
      "endOffset" : 344
    }, {
      "referenceID" : 14,
      "context" : "While Wasserstein distance is well suited for document analysis, a major obstacle of EMDbased approaches is the high-magnitude computation involved, especially for the original D2clustering method (Li and Wang, 2008) — an EMD-based clustering framework.",
      "startOffset" : 197,
      "endOffset" : 216
    }, {
      "referenceID" : 6,
      "context" : "Thanks to the recent advance of algorithms for efficient solving of Wasserstein barycenters, one can now perform document clustering in a nonparametric way by directly treating them as empirical measures over word embedding space (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017).",
      "startOffset" : 230,
      "endOffset" : 311
    }, {
      "referenceID" : 34,
      "context" : "Thanks to the recent advance of algorithms for efficient solving of Wasserstein barycenters, one can now perform document clustering in a nonparametric way by directly treating them as empirical measures over word embedding space (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017).",
      "startOffset" : 230,
      "endOffset" : 311
    }, {
      "referenceID" : 2,
      "context" : "Thanks to the recent advance of algorithms for efficient solving of Wasserstein barycenters, one can now perform document clustering in a nonparametric way by directly treating them as empirical measures over word embedding space (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017).",
      "startOffset" : 230,
      "endOffset" : 311
    }, {
      "referenceID" : 35,
      "context" : "Thanks to the recent advance of algorithms for efficient solving of Wasserstein barycenters, one can now perform document clustering in a nonparametric way by directly treating them as empirical measures over word embedding space (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017).",
      "startOffset" : 230,
      "endOffset" : 311
    }, {
      "referenceID" : 23,
      "context" : "Because it is a crucial step in many techniques and applications of natural language processing, such as cross-document co-reference resolution (Singh et al., 2011), document summarization (Radev et al.",
      "startOffset" : 144,
      "endOffset" : 164
    }, {
      "referenceID" : 20,
      "context" : ", 2011), document summarization (Radev et al., 2004; Wang and Li, 2010), retrospective events detection (Yang et al.",
      "startOffset" : 32,
      "endOffset" : 71
    }, {
      "referenceID" : 29,
      "context" : ", 2011), document summarization (Radev et al., 2004; Wang and Li, 2010), retrospective events detection (Yang et al.",
      "startOffset" : 32,
      "endOffset" : 71
    }, {
      "referenceID" : 33,
      "context" : ", 2004; Wang and Li, 2010), retrospective events detection (Yang et al., 1998), and opinion mining (Zhai et al.",
      "startOffset" : 59,
      "endOffset" : 78
    }, {
      "referenceID" : 36,
      "context" : ", 1998), and opinion mining (Zhai et al., 2011).",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "In the original D2-clustering framework (Li and Wang, 2008), calculating Wasserstein barycenter involves solving a large-scale LP problem at each inner iteration, severely limiting the scalability and robustness of the framework.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "To accelerate the computation of Wasserstein barycenter, and ultimately to improve D2clustering, multiple numerical algorithmic efforts have been made in the recent few years (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017).",
      "startOffset" : 175,
      "endOffset" : 256
    }, {
      "referenceID" : 34,
      "context" : "To accelerate the computation of Wasserstein barycenter, and ultimately to improve D2clustering, multiple numerical algorithmic efforts have been made in the recent few years (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017).",
      "startOffset" : 175,
      "endOffset" : 256
    }, {
      "referenceID" : 2,
      "context" : "To accelerate the computation of Wasserstein barycenter, and ultimately to improve D2clustering, multiple numerical algorithmic efforts have been made in the recent few years (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017).",
      "startOffset" : 175,
      "endOffset" : 256
    }, {
      "referenceID" : 35,
      "context" : "To accelerate the computation of Wasserstein barycenter, and ultimately to improve D2clustering, multiple numerical algorithmic efforts have been made in the recent few years (Cuturi and Doucet, 2014; Ye and Li, 2014; Benamou et al., 2015; Ye et al., 2017).",
      "startOffset" : 175,
      "endOffset" : 256
    }, {
      "referenceID" : 11,
      "context" : "A closely related work to ours was proposed by authors of (Kusner et al., 2015) who recently connected the Wasserstein distance to the word embeddings for comparing documents.",
      "startOffset" : 58,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "The authors of (Kusner et al., 2015) used a lower bound that was cheaper to compute in order to prune unnecessary full distance calculation, but the scalability of this modified approach is still considered very limited, an issue to be discussed in Section 4.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 35,
      "context" : "The computation of D2-clustering, though in its original form was magnitudes heavier than other document clustering methods, can now be done with efficient parallelization and proper implementations (Ye et al., 2017).",
      "startOffset" : 199,
      "endOffset" : 216
    }, {
      "referenceID" : 26,
      "context" : "Wasserstein distance is a true metric (Villani, 2003) for measures, and its best exact algorithm has a complexity of O(m3 logm) (Pele and Werman, 2009; Cuturi, 2013), if m1 = m2 = m.",
      "startOffset" : 38,
      "endOffset" : 53
    }, {
      "referenceID" : 17,
      "context" : "Wasserstein distance is a true metric (Villani, 2003) for measures, and its best exact algorithm has a complexity of O(m3 logm) (Pele and Werman, 2009; Cuturi, 2013), if m1 = m2 = m.",
      "startOffset" : 128,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "Wasserstein distance is a true metric (Villani, 2003) for measures, and its best exact algorithm has a complexity of O(m3 logm) (Pele and Werman, 2009; Cuturi, 2013), if m1 = m2 = m.",
      "startOffset" : 128,
      "endOffset" : 165
    }, {
      "referenceID" : 14,
      "context" : "D2-clustering (Li and Wang, 2008) iterates between the assignment step and centroids updating step in a similar way as the Lloyd’s K-means.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "The Wasserstein barycenter (Agueh and Carlier, 2011; Cuturi and Doucet, 2014) is by definition the solution of",
      "startOffset" : 27,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "The Wasserstein barycenter (Agueh and Carlier, 2011; Cuturi and Doucet, 2014) is by definition the solution of",
      "startOffset" : 27,
      "endOffset" : 77
    }, {
      "referenceID" : 35,
      "context" : "3 Modified Bregman ADMM for Computing Wasserstein Barycenter The recent modified Bregman alternating direction method of multiplier (B-ADMM) algorithm (Ye et al., 2017), motivated by (Wang and Banerjee, 2014), is a practical choice for computing Wasserstein barycenters.",
      "startOffset" : 151,
      "endOffset" : 168
    }, {
      "referenceID" : 30,
      "context" : ", 2017), motivated by (Wang and Banerjee, 2014), is a practical choice for computing Wasserstein barycenters.",
      "startOffset" : 22,
      "endOffset" : 47
    }, {
      "referenceID" : 35,
      "context" : "The software package detailed in (Ye et al., 2017) was used to generate relevant experiments.",
      "startOffset" : 33,
      "endOffset" : 50
    }, {
      "referenceID" : 31,
      "context" : "(2) Wiki events: each cluster/class contains a set of news abstracts on the same story such as “2014 Crimean Crisis” crawled from Wikipedia current events following (Wu et al., 2015); this dataset offers more challenges because it has more finegrained classes and fewer documents (with shorter length) per class than the others have.",
      "startOffset" : 165,
      "endOffset" : 182
    }, {
      "referenceID" : 22,
      "context" : "We use the following three sets of quantitative metrics to assess the quality of clusters by knowing the groundtruth categorical labels of documents: (1) Homogeneity, Completeness, and V-measure (Rosenberg and Hirschberg, 2007); (2) Adjusted Mutual Information (AMI) (Vinh et al.",
      "startOffset" : 195,
      "endOffset" : 227
    }, {
      "referenceID" : 27,
      "context" : "We use the following three sets of quantitative metrics to assess the quality of clusters by knowing the groundtruth categorical labels of documents: (1) Homogeneity, Completeness, and V-measure (Rosenberg and Hirschberg, 2007); (2) Adjusted Mutual Information (AMI) (Vinh et al., 2010); (3) Adjusted Rand Index (ARI) (Rand, 1971).",
      "startOffset" : 267,
      "endOffset" : 286
    }, {
      "referenceID" : 21,
      "context" : ", 2010); (3) Adjusted Rand Index (ARI) (Rand, 1971).",
      "startOffset" : 39,
      "endOffset" : 51
    }, {
      "referenceID" : 22,
      "context" : "For sensitivity analysis, we use the homogeneity score (Rosenberg and Hirschberg, 2007) as a projection dimension of other metrics, creating a 2D plot to visualize the metrics of a method along different homogeneity levels.",
      "startOffset" : 55,
      "endOffset" : 87
    }, {
      "referenceID" : 25,
      "context" : "To diminish the randomness brought by K-mean initialization, we ensemble the clustering results of 50 repeated runs (Strehl and Ghosh, 2003), and report the metrics for the ensembled one.",
      "startOffset" : 116,
      "endOffset" : 140
    }, {
      "referenceID" : 7,
      "context" : "They are (3) Spectral Clustering (Laplacian) (4) Latent Semantic Indexing (LSI) (Deerwester et al., 1990).",
      "startOffset" : 80,
      "endOffset" : 105
    }, {
      "referenceID" : 9,
      "context" : "(5) Locality Preserving Projection (LPP) (He and Niyogi, 2004; Cai et al., 2005).",
      "startOffset" : 41,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "(5) Locality Preserving Projection (LPP) (He and Niyogi, 2004; Cai et al., 2005).",
      "startOffset" : 41,
      "endOffset" : 80
    }, {
      "referenceID" : 13,
      "context" : "(6) Nonnegative Matrix Factorization (NMF) (Lee and Seung, 1999; Xu et al., 2003).",
      "startOffset" : 43,
      "endOffset" : 81
    }, {
      "referenceID" : 32,
      "context" : "(6) Nonnegative Matrix Factorization (NMF) (Lee and Seung, 1999; Xu et al., 2003).",
      "startOffset" : 43,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "(7) Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Hoffman et al., 2010; Pritchard et al., 2000).",
      "startOffset" : 38,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "(7) Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Hoffman et al., 2010; Pritchard et al., 2000).",
      "startOffset" : 38,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "(7) Latent Dirichlet Allocation (LDA) (Blei et al., 2003; Hoffman et al., 2010; Pritchard et al., 2000).",
      "startOffset" : 38,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "(8) Average of word vectors (AvgDoc) (9) Paragraph Vectors (PV) (Le and Mikolov, 2014).",
      "startOffset" : 64,
      "endOffset" : 86
    }, {
      "referenceID" : 8,
      "context" : "Our approach adopts the Elkan’s algorithm pruning unnecessary computations of Wasserstein distance in assignment steps of K-means (Elkan, 2003).",
      "startOffset" : 130,
      "endOffset" : 143
    }, {
      "referenceID" : 11,
      "context" : "The KNN graph based on 1st order Wasserstein distance is computed from the prefetch-and-prune approach according to (Kusner et al., 2015).",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "K nearest neighbor graph with the prefetch-andprune method of (Kusner et al., 2015) needs substantially more pairs to compute Wasserstein distance, meanwhile the speed-ups also suffer from the curse of dimensionality.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 27,
      "context" : "Table 3: Best AMIs (Vinh et al., 2010) of compared methods on different datasets and their averaging.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 37,
      "context" : "An extra evaluation index — categorical cluster distance (Zhou et al., 2005) is also used.",
      "startOffset" : 57,
      "endOffset" : 76
    } ],
    "year" : 2017,
    "abstractText" : "Word embeddings have become widelyused in document analysis. A large number of models have been proposed, but the net gain these models can achieve expectably beyond the traditional bag-ofwords based approaches remains undetermined. Our empirical studies, conducted from a nonparametric unsupervised perspective, reveal where and how word embeddings can contribute to document analysis. Our approach is based on a recent algorithmic advance in nonparametric clustering for empirical measures, which neither invents nor relies on any document vector representations. The new document clustering approach proposed in this work is easy to use and stably outperforms other existing methodologies on a variety of document-clustering tasks.",
    "creator" : "Preview"
  }
}