{
  "name" : "270.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Enhanced LSTM for Natural Language Inference",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n010\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n060\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Reasoning and inference are central to both human and artificial intelligence. Modeling inference in human language is notoriously challenging but is a basic problem towards true natural language understanding, as pointed out by MacCartney and Manning (2008), “a necessary (if not sufficient) condition for true natural language understanding is a mastery of open-domain natural language inference.” The efforts have also included a large bulk of work on recognizing textual entailment.\nSpecifically, natural language inference (NLI) is concerned with determining whether a naturallanguage hypothesis h can be inferred from a\npremise p, as depicted in the following example from MacCartney (2009), where the hypothesis is regarded to be entailed from the premise.\np: Several airlines polled saw costs grow more than expected, even after adjusting for inflation.\nh: Some of the companies in the poll reported cost increases.\nThe most recent years have seen advances in modeling natural language inference. An important contribution is the creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). The corpus has 570,000 human-written English sentence pairs manually labeled by multiple human subjects. This makes it feasible to train more complex inference models. Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (Bowman et al., 2015, 2016; Munkhdalai and Yu, 2016b; Parikh et al., 2016; Sha et al., 2016; Paria et al., 2016).\nWhile some previous top-performing models use rather complicated network architectures to achieve the state-of-the-art results (Munkhdalai and Yu, 2016b), we demonstrate in this paper that enhancing sequential inference models based on chain models can outperform all previous results, suggesting that the potential of such sequential inference approaches have not been fully exploited yet. Our model may serve as a new baseline or starting point for deploying more complicated models for NLI. More specifically, we show that our sequential inference model achieves an accuracy of 88.0% on the SNLI benchmark.\nExploring syntax for NLI is very attractive to us. In many problems, syntax and semantics interact closely, as generally phrased in the slogan “the syntax and the semantics work together in tandem” (Barker and Jacobson, 2007), among others. Complicated tasks such as natural language\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ninference could well involve both, which has been discussed in the context of recognizing textual entailment (RTE) (Mehdad et al., 2010; Ferrone and Zanzotto, 2014). In this paper, we are interested in exploring this within the neural network frameworks, with the presence of relatively large training data. We show that by explicitly encoding parsing information with recursive networks in both local inference modeling and inference composition, and incorporating it into our framework, we achieve additional improvement, increasing the performance to a new state of the art with a 88.6% accuracy."
    }, {
      "heading" : "2 Related Work",
      "text" : "Early work on natural language inference has been performed on rather small datasets with more conventional methods (refer to MacCartney (2009) for a good literature survey), which includes a large bulk of early work on recognizing textual entailment, such as (Dagan et al., 2005; Iftene and Balahur-Dobrescu, 2007), among others. More recently, Bowman et al. (2015) made available the SNLI dataset with the 570,000 human annotated sentence pairs. They also experimented with simple classification models as well as simple neural networks that encode the premise and hypothesis independently. Rocktäschel et al. (2015) proposed neural attention-based models for NLI, which captured the attention information. In general, attention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al., 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others. For NLI, the idea allows neural models to pay attention to specific areas of the sentences.\nA variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016). Sha et al. (2016) proposes a special LSTM variant which considers the attention vector of another sentence as an inner state of LSTM. Paria et al. (2016) use a neural architecture with a complete binary tree-LSTM encoders while without syntactic information. Among them, more relevant to our work are the approaches proposed\nby Parikh et al. (2016) and Munkhdalai and Yu (2016b), which are among the best models.\nParikh et al. (2016) propose a relatively simple but very effective decomposable model. The model decomposes the NLI problem into subproblems that can be solved separately. On the other hand, Munkhdalai and Yu (2016b) propose much more complicated networks that consider sequential LSTM-based encoding, recursive networks, and complicated combinations of attention models, which provide about 0.5% gain over the results reported by Parikh et al. (2016).\nIt is, however, not very clear if the potential of the sequential inference networks has been well exploited for NLI. In this paper, we first revisit this problem and show that enhancing sequential inference models based on chain networks can actually outperform all previous results. Our model may serve as a new baseline or starting point for deploying more complicated models for NLI. We further show that explicitly considering recursive architectures to encode syntactic parsing information for NLI could further improve the performance."
    }, {
      "heading" : "3 Hybrid Neural Inference Models",
      "text" : "We present here our natural language inference networks which are composed of the following major components: input encoding, local inference modeling, and inference composition. Figure 1 shows a high-level view of the architecture. Vertically, the figure depicts the three major components, and horizontally, the left side of the figure represents our sequential NLI model named ESIM, and the right side represents networks that incorporate syntactic parsing information in tree LSTMs.\nIn our notation, we have two sentences a = (a1, . . . ,a`a) and b = (b1, . . . ,b`b), where a is a premise and b a hypothesis. The ai or bj ∈ Rl is an embedding of l-dimensional vector, which can be initialized with some pre-trained word embeddings and organized with parse trees. The goal is to predict a label y that indicates the logic relationship between a and b."
    }, {
      "heading" : "3.1 Input Encoding",
      "text" : "We employ bidirectional LSTM (BiLSTM) as one of our basic building blocks for NLI. We first use it to encode the input premise and hypothesis (Equation (1) and (2)). Here BiLSTM learns to represent a word (e.g., ai) and its context. Later we will also use BiLSTM to perform inference composition to\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nFigure 1: A high-level view of our hybrid neural inference networks.\nconstruct the final prediction, where BiLSTM encodes local inference information and its interaction. To bookkeep the notations for later use, we write as āi the hidden (output) state generated by the BiLSTM at time i over the input sequence a. The same is applied to b̄j :\nāi = BiLSTM(a, i),∀i ∈ [1, . . . , `a], (1) b̄j = BiLSTM(b, j),∀j ∈ [1, . . . , `b]. (2)\nDue to the space limit, we will skip the description of the basic chain LSTM and readers can refer to Hochreiter and Schmidhuber (1997) for details. Briefly, when modeling a sequence, an LSTM employs a set of soft gates together with a memory cell to control message flows, resulting in an effective modeling of tracking long-distance information/dependencies in a sequence.\nA bidirectional LSTM runs a forward and backward LSTM on a sequence starting from the left and the right end, respectively. The hidden states generated by these two LSTMs at each time step are concatenated to represent that time step and its context. Note that we used LSTM memory blocks in our models. We examined other recurrent memory blocks such as GRUs (Gated Recurrent Units) (Cho et al., 2014) and they are inferior to LSTMs on the heldout set for our NLI task.\nAs discussed above, it is intriguing to explore the effectiveness of syntax for natural language inference; for example, whether it is useful even when incorporated into the best-performing models. To this end, we will also encode syntactic parse trees of a premise and hypothesis through treeLSTM (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which extends the chain LSTM to a recursive network (Socher et al., 2011).\nSpecifically, given the parse of a premise or hypothesis, a tree node is deployed with a tree-LSTM memory block depicted as in Figure 2 and computed with Equations (3–10). In short, at each node, an input vector xt and the hidden vectors of its two children (the left child hLt−1 and the right h R t−1) are taken in as the input to calculate the current node’s hidden vector ht.\nWe describe the updating of a node at a high level with Equation (3) to facilitate references later in the paper, and the detailed computation is described in (4–10). Specifically, the input of a node is used to configure four gates: the input gate it, output gate ot, and the two forget gates fLt and f R t . The memory cell ct considers each child’s cell vector, cLt−1 and c R t−1, which are gated by the left forget gate fLt and right forget gate f R t , respectively.\nht = TrLSTM(xt,hLt−1,h R t−1), (3)\nht = ot tanh(ct), (4) ot = σ(Woxt + U L o h L t−1 + U R o h R t−1), (5) ct = f L t cLt−1 + fRt cRt−1 + it ut, (6) fLt = σ(Wfxt + U LL f h L t−1 + U LR f h R t−1), (7) fRt = σ(Wfxt + U RL f h L t−1 + U RR f h R t−1), (8)\nit = σ(Wixt + U L i h L t−1 + U R i h R t−1), (9)\nut = tanh(Wcxt + U L c h L t−1 + U R c h R t−1), (10)\nwhere σ is the sigmoid function, is the elementwise multiplication of two vectors, and all W ∈ Rd×l, U ∈ Rd×d are weight matrices to be learned.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nIn the current input encoding layer, xt is used to encode a word embedding for a leaf node. Since a non-leaf node does not correspond to a specific word, we use a special vector x′t as its input, which is like an unknown word. However, in the inference composition layer that we discuss later, the goal of using tree-LSTM is very different and the input xt will be very different as well—it will encode local inference information and will have values at all tree nodes."
    }, {
      "heading" : "3.2 Local Inference Modeling",
      "text" : "Carefully modeling local subsentential inference between a premise and hypothesis is critical to help determine the overall inference between these two statements.\nTo closely examine local inference, we explore both the sequential and syntactic tree models that have been discussed above. The former helps collect local inference for words and their context, and the tree LSTM helps collect local information between (linguistic) phrases and clauses.\nLocality of inference Modeling local inference needs to employ some forms of hard or soft alignment to associate the relevant subcomponents between a premise and a hypothesis. This includes early methods motivated from the alignment in conventional automatic machine translation (MacCartney, 2009). In neural network models, this is often achieved with soft attention.\nParikh et al. (2016) decomposed this process: the word sequence of the premise (or hypothesis) is regarded as a bag-of-word embedding vector and inter-sentence “alignment\" (or attention) is computed individually to softly align each word to the content of hypothesis (or premise, respectively). While their basic framework is very effective, achieving one of the previous best results, using a pre-trained word embedding by itself does not automatically consider the context around a word in NLI. Parikh et al. (2016) did take into account the word order and context information through an optional distance-sensitive intra-sentence attention.\nIn this paper, we argue for leveraging attention over the bidirectional sequential encoding of the input, as discussed above. We will show that this plays an important role in achieving our best results, and the intra-sentence attention used by (Parikh et al., 2016) actually does not further improve over our model, while the overall framework they proposed is very effective.\nOur soft alignment layer computes the attention weights as the similarity of a hidden state tuple <āi, b̄j> between a premise and a hypothesis with Equation (11). We did study more complicated relationships between āi and b̄j with multilayer perceptrons, but observed no further improvement on the heldout data.\neij = ā T i b̄j (11)\nāi and b̄j are computed above in Equations (1) and (2), or with Equation (3) when tree-LSTM is used. Again, as discussed above, we will use bidirectional LSTM and tree-LSTM to encode the premise and hypothesis, respectively. In our sequential inference model, unlike in Parikh et al. (2016) which proposed to use a function F (āi), i.e., a feed-forward neural network, to map the original word representation for calculating eij , we instead advocate to use BiLSTM, which encodes the information in premise and hypothesis very well and achieves better performance shown in the experiment section; we tried to apply the F (.) function on our hidden states before computing eij and it did not further help our models. Local inference collected over sequences Local inference is determined by the attention weight eij computed above, which is used to obtain the local relevance between a premise and hypothesis. For the hidden state of a word in a premise, i.e., āi (already encoding the word itself and its context), the relevant semantics in the hypothesis is identified and composed using eij , more specifically with Equation (12).\nãi = `b∑ j=1 exp(eij)∑`b k=1 exp(eik) b̄j ,∀i ∈ [1, . . . , `a], (12)\nb̃j = `a∑ i=1 exp(eij)∑`a k=1 exp(ekj) āi,∀j ∈ [1, . . . , `b], (13)\nwhere ãi is a weighted summation of {b̄j}`bj=1. Intuitively, the content in {b̄j}`bj=1 that is relevant to āi will be selected and represented as ãi. The same is performed for each word in the hypothesis with Equation (13). Local inference collected over parse trees We use tree models to help collect local inference information over linguistic phrases and clauses in this layer. The tree structures of the premise and hypothesis are produced by a constituency parser.\nOnce the hidden states of a tree are all computed with Equation (3), we treat all tree nodes equally\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nas we do not have further heuristics to discriminate them, but leave the attention weights to figure out their relationship. So, we use Equation (11) to compute the attention weights for all node pairs between a premise and hypothesis. This connects all words, constituent phrases, and clauses between the premise and hypothesis. We then collect the information between all the pairs with Equations (12) and (13) and feed them into the next layer. Enhancement of local inference information In our models, we further enhance the local inference information collected. We compute the difference and the element-wise product for the tuple <ā, ã> as well as for <b̄, b̃>. We expect that such operations could help sharpen local inference information between elements in the tuples and capture inference relationships such as contradiction. The difference and element-wise product are then concatenated with the original vectors, ā and ã, or b̄ and b̃, respectively. The enhancement is performed for both the sequential and the tree models.\nma = [ā; ã; ā− ã; ā ã], (14) mb = [b̄; b̃; b̄− b̃; b̄ b̃]. (15)\nThis process could be regarded as a special case of modeling some high-order interaction between the tuple elements. Along this direction, we have also further modeled the interaction by feeding the tuples into feed-forward networks and added the top layer hidden states to the above concatenation. We found that it does not further help the inference accuracy on the heldout dataset."
    }, {
      "heading" : "3.3 Inference Composition",
      "text" : "To determine the overall inference relationship between a premise and hypothesis, we explore a composition layer to compose the enhanced local inference information ma and mb. We perform the composition sequentially or in its parse context using BiLSTM and tree-LSTM, respectively. The composition layer In our sequential inference model, we keep using BiLSTM to compose local inference information sequentially. The formulas for BiLSTM are similar to those in Equations (1) and (2) in their forms so we skip the details, but the aim is very different here—they are used to capture local inference information ma and mb and their context here for inference composition.\nIn the tree composition, the high-level formulas of how a tree node is updated to compose local inference is as follows:\nva,t = TrLSTM(F (ma,t),hLt−1,h R t−1), (16) vb,t = TrLSTM(F (mb,t),hLt−1,h R t−1). (17)\nWe propose to control model complexity in this layer, since the concatenation we described above to compute ma and mb can significantly increase the overall parameter size to potentially overfit the models. We propose to use a mapping F as in Equation (16) and (17). More specifically, we use a 1-layer feed-forward neural network with the ReLU activation. This function is also applied to BiLSTM in our sequential inference composition.\nPooling Our inference model converts the resulting vectors obtained above to a fixed-length vector with pooling and feeds it to the final classifier to determine the overall inference relationship.\nWe consider that summation (Parikh et al., 2016) could be sensitive to the sequence length and hence less robust. We instead suggest the following strategy: compute both average and max pooling, and concatenate all these vectors to form the final fixed length vector v. Our experiments show that this leads to significantly better results than summation. The final fixed length vector v is calculated as follows:\nva,ave = `a∑ i=1 va,i `a , va,max = `a max i=1 va,i, (18)\nvb,ave = `b∑ j=1 vb,j `b , vb,max = `b max j=1 vb,j , (19)\nv = [va,ave;va,max;vb,ave;vb,max]. (20)\nNote that for tree composition, Equation (20) is slightly different from that in sequential composition. Our tree composition will concatenate also the hidden states computed for the roots with Equations (16) and (17), which are not shown here.\nWe then put v into a final multilayer perceptron (MLP) classifier. The MLP has a hidden layer with tanh activation and softmax output layer in our experiments. The entire model (all three components described above) is trained end-to-end. For training, we use multi-class cross-entropy loss.\nOverall inference models Our model can be based only on the sequential networks by removing all tree components and we call it Enhanced Sequential Inference Model (ESIM) (see the left part of Figure 1). We will show that ESIM outperforms all previous results. We will also encode parse information with tree LSTMs in multiple layers as described (see the right side of Figure 1). We train\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nthis model and incorporate it into ESIM by averaging the predicted probabilities to get the final label for a premise-hypothesis pair. We will show that parsing information complements very well with ESIM and further improves the performance, and we call the final model Hybrid Inference Model (HIM)."
    }, {
      "heading" : "4 Experimental Setup",
      "text" : "Data The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) focuses on three basic relationships between a premise and a potential hypothesis: the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral). The original SNLI corpus contains also “the other” category, which includes the sentence pairs lacking consensus among multiple human annotators. As in the related work, we remove this category. We used the same split as in Bowman et al. (2015) and other previous work.\nThe parse trees used in this paper are produced by the Stanford PCFG Parser 3.5.3 (Klein and D. Manning, 2003) and they are delivered as part of the SNLI corpus. We use classification accuracy as the evaluation metric, as in related work.\nTraining We use the development set to select models for testing. To help replicate our results, we publish all our code at [xxx]. Below, we list our training details. We use the Adam method (Kingma and Ba, 2014) for optimization. The first momentum is set to be 0.9 and the second 0.999. The initial learning rate is 0.0004 and the batch size is 32. All hidden states of LSTMs, tree-LSTMs, and word embeddings have 300 dimensions.\nWe use dropout with a rate of 0.5, which is applied to all feed-forward connections. We use pre-trained 300-D Glove 840B vectors (Pennington et al., 2014) to initialize our word embeddings. Out-of-vocabulary (OOV) words are initialized randomly with Gaussian samples. All vectors including word embedding are updated during training."
    }, {
      "heading" : "5 Results",
      "text" : "Overall performance Table 1 shows the results of different models. The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc.\nThe next group of models (2)-(7) are based on sentence encoding. The model of Bowman et al. (2016) encodes the premise and hypothesis with two different LSTMs. The model in Vendrov et al. (2015) uses unsupervised ’skip-thoughts’ pre-training in GRU encoders. The approach proposed by Mou et al. (2016) considers tree-based CNN to capture sentence-level semantics, while the model of Bowman et al. (2016) introduces a stack-augmented parser-interpreter neural network (SPINN) which combines parsing and interpretation within a single tree-sequence hybrid model. The work by Liu et al. (2016) uses BiLSTM to generate sentence representations, and then replaces average pooling with intra-attention. The approach proposed by Munkhdalai and Yu (2016a) presents a memory augmented neural network, neural semantic encoders (NSE), to encode sentences.\nThe next group of methods in the table, models (8)-(15), are inter-sentence attention-based model. The model marked with Rocktäschel et al. (2015) is LSTMs enforcing the so called word-by-word attention. The model of Wang and Jiang (2016) extends this idea to explicitly enforce word-by-word matching between the hypothesis and the premise. Long short-term memory-networks (LSTMN) with deep attention fusion (Cheng et al., 2016) link the current word to previous words stored in memory. Parikh et al. (2016) proposed a decomposable attention model without relying on any word-order information. In general, adding intra-sentence attention yields further improvement, which is not very surprising as it could help align the relevant text spans between premise and hypothesis. The model of Munkhdalai and Yu (2016b) extends the framework of Wang and Jiang (2016) to a full n-ary tree model and achieves further improvement. Sha et al. (2016) proposes a special LSTM variant which considers the attention vector of another sentence as an inner state of LSTM. Paria et al. (2016) use a neural architecture with a complete binary tree-LSTM encoders without syntactic information.\nThe table shows that our ESIM model achieves an accuracy of 88.0%, which has already outperformed all the previous models, including those using much more complicated network architectures (Munkhdalai and Yu, 2016b).\nWe ensemble our ESIM model with syntactic tree-LSTMs (Zhu et al., 2015) based on syntactic parse trees and achieve significant improvement over our best sequential encoding model ESIM, at-\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nModel #Para. Train Test\n(1) Handcrafted features (Bowman et al., 2015) - 99.7 78.2\n(2) 300D LSTM encoders (Bowman et al., 2016) 3.0M 83.9 80.6 (3) 1024D pretrained GRU encoders (Vendrov et al., 2015) 15M 98.8 81.4 (4) 300D tree-based CNN encoders (Mou et al., 2016) 3.5M 83.3 82.1 (5) 300D SPINN-PI encoders (Bowman et al., 2016) 3.7M 89.2 83.2 (6) 600D BiLSTM intra-attention encoders (Liu et al., 2016) 2.8M 84.5 84.2 (7) 300D NSE encoders (Munkhdalai and Yu, 2016a) 3.0M 86.2 84.6\n(8) 100D LSTM with attention (Rocktäschel et al., 2015) 250K 85.3 83.5 (9) 300D mLSTM (Wang and Jiang, 2016) 1.9M 92.0 86.1 (10) 450D LSTMN with deep attention fusion (Cheng et al., 2016) 3.4M 88.5 86.3 (11) 200D decomposable attention model (Parikh et al., 2016) 380K 89.5 86.3 (12) Intra-sentence attention + (11) (Parikh et al., 2016) 580K 90.5 86.8 (13) 300D NTI-SLSTM-LSTM (Munkhdalai and Yu, 2016b) 3.2M 88.5 87.3 (14) 300D re-read LSTM (Sha et al., 2016) 2.0M 90.7 87.5 (15) 300D btree-LSTM encoders (Paria et al., 2016) 2.0M 88.6 87.6\n(16) 600D ESIM 4.3M 92.6 88.0 (17) HIM (600D ESIM + 300D Syntactic tree-LSTM) 7.7M 93.5 88.6\nTable 1: Accuracies of the models on SNLI. Our final model achieves the accuracy of 88.6%, the best result observed on SNLI, while our enhanced sequential encoding model attains an accuracy of 88.0%, which also outperform the previous models.\ntaining an accuracy of 88.6%. This shows that syntactic tree-LSTMs complement well with ESIM.\nModel #Para. Train Test\n(17) HIM (ESIM+syn.tree) 7.7M 93.5 88.6 (18) ESIM+tree 7.7M 91.9 88.2 (16) ESIM 4.3M 92.6 88.0 (19) (16)-ave./max 4.0M 92.9 87.1 (20) (19)-diff./prod. 3.6M 91.6 86.8\nTable 2: Ablation performance of our best models. The difference between each pair of the results is statistically significant (t-test, p < 0.01).\nAblation analysis We further analyze the major components that are of importance to help us achieve good performance. From the best model, we first replace the syntactic tree-LSTM with the full tree-LSTM without encoding syntactic parse information. More specifically, two adjacent words in a sentence are merged to form a parent node, and this process continues and results in a full binary tree, where padding nodes are inserted when there are no enough leaves to form a full tree. Each tree node is implemented with a tree-LSTM block (Zhu et al., 2015) same as in model (17). Table 2 shows that with this replacement, the performance drops to 88.2%.\nFurthermore, we note the importance of the layer\nperforming the enhancement for local inference information in Section 3.2 and the pooling layer in inference composition in Section 3.3. Table 2 suggests that the NLI task seems very sensitive to the layers. If we remove the pooling layer in inference composition and replace it with summation as in Parikh et al. (2016), the accuracy drops to 87.1%. We further remove the difference and element-wise product from the local inference enhancement layer, the accuracy drops to 86.8%.\nThe difference between any pairs of the models in Table 2 is statistically significant (t-test, p < 0.01). Note that in Table 1 above we do not have the output from other systems to perform a significance test, but the 0.2% difference is statistically significant among our systems. To provide some detailed comparison with Parikh et al. (2016), replacing bidirectional LSTMs in inference composition and also input encoding with MLP reduces the accuracy to 86.1% and 84.0% respectively.\nFurther analysis We showed that encoding syntactic parsing information helps recognize natural language inference—it additionally improves the strong system. Figure 3 shows an example where tree-LSTM makes a different and correct decision. In subfigure (c), the larger values at the input gates on nodes 9 and 10 indicate that those nodes are important in making the final decision. We observe\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n(a) Binarized constituency tree of premise\n(b) Binarized constituency tree of hypothesis\n(c) Input gate of tree-LSTM in inference composition (l2-norm)\n(d) Input gate of BiLSTM in inference composition (l2-norm)\n(e) Normalized attention weights of tree-LSTM\n(f) Normalized attention weights of BiLSTM\nFigure 3: An example for analysis. Subfigures (a) and (b) are the constituency parse trees of the premise and hypothesis, respectively. ’-’ means a non-leaf or a null node. Subfigures (c) and (d) are input gates’ l2-norm of tree-LSTM and BiLSTM in inference composition, respectively. Subfigures (e) and (f) are attention visualization of the tree model and ESIM, respectively. The darker the color, the greater the value. The premise is on the x-axis and the hypothesis is on y-axis.\nthat in subfigure (e), nodes 9 and 10 are aligned to node 29 in the premise. Such information helps the system decide that this pair is a contradiction. Accordingly, in the sequential BiLSTM, the words sitting and down do not play an important role for making the final decision. Subfigure (f) shows that sitting is equally aligned with reading and standing and the alignment for word down is not that useful."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "We propose neural network models for natural language inference, which achieve the best results reported on the SNLI benchmark. The results are first achieved through our enhanced sequential inference model, which outperformed the previous models, including those employing more complicated network architectures, suggesting that the potential of sequential inference models have not been fully exploited yet. Our model may serve as a new baseline or starting point for deploying\nmore complicated architectures for NLI. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result—it further improves the performance even when added to the already very strong model.\nFuture work interesting to us includes exploring the usefulness of knowledge resources to help alleviate data sparseness issues. We are also interested in studying more the fragments of sentences or parses highlighted by the attention mechanism in order to provide human-readable explanations of the decisions.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "CoRR abs/1409.0473. http://arxiv.org/abs/1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Direct Compositionality",
      "author" : [ "Chris Barker", "Pauline Jacobson." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Barker and Jacobson.,? 2007",
      "shortCiteRegEx" : "Barker and Jacobson.",
      "year" : 2007
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "R. Samuel Bowman", "Gabor Angeli", "Christopher Potts", "D. Christopher Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Associa-",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "A fast unified model for parsing and sentence understanding",
      "author" : [ "R. Samuel Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "D. Christopher Manning", "Christopher Potts." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for",
      "citeRegEx" : "Bowman et al\\.,? 2016",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2016
    }, {
      "title" : "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition",
      "author" : [ "William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals." ],
      "venue" : "2016 IEEE International Conference on Acoustics, Speech and Sig-",
      "citeRegEx" : "Chan et al\\.,? 2016",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2016
    }, {
      "title" : "Distraction-based neural networks for modeling document",
      "author" : [ "Qian Chen", "Xiao-Dan Zhu", "Zhen-Hua Ling", "Si Wei", "Hui Jiang." ],
      "venue" : "Subbarao Kambhampati, editor, Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intel-",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory-networks for machine reading",
      "author" : [ "Jianpeng Cheng", "Li Dong", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages",
      "citeRegEx" : "Cheng et al\\.,? 2016",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2016
    }, {
      "title" : "On the properties of neural machine translation: Encoder-decoder approaches",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio." ],
      "venue" : "Dekai Wu, Marine Carpuat, Xavier Carreras, and Eva Maria Vecchi, editors, Proceed-",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Attention-based models for speech recognition",
      "author" : [ "Jan Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Corinna Cortes, Neil D. Lawrence, Daniel D. Lee, Masashi Sugiyama, and Roman Garnett, ed-",
      "citeRegEx" : "Chorowski et al\\.,? 2015",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2015
    }, {
      "title" : "The pascal recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "MLCW.",
      "citeRegEx" : "Dagan et al\\.,? 2005",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2005
    }, {
      "title" : "Towards syntax-aware compositional distributional semantic models",
      "author" : [ "Lorenzo Ferrone", "Massimo Fabio Zanzotto." ],
      "venue" : "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers. Dublin City Univer-",
      "citeRegEx" : "Ferrone and Zanzotto.,? 2014",
      "shortCiteRegEx" : "Ferrone and Zanzotto.",
      "year" : 2014
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation 9(8):1735–1780. https://doi.org/10.1162/neco.1997.9.8.1735.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, Association for Computational Linguistics, chapter Hypothesis Transformation and Semantic Variability Rules",
      "author" : [ "Adrian Iftene", "Alexandra Balahur-Dobrescu" ],
      "venue" : null,
      "citeRegEx" : "Iftene and Balahur.Dobrescu.,? \\Q2007\\E",
      "shortCiteRegEx" : "Iftene and Balahur.Dobrescu.",
      "year" : 2007
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Accurate unlexicalized parsing",
      "author" : [ "Dan Klein", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics. http://aclweb.org/anthology/P031054.",
      "citeRegEx" : "Klein and Manning.,? 2003",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2003
    }, {
      "title" : "Compositional distributional semantics with long short term memory",
      "author" : [ "Phong Le", "Willem Zuidema." ],
      "venue" : "Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics, pages 10–19.",
      "citeRegEx" : "Le and Zuidema.,? 2015",
      "shortCiteRegEx" : "Le and Zuidema.",
      "year" : 2015
    }, {
      "title" : "Learning natural language inference using bidirectional LSTM model and inner-attention",
      "author" : [ "Yang Liu", "Chengjie Sun", "Lei Lin", "Xiaolong Wang." ],
      "venue" : "CoRR abs/1605.09090. http://arxiv.org/abs/1605.09090.",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Natural Language Inference",
      "author" : [ "Bill MacCartney." ],
      "venue" : "Ph.D. thesis, Stanford University.",
      "citeRegEx" : "MacCartney.,? 2009",
      "shortCiteRegEx" : "MacCartney.",
      "year" : 2009
    }, {
      "title" : "Modeling semantic containment and exclusion in natural language inference",
      "author" : [ "Bill MacCartney", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1.",
      "citeRegEx" : "MacCartney and Manning.,? 2008",
      "shortCiteRegEx" : "MacCartney and Manning.",
      "year" : 2008
    }, {
      "title" : "Syntactic/semantic structures for textual entailment recognition",
      "author" : [ "Yashar Mehdad", "Alessandro Moschitti", "Massimo Fabio Zanzotto." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the",
      "citeRegEx" : "Mehdad et al\\.,? 2010",
      "shortCiteRegEx" : "Mehdad et al\\.",
      "year" : 2010
    }, {
      "title" : "Natural language inference by tree-based convolution and heuristic matching",
      "author" : [ "Lili Mou", "Rui Men", "Ge Li", "Yan Xu", "Lu Zhang", "Rui Yan", "Zhi Jin." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Mou et al\\.,? 2016",
      "shortCiteRegEx" : "Mou et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural semantic encoders",
      "author" : [ "Tsendsuren Munkhdalai", "Hong Yu." ],
      "venue" : "CoRR abs/1607.04315. http://arxiv.org/abs/1607.04315.",
      "citeRegEx" : "Munkhdalai and Yu.,? 2016a",
      "shortCiteRegEx" : "Munkhdalai and Yu.",
      "year" : 2016
    }, {
      "title" : "Neural tree indexers for text understanding",
      "author" : [ "Tsendsuren Munkhdalai", "Hong Yu." ],
      "venue" : "CoRR abs/1607.04492. http://arxiv.org/abs/1607.04492.",
      "citeRegEx" : "Munkhdalai and Yu.,? 2016b",
      "shortCiteRegEx" : "Munkhdalai and Yu.",
      "year" : 2016
    }, {
      "title" : "A neural architecture mimicking humans end-to-end for natural language inference",
      "author" : [ "Biswajit Paria", "K.M. Annervaz", "Ambedkar Dukkipati", "Ankush Chatterjee", "Sanjay Podder." ],
      "venue" : "CoRR abs/1611.04741. http://arxiv.org/abs/1611.04741.",
      "citeRegEx" : "Paria et al\\.,? 2016",
      "shortCiteRegEx" : "Paria et al\\.",
      "year" : 2016
    }, {
      "title" : "A decomposable attention model for natural language inference",
      "author" : [ "Ankur Parikh", "Oscar Täckström", "Dipanjan Das", "Jakob Uszkoreit." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association",
      "citeRegEx" : "Parikh et al\\.,? 2016",
      "shortCiteRegEx" : "Parikh et al\\.",
      "year" : 2016
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Reasoning about entailment with neural attention",
      "author" : [ "Tim Rocktäschel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomás Kociský", "Phil Blunsom." ],
      "venue" : "CoRR abs/1509.06664. http://arxiv.org/abs/1509.06664.",
      "citeRegEx" : "Rocktäschel et al\\.,? 2015",
      "shortCiteRegEx" : "Rocktäschel et al\\.",
      "year" : 2015
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "M. Alexander Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Associa-",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Reading and thinking: Re-read lstm unit for textual entailment recognition",
      "author" : [ "Lei Sha", "Baobao Chang", "Zhifang Sui", "Sujian Li." ],
      "venue" : "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers.",
      "citeRegEx" : "Sha et al\\.,? 2016",
      "shortCiteRegEx" : "Sha et al\\.",
      "year" : 2016
    }, {
      "title" : "Parsing natural scenes and natural language with recursive neural networks",
      "author" : [ "Richard Socher", "Cliff Chiung-Yu Lin", "Andrew Y. Ng", "Christopher D. Manning." ],
      "venue" : "Lise Getoor and Tobias Scheffer, editors, Proceedings of the 28th International Con-",
      "citeRegEx" : "Socher et al\\.,? 2011",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2011
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks",
      "author" : [ "Sheng Kai Tai", "Richard Socher", "D. Christopher Manning." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Tai et al\\.,? 2015",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "Order-embeddings of images and language",
      "author" : [ "Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun." ],
      "venue" : "CoRR abs/1511.06361. http://arxiv.org/abs/1511.06361.",
      "citeRegEx" : "Vendrov et al\\.,? 2015",
      "shortCiteRegEx" : "Vendrov et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning natural language inference with lstm",
      "author" : [ "Shuohang Wang", "Jing Jiang." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association",
      "citeRegEx" : "Wang and Jiang.,? 2016",
      "shortCiteRegEx" : "Wang and Jiang.",
      "year" : 2016
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory over recursive structures",
      "author" : [ "Xiao-Dan Zhu", "Parinaz Sobhani", "Hongyu Guo." ],
      "venue" : "(Bach and Blei, 2015), pages 1604–1612. http://jmlr.org/proceedings/papers/v37/zhub15.html.",
      "citeRegEx" : "Zhu et al\\.,? 2015",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : "Modeling inference in human language is notoriously challenging but is a basic problem towards true natural language understanding, as pointed out by MacCartney and Manning (2008), “a necessary (if not sufficient) condition for true natural language understanding is a mastery of open-domain natural language inference.",
      "startOffset" : 150,
      "endOffset" : 180
    }, {
      "referenceID" : 17,
      "context" : "Modeling inference in human language is notoriously challenging but is a basic problem towards true natural language understanding, as pointed out by MacCartney and Manning (2008), “a necessary (if not sufficient) condition for true natural language understanding is a mastery of open-domain natural language inference.” The efforts have also included a large bulk of work on recognizing textual entailment. Specifically, natural language inference (NLI) is concerned with determining whether a naturallanguage hypothesis h can be inferred from a premise p, as depicted in the following example from MacCartney (2009), where the hypothesis is regarded to be entailed from the premise.",
      "startOffset" : 150,
      "endOffset" : 618
    }, {
      "referenceID" : 2,
      "context" : "An important contribution is the creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015).",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 22,
      "context" : "Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (Bowman et al., 2015, 2016; Munkhdalai and Yu, 2016b; Parikh et al., 2016; Sha et al., 2016; Paria et al., 2016).",
      "startOffset" : 153,
      "endOffset" : 265
    }, {
      "referenceID" : 24,
      "context" : "Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (Bowman et al., 2015, 2016; Munkhdalai and Yu, 2016b; Parikh et al., 2016; Sha et al., 2016; Paria et al., 2016).",
      "startOffset" : 153,
      "endOffset" : 265
    }, {
      "referenceID" : 28,
      "context" : "Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (Bowman et al., 2015, 2016; Munkhdalai and Yu, 2016b; Parikh et al., 2016; Sha et al., 2016; Paria et al., 2016).",
      "startOffset" : 153,
      "endOffset" : 265
    }, {
      "referenceID" : 23,
      "context" : "Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (Bowman et al., 2015, 2016; Munkhdalai and Yu, 2016b; Parikh et al., 2016; Sha et al., 2016; Paria et al., 2016).",
      "startOffset" : 153,
      "endOffset" : 265
    }, {
      "referenceID" : 22,
      "context" : "While some previous top-performing models use rather complicated network architectures to achieve the state-of-the-art results (Munkhdalai and Yu, 2016b), we demonstrate in this paper that enhancing sequential inference models based on chain models can outperform all previous results, suggesting that the potential of such sequential inference approaches have not been fully exploited yet.",
      "startOffset" : 127,
      "endOffset" : 153
    }, {
      "referenceID" : 1,
      "context" : "In many problems, syntax and semantics interact closely, as generally phrased in the slogan “the syntax and the semantics work together in tandem” (Barker and Jacobson, 2007), among others.",
      "startOffset" : 147,
      "endOffset" : 174
    }, {
      "referenceID" : 19,
      "context" : "inference could well involve both, which has been discussed in the context of recognizing textual entailment (RTE) (Mehdad et al., 2010; Ferrone and Zanzotto, 2014).",
      "startOffset" : 115,
      "endOffset" : 164
    }, {
      "referenceID" : 10,
      "context" : "inference could well involve both, which has been discussed in the context of recognizing textual entailment (RTE) (Mehdad et al., 2010; Ferrone and Zanzotto, 2014).",
      "startOffset" : 115,
      "endOffset" : 164
    }, {
      "referenceID" : 9,
      "context" : "Early work on natural language inference has been performed on rather small datasets with more conventional methods (refer to MacCartney (2009) for a good literature survey), which includes a large bulk of early work on recognizing textual entailment, such as (Dagan et al., 2005; Iftene and Balahur-Dobrescu, 2007), among others.",
      "startOffset" : 260,
      "endOffset" : 315
    }, {
      "referenceID" : 12,
      "context" : "Early work on natural language inference has been performed on rather small datasets with more conventional methods (refer to MacCartney (2009) for a good literature survey), which includes a large bulk of early work on recognizing textual entailment, such as (Dagan et al., 2005; Iftene and Balahur-Dobrescu, 2007), among others.",
      "startOffset" : 260,
      "endOffset" : 315
    }, {
      "referenceID" : 0,
      "context" : "In general, attention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al.",
      "startOffset" : 123,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : ", 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : ", 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 33,
      "context" : ", 2016), image caption (Xu et al., 2015), and text summarization (Rush et al.",
      "startOffset" : 23,
      "endOffset" : 40
    }, {
      "referenceID" : 27,
      "context" : ", 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others.",
      "startOffset" : 32,
      "endOffset" : 70
    }, {
      "referenceID" : 5,
      "context" : ", 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others.",
      "startOffset" : 32,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : "A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al.",
      "startOffset" : 67,
      "endOffset" : 172
    }, {
      "referenceID" : 31,
      "context" : "A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al.",
      "startOffset" : 67,
      "endOffset" : 172
    }, {
      "referenceID" : 20,
      "context" : "A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al.",
      "startOffset" : 67,
      "endOffset" : 172
    }, {
      "referenceID" : 16,
      "context" : "A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al.",
      "startOffset" : 67,
      "endOffset" : 172
    }, {
      "referenceID" : 21,
      "context" : "A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al.",
      "startOffset" : 67,
      "endOffset" : 172
    }, {
      "referenceID" : 26,
      "context" : ", 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016).",
      "startOffset" : 77,
      "endOffset" : 230
    }, {
      "referenceID" : 32,
      "context" : ", 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016).",
      "startOffset" : 77,
      "endOffset" : 230
    }, {
      "referenceID" : 6,
      "context" : ", 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016).",
      "startOffset" : 77,
      "endOffset" : 230
    }, {
      "referenceID" : 24,
      "context" : ", 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016).",
      "startOffset" : 77,
      "endOffset" : 230
    }, {
      "referenceID" : 22,
      "context" : ", 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016).",
      "startOffset" : 77,
      "endOffset" : 230
    }, {
      "referenceID" : 28,
      "context" : ", 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016).",
      "startOffset" : 77,
      "endOffset" : 230
    }, {
      "referenceID" : 23,
      "context" : ", 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016).",
      "startOffset" : 77,
      "endOffset" : 230
    }, {
      "referenceID" : 7,
      "context" : "Early work on natural language inference has been performed on rather small datasets with more conventional methods (refer to MacCartney (2009) for a good literature survey), which includes a large bulk of early work on recognizing textual entailment, such as (Dagan et al.",
      "startOffset" : 126,
      "endOffset" : 144
    }, {
      "referenceID" : 1,
      "context" : "More recently, Bowman et al. (2015) made available the SNLI dataset with the 570,000 human annotated sentence pairs.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "More recently, Bowman et al. (2015) made available the SNLI dataset with the 570,000 human annotated sentence pairs. They also experimented with simple classification models as well as simple neural networks that encode the premise and hypothesis independently. Rocktäschel et al. (2015) proposed neural attention-based models for NLI, which captured the attention information.",
      "startOffset" : 15,
      "endOffset" : 288
    }, {
      "referenceID" : 0,
      "context" : "In general, attention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al., 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others. For NLI, the idea allows neural models to pay attention to specific areas of the sentences. A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016). Sha et al. (2016) proposes a special LSTM variant which considers the attention vector of another sentence as an inner state of LSTM.",
      "startOffset" : 124,
      "endOffset" : 803
    }, {
      "referenceID" : 0,
      "context" : "In general, attention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al., 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others. For NLI, the idea allows neural models to pay attention to specific areas of the sentences. A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016). Sha et al. (2016) proposes a special LSTM variant which considers the attention vector of another sentence as an inner state of LSTM. Paria et al. (2016) use a neural architecture with a complete binary tree-LSTM encoders while without syntactic information.",
      "startOffset" : 124,
      "endOffset" : 939
    }, {
      "referenceID" : 0,
      "context" : "In general, attention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al., 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others. For NLI, the idea allows neural models to pay attention to specific areas of the sentences. A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016). Sha et al. (2016) proposes a special LSTM variant which considers the attention vector of another sentence as an inner state of LSTM. Paria et al. (2016) use a neural architecture with a complete binary tree-LSTM encoders while without syntactic information. Among them, more relevant to our work are the approaches proposed by Parikh et al. (2016) and Munkhdalai and Yu (2016b), which are among the best models.",
      "startOffset" : 124,
      "endOffset" : 1134
    }, {
      "referenceID" : 0,
      "context" : "In general, attention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al., 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others. For NLI, the idea allows neural models to pay attention to specific areas of the sentences. A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016). Sha et al. (2016) proposes a special LSTM variant which considers the attention vector of another sentence as an inner state of LSTM. Paria et al. (2016) use a neural architecture with a complete binary tree-LSTM encoders while without syntactic information. Among them, more relevant to our work are the approaches proposed by Parikh et al. (2016) and Munkhdalai and Yu (2016b), which are among the best models.",
      "startOffset" : 124,
      "endOffset" : 1164
    }, {
      "referenceID" : 0,
      "context" : "In general, attention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al., 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others. For NLI, the idea allows neural models to pay attention to specific areas of the sentences. A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016). Sha et al. (2016) proposes a special LSTM variant which considers the attention vector of another sentence as an inner state of LSTM. Paria et al. (2016) use a neural architecture with a complete binary tree-LSTM encoders while without syntactic information. Among them, more relevant to our work are the approaches proposed by Parikh et al. (2016) and Munkhdalai and Yu (2016b), which are among the best models. Parikh et al. (2016) propose a relatively simple but very effective decomposable model.",
      "startOffset" : 124,
      "endOffset" : 1219
    }, {
      "referenceID" : 0,
      "context" : "In general, attention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al., 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others. For NLI, the idea allows neural models to pay attention to specific areas of the sentences. A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016). Sha et al. (2016) proposes a special LSTM variant which considers the attention vector of another sentence as an inner state of LSTM. Paria et al. (2016) use a neural architecture with a complete binary tree-LSTM encoders while without syntactic information. Among them, more relevant to our work are the approaches proposed by Parikh et al. (2016) and Munkhdalai and Yu (2016b), which are among the best models. Parikh et al. (2016) propose a relatively simple but very effective decomposable model. The model decomposes the NLI problem into subproblems that can be solved separately. On the other hand, Munkhdalai and Yu (2016b) propose much more complicated networks that consider sequential LSTM-based encoding, recursive networks, and complicated combinations of attention models, which provide about 0.",
      "startOffset" : 124,
      "endOffset" : 1416
    }, {
      "referenceID" : 0,
      "context" : "In general, attention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al., 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others. For NLI, the idea allows neural models to pay attention to specific areas of the sentences. A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a), and inter-sentence attention-based models (Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016). Sha et al. (2016) proposes a special LSTM variant which considers the attention vector of another sentence as an inner state of LSTM. Paria et al. (2016) use a neural architecture with a complete binary tree-LSTM encoders while without syntactic information. Among them, more relevant to our work are the approaches proposed by Parikh et al. (2016) and Munkhdalai and Yu (2016b), which are among the best models. Parikh et al. (2016) propose a relatively simple but very effective decomposable model. The model decomposes the NLI problem into subproblems that can be solved separately. On the other hand, Munkhdalai and Yu (2016b) propose much more complicated networks that consider sequential LSTM-based encoding, recursive networks, and complicated combinations of attention models, which provide about 0.5% gain over the results reported by Parikh et al. (2016). It is, however, not very clear if the potential of the sequential inference networks has been well exploited for NLI.",
      "startOffset" : 124,
      "endOffset" : 1651
    }, {
      "referenceID" : 7,
      "context" : "We examined other recurrent memory blocks such as GRUs (Gated Recurrent Units) (Cho et al., 2014) and they are inferior to LSTMs on the heldout set for our NLI task.",
      "startOffset" : 79,
      "endOffset" : 97
    }, {
      "referenceID" : 34,
      "context" : "To this end, we will also encode syntactic parse trees of a premise and hypothesis through treeLSTM (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which extends the chain LSTM to a recursive network (Socher et al.",
      "startOffset" : 100,
      "endOffset" : 158
    }, {
      "referenceID" : 30,
      "context" : "To this end, we will also encode syntactic parse trees of a premise and hypothesis through treeLSTM (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which extends the chain LSTM to a recursive network (Socher et al.",
      "startOffset" : 100,
      "endOffset" : 158
    }, {
      "referenceID" : 15,
      "context" : "To this end, we will also encode syntactic parse trees of a premise and hypothesis through treeLSTM (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which extends the chain LSTM to a recursive network (Socher et al.",
      "startOffset" : 100,
      "endOffset" : 158
    }, {
      "referenceID" : 29,
      "context" : ", 2015; Le and Zuidema, 2015), which extends the chain LSTM to a recursive network (Socher et al., 2011).",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 10,
      "context" : "(2) Due to the space limit, we will skip the description of the basic chain LSTM and readers can refer to Hochreiter and Schmidhuber (1997) for details.",
      "startOffset" : 106,
      "endOffset" : 140
    }, {
      "referenceID" : 17,
      "context" : "This includes early methods motivated from the alignment in conventional automatic machine translation (MacCartney, 2009).",
      "startOffset" : 103,
      "endOffset" : 121
    }, {
      "referenceID" : 24,
      "context" : "We will show that this plays an important role in achieving our best results, and the intra-sentence attention used by (Parikh et al., 2016) actually does not further improve over our model, while the overall framework they proposed is very effective.",
      "startOffset" : 119,
      "endOffset" : 140
    }, {
      "referenceID" : 17,
      "context" : "This includes early methods motivated from the alignment in conventional automatic machine translation (MacCartney, 2009). In neural network models, this is often achieved with soft attention. Parikh et al. (2016) decomposed this process: the word sequence of the premise (or hypothesis) is regarded as a bag-of-word embedding vector and inter-sentence “alignment\" (or attention) is computed individually to softly align each word to the content of hypothesis (or premise, respectively).",
      "startOffset" : 104,
      "endOffset" : 214
    }, {
      "referenceID" : 17,
      "context" : "This includes early methods motivated from the alignment in conventional automatic machine translation (MacCartney, 2009). In neural network models, this is often achieved with soft attention. Parikh et al. (2016) decomposed this process: the word sequence of the premise (or hypothesis) is regarded as a bag-of-word embedding vector and inter-sentence “alignment\" (or attention) is computed individually to softly align each word to the content of hypothesis (or premise, respectively). While their basic framework is very effective, achieving one of the previous best results, using a pre-trained word embedding by itself does not automatically consider the context around a word in NLI. Parikh et al. (2016) did take into account the word order and context information through an optional distance-sensitive intra-sentence attention.",
      "startOffset" : 104,
      "endOffset" : 711
    }, {
      "referenceID" : 24,
      "context" : "In our sequential inference model, unlike in Parikh et al. (2016) which proposed to use a function F (āi), i.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 24,
      "context" : "We consider that summation (Parikh et al., 2016) could be sensitive to the sequence length and hence less robust.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "4 Experimental Setup Data The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) focuses on three basic relationships between a premise and a potential hypothesis: the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral).",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : "We use the Adam method (Kingma and Ba, 2014) for optimization.",
      "startOffset" : 23,
      "endOffset" : 44
    }, {
      "referenceID" : 25,
      "context" : "We use pre-trained 300-D Glove 840B vectors (Pennington et al., 2014) to initialize our word embeddings.",
      "startOffset" : 44,
      "endOffset" : 69
    }, {
      "referenceID" : 2,
      "context" : "4 Experimental Setup Data The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) focuses on three basic relationships between a premise and a potential hypothesis: the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral). The original SNLI corpus contains also “the other” category, which includes the sentence pairs lacking consensus among multiple human annotators. As in the related work, we remove this category. We used the same split as in Bowman et al. (2015) and other previous work.",
      "startOffset" : 81,
      "endOffset" : 558
    }, {
      "referenceID" : 6,
      "context" : "Long short-term memory-networks (LSTMN) with deep attention fusion (Cheng et al., 2016) link the current word to previous words stored in memory.",
      "startOffset" : 67,
      "endOffset" : 87
    }, {
      "referenceID" : 22,
      "context" : "0%, which has already outperformed all the previous models, including those using much more complicated network architectures (Munkhdalai and Yu, 2016b).",
      "startOffset" : 126,
      "endOffset" : 152
    }, {
      "referenceID" : 34,
      "context" : "We ensemble our ESIM model with syntactic tree-LSTMs (Zhu et al., 2015) based on syntactic parse trees and achieve significant improvement over our best sequential encoding model ESIM, at-",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 2,
      "context" : "The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc. The next group of models (2)-(7) are based on sentence encoding. The model of Bowman et al. (2016) encodes the premise and hypothesis with two different LSTMs.",
      "startOffset" : 52,
      "endOffset" : 341
    }, {
      "referenceID" : 2,
      "context" : "The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc. The next group of models (2)-(7) are based on sentence encoding. The model of Bowman et al. (2016) encodes the premise and hypothesis with two different LSTMs. The model in Vendrov et al. (2015) uses unsupervised ’skip-thoughts’ pre-training in GRU encoders.",
      "startOffset" : 52,
      "endOffset" : 437
    }, {
      "referenceID" : 2,
      "context" : "The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc. The next group of models (2)-(7) are based on sentence encoding. The model of Bowman et al. (2016) encodes the premise and hypothesis with two different LSTMs. The model in Vendrov et al. (2015) uses unsupervised ’skip-thoughts’ pre-training in GRU encoders. The approach proposed by Mou et al. (2016) considers tree-based CNN to capture sentence-level semantics, while the model of Bowman et al.",
      "startOffset" : 52,
      "endOffset" : 544
    }, {
      "referenceID" : 2,
      "context" : "The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc. The next group of models (2)-(7) are based on sentence encoding. The model of Bowman et al. (2016) encodes the premise and hypothesis with two different LSTMs. The model in Vendrov et al. (2015) uses unsupervised ’skip-thoughts’ pre-training in GRU encoders. The approach proposed by Mou et al. (2016) considers tree-based CNN to capture sentence-level semantics, while the model of Bowman et al. (2016) introduces a stack-augmented parser-interpreter neural network (SPINN) which combines parsing and interpretation within a single tree-sequence hybrid model.",
      "startOffset" : 52,
      "endOffset" : 646
    }, {
      "referenceID" : 2,
      "context" : "The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc. The next group of models (2)-(7) are based on sentence encoding. The model of Bowman et al. (2016) encodes the premise and hypothesis with two different LSTMs. The model in Vendrov et al. (2015) uses unsupervised ’skip-thoughts’ pre-training in GRU encoders. The approach proposed by Mou et al. (2016) considers tree-based CNN to capture sentence-level semantics, while the model of Bowman et al. (2016) introduces a stack-augmented parser-interpreter neural network (SPINN) which combines parsing and interpretation within a single tree-sequence hybrid model. The work by Liu et al. (2016) uses BiLSTM to generate sentence representations, and then replaces average pooling with intra-attention.",
      "startOffset" : 52,
      "endOffset" : 833
    }, {
      "referenceID" : 2,
      "context" : "The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc. The next group of models (2)-(7) are based on sentence encoding. The model of Bowman et al. (2016) encodes the premise and hypothesis with two different LSTMs. The model in Vendrov et al. (2015) uses unsupervised ’skip-thoughts’ pre-training in GRU encoders. The approach proposed by Mou et al. (2016) considers tree-based CNN to capture sentence-level semantics, while the model of Bowman et al. (2016) introduces a stack-augmented parser-interpreter neural network (SPINN) which combines parsing and interpretation within a single tree-sequence hybrid model. The work by Liu et al. (2016) uses BiLSTM to generate sentence representations, and then replaces average pooling with intra-attention. The approach proposed by Munkhdalai and Yu (2016a) presents a memory augmented neural network, neural semantic encoders (NSE), to encode sentences.",
      "startOffset" : 52,
      "endOffset" : 990
    }, {
      "referenceID" : 2,
      "context" : "The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc. The next group of models (2)-(7) are based on sentence encoding. The model of Bowman et al. (2016) encodes the premise and hypothesis with two different LSTMs. The model in Vendrov et al. (2015) uses unsupervised ’skip-thoughts’ pre-training in GRU encoders. The approach proposed by Mou et al. (2016) considers tree-based CNN to capture sentence-level semantics, while the model of Bowman et al. (2016) introduces a stack-augmented parser-interpreter neural network (SPINN) which combines parsing and interpretation within a single tree-sequence hybrid model. The work by Liu et al. (2016) uses BiLSTM to generate sentence representations, and then replaces average pooling with intra-attention. The approach proposed by Munkhdalai and Yu (2016a) presents a memory augmented neural network, neural semantic encoders (NSE), to encode sentences. The next group of methods in the table, models (8)-(15), are inter-sentence attention-based model. The model marked with Rocktäschel et al. (2015) is LSTMs enforcing the so called word-by-word attention.",
      "startOffset" : 52,
      "endOffset" : 1234
    }, {
      "referenceID" : 2,
      "context" : "The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc. The next group of models (2)-(7) are based on sentence encoding. The model of Bowman et al. (2016) encodes the premise and hypothesis with two different LSTMs. The model in Vendrov et al. (2015) uses unsupervised ’skip-thoughts’ pre-training in GRU encoders. The approach proposed by Mou et al. (2016) considers tree-based CNN to capture sentence-level semantics, while the model of Bowman et al. (2016) introduces a stack-augmented parser-interpreter neural network (SPINN) which combines parsing and interpretation within a single tree-sequence hybrid model. The work by Liu et al. (2016) uses BiLSTM to generate sentence representations, and then replaces average pooling with intra-attention. The approach proposed by Munkhdalai and Yu (2016a) presents a memory augmented neural network, neural semantic encoders (NSE), to encode sentences. The next group of methods in the table, models (8)-(15), are inter-sentence attention-based model. The model marked with Rocktäschel et al. (2015) is LSTMs enforcing the so called word-by-word attention. The model of Wang and Jiang (2016) extends this idea to explicitly enforce word-by-word matching between the hypothesis and the premise.",
      "startOffset" : 52,
      "endOffset" : 1326
    }, {
      "referenceID" : 2,
      "context" : "The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc. The next group of models (2)-(7) are based on sentence encoding. The model of Bowman et al. (2016) encodes the premise and hypothesis with two different LSTMs. The model in Vendrov et al. (2015) uses unsupervised ’skip-thoughts’ pre-training in GRU encoders. The approach proposed by Mou et al. (2016) considers tree-based CNN to capture sentence-level semantics, while the model of Bowman et al. (2016) introduces a stack-augmented parser-interpreter neural network (SPINN) which combines parsing and interpretation within a single tree-sequence hybrid model. The work by Liu et al. (2016) uses BiLSTM to generate sentence representations, and then replaces average pooling with intra-attention. The approach proposed by Munkhdalai and Yu (2016a) presents a memory augmented neural network, neural semantic encoders (NSE), to encode sentences. The next group of methods in the table, models (8)-(15), are inter-sentence attention-based model. The model marked with Rocktäschel et al. (2015) is LSTMs enforcing the so called word-by-word attention. The model of Wang and Jiang (2016) extends this idea to explicitly enforce word-by-word matching between the hypothesis and the premise. Long short-term memory-networks (LSTMN) with deep attention fusion (Cheng et al., 2016) link the current word to previous words stored in memory. Parikh et al. (2016) proposed a decomposable attention model without relying on any word-order information.",
      "startOffset" : 52,
      "endOffset" : 1595
    }, {
      "referenceID" : 2,
      "context" : "The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc. The next group of models (2)-(7) are based on sentence encoding. The model of Bowman et al. (2016) encodes the premise and hypothesis with two different LSTMs. The model in Vendrov et al. (2015) uses unsupervised ’skip-thoughts’ pre-training in GRU encoders. The approach proposed by Mou et al. (2016) considers tree-based CNN to capture sentence-level semantics, while the model of Bowman et al. (2016) introduces a stack-augmented parser-interpreter neural network (SPINN) which combines parsing and interpretation within a single tree-sequence hybrid model. The work by Liu et al. (2016) uses BiLSTM to generate sentence representations, and then replaces average pooling with intra-attention. The approach proposed by Munkhdalai and Yu (2016a) presents a memory augmented neural network, neural semantic encoders (NSE), to encode sentences. The next group of methods in the table, models (8)-(15), are inter-sentence attention-based model. The model marked with Rocktäschel et al. (2015) is LSTMs enforcing the so called word-by-word attention. The model of Wang and Jiang (2016) extends this idea to explicitly enforce word-by-word matching between the hypothesis and the premise. Long short-term memory-networks (LSTMN) with deep attention fusion (Cheng et al., 2016) link the current word to previous words stored in memory. Parikh et al. (2016) proposed a decomposable attention model without relying on any word-order information. In general, adding intra-sentence attention yields further improvement, which is not very surprising as it could help align the relevant text spans between premise and hypothesis. The model of Munkhdalai and Yu (2016b) extends the framework of Wang and Jiang (2016) to a full n-ary tree model and achieves further improvement.",
      "startOffset" : 52,
      "endOffset" : 1901
    }, {
      "referenceID" : 2,
      "context" : "The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc. The next group of models (2)-(7) are based on sentence encoding. The model of Bowman et al. (2016) encodes the premise and hypothesis with two different LSTMs. The model in Vendrov et al. (2015) uses unsupervised ’skip-thoughts’ pre-training in GRU encoders. The approach proposed by Mou et al. (2016) considers tree-based CNN to capture sentence-level semantics, while the model of Bowman et al. (2016) introduces a stack-augmented parser-interpreter neural network (SPINN) which combines parsing and interpretation within a single tree-sequence hybrid model. The work by Liu et al. (2016) uses BiLSTM to generate sentence representations, and then replaces average pooling with intra-attention. The approach proposed by Munkhdalai and Yu (2016a) presents a memory augmented neural network, neural semantic encoders (NSE), to encode sentences. The next group of methods in the table, models (8)-(15), are inter-sentence attention-based model. The model marked with Rocktäschel et al. (2015) is LSTMs enforcing the so called word-by-word attention. The model of Wang and Jiang (2016) extends this idea to explicitly enforce word-by-word matching between the hypothesis and the premise. Long short-term memory-networks (LSTMN) with deep attention fusion (Cheng et al., 2016) link the current word to previous words stored in memory. Parikh et al. (2016) proposed a decomposable attention model without relying on any word-order information. In general, adding intra-sentence attention yields further improvement, which is not very surprising as it could help align the relevant text spans between premise and hypothesis. The model of Munkhdalai and Yu (2016b) extends the framework of Wang and Jiang (2016) to a full n-ary tree model and achieves further improvement.",
      "startOffset" : 52,
      "endOffset" : 1948
    }, {
      "referenceID" : 2,
      "context" : "The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc. The next group of models (2)-(7) are based on sentence encoding. The model of Bowman et al. (2016) encodes the premise and hypothesis with two different LSTMs. The model in Vendrov et al. (2015) uses unsupervised ’skip-thoughts’ pre-training in GRU encoders. The approach proposed by Mou et al. (2016) considers tree-based CNN to capture sentence-level semantics, while the model of Bowman et al. (2016) introduces a stack-augmented parser-interpreter neural network (SPINN) which combines parsing and interpretation within a single tree-sequence hybrid model. The work by Liu et al. (2016) uses BiLSTM to generate sentence representations, and then replaces average pooling with intra-attention. The approach proposed by Munkhdalai and Yu (2016a) presents a memory augmented neural network, neural semantic encoders (NSE), to encode sentences. The next group of methods in the table, models (8)-(15), are inter-sentence attention-based model. The model marked with Rocktäschel et al. (2015) is LSTMs enforcing the so called word-by-word attention. The model of Wang and Jiang (2016) extends this idea to explicitly enforce word-by-word matching between the hypothesis and the premise. Long short-term memory-networks (LSTMN) with deep attention fusion (Cheng et al., 2016) link the current word to previous words stored in memory. Parikh et al. (2016) proposed a decomposable attention model without relying on any word-order information. In general, adding intra-sentence attention yields further improvement, which is not very surprising as it could help align the relevant text spans between premise and hypothesis. The model of Munkhdalai and Yu (2016b) extends the framework of Wang and Jiang (2016) to a full n-ary tree model and achieves further improvement. Sha et al. (2016) proposes a special LSTM variant which considers the attention vector of another sentence as an inner state of LSTM.",
      "startOffset" : 52,
      "endOffset" : 2027
    }, {
      "referenceID" : 2,
      "context" : "The first row is a baseline classifier presented by Bowman et al. (2015) that considers handcrafted features such as BLEU score of the hypothesis with respect to the premise, the overlapped words, and the length difference between them, etc. The next group of models (2)-(7) are based on sentence encoding. The model of Bowman et al. (2016) encodes the premise and hypothesis with two different LSTMs. The model in Vendrov et al. (2015) uses unsupervised ’skip-thoughts’ pre-training in GRU encoders. The approach proposed by Mou et al. (2016) considers tree-based CNN to capture sentence-level semantics, while the model of Bowman et al. (2016) introduces a stack-augmented parser-interpreter neural network (SPINN) which combines parsing and interpretation within a single tree-sequence hybrid model. The work by Liu et al. (2016) uses BiLSTM to generate sentence representations, and then replaces average pooling with intra-attention. The approach proposed by Munkhdalai and Yu (2016a) presents a memory augmented neural network, neural semantic encoders (NSE), to encode sentences. The next group of methods in the table, models (8)-(15), are inter-sentence attention-based model. The model marked with Rocktäschel et al. (2015) is LSTMs enforcing the so called word-by-word attention. The model of Wang and Jiang (2016) extends this idea to explicitly enforce word-by-word matching between the hypothesis and the premise. Long short-term memory-networks (LSTMN) with deep attention fusion (Cheng et al., 2016) link the current word to previous words stored in memory. Parikh et al. (2016) proposed a decomposable attention model without relying on any word-order information. In general, adding intra-sentence attention yields further improvement, which is not very surprising as it could help align the relevant text spans between premise and hypothesis. The model of Munkhdalai and Yu (2016b) extends the framework of Wang and Jiang (2016) to a full n-ary tree model and achieves further improvement. Sha et al. (2016) proposes a special LSTM variant which considers the attention vector of another sentence as an inner state of LSTM. Paria et al. (2016) use a neural architecture with a complete binary tree-LSTM encoders without syntactic information.",
      "startOffset" : 52,
      "endOffset" : 2163
    }, {
      "referenceID" : 2,
      "context" : "Train Test (1) Handcrafted features (Bowman et al., 2015) - 99.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "2 (2) 300D LSTM encoders (Bowman et al., 2016) 3.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 31,
      "context" : "6 (3) 1024D pretrained GRU encoders (Vendrov et al., 2015) 15M 98.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 20,
      "context" : "4 (4) 300D tree-based CNN encoders (Mou et al., 2016) 3.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "1 (5) 300D SPINN-PI encoders (Bowman et al., 2016) 3.",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 16,
      "context" : "2 (6) 600D BiLSTM intra-attention encoders (Liu et al., 2016) 2.",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 21,
      "context" : "2 (7) 300D NSE encoders (Munkhdalai and Yu, 2016a) 3.",
      "startOffset" : 24,
      "endOffset" : 50
    }, {
      "referenceID" : 26,
      "context" : "6 (8) 100D LSTM with attention (Rocktäschel et al., 2015) 250K 85.",
      "startOffset" : 31,
      "endOffset" : 57
    }, {
      "referenceID" : 32,
      "context" : "5 (9) 300D mLSTM (Wang and Jiang, 2016) 1.",
      "startOffset" : 17,
      "endOffset" : 39
    }, {
      "referenceID" : 6,
      "context" : "1 (10) 450D LSTMN with deep attention fusion (Cheng et al., 2016) 3.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 24,
      "context" : "3 (11) 200D decomposable attention model (Parikh et al., 2016) 380K 89.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 24,
      "context" : "3 (12) Intra-sentence attention + (11) (Parikh et al., 2016) 580K 90.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : "8 (13) 300D NTI-SLSTM-LSTM (Munkhdalai and Yu, 2016b) 3.",
      "startOffset" : 27,
      "endOffset" : 53
    }, {
      "referenceID" : 28,
      "context" : "3 (14) 300D re-read LSTM (Sha et al., 2016) 2.",
      "startOffset" : 25,
      "endOffset" : 43
    }, {
      "referenceID" : 23,
      "context" : "5 (15) 300D btree-LSTM encoders (Paria et al., 2016) 2.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 34,
      "context" : "Each tree node is implemented with a tree-LSTM block (Zhu et al., 2015) same as in model (17).",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 24,
      "context" : "If we remove the pooling layer in inference composition and replace it with summation as in Parikh et al. (2016), the accuracy drops to 87.",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 24,
      "context" : "If we remove the pooling layer in inference composition and replace it with summation as in Parikh et al. (2016), the accuracy drops to 87.1%. We further remove the difference and element-wise product from the local inference enhancement layer, the accuracy drops to 86.8%. The difference between any pairs of the models in Table 2 is statistically significant (t-test, p < 0.01). Note that in Table 1 above we do not have the output from other systems to perform a significance test, but the 0.2% difference is statistically significant among our systems. To provide some detailed comparison with Parikh et al. (2016), replacing bidirectional LSTMs in inference composition and also input encoding with MLP reduces the accuracy to 86.",
      "startOffset" : 92,
      "endOffset" : 619
    } ],
    "year" : 2017,
    "abstractText" : "Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford natural language inference dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result—it further improves the performance even when added to the already very strong model.",
    "creator" : "LaTeX with hyperref package"
  }
}