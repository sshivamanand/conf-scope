{
  "name" : "759.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Joint Modeling of Content and Discourse Relations in Dialogues",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Goal-oriented dialogues, such as meetings, negotiations, or customer service transcripts, play an important role in our daily life. Automatically extracting the critical points and important outcomes from dialogues would facilitate generating summaries for complicated conversations, understanding the decision-making process of meetings, or analyzing the effectiveness of collaborations.\nWe are interested in a specific type of dialogues — spoken meetings, which is a common way for collaboration and idea sharing. Previous work (Kirschner et al., 2012) has shown that discourse structure can be used capture the main discussion points and arguments put forward during problem-solving and decision-making processes in meetings. Indeed, content of different speaker turns do not occur in isolation, and should be interpreted within the context of discourse. Meanwhile, content can also reflect the purpose of speaker turns, thus facilitate with discourse relation understanding. Take the meeting snippet from AMI corpus (Carletta et al., 2006) in Figure 1 as\nD: Three different types of batteries. Um can either use a hand dynamo, or the kinetic type ones, you know that they use in watches, or else uh a solar powered one.\nB: Um the bat uh the battery for a a watch wouldn't require a lot of power, would be my one query. Is a kinetic one going to be able to supply enough power?\nD: Yeah, I don't think it would. C: Yeah.\nD: We should probably just use conventional batteries. B: Which I suppose as well would allow us to go off the shelf again, you'd say ?\nD: Yeah.\nUncertain\nOption\nan example. This discussion is annotated with discourse structure based on the Twente Argumentation Schema (TAS) by Rienks et al. (2005), which focuses on argumentative discourse information. As can be seen, meeting participants evaluate different options by showing doubt (UNCERTAIN), bringing up alternative solution (OPTION), or giving feedback. The discourse information helps with the identification of the key discussion point, i.e., “which type of battery to use”, by revealing the discussion flow.\nTo date, most efforts to leverage discourse information to detect salient content from dialogues have focused on encoding gold-standard discourse relations as features for use in classifier training (Murray et al., 2006; Galley, 2006; McKeown et al., 2007; Bui et al., 2009). However, automatic discourse parsing in dialogues is still a challenging problem (Perret et al., 2016). Moreover, acquiring human annotation on discourse relations is a timeconsuming and expensive process, and does not scale for large datasets.\nIn this paper, we propose a joint modeling approach to select salient phrases reflecting key dis-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ncussion points as well as label the discourse relations between speaker turns in spoken meetings. We hypothesize that leveraging the interaction between content and discourse has the potential to yield better prediction performance on both phrase-based content selection and discourse relation prediction. Specifically, we utilize argumentative discourse relations as defined in Twente Argument Schema (TAS) (Rienks et al., 2005), where discussions are organized into tree structures with discourse relations labeled between nodes (as shown in Figure 1). Algorithms for joint learning and joint inference are proposed for our model. We also present a variation of our model to treat discourse relations as latent variables when true labels are not available for learning. We envision that the extracted salient phrases by our model can be used as input to abstractive meeting summarization systems (Wang and Cardie, 2013; Mehdad et al., 2014). Combined with the predicted discourse structure, a visualization tool can be exploited to display conversation flow to support intelligent meeting assistant systems.\nTo the best of our knowledge, our work is the first to jointly model content and discourse relations in meetings. We test our model with two meeting corpora — the AMI corpus (Carletta et al., 2006) and the ICSI corpus (Janin et al., 2003). Experimental results show that our model yields accuracy of 63.2 and 59.2 on phrase selection and discourse prediction on AMI, which is significantly better than comparisons based on Support Vector Machines (SVM) classifiers (accuracy of 57.8 and 51.2). Our model trained with latent discourse also outperforms SVMs on both AMI and ICSI corpora. We further evaluate the usage of selected phrases as extractive meeting summaries. Results evaluated by ROUGE (Lin and Hovy, 2003) demonstrate that our system summaries obtain a ROUGE-SU4 F1 score of 21.3 on AMI corpus, which outperforms two utterancelevel extractive summarization baselines that select the longest and the most representative utterance from each discussion.\nMoreover, since both content and discourse structure are critical for building shared understanding among participants (Mulder et al., 2002; Mercer, 2004), we further investigate whether our learned model can be utilized to predict the consistency among team members’ understanding of their group decisions. This task is first defined as\nconsistency of understanding (COU) by Kim and Shah (2016), who have labeled a portion of AMI discussions with consistency or inconsistency labels. We construct features from our model predictions to capture different discourse patterns and word entrainment scores for discussion with different COU level. Results on AMI discussions show that SVM classifiers trained with our features significantly outperform the state-of-the-art results (Kim and Shah, 2016) (F1: 63.1 vs. 50.5) and non-trivial baselines."
    }, {
      "heading" : "2 Related Work",
      "text" : "Our model is inspired by research work that leverages discourse structure for identifying salient content in conversations, which is still largely reliant on features derived from gold-standard discourse labels (McKeown et al., 2007; Murray et al., 2010; Bokaei et al., 2016). There is much less work that jointly predicts the importance of content along with the discourse structure in dialogus. Oya and Carenini (2014) employs Dynamic Conditional Random Field to recognize sentences in email threads for use in summary as well as their dialogue acts. Only local discourse structures from adjacent utterances are considered. Our model is built on tree structures, which captures more global information.\nOur work is also in line with keyphrase identification or phrase-based summarization for conversations. Due to the noisy nature of dialogues, recent work focuses on identifying summary-worthy phrases from meetings (Fernández et al., 2008; Riedhammer et al., 2010) or email threads (Loza et al., 2014). Our work also targets at detecting salient phrases from meetings, but focuses on the joint modeling of critical discussion points and discourse relations held between them.\nFor the area of discourse analysis in dialogues, a significant amount of work has been done in predicting local discourse structures, such as recognizing dialogue acts or social acts of adjacent utterances from phone conversations (Stolcke et al., 2000; Kalchbrenner and Blunsom, 2013; Ji et al., 2016), spoken meetings (Dielmann and Renals, 2008), or emails (Cohen et al., 2004). Although discourse information from non-adjacent turns has been studied in the context of online discussion forums (Ghosh et al., 2014) and meetings (HakkaniTur, 2009), none of them models the effect of discourse structure on content selection, which is a\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\ngap that this work fills in."
    }, {
      "heading" : "3 The Joint Model of Content and Discourse Relations",
      "text" : ""
    }, {
      "heading" : "3.1 Model Description",
      "text" : "Our proposed model learns to jointly perform phrase-based content selection and discourse relation prediction by making use of the interaction between the two sources of information. Assume that a meeting discussion is denoted as x, where x consists of a sequence of discourse units x = {x1, x2, · · · , xn}. Each discourse unit can be a complete speaker turn or a part of it. As demonstrated in Figure 1, a tree-structured discourse diagram is constructed for each discussion with each discourse unit xi as a node of the tree. In this work, we consider the argumentative discourse structure by Twente Argument Schema (TAS) (Rienks et al., 2005). For each note xi, it is attached to another node xi′ (i′ < i) in the discussion, and a discourse relation di is hold on the link 〈xi, xi′〉 (di is empty if xi is the root). Let t denote the set of links 〈xi, xi′〉 in x. Following previous work on discourse analysis in meetings (Rienks et al., 2005; Hakkani-Tur, 2009), we assume that the attachment structure between discourse units are given during both training and testing.\nA set of candidate phrases are extracted from each discourse unit xi, from which salient phrases that contain gist information will be identified. We obtain constituent and dependency parses for utterances using Stanford parser (Klein and Manning, 2003). We restrict eligible candidate to be a noun phrase (NP), verb phrase (VP), prepositional phrase (PP), or adjective phrase (ADJP) with at most 5 words, and its head word cannot be a stop word. If a candidate is a parent of another candidate in the constituent parse tree, we will only keep the parent. We further merge a verb and a candidate noun phrase into one candidate if the later is the direct object or subject of the verb. For example, from utterance “let’s use a rubber case as well as rubber buttons”, we can identify candidates “use a rubber case” and “rubber buttons”. For xi, the set of candidate phrases are denoted as ci = {ci,1, ci,2, · · · , ci,mi}, where mi is the number of candidates. ci,j takes a value of 1 if the corresponding candidate is selected as salient phrase; otherwise, ci,j is equal to 0. All candidate phrases in discussion x are represented as c.\nWe then define a log-linear model with feature\nparameters w for the candidate phrases c and discourse relations d in x as: p(c,d|x,w) ∝ exp[w · Φ(c,d,x)]\n∝ exp[w · n∑\ni=1,<xi,xi′>∈t\nφ(ci, di, di′ ,x)]\n∝ exp[ n∑\ni=1,<xi,xi′>∈t\n(wc · mi∑ j=1 φc(ci,j ,x)\n+ wd · φd(di, di′ ,x) + wcd · mi∑ j=1 φcd(ci,j , di,x))]\n(1)\nHere Φ(·) and φ(·) denote feature vectors. We utilize three types of feature functions: (1) content-only features φc(·), which capture the importance of phrases, (2) discourse-only features φd(·), which characterize the (potentially higherorder) discourse relations, and (3) joint features of content and discourse φcd(·), which model the interaction between the two. wc, wd, and wcd are corresponding feature parameters. Detailed feature descriptions can be found in Section 3.4. Discourse Relations as Latent Variables. As we mentioned in the introduction, acquiring labeled training data for discourse relations is a timeconsuming process since it would require human annotators to inspect the full discussions. Therefore, we further propose a variation of our model where it treats the discourse relations as latent variables, so that p(c|x,w) = ∑ d p(c,d|x,w). Its learning algorithm is slightly different as described in the next section."
    }, {
      "heading" : "3.2 Joint Learning for Parameter Estimation",
      "text" : "For learning the model parameters w, we employ an algorithm based on SampleRank (Rohanimanesh et al., 2011), which is a stochastic structure learning method. In general, the learning algorithm constructs a sequence of configurations for sample labels as a Markov chain Monte Carlo (MCMC) chain based on a task-specific loss function, where stochastic gradients are distributed across the chain. This is suitable for our learning problem because we aim to optimize the prediction performance for both phrase selection and discourse relations with various types of features.\nThe full learning procedure is described in Algorithm 1. To start with, the feature weights w is initialized with each value randomly drawn from [−1, 1]. Multiple epochs are run through all samples. For each sample, we randomly initialize the assignment of candidate phrases labels c and discourse relations d. Then an MCMC chain is con-\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nstructed with a series of configurations σ = (c, d): at each step, it first samples a discourse structure d based on the proposal distribution q(d′|d,x), and then samples phrase labels conditional on the new discourse relations and previous phrase labels based on q(c′|c,d′,x). Local search is used for both proposal distributions. The new configuration is accepted if it improves on the score by ω(σ′). The parameters w are updated accordingly.\nFor the scorer ω, we use a weighted combination of F1 scores of phrase selection (F1c) and discourse relation prediction (F1d): ω(σ) = α · F1c + (1− α) · F1d. We fix α to 0.1.\nWhen discourse relations are treated as latent, we initialize discourse relations for each sample with a label in {1, 2, . . . ,K} if there are K relations indicated, and we only use F1c as the scorer.\nInput : X = {x}: discussions in the training set, η: learning rate, : number of epochs, δ: number of sampling rounds, ω(·): scoring function, Φ(·): feature functions\nOutput: feature weights 1|W| ∑ w∈W w Initialize w; W ← {w}; for e = 1 to do\nfor x in X do // Initialize configuration for\nx Initialize c and d; σ = (c,d); for s = 1 to δ do\n// New configuration via local search d′ ∼ qd(·|x,d); c′ ∼ qd(·|x, c,d′); σ′ = (c′,d′); σ+ = arg maxσ̃∈{σ,σ′} ω(σ̃); σ− = arg minσ̃∈{σ,σ′} ω(σ̃); ∇̂ = Φ(σ+)− Φ(σ−); ∆ω = ω(σ+)− ω(σ−); // Update parameters if w · ∇̂ < ∆ω & ∆ω 6= 0 then\nw′ = w + η · ∇̂; Add w′ inW;\nend // Accept or reject new\nconfiguration if σ+ == σ′ then\nσ = σ′ end\nend end\nend Algorithm 1: SampleRank-based joint learning."
    }, {
      "heading" : "3.3 Joint Inference for Prediction",
      "text" : "Given a new sample x and learned parameters w, we predict phrase labels and discourse relations as arg maxc,d p(c,d|x,w).\nDynamic programming can be employed to carry out joint inference, however, it would\nbe time-consuming since our objective function has a large search space for both content and discourse labels. Hence we propose an alternating optimizing algorithm to search for c and d iteratively. Concretely, for each iteration, we first optimize on d by maximizing ∑n i=1,<xi,x′i>∈t\n(wd · φd(di, di′ ,x) + wcd ·∑mi j=1 φcd(ci,j , di,x)). Message-passing (Smith and Eisner, 2008) is used to find the best d. In the second step, we search for c that max-\nimizes ∑n\ni=1,<xi,x′i>∈t (wc · ∑mi j=1 φc(ci,j ,x) +\nwcd · ∑mi\nj=1 φcd(ci,j , di,x)). We believe that candidate phrases based on the same concepts should have the same predicted label. Therefore, candidates of the same phrase type and sharing the same head word are grouped into one cluster. We then cast our task as an integer linear programming problem.1 We optimize our objective function under constraints: (1) ci,j = ci′,j′ if ci,j and ci′,j′ are in the same cluster, and (2) ci,j ∈ 0, 1, ∀i, j.\nThe inference process is the same for models trained with latent discourse relations."
    }, {
      "heading" : "3.4 Features",
      "text" : "We use features that characterize content, discourse relations, and the combination of both. Content Features. For modeling the salience of content, we calculate the minimum, maximum, and average of TF-IDF scores of words and number of content words in each phrase based on the intuition that important phrases tend to have more content words with high TF-IDF scores (Fernández et al., 2008). We also consider whether the head word of the phrase has been mentioned in preceding turn, which implies the focus of a discussion. The size of the cluster this phrase belongs to is also included. Number of POS tags and phrase types are counted to characterize the syntactic structure. Previous work (Wang and Cardie, 2012) has found that a discussion usually ends with decision-relevant information. We thus identify the absolute and relative positions of the turn containing the candidate phrase in the discussion. Finally, we record whether the candidate phrase is uttered by the main speaker, who speakers the most words in the discussion. Discourse Features. For each discourse unit, we\n1We use lpsolve: http://lpsolve. sourceforge.net/5.5/.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\ncollect the dialogue act types of the current unit and its parent node in discourse tree, whether there is any adjacency pair held between the two nodes (Hakkani-Tur, 2009), and the Jaccard similarity between them. We record whether two turns are uttered by the same speaker, for example, ELABORATION is commonly observed between the turns from the same participant. We also calculate the number of candidate phrases based on the observation that OPTION and SPECIALIZATION tend to contain more informative words than POSITIVE feedback. Length of the discourse unit is also relevant. Therefore, we compute the time span and number of words. To incorporate global structure features, we encode the depth of the node in the discourse tree and the number of its siblings. Finally, we include an order-2 discourse relation feature that encodes the relation between current discourse unit and its parent, and the relation between the parent and its grandparent if it exists. Joint Features. For modeling the interaction between content and discourse, the discourse relation is added to each content feature to compose a joint feature. For example, if candidate c in discussion x has a content feature φ[avg−TFIDF ](c,x) with a value of 0.5, and its discourse relation d is POSITIVE, then the joint feature takes the form of φ[avg−TFIDF,Positive](c, d,x) = 0.5."
    }, {
      "heading" : "4 Datasets and Experimental Setup",
      "text" : "Meeting Corpora. We evaluate our joint model on two meeting corpora with rich annotations: the AMI meeting corpus (Carletta et al., 2006) and the ICSI meeting corpus (Janin et al., 2003). AMI corpus consists of 139 scenario-driven meetings, and ICSI corpus contains 75 naturally occurring meetings. Both of the corpora are annotated with dialogue acts, adjacency pairs, and topic segmentation. We treat each topic segment as one discussion, and remove discussions with less than 10 turns or labeled as “opening”. 694 discussions from AMI and 1139 discussions from ICSI are extracted, and these two datasets are henceforth referred as AMI-FULL and ICSI-FULL. Acquiring Gold-Standard Labels. Both corpora contain human constructed abstractive summaries and extractive summaries on meeting level. Short abstracts, usually in one sentence, are constructed by meeting participants — participant summaries,\nand external annotators — abstractive summaries. Dialogue acts that contribute to important output of the meeting, e.g. decisions, are identified and used as extractive summaries, and some of them are also linked to the corresponding abstracts.\nSince the corpora do not contain phrase-level importance annotation, we induce gold-standard labels for candidate phrases based on the following rule. A candidate phrase is considered as a positive sample if its head word is contained in any abstractive summary or participant summary.\nFurthermore, a subset of discussions in AMIFULL are annotated with discourse structure and relations based on Twente Argumentation Schema (TAS) by Rienks et al. (2005)2. A tree-structured argument diagram (as shown in Figure 1) is created for each discussion or a part of the discussion. The nodes of the tree contain partial or complete speaker turns, and discourse relation types are labeled on the links between the nodes. In total, we have 129 discussions annotated with discourse labels. This dataset is called AMI-SUB hereafter. Experimental Setup. 5-fold cross validation is used for all experiments. All real-valued features are uniformly normalized to [0,1]. For the joint learning algorithm, we use 10 epochs and carry out 50 sampling for MCMC for each training sample. The learning rate is set to 0.01. We run the learning algorithm for 20 times, and use the average of the learned weights as the final parameter values. For models trained with latent discourse relations, we fix the number of relations to 9. Baselines and Comparisons. For both phrasebased content selection and discourse relation prediction tasks, we consider (1) a baseline that always predicts the majority label (Majority), and (2) a random baseline. Previous work has shown that Support Vector Machines (SVMs)-based classifiers achieve state-of-the-art performance for keyphrase selection in meetings (Fernández et al., 2008; Wang and Cardie, 2013) and discourse parsing for formal text (Hernault et al., 2010). Therefore, we compare with linear SVM-based classifiers, trained with the same feature set of content features or discourse features. We fix the trade-off parameter to 1.0 for all SVM-based experiments. For discourse relation prediction, we use one-vsrest strategy to build multiple binary classifiers.\n2There are 9 types of relations in TAS: POSITIVE, NEGATIVE, UNCERTAIN, REQUEST, SPECIALIZATION, ELABORATION, OPTION, OPTION EXCLUSION, and SUBJECT-TO.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nAcc F1 Comparisons Baseline (Majority) 60.1 37.5 Baseline (Random) 50.0 33.1 SVM (w content features in § 3.4) 57.8 54.6 Our Models Joint-Learn + Joint-Inference 63.2∗ 62.6∗ Joint-Learn + Separate-Inference 57.9 57.8 Separate-Learn 53.4 52.6 Our Models (Latent Discourse) w/ True Attachment Structure Joint-Learn + Joint-Inference 60.3∗ 60.3∗ Joint-Learn + Separate-Inference 56.4 56.2 w/o True Attachment Structure Joint-Learn + Joint-Inference 56.4 56.4 Joint-Learn + Separate-Inference 52.7 52.3\nTable 1: Phrase-based content selection performance on AMI-SUB with accuracy (acc) and F1. We display results of our models trained with gold-standard discourse relation labels and with latent discourse relations. For the later, we also show results based on True Attachment Structure, where the gold-standard attachments are known, and without the True Attachment Structure. Our models that significantly outperform SVM-based model are highlighted with ∗ (p < 0.05, paired t-test). Best result for each column is in bold.\nAcc F1 Comparisons Baseline (Majority) 51.2 7.5 Baseline (Random) 11.1 1.9 SVM (w discourse features in § 3.4) 51.2 22.8 Our Models Joint-Learn + Joint-Inference 58.0∗ 21.7 Joint-Learn + Separate-Inference 59.2∗ 23.4 Separate-Learn 58.2∗ 25.1\nTable 2: Discourse relation prediction performance on AMI-SUB. Our models that significantly outperform SVM-based model are highlighted with ∗ (p < 0.05, paired t-test)."
    }, {
      "heading" : "5 Experimental Results",
      "text" : ""
    }, {
      "heading" : "5.1 Phrase Selection and Discourse Labeling",
      "text" : "Here we present the experimental results on phrase-based content selection and discourse relation prediction. We experiment with two variations of our joint model: one is trained on goldstandard discourse relations, the other is trained by treating discourse relations as latent models as described in Section 3.1. Remember that we have gold-standard argument diagrams on the AMISUB dataset, we can thus conduct experiments by assuming the True Attachment Structure is given for latent versions. When argument diagrams are not available, we build a tree among the turns in each discussion as follows. Two turns are attached if there is any adjacency pair between them. If one turn is attached to more than one previous turns, the closest one is considered. For the rest of the turns, they are attached to the preceding turn.\nWe also investigate whether joint learning and joint inference can produce better prediction performance. We consider joint learning with separate inference, where only content features or discourse features are used for prediction. We further study learning separate classifiers for content selection and discourse relations without joint features (Separate-Learn).\nWe first show the phrase selection and discourse relation prediction results on AMI-SUB in Tables 1 and 2. As shown in Table 1, our models, trained with gold-standard discourse relations or latent ones with true attachment structure, yield significant better accuracy and F1 scores than SVM-based classifiers trained with the same feature sets for phrase selection (paired t-test, p < 0.05). Moreover, both Tables 1 and 2 demonstrate that joint learning usually produces superior performance for both tasks than separate learning. Combined with joint inference, our model obtains the best accuracy and F1 on phrase selection. This indicates that leveraging the interplay between content and discourse boost the prediction performance. Similar results are achieved on AMI-FULL and ICSI-FULL in Table 3, where latent discourse relations without true attachment structure are employed for training."
    }, {
      "heading" : "5.2 Phrase-Based Extractive Summarization",
      "text" : "We further evaluate whether the prediction of the content selection component can be used for summarizing the key points on discussion level. For each discussion, salient phrases identified by our model are concatenated in sequence for use as the summary. We consider two types of gold-standard summaries. One is utterance-level extractive summary, which consists of human labeled summaryworthy utterances. The other is abstractive summary, where we collect human abstract with at least one link from summary-worthy utterances.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nExtractive Summaries as Gold-Standard ROUGE-1 ROUGE-SU4 Len Prec Rec F1 Prec Rec F1 Longest DA 30.9 64.4 15.0 23.1 58.6 9.3 15.3 Centroid DA 17.5 73.9 13.4 20.8 62.5 6.9 11.3 SVM 49.8 47.1 24.1 27.5 22.7 10.7 11.8 Our Model 66.6 45.4 44.7 41.1∗ 24.1∗ 23.4∗ 20.9∗ Our Model-latent 85.9 42.9 49.3 42.4∗ 21.6 25.7∗ 21.3∗ Abstractive Summaries as Gold-Standard\nROUGE1 ROUGE-SU4 Len Prec Rec F1 Prec Rec F1\nLongest DA 30.9 14.8 5.5 7.4 4.8 1.4 1.9 Centroid DA 17.5 24.9 5.6 8.5 11.6 1.4 2.2 SVM 49.8 13.3 9.7 9.5 4.4 2.4 2.4 Our Model 66.6 12.6 18.9 13.1∗ 3.8 5.5∗ 3.7∗ Our Model-latent 85.9 11.4 20.0 12.4∗ 3.3 6.1∗ 3.5∗\nTable 4: ROUGE scores for phrase-based extractive summarization evaluated against human-constructed utterance-level extractive summaries and abstractive summaries. Our models that statistically significantly outperform all comparisons are highlighted with ∗ (p < 0.05, paired t-test).\nWe calculate scores based on ROUGE (Lin and Hovy, 2003), which is a popular tool for evaluating text summarization (Gillick et al., 2009; Liu and Liu, 2010). ROUGE-1 (unigrams) and ROUGE-SU4 (skip-bigrams with at most 4 words in between) are used. Following previous work on meeting summarization (Riedhammer et al., 2010; Wang and Cardie, 2013), we consider two dialogue act-level summarization baselines: (1) LONGEST DA in each discussion is selected as the summary, and (2) CENTROID DA, the one with the highest TF-IDF similarity with all DAs in the discussion. We also compare with summaries consisting of salient phrases predicted by an SVM classifier trained with on our content features.\nFrom the results in Table 4, we can see that phrase-based extractive summarization methods by our system and SVM-based classifiers can yield better ROUGE scores for recall and F1 than baselines that extract the whole sentences. Meanwhile, our system significantly outperforms the SVMbased classifiers when evaluated on ROUGE recall and F1, while achieving comparable precision.\nSample summaries by our model along with two baselines are displayed in Figure 2. Utterancelevel extract-based baselines unavoidably contain disfluency and unnecessary details. Our phrasebased extractive summary is able to capture the key points from both the argumentation process and important outcomes of the conversation."
    }, {
      "heading" : "5.3 Further Analysis and Discussions",
      "text" : "Features Analysis. We first discuss salient features with top weights learned by our joint model.\nFor content features, main speaker tends to utter more salient content. Higher TF-IDF scores also indicate important phrases. For discourse features, structure features matter the most. For instance, jointly modeling the discourse relation of the parent node along with the current node can lead to better inference. An example is that giving more details on the proposal (ELABORATION) tends to lead to POSITIVE feedback. For joint features, features that composite “phrase mentioned in previous turn” and relation POSITIVE feedback or REQUEST yield higher weight, which are indicators for both key phrases and discourse relations. We also find that main speaker information composite with ELABORATION and UNCERTAIN are associated with high weights. Error Analysis and Potential Directions. Taking a closer look at our prediction results, one major source of incorrect prediction for phrase selection is based on the fact that similar concepts might be expressed in different ways, and our model predicts inconsistently for different variations. For example, participants use both “thick” and “two centimeters” to talk about the desired shape of a remote control. However, our model does not group them into the same cluster and later makes different predictions. Furthermore, identifying discourse relations in dialogues is still a challenging task. For instance, “I wouldn’t choose a plastic case” should be labeled as OPTION EX-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nCLUSION, if the previous turns talk about different options. Otherwise, it can be labeled as NEGATIVE. Therefore, models that better handle semantics and context need to be considered."
    }, {
      "heading" : "6 Predicting Consistency of Understanding",
      "text" : "In this section, we test whether our joint model can be utilized to predict the consistency among team members’ understanding of their group decisions, which is defined as consistency of understanding (COU) in Kim and Shah (2016).\nKim and Shah (2016) establish gold-standard COU labels on a portion of AMI discussions, by comparing participant summaries to determine whether participants report the same decisions. If all decision points are consistent, the associated topic discussion is labeled as consistent; otherwise, the discussion is identified as inconsistent. Their annotation covers the AMI-SUB dataset. Therefore, we run the prediction experiments on AMI-SUB by using the same annotation.3 Out of total 129 discussions in AMI-SUB, 86 discussions are labeled as consistent and 43 are inconsistent.\nWe construct three types of features by using our model’s predicted labels. Firstly, we learn two versions of our model based on the “consistent” discussions and the “inconsistent” ones in the training set, with learned parameters wcon and wincon. For a discussion in the test set, these two models output two probabilities pcon = maxc,d P (c,d|x,wcon) and pincon = maxc,d P (c,d|x,wincon). We use pcon − pincon as a feature.\nFurthermore, we consider discourse relations of length one and two from the discourse structure tree. Intuitively, some discourse relations, e.g., ELABORATION followed by several POSITIVE implied consistent understanding.\nThe third feature is based on word entrainment, which has been shown to correlate with task success for groups (Nenkova et al., 2008). Using the formula in Nenkova et al. (2008), we compute the average word entrainment between the main speaker who utters the most words and all the other participants. The content words in the salient phrases predicted by our model is considered for entrainment computation. Results. Leave-one-out is used for experiments.\n3We thank Joseph Kim for providing the dataset and rerunning their system based on our setup.\nFor training, our features are constructed from gold-standard phrase and discourse labels. Predicted labels by our model is used for constructing features during testing. SVM-based classifier is used. A majority class baseline and a random baseline are constructed as well. We also consider an SVM classifier trained with ngram features (unigrams and bigrams). Finally, we compare with the state-of-the-art method in Kim and Shah (2016), where discourse-relevant features and head gesture features are utilized in Hidden Markov Models to predict the consistency label.\nThe results are displayed in Table 5. All SVMs trained with our features surpass the ngrams-based baseline. Especially, the discourse features, word entrainment feature, and the combination of the three, all significantly outperform the state-of-theart system by Kim and Shah (2016)."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We presented a joint model for performing phraselevel content selection and discourse relation prediction in spoken meetings. Experimental results on AMI and ICSI meeting corpora showed that our model can outperform SVM-based classifiers trained with the same feature sets. Further evaluation on the task of predicting consistencyof-understanding in meetings demonstrated that classifiers trained with features constructed from our model’s predictions produced superior performance compared to state-of-the-art model.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Extractive summarization of multi-party meetings through discourse segmentation",
      "author" : [ "Mohammad Hadi Bokaei", "Hossein Sameti", "Yang Liu." ],
      "venue" : "Natural Language Engineering 22(01):41–72.",
      "citeRegEx" : "Bokaei et al\\.,? 2016",
      "shortCiteRegEx" : "Bokaei et al\\.",
      "year" : 2016
    }, {
      "title" : "Extracting decisions from multi-party dialogue using directed graphical models and semantic similarity",
      "author" : [ "Trung H. Bui", "Matthew Frampton", "John Dowding", "Stanley Peters." ],
      "venue" : "Proceedings of the SIGDIAL 2009 Conference:",
      "citeRegEx" : "Bui et al\\.,? 2009",
      "shortCiteRegEx" : "Bui et al\\.",
      "year" : 2009
    }, {
      "title" : "The ami meeting corpus: A pre-announcement",
      "author" : [ "Dennis Reidsma", "Pierre Wellner." ],
      "venue" : "Proceedings of the Second International Conference on Machine Learning for Multimodal Interaction. Springer-Verlag, Berlin, Heidelberg, MLMI’05,",
      "citeRegEx" : "Reidsma and Wellner.,? 2006",
      "shortCiteRegEx" : "Reidsma and Wellner.",
      "year" : 2006
    }, {
      "title" : "Learning to classify email into speech acts",
      "author" : [ "William W. Cohen", "Vitor R. Carvalho", "Tom M. Mitchell." ],
      "venue" : "In Proceedings of Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Cohen et al\\.,? 2004",
      "shortCiteRegEx" : "Cohen et al\\.",
      "year" : 2004
    }, {
      "title" : "Recognition of dialogue acts in multiparty meetings using a switching dbn",
      "author" : [ "Alfred Dielmann", "Steve Renals." ],
      "venue" : "IEEE transactions on audio, speech, and language processing 16(7):1303–1314.",
      "citeRegEx" : "Dielmann and Renals.,? 2008",
      "shortCiteRegEx" : "Dielmann and Renals.",
      "year" : 2008
    }, {
      "title" : "Identifying relevant phrases to summarize decisions in spoken meetings",
      "author" : [ "Raquel Fernández", "Matthew Frampton", "John Dowding", "Anish Adukuzhiyil", "Patrick Ehlen", "Stanley Peters." ],
      "venue" : "INTERSPEECH. pages 78–81.",
      "citeRegEx" : "Fernández et al\\.,? 2008",
      "shortCiteRegEx" : "Fernández et al\\.",
      "year" : 2008
    }, {
      "title" : "A skip-chain conditional random field for ranking meeting utterances by importance",
      "author" : [ "Michel Galley." ],
      "venue" : "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Strouds-",
      "citeRegEx" : "Galley.,? 2006",
      "shortCiteRegEx" : "Galley.",
      "year" : 2006
    }, {
      "title" : "Analyzing argumentative discourse units in online interactions",
      "author" : [ "Debanjan Ghosh", "Smaranda Muresan", "Nina Wacholder", "Mark Aakhus", "Matthew Mitsui." ],
      "venue" : "Proceedings of the First Workshop on Argumentation Mining. pages 39–48.",
      "citeRegEx" : "Ghosh et al\\.,? 2014",
      "shortCiteRegEx" : "Ghosh et al\\.",
      "year" : 2014
    }, {
      "title" : "A global optimization framework for meeting summarization",
      "author" : [ "Dan Gillick", "Korbinian Riedhammer", "Benoit Favre", "Dilek Hakkani-Tur." ],
      "venue" : "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on. IEEE,",
      "citeRegEx" : "Gillick et al\\.,? 2009",
      "shortCiteRegEx" : "Gillick et al\\.",
      "year" : 2009
    }, {
      "title" : "Towards automatic argument diagramming of multiparity meetings",
      "author" : [ "Dilek Hakkani-Tur." ],
      "venue" : "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on. IEEE, pages 4753–4756.",
      "citeRegEx" : "Hakkani.Tur.,? 2009",
      "shortCiteRegEx" : "Hakkani.Tur.",
      "year" : 2009
    }, {
      "title" : "HILDA: A discourse parser using support vector machine classification",
      "author" : [ "Hugo Hernault", "Helmut Prendinger", "David A. duVerle", "Mitsuru Ishizuka." ],
      "venue" : "Dialogue & Discourse 1(3):1–33. http://dad.unibielefeld.de/index.php/dad/article/view/591.",
      "citeRegEx" : "Hernault et al\\.,? 2010",
      "shortCiteRegEx" : "Hernault et al\\.",
      "year" : 2010
    }, {
      "title" : "The icsi meeting corpus",
      "author" : [ "Adam Janin", "Don Baron", "Jane Edwards", "Dan Ellis", "David Gelbart", "Nelson Morgan", "Barbara Peskin", "Thilo Pfau", "Elizabeth Shriberg", "Andreas Stolcke" ],
      "venue" : "In Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "Janin et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Janin et al\\.",
      "year" : 2003
    }, {
      "title" : "A latent variable recurrent neural network for discourse-driven language models",
      "author" : [ "Yangfeng Ji", "Gholamreza Haffari", "Jacob Eisenstein." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association",
      "citeRegEx" : "Ji et al\\.,? 2016",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent convolutional neural networks for discourse compositionality",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "arXiv preprint arXiv:1306.3584 .",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Improving team’s consistency of understanding in meetings",
      "author" : [ "Joseph Kim", "Julie A Shah." ],
      "venue" : "IEEE Transactions on Human-Machine Systems 46(5):625–637.",
      "citeRegEx" : "Kim and Shah.,? 2016",
      "shortCiteRegEx" : "Kim and Shah.",
      "year" : 2016
    }, {
      "title" : "Visualizing argumentation: Software tools for collaborative and educational sensemaking",
      "author" : [ "Paul A Kirschner", "Simon J Buckingham-Shum", "Chad S Carr." ],
      "venue" : "Springer Science & Business Media.",
      "citeRegEx" : "Kirschner et al\\.,? 2012",
      "shortCiteRegEx" : "Kirschner et al\\.",
      "year" : 2012
    }, {
      "title" : "Accurate unlexicalized parsing",
      "author" : [ "Dan Klein", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1. Association for Computational Linguistics, Strouds-",
      "citeRegEx" : "Klein and Manning.,? 2003",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2003
    }, {
      "title" : "Automatic evaluation of summaries using n-gram co-occurrence statistics",
      "author" : [ "Chin-Yew Lin", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Hu-",
      "citeRegEx" : "Lin and Hovy.,? 2003",
      "shortCiteRegEx" : "Lin and Hovy.",
      "year" : 2003
    }, {
      "title" : "Using spoken utterance compression for meeting summarization: A pilot study",
      "author" : [ "Fei Liu", "Yang Liu." ],
      "venue" : "Spoken Language Technology Workshop (SLT), 2010 IEEE. IEEE, pages 37–42.",
      "citeRegEx" : "Liu and Liu.,? 2010",
      "shortCiteRegEx" : "Liu and Liu.",
      "year" : 2010
    }, {
      "title" : "Building a dataset for summarization and keyword extraction from emails",
      "author" : [ "Vanessa Loza", "Shibamouli Lahiri", "Rada Mihalcea", "Po-Hsiang Lai." ],
      "venue" : "LREC. pages 2441–2446.",
      "citeRegEx" : "Loza et al\\.,? 2014",
      "shortCiteRegEx" : "Loza et al\\.",
      "year" : 2014
    }, {
      "title" : "Using question-answer pairs in extractive summarization of email conversations",
      "author" : [ "Kathleen McKeown", "Lokesh Shrestha", "Owen Rambow." ],
      "venue" : "International Conference on Intelligent Text Processing and Computational Linguistics. Springer, pages",
      "citeRegEx" : "McKeown et al\\.,? 2007",
      "shortCiteRegEx" : "McKeown et al\\.",
      "year" : 2007
    }, {
      "title" : "Abstractive summarization of spoken and written conversations based on phrasal queries",
      "author" : [ "Yashar Mehdad", "Giuseppe Carenini", "Raymond T. Ng." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1:",
      "citeRegEx" : "Mehdad et al\\.,? 2014",
      "shortCiteRegEx" : "Mehdad et al\\.",
      "year" : 2014
    }, {
      "title" : "Sociocultural discourse analysis",
      "author" : [ "Neil Mercer." ],
      "venue" : "Journal of applied linguistics 1(2):137–168.",
      "citeRegEx" : "Mercer.,? 2004",
      "shortCiteRegEx" : "Mercer.",
      "year" : 2004
    }, {
      "title" : "Assessing group learning and shared understanding in technology-mediated interaction",
      "author" : [ "Ingrid Mulder", "Janine Swaak", "Joseph Kessels." ],
      "venue" : "Educational Technology & Society 5(1):35–47.",
      "citeRegEx" : "Mulder et al\\.,? 2002",
      "shortCiteRegEx" : "Mulder et al\\.",
      "year" : 2002
    }, {
      "title" : "Generating and validating abstracts of meeting conversations: A user study",
      "author" : [ "Gabriel Murray", "Giuseppe Carenini", "Raymond Ng." ],
      "venue" : "Proceedings of the 6th International Natural Language Generation Conference. Associa-",
      "citeRegEx" : "Murray et al\\.,? 2010",
      "shortCiteRegEx" : "Murray et al\\.",
      "year" : 2010
    }, {
      "title" : "Incorporating speaker and discourse features into speech summarization",
      "author" : [ "Gabriel Murray", "Steve Renals", "Jean Carletta", "Johanna Moore." ],
      "venue" : "Proceedings of the main conference on Human Language Technology Conference of the North Amer-",
      "citeRegEx" : "Murray et al\\.,? 2006",
      "shortCiteRegEx" : "Murray et al\\.",
      "year" : 2006
    }, {
      "title" : "High frequency word entrainment in spoken dialogue",
      "author" : [ "Ani Nenkova", "Agustin Gravano", "Julia Hirschberg." ],
      "venue" : "Proceedings of the 46th annual meeting of the association for computational linguistics on human language technologies: Short papers. As-",
      "citeRegEx" : "Nenkova et al\\.,? 2008",
      "shortCiteRegEx" : "Nenkova et al\\.",
      "year" : 2008
    }, {
      "title" : "Extractive summarization and dialogue act modeling on email threads: An integrated probabilistic approach",
      "author" : [ "Tatsuro Oya", "Giuseppe Carenini." ],
      "venue" : "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue. page 133.",
      "citeRegEx" : "Oya and Carenini.,? 2014",
      "shortCiteRegEx" : "Oya and Carenini.",
      "year" : 2014
    }, {
      "title" : "Integer linear programming for discourse parsing",
      "author" : [ "Jérémy Perret", "Stergos Afantenos", "Nicholas Asher", "Mathieu Morey." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the",
      "citeRegEx" : "Perret et al\\.,? 2016",
      "shortCiteRegEx" : "Perret et al\\.",
      "year" : 2016
    }, {
      "title" : "Long story short - global unsupervised models for keyphrase based meeting summarization",
      "author" : [ "Korbinian Riedhammer", "Benoit Favre", "Dilek Hakkani-Tür." ],
      "venue" : "Speech Commun. 52(10):801–815. https://doi.org/10.1016/j.specom.2010.06.002.",
      "citeRegEx" : "Riedhammer et al\\.,? 2010",
      "shortCiteRegEx" : "Riedhammer et al\\.",
      "year" : 2010
    }, {
      "title" : "Argument diagramming of meeting conversations",
      "author" : [ "Rutger Rienks", "Dirk Heylen", "E. van der Weijden." ],
      "venue" : "A. Vinciarelli and J-M. Odobez, editors, International Workshop on Multimodal Multiparty Meeting Processing, MMMP 2005, part of",
      "citeRegEx" : "Rienks et al\\.,? 2005",
      "shortCiteRegEx" : "Rienks et al\\.",
      "year" : 2005
    }, {
      "title" : "Samplerank: Training factor graphs with atomic gradients",
      "author" : [ "Khashayar Rohanimanesh", "Kedar Bellare", "Aron Culotta", "Andrew McCallum", "Michael L Wick." ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning (ICML-",
      "citeRegEx" : "Rohanimanesh et al\\.,? 2011",
      "shortCiteRegEx" : "Rohanimanesh et al\\.",
      "year" : 2011
    }, {
      "title" : "Dependency parsing by belief propagation",
      "author" : [ "David A Smith", "Jason Eisner." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 145–156.",
      "citeRegEx" : "Smith and Eisner.,? 2008",
      "shortCiteRegEx" : "Smith and Eisner.",
      "year" : 2008
    }, {
      "title" : "Dialogue act modeling for automatic tagging and recognition",
      "author" : [ "Andreas Stolcke", "Klaus Ries", "Noah Coccaro", "Elizabeth Shriberg", "Rebecca Bates", "Daniel Jurafsky", "Paul Taylor", "Rachel Martin", "Carol Van Ess-Dykema", "Marie Meteer" ],
      "venue" : null,
      "citeRegEx" : "Stolcke et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Stolcke et al\\.",
      "year" : 2000
    }, {
      "title" : "Focused Meeting Summarization via Unsupervised Relation Extraction",
      "author" : [ "Lu Wang", "Claire Cardie." ],
      "venue" : "Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computa-",
      "citeRegEx" : "Wang and Cardie.,? 2012",
      "shortCiteRegEx" : "Wang and Cardie.",
      "year" : 2012
    }, {
      "title" : "Domainindependent abstract generation for focused meeting summarization",
      "author" : [ "Lu Wang", "Claire Cardie." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for",
      "citeRegEx" : "Wang and Cardie.,? 2013",
      "shortCiteRegEx" : "Wang and Cardie.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Previous work (Kirschner et al., 2012) has shown that discourse structure can be used capture the main discussion points and arguments put forward during problem-solving and decision-making processes in meetings.",
      "startOffset" : 14,
      "endOffset" : 38
    }, {
      "referenceID" : 25,
      "context" : "To date, most efforts to leverage discourse information to detect salient content from dialogues have focused on encoding gold-standard discourse relations as features for use in classifier training (Murray et al., 2006; Galley, 2006; McKeown et al., 2007; Bui et al., 2009).",
      "startOffset" : 199,
      "endOffset" : 274
    }, {
      "referenceID" : 6,
      "context" : "To date, most efforts to leverage discourse information to detect salient content from dialogues have focused on encoding gold-standard discourse relations as features for use in classifier training (Murray et al., 2006; Galley, 2006; McKeown et al., 2007; Bui et al., 2009).",
      "startOffset" : 199,
      "endOffset" : 274
    }, {
      "referenceID" : 20,
      "context" : "To date, most efforts to leverage discourse information to detect salient content from dialogues have focused on encoding gold-standard discourse relations as features for use in classifier training (Murray et al., 2006; Galley, 2006; McKeown et al., 2007; Bui et al., 2009).",
      "startOffset" : 199,
      "endOffset" : 274
    }, {
      "referenceID" : 1,
      "context" : "To date, most efforts to leverage discourse information to detect salient content from dialogues have focused on encoding gold-standard discourse relations as features for use in classifier training (Murray et al., 2006; Galley, 2006; McKeown et al., 2007; Bui et al., 2009).",
      "startOffset" : 199,
      "endOffset" : 274
    }, {
      "referenceID" : 28,
      "context" : "However, automatic discourse parsing in dialogues is still a challenging problem (Perret et al., 2016).",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 24,
      "context" : "This discussion is annotated with discourse structure based on the Twente Argumentation Schema (TAS) by Rienks et al. (2005), which focuses on argumentative discourse information.",
      "startOffset" : 104,
      "endOffset" : 125
    }, {
      "referenceID" : 30,
      "context" : "Specifically, we utilize argumentative discourse relations as defined in Twente Argument Schema (TAS) (Rienks et al., 2005), where discussions are organized into tree structures with discourse relations labeled between nodes (as shown in Figure 1).",
      "startOffset" : 102,
      "endOffset" : 123
    }, {
      "referenceID" : 35,
      "context" : "We envision that the extracted salient phrases by our model can be used as input to abstractive meeting summarization systems (Wang and Cardie, 2013; Mehdad et al., 2014).",
      "startOffset" : 126,
      "endOffset" : 170
    }, {
      "referenceID" : 21,
      "context" : "We envision that the extracted salient phrases by our model can be used as input to abstractive meeting summarization systems (Wang and Cardie, 2013; Mehdad et al., 2014).",
      "startOffset" : 126,
      "endOffset" : 170
    }, {
      "referenceID" : 11,
      "context" : ", 2006) and the ICSI corpus (Janin et al., 2003).",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 17,
      "context" : "Results evaluated by ROUGE (Lin and Hovy, 2003) demonstrate that our system summaries obtain a ROUGE-SU4 F1 score of 21.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 23,
      "context" : "Moreover, since both content and discourse structure are critical for building shared understanding among participants (Mulder et al., 2002; Mercer, 2004), we further investigate whether our learned model can be utilized to predict the consistency among team members’ understanding of their group decisions.",
      "startOffset" : 119,
      "endOffset" : 154
    }, {
      "referenceID" : 22,
      "context" : "Moreover, since both content and discourse structure are critical for building shared understanding among participants (Mulder et al., 2002; Mercer, 2004), we further investigate whether our learned model can be utilized to predict the consistency among team members’ understanding of their group decisions.",
      "startOffset" : 119,
      "endOffset" : 154
    }, {
      "referenceID" : 14,
      "context" : "Results on AMI discussions show that SVM classifiers trained with our features significantly outperform the state-of-the-art results (Kim and Shah, 2016) (F1: 63.",
      "startOffset" : 133,
      "endOffset" : 153
    }, {
      "referenceID" : 14,
      "context" : "This task is first defined as consistency of understanding (COU) by Kim and Shah (2016), who have labeled a portion of AMI discussions with consistency or inconsistency labels.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 20,
      "context" : "Our model is inspired by research work that leverages discourse structure for identifying salient content in conversations, which is still largely reliant on features derived from gold-standard discourse labels (McKeown et al., 2007; Murray et al., 2010; Bokaei et al., 2016).",
      "startOffset" : 211,
      "endOffset" : 275
    }, {
      "referenceID" : 24,
      "context" : "Our model is inspired by research work that leverages discourse structure for identifying salient content in conversations, which is still largely reliant on features derived from gold-standard discourse labels (McKeown et al., 2007; Murray et al., 2010; Bokaei et al., 2016).",
      "startOffset" : 211,
      "endOffset" : 275
    }, {
      "referenceID" : 0,
      "context" : "Our model is inspired by research work that leverages discourse structure for identifying salient content in conversations, which is still largely reliant on features derived from gold-standard discourse labels (McKeown et al., 2007; Murray et al., 2010; Bokaei et al., 2016).",
      "startOffset" : 211,
      "endOffset" : 275
    }, {
      "referenceID" : 5,
      "context" : "Due to the noisy nature of dialogues, recent work focuses on identifying summary-worthy phrases from meetings (Fernández et al., 2008; Riedhammer et al., 2010) or email threads (Loza et al.",
      "startOffset" : 110,
      "endOffset" : 159
    }, {
      "referenceID" : 29,
      "context" : "Due to the noisy nature of dialogues, recent work focuses on identifying summary-worthy phrases from meetings (Fernández et al., 2008; Riedhammer et al., 2010) or email threads (Loza et al.",
      "startOffset" : 110,
      "endOffset" : 159
    }, {
      "referenceID" : 19,
      "context" : ", 2010) or email threads (Loza et al., 2014).",
      "startOffset" : 25,
      "endOffset" : 44
    }, {
      "referenceID" : 33,
      "context" : "For the area of discourse analysis in dialogues, a significant amount of work has been done in predicting local discourse structures, such as recognizing dialogue acts or social acts of adjacent utterances from phone conversations (Stolcke et al., 2000; Kalchbrenner and Blunsom, 2013; Ji et al., 2016), spoken meetings (Dielmann and Renals, 2008), or emails (Cohen et al.",
      "startOffset" : 231,
      "endOffset" : 302
    }, {
      "referenceID" : 13,
      "context" : "For the area of discourse analysis in dialogues, a significant amount of work has been done in predicting local discourse structures, such as recognizing dialogue acts or social acts of adjacent utterances from phone conversations (Stolcke et al., 2000; Kalchbrenner and Blunsom, 2013; Ji et al., 2016), spoken meetings (Dielmann and Renals, 2008), or emails (Cohen et al.",
      "startOffset" : 231,
      "endOffset" : 302
    }, {
      "referenceID" : 12,
      "context" : "For the area of discourse analysis in dialogues, a significant amount of work has been done in predicting local discourse structures, such as recognizing dialogue acts or social acts of adjacent utterances from phone conversations (Stolcke et al., 2000; Kalchbrenner and Blunsom, 2013; Ji et al., 2016), spoken meetings (Dielmann and Renals, 2008), or emails (Cohen et al.",
      "startOffset" : 231,
      "endOffset" : 302
    }, {
      "referenceID" : 4,
      "context" : ", 2016), spoken meetings (Dielmann and Renals, 2008), or emails (Cohen et al.",
      "startOffset" : 25,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : ", 2016), spoken meetings (Dielmann and Renals, 2008), or emails (Cohen et al., 2004).",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 7,
      "context" : "Although discourse information from non-adjacent turns has been studied in the context of online discussion forums (Ghosh et al., 2014) and meetings (HakkaniTur, 2009), none of them models the effect of discourse structure on content selection, which is a",
      "startOffset" : 115,
      "endOffset" : 135
    }, {
      "referenceID" : 0,
      "context" : ", 2010; Bokaei et al., 2016). There is much less work that jointly predicts the importance of content along with the discourse structure in dialogus. Oya and Carenini (2014) employs Dynamic Conditional Random Field to recognize sentences in email threads for use in summary as well as their dialogue acts.",
      "startOffset" : 8,
      "endOffset" : 174
    }, {
      "referenceID" : 30,
      "context" : "In this work, we consider the argumentative discourse structure by Twente Argument Schema (TAS) (Rienks et al., 2005).",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 30,
      "context" : "Following previous work on discourse analysis in meetings (Rienks et al., 2005; Hakkani-Tur, 2009), we assume that the attachment structure between discourse units are given during both training and testing.",
      "startOffset" : 58,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "Following previous work on discourse analysis in meetings (Rienks et al., 2005; Hakkani-Tur, 2009), we assume that the attachment structure between discourse units are given during both training and testing.",
      "startOffset" : 58,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : "We obtain constituent and dependency parses for utterances using Stanford parser (Klein and Manning, 2003).",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 31,
      "context" : "2 Joint Learning for Parameter Estimation For learning the model parameters w, we employ an algorithm based on SampleRank (Rohanimanesh et al., 2011), which is a stochastic structure learning method.",
      "startOffset" : 122,
      "endOffset" : 149
    }, {
      "referenceID" : 32,
      "context" : "Message-passing (Smith and Eisner, 2008) is used to find the best d.",
      "startOffset" : 16,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "For modeling the salience of content, we calculate the minimum, maximum, and average of TF-IDF scores of words and number of content words in each phrase based on the intuition that important phrases tend to have more content words with high TF-IDF scores (Fernández et al., 2008).",
      "startOffset" : 256,
      "endOffset" : 280
    }, {
      "referenceID" : 34,
      "context" : "Previous work (Wang and Cardie, 2012) has found that a discussion usually ends with decision-relevant information.",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : "collect the dialogue act types of the current unit and its parent node in discourse tree, whether there is any adjacency pair held between the two nodes (Hakkani-Tur, 2009), and the Jaccard similarity between them.",
      "startOffset" : 153,
      "endOffset" : 172
    }, {
      "referenceID" : 11,
      "context" : ", 2006) and the ICSI meeting corpus (Janin et al., 2003).",
      "startOffset" : 36,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : "Previous work has shown that Support Vector Machines (SVMs)-based classifiers achieve state-of-the-art performance for keyphrase selection in meetings (Fernández et al., 2008; Wang and Cardie, 2013) and discourse parsing for formal text (Hernault et al.",
      "startOffset" : 151,
      "endOffset" : 198
    }, {
      "referenceID" : 35,
      "context" : "Previous work has shown that Support Vector Machines (SVMs)-based classifiers achieve state-of-the-art performance for keyphrase selection in meetings (Fernández et al., 2008; Wang and Cardie, 2013) and discourse parsing for formal text (Hernault et al.",
      "startOffset" : 151,
      "endOffset" : 198
    }, {
      "referenceID" : 10,
      "context" : ", 2008; Wang and Cardie, 2013) and discourse parsing for formal text (Hernault et al., 2010).",
      "startOffset" : 69,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : ", 2006) and the ICSI meeting corpus (Janin et al., 2003). AMI corpus consists of 139 scenario-driven meetings, and ICSI corpus contains 75 naturally occurring meetings. Both of the corpora are annotated with dialogue acts, adjacency pairs, and topic segmentation. We treat each topic segment as one discussion, and remove discussions with less than 10 turns or labeled as “opening”. 694 discussions from AMI and 1139 discussions from ICSI are extracted, and these two datasets are henceforth referred as AMI-FULL and ICSI-FULL. Acquiring Gold-Standard Labels. Both corpora contain human constructed abstractive summaries and extractive summaries on meeting level. Short abstracts, usually in one sentence, are constructed by meeting participants — participant summaries, and external annotators — abstractive summaries. Dialogue acts that contribute to important output of the meeting, e.g. decisions, are identified and used as extractive summaries, and some of them are also linked to the corresponding abstracts. Since the corpora do not contain phrase-level importance annotation, we induce gold-standard labels for candidate phrases based on the following rule. A candidate phrase is considered as a positive sample if its head word is contained in any abstractive summary or participant summary. Furthermore, a subset of discussions in AMIFULL are annotated with discourse structure and relations based on Twente Argumentation Schema (TAS) by Rienks et al. (2005)2.",
      "startOffset" : 37,
      "endOffset" : 1470
    }, {
      "referenceID" : 17,
      "context" : "We calculate scores based on ROUGE (Lin and Hovy, 2003), which is a popular tool for evaluating text summarization (Gillick et al.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "We calculate scores based on ROUGE (Lin and Hovy, 2003), which is a popular tool for evaluating text summarization (Gillick et al., 2009; Liu and Liu, 2010).",
      "startOffset" : 115,
      "endOffset" : 156
    }, {
      "referenceID" : 18,
      "context" : "We calculate scores based on ROUGE (Lin and Hovy, 2003), which is a popular tool for evaluating text summarization (Gillick et al., 2009; Liu and Liu, 2010).",
      "startOffset" : 115,
      "endOffset" : 156
    }, {
      "referenceID" : 29,
      "context" : "Following previous work on meeting summarization (Riedhammer et al., 2010; Wang and Cardie, 2013), we consider two dialogue act-level summarization baselines: (1) LONGEST DA in each discussion is selected as the summary, and (2) CENTROID DA, the one with the highest TF-IDF similarity with all DAs in the discussion.",
      "startOffset" : 49,
      "endOffset" : 97
    }, {
      "referenceID" : 35,
      "context" : "Following previous work on meeting summarization (Riedhammer et al., 2010; Wang and Cardie, 2013), we consider two dialogue act-level summarization baselines: (1) LONGEST DA in each discussion is selected as the summary, and (2) CENTROID DA, the one with the highest TF-IDF similarity with all DAs in the discussion.",
      "startOffset" : 49,
      "endOffset" : 97
    }, {
      "referenceID" : 26,
      "context" : "The third feature is based on word entrainment, which has been shown to correlate with task success for groups (Nenkova et al., 2008).",
      "startOffset" : 111,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "In this section, we test whether our joint model can be utilized to predict the consistency among team members’ understanding of their group decisions, which is defined as consistency of understanding (COU) in Kim and Shah (2016). Kim and Shah (2016) establish gold-standard COU labels on a portion of AMI discussions, by comparing participant summaries to determine whether participants report the same decisions.",
      "startOffset" : 210,
      "endOffset" : 230
    }, {
      "referenceID" : 14,
      "context" : "In this section, we test whether our joint model can be utilized to predict the consistency among team members’ understanding of their group decisions, which is defined as consistency of understanding (COU) in Kim and Shah (2016). Kim and Shah (2016) establish gold-standard COU labels on a portion of AMI discussions, by comparing participant summaries to determine whether participants report the same decisions.",
      "startOffset" : 210,
      "endOffset" : 251
    }, {
      "referenceID" : 14,
      "context" : "In this section, we test whether our joint model can be utilized to predict the consistency among team members’ understanding of their group decisions, which is defined as consistency of understanding (COU) in Kim and Shah (2016). Kim and Shah (2016) establish gold-standard COU labels on a portion of AMI discussions, by comparing participant summaries to determine whether participants report the same decisions. If all decision points are consistent, the associated topic discussion is labeled as consistent; otherwise, the discussion is identified as inconsistent. Their annotation covers the AMI-SUB dataset. Therefore, we run the prediction experiments on AMI-SUB by using the same annotation.3 Out of total 129 discussions in AMI-SUB, 86 discussions are labeled as consistent and 43 are inconsistent. We construct three types of features by using our model’s predicted labels. Firstly, we learn two versions of our model based on the “consistent” discussions and the “inconsistent” ones in the training set, with learned parameters wcon and wincon. For a discussion in the test set, these two models output two probabilities pcon = maxc,d P (c,d|x,wcon) and pincon = maxc,d P (c,d|x,wincon). We use pcon − pincon as a feature. Furthermore, we consider discourse relations of length one and two from the discourse structure tree. Intuitively, some discourse relations, e.g., ELABORATION followed by several POSITIVE implied consistent understanding. The third feature is based on word entrainment, which has been shown to correlate with task success for groups (Nenkova et al., 2008). Using the formula in Nenkova et al. (2008), we compute the average word entrainment between the main speaker who utters the most words and all the other participants.",
      "startOffset" : 210,
      "endOffset" : 1634
    }, {
      "referenceID" : 14,
      "context" : "Results that statistically significantly outperform ngrams-based baseline and Kim and Shah (2016) are highlighted with ∗ (p < 0.",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "Finally, we compare with the state-of-the-art method in Kim and Shah (2016), where discourse-relevant features and head gesture features are utilized in Hidden Markov Models to predict the consistency label.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "Finally, we compare with the state-of-the-art method in Kim and Shah (2016), where discourse-relevant features and head gesture features are utilized in Hidden Markov Models to predict the consistency label. The results are displayed in Table 5. All SVMs trained with our features surpass the ngrams-based baseline. Especially, the discourse features, word entrainment feature, and the combination of the three, all significantly outperform the state-of-theart system by Kim and Shah (2016).",
      "startOffset" : 56,
      "endOffset" : 491
    } ],
    "year" : 2017,
    "abstractText" : "We present a joint modeling approach to identify salient discussion points in spoken meetings as well as to label the discourse relations between speaker turns. A variation of our model is also discussed when discourse relations are treated as latent variables. Experimental results on two popular meeting corpora show that our joint model can outperform SVM-based classifiers for both phrase-based content selection and discourse relation prediction tasks. We also evaluate our model on predicting the consistency among team members’ understanding of their group decisions. Classifiers trained with features constructed from our model achieve significant better predictive performance than the state-of-the-art.",
    "creator" : "LaTeX with hyperref package"
  }
}