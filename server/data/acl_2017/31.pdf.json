{
  "name" : "31.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Event Factuality Identification via Deep Neural Networks",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Event factuality is the information expressing the commitment of relevant sources towards the factual nature of events. That is, event factuality conveys whether an event is characterized as a fact, a certain possibility, or an impossible situation, and thus is helpful for various NLP applications, e.g., opinion detection (Wiebe et al., 2005), QA (Pustejovsky et al., 2005), textual entailment (de Marneffe et al., 2006; Hickl and Bensley, 2007), rumor identification (Qazvinian et al., 2011).\nIn principle, the value of event factuality is related to some elements, e.g., predicates, speculative and negative cues. Consider following sentences as examples1:\n(S1) McCulley, a famous economist, doubts that the tax rate will increase soon.\n(S2) He knows they are not able to go to the village due to the flood.\nIn sentence S1, the event increase is a possibility according to the predicate doubts, while in\n1In this paper, events are in bold and sources are underlined in example sentences.\nS2, the event go is regarded as an impossible situation due to the negation word not. From these sentences, we can learn that predicates and cues can determine the event factuality to a great degree. Such event factuality provides the way to distinguish fact events from speculative and negative ones.\nTraditional methods for event factuality identification usually employed either hand-crafted rules (Saurı́, 2008; Saurı́ and Pustejovsky, 2012) relying on expert knowledge, or features (Prabhakaran et al., 2010; de Marneffe et al., 2012) relying on various kinds of annotated information, such as source introducing predicates, predicate classes, speculative and negative cues. Due to the recent success of deep learning on various NLP tasks in learning useful representations from ordinary sentences (Socher et al., 2012; Zeng et al., 2014; Cheng et al., 2016) and syntactic paths (Xu et al., 2015a,b; Roth and Lapata, 2016) and the attention mechanism on capturing the focus change in sequence modeling (Chen et al., 2016; Wang et al., 2016; Zhou et al., 2016), this paper proposes a framework to identify event factuality in raw texts with neural networks. Our main contributions can be summarized as follows:\n1) the proposal of a two-step supervised framework for identifying event factuality in raw texts.\n2) the utilization of an attention-based CNN to detect source introducing predicates (SIPs), the most important factors to identify event factuality.\n3) the proposal of an attention-based deep neural network model with a proper combination of BiLSTM and CNN to identify the factuality of events."
    }, {
      "heading" : "2 Background",
      "text" : "This section presents the concepts and basic factors of event factuality.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199"
    }, {
      "heading" : "2.1 Event Factuality",
      "text" : "Factuality can be characterized by the combination of two dimensions: epistemic modality and polarity (Saurı́, 2008). While modality conveys the certainty degree of events, such as certain (CT), probable (PR), possible (PS) and underspecified (U), polarity expresses whether the event happened, including positive (+), negative (-) and underspecified (u). Table 1 shows various values of event factuality. For example, CT+ means it is certain that the event happened, while PR- denotes it is probable that the event did not happen. Notice that some values are non-application (NA) grammatically, e.g., PRu, PSu, U+/-.\n+ - u CT CT+ CT- CTu PR PR+ PR- (NA) PS PS+ PS- (NA) U (NA) (NA) Uu\nTable 1: Various values of event factuality."
    }, {
      "heading" : "2.2 Basic Factors",
      "text" : "Considering that FactBank is built on top of TimeBank (Pustejovsky et al., 2003b), we adopt the definition of events proposed by TimeML (Pustejovsky et al., 2003a) from a more grammatical and generalized perspective. We consider the events which can be critical for computing the factuality. Such events can be detected by feature-based methods (Chambers, 2013).\nIn the literature, three kinds of factors play critical roles in event factuality identification: Source Introducing Predicate (SIP), source and cue.\nSIP. Source Introducing Predicates (SIPs) are events that can not only introduce additional sources to assess the factuality of embedded events, but also influence their factuality. For example, the SIP doubts introduces McCulley as a new source in S1, and McCulley evaluates event increase as a possibility according to doubts.\nSource. By default, AUTHOR is considered the only source if the sentence contains no SIPs. Further sources can be incorporated by SIPs. In S1, the event increase has two sources: AUTHOR and McCulley. The factuality value is not the inherent property of the event but always relevant to the sources. McCulley commits to the event increase as a possibility, while AUTHOR remains uncommitted. Strictly speaking, we as readers know about McCulley’s perspective only accord-\ning to AUTHOR. Hence, we appeal to the notion of the nested source structure (Wiebe et al., 2005) and represent the sources in the chain form: McCulley AUTHOR, which means McCulley is Embedded in AUTHOR. Only AUTHOR can be displayed by a simple source.\nCues are words that have speculative or negative meanings. The modality of a Non-Uu event is PR/PS if this event is modified by a PR/PS cue, while the polarity of a Non-Uu event is negative if this event is modified by a negative (NEG) cue. So cues are key signals to identify the event factuality. For instance, the factuality of event go in S1 is CT- due to the NEG cue not."
    }, {
      "heading" : "3 Baseline",
      "text" : "As a baseline, we present a two-step framework shown in Figure 1 to identify event factuality in raw texts. Firstly, various useful factors, such as SIPs, sources, and cues are extracted from raw texts. Then, a classification algorithm is employed to identify event factuality."
    }, {
      "heading" : "3.1 Event and Cue Detection",
      "text" : "For event detection task, we only consider nouns, verbs, adjectives as the candidate tokens to reduce the complexity, and we utilize a maximum entropy classification model with the features used by Chambers (2013).\nØvrelid et al. (2010) and Velldal et al. (2012) concluded that syntactic features are not necessary and lexical sequence-oriented n-gram features can achieve excellent performances on cue detection\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\ntask. Hence, we employ the lexical features developed by Velldal et al. (2012) to classify each token into PR cue, PS cue, Neg cue, or not cue."
    }, {
      "heading" : "3.2 SIP Detection",
      "text" : "Whether an event is a SIP has relation to its semantic information. For instance, verbs state, say, tell are usually used to express opinions which are promoted by certain participants of events, so they can all introduce additional sources. Therefore, they are in the same synonym set {state, say, tell} and share the same hypernym in WordNet2. Hence, we consider token, part-of-speech (POS) and hypernym of the token as lexical level features, and concatenate them into the vector l.\nAccording to the definition of SIPs, if there is no event in the syntactic scope of a word, this word is not a SIP. For example:\n(S3) Tom suggests we should have a rest now. In S3, although the event suggests can introduce a new source Tom for the clause “we should have a rest now”, however, this clause expresses a suggestion and is not an event. So the event suggests is NOT a SIP in S3. This example demonstrates the structure of a sentence is also able to determine whether a token is a SIP. Therefore, we also consider the sentence level feature. Instead of original sentences, we propose the following Pruned Sentence structure:\nPruned Sentence (PSen): If a clause of the candidate token contains events, this clause is replaced by the tag 〈event〉; Nouns, pronouns and the current candidate token are unchanged, while other tokens are replaced by the tag 〈O〉.\nWe argue that PSen can characterize clearly whether there are events in the clause of the token. Besides, in PSen only the candidate token and the tokens that are possible to be new sources are reserved, and effects of other tokens are omitted.\nFor example, in S4, the current candidate event says governs the clause “the manager will attend a meeting later”, which contains the event attend. Hence this clause is replaced by the tag 〈event〉. While the tokens which are impossible to be new sources are replaced by 〈O〉. Sentence S5 is the PSen structure of S4.\n(S4) Tom, who is the secretary of the manager, says the manager will attend a meeting later .\n(S5) Tom 〈O〉 who 〈O〉 〈O〉 secretary 〈O〉 〈O〉 manager 〈O〉 says 〈event〉 〈O〉\n2http://wordnet.princeton.edu\nConsidering that PSen is a simplified and pruned structure, we extract sentence level features through an attention-based CNN illustrated in Figure 2 instead of RNN model. A pruned sentence S = (t0, t1, . . . , tn−1) is transferred to a matrix X0 ∈ Rd0×n according to pre-trained word embeddings. Our CNN is computed as follows:\nY1 =WcX0 + bc (1)\nYm = tanh (Y1) (2)\nα = softmax(vTc Ym) (3) c = tanh (Y1α T ) (4)\nwhere Wc ∈ Rnc×d0 , bc,vc, c ∈ Rnc . The lexical feature l includes the embeddings of the token, the POS tag and the hypernym with the dimension of d0, dpos and dhyp, respectively. We concatenate c and l into f0 = [lT , cT ]T for each token. Finally, f0 is fed into the softmax layer:\no = softmax(Ws0f0 + bs0) (5)\nwhere o ∈ R2. To train the network, we exploit the following objective function:\nJ(θ) = − 1 m m−1∑ i=0 log p(y(i)|x(i), θ)+ λ 2 ‖θ‖2 (6)\nwhere p(y(i)|x(i), θ) is the confidence score of the golden label y(i) of the training instance x(i), m is the number of the training instances, λ is the regularization coefficient and θ is the parameter set.\nThe grammatical subjects of SIPs are chosen as the New Sources introduced by corresponding\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nSIPs. Since we have identified events and SIPs, we identify relevant sources RS, which is initialized as RS = {AUTHOR}, for each event according to the definition. When traversing from the root of the dependency parse tree of the sentence to the current event, RS is updated as:\nRSn = RSn−1 ∪ {ns s} (7)\nwhere ns is a new source introduced by the corresponding SIP and s ∈ RSn−1. This is a recursive algorithm defined by Saurı́ (2008)."
    }, {
      "heading" : "4 Deep Neural Network Framework",
      "text" : "This section describes our neural networks for event factuality identification in details shown in Figure 3. The inputs are developed according to the outputs of Section 3. We learn effective representations from Shortest Dependency Paths (SDP) by means of neural networks with attention. Different from previous studies, our model combines both BiLSTM and CNN properly which can learn meaningful features from SDPs and words, respectively.\nSIP_Path RS_Path Cue_Path Auxiliary\nWords\nBiLSTM CNN\nsoftmax(Ws1fu+bs1)\nInput Layer\nNeural Network\nLayer\nSoftmax\nLayer\nsoftmax(Ws2fcue+bs2)\nFeature Layer\nLexical Features\nlp lt\nfu fcue\nFigure 3: The architecture of our neural networks for event factuality identification.\nOur model has two outputs: one represents whether the event is Uu, Non-Uu or other, and the other shows whether the event is modified by the cue and then Non-Uu events are further classified into CT+/-, PR+/-, PS+/-. We have two main reasons for the design of the two outputs: 1) We can identify negative and speculative values (e.g., CT-, PR+/-, PS+/-) more precisely with the assistance of cues, and 2) this design can tackle the imbalance among instances, because the negative and speculative factuality values are usually in the minority. Finally, the event factuality is determined by these two outputs directly."
    }, {
      "heading" : "4.1 Inputs",
      "text" : "We develop the following inputs for our neural network model:\nRelevant Source Path (RS Path): The SDP from the root of the dependency tree to the relevant sources of the event. Notice that this path contains all the sources in the chain form.\nSIP Path: The SDP from the SIP that introduces the current source to the event.\nCue Path: The SDP from the cue to the event. In addition, we consider the Lexical Features to judge whether the event is modified by the cue: 1) Relative Position is defined as the distance from the cue to the event in the sentence, and is mapped into the vector lp with the dimension dp; 2) Type of Cue includes PR, PS and NEG and is mapped into the vector lt with the dimension dt.\nAuxiliary Words (Aux Words) include auxiliary and marker words that share the dependency relations aux or mark with the current event. Here, a marker is the word introducing a finite clause subordinate to the event. We argue that Aux Words can describe the syntactic structures of sentences.\nAn example sentence and its inputs for our neural networks are shown in Figure 4."
    }, {
      "heading" : "4.2 LSTM with Attention",
      "text" : "Traditional RNNs have problems called vanishing gradients during the gradient back-propagation phase. Overcoming these problems is the motivation behind the LSTM model (Hochreiter and Schmidhuber, 1997) which introduces a memory cell including an input gate it, a neuron with a self-recurrent connection ct, a forget gate ft and an output gate ot. The values of components depend on the previous state ht−1 and the current input xt. Formally, LSTM is computed according to the following equations, where xt ∈ X , X ∈ Rd0×n is the matrix of the SDP:\nit = σ(Wixt +Uiht−1 + bi) (8)\nft = σ(Wfxt +Ufht−1 + bf ) (9)\not = σ(Woxt +Uoht−1 + bo) (10)\nc̃t = tanh(Wcxt +Ucht−1 + bc) (11)\nct = ft ct−1 + it c̃t (12) ht = ot tanh(ct) (13)\nwhere is the element-wise multiplication and σ is the sigmoid function. The dimension of hidden units in LSTM is set as d0. We can obtain H ∈ Rd0×n via the LSTM layer, where H = (h0,h1, . . . ,hn−1), ht ∈ Rd0(0 ≤ t ≤ n− 1).\nMany sequence modeling tasks are beneficial from the access to the future as well as past con-\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nmark\nThe journalist reports that a famous economist claims that there is a high possibility that the economy will recover soon .\nnsubj\ndebt\nROOT\nroot\nmark\ndet\namod nsubj\nccomp\nexpl\nccomp\ndet\namod\nnsubj\nmark\ndet nsubj\naux\nccomp\nadvmod\nCurrent event: recover Current Relevant Source:\neconomist_journalist_AUTHOR\nCurrent SIP: claims Current cue: possibility\nRS_Path: “ROOT root reports nsubj journalist nsubj reports ccomp claims nsubj economist” SIP_Path: “claims ccomp is ccomp recover” Cue_Path: “possibility nsubj is ccomp recover”\nRelative Position from cue to event: -5 Type of Cue: PS Aux_Words: “that will”\nFigure 4: An example sentence and its inputs.\nLSTM LSTM LSTM LSTM LSTM\nLSTM LSTM LSTM LSTM LSTM\nInput Layer\nLSTM Layer\nAttention\nLayer\nOutput Layer\nx0 x1 x2 x3 x4\nh0 h1 h2 h3 h4\n×α0 ×α1 ×α2 ×α3 ×α4\ntanh\nFigure 5: The bidirectional LSTM network with attention mechanism.\ntext. Therefore, to model the representations of syntactic paths, we utilize the bidirectional LSTM network shown in Figure 5 which processes the syntactic path in both directions. It produces the forward hidden sequence ~H , the backward hidden sequence ~H and the output sequenceHp:\nHp = ~H + ~H (14)\nTo capture the most important information from the syntactic path effectively, we adopt the attention mechanism and get the output hp:\nM = tanh(Hp) (15)\nα = softmax(vTM) (16)\nhp = tanh(Hpα T ) (17)"
    }, {
      "heading" : "4.3 Outputs",
      "text" : "The feature representations hsp, hrp and hcp are extracted from SIP Path, RS Path and Cue Path,\nrespectively. Noticing that the auxiliary words are just a collection of tokens instead of a sequence with specific meanings, we employ the CNN in Section 3, i.e., Equation (1) to (4), to learn the representations of auxiliary words:\nfw = CNN(Xaux words) (18)\nWe concatenate hsp, hrp and fw into fu to judge whether the event is Uu, Non-Uu or other:\nfu = [h T sp,h T rp,f T w ] T (19)\nTo determine whether a Non-Uu event is governed by a cue, we consider not only hcp but also the lexical features lp (Relative Position) and lt (Type of Cue) described above:\nfcue = [l T p , l T t ,h T cp] T (20)\nFinally, these feature representations are fed into the softmax layer:\no1 = softmax(Ws1fu + bs1) (21)\no2 = softmax(Ws2fcue + bs2) (22)\nwhereWs1, bs1,Ws2, bs2 are the parameters. The dimension of o1 is 3, which is equal to the number of labels representing whether the event is Uu, Non-Uu or other (label y1). While o2 is used to determine whether the event is governed by the cue, and Non-Uu events are classified into CT+/-, PR+/-, PS+/- according to o2. The dimension of o2 is 3 (label y2), considering that a sentence may have no cues. The objective function with l2-norm is designed as:\nJ(θ) = [− 1 m m−1∑ i=0 log p(y (i) 1 |x (i), θ)]+ (23)\n(1− )[− 1 m m−1∑ i=0 log p(y (i) 2 |x (i), θ)] + λ 2 ‖θ‖2\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nwhere given the training instance (x, y1) and (x, y2), p(y (i) 1 |x(i), θ) and p(y (i) 2 |x(i), θ) are the confidence scores of the golden label y1 and y2 in o1 and o2, respectively."
    }, {
      "heading" : "5 Experimentation",
      "text" : "This section first introduces experimental settings and then gives the detailed results and analysis of event factuality identification."
    }, {
      "heading" : "5.1 Experimental Settings",
      "text" : "We evaluate our models on FactBank (Saurı́ and Pustejovsky, 2009), which contains 3864 sentences, 9492 events and 13506 factuality values. Table 2 presents the distribution of factuality values in FactBank. Following previous studies (Saurı́, 2008; de Marneffe et al., 2012), we only take into account the five main categories of values, i.e., CT+, CT-, PR+, PS+, and Uu, which make up 99.05% of all the instances.\nCounts Percentage(%) CT+ 7749 53.37 CT- 433 3.21 PR+ 363 2.69 PS+ 226 1.67 Uu 4607 34.11\nOther 128 0.95 Total 13506 100\nTable 2: Distribution of factuality values in FactBank.\nWe divide the corpus into 5 folds to perform 5-fold cross-validation. Precision, Recall, and F1-measure are employed to report the performance of each factuality value. To obtain the performance for the whole corpus, macro-averaging and micro-averaging are applied (Manning and Schütze, 2001). We choose two-sample two-tailed t-test for significance test.\nFor the SIP detection task, we set d0 = 100, dpos = dhyp = 50, nc = 150. For the event factuality identification task, we set d0 = 100, dp = dt = 10, nc = 50, = 0.25. We initialize word embeddings via Word2Vec (Mikolov et al., 2013) and randomly initialize all the other parameters. The Stochastic Gradient Descent with momentum is applied to optimize our models.\nIf there is more than one cue in the sentence, we consider whether the event is modified by each cue separately. If a Non-Uu event is affected by both PR and PS cues, we adopt the cue with the\nhighest confidence score in o2 to decide whether the modality of the event is PR or PS.\nWe employ the following baselines, whose inputs are the outputs of our basic factor extraction for the fair comparison with our model:\nRules: The rule-based model developed by Saurı́ (2008) is a top-down algorithm traversing a dependency parse tree.\nMaxEnt: We employ a maximum entropy classification model using the features developed by de Marneffe et al. (2012). Some features are not available (i.e., predicate classes and general classes of events) because they rely on annotated information which are not outputs of our basic factor extraction tasks.\nCNN+CNN: This is a variant of our model whose LSTM is replaced by the CNN in Section 3 with the size of convolutional layer nc = 100."
    }, {
      "heading" : "5.2 Results and Analysis: Basic Factor Extraction",
      "text" : "Table 3 presents the performances of basic factor extraction tasks. It is worth noting that a SIP is correctly identified means both this SIP and the new source introduced by it are correctly detected.\nP(%) R(%) F Event 86.67 82.86 84.68 SIP 73.98 73.01 73.42 Source of events 79.30 76.53 77.82\nCue All 66.52 65.68 66.04 NEG 72.13 75.13 73.48 PR 55.92 47.97 51.36 PS 62.41 65.90 63.47\nTable 3: The performances of basic factor extraction tasks.\nWe get a better performance on event detection task (F1=84.68) than Chambers (2013) (F1=80.30). Remember that we only consider nouns, verbs, adjectives, and rule out the effects of other tokens. We also employ the features developed by Chambers (2013) to identify SIPs and obtain F1=72.56, while our attention-based CNN for SIP detection achieves a higher F1=73.42, proving that our CNN is beneficial from pruned sentences (PSen) structures. Although the improvement of F1 is slight, however, we argue that one SIP can determine all the sources of events embedded in it. Therefore, we adopt the attention-based CNN model for SIP detection.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nSystems Sources CT+ CT- PR+ PS+ Uu Micro-A Macro-A\nRules All 61.83 54.52 20.75 39.89 26.08 50.71 40.62\nAuthor 64.83 48.83 13.35 31.93 26.17 53.49 37.02 Embed 53.19 61.94 25.11 47.01 25.95 44.20 42.64\nMaxEnt All 64.37 47.57 35.74 17.08 58.86 61.00 44.87\nAuthor 70.93 48.36 28.25 14.67 67.19 68.41 50.30 Embed 50.61 45.78 38.49 24.97 24.26 43.69 36.82\nCNN+CNN(Avg) All 60.15 51.53 32.88 34.06 56.67 57.66 47.06\nAuthor 66.11 54.67 29.25 34.67 63.44 64.14 49.69 Embed 47.88 48.35 34.92 36.49 27.09 42.59 38.95\nCNN+CNN(Att) All 62.50 52.56 42.56 40.02 56.69 59.43 50.87\nAuthor 68.97 53.86 39.26 35.91 64.16 66.23 52.43 Embed 49.37 50.76 43.57 41.10 25.32 43.69 42.03\nBiLSTM+CNN(Avg) All 61.74 49.19 34.09 40.19 55.59 58.23 48.16\nAuthor 67.67 50.33 34.55 31.10 62.94 64.79 49.32 Embed 49.68 47.95 34.48 43.13 22.86 42.94 39.62\nBiLSTM+CNN(Att) All 64.17 53.89 41.83 42.97 58.46 61.06 52.26\nAuthor 70.16 55.39 36.84 31.75 65.90 67.59 52.01 Embed 51.68 52.51 43.39 47.45 26.26 45.81 44.26\nTable 4: The performances (F1-measures) of all the systems on event factuality identification. “Embed” are the sources in the chain form embedded in AUTHOR. “Avg” means using average pooling layers, and “Att” denotes the attention mechanism."
    }, {
      "heading" : "5.3 Results and Analysis: Models",
      "text" : "Table 4 displays the performances of various models on event factuality identification. CT-, PR+ and PS+ only cover 7.57% of all the factuality values. Hence, it is challenging to identify them. Compared with Rule-based and MaxEnt model, our attention-based neural network models achieve better results on CT-, PR+ and PS+. The higher macro-averaging indicates the performances are more balanced, and BiLSTM+CNN(Att) is superior to all the other baselines in terms of microand macro-averaging on All sources. However, if we concatenate fu and fcue in Equation (19)(20) into ONE vector and design only ONE output in BiLSTM+CNN(Att), we obtain good results on CT+ and Uu (F1=63.64 and 57.80), but results of CT-, PR+, PS+ are lower (F1=49.43, 33.06, 37.07). With the design of two outputs, BiLSTM+CNN(Att) achieves the highest F1 of PS+ (42.97) and the second highest F1 of CT-, PR+ (53.89 and 41.83), which can prove the advantages of the design of two outputs in our model and the usefulness of speculative and negative cues.\nParticularly, our BiLSTM model can obtain better results than CNN. Compared with the simple CNN which ignores the contexts, BiLSTM can learn features from both future and past contexts. Hence BiLSTM is able to ex-\ntract more effective representations from syntactic paths than CNN. For BiLSTM+CNN(Att) and CNN+CNN(Att) models, we get p < 0.05.\nThe performances of both CNN+CNN (p < 0.001) and BiLSTM+CNN (p < 0.05) models are improved after using attention, indicating that some factors (e.g., SIPs, cues, auxiliary words) in syntactic paths play the key roles in computing the factuality, while others have fewer effects. Therefore, attention is helpful to improve the results.\nThe F1 of BiLSTM+CNN(Att) on CT+ and Uu events whose sources are AUTHOR are slightly lower than those of MaxEnt model. CT+ and Uu values cover 95.45% among events with AUTHOR, so MaxEnt can also perform well on identifying them. But BiLSTM+CNN(Att) achieves higher F1 on CT- (p < 0.001), PR+ (p < 0.05) and PS+ (p < 0.001) on AUTHOR.\nHowever, F1 of Uu on embedded sources of BiLSTM+CNN(Att) is quite low (26.26), which is similar to other models. On one hand, Uu only covers 23.99% among events with embedded sources. On the other hand, the events embedded in other sources usually have complex syntactic structures. Therefore, it is difficult to judge whether these events are Uu or Non-Uu. Compared with MaxEnt model, however, BiLSTM+CNN(Att) achieves better performances on\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nCT-, PR+, PS+ events with AUTHOR (CT-, PS+: p < 0.001) and on all the five categories of values with embedded sources (p < 0.001). These results can prove the effectiveness of our model, especially cue-related features which are helpful to identify speculative and negative values."
    }, {
      "heading" : "5.4 Results and Analysis: Effects of Inputs",
      "text" : "We explore the effects of different inputs on the performances of our BiLSTM+CNN(Att) model and obtain the results in Figure 6. We can conclude that only RS Path is not sufficient to identify Uu and Non-Uu events. SIP Path can greatly improve the results (p < 0.001), proving that SIPs are key factors to determine the event factuality and can offer meaningful syntactic information to discriminate Non-Uu events from Uu ones. Finally, auxiliary words make further improvement on results, because they can reflect the pragmatic structures of sentences, just as described in Section 4. For example:\n(S6) He also is charged in the 1996 Olympic bombing, which killed one person.\n(S7) The environmental commission must adopt regulations to ensure people are not exposed to radioactive waste.\nIn S6, the auxiliary word “which” leads the relative clause “which killed one person” containing the CT+ event killed. Events in non-restrictive clauses are commonly evaluated as facts. In S7, event ensure is in the purpose clause and is presented as Uu grammatically.\nR: RS_Path S: SIP_Path AW: Auxiliary Words C: Cue_Info = Cue_Path + Lexical Features lp and lt\n0\n10\n20\n30\n40\n50\n60\n70\nR+C S+C S+R+C S+R+C+AW\nCT+ CT- PR+ PS+ Uu Micro-Ave Macro-Ave\nFigure 6: F1-measures of our BiLSTM+CNN(Att) model using different inputs."
    }, {
      "heading" : "6 Related Work",
      "text" : "Currently, only a few studies focused on event factuality identification. Diab et al. (2009) and Prabhakaran et al. (2010) presented a preliminary pilot study of belief annotation and automatic tagging,\nand limited the source to AUTHOR. They classified predicates into Committed Belief (CB), NonCB or Not Applicable under a supervised framework using lexical and syntactic features.\nSaurı́ (2008) proposed a rule-based top-down algorithm traversing dependency trees which can achieve excellent results (F1 of macro- and microaveraging are 73.59 and 85.45, respectively), and developed FactBank corpus (Saurı́ and Pustejovsky, 2009). Then de Marneffe et al. (2012) employed lexical, syntactic and pragmatic features to identify factuality of events in some sentences of FactBank. These studies utilized annotated events and relative factors directly.\nNeural networks, emerging in recent years, can not only extract effective features from sentences (Socher et al., 2012; Zeng et al., 2014; Zhou and Xu, 2015), but also capture important information from syntactic paths and improve performances of many NLP applications, such as relation classification (Xu et al., 2015a,b) and semantic role labeling (Roth and Lapata, 2016).\nMany recent researches show that attentionbased models can improve the performances of NLP tasks. Chen et al. (2016) built a hierarchical LSTM model with attention to generate sentence and document representations for sentiment classification. Zhou et al. (2016) utilized a bidirectional LSTM with attention to model sentences, and Wang et al. (2016) proposed a novel CNN relying on two levels of attention in terms of relation classification. In this paper we utilize attentionbased neural networks which consider both LSTM and CNN to learn effective features from syntactic paths and words, respectively."
    }, {
      "heading" : "7 Conclusions",
      "text" : "This paper presents a supervised framework to identify factuality of events from raw texts. We firstly extract events, SIPs, relevant sources and cues in texts, and then employ an attention-based neural network model combining BiLSTM and CNN for event factuality identification. Our model can identify negative and speculative factuality values more effectively with the help of corresponding cues. Experimental results show that our model can outperform the state-of-the-art ones. We will develop more useful features for basic factor extraction tasks and design better neural network models to improve the results of event factuality identification in the future work.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Navytime: Event and time ordering from raw text",
      "author" : [ "Nathanael Chambers." ],
      "venue" : "Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evalua-",
      "citeRegEx" : "Chambers.,? 2013",
      "shortCiteRegEx" : "Chambers.",
      "year" : 2013
    }, {
      "title" : "Neural sentiment classification with user and product attention",
      "author" : [ "Huimin Chen", "Maosong Sun", "Cunchao Tu", "Yankai Lin", "Zhiyuan Liu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory-networks for machine reading",
      "author" : [ "Jianpeng Cheng", "Li Dong", "Mirella Lapata." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computa-",
      "citeRegEx" : "Cheng et al\\.,? 2016",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to distinguish valid textual entailments",
      "author" : [ "Marie Catherine de Marneffe", "Bill Maccartney", "Trond Grenager", "Daniel Cer", "Anna Rafferty", "Christopher D Manning." ],
      "venue" : "Proceedings of the Second Pascal Challenges Workshop on Recognising",
      "citeRegEx" : "Marneffe et al\\.,? 2006",
      "shortCiteRegEx" : "Marneffe et al\\.",
      "year" : 2006
    }, {
      "title" : "Did it happen? the pragmatic complexity of veridicality assessment",
      "author" : [ "Marie Catherine de Marneffe", "Christopher D. Manning", "Christopher Potts." ],
      "venue" : "Computational Linguistics 38(2):301– 333. https://doi.org/10.1162/COLI a 00097.",
      "citeRegEx" : "Marneffe et al\\.,? 2012",
      "shortCiteRegEx" : "Marneffe et al\\.",
      "year" : 2012
    }, {
      "title" : "Committed belief annotation and tagging",
      "author" : [ "Mona Diab", "Lori Levin", "Teruko Mitamura", "Owen Rambow", "Vinodkumar Prabhakaran", "Weiwei Guo." ],
      "venue" : "Proceedings of the Third Linguistic Annotation Workshop. Association for Computa-",
      "citeRegEx" : "Diab et al\\.,? 2009",
      "shortCiteRegEx" : "Diab et al\\.",
      "year" : 2009
    }, {
      "title" : "A discourse commitment-based framework for recognizing textual entailment",
      "author" : [ "Andrew Hickl", "Jeremy Bensley." ],
      "venue" : "Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing. Association for",
      "citeRegEx" : "Hickl and Bensley.,? 2007",
      "shortCiteRegEx" : "Hickl and Bensley.",
      "year" : 2007
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation 9(8):1735–1780. https://doi.org/10.1162/neco.1997.9.8.1735.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Foundations of statistical natural language processing",
      "author" : [ "Christopher D. Manning", "Hinrich Schütze." ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Manning and Schütze.,? 2001",
      "shortCiteRegEx" : "Manning and Schütze.",
      "year" : 2001
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "Advances in Neural Information Processing Systems 26: 27th Annual Conference",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Syntactic scope resolution in uncertainty analysis",
      "author" : [ "Lilja Øvrelid", "Erik Velldal", "Stephan Oepen." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010). Coling 2010 Organizing Committee, Beijing, China, pages",
      "citeRegEx" : "Øvrelid et al\\.,? 2010",
      "shortCiteRegEx" : "Øvrelid et al\\.",
      "year" : 2010
    }, {
      "title" : "Automatic committed belief tagging",
      "author" : [ "Vinodkumar Prabhakaran", "Owen Rambow", "Mona Diab." ],
      "venue" : "Coling 2010: Posters. Coling 2010 Organizing Committee, Beijing, China, pages 1014–1022. http://www.aclweb.org/anthology/C10-2117.",
      "citeRegEx" : "Prabhakaran et al\\.,? 2010",
      "shortCiteRegEx" : "Prabhakaran et al\\.",
      "year" : 2010
    }, {
      "title" : "Timeml: Robust specification of event and temporal expressions in text",
      "author" : [ "James Pustejovsky", "José M. Castaño", "Robert Ingria", "Roser Saurı", "Robert J. Gaizauskas", "Andrea Setzer", "Graham Katz", "Dragomir R. Radev" ],
      "venue" : null,
      "citeRegEx" : "Pustejovsky et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Pustejovsky et al\\.",
      "year" : 2003
    }, {
      "title" : "The timebank corpus",
      "author" : [ "James Pustejovsky", "Patrick Hanks", "Roser Saur", "Andrew See", "Robert Gaizauskas", "Andrea Setzer", "Dragomir Radev", "Beth Sundheim", "David Day", "Lisa Ferro", "Marcia Lazo." ],
      "venue" : "Proceedings of Corpus Linguistics. Lancaster, UK,",
      "citeRegEx" : "Pustejovsky et al\\.,? 2003b",
      "shortCiteRegEx" : "Pustejovsky et al\\.",
      "year" : 2003
    }, {
      "title" : "Temporal and event information in natural language text",
      "author" : [ "James Pustejovsky", "Robert Knippen", "Jessica Littman." ],
      "venue" : "Language Resources and Evaluation 39(2):123–164. https://doi.org/10.1007/s10579-005-7882-7.",
      "citeRegEx" : "Pustejovsky et al\\.,? 2005",
      "shortCiteRegEx" : "Pustejovsky et al\\.",
      "year" : 2005
    }, {
      "title" : "Rumor has it: Identifying misinformation in microblogs",
      "author" : [ "Vahed Qazvinian", "Emily Rosengren", "Dragomir R. Radev", "Qiaozhu Mei." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Process-",
      "citeRegEx" : "Qazvinian et al\\.,? 2011",
      "shortCiteRegEx" : "Qazvinian et al\\.",
      "year" : 2011
    }, {
      "title" : "Neural semantic role labeling with dependency path embeddings",
      "author" : [ "Michael Roth", "Mirella Lapata." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Compu-",
      "citeRegEx" : "Roth and Lapata.,? 2016",
      "shortCiteRegEx" : "Roth and Lapata.",
      "year" : 2016
    }, {
      "title" : "A Factuality Profiler for Eventualities in Text",
      "author" : [ "Roser Saurı" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "Saurı́.,? \\Q2008\\E",
      "shortCiteRegEx" : "Saurı́.",
      "year" : 2008
    }, {
      "title" : "Factbank: a corpus annotated with event factuality. Language Resources and Evaluation 43(3):227–268",
      "author" : [ "Roser Saurı", "James Pustejovsky" ],
      "venue" : null,
      "citeRegEx" : "Saurı́ and Pustejovsky.,? \\Q2009\\E",
      "shortCiteRegEx" : "Saurı́ and Pustejovsky.",
      "year" : 2009
    }, {
      "title" : "Are you sure that this happened? assessing the factuality degree of events in text. Computational Linguistics 38(2):1–39",
      "author" : [ "Roser Saurı", "James Pustejovsky" ],
      "venue" : null,
      "citeRegEx" : "Saurı́ and Pustejovsky.,? \\Q2012\\E",
      "shortCiteRegEx" : "Saurı́ and Pustejovsky.",
      "year" : 2012
    }, {
      "title" : "Semantic compositionality through recursive matrix-vector spaces",
      "author" : [ "Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Socher et al\\.,? 2012",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2012
    }, {
      "title" : "Speculation and negation: Rules, rankers, and the role of syntax",
      "author" : [ "Erik Velldal", "Lilja Øvrelid", "Jonathon Read", "Stephan Oepen." ],
      "venue" : "Computational Linguistics 38(2):369–410. https://doi.org/10.1162/COLI a 00126.",
      "citeRegEx" : "Velldal et al\\.,? 2012",
      "shortCiteRegEx" : "Velldal et al\\.",
      "year" : 2012
    }, {
      "title" : "Relation classification via multi-level attention cnns",
      "author" : [ "Linlin Wang", "Zhu Cao", "Gerard de Melo", "Zhiyuan Liu." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Annotating expressions of opinions and emotions in language",
      "author" : [ "Janyce Wiebe", "Theresa Wilson", "Claire Cardie." ],
      "venue" : "Language Resources and Evaluation 39(2):165–210. https://doi.org/10.1007/s10579-005-7880-9.",
      "citeRegEx" : "Wiebe et al\\.,? 2005",
      "shortCiteRegEx" : "Wiebe et al\\.",
      "year" : 2005
    }, {
      "title" : "Semantic relation classification via convolutional neural networks with simple negative sampling",
      "author" : [ "Kun Xu", "Yansong Feng", "Songfang Huang", "Dongyan Zhao." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natu-",
      "citeRegEx" : "Xu et al\\.,? 2015a",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Classifying relations via long short term memory networks along shortest dependency paths",
      "author" : [ "Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "Xu et al\\.,? 2015b",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Relation classification via convolutional deep neural network",
      "author" : [ "Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Zeng et al\\.,? 2014",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2014
    }, {
      "title" : "End-to-end learning of semantic role labeling using recurrent neural networks",
      "author" : [ "Jie Zhou", "Wei Xu." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint",
      "citeRegEx" : "Zhou and Xu.,? 2015",
      "shortCiteRegEx" : "Zhou and Xu.",
      "year" : 2015
    }, {
      "title" : "Attentionbased bidirectional long short-term memory networks for relation classification",
      "author" : [ "Peng Zhou", "Wei Shi", "Jun Tian", "Zhenyu Qi", "Bingchen Li", "Hongwei Hao", "Bo Xu." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the As-",
      "citeRegEx" : "Zhou et al\\.,? 2016",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : ", opinion detection (Wiebe et al., 2005), QA (Pustejovsky et al.",
      "startOffset" : 20,
      "endOffset" : 40
    }, {
      "referenceID" : 14,
      "context" : ", 2005), QA (Pustejovsky et al., 2005), textual entailment (de Marneffe et al.",
      "startOffset" : 12,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : ", 2005), textual entailment (de Marneffe et al., 2006; Hickl and Bensley, 2007), rumor identification (Qazvinian et al.",
      "startOffset" : 28,
      "endOffset" : 79
    }, {
      "referenceID" : 15,
      "context" : ", 2006; Hickl and Bensley, 2007), rumor identification (Qazvinian et al., 2011).",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 17,
      "context" : "Traditional methods for event factuality identification usually employed either hand-crafted rules (Saurı́, 2008; Saurı́ and Pustejovsky, 2012) relying on expert knowledge, or features (Prabhakaran et al.",
      "startOffset" : 99,
      "endOffset" : 143
    }, {
      "referenceID" : 19,
      "context" : "Traditional methods for event factuality identification usually employed either hand-crafted rules (Saurı́, 2008; Saurı́ and Pustejovsky, 2012) relying on expert knowledge, or features (Prabhakaran et al.",
      "startOffset" : 99,
      "endOffset" : 143
    }, {
      "referenceID" : 11,
      "context" : "Traditional methods for event factuality identification usually employed either hand-crafted rules (Saurı́, 2008; Saurı́ and Pustejovsky, 2012) relying on expert knowledge, or features (Prabhakaran et al., 2010; de Marneffe et al., 2012) relying on various kinds of annotated information, such as source introducing predicates, predicate classes, speculative and negative cues.",
      "startOffset" : 185,
      "endOffset" : 237
    }, {
      "referenceID" : 20,
      "context" : "Due to the recent success of deep learning on various NLP tasks in learning useful representations from ordinary sentences (Socher et al., 2012; Zeng et al., 2014; Cheng et al., 2016) and syntactic paths (Xu et al.",
      "startOffset" : 123,
      "endOffset" : 183
    }, {
      "referenceID" : 26,
      "context" : "Due to the recent success of deep learning on various NLP tasks in learning useful representations from ordinary sentences (Socher et al., 2012; Zeng et al., 2014; Cheng et al., 2016) and syntactic paths (Xu et al.",
      "startOffset" : 123,
      "endOffset" : 183
    }, {
      "referenceID" : 2,
      "context" : "Due to the recent success of deep learning on various NLP tasks in learning useful representations from ordinary sentences (Socher et al., 2012; Zeng et al., 2014; Cheng et al., 2016) and syntactic paths (Xu et al.",
      "startOffset" : 123,
      "endOffset" : 183
    }, {
      "referenceID" : 16,
      "context" : ", 2016) and syntactic paths (Xu et al., 2015a,b; Roth and Lapata, 2016) and the attention mechanism on capturing the focus change in sequence modeling (Chen et al.",
      "startOffset" : 28,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : ", 2015a,b; Roth and Lapata, 2016) and the attention mechanism on capturing the focus change in sequence modeling (Chen et al., 2016; Wang et al., 2016; Zhou et al., 2016), this paper proposes a framework to identify event factuality in raw texts with neural networks.",
      "startOffset" : 113,
      "endOffset" : 170
    }, {
      "referenceID" : 22,
      "context" : ", 2015a,b; Roth and Lapata, 2016) and the attention mechanism on capturing the focus change in sequence modeling (Chen et al., 2016; Wang et al., 2016; Zhou et al., 2016), this paper proposes a framework to identify event factuality in raw texts with neural networks.",
      "startOffset" : 113,
      "endOffset" : 170
    }, {
      "referenceID" : 28,
      "context" : ", 2015a,b; Roth and Lapata, 2016) and the attention mechanism on capturing the focus change in sequence modeling (Chen et al., 2016; Wang et al., 2016; Zhou et al., 2016), this paper proposes a framework to identify event factuality in raw texts with neural networks.",
      "startOffset" : 113,
      "endOffset" : 170
    }, {
      "referenceID" : 17,
      "context" : "1 Event Factuality Factuality can be characterized by the combination of two dimensions: epistemic modality and polarity (Saurı́, 2008).",
      "startOffset" : 121,
      "endOffset" : 135
    }, {
      "referenceID" : 13,
      "context" : "2 Basic Factors Considering that FactBank is built on top of TimeBank (Pustejovsky et al., 2003b), we adopt the definition of events proposed by TimeML (Pustejovsky et al.",
      "startOffset" : 70,
      "endOffset" : 97
    }, {
      "referenceID" : 0,
      "context" : "Such events can be detected by feature-based methods (Chambers, 2013).",
      "startOffset" : 53,
      "endOffset" : 69
    }, {
      "referenceID" : 23,
      "context" : "Hence, we appeal to the notion of the nested source structure (Wiebe et al., 2005) and represent the sources in the chain form: McCulley AUTHOR, which means McCulley is Embedded in AUTHOR.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "For event detection task, we only consider nouns, verbs, adjectives as the candidate tokens to reduce the complexity, and we utilize a maximum entropy classification model with the features used by Chambers (2013). Øvrelid et al.",
      "startOffset" : 198,
      "endOffset" : 214
    }, {
      "referenceID" : 0,
      "context" : "For event detection task, we only consider nouns, verbs, adjectives as the candidate tokens to reduce the complexity, and we utilize a maximum entropy classification model with the features used by Chambers (2013). Øvrelid et al. (2010) and Velldal et al.",
      "startOffset" : 198,
      "endOffset" : 237
    }, {
      "referenceID" : 0,
      "context" : "For event detection task, we only consider nouns, verbs, adjectives as the candidate tokens to reduce the complexity, and we utilize a maximum entropy classification model with the features used by Chambers (2013). Øvrelid et al. (2010) and Velldal et al. (2012) concluded that syntactic features are not necessary and lexical sequence-oriented n-gram features can achieve excellent performances on cue detection",
      "startOffset" : 198,
      "endOffset" : 263
    }, {
      "referenceID" : 21,
      "context" : "Hence, we employ the lexical features developed by Velldal et al. (2012) to classify each token into PR cue, PS cue, Neg cue, or not cue.",
      "startOffset" : 51,
      "endOffset" : 73
    }, {
      "referenceID" : 17,
      "context" : "This is a recursive algorithm defined by Saurı́ (2008).",
      "startOffset" : 41,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "Overcoming these problems is the motivation behind the LSTM model (Hochreiter and Schmidhuber, 1997) which introduces a memory cell including an input gate it, a neuron with a self-recurrent connection ct, a forget gate ft and an output gate ot.",
      "startOffset" : 66,
      "endOffset" : 100
    }, {
      "referenceID" : 18,
      "context" : "1 Experimental Settings We evaluate our models on FactBank (Saurı́ and Pustejovsky, 2009), which contains 3864 sentences, 9492 events and 13506 factuality values.",
      "startOffset" : 59,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : "Following previous studies (Saurı́, 2008; de Marneffe et al., 2012), we only take into account the five main categories of values, i.",
      "startOffset" : 27,
      "endOffset" : 67
    }, {
      "referenceID" : 8,
      "context" : "To obtain the performance for the whole corpus, macro-averaging and micro-averaging are applied (Manning and Schütze, 2001).",
      "startOffset" : 96,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "We initialize word embeddings via Word2Vec (Mikolov et al., 2013) and randomly initialize all the other parameters.",
      "startOffset" : 43,
      "endOffset" : 65
    }, {
      "referenceID" : 6,
      "context" : "To obtain the performance for the whole corpus, macro-averaging and micro-averaging are applied (Manning and Schütze, 2001). We choose two-sample two-tailed t-test for significance test. For the SIP detection task, we set d0 = 100, dpos = dhyp = 50, nc = 150. For the event factuality identification task, we set d0 = 100, dp = dt = 10, nc = 50, = 0.25. We initialize word embeddings via Word2Vec (Mikolov et al., 2013) and randomly initialize all the other parameters. The Stochastic Gradient Descent with momentum is applied to optimize our models. If there is more than one cue in the sentence, we consider whether the event is modified by each cue separately. If a Non-Uu event is affected by both PR and PS cues, we adopt the cue with the highest confidence score in o2 to decide whether the modality of the event is PR or PS. We employ the following baselines, whose inputs are the outputs of our basic factor extraction for the fair comparison with our model: Rules: The rule-based model developed by Saurı́ (2008) is a top-down algorithm traversing a dependency parse tree.",
      "startOffset" : 97,
      "endOffset" : 1022
    }, {
      "referenceID" : 3,
      "context" : "MaxEnt: We employ a maximum entropy classification model using the features developed by de Marneffe et al. (2012). Some features are not available (i.",
      "startOffset" : 92,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "68) than Chambers (2013) (F1=80.",
      "startOffset" : 9,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "68) than Chambers (2013) (F1=80.30). Remember that we only consider nouns, verbs, adjectives, and rule out the effects of other tokens. We also employ the features developed by Chambers (2013) to identify SIPs and obtain F1=72.",
      "startOffset" : 9,
      "endOffset" : 193
    }, {
      "referenceID" : 18,
      "context" : "45, respectively), and developed FactBank corpus (Saurı́ and Pustejovsky, 2009).",
      "startOffset" : 49,
      "endOffset" : 79
    }, {
      "referenceID" : 20,
      "context" : "Neural networks, emerging in recent years, can not only extract effective features from sentences (Socher et al., 2012; Zeng et al., 2014; Zhou and Xu, 2015), but also capture important information from syntactic paths and improve performances of many NLP applications, such as relation classification (Xu et al.",
      "startOffset" : 98,
      "endOffset" : 157
    }, {
      "referenceID" : 26,
      "context" : "Neural networks, emerging in recent years, can not only extract effective features from sentences (Socher et al., 2012; Zeng et al., 2014; Zhou and Xu, 2015), but also capture important information from syntactic paths and improve performances of many NLP applications, such as relation classification (Xu et al.",
      "startOffset" : 98,
      "endOffset" : 157
    }, {
      "referenceID" : 27,
      "context" : "Neural networks, emerging in recent years, can not only extract effective features from sentences (Socher et al., 2012; Zeng et al., 2014; Zhou and Xu, 2015), but also capture important information from syntactic paths and improve performances of many NLP applications, such as relation classification (Xu et al.",
      "startOffset" : 98,
      "endOffset" : 157
    }, {
      "referenceID" : 16,
      "context" : ", 2015a,b) and semantic role labeling (Roth and Lapata, 2016).",
      "startOffset" : 38,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "Diab et al. (2009) and Prabhakaran et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "Diab et al. (2009) and Prabhakaran et al. (2010) presented a preliminary pilot study of belief annotation and automatic tagging, and limited the source to AUTHOR.",
      "startOffset" : 0,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "Diab et al. (2009) and Prabhakaran et al. (2010) presented a preliminary pilot study of belief annotation and automatic tagging, and limited the source to AUTHOR. They classified predicates into Committed Belief (CB), NonCB or Not Applicable under a supervised framework using lexical and syntactic features. Saurı́ (2008) proposed a rule-based top-down algorithm traversing dependency trees which can achieve excellent results (F1 of macro- and microaveraging are 73.",
      "startOffset" : 0,
      "endOffset" : 323
    }, {
      "referenceID" : 2,
      "context" : "Then de Marneffe et al. (2012) employed lexical, syntactic and pragmatic features to identify factuality of events in some sentences of FactBank.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "Chen et al. (2016) built a hierarchical LSTM model with attention to generate sentence and document representations for sentiment classification.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "Chen et al. (2016) built a hierarchical LSTM model with attention to generate sentence and document representations for sentiment classification. Zhou et al. (2016) utilized a bidirectional LSTM with attention to model sentences, and Wang et al.",
      "startOffset" : 0,
      "endOffset" : 165
    }, {
      "referenceID" : 1,
      "context" : "Chen et al. (2016) built a hierarchical LSTM model with attention to generate sentence and document representations for sentiment classification. Zhou et al. (2016) utilized a bidirectional LSTM with attention to model sentences, and Wang et al. (2016) proposed a novel CNN relying on two levels of attention in terms of relation classification.",
      "startOffset" : 0,
      "endOffset" : 253
    } ],
    "year" : 2017,
    "abstractText" : "Event factuality identification plays an important role in deep NLP applications. In this paper, we propose a deep learning framework for this task which first extracts essential information from raw texts as the inputs and then identifies the factuality of events via a deep neural network with a proper combination of Bidirectional Long Short-Term Memory (BiLSTM) neural network and Convolutional Neural Network (CNN). The experimental results on FactBank show that our framework significantly outperforms several state-of-the-art baselines.",
    "creator" : "LaTeX with hyperref package"
  }
}