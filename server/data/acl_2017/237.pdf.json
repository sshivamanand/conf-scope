{
  "name" : "237.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A New Approach for Measuring Sentiment Orientation based on Multi-Dimensional Vector Space",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\nThis study implements a vector space model approach to measure the sentiment orientations of words. Two representative vectors for positive/negative polarity are constructed using high-dimensional vector space in both an unsupervised and a semisupervised manner. A sentiment orientation value per word is determined by taking the difference between the cosine distances against the two reference vectors. These two conditions (unsupervised and semi-supervised) are compared against an existing unsupervised method (Turney, 2002). As a result of our experiment, we demonstrate that this novel approach significantly outperforms the previous unsupervised approach and is more practical and data efficient as well."
    }, {
      "heading" : "1 Introduction",
      "text" : "Previous research in sentiment analysis or opinion mining mostly focus on supervised methods, which requires labeled training data to identify properties of unseen input, and then classify input later. Probabilistic methods in particular often calculate which class a word or phrase most likely bears and then make predictions regarding the label of a given target text, using those estimations. While such methods have widely been adopted, there are few examples which measure the likelihood of lexical items in an unsupervised or semisupervised manner. However, there still exist situations where sentiment analysis should be performed using non-labeled datasets. In such cases, discovering information regarding the sentiment orientation of vocabulary in a non-supervised fashion becomes essential.\nOur approach employs VSM (Vector Space Models) as its main component. VSM is deeply related to the distributional hypothesis (Turney and Pantel, 2010). The distributional hypothesis states that words in similar contexts tend to have similar meaning (Rubenstein and Goodenough, 1965; Schütze and Pederson, 1995; Deerwester et al., 1990). Traditionally, the relation of two words in a ‘similar context’ has been distinguished into two classes: syntagmatic or paradigmatic (Murphy, 2003; Sahlgren, 2006). Syntagmatic relations are concerned with whether or not two entities are in a co-occurrence relation, and paradigmatic relations are concerned with whether the two items in question are interchangeable (substitution relation). Many collocation models using N-grams and Point-wise Mutual Information (PMI) analyze the former-type of word relations. On the other hand, recent dense vector-based models (Skip-gram, Continuous Bag-of-Words) exploit the paradigmatic relation, and thus they both give a high weight to the similarity of words if they share similar neighboring entities (Mikolov et al., 2013).\nOur work can best be understood as an exploration to find a sentiment dimension over a multidimensional vector space, which is constructed from the relations of words in a corpus. However, it is too complex to extract a specific type of relation between whole words on such a high-dimensional space. Thus, we start by selecting a small set of words that are believed to have an emotional value for a topic. We refer to these words as ‘point words’ and use them to construct a latent sentiment dimension. Our method for choosing these point words can be divided into two sub-types: unsupervised or semi-supervised.\nIf we use a supervised learning approach, one simple, intuitive way of calculating the sentiment\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\norientation of a word is to compute the log-likelihood ratio of probabilities in terms of occurrence per sentiment label. Should the labels not be given, one alternative method would be to use the similarity between words. Following the principle of the Distributional Hypothesis, we assume that positive or negative words will tend to share similar contexts relative to their opposing stance.\nWe argue that collocation-based methods are not a practical choice for obtaining the similarity due to data sparsity which is inherent to the model. Our implementation of PMI-IR method (Turney, 2002) for this problem demonstrates this point and hints as to why dense vector space models should be used instead. For this purpose, we also compared two well-known vector models (Word2Vec and GloVe), as the latter model mainly depends on the collocation relations of words, while Word2Vec’s Skip-gram model is dependent on the paradigmatic relations between words.\nIn the unsupervised condition, a dimensionality reduction algorithm is implemented to search for sentiment dimension using the selected point words. Under the semi-supervised condition, we depend on external estimations of the terms in order to skip the exploration stage. We believe that this study can be a direct comparison with the previous work of (Turney, 2002), because we use the same seed information and a similar procedure for calculating the semantic orientations of the words."
    }, {
      "heading" : "2 Related Work",
      "text" : "Although the majority of previous studies on sentiment analysis have preferred to use supervised methods, some researchers have tried to develop unsupervised or semi-unsupervised approaches. For instance, Turney (2002) suggests the PMI-IR algorithm to estimate the semantic orientation of a phrase for the unsupervised classification of various reviews. He uses two pre-chosen words (‘poor’ and ‘excellent’) to calculate the semantic orientation of the target phrases, which is defined as the relative PMI difference of the phrase from the two seed words. His work borrows heavily on the theory of semantic orientation of adjectives by Hatzivassiloglou and McKeown (1997). In this study, the authors discuss the existence of linguistic constraints on the semantic orientations of words in conjunctions. In line with the approach of Turney 1 Yandex reported that it indexes more than 4 billion pages written in the Latin Alphabet with the majority of them being in English (https://yandex.com/company\n(2002), Zagibalov and Carroll (2008) attempt to develop an automatic selection process for seed words in Chinese texts for unsupervised classifications.\nOne major constraint on Turney (2002) is the availability of a corpus to calculate the relevant PMI. If a given equipped corpus is not big enough for a PMI analysis, the problem of data sparseness will arise, and the PMI values become suspect. Note that Turney (2002) used a search engine (AltaVista) for his experiments, which contained 350 million web pages at the time. In our experiment, we instead use Yadex.com (http://www.yan-\ndex.com/), as it provides a more reliable Near operator among the current major search engines.1 With the operator, words in a query have to be within 10 words of each other, regardless of order. We used the same formula (Eq. 1) from Turney (2002) for our experiment:\n\uD835\uDC46\uD835\uDC42(\uD835\uDC5D\uD835\uDC5Fℎ\uD835\uDC4E\uD835\uDC60\uD835\uDC52) =\n\uD835\uDC59\uD835\uDC5C\uD835\uDC542 [ ℎ\uD835\uDC56\uD835\uDC61\uD835\uDC60(\uD835\uDC5Dℎ\uD835\uDC5F\uD835\uDC4E\uD835\uDC60\uD835\uDC52,\uD835\uDC41\uD835\uDC38\uD835\uDC34\uD835\uDC45 \"excellent\") ℎ\uD835\uDC56\uD835\uDC61\uD835\uDC60(\"poor\")\nℎ\uD835\uDC56\uD835\uDC61\uD835\uDC60(\uD835\uDC5Dℎ\uD835\uDC5F\uD835\uDC4E\uD835\uDC60\uD835\uDC52,\uD835\uDC41\uD835\uDC38\uD835\uDC34\uD835\uDC45 \"poor\") ℎ\uD835\uDC56\uD835\uDC61\uD835\uDC60(\"excellent\") ] (1)\nNote that hits(query) is the number of the returns,\ngiven the query. Additionally, we add 0.01 to hits when the number of the hits is zero, in order to prevent division-by-zero.\nIn contrast to collocation models, some researchers attempt to apply neural probabilistic language models to measure the semantic similarities of words based on context-window methods (Mikolov et al., 2013; Collobert and Weston, 2008), and word embedding methods (e.g., Word2Vec) have been found more effective for various tasks in NLP than other traditional techniques (Baroni et al., 2014). Continuous Bag-of-Words (CBOW) models and Skip-gram models used in Mikolov et al., (2013) both place a high weight on the similarity of words if they share similar neighboring entities.\nAdditionally, we consider another word-embedding model (GloVe). Unlike Skip-gram or the CBOW architecture of Word2Vec, GloVe uses the ratios of words’ probability of co-occurrence to learn word vectors (Pennington et al., 2014). We note that GloVe is a kind of word embedding model, but its vector space is constructed differently from Word2Vec, because it adopts the collocation modeling of words.\n/press_center/press_releases/2010/201005-19)\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nWe note that the implication of the Skipgram/CBOW architecture is very similar to the concept of paradigmatic relation of words. This is due to the fact that the distance between any two words in a paradigmatic relation is minimized when they share the most similar neighbors. For example, in a minuscule corpus which bears only two sentences (“This movie is very good” and “This movie is very bad”), the two words (‘good’ and ‘bad’) will likely have a high cosine similarity in the Word2Vec model.\nThis aspect can cause unexpected results when such a model is employed for clustering a set of items that share similar emotions, because two words in paradigmatic relations often instantiate a contrastive relation (e.g., antonym). However, as noted in Mikolov et al. (2013), words seem to have multiple syntactic/semantic relations to each other, and the Word2Vec model helps to observe the multiple degrees of similarity for words. From this perspective, we might find a specific relation to them in a subspace of the original vector space if the necessary vector calculation operations are known.\nA feature-space approach (Osgood, 1952; Smith and Medin, 1981; Waltz and Pollack, 1985) discusses how words are represented by feature vectors whose attributional factors are annotated by human participants. In a similar vein, the Conceptual Space theory of Gärdenfors (2000) provides a theoretical framework for understanding the multiple degrees of similarity in the space. He also suggests using dimensionality reduction algorithms (e.g., Multidimensional scaling) to explore the quality dimensions of multi-dimensional vector space and claims that applying these algorithms to the similarity-based vector space will generate an ordering relation for data points on an interested domain.\nAnother relevant prior work is the Word-Space model, which is a vector-based computational model for the semantic similarity of words (Schütze, 1993; Sahlgren, 2006). The Word-Space model, as the name implies, models word meaning with a spatial representation. Thus, semantic similarity is represented as proximity in n-dimensional space.\nOur goal is to find or construct a sentiment dimension from the similarity-based vector space of words. The vector representation for the sentiment dimension will be our ‘interested domain’ and the ‘ordering relation’ on the domain will map each\nword object to a real valued point, indicating the level of its ‘positive’ or ‘negative’ significance."
    }, {
      "heading" : "3 Methods and Data",
      "text" : "The purpose of our experiment is to find the sentiment orientations of words in a corpus and evaluate the effectiveness of the information by conducting an unsupervised/semi-supervised classification on our movie review datasets."
    }, {
      "heading" : "3.1 Data",
      "text" : "Our data consists of two movie review corpora: one of which is the IMDB movie dataset (Maas et al., 2011) and the other is the Stanford Sentiment Treebank (Socher et al., 2013). The IMDB dataset provides 25,000 movie reviews for training and 25,000 for testing. The corpus also contains the expected polarity values for all individual tokens occurring in the reviews. This dataset is used for our vector space construction and the polarity values per word are employed under the semi-supervised condition.\nThe Stanford Sentiment Treebank is a corpus based on the 11,855 movie reviews presented by Pang and Lee (2005), and all 215,154 phrases are manually annotated by three judges per phrase. Because we want the classification task to be binary, reviews with neutral labels are excluded from the dataset, and the number of reviews in the resulting dataset is 9,613. We select this corpus as our test dataset since it allows us to compare our sentiment orientation values with the annotated polarity value for each word."
    }, {
      "heading" : "3.2 Experiment Methods",
      "text" : "The first step of our methodology is to obtain a set of point words which are assumed to express positive/negative sentiment. Using the pattern extrac-\nFirst Word Second Word Third Word\n1 JJ NN or NNS anything\n2 RB, RBR or\nRBS\nJJ not NN nor\nNNS\n3 JJ JJ not NN nor\nNNS\n4 NN or NNS VB, VBD not NN nor\nNNS\n5 RB, RBR, or\nRBS\nVBN, or VBG anything\nTable 1. Pattern rules of tags for extraction twoword phrases (third word excluded)\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\ntion rules (Table. 1) from Turney (2002), we obtained 17,716 phrases by applying these rules to the Stanford Sentiment Treebank.\nWe use the adjectives or adverbs of the phrases for our point words, since these syntactic categories have been found very useful for recognizing subjectivity in written texts (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 1999; Bruce and Wiebe, 2000).\nTo validate this assumption, we POS-tagged the word tokens of the Stanford corpus and observe the variance of sentiment ratings of words depending on their POS-tag (Fig. 1).\nFigure 1. Variances of polarity values per POS-tags\nNot surprisingly, the main tags of adjective and adverb occupy the top places in the ranking, indicating that terms bearing these tags are often used with stronger subjectivity. Numerically, the top four tags (JJS, RBR, JJ, and RB) make up 37% of total variances over the whole dataset.\nBased on the theory and the observation, we select the top K modifiers (adjectives or adverbs) from the extracted patterns. The number of the modifiers are automatically determined by choosing a minimum frequency for the extracted phrases."
    }, {
      "heading" : "3.2.1 Unsupervised methodology",
      "text" : "After the point words (K modifiers) are obtained, a high-dimensional vector space is implemented to build a point-wise distance matrix for the selected words, resulting in a K×K matrix. Since the distances between the tokens are measured by Cosinedistance, the distance data is Euclidean. As mentioned in Section 3.1, we use the IMDB dataset for vector space construction.\nA local structure-oriented dimensionality reduction algorithm, the Principal Component Analysis (PCA) is then used to find the sentiment dimension between the modifiers. In the analysis, we only use\nthe value of two for the number of dimensions to project the entities into the reduced space.\nBased on the principle of the Distribution Hypothesis, we assume that semantically close modifiers are closer to each other than the opposites. Thus, the principal component of the PCA for the emotional terms will represent the positive/negative aspects of their collective meaning.\nWhen the dimensionality reduction phase is completed, it is possible to observe correlations between values on the found dimension and the goldstandard dataset (the annotated values of Stanford Sentiment Treebank). Since the signs of the coefficients are irrelevant for our purposes, only absolutes are considered.\nNow, we can determine the two sets of words distinguished by the origin of zero on the principal axis. To construct two reference vectors, simple vector averaging is used, and the vectors are classified as positive or negative by the criterion of being closer to the vector of the seed word (“excellent”). Note that this method was inspired by Turney (2002) and makes our study comparable to the previous work. We define the sentiment orientation of a word using Equation (2):\nSO(w) = \uD835\uDC36\uD835\uDC5C\uD835\uDC60\uD835\uDC37\uD835\uDC60\uD835\uDC61(\uD835\uDC49\uD835\uDC52\uD835\uDC50\uD835\uDC5D\uD835\uDC5C\uD835\uDC60, \uD835\uDC64) − \uD835\uDC36\uD835\uDC5C\uD835\uDC60\uD835\uDC37\uD835\uDC60\uD835\uDC61(\uD835\uDC49\uD835\uDC52\uD835\uDC50\uD835\uDC5B\uD835\uDC52\uD835\uDC54, \uD835\uDC64) (2)\nCosDst means Cosine distance in the equation. If the mean of the sentiment orientations in a review post is less than zero, the review is labeled as ‘negative’, and is ‘positive’ otherwise. We note that this approach allows us to calculate the orientations of all words in the vocabulary, unlike Turney’s phrase-oriented approach."
    }, {
      "heading" : "3.2.2 Semi-supervised methodology",
      "text" : "This method does not use the dimensionality reduction algorithm to distinguish the point words into two sets, but instead employs the expected star ratings of the tokens from the IMDB dataset. The ratings represent how strongly a word belongs to positive or negative sentiment polarity.\nThus, in this case, we start with almost certain information on the polarity of the words. Because adjectives and adverbs are generally used with their own static stance, we believe that the information can be applied to unseen texts over different domains. Since this semi-supervised setting is designed to compare with the unsupervised condition, the remainder of the experiment methodology is identical to the unsupervised methodology.\n0\n0.005\n0.01\n0.015\n0.02\n0.025\n0.03\nJJS RBR JJ RB VBG VBN NN JJR VBD\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499"
    }, {
      "heading" : "3.2.3 PMI-IR methodology",
      "text" : "We replicate the PMI-IR algorithm (Turney, 2002) against the Stanford corpus. The queries on the 17 thousand phrases are sent to the Search API of Yandex.com (https://yandex.com/search/) and the hits of the phrases with the two seeds (“excellent” and “poor”) are disregarded if both of the two hits are less than four counts. Since the method is only applicable to a review that contains at least the one of the defined pat terns (Table. 1), the test dataset consists of 7,646 posts out of the 9,613 reviews and the average number of phrases per review is 2.25."
    }, {
      "heading" : "4 Results",
      "text" : "Our first observation in the unsupervised setting is the visualization of the PCA result on the selected\nmodifiers using Word2Vec (Skip-gram, no_components: 100) and GloVe models (epochs: 30, no_components: 100). Fig. 1 shows exemplary results of the 82 tokens, which are from the extracted patterns of the Stanford dataset. The point words are colored red (positive) or blue (negative) by the middle value on the annotation scale.\nThe principal dimension in the embedding by PCA gives us an ordering relation between the terms. As explained in Section 3, we interpret the dimension as a sentiment domain in the high-dimensional vector space of words. The correlation coefficients in Table 2 demonstrates how the dimension correlates with our gold-standard dataset (the signs of the correlations are ignored). Although there undoubtedly exists noise in the results, we could still find correlations between the unsupervised ratings and the annotated values. We also note that varying the size of context window for the embedding models produces slightly different patterns. As Table 2 shows, the Word2Vec model tends to lose its potency as the size of context window increases, while GloVe remains the same.\nOur semi-supervised approach does not use the procedure incorporating the PCA, but the vector averaging and classification processes are identical to the unsupervised condition.\nFig. 2 represents the accuracy results of the two settings for the 9,613 reviews, while changing the minimum frequency of the extracted patterns in which the modifiers occurred. The cutoff frequency ranges from 2 to 9 and the corresponding number of the word tokens varies from 495 to 30. We set the context window size to 3 for the\nFigure 2. 2-D embedding of the Principal Component Analysis of the point words. The left figure shows the result from Word2Vec-based space and the right figure from GloVe model. The color is coded by the ratings of Stanford Sentiment Treebank (red for positive and blue for negative).\nModel Pearson Spear-\nman\nWord2Vec (size:3) 0.33 0.32 Word2Vec (size:5) 0.29 0.29 Word2Vec (size:7) 0.27 0.26 Word2Vec(size:10) 0.25 0.24\nGloVe (size:3) 0.28 0.33 GloVe (size:5) 0.27 0.30 GloVe (size: 7) 0.28 0.34\nGloVe (size: 10) 0.28 0.34\nTable 2. Correlations between the unsupervised ratings and annotated ratings\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nWord2Vec (Skip-gram based) and 10 for the GloVe model.\nAs can be seen in the graph, the Word2Vec conditions often outperform the GloVe conditions, and the unsupervised Word2Vec generally records higher accuracy than the GloVe. The highest accuracy is achieved by semi-supervised Word2Vec (66%) and the lowest score is falls to the unsupervised GloVe model (lower than chance). The best result for unsupervised Word2Vec is 61% (cutoff: 8), and the variance of the accuracies is relatively small compared to other models. Decreasing the number of the point words worsens the performance of all the semi-supervised models.\nThe general pattern of Fig. 2 is found in the results of our experiment combined with the PMI-IR algorithm against the reduced test dataset (Table 3). Semi-supervised/unsupervised Word2Vec models generally record higher accuracies than the GloVe models. On top of this, the semi-supervised methods show a better performance than the unsupervised settings.\nThe PMI-IR produces the lowest accuracy along with the unsupervised GloVe model (57%). As mentioned in Section 3, the PMI-based approach is based on the returned search results from Yandex engine, while our methods use vector space from the IMDB corpus of 50,000 movie reviews. The classification by PMI-IR is processed by calculating the mean of semantic orientations from the extracted phrases per review as suggested in Turney (2002)."
    }, {
      "heading" : "5 Discussion",
      "text" : "We present an unsupervised/semi-supervised approach that measures the sentiment orientation of words using vector space models. The core idea of our approach is to find a sentiment dimension from a high-dimensional vector space of words and use the extracted information to calculate the polarity of an individual word. We attempted to see whether or not the obtained sentiment orientations are useful by the sentiment classification task of movie reviews.\nOur approach borrows from the general paradigm of Word Space models (Schütze, 1993) and is inspired by the theoretical tools of feature space models (Osgood, 1952; Smith and Medin, 1981; Waltz and Pollack, 1985; Gärdenfors, 2000). The general framework of our experiment is based on Turney (2002) and our computational methods rely on recent successes in word-embedding research (Mikolov et al., 2013; Pennington et al., 2014).\nOur unsupervised methodology (Word2Vecbased) outperforms the PMI-IR approach, which is\nFigure 2. Accuracy of Classifications. The x-axis indicates the cutoff frequency.\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n2 3 4 5 6 7 8 9\nW2V-unsuper\nW2V-semi\nGlove-unsuper\nGlove-semi\nModel Cutoff Accuracy\nW2V (semi-supervised) 8 0.66\nW2V (unsupervised) 5 0.63\nGloVe (semi-supervised) 3 0.63\nGloVe (unsupervised) 9 0.57\nPMI-IR N/A 0.57\nTable 3. highest accuracy of classification results of the selected test set (7,646 reviews)\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\na well-known unsupervised tool in a sentiment classification task. Additionally, the unsupervised Word2Vec setting records much higher accuracy than the unsupervised GloVe model. We suspect that the cause of this low performance is from the common feature that the both models are based on word collocations.\nConsidered that the PMI-IR approach employs the results of a major search engine (Yandex.com), the low accuracy indicates that data-sparsity is a very difficult issue for a collocation-based method to overcome. Even the word-embedding model (GloVe) seems unable to break free of this problem, as the models show lower results than the those using Skip-gram. This conjecture is supported by the observation that the GloVe model does not lose its correlation coefficients as the context size increases. We note that Skip-grams essentially exploit paradigmatic relations between words and produce denser vector spaces for the relation of words. Thus, methods based on dense vector modeling should be more robust for the data-sparsity problem in our task, as demonstrated by our experiment.\nIt is worth noting that PMI-IR suffers from datadeficiency. In many practical situations, it is hard to collect information on the collocations of phrases using a sufficient search engine service, as major search engines often constrain the queries of users by their policies or do not provide the ‘Near’ operator for queries.\nOne issue in our study is how to find optimal reference vectors to represent the sentiment polarity of a vector space. We tried to approximate by using a traditional operation (vector averaging). Note that the performance of the semi-supervised approach did not dramatically increase with the added number of the word tokens. This likely indicates that the type of vector calculation is not efficient for the purpose in this research. Additionally, the results of our experiment do not meet the high standards of a supervised approach (usually above 80%). However, we believe that there is potential for huge improvement, as the reference vectors can be constructed in an optimal way to represent the sentiment domain between words. We predict that there could be two possible ways to achieve this goal. The first way is to select the point words that best capture the landscape of the sentiment entities in a corpus. And the second way is to exploit more relevant vector space models than the embedding\nmethods used in this study. We leave such explorations for future work."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this study, we introduce a novel approach which implements distributional semantic models to measure the sentiment orientation of a word. We divide our experiment into two separate types (unsupervised or semi-supervised) and compare the results with a previous unsupervised approach (PMI-IR). Our new unsupervised methodology (Word2Vec-based) outperforms the existing approach, using much smaller datasets, while being robust enough to overcome the problem of datasparseness."
    } ],
    "references" : [ {
      "title" : "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. Paper presented at the ACL",
      "author" : [ "M. Baroni", "G. Dinu", "G. Kruszewski" ],
      "venue" : null,
      "citeRegEx" : "Baroni et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2014
    }, {
      "title" : "Recognizing subjectivity: a case study in manual tagging",
      "author" : [ "R.F. Bruce", "J.M. Wiebe" ],
      "venue" : "Natural Language Engineering,",
      "citeRegEx" : "Bruce and Wiebe,? \\Q1999\\E",
      "shortCiteRegEx" : "Bruce and Wiebe",
      "year" : 1999
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "R. Collobert", "J. Weston" ],
      "venue" : "Paper presented at the Proceedings of the 25th international conference on Machine learning",
      "citeRegEx" : "Collobert and Weston,? \\Q2008\\E",
      "shortCiteRegEx" : "Collobert and Weston",
      "year" : 2008
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "S Deerwester" ],
      "venue" : "Journal of the American society for information science",
      "citeRegEx" : "Deerwester,? \\Q1990\\E",
      "shortCiteRegEx" : "Deerwester",
      "year" : 1990
    }, {
      "title" : "Conceptual spaces: The geometry of thought: MIT press",
      "author" : [ "P. Gärdenfors" ],
      "venue" : null,
      "citeRegEx" : "Gärdenfors,? \\Q2004\\E",
      "shortCiteRegEx" : "Gärdenfors",
      "year" : 2004
    }, {
      "title" : "Predicting the semantic orientation of adjectives. Paper presented at the Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics",
      "author" : [ "V. Hatzivassiloglou", "K.R. McKeown" ],
      "venue" : null,
      "citeRegEx" : "Hatzivassiloglou and McKeown,? \\Q1997\\E",
      "shortCiteRegEx" : "Hatzivassiloglou and McKeown",
      "year" : 1997
    }, {
      "title" : "Effects of adjective orientation and gradability on sentence subjectivity",
      "author" : [ "V. Hatzivassiloglou", "J.M. Wiebe" ],
      "venue" : "Paper presented at the Proceedings of the 18th conference on Computational linguistics-Volume",
      "citeRegEx" : "Hatzivassiloglou and Wiebe,? \\Q2000\\E",
      "shortCiteRegEx" : "Hatzivassiloglou and Wiebe",
      "year" : 2000
    }, {
      "title" : "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : null,
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Semantic relations and the lexicon: antonymy, synonymy and other paradigms",
      "author" : [ "M.L. Murphy" ],
      "venue" : null,
      "citeRegEx" : "Murphy,? \\Q2003\\E",
      "shortCiteRegEx" : "Murphy",
      "year" : 2003
    }, {
      "title" : "The nature and measurement of meaning",
      "author" : [ "C.E. Osgood" ],
      "venue" : "Psychological bulletin, 49(3), 197.",
      "citeRegEx" : "Osgood,? 1952",
      "shortCiteRegEx" : "Osgood",
      "year" : 1952
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "B. Pang", "L. Lee" ],
      "venue" : "Paper presented at the Proceedings of the 43rd",
      "citeRegEx" : "Pang and Lee,? \\Q2005\\E",
      "shortCiteRegEx" : "Pang and Lee",
      "year" : 2005
    }, {
      "title" : "Glove: Global Vectors for Word Representation. Paper presented at the EMNLP",
      "author" : [ "J. Pennington", "R. Socher", "C.D. Manning" ],
      "venue" : null,
      "citeRegEx" : "Pennington et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Contextual correlates of synonymy",
      "author" : [ "H. Rubenstein", "J.B. Goodenough." ],
      "venue" : "Communications of the ACM 8(10): 627-633.",
      "citeRegEx" : "Rubenstein and Goodenough.,? 1965",
      "shortCiteRegEx" : "Rubenstein and Goodenough.",
      "year" : 1965
    }, {
      "title" : "The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces",
      "author" : [ "M. Sahlgren" ],
      "venue" : "Institutionen för lingvistik.",
      "citeRegEx" : "Sahlgren,? 2006",
      "shortCiteRegEx" : "Sahlgren",
      "year" : 2006
    }, {
      "title" : "Word space",
      "author" : [ "H. Schiitze" ],
      "venue" : "Advances in neural information processing systems, 5, 895902.",
      "citeRegEx" : "Schiitze,? 1993",
      "shortCiteRegEx" : "Schiitze",
      "year" : 1993
    }, {
      "title" : "Categories and concepts: Harvard University",
      "author" : [ "E.E. Smith", "D.L. Medin" ],
      "venue" : null,
      "citeRegEx" : "Smith and Medin,? \\Q1981\\E",
      "shortCiteRegEx" : "Smith and Medin",
      "year" : 1981
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts" ],
      "venue" : null,
      "citeRegEx" : "Socher et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Thumbs up or thumbs down?: semantic orientation applied to unsupervised",
      "author" : [ "P.D. Turney" ],
      "venue" : null,
      "citeRegEx" : "Turney,? \\Q2002\\E",
      "shortCiteRegEx" : "Turney",
      "year" : 2002
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics",
      "author" : [ "P.D. Turney", "P. Pantel" ],
      "venue" : "Journal of artificial intelligence research 37: 141-188.",
      "citeRegEx" : "Turney and Pantel,? 2010",
      "shortCiteRegEx" : "Turney and Pantel",
      "year" : 2010
    }, {
      "title" : "Massively parallel parsing: A strongly interactive model of natural language interpretation",
      "author" : [ "D.L. Waltz", "J.B. Pollack" ],
      "venue" : "Cognitive science,",
      "citeRegEx" : "Waltz and Pollack,? \\Q1985\\E",
      "shortCiteRegEx" : "Waltz and Pollack",
      "year" : 1985
    }, {
      "title" : "Development and use of a gold-standard data set for subjectivity classifications. Paper presented at the Proceedings of the 37th annual meeting of the Association for Computational",
      "author" : [ "J.M. Wiebe", "R.F. Bruce", "T.P. O'Hara" ],
      "venue" : null,
      "citeRegEx" : "Wiebe et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Wiebe et al\\.",
      "year" : 1999
    }, {
      "title" : "Automatic seed word selection for unsupervised sentiment classification of Chinese text",
      "author" : [ "T. Zagibalov", "J. Carroll" ],
      "venue" : "Paper presented at the Proceedings of the 22nd International Conference on Computational Linguistics-Volume",
      "citeRegEx" : "Zagibalov and Carroll,? \\Q2008\\E",
      "shortCiteRegEx" : "Zagibalov and Carroll",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "These two conditions (unsupervised and semi-supervised) are compared against an existing unsupervised method (Turney, 2002).",
      "startOffset" : 109,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "VSM is deeply related to the distributional hypothesis (Turney and Pantel, 2010).",
      "startOffset" : 55,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : "The distributional hypothesis states that words in similar contexts tend to have similar meaning (Rubenstein and Goodenough, 1965; Schütze and Pederson, 1995; Deerwester et al., 1990).",
      "startOffset" : 97,
      "endOffset" : 183
    }, {
      "referenceID" : 8,
      "context" : "Traditionally, the relation of two words in a ‘similar context’ has been distinguished into two classes: syntagmatic or paradigmatic (Murphy, 2003; Sahlgren, 2006).",
      "startOffset" : 133,
      "endOffset" : 163
    }, {
      "referenceID" : 13,
      "context" : "Traditionally, the relation of two words in a ‘similar context’ has been distinguished into two classes: syntagmatic or paradigmatic (Murphy, 2003; Sahlgren, 2006).",
      "startOffset" : 133,
      "endOffset" : 163
    }, {
      "referenceID" : 7,
      "context" : "On the other hand, recent dense vector-based models (Skip-gram, Continuous Bag-of-Words) exploit the paradigmatic relation, and thus they both give a high weight to the similarity of words if they share similar neighboring entities (Mikolov et al., 2013).",
      "startOffset" : 232,
      "endOffset" : 254
    }, {
      "referenceID" : 17,
      "context" : "Our implementation of PMI-IR method (Turney, 2002) for this problem demonstrates this point and hints as to why dense vector space models should be used instead.",
      "startOffset" : 36,
      "endOffset" : 50
    }, {
      "referenceID" : 17,
      "context" : "We believe that this study can be a direct comparison with the previous work of (Turney, 2002), because we use the same seed information and a similar procedure for calculating the semantic orientations of the words.",
      "startOffset" : 80,
      "endOffset" : 94
    }, {
      "referenceID" : 16,
      "context" : "For instance, Turney (2002) suggests the PMI-IR algorithm to estimate the semantic orientation of a phrase for the unsupervised classification of various reviews.",
      "startOffset" : 14,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "His work borrows heavily on the theory of semantic orientation of adjectives by Hatzivassiloglou and McKeown (1997). In this study, the authors discuss the existence of linguistic constraints on the semantic orientations of words in conjunctions.",
      "startOffset" : 80,
      "endOffset" : 116
    }, {
      "referenceID" : 20,
      "context" : "com/company (2002), Zagibalov and Carroll (2008) attempt to develop an automatic selection process for seed words in Chinese texts for unsupervised classifications.",
      "startOffset" : 20,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : "One major constraint on Turney (2002) is the availability of a corpus to calculate the relevant PMI.",
      "startOffset" : 24,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : "One major constraint on Turney (2002) is the availability of a corpus to calculate the relevant PMI. If a given equipped corpus is not big enough for a PMI analysis, the problem of data sparseness will arise, and the PMI values become suspect. Note that Turney (2002) used a search engine (AltaVista) for his experiments, which contained 350 million web pages at the time.",
      "startOffset" : 24,
      "endOffset" : 268
    }, {
      "referenceID" : 17,
      "context" : "One major constraint on Turney (2002) is the availability of a corpus to calculate the relevant PMI. If a given equipped corpus is not big enough for a PMI analysis, the problem of data sparseness will arise, and the PMI values become suspect. Note that Turney (2002) used a search engine (AltaVista) for his experiments, which contained 350 million web pages at the time. In our experiment, we instead use Yadex.com (http://www.yandex.com/), as it provides a more reliable Near operator among the current major search engines. With the operator, words in a query have to be within 10 words of each other, regardless of order. We used the same formula (Eq. 1) from Turney (2002) for our experiment:",
      "startOffset" : 24,
      "endOffset" : 679
    }, {
      "referenceID" : 7,
      "context" : "In contrast to collocation models, some researchers attempt to apply neural probabilistic language models to measure the semantic similarities of words based on context-window methods (Mikolov et al., 2013; Collobert and Weston, 2008), and word embedding methods (e.",
      "startOffset" : 184,
      "endOffset" : 234
    }, {
      "referenceID" : 2,
      "context" : "In contrast to collocation models, some researchers attempt to apply neural probabilistic language models to measure the semantic similarities of words based on context-window methods (Mikolov et al., 2013; Collobert and Weston, 2008), and word embedding methods (e.",
      "startOffset" : 184,
      "endOffset" : 234
    }, {
      "referenceID" : 0,
      "context" : ", Word2Vec) have been found more effective for various tasks in NLP than other traditional techniques (Baroni et al., 2014).",
      "startOffset" : 102,
      "endOffset" : 123
    }, {
      "referenceID" : 11,
      "context" : "Unlike Skip-gram or the CBOW architecture of Word2Vec, GloVe uses the ratios of words’ probability of co-occurrence to learn word vectors (Pennington et al., 2014).",
      "startOffset" : 138,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : ", Word2Vec) have been found more effective for various tasks in NLP than other traditional techniques (Baroni et al., 2014). Continuous Bag-of-Words (CBOW) models and Skip-gram models used in Mikolov et al., (2013) both place a high weight on the similarity of words if they share similar neighboring entities.",
      "startOffset" : 103,
      "endOffset" : 215
    }, {
      "referenceID" : 9,
      "context" : "A feature-space approach (Osgood, 1952; Smith and Medin, 1981; Waltz and Pollack, 1985) discusses how words are represented by feature vectors whose attributional factors are annotated by human participants.",
      "startOffset" : 25,
      "endOffset" : 87
    }, {
      "referenceID" : 15,
      "context" : "A feature-space approach (Osgood, 1952; Smith and Medin, 1981; Waltz and Pollack, 1985) discusses how words are represented by feature vectors whose attributional factors are annotated by human participants.",
      "startOffset" : 25,
      "endOffset" : 87
    }, {
      "referenceID" : 19,
      "context" : "A feature-space approach (Osgood, 1952; Smith and Medin, 1981; Waltz and Pollack, 1985) discusses how words are represented by feature vectors whose attributional factors are annotated by human participants.",
      "startOffset" : 25,
      "endOffset" : 87
    }, {
      "referenceID" : 13,
      "context" : "Another relevant prior work is the Word-Space model, which is a vector-based computational model for the semantic similarity of words (Schütze, 1993; Sahlgren, 2006).",
      "startOffset" : 134,
      "endOffset" : 165
    }, {
      "referenceID" : 6,
      "context" : "However, as noted in Mikolov et al. (2013), words seem to have multiple syntactic/semantic relations to each other, and the Word2Vec model helps to observe the multiple degrees of similarity for words.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "In a similar vein, the Conceptual Space theory of Gärdenfors (2000) provides a theoretical framework for understanding the multiple degrees of similarity in the space.",
      "startOffset" : 50,
      "endOffset" : 68
    }, {
      "referenceID" : 16,
      "context" : ", 2011) and the other is the Stanford Sentiment Treebank (Socher et al., 2013).",
      "startOffset" : 57,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "The Stanford Sentiment Treebank is a corpus based on the 11,855 movie reviews presented by Pang and Lee (2005), and all 215,154 phrases are manually annotated by three judges per phrase.",
      "startOffset" : 91,
      "endOffset" : 111
    }, {
      "referenceID" : 6,
      "context" : "We use the adjectives or adverbs of the phrases for our point words, since these syntactic categories have been found very useful for recognizing subjectivity in written texts (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 1999; Bruce and Wiebe, 2000).",
      "startOffset" : 176,
      "endOffset" : 253
    }, {
      "referenceID" : 20,
      "context" : "We use the adjectives or adverbs of the phrases for our point words, since these syntactic categories have been found very useful for recognizing subjectivity in written texts (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 1999; Bruce and Wiebe, 2000).",
      "startOffset" : 176,
      "endOffset" : 253
    }, {
      "referenceID" : 15,
      "context" : "1) from Turney (2002), we obtained 17,716 phrases by applying these rules to the Stanford Sentiment Treebank.",
      "startOffset" : 8,
      "endOffset" : 22
    }, {
      "referenceID" : 17,
      "context" : "Note that this method was inspired by Turney (2002) and makes our study comparable to the previous work.",
      "startOffset" : 38,
      "endOffset" : 52
    }, {
      "referenceID" : 17,
      "context" : "3 PMI-IR methodology We replicate the PMI-IR algorithm (Turney, 2002) against the Stanford corpus.",
      "startOffset" : 55,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "The classification by PMI-IR is processed by calculating the mean of semantic orientations from the extracted phrases per review as suggested in Turney (2002).",
      "startOffset" : 145,
      "endOffset" : 159
    }, {
      "referenceID" : 9,
      "context" : "Our approach borrows from the general paradigm of Word Space models (Schütze, 1993) and is inspired by the theoretical tools of feature space models (Osgood, 1952; Smith and Medin, 1981; Waltz and Pollack, 1985; Gärdenfors, 2000).",
      "startOffset" : 149,
      "endOffset" : 229
    }, {
      "referenceID" : 15,
      "context" : "Our approach borrows from the general paradigm of Word Space models (Schütze, 1993) and is inspired by the theoretical tools of feature space models (Osgood, 1952; Smith and Medin, 1981; Waltz and Pollack, 1985; Gärdenfors, 2000).",
      "startOffset" : 149,
      "endOffset" : 229
    }, {
      "referenceID" : 19,
      "context" : "Our approach borrows from the general paradigm of Word Space models (Schütze, 1993) and is inspired by the theoretical tools of feature space models (Osgood, 1952; Smith and Medin, 1981; Waltz and Pollack, 1985; Gärdenfors, 2000).",
      "startOffset" : 149,
      "endOffset" : 229
    }, {
      "referenceID" : 7,
      "context" : "The general framework of our experiment is based on Turney (2002) and our computational methods rely on recent successes in word-embedding research (Mikolov et al., 2013; Pennington et al., 2014).",
      "startOffset" : 148,
      "endOffset" : 195
    }, {
      "referenceID" : 11,
      "context" : "The general framework of our experiment is based on Turney (2002) and our computational methods rely on recent successes in word-embedding research (Mikolov et al., 2013; Pennington et al., 2014).",
      "startOffset" : 148,
      "endOffset" : 195
    }, {
      "referenceID" : 4,
      "context" : "Our approach borrows from the general paradigm of Word Space models (Schütze, 1993) and is inspired by the theoretical tools of feature space models (Osgood, 1952; Smith and Medin, 1981; Waltz and Pollack, 1985; Gärdenfors, 2000). The general framework of our experiment is based on Turney (2002) and our computational methods rely on recent successes in word-embedding research (Mikolov et al.",
      "startOffset" : 212,
      "endOffset" : 297
    } ],
    "year" : 2017,
    "abstractText" : "This study implements a vector space model approach to measure the sentiment orientations of words. Two representative vectors for positive/negative polarity are constructed using high-dimensional vector space in both an unsupervised and a semisupervised manner. A sentiment orientation value per word is determined by taking the difference between the cosine distances against the two reference vectors. These two conditions (unsupervised and semi-supervised) are compared against an existing unsupervised method (Turney, 2002). As a result of our experiment, we demonstrate that this novel approach significantly outperforms the previous unsupervised approach and is more practical and data efficient as well.",
    "creator" : "Microsoft® Word 2016"
  }
}