{
  "name" : "562.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Zero-Shot Relation Extraction via Reading Comprehension",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Relation extraction systems populate knowledge bases with facts (relations) from an unstructured text corpus. When the type of facts (schema) are predefined, one can use crowdsourcing (Liu et al., 2016) or distant supervision (Hoffmann et al., 2011) to collect examples and train an extraction model for each relation. However, these approaches are incapable of extracting relation types that were not specified in advance and observed during training. In this paper, we propose an alternative approach for relation extraction, which can potentially extract relations of new types that were neither specified nor observed a priori.\nRelation Question Template educated at(x, y) Where did x graduate from? In which university did x study? What is x’s alma mater? occupation(x, y) What did x do for a living? What is x’s job? What is the profession of x? spouse(x, y) Who is x’s spouse? Who did x marry? Who is x married to?\nWe show that it is possible to reduce relation extraction to the problem of answering simple reading comprehension questions. We map each relation R(x, y) to at least one parametrized naturallanguage question qx whose answer is y. For example, the relation educated at(x, y) can be mapped to “Where did x study?” and “Which university did x graduate from?”. Given a particular entity x (“Turing”) and a text that mentions x (“Turing obtained his PhD from Princeton”), a non-null answer to any of these questions (“Princeton”) asserts the relation and also fills the slot y. Figure 1 illustrates a few more examples.\nThis reduction enables new ways of framing the learning problem. In particular, it allows us to perform zero-shot learning: define new relations “on the fly”, after the model has already been trained. More specifically, the zero-shot scenario assumes access to labeled data for N relations. This data is used to train a reading comprehension model through our reduction. However, at test time, we are asked about a previously unseen relation RN+1. Rather than providing labeled data for the new relation, we simply list questions that define the relation’s slot values. Assuming a good reading comprehension model has been learned, the correct values should be extracted.\nOur zero-shot setup includes innovations both\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nin data and models. We use distant supervision for a relatively large number of relations (120) from Wikidata (Vrandečić, 2012), which are easily gathered in practice via the WikiReading dataset (Hewlett et al., 2016). We introduce a crowdsourcing approach for gathering and verifying the questions for each relation. This process produced about 10 questions per relation on average, yielding a dataset of over 30,000,000 questionsentence-answer examples in total. Because questions are paired with relations, not specific examples, this overall procedure has very modest costs.\nThe key modeling challenge is that most existing reading comprehension problem formulations assume the answer to the question is always present in the given text. However, for relation extraction, this premise does not hold, and the model needs to reliably determine when a question is not answerable. We show that a recent state-of-the-art neural approach for reading comprehension (Seo et al., 2016) can be directly extended with variables to model answerability, and trained directly on our new dataset. This modeling approach is another key advantage of our reduction: as machine reading models improve with time, so should our ability to extract relations.\nExperiments demonstrate that our approach generalizes to new paraphrases of questions from the training set, while incurring only a minor loss in performance (4% relative F1 reduction). Furthermore, translating relation extraction to the realm of reading comprehension allows us to extract a significant portion of previously unseen relations, from virtually zero to an F1 of 41%. Our analysis suggests that our model is able to generalize to these cases by learning typing information that occurs across many relations (e.g. the answer to “Where” is a location), as well as detecting relation paraphrases to a certain extent. We also find that there are many feasible cases that our model does not quite master, providing an interesting challenge for future work."
    }, {
      "heading" : "2 Approach",
      "text" : "We consider the slot-filling challenge in relation extraction, in which we are given a knowledgebase relation R, an entity e, and a sentence s. For example, consider the relation occupation, the entity “Steve Jobs”, and the sentence “Steve Jobs was an American businessman, inventor, and industrial designer”. Our goal is to find\na set of text spans A in s for which R(e, a) holds for each a ∈ A. In our example, A = {businessman, inventor, industrial designer}. The empty set is also a valid answer (A = ∅) when s does not contain any phrase that satisfies R(e, ?). We observe that given a natural-language question q that expresses R(e, ?) (e.g. “What did Steve Jobs do for a living?”), solving the reading comprehension problem of answering q from s is equivalent to solving the slot-filling challenge.\nThe challenge now becomes one of querification: translating R(e, ?) into q. Rather than querify R(e, ?) for every entity e, we propose a method of querifying the relation R. We treat e as a variable x, querify the parametrized query R(x, ?) (e.g. occupation(x, ?)) as a question template qx (“What did x do for a living?”), and then instantiate this template with the relevant entities, creating a tailored natural-language question for each entity e (“What did Steve Jobs do for a living?”). This process, schema querification, is by an order of magnitude more efficient than instance querification because annotating a relation type automatically annotates all of its instances.\nApplying schema querification to N relations from a pre-existing relation-extraction dataset converts it into a reading-comprehension dataset. We then use this dataset to train a readingcomprehension model, which given a sentence s and a question q returns a set of text spans A within s that answer q (to the best of its ability).\nIn the zero-shot scenario, we are given a new relation RN+1(x, y) at test-time, which was neither specified nor observed beforehand. For example, the deciphered(x, y) relation, as in “Turing and colleagues came up with a method for efficiently deciphering the Enigma”, is too domainspecific to exist in common knowledge-bases. We then querify RN+1(x, y) into qx (“Which code did x break?”) or qy (“Who cracked y?”), and run our reading-comprehension model for each sentence in the document(s) of interest, while instantiating the question template with different entities that might participate in this relation. Each time the model returns a non-null answer a for a given question qe, it extracts the relation RN+1(e, a).\nUltimately, all we need to do for a new relation is define our information need in the form of a question. Our approach provides a naturallanguage API for application developers who are interested in incorporating a relation-extraction\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nRelation Question Sentence & Answers\neducated at What is Albert Einstein’s alma mater? Albert Einstein was awarded a PhD by the Universityof Zürich, with his dissertation titled... occupation What did Steve Jobs do for a living? Steve Jobs was an American businessman, inventor,and industrial designer.\nspouse Who is Angela Merkel married to? Angela Merkel’s second and current husband is quantumchemist and professor Joachim Sauer, who has largely...\nFigure 2: Examples from our reading-comprehension dataset. Each instance contains a relation R, a question q, a sentence s, and an answer set A. The question explicitly mentions an entity e, which also appears in s. For brevity, answers are underlined instead of being displayed in a separate column.\ncomponent in their programs; no linguistic knowledge or pre-defined schema is needed. To implement our approach, we require two components: training data and a reading-comprehension model. In Section 3, we construct a large relationextraction dataset and querify it using an efficient crowdsourcing procedure. We then adapt an existing state-of-the-art reading-comprehension model to suit our problem formulation (Section 4)."
    }, {
      "heading" : "3 Dataset",
      "text" : "To collect reading-comprehension examples as in Figure 2, we first gather labeled examples for the task of relation-slot filling. Slot-filling examples are similar to reading-comprehension examples, but contain a knowledge-base query R(e, ?) instead of a natural-language question; e.g. spouse(Angela Merkel, ?) instead of “Who is Angela Merkel married to?”. We collect many slot-filling examples via distant supervision, and then convert their queries into natural language.\nSlot-Filling Data We use the WikiReading dataset (Hewlett et al., 2016) to collect labeled slot-filling examples. WikiReading was collected by aligning each Wikidata (Vrandečić, 2012) relation R(e, a) with the corresponding Wikipedia article D for the entity e, under the reasonable assumption that the relation can be derived from the article’s text. Each instance in this dataset contains a relation R, an entity e, a document D, and an answer a. We used distant supervision to select the specific sentences in which each R(e, a) manifests. Specifically, we took the first sentence s in D to contain both e and a. We then grouped instances by R, e, and s to merge all the answers for R(e, ?) given s into one answer set A.\nSchema Querification We then conducted two annotation phases on Mechanical Turk to querify relations: collection and verification.\nFor each relation R, we present the annotator with 4 example sentences, where the entity e in each sentence s is masked by the variable x. In addition, we underline the extractable answers a ∈ A that appear in s (see Figure 3). The annotator must then come up with a question about x whose answer, given each sentence s, is the underlined span within that sentence. For example, “In which country is x?” captures the exact set of answers for each sentence in Figure 3. Asking a more general question, such as “Where is x?” might return false positives (“North America” in sentence 2).\nEach worker produced 3 different question templates for each example set. For each relation, we sampled 3 different example sets, and hired 3 different annotators for each set. We ran one instance of this annotation phase where the workers were also given, in addition to the example set, the name of the relation (e.g. country), and another instance where it was hidden. Overall, this process collected 40 unique question templates on average, out of a potential 54.\nIn the verification phase, we measure the question templates’ quality by sampling additional sentences and instantiating each question template with the example entity e. Annotators are then asked to answer the question from the sentence s, or mark it as unanswerable; if the annotators’ answers match A, the question template is valid. We discarded the templates that were not answered correctly in the majority of the examples (6/10).1\nOverall, we applied schema querification to 178 relations that had at least 100 examples each (accounting for 99.77% of the data), costing roughly $1,250. After the verification phase, we were left with 1,192 high-quality question templates span-\n1We used this relatively lenient measure because many annotators selected the correct answer, but with a slightly incorrect span; e.g. “American businessman” instead of “businessman”. We therefore used token-overlap F1 as a secondary filter, requiring an average score of at least 0.75.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n(1) The wine is produced in the X region of France. (2) X, the capital of Mexico, is the most populous city in North America. (3) X is an unincorporated and organized territory of the United States. (4) The X mountain range stretches across the United States and Canada.\nFigure 3: An example of the annotator’s input when querifying the country(x, ?) relation. The annotator is required to ask a question about x whose answer is, for each sentence, the underlined spans.\nning 120 relations.2 We then join these templates with our slot-filling dataset along relations, instantiating each template qx with its matching entities. This process yields a reading-comprehension dataset of over 30,000,000 examples, where each instance contains the original relation R (unobserved by the machine), a question q, a sentence s, and the set of answers A (see Figure 2).\nNegative Examples Following the methodology of InfoboxQA (Morales et al., 2016), we generate negative examples by matching (for the same entity e) a question q that pertains to one relation with a sentence s that expresses another relation. We also assert that the sentence does not contain the answer to q. For instance, we match “Who is Angela Merkel married to?” with a sentence about her occupation: “Angela Merkel is a German politician who is currently the Chancellor of Germany”. This process generated 2,249,637 negative examples. While this is a relatively naive method of generating negative examples, our analysis shows that about a third of negative examples contain good distractors (see Section 6)."
    }, {
      "heading" : "4 Model",
      "text" : "Given a sentence s and a question q, our algorithm either returns an answer span3 a within s, or indicates that there is no answer.\nThe task of obtaining answer spans to naturallanguage questions has been recently studied on the SQuAD dataset (Rajpurkar et al., 2016; Xiong et al., 2016; Lee et al., 2016; Wang et al., 2016). In SQuAD, every question is answerable from the text, which is why these models assume that there exists a correct answer span. Therefore, we modify an existing model in a way that allows it to decide whether an answer exists. We first give a high-level description of the original model, and then describe our modification.\n258 relations had zero questions after verification due to noisy distant supervision and little annotator quality control.\n3While our setting allows for multiple answer spans per question, our algorithm assumes a single span; in practice, less than 5% of our data has multiple answers.\nWe start from the BiDAF model (Seo et al., 2016), whose input is two sequences of words: a sentence s and a question q. The model predicts the start and end positions ystart,yend of the answer span in s. BiDAF uses recurrent neural networks to encode contextual information within s and q alongside an attention mechanism to align parts of q with s and vice-versa.\nThe outputs of the BiDAF model are the confidence scores of ystart and yend, for each potential start and end. We denote these scores as zstart, zend ∈ RN , where N is the number of words in the sentence s. In other words, zstarti indicates how likely the answer is to start at position i of the sentence (the higher the more likely); similarly, zendi indicates how likely the answer is to end at that index. Assuming the answer exists, we can transform these confidence scores into pseudo-probability distributions pstart,pend via softmax. The probability of each i-to-j-span of the context can therefore be defined by:\nP (a = si...j) = p start i p end j (1)\nwhere pi indicates the i-th element of the vector pi, i.e. the probability of the answer starting at i. Seo et al. (2016) obtain the span with the highest probability during post-processing.\nTo allow the model to signal that there is no answer, we concatenate a trainable bias b to the end of both confidences score vectors zstart, zend. The new score vectors z̃start, z̃end ∈ RN+1 are defined as z̃start = [zstart; b] and similarly for z̃end, where [; ] indicates row-wise concatenation. Hence, the last elements of z̃start and z̃end indicate the model’s confidence that the answer has no start or end, respectively. We apply softmax to these augmented vectors to obtain pseudo-probability distributions, p̃start, p̃end. This means that the probability the model assigns to a null answer is:\nP (a = ∅) = p̃startN+1p̃endN+1. (2)\nIf P (a = ∅) is higher than the probability of the best span, argmaxi,j≤N P (a = si...j), then the model deems that the question cannot be answered\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nfrom the sentence. Conceptually, adding the bias enables the model to be sensitive to the absolute values of the raw confidence scores zstart, zend. We are essentially setting and learning a threshold b that decides whether the model is sufficiently confident of the best candidate answer span.\nWhile this threshold provides us with a dynamic per-example decision of whether the instance is answerable, we can also set a global confidence threshold pmin; if the best answer’s confidence is below that threshold, we infer that there is no answer. In Section 5.3 we use this global threshold to get a broader picture of the model’s performance."
    }, {
      "heading" : "5 Experiments",
      "text" : "To understand how well our method can generalize to unseen data, we design experiments for unseen entities (Section 5.1), unseen question templates (Section 5.2), and unseen relations (Section 5.3).\nEvaluation Metrics Each instance is evaluated by comparing the tokens in the labeled answer set with those of the predicted span.4 Precision is the true positive count divided by the number of times the system returned a non-null answer. Recall is the true positive count divided by the number of instances that have an answer.\nHyperparameters In our experiments, we initialized word embeddings with GloVe (Pennington et al., 2014), and did not fine-tune them. The typical training set was an order of 1 million examples, for which 3 epochs were enough for convergence. All training sets had a ratio of 1:1 positive and negative examples, which was chosen to match the test sets’ ratio."
    }, {
      "heading" : "5.1 Unseen Entities",
      "text" : "We show that our reading-comprehension approach works well in a typical relation-extraction setting by testing it on unseen entities and texts.\nSetup We partitioned our dataset along entities in the question, and randomly clustered each entity into one of three groups: train, dev, or test. For example, examples about Alan Turing appeared only in the training set, while examples regarding Steve Jobs were exclusive to the test set. We then sampled 1,000,000 examples for train, 1,000 for dev,\n4We ignore word order, case, punctuation, and articles (“a”, “an”, “the”). We also ignore “and”, which often appears when a single span captures multiple correct answers (e.g. “United States and Canada”).\nand 10,000 for test. This partition also ensures that the sentences at test time are different from those in train, since the sentences are gathered from the Wikipedia article of each entity.\nResults Our model correctly predicts the vast majority of test instances, yielding 89.44% F1 (87.66% precision, 91.32% recall). This result suggests that the model does indeed generalize to unseen entities. An analysis of 50 erroneous examples shows that 36% of errors can be attributed to annotation errors (chiefly missing entries in Wikidata), and an additional 42% result from inaccurate span selection (e.g. “8 February 1985” instead of “1985”), for which our model is fully penalized. In total, only 18% of our sample were pure system errors, suggesting that our model is performing even better in practice. This result demonstrates that reducing relation extraction to reading comprehension is indeed a viable approach for our Wikipedia slot-filling task."
    }, {
      "heading" : "5.2 Unseen Question Templates",
      "text" : "We test our method’s ability to generalize to new descriptions of the same relation, by holding out a question template for each relation during training.\nSetup We created 10 folds of train/dev/test samples of the data, in which one question template for each relation was held out for the test set, and another for the development set. For instance, “What did x do for a living?” may appear only in the training set, while “What is x’s job?” is exclusive to the test set. Each split was stratified by sampling N examples per question template (N = 1000, 10, 50 for train, dev, test, respectively). This process created 10 training sets of 966,000 examples with matching development and test sets of 940 and 4,700 examples each.\nComparison System To compare this scenario with one where the question templates were previously observed, we replicated the existing test sets and replaced the unseen question templates with templates from the training set. Revisiting our example, we convert test-set occurrences of “What is x’s job?” with “What did x do for a living?”.\nResults Table 1 shows that our approach is able to generalize to unseen question templates. Our system’s performance on unseen questions is nearly as strong as for previously observed templates (losing roughly 3.5 points in F1).\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nPrecision Recall F1 Seen Templates 86.73% 86.54% 86.63% Unseen Templates 84.37% 81.88% 83.10%\nTable 1: Performance on previously seen vs. unseen question templates. Both results use the same trained model, but Seen Templates tests on examples with templates that appeared in the training set, while Unseen Templates tests on examples with unobserved templates.\nPrecision Recall F1 KB Relation 19.32% 2.54% 4.32% NL Relation 40.50% 28.56% 33.40% Single Template 37.18% 31.24% 33.90% Multiple Templates 43.61% 36.45% 39.61% Question Ensemble 45.85% 37.44% 41.11%\nTable 2: Performance on previously unseen relations. KB Relation does not contain any linguistic information about the relation, NL Relation uses the relation’s name as a natural-language expression, Single Template observes one template per relation during training, while Multiple Templates observes multiple templates per relation during training. Question Ensemble uses the trained Multiple Templates model, but is provided with 3 templates per instance at test time."
    }, {
      "heading" : "5.3 Unseen Relations",
      "text" : "Finally, we test our approach in a true zero-shot setting, where the test-time question templates are not only unobserved during training, they describe new, unseen relations.\nSetup We created 10 folds of train/dev/test samples, partitioned along relations: 84 relations for training, 12 for development, and 24 for test. For example, when the examples of educated at are allocated to the test set, none of them appear in the training set. Each split was stratified by sampling N examples per relation (N = 10000, 50, 500 for train, dev, test, respectively). This process created 10 training sets of 840,000 examples each with matching development and test sets of 600 and 12,000 examples per fold.\nComparison Systems We compare our querification approach to two alternatives. First, we consider using formal knowledge-base relations instead of questions; these are essentially indicators (e.g. relation1) that tie training examples of the same relation together. We expect this alternative to fail, since the new relation’s description does not contain any tangible information. The second\nalternative uses the relation’s name as a naturallanguage string instead of a question. For example, educated at will be interpreted as “educated at”. Unlike the previous baseline, this method allows our machine reading system to interpret its lexical components via word and character embeddings. We also consider a weakened version of our approach where, during training, only one question template per relation is observed. This setting should somewhat hinder the model’s ability to learn paraphrases (e.g. “Where” and “Which country”, active and passive voice, etc).\nQuestion Ensemble We also evaluated how asking (at test time) about the same relation in multiple ways improves performance. This scenario may be more realistic in actual applications, where extractor authors could easily provide a few question paraphrases to define their target relation. We created an ensemble by sampling 3 questions per test example and predicting the answer for each. We then chose the answer with the highest sum of confidence scores.\nResults Table 2 shows the performance of each setting; Figure 4 extends these results by applying a global threshold on the answers’ confidence scores to generate precision/recall curves (see Section 4). As expected, knowledge-base relations are insufficient in a zero-shot setting, and must be interpreted as natural-language to allow for some generalization. The difference between using a single question template and the relation’s name appears to be minor. However, training on a variety of question templates rather than on a single expression substantially increases performance. We conjecture that multiple phrasings of the same\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nVerbatim Relation András Dombai plays for what team?András Dombai... ...currently plays as a goalkeeper for FC Tatabánya.\nType Which airport is most closely associated with Royal Jordanian?Royal Jordanian Airlines... ...from its main base at Queen Alia International Airport...\nGlobal Relation Who was responsible for directing Les petites fugues?Les petites fugues is a 1979 Swiss comedy film directed by Yves Yersin.\nType When was The Snow Hawk released?The Snow Hawk is a 1925 film...\nSpecific Relation Who started Fürstenberg China?The Fürstenberg China Factory was founded... ...by Johann Georg von Langen...\nType What voice type does Étienne Lainez have? Étienne Lainez... ...was a French operatic tenor...\nrelation allows our model to learn answer-type paraphrases that occur across many relations (see further discussion in Section 6). There is also an advantage to having multiple questions at test time; an ensemble of multiple questions per instance allows for a slight but significant improvement in both precision and recall."
    }, {
      "heading" : "6 Analysis",
      "text" : "To understand how our method extracts unseen relations, we analyzed 100 random examples, of which 60 had answers in the sentence and 40 did not (negative examples).\nFor negative examples, we checked whether a distractor – an incorrect answer of the correct answer type – appears in the sentence. For example, the question “Who is John McCain married to?” does not have an answer in “John McCain chose Sarah Palin as his running mate”, but “Sarah Palin” is of the correct answer type. We noticed that 14 negative examples (35%) contain distractors. When pairing these examples with the results from the unseen relations experiment in Section 5.3, we found that our method answered 2/14 of the distractor examples incorrectly, compared to only 1/26 of the easier examples. It appears that while most of the negative examples are easy, a significant portion of them are not trivial.\nFor positive examples, we observed that some instances can be solved by matching the relation in the sentence to that in the question, while others rely more on the answer’s type. Moreover, we notice that each cue can be further categorized according to the type of information needed to detect it: (1) when part of the question appears verbatim in the text, (2) when the phrasing in the text deviates from the question in a way that is typical of other relations as well (e.g. syntactic variability), (3) when the phrasing in the text deviates\nRelation Type Verbatim 5% 12% Global 25% 8% Specific 28% 22%\nTable 3: The distribution of cues by type, based on a sample of 60.\nRelation Type Verbatim 33% 43% Global 73% 60% Specific 18% 46%\nTable 4: Our method’s accuracy on subsets of examples pertaining to different cue types. Results in italics are based on a sample of less than 10.\nfrom the question in a way that is unique to this relation (e.g. lexical variability). We name these categories verbatim, global, and specific, respectively. Figure 5 illustrates all the different types of cues we discuss in our analysis.\nWe selected the most important cue for solving each instance. If there were two important cues, each one was counted as half. Table 3 shows their distribution. Type cues appear to be somewhat more dominant than relation cues (58% vs. 42%). Half of the cues are relation-specific, whereas global cues account for one third of the cases and verbatim cues for one sixth. This is an encouraging result, because we can potentially learn to accurately recognize verbatim and global cues from other relations. However, our method was only able to exploit these cues partially.\nWe paired these examples with the results from the unseen relations experiment in Section 5.3 to see how well our method performs in each category. Table 4 shows the results for the Multiple Templates setting. On one hand, the model appears agnostic to whether the relation cue is verbatim, global, or specific, and is able to correctly answer these instances with similar accuracy (there\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nis no clear trend due to the small sample size). For examples that rely on typing information, the trend is much clearer; our model is much better at detecting global type cues than specific ones.\nBased on these observations, we think that the primary sources of our model’s ability to generalize to new relations are: global type detection, which is acquired from training on many different relations, and relation paraphrase detection (of all types), which probably relies on its pre-trained word embeddings."
    }, {
      "heading" : "7 Related Work",
      "text" : "Zero-Shot Relation Extraction There has been relatively little work on zero-shot learning in information extraction. Rocktäschel et al. (2015) and Demeester et al. (2016) show that injecting precomputed inference rules between naturallanguage relations and knowledge-base relations in a universal-schema setting (Riedel et al., 2013), allows their system to predict whether a given entity pair takes part in an unseen knowledgebase relation. In their setting, all natural-language relations are observed at train time, while all knowledge-base relations are hidden until test time. This setting is akin to our unseen question templates experiment (Section 5.2) since a naturallanguage description of each target relation appears in the training data. The “pure” zero-shot scenario – in which no manifestation of the test relation was observed – is substantially more challenging (see Section 5.3).\nBronstein et al. (2015) offered an almost unsupervised approach for event-trigger identification. In their setting, a few seed trigger words are given at test time, and by using a collection of lexical inference and similarity features (tuned by different event types during training), their system is able to detect the desired event-type. We focus instead on the complementary challenge of slot filling, where natural-language questions align perhaps more directly with the values we hope to recover.\nOpen information extraction (Mausam et al., 2012) can be seen as a type of zero-shot extraction, since there is no need for relation-specific training data. However, such systems often treat every possible string as a new relation while we hope to, through the reduction to reading comprehension, extract a canonical slot value independent of how the original text is phrased.\nReading Comprehension Many reading comprehension formulations have been proposed recently (Richardson et al., 2013; Berant et al., 2014; Hermann et al., 2015; Weston et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016). A typical example involves a text, a question (or cloze), and assumes that the answer is mentioned in the text. In comparison, our questions are relatively direct and easy, but we must also predict when the question is not answerable, to support relation extraction.\nQuerification Some recent question-answering datasets were collected by expressing knowledgebase assertions in natural language. The Simple QA dataset (Bordes et al., 2015) was created by annotating questions about individual Freebase facts (e.g. educated at(Turing, Princeton)), collecting roughly 100,000 natural-language questions to support QA against a knowledge graph. Morales et al. (2016) used a similar process to collect questions from Wikipedia infoboxes, yielding the 15,000-example InfoboxQA dataset.\nFor the task of identifying predicate-argument structures, QA-SRL (He et al., 2015) was proposed as an open schema for semantic roles, in which the relation between an argument and a predicate is expressed as a natural-language question containing the predicate (“Where was someone educated?”) whose answer is the argument (“Princeton”). The authors collected about 19,000 question-answer pairs from 3,200 sentences.\nIn these efforts, the costs scale linearly in the number of instances, requiring significant investments for large datasets. In contrast, schema querification can generate an enormous amount of data for a fraction of the cost by labeling at the relation level; in this work, we generated 30,000,000 examples with a budget of $1,250 (see Section 3)."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We showed that relation extraction can be reduced to a reading comprehension problem, allowing us to generalize to unseen relations that are defined on-the-fly in natural language. However, the problem of zero-shot relation extraction is far from solved, and poses an interesting challenge to both the information extraction and machine reading communities. As research into machine reading progresses, we may find that more tasks can benefit from a similar approach. We hope the contributions and insights presented in this paper will support future work in this avenue.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Modeling biological processes for reading comprehension",
      "author" : [ "Jonathan Berant", "Vivek Srikumar", "Pei-Chun Chen", "Abby Vander Linden", "Brittany Harding", "Brad Huang", "Peter Clark", "Christopher D Manning." ],
      "venue" : "Proceedings of the Conference",
      "citeRegEx" : "Berant et al\\.,? 2014",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2014
    }, {
      "title" : "Large-scale simple question answering with memory networks",
      "author" : [ "Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1506.02075 .",
      "citeRegEx" : "Bordes et al\\.,? 2015",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2015
    }, {
      "title" : "Seed-based event trigger labeling: How far can event descriptions get us",
      "author" : [ "Ofer Bronstein", "Ido Dagan", "Qi Li", "Heng Ji", "Anette Frank" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-",
      "citeRegEx" : "Bronstein et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bronstein et al\\.",
      "year" : 2015
    }, {
      "title" : "Lifted rule injection for relation embeddings",
      "author" : [ "Thomas Demeester", "Tim Rocktäschel", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computa-",
      "citeRegEx" : "Demeester et al\\.,? 2016",
      "shortCiteRegEx" : "Demeester et al\\.",
      "year" : 2016
    }, {
      "title" : "Question-answer driven semantic role labeling: Using natural language to annotate natural language",
      "author" : [ "Luheng He", "Mike Lewis", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Lan-",
      "citeRegEx" : "He et al\\.,? 2015",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomáš Kočiský", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Wikireading: A novel large-scale language understanding task over wikipedia",
      "author" : [ "Daniel Hewlett", "Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot." ],
      "venue" : "Proceedings of the Conference of the",
      "citeRegEx" : "Hewlett et al\\.,? 2016",
      "shortCiteRegEx" : "Hewlett et al\\.",
      "year" : 2016
    }, {
      "title" : "The goldilocks principle: Reading children’s books with explicit memory representations",
      "author" : [ "Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston." ],
      "venue" : "The International Conference on Learning Representations .",
      "citeRegEx" : "Hill et al\\.,? 2015",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2015
    }, {
      "title" : "Knowledgebased weak supervision for information extraction",
      "author" : [ "Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld" ],
      "venue" : null,
      "citeRegEx" : "Hoffmann et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hoffmann et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning recurrent span representations for extractive question answering",
      "author" : [ "Kenton Lee", "Tom Kwiatkowski", "Ankur Parikh", "Dipanjan Das." ],
      "venue" : "arXiv preprint arXiv:1611.01436 .",
      "citeRegEx" : "Lee et al\\.,? 2016",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2016
    }, {
      "title" : "Effective crowd annotation for relation extraction",
      "author" : [ "Angli Liu", "Stephen Soderland", "Jonathan Bragg", "Christopher H. Lin", "Xiao Ling", "Daniel S. Weld." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Associa-",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Open language learning for information extraction",
      "author" : [ "Mausam", "Michael Schmitz", "Stephen Soderland", "Robert Bart", "Oren Etzioni." ],
      "venue" : "EMNLP-CoNLL.",
      "citeRegEx" : "Mausam et al\\.,? 2012",
      "shortCiteRegEx" : "Mausam et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning to answer questions from wikipedia infoboxes",
      "author" : [ "Alvaro Morales", "Varot Premtoon", "Cordelia Avery", "Sue Felshin", "Boris Katz." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association",
      "citeRegEx" : "Morales et al\\.,? 2016",
      "shortCiteRegEx" : "Morales et al\\.",
      "year" : 2016
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "P. Rajpurkar", "J. Zhang", "K. Lopyrev", "P. Liang." ],
      "venue" : "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Mctest: A challenge dataset for the open-domain machine comprehension of text",
      "author" : [ "Matthew Richardson", "Christopher JC Burges", "Erin Renshaw." ],
      "venue" : "Proceedings of the Conference of the Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Richardson et al\\.,? 2013",
      "shortCiteRegEx" : "Richardson et al\\.",
      "year" : 2013
    }, {
      "title" : "Relation extraction with matrix factorization and universal schemas",
      "author" : [ "Sebastian Riedel", "Limin Yao", "Andrew McCallum", "Benjamin M. Marlin." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the As-",
      "citeRegEx" : "Riedel et al\\.,? 2013",
      "shortCiteRegEx" : "Riedel et al\\.",
      "year" : 2013
    }, {
      "title" : "Injecting logical background knowledge into embeddings for relation extraction",
      "author" : [ "Tim Rocktäschel", "Sameer Singh", "Sebastian Riedel." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Associa-",
      "citeRegEx" : "Rocktäschel et al\\.,? 2015",
      "shortCiteRegEx" : "Rocktäschel et al\\.",
      "year" : 2015
    }, {
      "title" : "Bidirectional attention flow for machine comprehension",
      "author" : [ "Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi." ],
      "venue" : "arXiv preprint arXiv:1611.01603 .",
      "citeRegEx" : "Seo et al\\.,? 2016",
      "shortCiteRegEx" : "Seo et al\\.",
      "year" : 2016
    }, {
      "title" : "Wikidata: A new platform for collaborative data collection",
      "author" : [ "Denny Vrandečić." ],
      "venue" : "Proceedings of the 21st international conference companion on World Wide Web. ACM, pages 1063–1064.",
      "citeRegEx" : "Vrandečić.,? 2012",
      "shortCiteRegEx" : "Vrandečić.",
      "year" : 2012
    }, {
      "title" : "Multi-perspective context matching for machine comprehension",
      "author" : [ "Zhiguo Wang", "Haitao Mi", "Wael Hamza", "Radu Florian." ],
      "venue" : "arXiv preprint arXiv:1612.04211 .",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards ai-complete question answering: A set of prerequisite toy tasks",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merriënboer", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "arXiv preprint arXiv:1502.05698 .",
      "citeRegEx" : "Weston et al\\.,? 2015",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    }, {
      "title" : "Dynamic coattention networks for question answering",
      "author" : [ "Caiming Xiong", "Victor Zhong", "Richard Socher." ],
      "venue" : "arXiv preprint arXiv:1611.01604 .",
      "citeRegEx" : "Xiong et al\\.,? 2016",
      "shortCiteRegEx" : "Xiong et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "When the type of facts (schema) are predefined, one can use crowdsourcing (Liu et al., 2016) or distant supervision (Hoffmann et al.",
      "startOffset" : 74,
      "endOffset" : 92
    }, {
      "referenceID" : 8,
      "context" : ", 2016) or distant supervision (Hoffmann et al., 2011) to collect examples and train an extraction model for each relation.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 19,
      "context" : "We use distant supervision for a relatively large number of relations (120) from Wikidata (Vrandečić, 2012), which are easily gathered in practice via the WikiReading dataset (Hewlett et al.",
      "startOffset" : 90,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "We use distant supervision for a relatively large number of relations (120) from Wikidata (Vrandečić, 2012), which are easily gathered in practice via the WikiReading dataset (Hewlett et al., 2016).",
      "startOffset" : 175,
      "endOffset" : 197
    }, {
      "referenceID" : 18,
      "context" : "We show that a recent state-of-the-art neural approach for reading comprehension (Seo et al., 2016) can be directly extended with variables to model answerability, and trained directly on our new dataset.",
      "startOffset" : 81,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "Slot-Filling Data We use the WikiReading dataset (Hewlett et al., 2016) to collect labeled slot-filling examples.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 19,
      "context" : "WikiReading was collected by aligning each Wikidata (Vrandečić, 2012) relation R(e, a) with the corresponding Wikipedia article D for the entity e, under the reasonable assumption that the relation can be derived from the article’s text.",
      "startOffset" : 52,
      "endOffset" : 69
    }, {
      "referenceID" : 12,
      "context" : "Negative Examples Following the methodology of InfoboxQA (Morales et al., 2016), we generate negative examples by matching (for the same entity e) a question q that pertains to one relation with a sentence s that expresses another relation.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 14,
      "context" : "The task of obtaining answer spans to naturallanguage questions has been recently studied on the SQuAD dataset (Rajpurkar et al., 2016; Xiong et al., 2016; Lee et al., 2016; Wang et al., 2016).",
      "startOffset" : 111,
      "endOffset" : 192
    }, {
      "referenceID" : 22,
      "context" : "The task of obtaining answer spans to naturallanguage questions has been recently studied on the SQuAD dataset (Rajpurkar et al., 2016; Xiong et al., 2016; Lee et al., 2016; Wang et al., 2016).",
      "startOffset" : 111,
      "endOffset" : 192
    }, {
      "referenceID" : 9,
      "context" : "The task of obtaining answer spans to naturallanguage questions has been recently studied on the SQuAD dataset (Rajpurkar et al., 2016; Xiong et al., 2016; Lee et al., 2016; Wang et al., 2016).",
      "startOffset" : 111,
      "endOffset" : 192
    }, {
      "referenceID" : 20,
      "context" : "The task of obtaining answer spans to naturallanguage questions has been recently studied on the SQuAD dataset (Rajpurkar et al., 2016; Xiong et al., 2016; Lee et al., 2016; Wang et al., 2016).",
      "startOffset" : 111,
      "endOffset" : 192
    }, {
      "referenceID" : 18,
      "context" : "We start from the BiDAF model (Seo et al., 2016), whose input is two sequences of words: a sentence s and a question q.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 18,
      "context" : "Seo et al. (2016) obtain the span with the highest probability during post-processing.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 13,
      "context" : "Hyperparameters In our experiments, we initialized word embeddings with GloVe (Pennington et al., 2014), and did not fine-tune them.",
      "startOffset" : 78,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "(2016) show that injecting precomputed inference rules between naturallanguage relations and knowledge-base relations in a universal-schema setting (Riedel et al., 2013), allows their system to predict whether a given entity pair takes part in an unseen knowledgebase relation.",
      "startOffset" : 148,
      "endOffset" : 169
    }, {
      "referenceID" : 11,
      "context" : "Open information extraction (Mausam et al., 2012) can be seen as a type of zero-shot extraction, since there is no need for relation-specific training data.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 15,
      "context" : "Reading Comprehension Many reading comprehension formulations have been proposed recently (Richardson et al., 2013; Berant et al., 2014; Hermann et al., 2015; Weston et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 222
    }, {
      "referenceID" : 0,
      "context" : "Reading Comprehension Many reading comprehension formulations have been proposed recently (Richardson et al., 2013; Berant et al., 2014; Hermann et al., 2015; Weston et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 222
    }, {
      "referenceID" : 5,
      "context" : "Reading Comprehension Many reading comprehension formulations have been proposed recently (Richardson et al., 2013; Berant et al., 2014; Hermann et al., 2015; Weston et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 222
    }, {
      "referenceID" : 21,
      "context" : "Reading Comprehension Many reading comprehension formulations have been proposed recently (Richardson et al., 2013; Berant et al., 2014; Hermann et al., 2015; Weston et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 222
    }, {
      "referenceID" : 7,
      "context" : "Reading Comprehension Many reading comprehension formulations have been proposed recently (Richardson et al., 2013; Berant et al., 2014; Hermann et al., 2015; Weston et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 222
    }, {
      "referenceID" : 14,
      "context" : "Reading Comprehension Many reading comprehension formulations have been proposed recently (Richardson et al., 2013; Berant et al., 2014; Hermann et al., 2015; Weston et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016).",
      "startOffset" : 90,
      "endOffset" : 222
    }, {
      "referenceID" : 8,
      "context" : "Rocktäschel et al. (2015) and Demeester et al.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "(2015) and Demeester et al. (2016) show that injecting precomputed inference rules between naturallanguage relations and knowledge-base relations in a universal-schema setting (Riedel et al.",
      "startOffset" : 11,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "Bronstein et al. (2015) offered an almost unsupervised approach for event-trigger identification.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : "The Simple QA dataset (Bordes et al., 2015) was created by annotating questions about individual Freebase facts (e.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "For the task of identifying predicate-argument structures, QA-SRL (He et al., 2015) was proposed as an open schema for semantic roles, in which the relation between an argument and a predicate is expressed as a natural-language question containing the predicate (“Where was someone educated?”) whose answer is the argument (“Princeton”).",
      "startOffset" : 66,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "The Simple QA dataset (Bordes et al., 2015) was created by annotating questions about individual Freebase facts (e.g. educated at(Turing, Princeton)), collecting roughly 100,000 natural-language questions to support QA against a knowledge graph. Morales et al. (2016) used a similar process to collect questions from Wikipedia infoboxes, yielding the 15,000-example InfoboxQA dataset.",
      "startOffset" : 23,
      "endOffset" : 268
    } ],
    "year" : 2017,
    "abstractText" : "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn high-quality relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relations that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relations with high accuracy, and that zero-shot generalization to unseen relations is possible, at lower accuracy levels, setting the bar for future work on this task.",
    "creator" : "LaTeX with hyperref package"
  }
}