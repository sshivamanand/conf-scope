{
  "name" : "501.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\nWe introduce a new multi-modal task for computer systems, posed as a combined vision-language comprehension challenge: identifying the most suitable text describing a scene, given several similar options. Accomplishing the task entails demonstrating comprehension beyond just recognizing “keywords” (or key-phrases) and their corresponding visual concepts. Instead, it requires an alignment between the representations of the two modalities that achieves a visually-grounded “understanding” of various linguistic elements and their dependencies. This new task also admits an easy-to-compute and wellstudied metric: the accuracy in detecting the true target among the decoys.\nThe paper makes several contributions: an effective and extensible mechanism for generating decoys from (human-created) image captions; an instance of applying this mechanism, yielding a large-scale machine comprehension dataset (based on the COCO images and captions) that we make publicly available; human evaluation results on this dataset, informing a performance upper-bound; and several baseline and competitive learning approaches that illustrate the utility of the proposed task and dataset in advancing both image and language comprehension. We also show that, in a multi-task learning setting, the performance on the proposed task is positively correlated with the end-to-end task of image captioning."
    }, {
      "heading" : "1 Introduction",
      "text" : "There has been a great deal of interest in multimodal artificial intelligence research recently, bringing together the fields of Computer Vision and Natural Language Processing. This interest has been fueled in part by the availability of many large-scale image datasets with textual annotations. Several vision+language tasks have been proposed around these datasets (Hodosh et al., 2013; Karpathy and Fei-Fei, 2015; Lin et al., 2014; Antol et al., 2015). Image Captioning (Hodosh et al., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Fang et al., 2015; Kiros et al., 2015; Vinyals et al., 2015; Mao et al., 2015; Xu et al., 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al., 2015; Tu et al., 2014; Antol et al., 2015; Yu et al., 2015; Wu et al., 2016b; Ren et al., 2015; Gao et al., 2015; Yang et al., 2016; Zhu et al., 2016; Lin and Parikh, 2016) have in particular attracted a lot of attention. The performances on these tasks have been steadily improving, owing much to the wide use of deep learning architectures (Bengio, 2009).\nA central theme underlying these efforts is the use of natural language to identify how much visual information is perceived and understood by a computer system. Presumably, a system that understands a visual scene well enough ought to be able to describe what the scene is about (thus “captioning”) or provide correct and visuallygrounded answers when queried (thus “questionanswering”).\nIn this paper, we argue for directly measuring how well the semantic representations of the visual and linguistic modalities align (in some abstract semantic space). For instance, given an image and two captions – a correct one and an incorrect yet-cunningly-similar one – can we both qualitatively and quantitatively measure the ex-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ntent to which humans can dismiss the incorrect one but computer systems blunder? Arguably, the degree of the modal alignment is a strong indicator of task-specific performance on any vision+language task. Consequentially, computer systems that can learn to maximize and exploit such alignment should outperform those that do not.\nWe take a two-pronged approach for addressing this issue. First, we introduce a new and challenging Dual Machine Comprehension (DMC) task, in which a computer system must identify the most suitable textual description from several options: one being the target and the others being “adversarialy”-chosen decoys. All options are free-form, coherent, and fluent sentences with high degrees of semantic similarity (hence, they are “cunningly similar”). A successful computer system has to demonstrate comprehension beyond just recognizing “keywords” (or key phrases) and their corresponding visual concepts; they must arrive at a coinciding and visually-grounded understanding of various linguistic elements and their dependencies. What makes the DMC task even more appealing is that it admits an easy-tocompute and well-studied performance metric: the accuracy in detecting the true target among the decoys. Second, we illustrate how solving the DMC task benefits related vision+language tasks. To this end, we render the DMC task as a classification problem, and incorporate it in a multitask learning framework for end-to-end training of joint objectives.\nOur work makes the following contributions: (1) an effective and extensible algorithm for generating decoys from human-created image captions (Section 3.2); (2) an instantiation of applying this algorithm to the COCO dataset (Lin et al., 2014), resulting in a large-scale dual machinecomprehension dataset that we make publicly available (Section 3.3); (3) a human evaluation on this dataset, which provides an upper-bound on performance (Section 3.4); (4) a benchmark study of baseline and competitive learning approaches (Section 5), which underperform humans by a substantial gap (about 20%); and (5) a multi-task learning model that simultaneously learns to solve the DMC task and the Image Captioning task (Section 4).\nOur empirical study shows that performance on the DMC task positively correlates with per-\nformance on the Image Captioning task. Therefore, besides acting as a standalone benchmark, the new DMC task can be useful in improving other complex vision+language tasks. Both suggest the DMC task as a fruitful direction for future research."
    }, {
      "heading" : "2 Related work",
      "text" : "Image understanding is a long-standing challenge in computer vision. There has recently been a great deal of interest in bringing together vision and language understanding. Particularly relevant to our work are image captioning (IC) and visual question-answering (VQA). Both have instigated a large body of publications, a detailed exposition of which is beyond the scope of this paper. Interested readers should refer to two recent surveys (Bernardi et al., 2016; Wu et al., 2016a).\nIn IC tasks, systems attempt to generate a fluent and correct sentence describing an input image. IC systems are usually evaluated on how well the generated descriptions align with human-created captions (ground-truth). The language generation model of an IC system plays a crucial role; it is often trained such that the probabilities of the ground-truth captions are maximized (MLE training), though more advanced methods based on techniques borrowed from Reinforcement Learning have been proposed (Ranzato et al., 2015). To provide visual grounding, image features are extracted and injected into the language model. Note that language generation models need to both decipher the information encoded in the visual features, and model natural language generation.\nIn VQA tasks, the aim is to answer an input question correctly with respect to a given input image. In many variations of this task, answers are limited to single words or a binary response (“yes” or “no”) (Antol et al., 2015). The Visual7W dataset (Zhu et al., 2016) contains anaswers in a richer format such as phrases, but limits questions to “wh-”style (what, where, who, etc). The Visual Genome dataset (Krishna et al., 2016), on the other hand, can potentially define more complex questions and answers due to its extensive textual annotations.\nOur DMC task is related but significantly different. In our task, systems attempt to discriminate the best caption for an input image from a set of captions — all but one are decoys. Arguably, it is a form of VQA task, where the same default\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n(thus uninformative) question is asked: Which of the following sentences best describes this image? However, unlike current VQA tasks, choosing the correct answer in our task entails a deeper “understanding” of the available answers. Thus, to perform well, a computer system needs to understand both complex scenes (visual understanding) and complex sentences (language understanding), and be able to reconcile them.\nThe DMC task admits a simple classificationbased evaluation metric: the accuracy of selecting the true target. This is a clear advantage over the IC tasks, which often rely on imperfect metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), or SPICE (Anderson et al., 2016).\nRelated to our proposal is the work in (Hodosh et al., 2013), which frames image captioning as a ranking problem. While both share the idea of selecting captions from a large set, our framework has some important and distinctive components. First, we devise an algorithm for smart selection of candidate decoys, with the goal of selecting those that are sufficiently similar to the true targets to be challenging, and yet still be reliably identifiable by human raters. Second, we have conducted a thorough human evaluation in order to establish a performance ceiling, while also quantifying the level to which current learning systems underperform. Lastly, we show that there exists a positive correlation between the performance on the DMC task and the performance on related vision+language tasks by proposing and experimenting with a multi-task learning model. Our work is also substantially different from their more recent work (Hodosh and Hockenmaier, 2016), where only one decoy is considered and its generation is either random, or focusing on visual concept similarity (“switching people or scenes”) instead of our focus on both linguistic surface and paragraph vector embedding similarity."
    }, {
      "heading" : "3 The Dual Machine Comprehension Task",
      "text" : ""
    }, {
      "heading" : "3.1 Design overview",
      "text" : "We propose a new multi-modal machine comprehension task to examine how well visual and textual semantic understanding are aligned. Given an image, human evaluators or machines must accurately identify the best sentence describing the\nscene from several decoy sentences. Accuracy on this task is defined as the percentage that the true targets are identified.\nIt seems straightforward to construct a dataset for this task, as there are several existing datasets which are composed of images and their (multiple) ground-truth captions, including the popular COCO dataset (Lin et al., 2014). Thus, for any given image, it appears that one just needs to use the captions corresponding to other images as decoys. However, this naı̈ve approach could be overly simplistic as it is provides no control over the properties of the decoys.\nSpecifically, our desideratum is to recruit challenging decoys that are sufficiently similar to the targets. However, for a small number of decoys, e.g. 4-5, randomly selected captions could be significantly different from the target. The resulting dataset would be too “easy” to shed any insight on the task. Since we are also interested in human performance on this task, it is thus impractical to increase the number of decoys to raise the difficulty level of the task at the expense of demanding humans to examine tediously and unreliably a large number of decoys. In short, we need an automatic procedure to reliably create difficult sets of decoy captions that are sufficiently similar to the targets.\nWe describe such a procedure in the following. While it focuses on identifying decoy captions, the main idea is potentially adaptable to other settings. The algorithm is flexible in that the “difficulty” of the dataset can be controlled to some extent through the algorithm’s parameters."
    }, {
      "heading" : "3.2 Algorithm to create an MC-IC dataset",
      "text" : "The main idea behind our algorithm is to carefully define a “good decoy”. The algorithm exploits recent advances in paragraph vector (PV) models (Le and Mikolov, 2014), while also using linguistic surface analysis to define similarity between two sentences. Due to space limits, we omit a detailed introduction of the PV model. It suffices to note that the model outputs a continuouslyvalued embedding for a sentence, a paragraph, or even a document.\nThe pseudo-code for the algorithm is in the Listing 1 (the name MC-IC stands for “MachineComprehension for Image-Captions”). As input, the algorithm takes a large set C of\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nAlgorithm 1: MC-IC(C, N , Score) Result: Dataset MCIC PV← OPTIMIZE-PV(C) λ← OPTIMIZE-SCORE(PV, C, Score) MCIC ← ∅ nr decoys = 4 for 〈ii, ci〉 ∈ C do\nA← [] Tci ← PV(ci)[1..N ] for cd ∈ Tci do\nscore← Score(PV, λ, cd, ci) if score > 0 then\nA.append(〈score, cd〉) end\nend if |A|≥ nr decoys then\nR← descending-sort(A) for l ∈ [1..nr decoys] do 〈score, cd〉 ← R[l] MCIC ← MCIC ∪{(〈ii, cd〉, false)} end MCIC ← MCIC ∪{(〈ii, ci〉, true)}\nend end\n〈image, {caption(s)} 〉 pairs∗, as those extracted from a variety of publicly-available corpora, including the COCO dataset (Lin et al., 2014). The output of the algorithm is the dataset MCIC which is used for the DMC task.\nConcretely, the MC-IC Algorithm has three main arguments: a dataset C = {〈ii, ci〉|1 ≤ i ≤ m} where ii is an image and ci is its ground-truth caption. For an image with multiple groundtruth captions, we split it to multiple instances with the same image and one unique groundtruth caption per instance. An integerN which controls the size of ci’s neighborhood in the embedding space defined by the paragraph vector model PV; and a function Score which is used to score theN items in each such neighborhood.\nThe first two steps of the algorithm tune several hyperparameters. The first step finds optimal settings for the PV model given the dataset C. The second finds a weight parameter λ given PV, dataset C, and the Score function. These hyperparameters are dataset-specific. Details are discussed in the next section.\nThe main body of the algorithm, the outer for loop, generates a set of nr decoys (4 here) decoys for each ground-truth caption. It accomplishes this by first extracting N candidates from the PV neighborhood of the ground-truth caption, excluding those that belong to the same image.\n∗On the order of at least hundreds of thousands of examples; smaller sets result in less challenging datasets.\nIn the inner for loop, it computes the similarity of each candidate to the ground-truth and stores them in a list A. If enough candidates are generated, the list is sorted in descending order of score. The top nr decoys captions are marked as “decoys” (i.e.false), while the ground-truth caption is marked as “target” (i.e.true).\nThe score function Score(PV, λ, c′, c) is a crucial component of the decoy selection mechanism. Its definition leverages our linguistic intuition by combining linguistic surface similarity, simSURF(c′, c), with the similarity suggested by the embedding model, simPV(c′, c):\nScore= {\n0 if simSURF≥L λ simPV+(1−λ) simSURF otherwise (1)\nwhere the common argument (c′, c) is omitted. The higher the similarity score, the more likely that c′ is a good decoy for c. Note that if the surface similarity is above the threshold L, the function returns 0, flagging that the two captions are too similar to be used as a pair of target and decoy.\nIn this work, simSURF is computed as the BLEU score between the inputs (Papineni et al., 2002) (with the brevity penalty set to 1). The embedding similarity, simPV, is computed as the cosine similarity between the two in the PV embedding space."
    }, {
      "heading" : "3.3 The MCIC dataset",
      "text" : "We applied the MC-IC Algorithm to the COCO dataset (Lin et al., 2014) to generate a dataset for the visual-language dual machine comprehension task. The dataset, called MCIC , is made publicly available (address anonymized). We describe the details of this dataset below.\nWe set the neighborhood size at N = 500, and the threshold atL = 0.5 (see Eq. 1). As the COCO dataset has a large body of images (thus captions) focusing on a few categories (such as sports activities), this threshold is important in discarding significantly similar captions to be decoys – otherwise, even human annotators will experience difficulty in selecting the ground-truth captions.\nThe hyperparameters of the PV model, dim (embedding dimension) and epochs (number of training epochs), are optimized in the OPTIMIZE-PV step of the MC-IC Algorithm. The main idea is to learn embeddings such that ground-truth captions from the same image have similar embeddings. Details are in the Suppl. Ma-\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nSplit dev test train total #unique˙images 2,000 2,000 110,800 114,800\n# instances 9,999 10,253 554,063 574,315\nTable 1: MCIC dataset descriptive statistics\nterial. The optimal settings are dim=1024 and epochs=5.\nLikewise, the λ in the Score function is optimized by the OPTIMIZE-SCORE step, such that ground-truth captions associated with the same image have similar scores. Details are in the Suppl. Material. The optimal λ is 0.3.\nThe resulting MCIC dataset has 574,315 instances that are in the format of {i : (〈ii, cji 〉, label j i ), j = 1 . . . 5} where label j i ∈ {true, false}. For each such instance, there is one and only one j such that the label is true. We have created a train/dev/test split such that all of the instances for the same image occur in the same split. Table 1 reports the basic statistics for the dataset. Figure 1 shows several instances from the MCIC dataset."
    }, {
      "heading" : "3.4 Human performance on MCIC",
      "text" : "To measure how well humans can perform on the DMC task, we randomly drew 1,000 instances from the MCIC dev set and submitted those instances to human “raters”† via a crowdsourcing platform.\nThree independent responses from 3 different raters were gathered for each instance, for a total of 3,000 responses. To ensure diversity, raters were prohibited from evaluating more than six instances or from responding to the same task instance twice. In total, 807 distinct raters were employed.\nRaters were shown one instance at a time. They were shown the image and the five caption choices (ground-truth and four decoys) and were instructed to choose the best caption for the image. To supplement the instructions, raters were initially shown a few examples from the training set with the ground-truth caption highlighted, to illustrate how to discern the most appropriate caption for the image (see the Suppl. Material for details).\nWe assessed human performance using two metrics: (1) Percentage of correct rater responses (1-human system): 81.1% (2432 out of 3000); (2) †Raters are vetted, screened and tested before working on any tasks; requirements include native-language proficiency level.\nPercentage of instances with at least 50% (i.e.2) correct responses (3-human system): 82.8% (828 out of 1000). Due to space limitation, more discussions about the human raters disagreement is available in the Suppl. Material."
    }, {
      "heading" : "4 Learning Methods",
      "text" : "We describe several learning methods for the dual machine comprehension (DMC) task with the MCIC dataset.\nRegression. To examine how well the two embeddings are aligned in “semantic understanding space”, a simple approach is to assume that the learners do not have access to the decoys. Instead, by accessing the ground-truth captions only,\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nthe models learn a linear regressor from the image embeddings to the target captions’ embeddings (“forward regression”), or from the captions to the images (“backward regression”). With the former approach, referred as Baseline-I2C, we check whether the predicted caption for any given image is closest to its true caption. With the latter, referred as Baseline-C2I, we check whether the predicted image embedding by the ground-truth caption is the closest among predicted ones by decoy captions to the real image embeddings.\nLinear classifier. Our next approach BaselineLinM is a linear classifier learned to discriminate true targets from the decoys. Specifically, we learn a linear discriminant function f(i, c;θ) = i> θ c where θ is a matrix measuring the compatibility between two types of embeddings, cf. (Frome et al., 2013). The loss function is then given by\nL(θ) = ∑ i [max j 6=j∗ f(ii, c j i ;θ)− f(ii, c j∗ i ;θ)]+ (2)\nwhere [ ]+ is the hinge function and j indexes over all the available decoys and i indexes over all training instances. The optimization tries to increase the gap between the target cj ∗\ni and the worst “offending” decoy. We use stochastic (sub)gradient methods to optimize θ, and select the best model in terms of accuracy on the MCIC dev set.\nFFNN Model. To present our neural-network– based models, we use the following notations. Each training instance pair is a tuple 〈ii, cji 〉, where i denotes the image, and cji denotes the caption options, which can either be the target or the decoys. We use a binary variable yijk ∈ {0, 1} to denote whether j-th caption of the instance i is labeled as k, and ∑ k yijk = 1.\nWe first employ the standard feedforward neural-network models to solve the MCIC task. For each instance pair 〈ii, cji 〉, the input to the neural network is an embedding tuple 〈DNN(ii; Γ),Emb(cji ; Ω)〉, where Γ denotes the parameters of a deep convolutional neural network DNN. DNN takes an image and outputs an image embedding vector. Ω is the embedding matrix, and Emb(.) denotes the mapping from a list of word IDs to a list of embedding vectors using Ω. The loss function for our FFNN is given by: L(Γ,Ω,u) = ∑ i,j,k yijk log FNk(DNN(ii; Γ),Emb(cji ; Ω);u)\n(3)\nwhere FNk denotes the k-th output of a feedforward neural network, and ∑ k FNk(.) = 1. Our architecture uses a two hidden-layer fully connected network with Rectified Linear hidden units, and a softmax layer on top.\nThe formula in Eq. 3 is generic with respect to the number of classes. In particular, we consider a 2-class–classifier (k ∈ {0, 1}, 1 for ’yes’, this is a correct answer; 0 for ’no’, this is an incorrect answer), applied independently on all the 〈ii, cji 〉 pairs and apply one FFNN-based binary classifier for each; the final prediction is the caption with the highest ’yes’ probability among all instance pairs belonging to instance i.\nVec2seq + FFNN Model. We describe here a hybrid neural-network model that combines a recurrent neural-network with a feedforward one. We encode the image into a single-cell RNN encoder, and the caption into an RNN decoder. Because the first sequence only contains one cell, we call this model a vector-to-sequence (Vec2seq) model as a special case of Seq2seq model as in (Sutskever et al., 2014; Bahdanau et al., 2015). The output of each unit cell of a Vec2seq model (both on the encoding side and the decoding side) can be fed into an FFNN architecture for binary classification (see the Suppl. Material for an architecture illustration).\nIn addition to the classification loss (Eq. 3), we also include a loss for generating an output sequence cji based on an input ii image. We define a binary variable zijlv ∈ {0, 1} to indicate whether the lth word of cji is equal to word v. O d ijl denotes the l-th output of the decoder of instance pair 〈ii, cji 〉, O e ij denotes the output of the encoder, and Odij: denotes the concatenation of decoder outputs. With these definitions, the loss function for the Vec2seq + FFNN model is:\nL(θ,w,u) = ∑ i,j,k yijk log FNk(Oeij(ii, c j i ;θ),O d ij:(ii, c j i ;θ);u)\n+λgen ∑ i,j,l,v yij1zijlv log softmaxv(Odijl(ii, c j i ;θ);w)\n(4) where ∑\nv softmaxv(.) = 1; θ are the parameters of the Vec2seq model, which include the parameters within each unit cell, as well as the elements in the embedding matrices for images and target sequences; w are the output projection parameters that transform the output space of the decoder to the vocabulary space. u are the parameters of the\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nFFNN model (Eq. 3); λgen is the weight assigned to the sequence-to-sequence generation loss. Only the true target candidates (the ones with yij1 = 1) are included in this loss, as we do not want the decoy target options to affect this computation.\nThe Vec2seq model we use here is an instantiation of the attention-enhanced models proposed in (Bahdanau et al., 2015; Chen et al., 2016). However, our current model does not support location-wise attention, as in the Show-Attendand-Tell (Xu et al., 2015a) model. In this sense, our model is an extension of the Show-and-Tell model with a single attention state representing the entire image, used as image memory representation for all decoder decisions. We apply Gated Recurrent Unit (GRU) as the unit cell (Cho et al., 2014). We also compare the influence on performance of the λgen parameter."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "Baseline models For the baseline models, we use the 2048-dimensional outputs of GoogleInception-v3 (Szegedy et al., 2015) (pre-trained on ImageNet ILSSVR 2012) to represent the images, and 1024-dimensional paragraph-vector embeddings (section 3.2) to represent captions. To reduce computation time, both are reduced to 256- dimensional vectors using random projections. Neural-nets based models The experiments with these models are done using the Tensorflow package (Abadi et al., 2015). The hyper-parameter choices are decided using the hold-out development portion of the MCIC set. For modeling the input tokens, we use a vocabulary size of 8,855 types, selected as the most frequent tokens over the captions from the COCO training set (words occurring at least 5 times). The models are optimized using ADAGRAD with an initial learning rate of 0.01, and clipped gradients (maximum norm 4). We run the training procedures for 3, 000, 000 steps, with a mini-batch size of 20. We use 40 workers for computing the updates, and 10 parameter servers for model storing and (asynchronous and distributed) updating.\nWe use the following notations to refer to the neural network models: FFNNargmax 1..52-class refers to the version of feedforward neural network architecture with a 2-class–classifier (’yes’ or ’no’ for answer correctness), over which an argmax function computes a 5-way decision (i.e., the choice\nwith the highest ’yes’ probability); we henceforth refer to this model simply as FFNN.\nThe Vec2seq+FFNN refers to the hybrid model combining Vec2seq and FFNNargmax 1..52-class . The RNN part of the model uses a two-hidden–layer GRU unit-cell (Cho et al., 2014) configuration, while the FFNN part uses a two-hidden–layer architecture. The λgen hyper-parameter from the loss-function L(θ,w,u) (Eq. 4) is by default set to 1.0 (except for Section 5.3 where we directly measure its effect on performance).\nWe also include the result of a Vec2Seq model that is trained for caption generation only. To use the model for classification, we feed both image and each of its caption candidates to the Vec2Seq model, and then pick the caption candidate which has the lowest perplexity. Evaluation metrics The metrics we use to measure performance come in two flavors. First, the accuracy in detecting (the index of) the true target among the decoys provides a direct way of measuring the performance level on the comprehension task. We use this metric as the main indicator of comprehension performance. Second, because our Vec2seq+FFNN models are multitask models, they can also generate new captions given the input image. The performance level for the generation task is measured using the standard scripts measuring ROUGE-L (Lin and Och, 2004) and CIDEr (Vedantam et al., 2015), using as reference the available captions from the COCO data (around 5 for most of the images). Code for these metrics is available as part of the COCO evaluation toolkit ‡. As usual, both the hypothesis strings and the reference strings are preprocessed: remove all the non-alphabetic characters; transform all letters to lowercase, and tokenize using white space; replace all words occurring less than 5 times with an unknown token 〈UNK〉 (total vocabulary of 8,855 types); truncate to the first 30 tokens."
    }, {
      "heading" : "5.2 Results",
      "text" : "Table 2 summarizes our main results on the comprehension task. We report the accuracies (and their standard deviations) for random choice, baselines, and neural network-based models.\nInterestingly, the Baseline-I2C model performs at the level of random choice, and much worse than the Baseline-C2I model. This discrepancy reflects the inherent difficulty in vision-\n‡https://github.com/tylin/coco-caption\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nModel dim Dev Test Baseline-I2C 256 19.6 ±0.4 19.3±0.4 Baseline-C2I 256 32.8 ±0.5 32.0±0.5 Baseline-LinM 256 44.6 ±0.5 44.5±0.5 FFNN 256 56.3 ±0.5 55.1±0.5\nVec2seq+FFNN 256 60.5 ±0.5 59.0±0.5 Vec2Seq 256 15.6 ± 0.4 16.0 ± 0.4\nTable 2: Performance on the DMC Task, in accuracies (and standard deviations) on MCIC for baselines and NN models.\nLanguage tasks: for each image, there are several possible equally good descriptions, thus a linear mapping from the image embeddings to the captions might not be enough – statistically, the linear model will just predict the mean of those captions. However, for the reverse direction where the captions are the independent variables, the learned model does not have to capture the variability in image embeddings corresponding to the different but equally good captions – there is only one such image embedding.\nNonlinear neural networks overcome these modeling limitations. The results clearly indicate their superiority over the baselines. The Vec2seq+FFNN model obtains the best results, with accuracies of 60.5% (dev) and 59.0% (test); the accuracy numbers indicate that the Vec2seq+FFNN architecture is superior to the non-recursive fully-connected FFNN architecture (at 55.1% accuracy on test). In the Suppl. Material, we show the impact on performance of the mebedding dimension and neural-network sizes.\nLast but not least, the Vec2Seq model performs the worst in the classification task. This result indicates that a caption generation model alone is unable to discriminate among captions that are close in a pretrained embedding space, even when it has a good caption generation performance (CIDEr 0.983 on dev and 0.927 on test)."
    }, {
      "heading" : "5.3 DMC and Image Captioning",
      "text" : "In this section, we compare models with different values of λgen in Eq. 4. This parameter allows for a natural progression from learning for the DMC task only (λgen = 0) to focusing more on the image captioning loss (λgen = 16).\nThe results in Table 3 illustrate one of the main points of this paper. That is, the ability to perform the comprehension task (as measured by the accuracy metric) positively correlates with the ability to perform other tasks that require machine comprehension, such as caption generation. At\nλgen = 4, the Vec2seq+FFNN model not only has a high accuracy of detecting the ground-truth option, but it also generates its own captions given the input image, with an accuracy measured on MCIC at 0.9890 (dev) and 0.9380 (test) CIDEr scores. On the other hand, at an accuracy level of about 59% (on test, at λgen = 0.1), the generation performance is at only 0.9010 (dev) and 0.8650 (test) CIDEr scores.\nWe note that there is an inherent trade-off between prediction accuracy and generation performance, as seen for λgen values above 4.0. This agrees with the intuition that training a Vec2seq+FFNN model using a loss L(θ,w,u) with a larger λgen means that the ground-truth detection loss (the first term of the loss in Eq.4) may get overwhelmed by the word-generation loss (the second term). However, our empirical results suggest that there is value in training models with a multi-task setup, in which both the comprehension side as well as the generation side are carefully tuned to maximize performance."
    }, {
      "heading" : "6 Discussion",
      "text" : "We have proposed and described in detail a new multi-modal machine comprehension task (DMC). The underlying hypothesis is that computer systems that can be shown to perform increasingly well on this task will do so by constructing a visually-grounded understanding of various linguistic elements and their dependencies.\nThe Vec2seq+FFNN architecture can be trained end-to-end to display both the ability to choose the most likely text associated with an image, as well as the ability to generate a complex description of that image. The empirical results validate that improvements in comprehension and generation happen in tandem.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Viégas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng." ],
      "venue" : "Software available",
      "citeRegEx" : "Schuster et al\\.,? 2015",
      "shortCiteRegEx" : "Schuster et al\\.",
      "year" : 2015
    }, {
      "title" : "SPICE: semantic propositional image caption evaluation",
      "author" : [ "Peter Anderson", "Basura Fernando", "Mark Johnson", "Stephen Gould." ],
      "venue" : "CoRR abs/1607.08822. http://arxiv.org/abs/1607.08822.",
      "citeRegEx" : "Anderson et al\\.,? 2016",
      "shortCiteRegEx" : "Anderson et al\\.",
      "year" : 2016
    }, {
      "title" : "VQA: Visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "International Conference on Computer Vision (ICCV).",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
      "author" : [ "Satanjeev Banerjee", "Alon Lavie." ],
      "venue" : "Proceedings of the ACL Workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization.",
      "citeRegEx" : "Banerjee and Lavie.,? 2005",
      "shortCiteRegEx" : "Banerjee and Lavie.",
      "year" : 2005
    }, {
      "title" : "Learning deep architectures for ai",
      "author" : [ "Yoshua Bengio." ],
      "venue" : "Found. Trends Mach. Learn. 2(1):1–127. https://doi.org/10.1561/2200000006.",
      "citeRegEx" : "Bengio.,? 2009",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2009
    }, {
      "title" : "Automatic description generation from images: A survey of models, datasets, and evaluation measures",
      "author" : [ "Raffaella Bernardi", "Ruket Cakici", "Desmond Elliott", "Aykut Erdem", "Erkut Erdem", "Nazli Ikizler-Cinbis", "Frank Keller", "Adrian Muscat", "Barbara Plank." ],
      "venue" : "JAIR 55.",
      "citeRegEx" : "Bernardi et al\\.,? 2016",
      "shortCiteRegEx" : "Bernardi et al\\.",
      "year" : 2016
    }, {
      "title" : "A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task",
      "author" : [ "Danqi Chen", "Jason Bolton", "Christopher D. Manning." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Çaglar Gülçehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of EMNLP. pages 1724–1734.",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Long-term recurrent convolutional networks for visual recognition and description",
      "author" : [ "Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell." ],
      "venue" : "Proc. of IEEE Conference on Computer Vision and Pattern",
      "citeRegEx" : "Donahue et al\\.,? 2014",
      "shortCiteRegEx" : "Donahue et al\\.",
      "year" : 2014
    }, {
      "title" : "From captions to visual concepts and back",
      "author" : [ "Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh Srivastava", "Li Deng", "Piotr Dollár", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John Platt" ],
      "venue" : "In Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "Fang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Fang et al\\.",
      "year" : 2015
    }, {
      "title" : "Devise: A deep visual-semantic embedding model",
      "author" : [ "Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Frome et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Frome et al\\.",
      "year" : 2013
    }, {
      "title" : "Are you talking to a machine? dataset and methods for multilingual image question answering",
      "author" : [ "H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Gao et al\\.,? 2015",
      "shortCiteRegEx" : "Gao et al\\.",
      "year" : 2015
    }, {
      "title" : "Focused evaluation for image description with binary forced-choice tasks",
      "author" : [ "M. Hodosh", "J. Hockenmaier." ],
      "venue" : "Proc. 5th Vision and Language Workshop.",
      "citeRegEx" : "Hodosh and Hockenmaier.,? 2016",
      "shortCiteRegEx" : "Hodosh and Hockenmaier.",
      "year" : 2016
    }, {
      "title" : "Framing image description as a ranking task: Data, models and evaluation metrics",
      "author" : [ "Micah Hodosh", "Peter Young", "Julia Hockenmaier." ],
      "venue" : "JAIR .",
      "citeRegEx" : "Hodosh et al\\.,? 2013",
      "shortCiteRegEx" : "Hodosh et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep visual-semantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Li Fei-Fei." ],
      "venue" : "Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Karpathy and Fei.Fei.,? 2015",
      "shortCiteRegEx" : "Karpathy and Fei.Fei.",
      "year" : 2015
    }, {
      "title" : "Unifying visual-semantic embeddings with multimodal neural language models",
      "author" : [ "Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel." ],
      "venue" : "Transactions of the Association for Computational Linguistics .",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Visual Genome: Connecting language and vision using crowdsourced dense image",
      "author" : [ "Ranjay Krishna", "Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma", "Michael Bernstein", "Li Fei-Fei" ],
      "venue" : null,
      "citeRegEx" : "Krishna et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Krishna et al\\.",
      "year" : 2016
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc Le", "Tomas Mikolov." ],
      "venue" : "Proceedings of the 31st ICML. Beijing, China.",
      "citeRegEx" : "Le and Mikolov.,? 2014",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    }, {
      "title" : "Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics",
      "author" : [ "Chin-Yew Lin", "Franz Josef Och." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Lin and Och.,? 2004",
      "shortCiteRegEx" : "Lin and Och.",
      "year" : 2004
    }, {
      "title" : "Microsoft COCO: common objects in context",
      "author" : [ "Tsung-Yi Lin", "Michael Maire", "Serge J. Belongie", "Lubomir D. Bourdev", "Ross B. Girshick", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Dollár", "C. Lawrence Zitnick." ],
      "venue" : "CoRR abs/1405.0312. http://arxiv.org/abs/1405.0312.",
      "citeRegEx" : "Lin et al\\.,? 2014",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Leveraging visual question answering for image-caption ranking",
      "author" : [ "Xiao Lin", "Devi Parikh." ],
      "venue" : "CoRR abs/1605.01379.",
      "citeRegEx" : "Lin and Parikh.,? 2016",
      "shortCiteRegEx" : "Lin and Parikh.",
      "year" : 2016
    }, {
      "title" : "A multi-world approach to question answering about real-world scenes based on uncertain input",
      "author" : [ "M. Malinowski", "M. Fritz." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Malinowski and Fritz.,? 2014",
      "shortCiteRegEx" : "Malinowski and Fritz.",
      "year" : 2014
    }, {
      "title" : "Ask your neurons: A neural-based approach to answering questions about images",
      "author" : [ "M. Malinowski", "M. Rohrbach", "M. Fritz." ],
      "venue" : "ICCV .",
      "citeRegEx" : "Malinowski et al\\.,? 2015",
      "shortCiteRegEx" : "Malinowski et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep captioning with multimodal recurrent neural networks (mRNN)",
      "author" : [ "J. Mao", "W. Xu", "Y. Yang", "J. Wang", "A. Yuille." ],
      "venue" : "Proc. Int. Conf. Learn. Representations.",
      "citeRegEx" : "Mao et al\\.,? 2015",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2015
    }, {
      "title" : "Bleu: A method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu." ],
      "venue" : "Proceedings of ACL. pages 311– 318. http://dx.doi.org/10.3115/1073083.1073135.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Sequence level training with recurrent neural networks",
      "author" : [ "Marc’Aurelio Ranzato", "Sumit Chopra", "Michael Auli", "Wojciech Zaremba" ],
      "venue" : "CoRR abs/1511.06732",
      "citeRegEx" : "Ranzato et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2015
    }, {
      "title" : "Image question answering: A visual semantic embedding model and a new dataset",
      "author" : [ "M. Ren", "R. Kiros", "R. Zemel." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. V Le." ],
      "venue" : "Advances in Neural Information Processing Systems 27.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna." ],
      "venue" : "volume abs/1512.00567. http://arxiv.org/abs/1512.00567.",
      "citeRegEx" : "Szegedy et al\\.,? 2015",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "Joint video and text parsing for understanding events and answering queries",
      "author" : [ "K. Tu", "M. Meng", "M.W. Lee", "T.E. Choe", "S.C. Zhu." ],
      "venue" : "IEEE MultiMedia .",
      "citeRegEx" : "Tu et al\\.,? 2014",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2014
    }, {
      "title" : "Cider: Consensus-based image description evaluation",
      "author" : [ "Ramakrishna Vedantam", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Vedantam et al\\.,? 2015",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2015
    }, {
      "title" : "Show and tell: A neural image caption generator",
      "author" : [ "Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan." ],
      "venue" : "Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR).",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Visual question answering: A survey of methods and datasets",
      "author" : [ "Qi Wu", "Damien Teney", "Peng Wang", "Chunhua Shen", "Anthony R. Dick", "Anton van den Hengel." ],
      "venue" : "CoRR abs/1607.05910.",
      "citeRegEx" : "Wu et al\\.,? 2016a",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "Ask me anything: Free-form visual question answering based on knowledge from external sources",
      "author" : [ "Qi Wu", "Peng Wang", "Chunhua Shen", "Anthony Dick", "Anton van den Hengel." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Wu et al\\.,? 2016b",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C. Courville", "Ruslan Salakhutdinov", "Richard S. Zemel", "Yoshua Bengio." ],
      "venue" : "CoRR abs/1502.03044. http://arxiv.org/abs/1502.03044.",
      "citeRegEx" : "Xu et al\\.,? 2015a",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio." ],
      "venue" : "Proc. of the 32nd International Conference on Machine Learning (ICML).",
      "citeRegEx" : "Xu et al\\.,? 2015b",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Stacked attention networks for image question answering",
      "author" : [ "Zichao Yang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Smola." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Visual madlibs: Fill-in-theblank description generation and question answering",
      "author" : [ "L. Yu", "E. Park", "A.C. Berg", "T.L. Berg." ],
      "venue" : "ICCV .",
      "citeRegEx" : "Yu et al\\.,? 2015",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2015
    }, {
      "title" : "Visual7w: Grounded question answering in images",
      "author" : [ "Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li Fei-Fei." ],
      "venue" : "CVPR.",
      "citeRegEx" : "Zhu et al\\.,? 2016",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Several vision+language tasks have been proposed around these datasets (Hodosh et al., 2013; Karpathy and Fei-Fei, 2015; Lin et al., 2014; Antol et al., 2015).",
      "startOffset" : 71,
      "endOffset" : 158
    }, {
      "referenceID" : 15,
      "context" : "Several vision+language tasks have been proposed around these datasets (Hodosh et al., 2013; Karpathy and Fei-Fei, 2015; Lin et al., 2014; Antol et al., 2015).",
      "startOffset" : 71,
      "endOffset" : 158
    }, {
      "referenceID" : 20,
      "context" : "Several vision+language tasks have been proposed around these datasets (Hodosh et al., 2013; Karpathy and Fei-Fei, 2015; Lin et al., 2014; Antol et al., 2015).",
      "startOffset" : 71,
      "endOffset" : 158
    }, {
      "referenceID" : 2,
      "context" : "Several vision+language tasks have been proposed around these datasets (Hodosh et al., 2013; Karpathy and Fei-Fei, 2015; Lin et al., 2014; Antol et al., 2015).",
      "startOffset" : 71,
      "endOffset" : 158
    }, {
      "referenceID" : 14,
      "context" : "Image Captioning (Hodosh et al., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Fang et al., 2015; Kiros et al., 2015; Vinyals et al., 2015; Mao et al., 2015; Xu et al., 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al.",
      "startOffset" : 17,
      "endOffset" : 185
    }, {
      "referenceID" : 9,
      "context" : "Image Captioning (Hodosh et al., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Fang et al., 2015; Kiros et al., 2015; Vinyals et al., 2015; Mao et al., 2015; Xu et al., 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al.",
      "startOffset" : 17,
      "endOffset" : 185
    }, {
      "referenceID" : 15,
      "context" : "Image Captioning (Hodosh et al., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Fang et al., 2015; Kiros et al., 2015; Vinyals et al., 2015; Mao et al., 2015; Xu et al., 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al.",
      "startOffset" : 17,
      "endOffset" : 185
    }, {
      "referenceID" : 10,
      "context" : "Image Captioning (Hodosh et al., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Fang et al., 2015; Kiros et al., 2015; Vinyals et al., 2015; Mao et al., 2015; Xu et al., 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al.",
      "startOffset" : 17,
      "endOffset" : 185
    }, {
      "referenceID" : 16,
      "context" : "Image Captioning (Hodosh et al., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Fang et al., 2015; Kiros et al., 2015; Vinyals et al., 2015; Mao et al., 2015; Xu et al., 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al.",
      "startOffset" : 17,
      "endOffset" : 185
    }, {
      "referenceID" : 32,
      "context" : "Image Captioning (Hodosh et al., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Fang et al., 2015; Kiros et al., 2015; Vinyals et al., 2015; Mao et al., 2015; Xu et al., 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al.",
      "startOffset" : 17,
      "endOffset" : 185
    }, {
      "referenceID" : 24,
      "context" : "Image Captioning (Hodosh et al., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Fang et al., 2015; Kiros et al., 2015; Vinyals et al., 2015; Mao et al., 2015; Xu et al., 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al.",
      "startOffset" : 17,
      "endOffset" : 185
    }, {
      "referenceID" : 36,
      "context" : "Image Captioning (Hodosh et al., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Fang et al., 2015; Kiros et al., 2015; Vinyals et al., 2015; Mao et al., 2015; Xu et al., 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al.",
      "startOffset" : 17,
      "endOffset" : 185
    }, {
      "referenceID" : 22,
      "context" : ", 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al., 2015; Tu et al., 2014; Antol et al., 2015; Yu et al., 2015; Wu et al., 2016b; Ren et al., 2015; Gao et al., 2015; Yang et al., 2016; Zhu et al., 2016; Lin and Parikh, 2016) have in particular attracted a lot of attention.",
      "startOffset" : 39,
      "endOffset" : 259
    }, {
      "referenceID" : 23,
      "context" : ", 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al., 2015; Tu et al., 2014; Antol et al., 2015; Yu et al., 2015; Wu et al., 2016b; Ren et al., 2015; Gao et al., 2015; Yang et al., 2016; Zhu et al., 2016; Lin and Parikh, 2016) have in particular attracted a lot of attention.",
      "startOffset" : 39,
      "endOffset" : 259
    }, {
      "referenceID" : 30,
      "context" : ", 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al., 2015; Tu et al., 2014; Antol et al., 2015; Yu et al., 2015; Wu et al., 2016b; Ren et al., 2015; Gao et al., 2015; Yang et al., 2016; Zhu et al., 2016; Lin and Parikh, 2016) have in particular attracted a lot of attention.",
      "startOffset" : 39,
      "endOffset" : 259
    }, {
      "referenceID" : 2,
      "context" : ", 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al., 2015; Tu et al., 2014; Antol et al., 2015; Yu et al., 2015; Wu et al., 2016b; Ren et al., 2015; Gao et al., 2015; Yang et al., 2016; Zhu et al., 2016; Lin and Parikh, 2016) have in particular attracted a lot of attention.",
      "startOffset" : 39,
      "endOffset" : 259
    }, {
      "referenceID" : 38,
      "context" : ", 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al., 2015; Tu et al., 2014; Antol et al., 2015; Yu et al., 2015; Wu et al., 2016b; Ren et al., 2015; Gao et al., 2015; Yang et al., 2016; Zhu et al., 2016; Lin and Parikh, 2016) have in particular attracted a lot of attention.",
      "startOffset" : 39,
      "endOffset" : 259
    }, {
      "referenceID" : 34,
      "context" : ", 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al., 2015; Tu et al., 2014; Antol et al., 2015; Yu et al., 2015; Wu et al., 2016b; Ren et al., 2015; Gao et al., 2015; Yang et al., 2016; Zhu et al., 2016; Lin and Parikh, 2016) have in particular attracted a lot of attention.",
      "startOffset" : 39,
      "endOffset" : 259
    }, {
      "referenceID" : 27,
      "context" : ", 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al., 2015; Tu et al., 2014; Antol et al., 2015; Yu et al., 2015; Wu et al., 2016b; Ren et al., 2015; Gao et al., 2015; Yang et al., 2016; Zhu et al., 2016; Lin and Parikh, 2016) have in particular attracted a lot of attention.",
      "startOffset" : 39,
      "endOffset" : 259
    }, {
      "referenceID" : 12,
      "context" : ", 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al., 2015; Tu et al., 2014; Antol et al., 2015; Yu et al., 2015; Wu et al., 2016b; Ren et al., 2015; Gao et al., 2015; Yang et al., 2016; Zhu et al., 2016; Lin and Parikh, 2016) have in particular attracted a lot of attention.",
      "startOffset" : 39,
      "endOffset" : 259
    }, {
      "referenceID" : 37,
      "context" : ", 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al., 2015; Tu et al., 2014; Antol et al., 2015; Yu et al., 2015; Wu et al., 2016b; Ren et al., 2015; Gao et al., 2015; Yang et al., 2016; Zhu et al., 2016; Lin and Parikh, 2016) have in particular attracted a lot of attention.",
      "startOffset" : 39,
      "endOffset" : 259
    }, {
      "referenceID" : 39,
      "context" : ", 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al., 2015; Tu et al., 2014; Antol et al., 2015; Yu et al., 2015; Wu et al., 2016b; Ren et al., 2015; Gao et al., 2015; Yang et al., 2016; Zhu et al., 2016; Lin and Parikh, 2016) have in particular attracted a lot of attention.",
      "startOffset" : 39,
      "endOffset" : 259
    }, {
      "referenceID" : 21,
      "context" : ", 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al., 2015; Tu et al., 2014; Antol et al., 2015; Yu et al., 2015; Wu et al., 2016b; Ren et al., 2015; Gao et al., 2015; Yang et al., 2016; Zhu et al., 2016; Lin and Parikh, 2016) have in particular attracted a lot of attention.",
      "startOffset" : 39,
      "endOffset" : 259
    }, {
      "referenceID" : 5,
      "context" : "The performances on these tasks have been steadily improving, owing much to the wide use of deep learning architectures (Bengio, 2009).",
      "startOffset" : 120,
      "endOffset" : 134
    }, {
      "referenceID" : 20,
      "context" : "2); (2) an instantiation of applying this algorithm to the COCO dataset (Lin et al., 2014), resulting in a large-scale dual machinecomprehension dataset that we make publicly available (Section 3.",
      "startOffset" : 72,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "Interested readers should refer to two recent surveys (Bernardi et al., 2016; Wu et al., 2016a).",
      "startOffset" : 54,
      "endOffset" : 95
    }, {
      "referenceID" : 33,
      "context" : "Interested readers should refer to two recent surveys (Bernardi et al., 2016; Wu et al., 2016a).",
      "startOffset" : 54,
      "endOffset" : 95
    }, {
      "referenceID" : 26,
      "context" : "The language generation model of an IC system plays a crucial role; it is often trained such that the probabilities of the ground-truth captions are maximized (MLE training), though more advanced methods based on techniques borrowed from Reinforcement Learning have been proposed (Ranzato et al., 2015).",
      "startOffset" : 280,
      "endOffset" : 302
    }, {
      "referenceID" : 2,
      "context" : "In many variations of this task, answers are limited to single words or a binary response (“yes” or “no”) (Antol et al., 2015).",
      "startOffset" : 106,
      "endOffset" : 126
    }, {
      "referenceID" : 39,
      "context" : "The Visual7W dataset (Zhu et al., 2016) contains anaswers in a richer format such as phrases, but limits questions to “wh-”style (what, where, who, etc).",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 17,
      "context" : "The Visual Genome dataset (Krishna et al., 2016), on the other hand, can potentially define more complex questions and answers due to its extensive textual annotations.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 25,
      "context" : "This is a clear advantage over the IC tasks, which often rely on imperfect metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al.",
      "startOffset" : 96,
      "endOffset" : 119
    }, {
      "referenceID" : 19,
      "context" : ", 2002), ROUGE (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : ", 2002), ROUGE (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al.",
      "startOffset" : 43,
      "endOffset" : 69
    }, {
      "referenceID" : 31,
      "context" : ", 2002), ROUGE (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), or SPICE (Anderson et al.",
      "startOffset" : 77,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : ", 2015), or SPICE (Anderson et al., 2016).",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : "Related to our proposal is the work in (Hodosh et al., 2013), which frames image captioning as a ranking problem.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "Our work is also substantially different from their more recent work (Hodosh and Hockenmaier, 2016), where only one decoy is considered and its generation is either random, or focusing on visual concept similarity (“switching people or scenes”) instead of our focus on both linguistic surface and paragraph vector embedding similarity.",
      "startOffset" : 69,
      "endOffset" : 99
    }, {
      "referenceID" : 20,
      "context" : "It seems straightforward to construct a dataset for this task, as there are several existing datasets which are composed of images and their (multiple) ground-truth captions, including the popular COCO dataset (Lin et al., 2014).",
      "startOffset" : 210,
      "endOffset" : 228
    }, {
      "referenceID" : 18,
      "context" : "The algorithm exploits recent advances in paragraph vector (PV) models (Le and Mikolov, 2014), while also using linguistic surface analysis to define similarity between two sentences.",
      "startOffset" : 71,
      "endOffset" : 93
    }, {
      "referenceID" : 20,
      "context" : "〈image, {caption(s)} 〉 pairs∗, as those extracted from a variety of publicly-available corpora, including the COCO dataset (Lin et al., 2014).",
      "startOffset" : 123,
      "endOffset" : 141
    }, {
      "referenceID" : 25,
      "context" : "In this work, simSURF is computed as the BLEU score between the inputs (Papineni et al., 2002) (with the brevity penalty set to 1).",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 20,
      "context" : "3 The MCIC dataset We applied the MC-IC Algorithm to the COCO dataset (Lin et al., 2014) to generate a dataset for the visual-language dual machine comprehension task.",
      "startOffset" : 70,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "(Frome et al., 2013).",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 28,
      "context" : "Because the first sequence only contains one cell, we call this model a vector-to-sequence (Vec2seq) model as a special case of Seq2seq model as in (Sutskever et al., 2014; Bahdanau et al., 2015).",
      "startOffset" : 148,
      "endOffset" : 195
    }, {
      "referenceID" : 3,
      "context" : "Because the first sequence only contains one cell, we call this model a vector-to-sequence (Vec2seq) model as a special case of Seq2seq model as in (Sutskever et al., 2014; Bahdanau et al., 2015).",
      "startOffset" : 148,
      "endOffset" : 195
    }, {
      "referenceID" : 3,
      "context" : "The Vec2seq model we use here is an instantiation of the attention-enhanced models proposed in (Bahdanau et al., 2015; Chen et al., 2016).",
      "startOffset" : 95,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "The Vec2seq model we use here is an instantiation of the attention-enhanced models proposed in (Bahdanau et al., 2015; Chen et al., 2016).",
      "startOffset" : 95,
      "endOffset" : 137
    }, {
      "referenceID" : 35,
      "context" : "However, our current model does not support location-wise attention, as in the Show-Attendand-Tell (Xu et al., 2015a) model.",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 8,
      "context" : "We apply Gated Recurrent Unit (GRU) as the unit cell (Cho et al., 2014).",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 29,
      "context" : "1 Experimental Setup Baseline models For the baseline models, we use the 2048-dimensional outputs of GoogleInception-v3 (Szegedy et al., 2015) (pre-trained on ImageNet ILSSVR 2012) to represent the images, and 1024-dimensional paragraph-vector embeddings (section 3.",
      "startOffset" : 120,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : "The RNN part of the model uses a two-hidden–layer GRU unit-cell (Cho et al., 2014) configuration, while the FFNN part uses a two-hidden–layer architecture.",
      "startOffset" : 64,
      "endOffset" : 82
    }, {
      "referenceID" : 19,
      "context" : "The performance level for the generation task is measured using the standard scripts measuring ROUGE-L (Lin and Och, 2004) and CIDEr (Vedantam et al.",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 31,
      "context" : "The performance level for the generation task is measured using the standard scripts measuring ROUGE-L (Lin and Och, 2004) and CIDEr (Vedantam et al., 2015), using as reference the available captions from the COCO data (around 5 for most of the images).",
      "startOffset" : 133,
      "endOffset" : 156
    } ],
    "year" : 2017,
    "abstractText" : "We introduce a new multi-modal task for computer systems, posed as a combined vision-language comprehension challenge: identifying the most suitable text describing a scene, given several similar options. Accomplishing the task entails demonstrating comprehension beyond just recognizing “keywords” (or key-phrases) and their corresponding visual concepts. Instead, it requires an alignment between the representations of the two modalities that achieves a visually-grounded “understanding” of various linguistic elements and their dependencies. This new task also admits an easy-to-compute and wellstudied metric: the accuracy in detecting the true target among the decoys. The paper makes several contributions: an effective and extensible mechanism for generating decoys from (human-created) image captions; an instance of applying this mechanism, yielding a large-scale machine comprehension dataset (based on the COCO images and captions) that we make publicly available; human evaluation results on this dataset, informing a performance upper-bound; and several baseline and competitive learning approaches that illustrate the utility of the proposed task and dataset in advancing both image and language comprehension. We also show that, in a multi-task learning setting, the performance on the proposed task is positively correlated with the end-to-end task of image captioning.",
    "creator" : "LaTeX with hyperref package"
  }
}