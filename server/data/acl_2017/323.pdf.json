{
  "name" : "323.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Neural Local Coherence Model",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction and Motivation",
      "text" : "What distinguishes a coherent text from a random sequence of sentences is that it binds the sentences together to express a meaning as a whole — the interpretation of a sentence usually depends on the meaning of its neighbors. Coherence models that can distinguish a coherent from incoherent texts have a wide range of applications in text generation, summarization, and coherence scoring.\nSeveral formal theories of coherence have been proposed (Mann and Thompson, 1988a; Grosz et al., 1995; Asher and Lascarides, 2003), and their principles have inspired development of existing coherence models (Barzilay and Lapata, 2008; Lin et al., 2011; Li and Hovy, 2014). Among these models, the entity grid (Barzilay and Lapata, 2008), which is based on Centering Theory (Grosz et al., 1995), is arguably the most popular, and has seen a number of improvements over the years. As shown in Figure 2, the entity grid model represents a text by a grid that captures how grammatical roles of different entities change from\nsentence to sentence. The grid is then converted into a feature vector containing probabilities of local entity transitions, which enables machine learning models to learn the degree of text coherence. Extensions of this basic grid model incorporate entity-specific features (Elsner and Charniak, 2011), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014).\nWhile the entity grid and its extensions have been successful in many applications, they are limited in several ways. Firstly, they use discrete representation for grammatical roles and features, which limits the model to consider sufficiently long transitions (Bengio et al., 2003). Secondly, feature vector computation in existing models is decoupled from the target task, which limits the models to learn task-specific features.\nIn this paper, we propose a neural architecture for coherence assessment that can capture long range entity transitions along with arbitrary entityspecific features. Our model obtains generalization through distributed representations of entity transitions and entity features. We also present an end-to-end training method to learn task-specific high level features automatically in our model.\nWe evaluate our approach on three different evaluation tasks: discrimination, insertion, and summary coherence rating, proposed previously for evaluating coherence models (Barzilay and Lapata, 2008; Elsner and Charniak, 2011). Discrimination and insertion involve identifying the right order of the sentences in a text with different levels of difficulty. In summary coherence rating task, we compare the rankings, given by the model, against human pairwise judgments of coherence.\nThe experimental results show that our neural models consistently improve over the nonneural counterparts (i.e., existing entity grid models) yielding absolute gains of about 4% on discrimination, up to 2.5% on insertion, and more\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nthan 4% on summary coherence rating. Our model achieves state of the art results in all these tasks. We have released our code with the submission.\nThe remainder of this paper is organized as follows. We describe entity grid, its extensions, and its limitations in Section 2. In Section 3, we present our neural model. We describe evaluation tasks and results in Sections 4 and 5. We give a brief account of related work in Section 6. Finally, we conclude with future directions in Section 7."
    }, {
      "heading" : "2 Entity Grid and Its Extensions",
      "text" : "Motivated by Centering Theory (Grosz et al., 1995), Barzilay and Lapata (2008) proposed an entity-based model for representing and assessing text coherence. Their model represents a text by a two-dimensional array called entity grid that captures transitions of discourse entities across sentences. As shown in Figure 2, the rows of the grid correspond to sentences, and the columns correspond to discourse entities appearing in the text. They consider noun phrases (NP) as entities, and employ a coreference resolver to detect mentions of the same entity (e.g., Obama, the president). Each entry Gi,j in the entity grid represents the syntactic role that entity ej plays in sentence si, which can be one of: subject (S), object (O), or other (X). In addition, entities not appearing in a sentence are marked by a special symbol (-). If an entity appears more than once with different grammatical roles in the same sentence, the role with the highest rank (S O X) is considered.\nTo represent an entity grid with a feature vector, Barzilay and Lapata (2008) compute probability for each local entity transition of length k (i.e., {S,O,X,−}k), and represent each grid by a vector of 4k transitions probabilities. To distinguish between transitions of important entities from unimportant ones, they consider the salience of the entities, which they quantify by their occurrence frequency in the document. Assessment of text coherence is then formulated as a ranking problem in an SVM preference ranking framework (Joachims, 2002).\nSubsequent studies proposed to extend the basic entity grid model. Filippova and Strube (2007) attempted to improve the model by grouping entities based on semantic relatedness, but did not get significant improvement. Elsner and Charniak (2011) proposed a number of improvements. They initially show significant improvement by includ-\ning non-head nouns (i.e., nouns that do not head NPs) as entities in the grid.1 Then, they extend the grid to distinguish between entities of different types by incorporating entity-specific features like named entity, noun class, modifiers, etc. These extensions led to the best results reported so far.\nEntity grid and its extensions have been successfully applied to many downstream tasks including coherence rating (Barzilay and Lapata, 2008), essay scoring (Burstein et al., 2010), story generation (McIntyre and Lapata, 2010), and readability assessment (Pitler et al., 2010; Barzilay and Lapata, 2008). They have also been critical components in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2011; Lin et al., 2011)."
    }, {
      "heading" : "2.1 Limitations of Entity Grid Models",
      "text" : "Despite its success, existing entity grid models are limited in several ways.\n• Existing models use discrete representation for grammatical roles and features, which leads to the so-called curse of dimensionality problem (Bengio et al., 2003). In particular, to model transitions of length k withR different grammatical roles, the basic entity grid model needs to computeRk transition probabilities from a grid. One can imagine that the estimated distribution becomes sparse as k increases. This limits the model to consider longer transitions – existing models use k ≤ 3.\n1They match the nouns to detect coreferent entities.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nThis problem is exacerbated when we want to include entity-specific features, as the number of parameters grows exponentially with the number of features (Elsner and Charniak, 2011).\n• Existing models compute feature representations from entity grids in a task-agnostic way. In other words, feature extraction is decoupled from the target downstream tasks. This can limit the models to learn task-specific features. Therefore, models that can be trained in an end-to-end fashion on different target tasks are desirable.\nIn the following section, we present a neural architecture that allows us to capture long range entity transitions along with arbitrary entity-specific features without loosing generalization. We also present an end-to-end training method to learn task-specific features automatically."
    }, {
      "heading" : "3 The Neural Coherence Model",
      "text" : "Figure 2 summarizes our neural architecture for modeling local coherence, and how it can be trained in a pairwise fashion. The architecture takes a document as input, and first extracts its entity grid.2 The first layer of the neural network transforms each grammatical role in the grid into a distributed representation, a real-valued vector. The second layer computes high-level features by going over each column (transitions) of the grid. The following layer selects the most important high-level features, which are in turn used for coherence scoring. The features computed at different layers of the network are automatically trained by backpropagation to be relevant to the task. In the following, we elaborate on the layers of the neural network model.\n(I) Transforming grammatical roles into feature vectors: Grammatical roles are fed to our model as indices taken from a finite vocabulary V . In the simplest scenario, V contains {S,O,X,−}. However, we will see in Section 3.1 that as we include more entity-specific features, V can contain more symbols. The first layer of our network maps each of these indices into a distributed representation Rd by looking up a shared embedding matrix E ∈ R|V |×d. We consider E a model parameter to be learned by backpropagation on a given task. We can initialize E randomly or using pretrained vectors trained on a general coherence task.\n2For clarification, pairwise input as shown in the figure is required only to train the model.\nGiven an entity grid G with columns representing entity transitions over sentences in a document, the lookup layer extracts a d-dimensional vector for each entry Gi,j from E. More formally,\nL(G) = 〈 E(G1,1) · · · E(Gi,j) · · · E(Gm,n) 〉 (1) where E(Gi,j) refers to the row in E that corresponds to the grammatical role Gi,j ∈ V ; m is the total number of sentences and n is the total number of entities in the document. The output L(G) is a tensor in Rm×n×d, which is fed to the next layer of the network as we describe below.\n(II) Modeling entity transitions: The vectors produced by the lookup layer are combined by subsequent layers of the network to generate a coherence score for the document. To compose higher-level features from the embedding vectors, we make the following modeling assumptions:\n• Similar to existing entity grid models, we assume there is no spatio-temporal relation between the entities in a document. In other words, columns in a grid are treated independently.\n• We are interested in modeling entity transitions of arbitrary lengths in a location-invariant way. This means, we aim to compose local patches of entity transitions into higher-level representations, while treating the patches independently of their position in the entity grid.\nUnder these assumptions, the natural choice to tackle this problem is to use a convolutional approach, used previously to solve other NLP tasks (Collobert et al., 2011; Kim, 2014).\nConvolution layer: A convolution operation involves applying a filter w ∈ Rk.d (i.e., a vector of weight parameters) to each entity transition of length k to produce a new abstract feature\nht = f(w TLt:t+k−1,j + bt) (2)\nwhere Lt:t+k−1,j denotes the concatenation of k vectors in the lookup layer representing a transition of length k for entity ej in the grid, bt is a bias term, and f is a nonlinear activation function, e.g., ReLU (Nair and Hinton, 2010) in our model.\nWe apply this filter to each possible k-length transitions of different entities in the grid to generate a feature map, hi = [h1, · · · , hm.n+k−1]. We\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nFigure 2: Neural architecture for modeling local coherence and the pairwise training method.\nrepeat this processN times withN different filters to get N different feature maps (Figure 2). Notice that we use a wide convolution (Kalchbrenner et al., 2014), as opposed to narrow, to ensure that the filters reach entire columns of a grid, including the boundary entities. This is done by performing zero-padding, where out-of-range (i.e., for t < 0 or t > {m,n}) vectors are assumed to be zero.\nConvolutional filters learn to compose local transition features of a grid into higher-level representations automatically. Since it operates over the distributed representation of grid entries, compared to traditional grid models, the transition length k can be sufficiently large (e.g., 5 − 8 in our experiments) to capture long-range transitional dependencies without overfitting on the training data. Moreover, unlike existing grid models that compute transition probabilities from a single document, embedding vectors and convolutional filters are learned from all training documents, which helps the neural framework to obtain better generalization and robustness.\nPooling layer: After the convolution, we apply a max-pooling operation to each feature map.\nm = [µp(h 1), · · · , µp(hN )] (3)\nwhere µp(hi) refers to the max operation applied to each non-overlapping3 window of p features in the feature map hi. Max-pooling reduces the output dimensionality by a factor of p, and it drives the model to capture the most salient local features\n3We set the stride size to be the same as the pooling length p to get non-overlapping regions.\nfrom each feature map in the convolutional layer.\nCoherence scoring: Finally, the max-pooled features are used in the output layer of the network to produce a coherence score y ∈ R.\ny = vTm+ b (4)\nwhere v is the weight vector and b is a bias term.\nWhy it works: Intuitively, each filter detects a specific transition pattern (e.g., ‘SS-O-X’ for a coherent text), and if this pattern occurs somewhere in the grid, the resulting feature map will have a large value for that particular region and small values for other regions. By applying max pooling on this feature map, the network then discovers that the transition appeared in the grid."
    }, {
      "heading" : "3.1 Incorporating Entity-Specific Features",
      "text" : "Our model as described above neuralizes the basic entity grid model that considers only entity transitions without distinguishing between types of the entities. However, as Elsner and Charniak (2011) pointed out entity-specific features could be crucial for modeling local coherence. One simple way to incorporate entity-specific features into our model is to attach the feature value (e.g., named entity type) with the grammatical role in the grid. For example, if an entity ej of type PERSON appears as a subject (S) in sentence si, the grid entry Gi,j can be encoded as PERSON-S."
    }, {
      "heading" : "3.2 Training",
      "text" : "Our neural model assigns a coherence score to an input document d based on the degree of lo-\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\ncal coherence observed in its entity grid G. Let y = φ(G|θ) define our model that transforms an input grid G to a coherence score y through a sequence of lookup, convolutional, pooling, and linear projection layers with parameter set θ. The parameter set θ includes the embedding matrix E, the filter matrix W , the weight vector v, and the biases. We use a pairwise ranking approach (Collobert et al., 2011) to learn θ.\nThe training set comprises ordered pairs (di, dj), where document di exhibits a higher degree of coherence than document dj . As we will see in Section 4 such orderings can be obtained automatically or through manual annotation. In training, we seek to find θ that assigns a higher coherence score to di than to dj . We minimize the following ranking objective with respect to θ:\nJ (θ) = max{0, 1− φ(Gi|θ) + φ(Gj |θ)} (5)\nwhere Gi and Gj are the entity grids corresponding to documents di and dj , respectively. Notice that (also shown in Figure 2) the network shares its layers (and hence θ) to obtain φ(Gi|θ) and φ(Gj |θ) from a pair of input grids (Gi, Gj).\nBarzilay and Lapata (2008) adopted a similar ranking criterion using an SVM preference kernel learner as they argue coherence assessment is best seen as a ranking problem as opposed to classification (coherent vs. incoherent). Also, the ranker gives a scoring function φ that a text generation system can use to compare alternative hypotheses."
    }, {
      "heading" : "4 Evaluation Tasks",
      "text" : "We evaluate the effectiveness of our coherence models on two different evaluation tasks: sentence ordering and summary coherence rating."
    }, {
      "heading" : "4.1 Sentence Ordering",
      "text" : "Following (Elsner and Charniak, 2011), we evaluate our models on two sentence ordering tasks: discrimination and insertion.\nIn the discrimination task (Barzilay and Lapata, 2008), a document is compared to a random permutation of its sentences, and the model is considered correct if it scores the original document higher than the permuted one. We use 20 permutations of each document in the test set in accordance with previous work.\nIn the insertion task (Elsner and Charniak, 2011), we evaluate models based on their ability\nSections # Doc. # Pairs Avg. # Sen.\nTRAIN 00-13 1,378 26,422 21.5 TEST 14-24 1,053 20,411 22.3\nTable 1: Statistics on WSJ dataset.\nto locate the original position of a sentence previously removed from a document. To measure this, each sentence in the document is removed in turn, and an insertion place is located for which the model gives the highest coherence score to the document. The insertion score is then computed as the average fraction of sentences per document reinserted in their actual position.\nDiscrimination can be easier for longer documents, since a random permutation is likely to be different than the original one. Insertion is a much more difficult task since the candidate documents differ only by the position of one sentence.\nDataset: For sentence ordering tasks, we use the Wall Street Journal (WSJ) portion of Penn Treebank, as used by (Elsner and Charniak, 2008, 2011; Lin et al., 2011; Feng et al., 2014). Table 1 gives basic statistics about the dataset. Following previous works, we use 20 random permutations of each article, and we exclude permutations that match the original document.4 The fourth column (# Pairs) in Table 1 shows the resulting number of (original, permuted) pairs used for training our model and for testing in the discrimination task.\nSome previous studies (Barzilay and Lapata, 2008; Li and Hovy, 2014) used the AIRPLANES and the EARTHQUAKES corpora, which contain reports on airplane crashes and earthquakes, respectively. Each of these corpora contains 100 articles for training and 100 articles for testing. The average number of sentences per article in these two corpora is 10.4 and 11.5, respectively.\nWe preferred WSJ corpus for several reasons. First and most importantly, WSJ corpus is larger than other corpora (see Table 1). Large training set is crucial for learning effective deep learning models (Collobert et al., 2011), and a large enough test set is necessary to make a general comment about model performance. Secondly, as Elsner and Charniak (2011) pointed out, texts in AIRPLANES and EARTHQUAKES are constrained in style, whereas WSJ documents are more like normal informative articles. Thirdly, we could re-\n4Short articles may produce many matches.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nproduce results on this dataset for the competing systems (e.g., entity grid and its extensions) using publicly available Brown coherence toolkit.5"
    }, {
      "heading" : "4.2 Summary Coherence Rating",
      "text" : "We further evaluate our models on the summary coherence rating task proposed by Barzilay and Lapata (2008), where we compare rankings given by a model to a pair of summaries against rankings elicited from human judges.\nDataset: The summary dataset was extracted from the Document Understanding Conference (DUC’03), which contains 6 clusters of multidocument summaries produced by human experts and 5 automatic summarization systems. Each cluster has 16 summaries of a document with pairwise coherence rankings given by humans judges; see (Barzilay and Lapata, 2008) for details on the annotation method. There are 144 pairs of summaries for training and 80 pairs for testing."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we present our experiments — the models we compare, their settings, and the results."
    }, {
      "heading" : "5.1 Models Compared",
      "text" : "We compare our coherence model against a random baseline and several existing models.\nRandom: The Random baseline makes a random decision for the evaluation tasks.\nGraph-based Model: This is the graph-based unsupervised model proposed by Guinaudeau and Strube (2013). We use the implementation from the cohere6 toolkit (Smith et al., 2016), and run it on the test set with syntactic projection (command line option ‘projection=3’) for graph construction. This setting yielded best scores for this model.\nGrid-all nouns (E&C): This is the simple extension of the original entity grid model, where all nouns are considered as entities. Elsner and Charniak (2011) report significant gains by considering all nouns as opposed to only head-nouns. Results for this model were obtained by training the baseline entity grid model (command line option ‘-n’) in Brown coherence toolkit on our dataset.\n5https://bitbucket.org/melsner/browncoherence 6https://github.com/karins/CoherenceFramework\nExtended grid (E&C): This represents the extended entity grid model of Elsner and Charniak (2011) that uses 9 entity-specific features; 4 of them were computed from external corpora. This model considers all nouns as entities. For this system, we train the extended grid model (command line option ‘-f’) in Brown coherence toolkit.\nGrid-CNN: This is our proposed neural extension of the basic entity grid (all nouns), where we only consider entity transitions as input.\nExtended Grid-CNN: This corresponds to our neural model that incorporates entity-specific features following the method described in Section 3.1. To keep the model simple, we include only three entity-specific features from (Elsner and Charniak, 2011) that are easy to compute and do not require any external corpus. The features are: (i) named entity type, (ii) salience as determined by occurrence frequency of the entity, and (iii) whether the entity has a proper mention.\nA remark: We also experimented with the distributed sentence model proposed recently by Li and Hovy (2014). We trained it on our WSJ corpus using their code7 with the same setting that produced the best results on the discrimination task. However, the results on our dataset were very disappointing – accuracy of only 19.34% in discrimination. We were not sure what could go wrong, therefore, we excluded it from our table of results."
    }, {
      "heading" : "5.2 Settings for Neural Models",
      "text" : "We held out 10% of the training documents to form a development set (DEV) on which we tune the hyper-parameters of our neural models. For discrimination and insertion tasks, the resulting DEV set contains 138 articles and 2,678 pairs after removing the permutations that match the original documents. For the summary rating task, DEV contains 14 pairs of summaries.\nWe implement our models in Theano (Theano Development Team, 2016). We use rectified linear units (ReLU) as activations (f ). The embedding matrix is initialized with samples from uniform distribution U(−0.01, 0.01), and the weight matrices are initialized with samples from glorotuniform distribution (Glorot and Bengio, 2010).\nWe train the models by optimizing the pairwise ranking loss in Equation 5 using the gradientbased online learning algorithm RMSprop with\n7http://cs.stanford.edu/ bdlijiwei/code/\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nBatch Emb. Dropout Filter Win. Pool\nGrid-CNN 128 100 0.5 150 6 6 Ext. Grid-CNN 32 100 0.5 150 5 6\nTable 2: Optimal hyper-parameter setting for our neural models based on development set accuracy.\nDiscr. Ins. Acc F1\nRandom 50.0 50.0 12.60\nGraph-based (G&S) 64.23 65.01 11.93\nGrid-all nouns (E&C) 81.58 81.60 22.13 Extended Grid (E&C) 84.95 84.95 23.28\nGrid-CNN 85.57† 85.57† 23.12 Extended Grid-CNN 88.69† 88.69† 25.95†\nTable 3: Results on Discrimination and Insertion tasks. † indicates a neural model is significantly superior to its nonneural counterpart with p-value < 0.01.\nparameters (ρ and ) set to the values suggested by Tieleman and Hinton (2012).8 We use up to 25 epochs. To avoid overfitting, we use dropout (Srivastava et al., 2014) of hidden units, and do early stopping by observing accuracy on the DEV set – if the accuracy does not increase for 10 consecutive epochs, we exit with the best model recorded so far. We search for optimal minibatch size in {16, 32, 64, 128}, embedding size in {80, 100, 200}, dropout rate in {0.2, 0.3, 0.5}, filter number in {100, 150, 200, 300}, window size in {2, 3, 4, 5, 6, 7, 8}, and pooling length in {3, 4, 5, 6, 7}. Table 2 shows the optimal hyperparameter setting for our models. The best model on DEV is then used for the final evaluation on the TEST set. We run each experiment five times, each time with a different random seed, and we report the average of the runs to avoid any randomness in results. Statistical significance tests are done using an approximate randomization test based on the accuracy. We used SIGF V.2 (Padó, 2006) with 10,000 iterations."
    }, {
      "heading" : "5.3 Results on Sentence Ordering",
      "text" : "Table 3 shows the results on discrimination and insertion tasks. Among the existing models, the graph-based model gets the lowest scores, where the extended grid gets the highest scores on both tasks. By neuralizing the basic grid model (Grid-\n8Other adaptive algorithms, e.g., ADAM (Kingma and Ba, 2014), ADADELTA (Zeiler, 2012) gave similar results.\nall nouns), our Grid-CNN model delivers absolute improvements of about 4% in discrimination and 1% in insertion. When we compare our Extended Grid-CNN with its non-neural counterpart Extended Grid, we observe similar gains in discrimination and more gains (2.5%) in insertion. Note that the Extended Grid-CNN yields these improvements considering only a subset of the Extended Grid features. This demonstrates the effectiveness of distributed representation and convolutional feature learning method.\nCompared to discrimination, gain in insertion is less verbose. There could be two reasons. First, as mentioned before, insertion is a harder task than discrimination. Second, our models were not trained specifically on the insertion task. The model that is trained to distinguish an original document from its random permutation may learn features that are not specific enough to distinguish documents when only one sentence differs. It will be interesting to see how the model performs when we train it on the insertion task directly."
    }, {
      "heading" : "5.4 Results on Summary Coherence Rating",
      "text" : "Table 4 presents the results on the summary coherence rating task, where we compare our models with the graph-based method and the reported results of Barzilay and Lapata (2008) on the same experimental setting.9 Since there are not many training instances, our neural models may not learn well for this task. Therefore, we also present versions of our model, where we use pre-trained models from discrimination task on WSJ corpus (last two rows in the table ). The pre-trained models are then fine-tuned on the summary rating task.\nWe can observe that even without pre-training\n9The extended grid model does not use pairwise training, therefore could not be trained on the summarization dataset.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nour models outperform existing models, and pretraining gives further improvements. Specifically, Pre-trained Grid-CNN gives an improvement of 2.5% over the Grid model, and including entity features pushes the improvement further to 3.7%."
    }, {
      "heading" : "6 Related Work",
      "text" : "Barzilay and Lapata (2005, 2008) introduced the entity grid representation of discourse to model local coherence that captures the distribution of discourse entities across sentences in a text. They also introduced three tasks to evaluate the performance of coherence models: discrimination, summary coherence rating, and readability.\nA number of extensions of the basic entity grid model has been proposed. Elsner and Charniak (2011) included entity-specific features to distinguish between entities. Feng and Hirst (2012) used the basic grid representation, but improved its learning to rank scheme. Their model learns not only from original document and its permutations but also from ranking preferences among the permutations themselves. Guinaudeau and Strube (2013) convert a standard entity grid into a bipartite graph representing entity occurrences in sentences. To model local entity transition, the method constructs a directed projection graph representing the connection between adjacent sentences. Two sentences have a connected edge if they share at least one entity in common. The coherence score of the document is then computed as the average out-degree of sentence nodes.\nIn addition, there are some approaches that model text coherence based on coreferences and discourse relations. Elsner and Charniak (2008) proposed the discourse-new model by taking into account mentions of all referring expression (i.e., NPs) whether they are first mention (discoursenew) or subsequent (discourse-old) mentions. Given a document, they run a maximum-entropy classifier to detect each NP as a label Lnp ∈ {new, old}. The coherence score of the document is then estimated by ∏ np:NPs P (Lnp|np). In this work, they also estimate text coherence through pronoun coreference modeling. Lin et al. (2011) assume that a coherent text has certain discourse relation patterns. Instead of modeling entity transitions, they model discourse role transitions between sentences. In a follow up work, Feng et al. (2014) trained the same model but using features derived from deep discourse struc-\ntures annotated with Rhetorical Structure Theory or RST (Mann and Thompson, 1988b) relations. Louis and Nenkova (2012) introduced a coherence model based on syntactic patterns in text by assuming that sentences in a coherent discourse should share the same structural syntactic patterns.\nIn recent years, there has been a growing interest in neuralizing traditional NLP approaches – language modeling (Bengio et al., 2003), sequence tagging (Collobert et al., 2011), syntactic parsing (Socher et al., 2013), and discourse parsing (Li et al., 2014), etc. Following this tradition, in this paper we propose to neuralize the popular entity grid models. Li and Hovy (2014) also proposed a neural framework to compute coherence score of a document by estimating coherence probability for every window of L sentences (in their experiments, L = 3). First, they use a recurrent or a recursive neural network to compute the representation for each sentence in L from its words and their pre-trained embeddings. Then the concatenated vector is passed through a non-linear hidden layer, and finally the output layer decides if the window of sentences is a coherent text or not. Our approach is fundamentally different from their approach; our model operates over entity grids, and we use convolutional architecture to model sufficiently long entity transitions."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "We presented a local coherence model based on a convolutional neural network that operates over the distributed representation of entity transitions in the grid representation of a text. Our architecture can model sufficiently long entity transitions, and can incorporate entity-specific features without loosing generalization power. We described a pairwise ranking approach to train the model on a target task and learn task-specific features. Our evaluation on discrimination, insertion and summary coherence rating tasks demonstrates the effectiveness of our approach yielding the best results reported so far on these tasks.\nIn future, we would like to include other sources of information in our model. Our initial plan is to include rhetorical relations, which has been shown to benefit existing grid models (Feng et al., 2014). We would also like to extend our model to other forms of discourse, especially, asynchronous conversations, where participants communicate with each other at different times (e.g., forum, email).\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Modeling local coherence: An entity-based approach",
      "author" : [ "Regina Barzilay", "Mirella Lapata." ],
      "venue" : "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, Ann Arbor, Michi-",
      "citeRegEx" : "Barzilay and Lapata.,? 2005",
      "shortCiteRegEx" : "Barzilay and Lapata.",
      "year" : 2005
    }, {
      "title" : "Modeling local coherence: An entity-based approach",
      "author" : [ "Regina Barzilay", "Mirella Lapata." ],
      "venue" : "Computational Linguistics 34(1):1–34. http://www.aclweb.org/anthology/J08-1001.",
      "citeRegEx" : "Barzilay and Lapata.,? 2008",
      "shortCiteRegEx" : "Barzilay and Lapata.",
      "year" : 2008
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin." ],
      "venue" : "J. Mach. Learn. Res. 3. http://dl.acm.org/citation.cfm?id=944919.944966.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Using entity-based features to model coherence in student essays",
      "author" : [ "Jill Burstein", "Joel Tetreault", "Slava Andreyev." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-",
      "citeRegEx" : "Burstein et al\\.,? 2010",
      "shortCiteRegEx" : "Burstein et al\\.",
      "year" : 2010
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "The Journal of Machine Learning Research 12:2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Coreference-inspired coherence modeling",
      "author" : [ "Micha Elsner", "Eugene Charniak." ],
      "venue" : "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers. Association",
      "citeRegEx" : "Elsner and Charniak.,? 2008",
      "shortCiteRegEx" : "Elsner and Charniak.",
      "year" : 2008
    }, {
      "title" : "Extending the entity grid with entity-specific features",
      "author" : [ "Micha Elsner", "Eugene Charniak." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2. As-",
      "citeRegEx" : "Elsner and Charniak.,? 2011",
      "shortCiteRegEx" : "Elsner and Charniak.",
      "year" : 2011
    }, {
      "title" : "Extending the entity-based coherence model with multiple ranks",
      "author" : [ "Vanessa Wei Feng", "Graeme Hirst." ],
      "venue" : "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. Association for Compu-",
      "citeRegEx" : "Feng and Hirst.,? 2012",
      "shortCiteRegEx" : "Feng and Hirst.",
      "year" : 2012
    }, {
      "title" : "The impact of deep hierarchical discourse structures in the evaluation of text coherence",
      "author" : [ "Vanessa Wei Feng", "Ziheng Lin", "Graeme Hirst." ],
      "venue" : "COLING.",
      "citeRegEx" : "Feng et al\\.,? 2014",
      "shortCiteRegEx" : "Feng et al\\.",
      "year" : 2014
    }, {
      "title" : "Extending the entity-grid coherence model to semantically related entities",
      "author" : [ "Katja Filippova", "Michael Strube." ],
      "venue" : "Proceedings of the Eleventh European Workshop on Natural Language Generation. Association for Computational Linguistics,",
      "citeRegEx" : "Filippova and Strube.,? 2007",
      "shortCiteRegEx" : "Filippova and Strube.",
      "year" : 2007
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio." ],
      "venue" : "JMLR W&CP: Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS 2010). Sardinia,",
      "citeRegEx" : "Glorot and Bengio.,? 2010",
      "shortCiteRegEx" : "Glorot and Bengio.",
      "year" : 2010
    }, {
      "title" : "Centering: A framework for modeling the local coherence of discourse",
      "author" : [ "Barbara J. Grosz", "Scott Weinstein", "Aravind K. Joshi." ],
      "venue" : "Comput. Linguist. 21(2):203–225.",
      "citeRegEx" : "Grosz et al\\.,? 1995",
      "shortCiteRegEx" : "Grosz et al\\.",
      "year" : 1995
    }, {
      "title" : "Graph-based local coherence modeling",
      "author" : [ "Camille Guinaudeau", "Michael Strube." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013, Sofia, Bulgaria, Volume 1: Long Papers.",
      "citeRegEx" : "Guinaudeau and Strube.,? 2013",
      "shortCiteRegEx" : "Guinaudeau and Strube.",
      "year" : 2013
    }, {
      "title" : "Optimizing search engines using clickthrough data",
      "author" : [ "Thorsten Joachims." ],
      "venue" : "Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, Edmonton, Alberta, Canada, KDD ’02, pages 133–142.",
      "citeRegEx" : "Joachims.,? 2002",
      "shortCiteRegEx" : "Joachims.",
      "year" : 2002
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). As-",
      "citeRegEx" : "Kalchbrenner et al\\.,? 2014",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar, pages 1746–",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR abs/1412.6980. http://arxiv.org/abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "A model of coherence based on distributed sentence representation",
      "author" : [ "Jiwei Li", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computa-",
      "citeRegEx" : "Li and Hovy.,? 2014",
      "shortCiteRegEx" : "Li and Hovy.",
      "year" : 2014
    }, {
      "title" : "Recursive deep models for discourse parsing",
      "author" : [ "Jiwei Li", "Rumeng Li", "Eduard H Hovy." ],
      "venue" : "EMNLP. pages 2061–2069.",
      "citeRegEx" : "Li et al\\.,? 2014",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2014
    }, {
      "title" : "Automatically evaluating text coherence using discourse relations",
      "author" : [ "Ziheng Lin", "Hwee Tou Ng", "Min-Yen Kan." ],
      "venue" : "Proceedings of the 49th Annual",
      "citeRegEx" : "Lin et al\\.,? 2011",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2011
    }, {
      "title" : "Discourse generation using utility-trained coherence models",
      "author" : [ "Radu Soricut", "Daniel Marcu." ],
      "venue" : "Proceedings of the COLING/ACL on Main Conference Poster Sessions. Association for Computational Linguistics, Sydney, Australia, COLING-ACL ’06,",
      "citeRegEx" : "Soricut and Marcu.,? 2006",
      "shortCiteRegEx" : "Soricut and Marcu.",
      "year" : 2006
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "Journal of Machine Learning Research 15:1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Theano: A Python framework for fast computation of mathematical expressions",
      "author" : [ "Theano Development Team." ],
      "venue" : "arXiv e-prints abs/1605.02688. http://arxiv.org/abs/1605.02688.",
      "citeRegEx" : "Team.,? 2016",
      "shortCiteRegEx" : "Team.",
      "year" : 2016
    }, {
      "title" : "RMSprop, COURSERA: Neural Networks",
      "author" : [ "T. Tieleman", "G Hinton" ],
      "venue" : null,
      "citeRegEx" : "Tieleman and Hinton.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tieleman and Hinton.",
      "year" : 2012
    }, {
      "title" : "ADADELTA: an adaptive learning rate method",
      "author" : [ "Matthew D. Zeiler." ],
      "venue" : "CoRR abs/1212.5701. http://arxiv.org/abs/1212.5701.",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Several formal theories of coherence have been proposed (Mann and Thompson, 1988a; Grosz et al., 1995; Asher and Lascarides, 2003), and their principles have inspired development of existing coherence models (Barzilay and Lapata, 2008; Lin et al.",
      "startOffset" : 56,
      "endOffset" : 130
    }, {
      "referenceID" : 1,
      "context" : ", 1995; Asher and Lascarides, 2003), and their principles have inspired development of existing coherence models (Barzilay and Lapata, 2008; Lin et al., 2011; Li and Hovy, 2014).",
      "startOffset" : 113,
      "endOffset" : 177
    }, {
      "referenceID" : 19,
      "context" : ", 1995; Asher and Lascarides, 2003), and their principles have inspired development of existing coherence models (Barzilay and Lapata, 2008; Lin et al., 2011; Li and Hovy, 2014).",
      "startOffset" : 113,
      "endOffset" : 177
    }, {
      "referenceID" : 17,
      "context" : ", 1995; Asher and Lascarides, 2003), and their principles have inspired development of existing coherence models (Barzilay and Lapata, 2008; Lin et al., 2011; Li and Hovy, 2014).",
      "startOffset" : 113,
      "endOffset" : 177
    }, {
      "referenceID" : 1,
      "context" : "Among these models, the entity grid (Barzilay and Lapata, 2008), which is based on Centering Theory (Grosz et al.",
      "startOffset" : 36,
      "endOffset" : 63
    }, {
      "referenceID" : 11,
      "context" : "Among these models, the entity grid (Barzilay and Lapata, 2008), which is based on Centering Theory (Grosz et al., 1995), is arguably the most popular, and has seen a number of improvements over the years.",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 6,
      "context" : "Extensions of this basic grid model incorporate entity-specific features (Elsner and Charniak, 2011), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al.",
      "startOffset" : 73,
      "endOffset" : 100
    }, {
      "referenceID" : 7,
      "context" : "Extensions of this basic grid model incorporate entity-specific features (Elsner and Charniak, 2011), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al.",
      "startOffset" : 117,
      "endOffset" : 139
    }, {
      "referenceID" : 8,
      "context" : "Extensions of this basic grid model incorporate entity-specific features (Elsner and Charniak, 2011), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014).",
      "startOffset" : 165,
      "endOffset" : 184
    }, {
      "referenceID" : 2,
      "context" : "Firstly, they use discrete representation for grammatical roles and features, which limits the model to consider sufficiently long transitions (Bengio et al., 2003).",
      "startOffset" : 143,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "We evaluate our approach on three different evaluation tasks: discrimination, insertion, and summary coherence rating, proposed previously for evaluating coherence models (Barzilay and Lapata, 2008; Elsner and Charniak, 2011).",
      "startOffset" : 171,
      "endOffset" : 225
    }, {
      "referenceID" : 6,
      "context" : "We evaluate our approach on three different evaluation tasks: discrimination, insertion, and summary coherence rating, proposed previously for evaluating coherence models (Barzilay and Lapata, 2008; Elsner and Charniak, 2011).",
      "startOffset" : 171,
      "endOffset" : 225
    }, {
      "referenceID" : 11,
      "context" : "Motivated by Centering Theory (Grosz et al., 1995), Barzilay and Lapata (2008) proposed an entity-based model for representing and assessing text coherence.",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 13,
      "context" : "Assessment of text coherence is then formulated as a ranking problem in an SVM preference ranking framework (Joachims, 2002).",
      "startOffset" : 108,
      "endOffset" : 124
    }, {
      "referenceID" : 0,
      "context" : ", 1995), Barzilay and Lapata (2008) proposed an entity-based model for representing and assessing text coherence.",
      "startOffset" : 9,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : ", 1995), Barzilay and Lapata (2008) proposed an entity-based model for representing and assessing text coherence. Their model represents a text by a two-dimensional array called entity grid that captures transitions of discourse entities across sentences. As shown in Figure 2, the rows of the grid correspond to sentences, and the columns correspond to discourse entities appearing in the text. They consider noun phrases (NP) as entities, and employ a coreference resolver to detect mentions of the same entity (e.g., Obama, the president). Each entry Gi,j in the entity grid represents the syntactic role that entity ej plays in sentence si, which can be one of: subject (S), object (O), or other (X). In addition, entities not appearing in a sentence are marked by a special symbol (-). If an entity appears more than once with different grammatical roles in the same sentence, the role with the highest rank (S O X) is considered. To represent an entity grid with a feature vector, Barzilay and Lapata (2008) compute probability for each local entity transition of length k (i.",
      "startOffset" : 9,
      "endOffset" : 1014
    }, {
      "referenceID" : 0,
      "context" : ", 1995), Barzilay and Lapata (2008) proposed an entity-based model for representing and assessing text coherence. Their model represents a text by a two-dimensional array called entity grid that captures transitions of discourse entities across sentences. As shown in Figure 2, the rows of the grid correspond to sentences, and the columns correspond to discourse entities appearing in the text. They consider noun phrases (NP) as entities, and employ a coreference resolver to detect mentions of the same entity (e.g., Obama, the president). Each entry Gi,j in the entity grid represents the syntactic role that entity ej plays in sentence si, which can be one of: subject (S), object (O), or other (X). In addition, entities not appearing in a sentence are marked by a special symbol (-). If an entity appears more than once with different grammatical roles in the same sentence, the role with the highest rank (S O X) is considered. To represent an entity grid with a feature vector, Barzilay and Lapata (2008) compute probability for each local entity transition of length k (i.e., {S,O,X,−}k), and represent each grid by a vector of 4k transitions probabilities. To distinguish between transitions of important entities from unimportant ones, they consider the salience of the entities, which they quantify by their occurrence frequency in the document. Assessment of text coherence is then formulated as a ranking problem in an SVM preference ranking framework (Joachims, 2002). Subsequent studies proposed to extend the basic entity grid model. Filippova and Strube (2007) attempted to improve the model by grouping entities based on semantic relatedness, but did not get significant improvement.",
      "startOffset" : 9,
      "endOffset" : 1580
    }, {
      "referenceID" : 0,
      "context" : ", 1995), Barzilay and Lapata (2008) proposed an entity-based model for representing and assessing text coherence. Their model represents a text by a two-dimensional array called entity grid that captures transitions of discourse entities across sentences. As shown in Figure 2, the rows of the grid correspond to sentences, and the columns correspond to discourse entities appearing in the text. They consider noun phrases (NP) as entities, and employ a coreference resolver to detect mentions of the same entity (e.g., Obama, the president). Each entry Gi,j in the entity grid represents the syntactic role that entity ej plays in sentence si, which can be one of: subject (S), object (O), or other (X). In addition, entities not appearing in a sentence are marked by a special symbol (-). If an entity appears more than once with different grammatical roles in the same sentence, the role with the highest rank (S O X) is considered. To represent an entity grid with a feature vector, Barzilay and Lapata (2008) compute probability for each local entity transition of length k (i.e., {S,O,X,−}k), and represent each grid by a vector of 4k transitions probabilities. To distinguish between transitions of important entities from unimportant ones, they consider the salience of the entities, which they quantify by their occurrence frequency in the document. Assessment of text coherence is then formulated as a ranking problem in an SVM preference ranking framework (Joachims, 2002). Subsequent studies proposed to extend the basic entity grid model. Filippova and Strube (2007) attempted to improve the model by grouping entities based on semantic relatedness, but did not get significant improvement. Elsner and Charniak (2011) proposed a number of improvements.",
      "startOffset" : 9,
      "endOffset" : 1731
    }, {
      "referenceID" : 1,
      "context" : "Entity grid and its extensions have been successfully applied to many downstream tasks including coherence rating (Barzilay and Lapata, 2008), essay scoring (Burstein et al.",
      "startOffset" : 114,
      "endOffset" : 141
    }, {
      "referenceID" : 3,
      "context" : "Entity grid and its extensions have been successfully applied to many downstream tasks including coherence rating (Barzilay and Lapata, 2008), essay scoring (Burstein et al., 2010), story generation (McIntyre and Lapata, 2010), and readability assessment (Pitler et al.",
      "startOffset" : 157,
      "endOffset" : 180
    }, {
      "referenceID" : 1,
      "context" : ", 2010), story generation (McIntyre and Lapata, 2010), and readability assessment (Pitler et al., 2010; Barzilay and Lapata, 2008).",
      "startOffset" : 82,
      "endOffset" : 130
    }, {
      "referenceID" : 20,
      "context" : "They have also been critical components in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2011; Lin et al., 2011).",
      "startOffset" : 85,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "They have also been critical components in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2011; Lin et al., 2011).",
      "startOffset" : 85,
      "endOffset" : 155
    }, {
      "referenceID" : 19,
      "context" : "They have also been critical components in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2011; Lin et al., 2011).",
      "startOffset" : 85,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "• Existing models use discrete representation for grammatical roles and features, which leads to the so-called curse of dimensionality problem (Bengio et al., 2003).",
      "startOffset" : 143,
      "endOffset" : 164
    }, {
      "referenceID" : 6,
      "context" : "This problem is exacerbated when we want to include entity-specific features, as the number of parameters grows exponentially with the number of features (Elsner and Charniak, 2011).",
      "startOffset" : 154,
      "endOffset" : 181
    }, {
      "referenceID" : 4,
      "context" : "Under these assumptions, the natural choice to tackle this problem is to use a convolutional approach, used previously to solve other NLP tasks (Collobert et al., 2011; Kim, 2014).",
      "startOffset" : 144,
      "endOffset" : 179
    }, {
      "referenceID" : 15,
      "context" : "Under these assumptions, the natural choice to tackle this problem is to use a convolutional approach, used previously to solve other NLP tasks (Collobert et al., 2011; Kim, 2014).",
      "startOffset" : 144,
      "endOffset" : 179
    }, {
      "referenceID" : 14,
      "context" : "Notice that we use a wide convolution (Kalchbrenner et al., 2014), as opposed to narrow, to ensure that the filters reach entire columns of a grid, including the boundary entities.",
      "startOffset" : 38,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "However, as Elsner and Charniak (2011) pointed out entity-specific features could be crucial for modeling local coherence.",
      "startOffset" : 12,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "We use a pairwise ranking approach (Collobert et al., 2011) to learn θ.",
      "startOffset" : 35,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "Barzilay and Lapata (2008) adopted a similar ranking criterion using an SVM preference kernel learner as they argue coherence assessment is best seen as a ranking problem as opposed to classification (coherent vs.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 6,
      "context" : "1 Sentence Ordering Following (Elsner and Charniak, 2011), we evaluate our models on two sentence ordering tasks: discrimination and insertion.",
      "startOffset" : 30,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "In the discrimination task (Barzilay and Lapata, 2008), a document is compared to a random permutation of its sentences, and the model is considered correct if it scores the original document higher than the permuted one.",
      "startOffset" : 27,
      "endOffset" : 54
    }, {
      "referenceID" : 6,
      "context" : "In the insertion task (Elsner and Charniak, 2011), we evaluate models based on their ability Sections # Doc.",
      "startOffset" : 22,
      "endOffset" : 49
    }, {
      "referenceID" : 19,
      "context" : "Dataset: For sentence ordering tasks, we use the Wall Street Journal (WSJ) portion of Penn Treebank, as used by (Elsner and Charniak, 2008, 2011; Lin et al., 2011; Feng et al., 2014).",
      "startOffset" : 112,
      "endOffset" : 182
    }, {
      "referenceID" : 8,
      "context" : "Dataset: For sentence ordering tasks, we use the Wall Street Journal (WSJ) portion of Penn Treebank, as used by (Elsner and Charniak, 2008, 2011; Lin et al., 2011; Feng et al., 2014).",
      "startOffset" : 112,
      "endOffset" : 182
    }, {
      "referenceID" : 1,
      "context" : "Some previous studies (Barzilay and Lapata, 2008; Li and Hovy, 2014) used the AIRPLANES and the EARTHQUAKES corpora, which contain reports on airplane crashes and earthquakes, respectively.",
      "startOffset" : 22,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "Some previous studies (Barzilay and Lapata, 2008; Li and Hovy, 2014) used the AIRPLANES and the EARTHQUAKES corpora, which contain reports on airplane crashes and earthquakes, respectively.",
      "startOffset" : 22,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "Large training set is crucial for learning effective deep learning models (Collobert et al., 2011), and a large enough test set is necessary to make a general comment about model performance.",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "Some previous studies (Barzilay and Lapata, 2008; Li and Hovy, 2014) used the AIRPLANES and the EARTHQUAKES corpora, which contain reports on airplane crashes and earthquakes, respectively. Each of these corpora contains 100 articles for training and 100 articles for testing. The average number of sentences per article in these two corpora is 10.4 and 11.5, respectively. We preferred WSJ corpus for several reasons. First and most importantly, WSJ corpus is larger than other corpora (see Table 1). Large training set is crucial for learning effective deep learning models (Collobert et al., 2011), and a large enough test set is necessary to make a general comment about model performance. Secondly, as Elsner and Charniak (2011) pointed out, texts in AIRPLANES and EARTHQUAKES are constrained in style, whereas WSJ documents are more like normal informative articles.",
      "startOffset" : 23,
      "endOffset" : 734
    }, {
      "referenceID" : 0,
      "context" : "We further evaluate our models on the summary coherence rating task proposed by Barzilay and Lapata (2008), where we compare rankings given by a model to a pair of summaries against rankings elicited from human judges.",
      "startOffset" : 80,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "Each cluster has 16 summaries of a document with pairwise coherence rankings given by humans judges; see (Barzilay and Lapata, 2008) for details on the annotation method.",
      "startOffset" : 105,
      "endOffset" : 132
    }, {
      "referenceID" : 12,
      "context" : "Graph-based Model: This is the graph-based unsupervised model proposed by Guinaudeau and Strube (2013). We use the implementation from the cohere6 toolkit (Smith et al.",
      "startOffset" : 74,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "Elsner and Charniak (2011) report significant gains by considering all nouns as opposed to only head-nouns.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "com/karins/CoherenceFramework Extended grid (E&C): This represents the extended entity grid model of Elsner and Charniak (2011) that uses 9 entity-specific features; 4 of them were computed from external corpora.",
      "startOffset" : 101,
      "endOffset" : 128
    }, {
      "referenceID" : 6,
      "context" : "To keep the model simple, we include only three entity-specific features from (Elsner and Charniak, 2011) that are easy to compute and do not require any external corpus.",
      "startOffset" : 78,
      "endOffset" : 105
    }, {
      "referenceID" : 17,
      "context" : "A remark: We also experimented with the distributed sentence model proposed recently by Li and Hovy (2014). We trained it on our WSJ corpus using their code7 with the same setting that produced the best results on the discrimination task.",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 10,
      "context" : "01), and the weight matrices are initialized with samples from glorotuniform distribution (Glorot and Bengio, 2010).",
      "startOffset" : 90,
      "endOffset" : 115
    }, {
      "referenceID" : 21,
      "context" : "To avoid overfitting, we use dropout (Srivastava et al., 2014) of hidden units, and do early stopping by observing accuracy on the DEV set – if the accuracy does not increase for 10 consecutive epochs, we exit with the best model recorded so far.",
      "startOffset" : 37,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "parameters (ρ and ) set to the values suggested by Tieleman and Hinton (2012).8 We use up to 25 epochs.",
      "startOffset" : 51,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : ", ADAM (Kingma and Ba, 2014), ADADELTA (Zeiler, 2012) gave similar results.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 24,
      "context" : ", ADAM (Kingma and Ba, 2014), ADADELTA (Zeiler, 2012) gave similar results.",
      "startOffset" : 39,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "4 Results on Summary Coherence Rating Table 4 presents the results on the summary coherence rating task, where we compare our models with the graph-based method and the reported results of Barzilay and Lapata (2008) on the same experimental setting.",
      "startOffset" : 189,
      "endOffset" : 216
    }, {
      "referenceID" : 2,
      "context" : "In recent years, there has been a growing interest in neuralizing traditional NLP approaches – language modeling (Bengio et al., 2003), sequence tagging (Collobert et al.",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 4,
      "context" : ", 2003), sequence tagging (Collobert et al., 2011), syntactic parsing (Socher et al.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 18,
      "context" : ", 2013), and discourse parsing (Li et al., 2014), etc.",
      "startOffset" : 31,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "Our initial plan is to include rhetorical relations, which has been shown to benefit existing grid models (Feng et al., 2014).",
      "startOffset" : 106,
      "endOffset" : 125
    } ],
    "year" : 2017,
    "abstractText" : "We propose a local coherence model based on a convolutional neural network that operates over the entity grid representation of a text. The model captures long range entity transitions along with entity-specific features without loosing generalization, thanks to the power of distributed representation. We present a pairwise ranking method to train the model in an end-to-end fashion on a task and learn task-specific high level features. Our evaluation on three different coherence assessment tasks demonstrates that our model achieves state of the art results outperforming existing models by a good margin.",
    "creator" : "LaTeX with hyperref package"
  }
}