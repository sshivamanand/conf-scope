{
  "name" : "352.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Multi-task learning is an effective approach to improve the performance of a single task with the help of other related tasks. Recently, neuralbased models for multi-task learning have become very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural language processing (Collobert and Weston, 2008; Luong et al., 2015), since they provide a convenient way of combining information frommultiple tasks.\nHowever, most existing work on multi-task learning attempts to divide the features of different tasks into private and shared spaces, merely based on whether parameters of some components should be shared. As shown in Figure 1- (a), the general shared-private model introduces two feature spaces for any task: one is used to\nstore task-dependent features, the other is used to capture shared features. The major limitation of this framework is that the shared feature space could contain some unnecessary task-specific features, while some sharable features could also be mixed in private space, suffering from feature redundancy. Taking the following two sentences as examples, which are extracted from two different sentiment classification tasks: Movie reviews and Baby products reviews.\nThe infantile cart is simple and easy to use. This kind of humour is infantile and boring.\nThe word “infantile” indicates negative sentiment in Movie task while it is neutral in Baby task. However, the general shared-private model could place the task-specific word “infantile” in a shared space, leaving potential hazards for other tasks. Additionally, the capacity of shared space could also be wasted by some unnecessary features. To address this problem, in this paper we propose an adversarial multi-task framework, in which the shared and private feature spaces are inherently disjoint by introducing orthogonality constraints. Specifically, we design a generic sharedprivate learning framework to model the text se-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nquence. To prevent the shared and private latent feature spaces from interfering with each other, we introduce two strategies: adversarial training and orthogonality constraints. The adversarial training is used to ensure that the shared feature space simply contains common and task-invariant information, while the orthogonality constraint is used to eliminate redundant features from the private and shared spaces.\nThe contributions of this paper can be summarized as follows.\n1. Proposed model divides the task-specific and shared space in a more precise way, rather than roughly sharing parameters. 2. We extend the original binary adversarial training to multi-class, which not only enables multiple tasks to be jointly trained, but allows us to utilize unlabeled data. 3. We can condense the shared knowledge among multiple tasks into an off-the-shelf neural layer, which can be easily transferred to new tasks."
    }, {
      "heading" : "2 Recurrent Models for Text Classification",
      "text" : "There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks.\nLong Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) (Elman, 1990), and specifically addresses the issue of learning long-term dependencies. While there are numerous LSTM variants, here we use the LSTM architecture used by (Jozefowicz et al., 2015), which is similar to the architecture of (Graves, 2013) but without peep-hole connections.\nWe define the LSTM units at each time step t to be a collection of vectors in Rd: an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. d is the number of the LSTM units. The elements of the gating vectors it, ft and ot are in [0, 1].\nThe LSTM is precisely specified as follows.\n c̃t ot it ft  =  tanh σ σ σ (Wp [ xtht−1 ] + bp ) , (1)\nct = c̃t ⊙ it + ct−1 ⊙ ft, (2) ht = ot ⊙ tanh (ct) , (3)\nwhere xt ∈ Re is the input at the current time step; Wp ∈ R4d×(d+e) and bp ∈ R4d are parameters of affine transformation; σ denotes the logistic sigmoid function and ⊙ denotes elementwise multiplication. The update of each LSTM unit can be written precisely as follows:\nht = LSTM(ht−1,xt, θp). (4)\nHere, the function LSTM(·, ·, ·, ·) is a shorthand for Eq. (1-3), and θp represents all the parameters of LSTM.\nText Classification with LSTM Given a text sequence x = {x1, x2, · · · , xT }, we first use a lookup layer to get the vector representation (embeddings) xi of the each word xi. The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes.\nŷ = softmax(WhT + b) (5)\nwhere ŷ is prediction probabilities, W is the weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi, yi), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions.\nL(ŷ, y) = − N∑ i=1 C∑ j=1 yji log(ŷ j i ), (6)\nwhere yji is the ground-truth label; ŷ j i is prediction probabilities, and C is the class number."
    }, {
      "heading" : "3 Multi-task Learning for Text Classification",
      "text" : "The goal of multi-task learning is to utilizes the correlation among these related tasks to improve\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nsoftmax Lmtask\nLSTM\nsoftmax Lntask\nxm xn\n(a) Fully Shared Model (FS-MTL)\nxm\nxn\nLSTM\nLSTM\nLSTM\nsoftmax\nsoftmax\nLmtask\nLntask\n(b) Shared-Private Model (SP-MTL)\nFigure 2: Two architectures for learning multiple tasks. Yellow and gray boxes represent shared and private LSTM layers respectively.\nclassification by learning tasks in parallel. To facilitate this, we give some explanation for notations used in this paper. Formally, we refer to Dk as a dataset with Nk samples for task k. Specifically,\nDk = {(xki , yki )} Nk i=1 (7)\nwhere xki and y k i denote a sentence and corresponding label for task k."
    }, {
      "heading" : "3.1 Two Sharing Schemes for Sentence Modeling",
      "text" : "The key factor of multi-task learning is the sharing scheme in latent feature space. In neural network based model, the latent features can be regarded as the states of hidden neurons. Specific to text classification, the latent features are the hidden states of LSTM at the end of a sentence. Therefore, the sharing schemes are different in how to group the shared features. Here, we first introduce two sharing schemes with multi-task learning: fully-shared scheme and shared-private scheme.\nFully-SharedModel (FS-MTL) In fully-shared model, we use a single shared LSTM layer to extract features for all the tasks. For example, given two tasks m and n, it takes the view that the features of taskm can be totally shared by task n and vice versa. This model ignores the fact that some features are task-dependent. Figure 2a illustrates the fully-shared model.\nShared-Private Model (SP-MTL) As shown in Figure 2b, the shared-private model introduces two feature spaces for each task: one is used to store task-dependent features, the other is used\nto capture task-invariant features. Accordingly, we can see each task is assigned a private LSTM layer and shared LSTM layer. Formally, for any sentence in task k, we can compute its shared representation skt and task-specific representation h k t as follows:\nskt = LSTM(xt, s k t−1, θs), (8)\nhkt = LSTM(xt,h m t−1, θk) (9)\nwhere LSTM(., θ) is defined as Eq. (4). The final features are concatenation of the features from private space and shared space."
    }, {
      "heading" : "3.2 Task-Specific Output Layer",
      "text" : "For a sentence in task k, its feature h(k), emitted by the deep muti-task architectures, is ultimately fed into the corresponding task-specific softmax layer for classification or other tasks. The parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions on all the tasks. The loss Ltask can be computed as:\nLTask = K∑ k=1 αkL(ŷ (k), y(k)) (10)\nwhere αk is the weights for each task k respectively. L(ŷ, y) is defined as Eq. 6."
    }, {
      "heading" : "4 Incorporating Adversarial Training",
      "text" : "Although the shared-private model separates the feature space into the shared and private spaces, there is no guarantee that sharable features can not exist in private feature space, or vice versa. Thus, some useful sharable features could be ignored in shared-private model, and the shared feature space is also vulnerable to contamination by some taskspecific information. Therefore, a simple principle can be applied into multi-task learning that a good shared feature space should contain more common information and no task-specific information. To address this problem, we introduce adversarial training into multi-task framework as shown in Figure 3 (ASPMTL)."
    }, {
      "heading" : "4.1 Adversarial Network",
      "text" : "Adversarial networks have recently surfaced and are first used for generative model (Goodfellow\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nxm\nxn\nLSTM\nLSTM\nLSTM\nLDiff LAdvLDiff\nsoftmax\nsoftmax\nLmtask\nLntask\nFigure 3: Adversarial shared-private model. Yellow and gray boxes represent shared and private LSTM layers respectively.\net al., 2014). The goal is to learn a generative distribution pG(x) that matches the real data distribution Pdata(x) Specifically, GAN learns a generative network G and discriminative model D, in which G generates samples from the generator distribution pG(x). and D learns to determine whether a sample is from pG(x) or Pdata(x). This min-max game can be optimized by the following risk:\nϕ = min G max D\n( Ex∼Pdata [logD(x)]\n+ Ez∼p(z)[log(1−D(G(z)))] )\n(11)\nWhile originally proposed for generating random samples, adversarial network can be used as a general tool to measure equivalence between distributions (Taigman et al., 2016). Formally, (Ajakan et al., 2014) linked the adversarial loss to the H-divergence between two distributions and successfully achieve unsupervised domain adaptation with adversarial network. Motivated by theory on domain adaptation (Ben-David et al., 2010, 2007; Bousmalis et al., 2016) that a transferable feature is one for which an algorithm cannot learn to identify the domain of origin of the input observation."
    }, {
      "heading" : "4.2 Task Adversarial Loss for MTL",
      "text" : "Inspired by adversarial networks (Goodfellow et al., 2014), we proposed an adversarial sharedprivate model for multi-task learning, in which a shared recurrent neural layer is working adversarially towards a learnable multi-layer perceptron, preventing it from making an accurate prediction about the types of tasks. This adversarial training encourages shared space to be more pure and ensure the shared representation not be contaminated by task-specific features.\nTask Discriminator Discriminator is used to map the shared representation of sentences into a\nprobability distribution, estimating what kinds of tasks the encoded sentence comes from.\nD(skT , θD) = softmax(b+Us k T ) (12)\nwhereU ∈ Rd×d is a learnable parameter and b ∈ Rd is a bias.\nAdversarial Loss Different with most existing multi-task learning algorithm, we add an extra task adversarial loss LAdv to prevent task-specific feature from creeping in to shared space. The task adversarial loss is used to train a model to produce shared features such that a classifier cannot reliably predict the task based on these features. The original loss of adversarial network is limited since it can only be used in binary situation. To overcome this, we extend it to multi-class form, which allow our model can be trained together with multiple tasks:\nLAdv = min θs\n( λmax\nθD ( K∑ k=1 Nk∑ i=1 dki log[D(E(x k))])\n) (13)\nwhere dki denotes the ground-truth label indicating the type of the current task. Here, there is a minmax optimization and the basic idea is that, given a sentence, the shared LSTM generates a representation to mislead the task discriminator. At the same time, the discriminator tries its best to make a correct classification on the type of task. After the training phase, the shared feature extractor and task discriminator reach a point at which both cannot improve and the discriminator is unable to differentiate among all the tasks.\nSemi-supervised Learning Multi-task Learning We notice that the LAdv requires only the input sentence x and does not require the corresponding label y, which makes it possible to combine our model with semi-supervised learning. Finally, in this semi-supervised multi-task learning framework, our model can not only utilize the data from related tasks, but can employ abundant unlabeled corpora."
    }, {
      "heading" : "4.3 Orthogonality Constraints",
      "text" : "We notice that there is a potential drawback of the above model. That is, the task-invariant features can appear both in shared space and private space. Motivated by recently work(Jia et al., 2010; Salzmann et al., 2010; Bousmalis et al., 2016)\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nDataset Train Dev. Test Unlab. Avg. L Vocab.\nBooks 1400 200 400 2000 159 62K Elec. 1398 200 400 2000 101 30K DVD 1400 200 400 2000 173 69K Kitchen 1400 200 400 2000 89 28K Apparel 1400 200 400 2000 57 21K Camera 1397 200 400 2000 130 26K Health 1400 200 400 2000 81 26K Music 1400 200 400 2000 136 60K Toys 1400 200 400 2000 90 28K Video 1400 200 400 2000 156 57K Baby 1300 200 400 2000 104 26K Mag. 1370 200 400 2000 117 30K Soft. 1315 200 400 2000 129 26K Sports 1400 200 400 2000 94 30K IMDB 1400 200 400 2000 269 44K MR 1400 200 400 2000 21 12K\nTable 1: Statistics of the 16 datasets. The columns 2-5 denote the number of samples in training, development, test and unlabeled sets. The last two columns represent the average length and vocabulary size of corresponding dataset.\non shared-private latent space analysis, we introduce orthogonality constraints, which penalize redundant latent representations and encourages the shared and private extractors to encode different aspects of the inputs.\nAfter exploring many optional methods, we find below loss is optimal, which is used by Bousmalis et al. (2016) and achieve a better performance:\nLdiff = K∑ k=1 ∥∥∥Sk⊤Hk∥∥∥2 F , (14)\nwhere ∥ · ∥2F is the squared Frobenius norm. S k and Hk are two matrics, whose rows are the output of shared extractor Es(, ; θs) and task-specific extrator Ek(, ; θk) of a input sentence."
    }, {
      "heading" : "4.4 Put It All Together",
      "text" : "The final loss function of our model can be written as:\nL = LTask + λLAdv + γLDiff (15)\nwhere λ and γ are hyper-parameter. The networks are trained with backpropagation and this minimax optimization becomes possible via the use of a gradient reversal layer (Ganin and Lempitsky, 2015)."
    }, {
      "heading" : "5 Experiment",
      "text" : ""
    }, {
      "heading" : "5.1 Dataset",
      "text" : "To make an extensive evaluation, we collect 16 different datasets from several popular review corpora. The first 14 datasets are product reviews, which contain Amazon product reviews from different domains, such as Books, DVDs, Electronics, ect. The goal is to classify a product review as either positive or negative. These datasets are collected based on the raw data 1 provided by (Blitzer et al., 2007). Specifically, we extract the sentences and corresponding labels from the unprocessed original data 2. The only preprocessing operation of these sentences is tokenized using the Stanford tokenizer 3. The remaining two datasets are about movie reviews. The IMDB dataset4 consists of movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. The MR dataset also consists of movie reviews from rotten tomato website with two classes 5(Pang and Lee, 2005). All the datasets in each task are partitioned randomly into training set, development set and testing set with the proportion of 70%, 20% and 10% respectively. The detailed statistics about all the datasets are listed in Table 1."
    }, {
      "heading" : "5.2 Competitor Methods for Multi-task Learning",
      "text" : "The multi-task frameworks proposed by previous works are various while not all can be applied to the tasks we focused. Nevertheless, we chose two most related neural models for multi-task learning and implement them as competitor methods.\n• MT-CNN: This model is proposed by Collobert and Weston (2008) with convolutional layer, in which lookup-tables are shared partially while other layers are task-specific.\n1https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/\n2Blitzer et al. (2007) also provides two extra processed datasets with the format of Bag-of-Words, which are not proper for neural-based models.\n3http://nlp.stanford.edu/software/ tokenizer.shtml\n4https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/unprocessed.tar.gz\n5https://www.cs.cornell.edu/people/ pabo/movie-review-data/.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nTask Single Task Multiple Tasks\nLSTM BiLSTM sLSTM Avg. MT-DNN MT-CNN FS-MTL SP-MTL ASP-MTL\nBooks 20.5 19.0 18.0 19.2 17.7(−1.5) 15.6(−3.6) 17.5(−1.7) 18.7(−0.5) 13.0(−6.2) Electronics 19.5 21.5 23.3 21.4 18.2(−3.2) 16.9(−4.5) 14.3(−7.1) 12.3(−9.1) 11.0(−10.4) DVD 18.3 19.5 22.0 19.9 15.8(−4.1) 16.1(−3.8) 16.5(−3.4) 16.1(−3.8) 12.6(−7.3) Kitchen 22.0 18.8 19.5 20.1 19.2(−0.9) 16.8(−3.3) 14.0(−6.1) 14.8(−5.3) 12.8(−7.3) Apparel 16.8 14.0 16.3 15.7 14.9(−0.8) 16.1(+0.4) 15.5(−0.2) 13.4(−2.3) 11.3(−4.4) Camera 14.8 14.0 15.0 14.6 13.7(−0.9) 14.0(−0.6) 13.5(−1.1) 12.1(−2.5) 8.7(−5.9) Health 15.5 21.3 16.5 17.8 14.3(−3.5) 12.9(−4.9) 12.0(−5.8) 12.8(−5.0) 10.9(−6.9) Music 23.3 22.8 23.0 23.0 15.3(−7.7) 16.3(−6.7) 18.8(−4.2) 17.0(−6.0) 17.4(−5.6) Toys 16.8 15.3 16.8 16.3 12.1(−4.2) 10.9(−5.4) 15.5(−0.8) 14.9(−1.4) 11.2(−5.1) Video 18.5 16.3 16.3 17.0 15.0(−2.0) 18.7(+1.7) 16.3(−0.7) 16.8(−0.2) 14.5(−2.5) Baby 15.3 16.5 15.8 15.9 12.1(−3.8) 12.4(−3.5) 12.0(−3.9) 13.2(−2.7) 10.2(−5.7) Magazines 10.8 8.5 12.3 10.5 10.6(+0.1) 12.3(+1.8) 7.5(−3.0) 8.1(−2.4) 7.6(−2.9) Software 15.3 14.3 14.5 14.7 14.4(−0.3) 13.4(−1.3) 13.8(−0.9) 13.1(−1.6) 12.7(−2.0) Sports 18.3 16.0 17.5 17.3 16.8(−0.5) 16.1(−1.2) 14.5(−2.8) 12.7(−4.6) 13.3(−4.0) IMDB 18.3 15.0 18.5 17.3 16.7(−0.6) 13.7(−3.6) 17.5(+0.2) 15.2(−2.1) 14.2(−3.1) MR 27.3 25.3 28.0 26.9 24.5(−2.4) 25.5(−1.4) 25.3(−1.6) 24.1(−2.8) 22.7(−4.2)\nAVG 18.2 17.4 18.3 18.0 15.7(−2.3) 15.5(−2.5) 15.3(−2.7) 14.7(−3.3) 12.8(−5.2)\nTable 2: Error rates of our models on 16 datasets against typical baselines. The numbers in brackets represent the improvements relative to the average performance (Avg.) of three single task baselines.\n• MT-DNN: The model is proposed by Liu et al. (2015) with bag-of-words input and multi-layer perceptrons, in which a hidden layer is shared."
    }, {
      "heading" : "5.3 Hyperparameters",
      "text" : "The word embeddings for all of the models are initialized with the 200d GloVe vectors (840B token version, (Pennington et al., 2014)). The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1]. The minibatch size is set to 16.\nFor each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [0.1, 0.01], λ ∈ [0.01, 0.1], and γ ∈ [0.01, 0.1]. Finally, we chose the learning rate as 0.01, λ as 0.05 and γ as 0.01."
    }, {
      "heading" : "5.4 Performance Evaluation",
      "text" : "Table 2 shows the error rates on 16 text classification tasks. The column of “Single Task” shows the results of vanilla LSTM, bidirectional LSTM (BiLSTM), stacked LSTM (sLSTM) and the average error rates of previous three models. The column of “Multiple Tasks” shows the results achieved by corresponding multi-task models. From this table, we can see that the performance of most tasks can be improved with a large margin with the help of multi-task learning, in which our model achieves the lowest error rates.\nMore concretely, compared with SP-MTL, ASPMTL achieves 5.2% average improvement surpassing SP-MTL with 1.9%, which indicates the importance of adversarial learning. It is noteworthy that for FS-MTL, the performances of some tasks are degraded, since this model puts all private and shared information into a unified space."
    }, {
      "heading" : "5.5 Shared Knowledge Transfer",
      "text" : "With the help of adversarial learning, the shared feature extractor Es can generate more pure taskinvariant representations, which can be considered as off-the-shelf knowledge and then be used for unseen new tasks. To test the transferability of our learned shared extractor, we also design an experiment, in which we take turns choosing 15 tasks to train our model MS with multi-task learning, then the learned shared layer are transferred to a second network MT that is used for the remaining one task. The parameters of transferred layer are kept frozen, and the rest of parameters of the network MT are randomly initialized. More formally, we investigate two mechanisms towards the transferred shared extractor. As shown in Figure 4. The first one Single Channel (SC) model consists of one shared feature extractor Es from MS , then the extracted representation will be sent to an output layer. By contrast, the BiChannel (BC) model introduces an extra LSTM layer to encode more task-specific information. To evaluate the effectiveness of our introduced adver-\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nSource Tasks Single Task Transfer Models\nLSTM BiLSTM sLSTM Avg. SP-MTL-SC SP-MTL-BC ASP-MTL-SC ASP-MTL-BC\nϕ (Books) 20.5 19.0 18.0 19.2 17.8(−3.6) 16.2(−3.0) 16.7(−2.5) 13.3(−5.9) ϕ (Electronics) 19.5 21.5 23.3 21.4 15.1(−4.5) 14.6(−6.8) 15.1(−6.3) 14.9(−6.5) ϕ (DVD) 18.3 19.5 22.0 19.9 14.7(−3.8) 15.5(−4.4) 12.1(−7.8) 12.4(−7.5) ϕ (Kitchen) 22.0 18.8 19.5 20.1 15.0(−3.3) 16.6(−3.5) 14.6(−5.5) 14.1(−6.0) ϕ (Apparel) 16.8 14.0 16.3 15.7 14.9(+0.4) 12.3(−3.4) 11.6(−4.1) 13.6(−2.1) ϕ (Camera) 14.8 14.0 15.0 14.6 13.1(−0.6) 12.1(−2.5) 11.6(−3.0) 10.3(−4.3) ϕ (Health) 15.5 21.3 16.5 17.8 14.1(−4.9) 14.2(−3.6) 12.2(−5.6) 10.5(−7.3) ϕ (Music) 23.3 22.8 23.0 23.0 19.9(−6.7) 17.9(−5.1) 16.4(−6.6) 18.2(−4.8) ϕ (Toys) 16.8 15.3 16.8 16.3 13.8(−5.4) 12.2(−4.1) 13.0(−4.7) 11.2(−5.1) ϕ (Video) 18.5 16.3 16.3 17.0 14.2(+1.7) 15.1(−1.9) 14.8(−2.2) 14.8(−2.2) ϕ (Baby) 15.3 16.5 15.8 15.9 16.6(−3.5) 16.9(+1.0) 11.5(−4.4) 10.0(−5.9) ϕ (Magazines) 10.8 8.5 12.3 10.5 10.6(+1.8) 10.2(−0.3) 8.6(−1.9) 9.7(−0.8) ϕ (Software) 15.3 14.3 14.5 14.7 13.0(−1.3) 12.7(−2.0) 14.3(−0.4) 11.1(−3.6) ϕ (Sports) 18.3 16.0 17.5 17.3 16.3(−1.2) 16.2(−1.1) 13.4(−3.9) 13.6(−3.7) ϕ (IMDB) 18.3 15.0 18.5 17.3 12.4(−3.6) 12.8(−4.5) 12.5(−4.8) 13.3(−4.0) ϕ (MR) 27.3 25.3 28.0 26.9 26.0(−1.4) 26.5(−0.4) 22.7(−4.2) 23.5(−3.4)\nAVG 18.2 17.4 18.3 18.0 15.5(−2.5) 15.1(−2.9) 13.6(−4.2) 13.4(−4.6)\nTable 3: Error rates of our models on 16 datasets against vanilla multi-task learning. ϕ (Books) means that we transfer the knowledge of the other 15 tasks to the target task Books.\nxt LSTM softmax\nEs\n(a) Single Channel\nxt LSTM\nLSTM\nsoftmax\nEs\n(b) Bi-Channel\nFigure 4: Two transfer strategies using a pretrained shared LSTM layer. Yellow box denotes shared feature extractor Es trained by 15 tasks.\nsarial training framework, we also make a comparison with vanilla multi-task learning method.\nResults and Analysis As shown in Table 3, we can see the shared layer from ASP-MTL achieves a better performance compared with SP-MTL. Besides, for the two kinds of transfer strategies, the Bi-Channel model performs better. The reason is that the task-specific layer introduced in the BiChannel model can store some private features. Overall, the results indicate that we can save the existing knowledge into a shared recurrent layer using adversarial multi-task learning, which is quite useful for a new task."
    }, {
      "heading" : "5.6 Visualization",
      "text" : "To get an intuitive understanding of how the introduced orthogonality constraints worked compared with vanilla shared-private model, we design an experiment to examine the behaviors of neurons from private layer and shared layer. More concretely, we refer to htj as the activation of the j-\nneuron at time step t, where t ∈ {1, . . . , n} and j ∈ {1, . . . , d}. By visualizing the hidden state hj and analyzing the maximum activation, we can find what kinds of patterns the current neuron focuses on. Figure 5 illustrates this phenomenon. Here, we randomly sample a sentence from the validation set of Baby task and analyze the changes of the predicted sentiment score at different time steps, which are obtained by SP-MTL and our proposed model. Additionally, to get more insights into how neurons in shared layer behave diversely towards different input word, we visualize the activation of two typical neurons. For the positive sentence “Five stars, my baby can fall asleep soon in the stroller”, both models capture the informative pattern “Five stars” 6. However, SP-MTL makes a wrong prediction due to misunderstanding of the word “asleep”. By contrast, our model makes a correct prediction and the reason can be inferred from the activation of Figure 5-(b), where the shared layer of SP-MTL is so sensitive that many features related to other tasks are included, such as ”asleep”, which misleads the final prediction. This indicates the importance of introducing adversarial learning to prevent the shared layer from being contaminated by task-specific features.\n6For this case, the vanilla LSTM also give a wrong answer due to ignoring the feature “Five stars”.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nFive stars , my baby can fall asleep soon in the stroller 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 SP-MTL Ours\n(a) Predicted Sentiment Score by Two Models (b) Behaviours of Neuron hs18 and h s 21\nFigure 5: (a) The change of the predicted sentiment score at different time steps. Y-axis represents the sentiment score, while X-axis represents the input words in chronological order. The darker grey horizontal line gives a border between the positive and negative sentiments. (b) The blue heat map describes the behaviour of neuron hs18 from shared layer of SP-MTL, while the purple one is used to show the behaviour of neuron hs21, which belongs to the shared layer of our model.\nModel Shared Layer Task-Movie Task-Baby\nSP-MTL\ngood, great bad, love, simple, cut, slow, cheap, infantile good, great, well-directed, pointless, cut, cheap, infantile love, bad, cute, safety, mild, broken simple\nASP-MTL good, great, love, bad poor well-directed, pointless, cut, cheap, infantile cute, safety, mild, broken simple\nTable 4: Typical patterns captured by shared layer and task-specific layer of SP-MTL and ASP-MTL models on Movie and Baby tasks.\nWe also list some typical patterns captured by neurons from shared layer and task-specific layer in Table 4, and we have observed that: 1) for SP-MTL, if some patterns are captured by taskspecific layer, they are likely to be placed into shared space. Clearly, suppose we have many tasks to be trained jointly, the shared layer bear much pressure and must sacrifice substantial amount of capacity to capture the patterns they actually do not need. Furthermore, some typical taskinvariant features also go into task-specific layer. 2) for ASP-MTL, we find the features captured by shared and task-specific layer have a small amount of intersection, which allows these two kinds of layers can work effectively."
    }, {
      "heading" : "6 Related Work",
      "text" : "There are two threads of related work. One thread is multi-task learning with neural network. Neural networks based multi-task learning has been proven effective in many NLP problems (Collobert and Weston, 2008; Glorot et al., 2011; Liu et al., 2015, 2016). In most of these models, the\nlower layers are shared across all tasks, while top layers are task-specific. These work has potential limitation of just learning a shared space solely on sharing parameters, while our model introduce two strategies to learn the clear and non-redundant shared-private space. Another thread of work is adversarial network. Adversarial networks have recently surfaced as a general tool measure equivalence between distributions and it has proven to be effective in a variety of tasks. Ajakan et al. (2014); Bousmalis et al. (2016) applied adverarial training to domain adaptation, aiming at transferring the knowledge of one source domain to target domain. Park and Im (2016) proposed a novel approach for multimodal representation learning which uses adversarial back-propagation concept. Different from these models, our model aims to find task-invariant sharable information for multiple related tasks using adversarial training strategy. Moreover, we extend binary adversarial training to multi-class, which enable multiple tasks to be jointly trained."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we have proposed an adversarial multi-task learning framework, in which the taskspecific and task-invariant features are learned non-redundantly, therefore capturing the sharedprivate separation of different tasks. We have demonstrated the effectiveness of our approach by applying our model to 16 different text classification tasks. We also perform extensive qualitative analysis, deriving insights and indirectly explaining the quantitative improvements in the overall performance.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Domain-adversarial neural networks",
      "author" : [ "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "François Laviolette", "Mario Marchand." ],
      "venue" : "arXiv preprint arXiv:1412.4446 .",
      "citeRegEx" : "Ajakan et al\\.,? 2014",
      "shortCiteRegEx" : "Ajakan et al\\.",
      "year" : 2014
    }, {
      "title" : "A theory of learning from different domains",
      "author" : [ "Shai Ben-David", "John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Wortman Vaughan." ],
      "venue" : "Machine learning 79(1-2):151–175.",
      "citeRegEx" : "Ben.David et al\\.,? 2010",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 2010
    }, {
      "title" : "Analysis of representations for domain adaptation. Advances in neural information processing systems 19:137",
      "author" : [ "Shai Ben-David", "John Blitzer", "Koby Crammer", "Fernando Pereira" ],
      "venue" : null,
      "citeRegEx" : "Ben.David et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 2007
    }, {
      "title" : "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification",
      "author" : [ "John Blitzer", "Mark Dredze", "Fernando Pereira" ],
      "venue" : "In ACL",
      "citeRegEx" : "Blitzer et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Blitzer et al\\.",
      "year" : 2007
    }, {
      "title" : "Domain separation networks",
      "author" : [ "Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 343– 351.",
      "citeRegEx" : "Bousmalis et al\\.,? 2016",
      "shortCiteRegEx" : "Bousmalis et al\\.",
      "year" : 2016
    }, {
      "title" : "Adversarial deep averaging networks for cross-lingual sentiment classification",
      "author" : [ "Xilun Chen", "Yu Sun", "Ben Athiwaratkun", "Claire Cardie", "Kilian Weinberger." ],
      "venue" : "arXiv preprint arXiv:1606.01614 .",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1412.3555 .",
      "citeRegEx" : "Chung et al\\.,? 2014",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "Ronan Collobert", "Jason Weston." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Collobert and Weston.,? 2008",
      "shortCiteRegEx" : "Collobert and Weston.",
      "year" : 2008
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "The Journal of Machine Learning Research 12:2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Jeffrey L Elman." ],
      "venue" : "Cognitive science 14(2):179–211.",
      "citeRegEx" : "Elman.,? 1990",
      "shortCiteRegEx" : "Elman.",
      "year" : 1990
    }, {
      "title" : "Unsupervised domain adaptation by backpropagation",
      "author" : [ "Yaroslav Ganin", "Victor Lempitsky." ],
      "venue" : "Proceedings of the 32nd International Conference on Machine Learning (ICML-15). pages 1180–1189.",
      "citeRegEx" : "Ganin and Lempitsky.,? 2015",
      "shortCiteRegEx" : "Ganin and Lempitsky.",
      "year" : 2015
    }, {
      "title" : "Domain adaptation for large-scale sentiment classification: A deep learning approach",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 513–520.",
      "citeRegEx" : "Glorot et al\\.,? 2011",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 2672–2680.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves." ],
      "venue" : "arXiv preprint arXiv:1308.0850 .",
      "citeRegEx" : "Graves.,? 2013",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Factorized latent spaces with structured sparsity",
      "author" : [ "Yangqing Jia", "Mathieu Salzmann", "Trevor Darrell." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 982–990.",
      "citeRegEx" : "Jia et al\\.,? 2010",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2010
    }, {
      "title" : "An empirical exploration of recurrent network architectures",
      "author" : [ "Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever." ],
      "venue" : "Proceedings of The 32nd International Conference on Machine Learning.",
      "citeRegEx" : "Jozefowicz et al\\.,? 2015",
      "shortCiteRegEx" : "Jozefowicz et al\\.",
      "year" : 2015
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Kalchbrenner et al\\.,? 2014",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent neural network for text classification with multi-task learning",
      "author" : [ "Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "Proceedings of International Joint Conference on Artificial Intelligence. https://arxiv.org/abs/1605.05101.",
      "citeRegEx" : "Liu et al\\.,? 2016",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Representation learning using multi-task deep neural networks for semantic classification and information retrieval",
      "author" : [ "Xiaodong Liu", "Jianfeng Gao", "Xiaodong He", "Li Deng", "Kevin Duh", "Ye-Yi Wang." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Multi-task sequence to sequence learning",
      "author" : [ "Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser." ],
      "venue" : "arXiv preprint arXiv:1511.06114 .",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the ACL. pages 142–150.",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Cross-stitch networks for multi-task learning",
      "author" : [ "Ishan Misra", "Abhinav Shrivastava", "Abhinav Gupta", "Martial Hebert." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pages 3994–4003.",
      "citeRegEx" : "Misra et al\\.,? 2016",
      "shortCiteRegEx" : "Misra et al\\.",
      "year" : 2016
    }, {
      "title" : "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Proceedings of the 43rd annual meeting on association for computational linguistics. Association for Computational",
      "citeRegEx" : "Pang and Lee.,? 2005",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2005
    }, {
      "title" : "Image-text multi-modal representation learning by adversarial backpropagation",
      "author" : [ "Gwangbeen Park", "Woobin Im." ],
      "venue" : "arXiv preprint arXiv:1612.08354 .",
      "citeRegEx" : "Park and Im.,? 2016",
      "shortCiteRegEx" : "Park and Im.",
      "year" : 2016
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) 12:1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Factorized orthogonal latent spaces",
      "author" : [ "Mathieu Salzmann", "Carl Henrik Ek", "Raquel Urtasun", "Trevor Darrell." ],
      "venue" : "AISTATS. pages 701–708.",
      "citeRegEx" : "Salzmann et al\\.,? 2010",
      "shortCiteRegEx" : "Salzmann et al\\.",
      "year" : 2010
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Unsupervised cross-domain image generation",
      "author" : [ "Yaniv Taigman", "Adam Polyak", "Lior Wolf." ],
      "venue" : "arXiv preprint arXiv:1611.02200 .",
      "citeRegEx" : "Taigman et al\\.,? 2016",
      "shortCiteRegEx" : "Taigman et al\\.",
      "year" : 2016
    }, {
      "title" : "Facial landmark detection by deep multi-task learning",
      "author" : [ "Zhanpeng Zhang", "Ping Luo", "Chen Change Loy", "Xiaoou Tang." ],
      "venue" : "European Conference on Computer Vision. Springer, pages 94–108.",
      "citeRegEx" : "Zhang et al\\.,? 2014",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "Recently, neuralbased models for multi-task learning have become very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural language processing (Collobert and Weston, 2008; Luong et al.",
      "startOffset" : 108,
      "endOffset" : 148
    }, {
      "referenceID" : 30,
      "context" : "Recently, neuralbased models for multi-task learning have become very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural language processing (Collobert and Weston, 2008; Luong et al.",
      "startOffset" : 108,
      "endOffset" : 148
    }, {
      "referenceID" : 7,
      "context" : ", 2014) to natural language processing (Collobert and Weston, 2008; Luong et al., 2015), since they provide a convenient way of combining information frommultiple tasks.",
      "startOffset" : 39,
      "endOffset" : 87
    }, {
      "referenceID" : 20,
      "context" : ", 2014) to natural language processing (Collobert and Weston, 2008; Luong et al., 2015), since they provide a convenient way of combining information frommultiple tasks.",
      "startOffset" : 39,
      "endOffset" : 87
    }, {
      "referenceID" : 28,
      "context" : "There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014), convolutional neural networks (Collobert et al.",
      "startOffset" : 113,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014), convolutional neural networks (Collobert et al.",
      "startOffset" : 113,
      "endOffset" : 157
    }, {
      "referenceID" : 8,
      "context" : ", 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al.",
      "startOffset" : 39,
      "endOffset" : 90
    }, {
      "referenceID" : 17,
      "context" : ", 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al.",
      "startOffset" : 39,
      "endOffset" : 90
    }, {
      "referenceID" : 27,
      "context" : ", 2014), and recursive neural networks (Socher et al., 2013).",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 14,
      "context" : "Long Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) (Elman, 1990), and specifically addresses the issue of learning long-term dependencies.",
      "startOffset" : 61,
      "endOffset" : 95
    }, {
      "referenceID" : 9,
      "context" : "Long Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) (Elman, 1990), and specifically addresses the issue of learning long-term dependencies.",
      "startOffset" : 140,
      "endOffset" : 153
    }, {
      "referenceID" : 16,
      "context" : "While there are numerous LSTM variants, here we use the LSTM architecture used by (Jozefowicz et al., 2015), which is similar to the architecture of (Graves, 2013) but without peep-hole connections.",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : ", 2015), which is similar to the architecture of (Graves, 2013) but without peep-hole connections.",
      "startOffset" : 49,
      "endOffset" : 63
    }, {
      "referenceID" : 29,
      "context" : "While originally proposed for generating random samples, adversarial network can be used as a general tool to measure equivalence between distributions (Taigman et al., 2016).",
      "startOffset" : 152,
      "endOffset" : 174
    }, {
      "referenceID" : 0,
      "context" : "Formally, (Ajakan et al., 2014) linked the adversarial loss to the H-divergence between two distributions and successfully achieve unsupervised domain adaptation with adversarial network.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 4,
      "context" : "Motivated by theory on domain adaptation (Ben-David et al., 2010, 2007; Bousmalis et al., 2016) that a transferable feature is one for which an algorithm cannot learn to identify the domain of origin of the input observation.",
      "startOffset" : 41,
      "endOffset" : 95
    }, {
      "referenceID" : 12,
      "context" : "Inspired by adversarial networks (Goodfellow et al., 2014), we proposed an adversarial sharedprivate model for multi-task learning, in which a shared recurrent neural layer is working adversarially towards a learnable multi-layer perceptron, preventing it from making an accurate prediction about the types of tasks.",
      "startOffset" : 33,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : "Motivated by recently work(Jia et al., 2010; Salzmann et al., 2010; Bousmalis et al., 2016)",
      "startOffset" : 26,
      "endOffset" : 91
    }, {
      "referenceID" : 26,
      "context" : "Motivated by recently work(Jia et al., 2010; Salzmann et al., 2010; Bousmalis et al., 2016)",
      "startOffset" : 26,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "Motivated by recently work(Jia et al., 2010; Salzmann et al., 2010; Bousmalis et al., 2016)",
      "startOffset" : 26,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "After exploring many optional methods, we find below loss is optimal, which is used by Bousmalis et al. (2016) and achieve a better performance:",
      "startOffset" : 87,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "The networks are trained with backpropagation and this minimax optimization becomes possible via the use of a gradient reversal layer (Ganin and Lempitsky, 2015).",
      "startOffset" : 134,
      "endOffset" : 161
    }, {
      "referenceID" : 3,
      "context" : "These datasets are collected based on the raw data 1 provided by (Blitzer et al., 2007).",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 21,
      "context" : "The IMDB dataset4 consists of movie reviews with binary classes (Maas et al., 2011).",
      "startOffset" : 64,
      "endOffset" : 83
    }, {
      "referenceID" : 23,
      "context" : "The MR dataset also consists of movie reviews from rotten tomato website with two classes 5(Pang and Lee, 2005).",
      "startOffset" : 91,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "• MT-CNN: This model is proposed by Collobert and Weston (2008) with convolutional layer, in which lookup-tables are shared partially while other layers are task-specific.",
      "startOffset" : 36,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "edu/ ̃mdredze/ datasets/sentiment/ Blitzer et al. (2007) also provides two extra processed datasets with the format of Bag-of-Words, which are not proper for neural-based models.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 18,
      "context" : "• MT-DNN: The model is proposed by Liu et al. (2015) with bag-of-words input and multi-layer perceptrons, in which a hidden layer is shared.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 25,
      "context" : "3 Hyperparameters The word embeddings for all of the models are initialized with the 200d GloVe vectors (840B token version, (Pennington et al., 2014)).",
      "startOffset" : 125,
      "endOffset" : 150
    }, {
      "referenceID" : 7,
      "context" : "Neural networks based multi-task learning has been proven effective in many NLP problems (Collobert and Weston, 2008; Glorot et al., 2011; Liu et al., 2015, 2016).",
      "startOffset" : 89,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "Neural networks based multi-task learning has been proven effective in many NLP problems (Collobert and Weston, 2008; Glorot et al., 2011; Liu et al., 2015, 2016).",
      "startOffset" : 89,
      "endOffset" : 162
    }, {
      "referenceID" : 0,
      "context" : "Ajakan et al. (2014); Bousmalis et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "Ajakan et al. (2014); Bousmalis et al. (2016) applied adverarial training to domain adaptation, aiming at transferring the knowledge of one source domain to target domain.",
      "startOffset" : 0,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "Ajakan et al. (2014); Bousmalis et al. (2016) applied adverarial training to domain adaptation, aiming at transferring the knowledge of one source domain to target domain. Park and Im (2016) proposed a novel approach for multimodal representation learning which uses adversarial back-propagation concept.",
      "startOffset" : 0,
      "endOffset" : 191
    } ],
    "year" : 2017,
    "abstractText" : "Neural network models have shown their promising opportunities for multi-task learning, which focus on learning the shared layers to extract the common and task-invariant features. However, in most existing approaches, the extracted shared features are prone to be contaminated by task-specific features or the noise brought by other tasks. In this paper, we propose an adversarial multi-task learning framework, alleviating the shared and private latent feature spaces from interfering with each other. We conduct extensive experiments on 16 different text classification tasks, which demonstrates the benefits of our approach. Besides, we show that the shared knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}