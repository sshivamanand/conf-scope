{
  "name" : "741.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Automatic Induction of Synsets from a Graph of Synonyms",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "A synset is a set of mutual synonyms, which can be represented as a clique where nodes are words and edges are synonymy relations. Synsets represent word senses and are building blocks of WordNet (Miller, 1995) and similar resources as thesauri and lexical ontologies. These resources are crucial for many natural language processing applications which require common sense reasoning, such as information retrieval (Gong et al., 2005) and question answering (Kwok et al., 2001; Zhou et al., 2013). However, for most languages no manually-constructed resource comparable to the English WordNet in terms of coverage and quality is available. For instance, Kiselev et al. (2015) presents a comparative analysis of lexical resources available for the Russian language. This\nlack of linguistic resources for many languages urges the development of new methods for automatic construction of WordNet-like resources.\nWikipedia1, Wiktionary2, OmegaWiki3 and other collaboratively-created resources contain a large amount of lexical semantic information— yet designed to be human-readable and not formally structured. While semantic relations can be automatically extracted using such tools as DKPro JWKTL4 and Wikokit5, words in these relations are not disambiguated. For instance, the synonymy pairs (bank, streambank) and (bank, banking company) will be connected via the word “bank”, while they refer to the different senses. This problem stems from the fact that articles in Wiktionary and similar resources list undisambiguated synonyms. They are easy to disambiguate for humans while reading a dictionary article, but can be a source of errors for a language processing system.\nThe contribution of this paper is a novel approach which resolves ambiguities of the input graph enabling fuzzy clustering. The method takes as an input synonymy relations between potentially ambiguous terms available in humanreadable dictionaries and transforms them into a disambiguated machine readable representation in the form of synsets. Our method, called WATSET, is based on a new meta-algorithm for fuzzy graph clustering. The name choice reflects the underlying principle: discover the correct word senses (“wat”) and then construct the synsets (“set”).\nIn contrast to the projects like BabelNet (Navigli and Ponzetto, 2012) and UBY (Gurevych et al., 2012), which rely on English WordNet as\n1http://www.wikipedia.org 2http://www.wiktionary.org 3http://www.omegawiki.org 4https://dkpro.github.io/dkpro-jwktl 5https://github.com/componavt/wikokit\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\na pivot for mapping of existing resources, our approach requires no such pivot lexical ontologies. Besides, it outperforms analogous state-of-the-art methods for synset induction. An implementation of the WATSET method is available online, along with induced lexical resources.6"
    }, {
      "heading" : "2 Related Work",
      "text" : "Methods based on resource mapping, such as BabelNet and UBY, gather various existing lexical resources across multiple languages and perform their linking to obtain a machine-readable repository of lexical semantic knowledge. In its core, BabelNet was obtained by mapping the Princeton WordNet and Wikipedia enhanced by the machine translation of the results. Later, other resources were mapped to this core, including Wiktionary and OmegaWiki. UBY has a similar architecture, but relies on similarity of dictionary definitions and existing cross-lingual links for mapping.\nThe potential advantages of our approach as compared to BabelNet/UBY is the (1) absence of the error-prone procedures of mapping and machine translation; (2) possibility to model the target language more accurately as senses of the same word in different languages may be different; (3) no need for a pivot English resource.\nA related branch of methods deals with coarsification of sense inventories of fine-grained lexical resources, such as (Snow et al., 2007).\nMethods based on word sense induction try to induce sense representations without the need for any initial lexical resource by extracting semantic relations from text. In particular, word sense induction (WSI) based on word ego networks clusters graphs of words semantically related to the ambiguous word (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Véronis, 2004; Hope and Keller, 2013). Each cluster corresponds to a word sense. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters. In the case of WSI, such a network is a local neighbourhood of one word. Nodes of the ego network are the words which are semantically similar to the target word.\nSuch approaches are able to discover homonymous senses of words, e.g., “bank” as slope versus “bank” as organisation (Di Marco and Navigli, 2012). However, as the graphs are composed\n6scheme://domain.tld/anonymized\nof semantically related words obtained using distributional methods (Baroni and Lenci, 2010; Biemann and Riedl, 2013), the resulting clusters by no means can be considered synsets. Namely, (1) they contain words related not only via synonymy relation, but via a mixture of relations such as synonymy, hypernymy, co-hyponymy, antonymy, etc. (Heylen et al., 2008); (2) clusters are not unique, i.e., one word can occur in several clusters referring to the same sense, while in WordNet a word used in the given sense occurs only in a single synset.\nIn our approach, to induce synsets, we use word ego network clustering similarly as in word sense induction approaches, but apply them to the graph of semantically clean synonyms.\nMethods based on clustering of synonyms, such as our approach, induce the resource from an ambiguous graph of synonyms where edges were extracted from manually-created resources. According to the best of our knowledge, most experiments either used graph-based word sense induction applied to text-derived graphs or used a mapping-based method which already assumes availability of a WordNet-like resource. A notable exception is the ECO approach by Gonçalo Oliveira and Gomes (2014), which was used to induce a WordNet of the Portuguese language called Onto.PT.7 We use this approach and five other state-of-the-art graph clustering algorithms as the baselines:\n• ECO (Gonçalo Oliveira and Gomes, 2014) is a fuzzy clustering algorithm that was used to induce synsets of a Portuguese WordNet out of available synonymy dictionaries. The algorithm starts by adding random noise to edge weights. Then, the approach launches Markov Clustering of this graph several times to estimate the probability of each word pair being in the same synset. Finally, these candidate pairs passing through the θ threshold are added to output synsets.\n• MaxMax (Hope and Keller, 2013) is a fuzzy clustering algorithm designed initially for the word sense induction task. In the nutshell, pairs of nodes are grouped if either node has a maximal affinity to the other. The algorithm starts by converting the input undirected graph into the directed graph by keep-\n7http://ontopt.dei.uc.pt\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nFigure 1: Outline of the WATSET method for synset induction.\ning maximal nodes of each node. Next, all nodes are marked as root nodes. Finally, for each root node, the following procedure is repeated: all transitive children of this root form a cluster and marked as non-root nodes; a root node together with all its transitive children form a fuzzy cluster.\n• Markov Clustering (MCL) (van Dongen, 2000) is a hard clustering algorithm for graphs based on simulation of stochastic flow in graphs. MCL simulates random walks within a graph by alternation of two operators called expansion and inflation, which recomputes the class labels. Notably, it has been successfully used for the word sense induction task (Dorow and Widdows, 2003).\n• Chinese Whispers (Biemann, 2006) is a hard clustering algorithm for weighted graphs that can be considered as a special case of MCL with a simplified class update step. At each iteration, the labels of all the nodes are updated according to the majority labels among the neighbouring nodes. The algorithm has a meta-parameter that controls graph weights that can be set to three values: (1) top sums over the neighbourhood’s classes; (2) nolog downgrades the influence of a neighbouring node by its degree or (3) log of its degree.\n• Clique Percolation Method (CPM) (Palla et al., 2005) is a fuzzy clustering algorithm for unweighted graphs that builds up the clusters from k-cliques corresponding to fully connected sub-graphs of k nodes. While this method is only commonly used in social network analysis, we decided to add it to the comparison as synsets are essentially cliques of synonyms, and it is natural to try using an algorithm based on detection of cliques."
    }, {
      "heading" : "3 The WATSET Method",
      "text" : "The goal of the method is to induce a set of unambiguous synsets by grouping individual ambigu-\nous synonyms. An outline of the proposed approach is depicted in Figure 1. The method takes a dictionary of ambiguous synonymy relations and a text corpus as an input and outputs synsets. Note that the method can be used without a background corpus, yet as our experiments will show, corpusbased information improves the results, when utilizing it for weighting the word graph.\nA synonymy dictionary can be perceived as a graph, where the nodes correspond to lexical entries (words) and the edges connect pairs of the nodes when the synonymy relation between them holds. The cliques in such a graph naturally form densely connected sets of synonyms corresponding to concepts (Kamps et al., 2004). Given the fact that the clique problem in a graph is NPcomplete (Bomze et al., 1999), an efficient graph clustering algorithm like the MCL algorithm can be used for finding a global segmentation of the graph. However, the hard clustering property of this algorithm does not handle polysemy: one word can have several senses but will be assigned to only one cluster. To deal with this limitation, a word sense induction procedure is used to induce senses for all words. Finally, the disambiguated word sense graph is clustered globally to induce the synsets from this disambiguated word graph.\nMore specifically, the method consists of five steps presented in Figure 1: (1) learning word embeddings; (2) constructing the ambiguous weighted graph of synonyms G; (3) inducing the word senses; (4) constructing the disambiguated weighted graph G′ by disambiguating of neighbours w.r.t. the induced word senses; (5) global clustering of the graph G′."
    }, {
      "heading" : "3.1 Learning Word Embeddings",
      "text" : "Since the different graph clustering algorithms are sensitive to edge weighing, we consider the distributional semantic similarity measures based on word embeddings as a possible edge weighing approach for our synonymy graph. As we show further, this approach yields the best results.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399"
    }, {
      "heading" : "3.2 Construction of a Synonymy Graph",
      "text" : "We construct the synonymy graph G = (V,E) as follows. The set of nodes V includes every lexeme appearing in the input synonymy dictionaries. The set of undirected edgesE is composed of all edges (u, v) ∈ V × V retrieved from one of the input synonymy dictionaries. We consider three edge weight representations:\n• ones that assigns every edge the constant weight of 1;\n• count that weights the edge (u, v) as the number of times the synonymy pair appeared in the input dictionaries;\n• sim that assigns every edge (u, v) a weight equal to the cosine similarity of the skip-gram word vectors.\nAs the graph G is likely to have polysemous words, the goal is to separate the individual word senses using graph-based word sense induction."
    }, {
      "heading" : "3.3 Word Sense Induction",
      "text" : "We use a graph-based word sense induction method that is similar to the curvature-based approach of Dorow and Widdows (2003). In particular, removal of the nodes participating in many triangles tends to separate the original graph into several connected components. Thus, given a word u, we extract a network of its nearest neighbours from the synonymy graph G. Then, we remove the original word u from this network and run a hard graph clustering algorithm that assigns one node to one and only one cluster. In our experiments, we test Chinese Whispers and Markov Clustering. As the result, each cluster is hopefully representing a different sense of the word u, e.g.:\nbank1 {streambank, riverbank, . . . } bank2 {bank company, . . . } bank3 {bank building, building, . . . } bank4 {coin bank, penny bank, . . . }\nWe denote man1, man2 and other items as word senses referred to as, e.g., senses(man). We denote as ctx(s) a cluster corresponding to the word sense s. Note that the context words have no sense labels. They are recovered by the disambiguation approach described below."
    }, {
      "heading" : "3.4 Disambiguation of Neighbours",
      "text" : "The result of the previous step is splitting word nodes into (one or more) sense nodes. However,\nnearest neighbours of each new node are still ambiguous, e.g., (bank3, building?).\nFor recovering these sense labels of the neighbouring words, we employ the following sense disambiguation approach. For each word u in the context ctx(s) of the sense s, we estimate the most similar sense of that word û using the cosine similarity measure between the context of the sense s and the context of the candidate sense u′ in a vector space model:\nû = argmax u′∈ senses(u)\ncos(ctx(s), ctx(u′)).\nThis approach makes it possible to construct a disambiguated context ĉtx(s) that corresponds to the certain word senses appearing in ctx(s):\nĉtx(s) = {û : u ∈ ctx(s)}."
    }, {
      "heading" : "3.5 Global Clustering",
      "text" : "Finally, we construct the word sense graph G′ = (V ′, E′) using the disambiguated senses instead of the regular words and establishing the edges between these disambiguated senses:\nV ′ = {s : u ∈ V ∧ s ∈ senses(u)}, E′ = {(s, û) : s ∈ V ′ ∧ û ∈ ĉtx(s)}.\nAs the result, running a hard clustering algorithm on G′ produces the desired set of synsets.\nFigure 2 illustrates the process of disambiguation of an input ambiguous graph on the example of the word “bank”. As one may observe, disambiguation of the nearest neighbours is a necessity to be able to construct a global version of the sense-aware graph. Note that current approaches to WSI, e.g., (Véronis, 2004; Biemann, 2006; Hope and Keller, 2013), do not perform this step, but perform only local analysis of the graph."
    }, {
      "heading" : "4 Evaluation",
      "text" : "We conduct our experiments on the data for two different languages. We evaluate our approach on two datasets for English to demonstrate its performance on a resource-rich language. Additionally, we evaluate it on two Russian datasets since Russian is a good example of an under-resourced language with a clear need for synset induction."
    }, {
      "heading" : "4.1 Gold Standard Datasets",
      "text" : "For each language, we used two differently constructed lexical semantic resources listed in Table 1 to obtain gold standard synsets.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nAmbiguous Graph before Local Clustering (WSI)\nbank?\nstreambank?\nriverbank?\nstreamside?\nbuilding?\nbank building?\nSense Inventory of Nodes, Undisambiguated Neighbours\nbank2 building?\nbank building?\nstreamside1\nbank1\nriverbank?\nstreambank?\nDisambiguated Graph before Global Clustering\nbank2 building2\nbank building1\nbank1 streambank3\nriverbank2 streamside1\nFigure 2: Disambiguation of an input ambiguous graph using local clustering (WSI) to facilitate global clustering of words into synsets.\nEnglish. We use WordNet8 (Miller, 1995), a popular English lexical database crafted by a group of expert lexicographers. WordNet contains general vocabulary and appears to be de facto gold standard in similar tasks. We used WordNet 3.1 to derive the synonymy pairs from the synsets for the evaluation. Also, we use BabelNet9 (Navigli and Ponzetto, 2012), a large-scale multilingual semantic network constructed automatically using WordNet, Wikipedia and other resources. We retrieved all the synonymy pairs from the BabelNet 3.7 synsets that were marked as English. We also considered using TWSI by Biemann (2013), but found that it contains mostly hypernymy and co-hyponymy relations instead of being a viable source of synonymy information.\nRussian. We use RuThes10 (Loukachevitch, 2011), a lexical ontology for Russian containing both general vocabulary and domain-specific synsets related to sport, finance, economics, etc. Up to a half of the words in this resource are multiword expressions (Kiselev et al., 2015), which is due to the coverage of domain-specific vocabulary. RuThes is constructed in the traditional way, namely by a small group of expert lexicog-\n8https://wordnet.princeton.edu/ 9http://babelnet.org/\n10http://www.labinform.ru/pub/ruthes\nraphers. Also, we use Yet Another RussNet11 (YARN) by Braslavski et al. (2016) as yet another gold standard for Russian. The resource is constructed using crowdsourcing and mostly covers general vocabulary. Particularly, non-expert users are allowed to edit synsets in a collaborative way loosely supervised by a team of project curators. Due to the ongoing development of the resource, we selected as the gold standard only those synsets that were edited at least eight times in order to filter out noisy incomplete synsets.\nResource # words # synsets # synonyms WordNet En 148 730 117 659 152 254 BabelNet En 11 710 137 6 667 855 28 822 400 RuThes Ru 119 836 31 528 474 537 YARN Ru 9 141 2 210 48 291\nTable 1: Statistics of the gold standard datasets."
    }, {
      "heading" : "4.2 Evaluation Metrics",
      "text" : "To evaluate the quality of the induced synsets, we transformed them into binary synonymy relations and computed precision, recall, and F-score on the basis of the overlap of these binary relations with the binary relations from the gold standard datasets. Given a synset uniting n words, we generate a set of n(n−1)2 pairs of synonyms."
    }, {
      "heading" : "4.3 Word Embeddings",
      "text" : "English. We use the standard 300-dimensional word embeddings trained on the 100 billion tokens Google News corpus (Mikolov et al., 2013).\nRussian. We use the 500-dimensional word embeddings trained using the skip-gram model with negative sampling (Mikolov et al., 2013) using a context window size of 10 with the minimal word frequency of 5 on a 12.9 billion tokens corpus of books lib.rus.ec. These embeddings were shown to produce state-of-the-art results for Russian in the RUSSE shared task12 and are part of the Russian Distributional Thesaurus13 (RDT)."
    }, {
      "heading" : "4.4 Input Dictionary of Synonyms",
      "text" : "For each language, we constructed a synonymy graph using openly available language resources. The statistics of the graphs used as the input in the further experiments are shown in Table 2.\n11https://russianword.net/en/ 12http://www.dialog-21.ru/en/\nevaluation/2015/semantic_similarity 13http://russe.nlpub.ru/downloads\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nCW MCL MaxMax ECO CPM Watset\n0.0\n0.1\n0.2\n0.3\nWordNet (English)\nF −\ns c o re\nCW MCL MaxMax ECO CPM Watset\n0.00\n0.05\n0.10\n0.15\nRuThes (Russian)\nF −\ns c o re\nCW MCL MaxMax ECO CPM Watset\n0.0\n0.1\n0.2\n0.3\nBabelNet (English)\nF −\ns c o re\nCW MCL MaxMax ECO CPM Watset\n0.0\n0.1\n0.2\n0.3\n0.4\nYARN (Russian)\nF −\ns c o re\nFigure 3: Impact of the different graph weighting schemas on the performance of synset induction:\nones, count, sim. Here, each bar corresponds to the top performance of each method.\nEnglish. The synonyms were extracted from the English Wiktionary, which is the largest Wiktionary as the present moment according to the lexical coverage, using the DKPro JWKTL tool by Zesch et al. (2008). Only the words marked as English have been extracted.\nRussian. The synonyms from three sources were combined to improve lexical coverage of the input dictionary and to enforce confidence in jointly observed synonyms: (1) synonyms listed in the Russian Wiktionary extracted using the Wikokit tool by Krizhanovsky and Smirnov (2013); (2) the dictionary of Abramov (2007); and (3) the Universal Dictionary of Concepts (Dikonov, 2013). While these two latter resources are specific to Russian, Wiktionary is available for most languages. Note that the same input synonymy dictionary was used by authors of YARN to construct synsets using crowdsourcing. Therefore, results on the YARN dataset show how close an automatic synset induction method can approximate manually created synsets provided the same starting raw linguistic materials.\nLanguage # words # synonyms English 77 871 71 816 Russian 74 395 202 313\nTable 2: Statistics of the input datasets."
    }, {
      "heading" : "5 Results",
      "text" : "We compare WATSET with five state-of-the art graph clustering methods presented in Section 2: Chinese Whispers (CW), Markov Clustering (MCL), MaxMax, ECO clustering, and\nthe clique percolation method (CPM). In our experiments, we rely on our own implementation of MaxMax and ECO as reference implementations are not available. For CW14, MCL15 and CPM16, the available implementations have been used. During the evaluation, we delete the clusters equal or larger than the threshold of 150 words as they hardly can represent any meaningful synset. The notation WATSET[MCL, CWtop] means using MCL for word sense induction and Chinese Whispers in the top mode for global clustering."
    }, {
      "heading" : "5.1 Impact of Graph Weighting Schema",
      "text" : "Figure 3 presents an overview of the evaluation results on both datasets. The first step being common for all of the tested synset induction methods is the graph construction. Thus, we started with an analysis of the three ways to weight edges of the graph introduced in Section 3.2: binary scores (ones), frequencies (count), and semantic similarity scores (sim). The results across various configurations and methods indicate that using the weights based on the similarity scores provided by word embeddings is the best strategy for all the methods but MaxMax on both English datasets. However, in these cases, its performance using the ones weighing does not exceed the other methods using the sim weighing. Therefore, we report all further results on the basis of the sim weights.\n14https://github.com/tudarmstadt-lt/ chinese-whispers\n15http://java-ml.sourceforge.net 16https://networkx.github.io\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nWordNet BabelNet Method # words # synsets # synonyms Precision Recall F-score Precision Recall F-score WATSET[CWtop, MCL] 77 871 55 874 78 303 0.290 0.397 0.335 0.272 0.478 0.346 WATSET[CWtop, CWtop] 77 871 55 873 78 337 0.290 0.396 0.335 0.271 0.478 0.346 WATSET[MCL, MCL] 77 871 33 948 127 331 0.328 0.334 0.331 0.340 0.414 0.374 WATSET[MCL, CWlog] 77 871 32 843 143 120 0.341 0.305 0.322 0.350 0.379 0.364 MCL 77 871 25 467 141 042 0.316 0.327 0.321 0.343 0.399 0.369 CWnolog 77 871 24 220 170 499 0.338 0.276 0.304 0.357 0.343 0.350 CPMk=2 54 374 19 749 80 325 0.148 0.551 0.234 0.243 0.525 0.332 MaxMax 77 871 25 532 436 571 0.363 0.131 0.192 0.380 0.163 0.228 ECO 77 870 56 152 25 726 0.075 0.765 0.136 0.109 0.717 0.190\nTable 3: Comparison of the synset induction methods on datasets for English. All methods rely on the similarity edge weighting (sim); best configurations of each method in terms of F-scores are shown for each dataset. Results are sorted by F-score on WordNet, top three values of each metric are boldfaced.\nRuThes YARN Method # words # synsets # synonyms Precision Recall F-score Precision Recall F-score WATSET[CWlog, MCL] 74 395 48 484 317 040 0.224 0.101 0.139 0.466 0.396 0.428 WATSET[MCL, MCL] 74 395 32 407 378 769 0.220 0.093 0.131 0.459 0.400 0.427 WATSET[CWnolog, CWnolog] 74 395 48 443 327 125 0.225 0.097 0.136 0.471 0.386 0.424 MCL 74 395 19 277 328 789 0.187 0.128 0.152 0.345 0.546 0.423 WATSET[MCL, CWnolog] 74 395 31 129 429 823 0.231 0.084 0.123 0.500 0.352 0.413 CWnolog 74 395 16 683 660 424 0.220 0.069 0.105 0.460 0.357 0.402 MaxMax 74 395 24 250 560 096 0.186 0.147 0.164 0.230 0.566 0.327 CPMk=3 14 662 3 729 46 007 0.047 0.189 0.075 0.061 0.561 0.110 ECO 74 395 61 126 15 784 0.022 0.630 0.042 0.002 0.898 0.004\nTable 4: Results on Russian sorted by F-score on YARN, top three values of each metric are boldfaced."
    }, {
      "heading" : "5.2 Performance Analysis",
      "text" : "Table 3 and 4 present the evaluation results on the all the datasets for both languages. For each method, we show the best configurations in terms of F-score. One may note that the granularity of the resulting synsets, especially on the Russian dataset, is very different, ranging from 3 729 synsets for the CPMk=3 method to 61 126 induced by the ECO method. Both tables report the number of words, synsets and synonyms after pruning huge clusters larger than 150 words. Without this pruning, the MaxMax and CPM methods tend to discover giant components obtaining almost zero precision as we generate all possible pairs of nodes in such clusters. The other methods did not demonstrate such behavior.\nWATSET robustly outperformed all other methods according to F-score on both English datasets (Table 3) and on the YARN dataset for Russian (Table 4). Also, it outperformed all other methods according to precision on both Russian datasets. The disambiguation of the input graph performed by the WATSET method splits nodes belonging to several local communities to several nodes significantly facilitating the clustering task otherwise complicated by the presence of the hubs that wrongly link semantically unrelated nodes.\nAmusingly, in all the cases, the toughest competitor was a hard clustering algorithm—MCL (van Dongen, 2000). We observed that the “plain” MCL successfully groups the monosemous words,\nbut produces nonsensical clusters of moderate size, e.g., 20–50 words, when a polysemous word appears, resulting in the precision drop w.r.t. WATSET. Chinese Whispers, a simplified version of MCL, converges faster due to node label randomization which leads to a stricter stopping condition. CW thus does not amplify the hubs between the unrelated nodes and therefore produces smaller clusters in average. As the result, the “plain” CW offers higher precision than the “plain” MCL at the cost of lower recall.\nUsing CW instead of MCL for word sense induction in WATSET expectedly produces finegrained senses. Interestingly, at the global clustering step, these senses erroneously tend to form coarse-grained synsets connecting unrelated senses of the ambiguous words. This explains the generally higher precision of WATSET[MCL, ·].\nThe MaxMax algorithm showed mixed results. On the one hand, it outputs large clusters uniting more than hundred nodes. This inevitably leads to a high recall, as it is clearly seen in the results for Russian because such synsets passed through our threshold of 150 words. The synsets produced on English datasets were even larger and did not pass, which resulted in low recall. On the other hand, the smaller synsets having at most 10–15 words were identified correctly. MaxMax appeared to be extremely sensible to the edge weighing, which complicates its practical use.\nThe CPM algorithm showed unsatisfactory results which indicated by emitting giant compo-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nnents uniting thousands words. Such clusters have been automatically pruned, but the rest clusters connect virtually every node left after the pruning. This is confirmed by the high values of recall. As one increase the minimal number of elements in the clique k, precision effectively grows, but at the cost of a dramatic drop in recall. We suppose that the network structure assumptions exploited by CPM do not accurately model the structure of the synonymy graphs in the present task.\nFinally, the ECO method yielded the worst results because the most cluster candidates failed to pass through the constant threshold used for estimating whether a pair of words should be included in the same cluster. Most synsets produced by this method were trivial, i.e., containing only one word. The remaining synsets for both languages have at most three words having been connected by a chance due to the edge noising procedure used in this method resulting in low precision."
    }, {
      "heading" : "6 Discussion",
      "text" : "On difference in absolute scores. While the results obtained for the English datasets are in agreement (Tables 3 and 5), the results measured on RuThes and YARN datasets for Russian show a similar global picture in terms of relevant ranking of the methods, yet absolute scores are largely different (cf. Figure 3). This difference stems from the difference of the gold standards. RuThes is more domain-specific in terms of vocabulary, so our input set of generic synonymy dictionaries has a limited coverage on this dataset. On the other hand, recall calculated on YARN reaching significantly higher levels as this resource was manually built on the basis of exactly the same initial resources. Low performance of the cross-evaluation of two resources presented in Table 5 confirms this observation. No single resource for Russian can obtain high precision scores on another one. Surprisingly, even BabelNet, which integrates most of available lexical resources, still does not reach recall largely higher than 0.5.17 Finally, note that the results of this cross-dataset evaluation are not directly comparable to results in Table 4 as in our experiments we use much smaller input dictionaries than those used by BabelNet.\nOn sparseness of the input dictionary. Table 6 presents some examples of the obtained synsets\n17We used BabelNet 3.7 as in our study for English and retrieved all 3 497 327 synsets that were marked as Russian.\nof various sizes for the best WATSET configuration on both languages. As one might observe, the quality of the results is highly plausible. However, one limitation of all approaches considered in this paper is the dependence on completeness of the input dictionary of synonyms. In some parts of the input synonymy graph, important bridges between words can be missing, leading to smallerthan-desired synsets. A promising extension of the present methodology is using the distributional models to enhance connectivity of the graph to further improve recall of the method by adding extra relations."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper, we presented a new robust approach to fuzzy graph clustering that relies on a hard clustering method. Using ego network clustering, the nodes belonging to several local communities are split into several nodes each belonging to one local community. The transformed “disambiguated” graph is then clustered using an efficient hard graph clustering algorithm, obtaining a fuzzy clustering as the result. The disambiguated graph contains fewer hubs connecting unrelated nodes from different communities and thus facilitates clustering. We apply this meta-clustering algorithm to the task of synset induction, obtaining the best results on two datasets for two different natural language in terms of precision and competitive results in terms of F-score, as compared to five state-ofthe-art graph clustering methods.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "The dictionary of Russian synonyms and semantically related expressions (Словарь русских синонимов и сходных по смыслу выражений)",
      "author" : [ "Nikolay Abramov." ],
      "venue" : "AST, Moscow, Russia, 8th edition.",
      "citeRegEx" : "Abramov.,? 2007",
      "shortCiteRegEx" : "Abramov.",
      "year" : 2007
    }, {
      "title" : "Distributional Memory: A General Framework for Corpus-based Semantics",
      "author" : [ "Marco Baroni", "Alessandro Lenci." ],
      "venue" : "Computational Linguistics 36(4):673–721. https://doi.org/10.1162/coli_a_00016.",
      "citeRegEx" : "Baroni and Lenci.,? 2010",
      "shortCiteRegEx" : "Baroni and Lenci.",
      "year" : 2010
    }, {
      "title" : "Chinese Whispers: An Efficient Graph Clustering Algorithm and Its Application to Natural Language Processing Problems",
      "author" : [ "Chris Biemann." ],
      "venue" : "Proceedings of the First Workshop on Graph Based Methods for Natural Language Processing. Associ-",
      "citeRegEx" : "Biemann.,? 2006",
      "shortCiteRegEx" : "Biemann.",
      "year" : 2006
    }, {
      "title" : "Creating a system for lexical substitutions from scratch using crowdsourcing",
      "author" : [ "Chris Biemann." ],
      "venue" : "Language Resources and Evaluation 47(1):97–122. https://doi.org/10.1007/s10579-012-9180-5.",
      "citeRegEx" : "Biemann.,? 2013",
      "shortCiteRegEx" : "Biemann.",
      "year" : 2013
    }, {
      "title" : "Text: now in 2D! A framework for lexical expansion with contextual similarity",
      "author" : [ "Chris Biemann", "Martin Riedl." ],
      "venue" : "Journal of Language Modelling 1(1). https://doi.org/10.15398/jlm.v1i1.60.",
      "citeRegEx" : "Biemann and Riedl.,? 2013",
      "shortCiteRegEx" : "Biemann and Riedl.",
      "year" : 2013
    }, {
      "title" : "The maximum clique problem",
      "author" : [ "Immanuel M. Bomze", "Marco Budinich", "Panos M. Pardalos", "Marcello Pelillo." ],
      "venue" : "Handbook of Combinatorial Optimization, Springer, pages 1–74.",
      "citeRegEx" : "Bomze et al\\.,? 1999",
      "shortCiteRegEx" : "Bomze et al\\.",
      "year" : 1999
    }, {
      "title" : "YARN: Spinning-inProgress",
      "author" : [ "Pavel Braslavski", "Dmitry Ustalov", "Mukhin Mukhin", "Yuri Kiselev." ],
      "venue" : "Proceedings of the 8th Global WordNet Conference. Global WordNet Association, Bucharest, Romania, GWC 2016, pages 58–65.",
      "citeRegEx" : "Braslavski et al\\.,? 2016",
      "shortCiteRegEx" : "Braslavski et al\\.",
      "year" : 2016
    }, {
      "title" : "Clustering and Diversifying Web Search Results with Graph-Based Word Sense Induction",
      "author" : [ "Antonio Di Marco", "Roberto Navigli." ],
      "venue" : "Computational Linguistics 39(3):709–754. https://doi.org/10.1162/COLI_a_00148.",
      "citeRegEx" : "Marco and Navigli.,? 2012",
      "shortCiteRegEx" : "Marco and Navigli.",
      "year" : 2012
    }, {
      "title" : "Development of lexical basis for the Universal Dictionary of UNL Concepts",
      "author" : [ "Vyachelav G. Dikonov." ],
      "venue" : "Computational Linguistics and Intellectual Technologies: Papers from the Annual International Conference “Dialogue”. RGGU, Moscow, volume",
      "citeRegEx" : "Dikonov.,? 2013",
      "shortCiteRegEx" : "Dikonov.",
      "year" : 2013
    }, {
      "title" : "Discovering Corpus-Specific Word Senses",
      "author" : [ "Beate Dorow", "Dominic Widdows." ],
      "venue" : "Proceedings of the Tenth Conference on European Chapter of the Association for Computational Linguistics Volume 2. Association for Computational Linguis-",
      "citeRegEx" : "Dorow and Widdows.,? 2003",
      "shortCiteRegEx" : "Dorow and Widdows.",
      "year" : 2003
    }, {
      "title" : "ECO and Onto.PT: a flexible approach for creating a Portuguese wordnet automatically. Language Resources and Evaluation 48(2):373–393",
      "author" : [ "Hugo Gonçalo Oliveira", "Paolo Gomes" ],
      "venue" : null,
      "citeRegEx" : "Oliveira and Gomes.,? \\Q2014\\E",
      "shortCiteRegEx" : "Oliveira and Gomes.",
      "year" : 2014
    }, {
      "title" : "Web Query Expansion by WordNet",
      "author" : [ "Zhiguo Gong", "Chan Wa Cheang", "U. Leong Hou." ],
      "venue" : "Proceedings of the 16th International Conference on Database and Expert Systems Applications - DEXA ’05, Springer Berlin Hei-",
      "citeRegEx" : "Gong et al\\.,? 2005",
      "shortCiteRegEx" : "Gong et al\\.",
      "year" : 2005
    }, {
      "title" : "UBY — A Large-Scale Unified Lexical-Semantic Resource Based on LMF",
      "author" : [ "Iryna Gurevych", "Judith Eckle-Kohler", "Silvana Hartmann", "Michael Matuschek", "Christian M. Meyer", "Christian Wirth." ],
      "venue" : "Proceedings of the 13th Conference of the European",
      "citeRegEx" : "Gurevych et al\\.,? 2012",
      "shortCiteRegEx" : "Gurevych et al\\.",
      "year" : 2012
    }, {
      "title" : "Modelling Word Similarity: an Evaluation of Automatic Synonymy Extraction Algorithms",
      "author" : [ "Kris Heylen", "Yves Peirsman", "Dirk Geeraerts", "Dirk Speelman." ],
      "venue" : "Proceedings of the Sixth International Conference on Language Resources and Evaluation.",
      "citeRegEx" : "Heylen et al\\.,? 2008",
      "shortCiteRegEx" : "Heylen et al\\.",
      "year" : 2008
    }, {
      "title" : "MaxMax: A GraphBased Soft Clustering Algorithm Applied to Word Sense Induction",
      "author" : [ "David Hope", "Bill Keller." ],
      "venue" : "Computational Linguistics and Intelligent Text Processing: 14th International Conference, CICLing 2013, Samos, Greece, March",
      "citeRegEx" : "Hope and Keller.,? 2013",
      "shortCiteRegEx" : "Hope and Keller.",
      "year" : 2013
    }, {
      "title" : "Using WordNet to Measure Semantic Orientations of Adjectives",
      "author" : [ "Jaap Kamps", "Maarten Marx", "Robert J. Mokken", "Maarten de Rijke." ],
      "venue" : "Proceedings of LREC 2004. European Language Resources Association, Lisbon, Portugal, pages 1115–1118.",
      "citeRegEx" : "Kamps et al\\.,? 2004",
      "shortCiteRegEx" : "Kamps et al\\.",
      "year" : 2004
    }, {
      "title" : "Current Status of Russian Electronic Thesauri: Quality, Completeness and Availability (Современное состояние электронных тезаурусов русского языка: качество, полнота и до",
      "author" : [ "Yuri Kiselev", "Sergey V. Porshnev", "Mikhail Mukhin" ],
      "venue" : null,
      "citeRegEx" : "Kiselev et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kiselev et al\\.",
      "year" : 2015
    }, {
      "title" : "An approach to automated construction of a general-purpose lexical ontology based on Wiktionary",
      "author" : [ "Andrew A. Krizhanovsky", "Alexander V. Smirnov." ],
      "venue" : "Journal of Computer and Systems Sciences International 52(2):215–225.",
      "citeRegEx" : "Krizhanovsky and Smirnov.,? 2013",
      "shortCiteRegEx" : "Krizhanovsky and Smirnov.",
      "year" : 2013
    }, {
      "title" : "Scaling Question Answering to the Web",
      "author" : [ "Cody Kwok", "Oren Etzioni", "Daniel S. Weld." ],
      "venue" : "ACM Transactions on Information Systems 19(3):242– 262. https://doi.org/10.1145/502115.502117.",
      "citeRegEx" : "Kwok et al\\.,? 2001",
      "shortCiteRegEx" : "Kwok et al\\.",
      "year" : 2001
    }, {
      "title" : "An Information-Theoretic Definition of Similarity",
      "author" : [ "Dekang Lin." ],
      "venue" : "Proceedings of the Fifteenth International Conference on Machine Learning. Morgan Kaufmann Publishers Inc., Madison, WI, USA, ICML ’98, pages 296–304.",
      "citeRegEx" : "Lin.,? 1998",
      "shortCiteRegEx" : "Lin.",
      "year" : 1998
    }, {
      "title" : "Thesauri in information retrieval tasks (Тезаурусы в задачах информационного поиска)",
      "author" : [ "Natalia Loukachevitch." ],
      "venue" : "Moscow University Press, Moscow, Russia.",
      "citeRegEx" : "Loukachevitch.,? 2011",
      "shortCiteRegEx" : "Loukachevitch.",
      "year" : 2011
    }, {
      "title" : "Distributed Representations of Words and Phrases and their Compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeffrey Dean." ],
      "venue" : "Advances in Neural Information Processing Systems 26, Curran Associates, Inc., Harrahs",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "WordNet: A Lexical Database for English",
      "author" : [ "George A. Miller" ],
      "venue" : null,
      "citeRegEx" : "Miller.,? \\Q1995\\E",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network",
      "author" : [ "Roberto Navigli", "Simone P. Ponzetto." ],
      "venue" : "Artificial Intelligence 193:217–250.",
      "citeRegEx" : "Navigli and Ponzetto.,? 2012",
      "shortCiteRegEx" : "Navigli and Ponzetto.",
      "year" : 2012
    }, {
      "title" : "Uncovering the overlapping community structure of complex networks in nature and society",
      "author" : [ "Gergely Palla", "Imre Derenyi", "Illes Farkas", "Tamas Vicsek." ],
      "venue" : "Nature 435:814–818. https://doi.org/10.1038/nature03607.",
      "citeRegEx" : "Palla et al\\.,? 2005",
      "shortCiteRegEx" : "Palla et al\\.",
      "year" : 2005
    }, {
      "title" : "Discovering Word Senses from Text",
      "author" : [ "Patrick Pantel", "Dekang Lin." ],
      "venue" : "Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, Edmonton, Alberta, Canada, KDD ’02, pages 613–619.",
      "citeRegEx" : "Pantel and Lin.,? 2002",
      "shortCiteRegEx" : "Pantel and Lin.",
      "year" : 2002
    }, {
      "title" : "Learning to Merge Word Senses",
      "author" : [ "Rion Snow", "Sushant Prakash", "Daniel Jurafsky", "Andrew Y. Ng." ],
      "venue" : "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language",
      "citeRegEx" : "Snow et al\\.,? 2007",
      "shortCiteRegEx" : "Snow et al\\.",
      "year" : 2007
    }, {
      "title" : "Graph Clustering by Flow Simulation",
      "author" : [ "Stijn van Dongen." ],
      "venue" : "Ph.D. thesis, University of Utrecht.",
      "citeRegEx" : "Dongen.,? 2000",
      "shortCiteRegEx" : "Dongen.",
      "year" : 2000
    }, {
      "title" : "HyperLex: lexical cartography for information retrieval",
      "author" : [ "Jean Véronis." ],
      "venue" : "Computer Speech & Language 18(3):223–252. https://doi.org/10.1016/j.csl.2004.05.002.",
      "citeRegEx" : "Véronis.,? 2004",
      "shortCiteRegEx" : "Véronis.",
      "year" : 2004
    }, {
      "title" : "Extracting Lexical Semantic Knowledge from Wikipedia and Wiktionary",
      "author" : [ "Torsten Zesch", "Christof Müller", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 6th International Conference on Language Resources and Evaluation. European Language Re-",
      "citeRegEx" : "Zesch et al\\.,? 2008",
      "shortCiteRegEx" : "Zesch et al\\.",
      "year" : 2008
    }, {
      "title" : "Improving question retrieval in community question answering using world knowledge",
      "author" : [ "Guangyou Zhou", "Yang Liu", "Fang Liu", "Daojian Zeng", "Jun Zhao." ],
      "venue" : "Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence.",
      "citeRegEx" : "Zhou et al\\.,? 2013",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "Synsets represent word senses and are building blocks of WordNet (Miller, 1995) and similar resources as thesauri and lexical ontologies.",
      "startOffset" : 65,
      "endOffset" : 79
    }, {
      "referenceID" : 11,
      "context" : "These resources are crucial for many natural language processing applications which require common sense reasoning, such as information retrieval (Gong et al., 2005) and question answering (Kwok et al.",
      "startOffset" : 146,
      "endOffset" : 165
    }, {
      "referenceID" : 18,
      "context" : ", 2005) and question answering (Kwok et al., 2001; Zhou et al., 2013).",
      "startOffset" : 31,
      "endOffset" : 69
    }, {
      "referenceID" : 30,
      "context" : ", 2005) and question answering (Kwok et al., 2001; Zhou et al., 2013).",
      "startOffset" : 31,
      "endOffset" : 69
    }, {
      "referenceID" : 23,
      "context" : "In contrast to the projects like BabelNet (Navigli and Ponzetto, 2012) and UBY (Gurevych et al.",
      "startOffset" : 42,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "In contrast to the projects like BabelNet (Navigli and Ponzetto, 2012) and UBY (Gurevych et al., 2012), which rely on English WordNet as",
      "startOffset" : 79,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "These resources are crucial for many natural language processing applications which require common sense reasoning, such as information retrieval (Gong et al., 2005) and question answering (Kwok et al., 2001; Zhou et al., 2013). However, for most languages no manually-constructed resource comparable to the English WordNet in terms of coverage and quality is available. For instance, Kiselev et al. (2015) presents a comparative analysis of lexical resources available for the Russian language.",
      "startOffset" : 147,
      "endOffset" : 407
    }, {
      "referenceID" : 26,
      "context" : "A related branch of methods deals with coarsification of sense inventories of fine-grained lexical resources, such as (Snow et al., 2007).",
      "startOffset" : 118,
      "endOffset" : 137
    }, {
      "referenceID" : 19,
      "context" : "In particular, word sense induction (WSI) based on word ego networks clusters graphs of words semantically related to the ambiguous word (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Véronis, 2004; Hope and Keller, 2013).",
      "startOffset" : 137,
      "endOffset" : 233
    }, {
      "referenceID" : 25,
      "context" : "In particular, word sense induction (WSI) based on word ego networks clusters graphs of words semantically related to the ambiguous word (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Véronis, 2004; Hope and Keller, 2013).",
      "startOffset" : 137,
      "endOffset" : 233
    }, {
      "referenceID" : 9,
      "context" : "In particular, word sense induction (WSI) based on word ego networks clusters graphs of words semantically related to the ambiguous word (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Véronis, 2004; Hope and Keller, 2013).",
      "startOffset" : 137,
      "endOffset" : 233
    }, {
      "referenceID" : 28,
      "context" : "In particular, word sense induction (WSI) based on word ego networks clusters graphs of words semantically related to the ambiguous word (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Véronis, 2004; Hope and Keller, 2013).",
      "startOffset" : 137,
      "endOffset" : 233
    }, {
      "referenceID" : 14,
      "context" : "In particular, word sense induction (WSI) based on word ego networks clusters graphs of words semantically related to the ambiguous word (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; Véronis, 2004; Hope and Keller, 2013).",
      "startOffset" : 137,
      "endOffset" : 233
    }, {
      "referenceID" : 1,
      "context" : "tld/anonymized of semantically related words obtained using distributional methods (Baroni and Lenci, 2010; Biemann and Riedl, 2013), the resulting clusters by no means can be considered synsets.",
      "startOffset" : 83,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "tld/anonymized of semantically related words obtained using distributional methods (Baroni and Lenci, 2010; Biemann and Riedl, 2013), the resulting clusters by no means can be considered synsets.",
      "startOffset" : 83,
      "endOffset" : 132
    }, {
      "referenceID" : 13,
      "context" : "(Heylen et al., 2008); (2) clusters are not unique, i.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "tld/anonymized of semantically related words obtained using distributional methods (Baroni and Lenci, 2010; Biemann and Riedl, 2013), the resulting clusters by no means can be considered synsets. Namely, (1) they contain words related not only via synonymy relation, but via a mixture of relations such as synonymy, hypernymy, co-hyponymy, antonymy, etc. (Heylen et al., 2008); (2) clusters are not unique, i.e., one word can occur in several clusters referring to the same sense, while in WordNet a word used in the given sense occurs only in a single synset. In our approach, to induce synsets, we use word ego network clustering similarly as in word sense induction approaches, but apply them to the graph of semantically clean synonyms. Methods based on clustering of synonyms, such as our approach, induce the resource from an ambiguous graph of synonyms where edges were extracted from manually-created resources. According to the best of our knowledge, most experiments either used graph-based word sense induction applied to text-derived graphs or used a mapping-based method which already assumes availability of a WordNet-like resource. A notable exception is the ECO approach by Gonçalo Oliveira and Gomes (2014), which was used to induce a WordNet of the Portuguese language called Onto.",
      "startOffset" : 84,
      "endOffset" : 1224
    }, {
      "referenceID" : 14,
      "context" : "• MaxMax (Hope and Keller, 2013) is a fuzzy clustering algorithm designed initially for the word sense induction task.",
      "startOffset" : 9,
      "endOffset" : 32
    }, {
      "referenceID" : 9,
      "context" : "Notably, it has been successfully used for the word sense induction task (Dorow and Widdows, 2003).",
      "startOffset" : 73,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : "• Chinese Whispers (Biemann, 2006) is a hard clustering algorithm for weighted graphs that can be considered as a special case of MCL with a simplified class update step.",
      "startOffset" : 19,
      "endOffset" : 34
    }, {
      "referenceID" : 24,
      "context" : "• Clique Percolation Method (CPM) (Palla et al., 2005) is a fuzzy clustering algorithm for unweighted graphs that builds up the clusters from k-cliques corresponding to fully connected sub-graphs of k nodes.",
      "startOffset" : 34,
      "endOffset" : 54
    }, {
      "referenceID" : 15,
      "context" : "The cliques in such a graph naturally form densely connected sets of synonyms corresponding to concepts (Kamps et al., 2004).",
      "startOffset" : 104,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "Given the fact that the clique problem in a graph is NPcomplete (Bomze et al., 1999), an efficient graph clustering algorithm like the MCL algorithm can be used for finding a global segmentation of the graph.",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "3 Word Sense Induction We use a graph-based word sense induction method that is similar to the curvature-based approach of Dorow and Widdows (2003). In particular, removal of the nodes participating in many triangles tends to separate the original graph into several connected components.",
      "startOffset" : 123,
      "endOffset" : 148
    }, {
      "referenceID" : 28,
      "context" : ", (Véronis, 2004; Biemann, 2006; Hope and Keller, 2013), do not perform this step, but perform only local analysis of the graph.",
      "startOffset" : 2,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : ", (Véronis, 2004; Biemann, 2006; Hope and Keller, 2013), do not perform this step, but perform only local analysis of the graph.",
      "startOffset" : 2,
      "endOffset" : 55
    }, {
      "referenceID" : 14,
      "context" : ", (Véronis, 2004; Biemann, 2006; Hope and Keller, 2013), do not perform this step, but perform only local analysis of the graph.",
      "startOffset" : 2,
      "endOffset" : 55
    }, {
      "referenceID" : 22,
      "context" : "We use WordNet8 (Miller, 1995), a popular English lexical database crafted by a group of expert lexicographers.",
      "startOffset" : 16,
      "endOffset" : 30
    }, {
      "referenceID" : 23,
      "context" : "Also, we use BabelNet9 (Navigli and Ponzetto, 2012), a large-scale multilingual semantic network constructed automatically using WordNet, Wikipedia and other resources.",
      "startOffset" : 23,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "We also considered using TWSI by Biemann (2013), but found that it contains mostly hypernymy and co-hyponymy relations instead of being a viable source of synonymy information.",
      "startOffset" : 33,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : "We use RuThes10 (Loukachevitch, 2011), a lexical ontology for Russian containing both general vocabulary and domain-specific synsets related to sport, finance, economics, etc.",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "Up to a half of the words in this resource are multiword expressions (Kiselev et al., 2015), which is due to the coverage of domain-specific vocabulary.",
      "startOffset" : 69,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : "Also, we use Yet Another RussNet11 (YARN) by Braslavski et al. (2016) as yet another gold standard for Russian.",
      "startOffset" : 45,
      "endOffset" : 70
    }, {
      "referenceID" : 21,
      "context" : "We use the standard 300-dimensional word embeddings trained on the 100 billion tokens Google News corpus (Mikolov et al., 2013).",
      "startOffset" : 105,
      "endOffset" : 127
    }, {
      "referenceID" : 21,
      "context" : "We use the 500-dimensional word embeddings trained using the skip-gram model with negative sampling (Mikolov et al., 2013) using a context window size of 10 with the minimal word frequency of 5 on a 12.",
      "startOffset" : 100,
      "endOffset" : 122
    }, {
      "referenceID" : 29,
      "context" : "The synonyms were extracted from the English Wiktionary, which is the largest Wiktionary as the present moment according to the lexical coverage, using the DKPro JWKTL tool by Zesch et al. (2008). Only the words marked as English have been extracted.",
      "startOffset" : 176,
      "endOffset" : 196
    }, {
      "referenceID" : 8,
      "context" : "The synonyms from three sources were combined to improve lexical coverage of the input dictionary and to enforce confidence in jointly observed synonyms: (1) synonyms listed in the Russian Wiktionary extracted using the Wikokit tool by Krizhanovsky and Smirnov (2013); (2) the dictionary of Abramov (2007); and (3) the Universal Dictionary of Concepts (Dikonov, 2013).",
      "startOffset" : 352,
      "endOffset" : 367
    }, {
      "referenceID" : 15,
      "context" : "The synonyms from three sources were combined to improve lexical coverage of the input dictionary and to enforce confidence in jointly observed synonyms: (1) synonyms listed in the Russian Wiktionary extracted using the Wikokit tool by Krizhanovsky and Smirnov (2013); (2) the dictionary of Abramov (2007); and (3) the Universal Dictionary of Concepts (Dikonov, 2013).",
      "startOffset" : 236,
      "endOffset" : 268
    }, {
      "referenceID" : 0,
      "context" : "The synonyms from three sources were combined to improve lexical coverage of the input dictionary and to enforce confidence in jointly observed synonyms: (1) synonyms listed in the Russian Wiktionary extracted using the Wikokit tool by Krizhanovsky and Smirnov (2013); (2) the dictionary of Abramov (2007); and (3) the Universal Dictionary of Concepts (Dikonov, 2013).",
      "startOffset" : 291,
      "endOffset" : 306
    } ],
    "year" : 2017,
    "abstractText" : "This paper presents a new graph-based approach that induces synsets using synonymy dictionaries and word embeddings. Firstly, we build a weighted graph of synonyms extracted from commonly available resources, such as Wiktionary. Secondly, we apply word sense induction to deal with ambiguous words. Finally, we cluster the disambiguated version of the ambiguous input graph into synsets. Our meta-clustering approach lets us use an efficient hard clustering algorithm to perform fuzzy clustering of the graph. Despite its simplicity, our approach shows excellent results outperforming five analogous state-of-the-art methods in terms of F-score on four different gold standard datasets for English and Russian derived from large-scale manually constructed lexical resources.",
    "creator" : "LaTeX with hyperref package"
  }
}