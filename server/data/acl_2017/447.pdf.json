{
  "name" : "447.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Advances in text categorization have the potential to improve systems for analyzing sentiment, inferring authorship or author attributes, making predictions, and many more. Several past researchers have noticed that methods that reason about the relative saliency or importance of passages within a text can lead to improvements (Ko et al., 2004). Latent variables (Yessenalina et al., 2010), structured-sparse regularizers (Yogatama and Smith, 2014), and neural attention models (Yang et al., 2016) have all been explored.\nDiscourse structure, which represents the organization of a text as a tree (for an example, see Figure 1), might provide cues for the importance of different parts of a text. Some promising results on sentiment classification tasks support this idea: Bhatia et al. (2015) and Hogenboom et al. (2015) applied hand-crafted weighting schemes to the sentences in a document, based on its discourse structure, and showed benefit to sentiment polarity classification.\nIn this paper, we investigate the value of discourse structure for text categorization more broadly, considering five tasks, through the use of a recursive neural network built on an\nR\nContrast\nElaboration\nA B\nExplanation\nC Joint\nD Constrast\nE F\n[Although the food was amazing]A [and I was in love with the spicy pork burrito,]B [the service was really awful.]C [We watched our waiter serve himself many drinks.]D [He kept running into the bathroom]E [instead of grabbing our bill.]F\nautomatically-derived document parse from a topperforming, open-source discourse parser, DPLP (Ji and Eisenstein, 2014). Our models learn to weight the importance of a document’s sentences, based on their positions and relations in the discourse tree. We introduce a new, unnormalized attention mechanism to this end.\nExperimental results show that variants of our model outperform prior work on four out of five tasks considered. Our method unsurprisingly underperforms on the fifth task, making predictions about legislative bills—a rather different genre in which discourse conventions are quite different from those in the discourse parser’s training data. Further experiments show the effect of discourse parse quality on text categorization performance, suggesting that future improvements to discourse parsing will pay off for text categorization, and validate our new attention mechanism.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nOur implementation is available at http:// anonymous.link."
    }, {
      "heading" : "2 Background: Rhetorical Structure",
      "text" : "Theory\nRhetorical Structure Theory (RST; Mann and Thompson, 1988) is a theory of discourse that has enjoyed popularity in NLP. RST posits that a document can be represented by a tree whose leaves are elementary discourse units (EDUs, typically clauses or sentences). Internal nodes in the tree correspond to spans of sentences that are connected via discourse relations such as CONTRAST and ELABORATION. In most cases, a discourse relation links adjacent spans denoted “nucleus” and “satellite,” with the former more essential to the writer’s purpose than the latter.1\nAn example of a manually constructed RST parse for a restaurant review is shown in Figure 1. The six EDUs are indexed from A to F ; the discourse tree organizes them hierarchically into increasingly larger spans, with the last CONTRAST relation resulting in a span that covers the whole review. Within each relation, the RST tree indicates the nucleus pointed by an arrow from its satellite (e.g., in the ELABORATION relation, A is the nucleus and B is the satellite).\nThe information embedded in RST trees has motivated many applications in NLP research, including document summarization (Marcu, 1999), argumentation mining (Azar, 1999), and sentiment analysis (Bhatia et al., 2015). In most applications, RST trees are built by automatic discourse parsing, due to the expensive cost of manual annotation. In this work, we use a state-of-the-art open-source RST-style discourse parser, DPLP (Ji and Eisenstein, 2014).2\nWe follow recent work that suggests transforming the RST tree into a dependency structure (Yoshida et al., 2014).3 Figure 2(a) shows the corresponding dependency structure of the RST tree in Figure 1. It is clear that C is the root of the tree, and in fact this clause summarizes the review and suffices to categorize it as negative. This dependency representation of the RST tree offers a\n1There are also a few exceptions in which a relation can be realized with multiple nuclei.\n2 https://github.com/jiyfeng/DPLP\n3The transformation is trivial and deterministic given the nucleus-satellite mapping for each relation. The procedure is analogous to the transformation of a headed phrase-structure parse in syntax into a dependency tree (e.g., Yamada and Matsumoto, 2003).\nform of inductive bias for our neural model, helping it to discern the most salient parts of a text in order to assign it a label."
    }, {
      "heading" : "3 Model",
      "text" : "Our model is a recursive neural network built on a discourse dependency tree. It includes a distributed representation computed for each EDU, and a composition function that combines EDUs and partial trees into larger trees. At the top of the tree, the representation of the complete document is used to make a categorization decision. Our approach is analogous to (and inspired by) the use of recursive neural networks on syntactic dependency trees, with word embeddings at the leaves (Socher et al., 2014)."
    }, {
      "heading" : "3.1 Representation of Sentences",
      "text" : "Let e be the distributed representation of an EDU. We use a bidirectional LSTM on the words’ embeddings within each EDU (details of word embeddings are given in section 4), concatenating the last hidden state vector from the forward LSTM ( e ) with that of the backward LSTM ( !e ) to get e.\nThere is extensive recent work on architectures for embedding representations of sentences and other short pieces of text, including, for example, (bi)recursive neural networks (Paulus et al., 2014) and convolutional neural networks (Kalchbrenner et al., 2014). Future work might consider alternatives; we chose the bidirectional LSTM due to its effectiveness in many settings."
    }, {
      "heading" : "3.2 Full Recursive Model",
      "text" : "Given the discourse dependency tree for an input text, our recursive model builds a vector representation through composition at each arc in the tree. Let v\ni denote the vector representation of EDU i and its descendants. For the base case where EDU i is a leaf in the tree, we let v\ni = tanh(e i ), which is the elementwise hyperbolic tangent function.\nFor an internal node i, the composition function considers a parent and all of its children, whose indices are denoted by children(i). In defining this composition function, we seek for (i.) the contribution of the parent node e\ni to be central; and (ii.) the contribution of each child node e\nj be determined by its content as well as the discourse relation it holds with the parent. We therefore define\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nC\nDA E\nB F\nElab.\nCont. Exp. Exp.\nCont.\n(a) dependency structure\ntanh(e C\n+ P\nj2{A,D,E} ↵C,jWC,jvj)\ntanh(e D )tanh(e A + ↵ A,B W A,B v B ) tanh(e E + ↵ F,E W F,E v F )\ntanh(e B ) tanh(e F )\nW A,B\nW C,A\nW C,D\nW C,E\nW F,E\n(b) recursive neural network structure\nFigure 2: (a.) The discourse dependency tree derived from the example RST tree in Figure 1. (b.) The corresponding recursive neural network model built on the tree.\nv i = tanh\n0\n@e i +\nX\nj2children(i)\n↵\ni,j W ri,jvj\n1\nA ,\n(1) where W\nri,j is a relation-specific composition matrix indexed by the relation between i and j, r\ni,j . ↵\ni,j\nis an “attention” weight, defined as\n↵\ni,j\n= ⇣ e> i W ↵ v j ⌘ , (2)\nwhere is the elementwise sigmoid and W ↵ contains attention parameters (these are relationindependent). Our attention mechanism differs from prior work (Bahdanau et al., 2015), in which attention weights are normalized to sum to one across competing candidates for attention. Here, ↵\ni,j does not depend on node i’s other children. This is motivated by RST, in which the presence of a node does not signify lesser importance to its siblings. Consider, for example, EDU D and text span E-F in Figure 1, which in parallel provide EXPLANATION for EDU C. This scenario differs from machine translation, where attention isused to implicitly and softly align output-language words to relatively few input-language words. It also differs from attention in composition functions used in syntactic parsing (Kuncoro et al., 2017), where attention can mimic head rules that follow from an endocentricity hypothesis of syntactic phrase representation.\nOur recursive composition function, through the attention mechanism and the relation-specific weight matrices, is designed to learn how to differently weight EDUs for the categorization task. This idea of using a weighting scheme along with discourse structure is explored in prior works (Bhatia et al., 2015; Hogenboom et al., 2015), although they are manually designed, rather than learned from training data.\nOnce we have vroot of a text, the prediction of its category is given by softmax (W\no vroot + b). We refer to this model as the FULL model, since it makes use of the entire discourse dependency tree."
    }, {
      "heading" : "3.3 Unlabeled Model",
      "text" : "The FULL model based on Equation 1 uses a dependency discourse tree with relations. Because alternate discourse relation labels have been proposed (e.g., Prasad et al., 2008), we seek to measure the effect of these labels. We therefore consider an UNLABELED model based only on the tree structure, without the relations:\nv i = tanh\n0\n@e i +\nX\nj2children(i)\n↵\ni,j v j\n1\nA . (3)\nHere, only attention weights are used to compose the children nodes’ representations, significantly reducing the number of model parameters.\nThis UNLABELED model is similar to the depth weighting scheme introduced by Bhatia et al. (2015), which also uses an unlabeled discourse dependency tree, but our attention weights are computed by a function whose parameters are learned. This approach sits squarely between Bhatia et al. (2015) and the flat document structure used by Yang et al. (2016); the UNLABELED model still uses discourse to bias the model toward some content (that which is closer to the tree’s root)."
    }, {
      "heading" : "3.4 Simpler Variants",
      "text" : "We consider two additional baselines that are even simpler. The first, ROOT, uses the discourse dependency structure only to select the root EDU, which is used to represent the entire text: vroot = eroot. No composition function is needed. This model variant is motivated by work on document summarization (Yoshida et al., 2014), where the\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nmost central EDU is used to represent the whole text.\nThe second variant, ADDITIVE, uses all the EDUs with a simple composition function, and does not depend on discourse structure at all: vroot = 1 N P N\ni=1 ei, where N is the total number of EDUs. This serves as a baseline to test the benefits of discourse, controlling for other design decisions and implementation choices. Although sentence representations {e\ni } are built in a different way from the work of Yang et al. (2016), this model is quite similar to their HN-AVE model on building document representations."
    }, {
      "heading" : "4 Implementation Details",
      "text" : "The parameters of all components of our model (top-level classification, composition, and EDU representation) are learned end-to-end using standard methods. We implement our learning procedure with the DyNet package (Neubig et al., 2017).\nPreprocessing. For all datasets, we use the same preprocessing steps, mostly following recent work from language modeling (e.g., Mikolov et al., 2010). We lowercased all the tokens and removed tokens that contain only punctuation symbols. We replaced numbers in the documents with a special number token. Low-frequency word types were replaced by UNK; we reduce the vocabulary for each dataset until approximately 5% of tokens are mapped to UNK. The vocabulary sizes after preprocessing are also shown in Table 1.\nDiscourse parsing. Our model requires the discourse structure for each document. We used DPLP, the RST parser from Ji and Eisenstein (2014), which is one of the best discourse parsers on the RST discourse treebank benchmark (Carlson et al., 2001). It employs a greedy decoding algorithm for parsing, producing 2,000 parses per minute on average on a single CPU. DPLP provides discourse segmentation, breaking a text into EDUs, typically clauses or sentences, based on syntactic parses provided by Stanford CoreNLP. RST trees are converted to dependencies following the method of Yoshida et al. (2014). DPLP as distributed is trained on 347 Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993).\nWord embeddings. In cases where there are 10,000 or fewer training examples, we used\npretrained GloVe word embeddings (Pennington et al., 2014), following previous work on neural discourse processing (Ji and Eisenstein, 2015). For larger datasets, we randomly initialize word embeddings and train them alongside other model parameters.\nLearning and hyperparameters. Online learning was performed with the optimization method and initial learning rate as hyperparameters. To avoid the exploding gradient problem, we used the norm clipping trick with a threshold of ⌧ = 5.0. In addition, dropout rate 0.3 was used on both input and hidden layers to avoid overfitting. We performed grid search over the word vector representation dimensionality, the LSTM hidden state dimensionality (both {32, 48, 64, 128, 256}), the initial learning rate ({0.1, 0.01, 0.001}), and the update method (SGD and Adam, Kingma and Ba, 2015). For each corpus, the highest-accuracy combination of these hyperparameters is selected using development data or ten-fold cross validation, which will be specified in section 5."
    }, {
      "heading" : "5 Datasets",
      "text" : "We selected five datasets of different sizes and corresponding to varying categorization tasks. Some information about these datasets are summarized in Table 1.\nSentiment analysis on Yelp reviews. Originally from the Yelp Dataset Challenge in 2015, this dataset contains 1.5 million examples. We used the preprocessed dataset from Zhang et al. (2015), which has 650,000 training and 50,000 test examples. The task is to predict an ordinal rating (1–5) from the text of the review. To select the best combination of hyperparameters, we randomly sampled 10% training examples as the development data. We compared with hierarchical attention networks (Yang et al., 2016), which use the normalized attention mechanism on both word and sentence layers with a flat document structure, and provide the state-of-the-art result on this corpus.\nFraming dimensions in news articles. The Media Frames Corpus (MFC; Card et al., 2015) includes around 4,200 news articles about immigration from 13 U.S. newspapers over the years 1980–2012. The annotations of these articles are in terms of a set of 15 general-purpose labels, such as ECONOMICS and MORALITY, designed to categorize the emphasis framing applied to the\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nNumber of docs.\nDataset Task Classes Total Training Development Test Vocab. size\nYelp Sentiment 5 700K 650K – 50K 10K MFC Frames 15 4.2K – – – 7.5K Debates Vote 2 1.6K 1,135 105 403 5K Movies Sentiment 2 2.0K – – – 5K Bills Survival 2 52K 46K – 6K 10K\nTable 1: Information about the five datasets used in our experiments. To compare with prior work, we use different experimental settings. For Yelp and Bill corpora, we use 10% of the training examples as development data. For MFC and Movies corpora, we use 10-fold cross validation and report averages across all folds.\nimmigration issue within the articles. We focused on predicting the single primary frame of each article. The state-of-the-art result on this corpus is from Card et al. (2016), where they used logistic regression together with unigrams, bigrams and Bamman-style personas (Bamman et al., 2014) as features. The best feature combination in their model alongside other hyperparameters was identified by a Bayesian optimization method (Bergstra et al., 2015). To select hyperparameters, we used a small set of examples from the corpus as a development set. Then, we report average accuracy across 10-fold cross validation as in (Card et al., 2016).\nCongressional floor debates. The corpus was originally collected by Thomas et al. (2006), and the data split we used was constructed by Yessenalina et al. (2010). The goal is to predict the vote (“yea” or “nay”) for the speaker of each speech segment. The most recent work on this corpus is from Yogatama and Smith (2014), which proposed structured regularization methods based on linguistic components, e.g., sentences, topics, and syntactic parses. Each regularization method induces a linguistic bias to improve text classification accuracy, where the best result we repeated here is from the model with sentence regularizers.\nMovie reviews. This classic movie review corpus was constructed by Pang and Lee (2004) and includes 1,000 positive and 1,000 negative reviews. On this corpus, we used the standard tenfold data split for cross validation and reported the average accuracy across folds. We compared with the work from both Bhatia et al. (2015) and Hogenboom et al. (2015), which are two recent works on discourse for sentiment analysis. Bha-\ntia et al. (2015) used a hand-crafted weighting scheme to bias the bag-of-word representations on sentences. Hogenboom et al. (2015) also considered manually-designed weighting schemes and a lexicon-based model as classifier, achieving performance inferior to fully-supervised methods like Bhatia et al. (2015) and ours.\nCongressional bill corpus. This corpus, collected by Yano et al. (2012), includes 51,762 legislative bills from the 103rd to 111th U.S. Congresses. The task is to predict whether a bill will survive based on its content. We randomly sampled 10% training examples as development data to search for the best hyperparameters. To our knowledge, the best published results are due to Yogatama and Smith (2014), which is the same baseline as for the congressional floor debates corpus."
    }, {
      "heading" : "6 Experiments",
      "text" : "We evaluated all variants of our model on the five datasets presented in section 5, comparing in each case to the published state of the art as well as the most relevant works.\nResults. See Table 2. On four out of five datasets, our UNLABELED model (line 8) outperforms past methods. In the case of the very large Yelp dataset, our FULL model (line 9) gives even stronger performance, but not elsewhere, suggesting that it is overparameterized for the smaller datasets. Indeed, on the MFC and Movies tasks, the discourse-ignorant ADDITIVE outperforms the FULL model. On these datasets, the selected FULL model had nearly 20 times as many parameters as the UNLABELED model, which in turn had twice as many parameters as the ADDITIVE.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nMethod Yelp MFC Debates Movies Bills\nPrior work 1. Yang et al. (2016) 71.0 — — — — 2. Card et al. (2016) — 56.8 — — — 3. Yogatama and Smith (2014) — — 74.0 — 88.5 4. Bhatia et al. (2015) — — — 82.9 — 5. Hogenboom et al. (2015) — — — 71.9 — Variants of our model 6. ADDITIVE 68.5 57.6 69.0 82.7 80.1 7. ROOT 54.3 51.2 60.3 68.7 70.5 8. UNLABELED 71.3 58.4 75.7 83.1 78.4 9. FULL 71.8 56.3 74.2 79.5 77.0\nTable 2: Test-set accuracy across five datasets. Results from prior work are reprinted from the corresponding publications. Boldface marks performance stronger than the previous state of the art.\nThis finding demonstrates the benefit of explicit discourse structure—even the output from an imperfect parser—for text categorization in some genres. Even though the discourse parser is trained on news text, it still offers benefit to restaurant and movie reviews and to the genre of congressional debates. Even for news text, if the training dataset is small (e.g., MFC), a lighter-weight variant of discourse (UNLABELED) is preferred.\nLegislative bills, which have technical legal content and highly specialized conventions (see Appendix A in the supplementary material for an example), are arguably the most distant genre from news among those we considered. On that task, we see discourse working against accuracy. Note that the corpus of bills is more than ten times larger than three cases where our UNLABELED model outperformed past methods, suggesting that the drop in performance is not due to lack of data.\nIt is also important to notice that the ROOT model performs quite poorly in all cases. This implies that discourse structure is not simply helping by finding a single EDU upon which to make the categorization decision.\nQualitative analysis. Figure 3 shows some example texts from the Yelp Review corpus with their discourse structures produced by DPLP, where the weights were generated with the FULL model. Figure 3(a) and 3(b) are two successful examples of the FULL model. Figure 3(a) shows a simple case with respect to the discourse structure. Figure 3(b) is slightly different—the text in this example may have more than one reasonable discourse structure, e.g., 2D could be a child of\n2C instead of 2A. In both cases, discourse structures help the FULL model bias to the important sentences.\nFigure 3(c), on the other hand, presents a negative example, where DPLP failed to identify the most salient sentence 3F . In addition, the weights produced by the FULL model do not make much sense, which we suspect the model was confused by the structure. Figure 3(c) also presents a manually-constructed discourse structure on the same text for reference.\nEffect of parsing performance. A natural question is whether further improvements to RST discourse parsing would lead to even greater gains in text categorization. While advances in discourse parsing are beyond the scope of this paper, we can gain some insight by exploring degradation to the DPLP parser. An easy way to do this is to train it on subsets of the RST discourse treebank. We repeated the conditions described above for our FULL model, training DPLP on 25%, 50%, and 75% of the training set (randomly selected in each case) before re-parsing the data for the sentiment analysis task. We did not repeat the hyperparameter search. In Figure 4, we plot accuracy of the classifier (y-axis) against the F1 performance of the discourse parser (x-axis). Unsurprisingly, lower parsing performance implies lower classification accuracy. Notably, if the RST discourse treebank were reduced to 25% of its size, our method would underperform the discourseignorant model of Yang et al. (2016). While we cannot extrapolate with certainty, these findings suggest that further improvements to discourse\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nFrom DPLP:\n1A 1B 1C\n0.66\nElaboration\n0.67\nCause\n[This store is somewhat convenient but I can never find\nany workers,]1A [it drives me crazy.]1B [I never come\nhere on the weekends or around holidays anymore.]1C\n(a) true label: 2, predicted label: 2\nFrom DPLP:\n2A2B2C 2D 0.87\nEvaluation\n0.61\nElaboration\n0.70\nEvaluation\n[I love these people.] 2A [They are very friendly and always ask about my\nlife.] 2B [They remember things I tell them then ask about it the next time\nI’m in.] 2C [Patrick and Lily are the best but everyone there is wonderful in\ntheir own ways.]\n2D\n(b) true label: 5, predicted label: 5\nFrom DPLP:\n3B\n3A 3C 3D 3E 3F\n0.47\nElaboration\n0.32\nElaboration\n0.62\nElaboration\n0.16\nElaboration\n0.32\nAttribution\nManually constructed:\n3F\n3B3A\n3C 3D\n3E\nCause\nBackground\nExplanation Explanation\nExplanation\n[We use to visit this pub 10 years ago because they had a nice english waitress and excellent fish and chips for the price.]3A [However we went back a few weeks ago and were disappointed.]3B [The price of the fish and chip dinner went up and they cut the portion in half.]3C [No one assisted us in putting two tables together we had to do it ourselves.]3D [Two guests wanted a good English hot tea and they didn’t brew it in advance.]3E [So we’ve decided there are newer and better places to eat fish and chips especially up in north phoenix.]3F\n(c) true label: 1, predicted label: 3\nFigure 3: Some example texts (with light revision for readability) from the Yelp Review corpus and their corresponding dependency discourse parses. The numbers on dependency edges are attention weights produced by the FULL model.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nFigure 4: Varying the amount of training data for the discourse parser, we can see how parsing F1 performance affects accuracy on the Yelp review task.\nparsing, through larger annotated datasets or improved models, could lead to greater gains.\nAttention mechanism. In section 3, we contrasted our new attention mechanism (Equation 2), which is inspired by RST’s lack of “competition” for salience among satellites, with the attention mechanism used in machine translation (Bahdanau et al., 2015). We consider here a variant of our model with normalized attention:\n↵0 i = softmax\n0\nBB@\n2\n664 ... v> j\n...\n3\n775\nj2children(i)\nW ↵ · e i\n1\nCCA .\n(4) The result here is a vector↵0\ni , with one element for each child node j 2 children(i), and which sums to one.\nThis variant of the FULL model achieves 70.3% accuracy, giving empirical support to our theoretically-motivated design decision not to normalize attention. Of course, further architecture improvements may yet be possible."
    }, {
      "heading" : "7 Related Work",
      "text" : "Early work on text categorization often treated text as a bag of words (e.g., Joachims, 1998; Yang and Pedersen, 1997). Representation learning, for example through matrix decomposition (Deerwester et al., 1990) or latent topic variables (Ramage et al., 2009), has been considered to avoid overfitting in the face of sparse data.\nThe assumption that all parts of a text should influence categorization equally persists even as\nmore powerful representation learners are considered. Zhang et al. (2015) treat a text as a sequence of characters, proposing to a deep convolutional neural network to build text representation. Xiao and Cho (2016) extended that architecture by inserting a recurrent neural network layer between the convolutional layer and the classification layer.\nIn contrast, our contributions follow Ko et al. (2004), who sought to weight the influence of different parts of an input text on the task. Two works that sought to learn the importance of sentences in a document are Yessenalina et al. (2010) and Yang et al. (2016). The former used a latent variable for the informativeness of each sentence, and the latter used a neural network to learn an attention function. Neither used any linguistic bias, relying only on task supervision to discover the latent variable distribution or attention function. Our work builds the neural network directly on a discourse dependency tree, favoring the most central EDUs over the others but giving the model the ability to overcome this bias.\nAnother way to use linguistic information was presented by Yogatama and Smith (2014), who used a bag-of-words model. The novelty in their approach was a data-driven regularization method that encouraged the model to collectively ignore groups of features found to coocur. Most related to our work is their “sentence regularizer,” which encouraged the model to try to ignore training-set sentences that were not informative for the task. Discourse structure was not considered.\nDiscourse for sentiment analysis. Recently, discourse structure has been considered for sentiment analysis, which can be cast as a text categorization problem. Bhatia et al. (2015) proposed two discourse-motivated models for sentiment polarity prediction. One of the models is also based on discourse dependency trees, but using a handcrafted weighting scheme. Our method’s attention mechanism automates the weighting."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We conclude that automatically-derived discourse structure is often helpful to categorization of many kinds of text, and the benefit increases with the accuracy of discourse parsing. This effect reverses for legislative bills, a text genre whose discourse structure diverges from that of news. These findings motivate further improvements to discourse parsing, especially for new genres.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Argumentative text as rhetorical structure: An application of rhetorical structure theory",
      "author" : [ "Moshe Azar." ],
      "venue" : "Argumentation 13(1):97–114.",
      "citeRegEx" : "Azar.,? 1999",
      "shortCiteRegEx" : "Azar.",
      "year" : 1999
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning latent personas of film characters. In Annual Meeting of the Association for Computational Linguistics (ACL)",
      "author" : [ "David Bamman", "Brendan O’Connor", "Noah A Smith" ],
      "venue" : null,
      "citeRegEx" : "Bamman et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bamman et al\\.",
      "year" : 2014
    }, {
      "title" : "Hyperopt: a python library for model selection and hyperparameter optimization",
      "author" : [ "James Bergstra", "Brent Komer", "Chris Eliasmith", "Dan Yamins", "David D Cox." ],
      "venue" : "Computational Science & Discovery 8(1).",
      "citeRegEx" : "Bergstra et al\\.,? 2015",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2015
    }, {
      "title" : "Better document-level sentiment analysis from rst discourse parsing",
      "author" : [ "Parminder Bhatia", "Yangfeng Ji", "Jacob Eisenstein." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Bhatia et al\\.,? 2015",
      "shortCiteRegEx" : "Bhatia et al\\.",
      "year" : 2015
    }, {
      "title" : "The media frames corpus: Annotations of frames across issues",
      "author" : [ "Dallas Card", "Amber E Boydstun", "Justin H Gross", "Philip Resnik", "Noah A Smith." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-",
      "citeRegEx" : "Card et al\\.,? 2015",
      "shortCiteRegEx" : "Card et al\\.",
      "year" : 2015
    }, {
      "title" : "Analyzing framing through the casts of characters in the news",
      "author" : [ "Dallas Card", "Justin Gross", "Amber Boydstun", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computa-",
      "citeRegEx" : "Card et al\\.,? 2016",
      "shortCiteRegEx" : "Card et al\\.",
      "year" : 2016
    }, {
      "title" : "Building a Discourse-tagged Corpus in the Framework of Rhetorical Structure Theory",
      "author" : [ "Lynn Carlson", "Daniel Marcu", "Mary Ellen Okurowski." ],
      "venue" : "Proceedings of Second SIGdial Workshop on Discourse and Dialogue.",
      "citeRegEx" : "Carlson et al\\.,? 2001",
      "shortCiteRegEx" : "Carlson et al\\.",
      "year" : 2001
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "Scott Deerwester", "Susan T Dumais", "George W Furnas", "Thomas K Landauer", "Richard Harshman." ],
      "venue" : "Journal of the American society for information science 41(6):391.",
      "citeRegEx" : "Deerwester et al\\.,? 1990",
      "shortCiteRegEx" : "Deerwester et al\\.",
      "year" : 1990
    }, {
      "title" : "Using rhetorical structure in sentiment analysis",
      "author" : [ "Alexander Hogenboom", "Flavius Frasincar", "Franciska de Jong", "Uzay Kaymak." ],
      "venue" : "Communications of the ACM 58(7):69–77.",
      "citeRegEx" : "Hogenboom et al\\.,? 2015",
      "shortCiteRegEx" : "Hogenboom et al\\.",
      "year" : 2015
    }, {
      "title" : "Representation learning for document-level discourse parsing",
      "author" : [ "Yangfeng Ji", "Jacob Eisenstein." ],
      "venue" : "ACL.",
      "citeRegEx" : "Ji and Eisenstein.,? 2014",
      "shortCiteRegEx" : "Ji and Eisenstein.",
      "year" : 2014
    }, {
      "title" : "One Vector is Not Enough: Entity-Augmented Distributed Semantics for Discourse Relations",
      "author" : [ "Yangfeng Ji", "Jacob Eisenstein." ],
      "venue" : "Transactions of the Association of Computational Linguistics 3:329–344.",
      "citeRegEx" : "Ji and Eisenstein.,? 2015",
      "shortCiteRegEx" : "Ji and Eisenstein.",
      "year" : 2015
    }, {
      "title" : "Text categorization with support vector machines: Learning with many relevant features",
      "author" : [ "Thorsten Joachims." ],
      "venue" : "European conference on machine learning. Springer, pages 137–142.",
      "citeRegEx" : "Joachims.,? 1998",
      "shortCiteRegEx" : "Joachims.",
      "year" : 1998
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom." ],
      "venue" : "arXiv preprint arXiv:1404.2188 .",
      "citeRegEx" : "Kalchbrenner et al\\.,? 2014",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "International Conference on Learning Representations.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Improving text categorization using the importance of sentences",
      "author" : [ "Youngjoong Ko", "Jinwoo Park", "Jungyun Seo." ],
      "venue" : "Information Processing & Management 40(1):65–79.",
      "citeRegEx" : "Ko et al\\.,? 2004",
      "shortCiteRegEx" : "Ko et al\\.",
      "year" : 2004
    }, {
      "title" : "What do recurrent neural network grammars learn about syntax? In EACL",
      "author" : [ "Adhiguna Kuncoro", "Miguel Ballesteros", "Lingpeng Kong", "Chris Dyer", "Graham Neubig", "Noah A. Smith" ],
      "venue" : null,
      "citeRegEx" : "Kuncoro et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kuncoro et al\\.",
      "year" : 2017
    }, {
      "title" : "Rhetorical Structure Theory: Toward a Functional Theory of Text Organization",
      "author" : [ "William Mann", "Sandra Thompson." ],
      "venue" : "Text 8(3):243–281.",
      "citeRegEx" : "Mann and Thompson.,? 1988",
      "shortCiteRegEx" : "Mann and Thompson.",
      "year" : 1988
    }, {
      "title" : "Discourse trees are good indicators of importance in text",
      "author" : [ "Daniel Marcu." ],
      "venue" : "Advances in automatic text summarization pages 123–136.",
      "citeRegEx" : "Marcu.,? 1999",
      "shortCiteRegEx" : "Marcu.",
      "year" : 1999
    }, {
      "title" : "Building a large annotated corpus of english: The penn treebank",
      "author" : [ "Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini." ],
      "venue" : "Computational linguistics 19(2):313–330.",
      "citeRegEx" : "Marcus et al\\.,? 1993",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1993
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur." ],
      "venue" : "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association,",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Dynet: The dynamic neural network toolkit",
      "author" : [ "Graham Neubig", "Chris Dyer", "Yoav Goldberg", "Austin Matthews", "Waleed Ammar", "Antonios Anastasopoulos", "Miguel Ballesteros", "David Chiang", "Daniel Clothiaux", "Trevor Cohn" ],
      "venue" : null,
      "citeRegEx" : "Neubig et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Neubig et al\\.",
      "year" : 2017
    }, {
      "title" : "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Proceedings of the 42nd annual meeting on Association for Computational Linguistics. Association for Computational",
      "citeRegEx" : "Pang and Lee.,? 2004",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2004
    }, {
      "title" : "Global Belief Recursive Neural Networks",
      "author" : [ "Romain Paulus", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 2888–2896.",
      "citeRegEx" : "Paulus et al\\.,? 2014",
      "shortCiteRegEx" : "Paulus et al\\.",
      "year" : 2014
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Labeled lda: A supervised topic model for credit attribution in multilabeled corpora",
      "author" : [ "Daniel Ramage", "David Hall", "Ramesh Nallapati", "Christopher D Manning." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Ramage et al\\.,? 2009",
      "shortCiteRegEx" : "Ramage et al\\.",
      "year" : 2009
    }, {
      "title" : "Grounded compositional semantics for finding and describing images with sentences",
      "author" : [ "Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng." ],
      "venue" : "Transactions of the Association for Computational Linguistics",
      "citeRegEx" : "Socher et al\\.,? 2014",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2014
    }, {
      "title" : "Get out the vote: Determining support or opposition from congressional floor-debate transcripts",
      "author" : [ "Matt Thomas", "Bo Pang", "Lillian Lee." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Thomas et al\\.,? 2006",
      "shortCiteRegEx" : "Thomas et al\\.",
      "year" : 2006
    }, {
      "title" : "Efficient character-level document classification by combining convolution and recurrent layers",
      "author" : [ "Yijun Xiao", "Kyunghyun Cho." ],
      "venue" : "arXiv preprint arXiv:1602.00367 .",
      "citeRegEx" : "Xiao and Cho.,? 2016",
      "shortCiteRegEx" : "Xiao and Cho.",
      "year" : 2016
    }, {
      "title" : "Statistical dependency analysis with support vector machines",
      "author" : [ "H. Yamada", "Y. Matsumoto." ],
      "venue" : "IWPT .",
      "citeRegEx" : "Yamada and Matsumoto.,? 2003",
      "shortCiteRegEx" : "Yamada and Matsumoto.",
      "year" : 2003
    }, {
      "title" : "A comparative study on feature selection in text categorization",
      "author" : [ "Yiming Yang", "Jan O. Pedersen." ],
      "venue" : "Fourteenth International Conference on Machine Learning.",
      "citeRegEx" : "Yang and Pedersen.,? 1997",
      "shortCiteRegEx" : "Yang and Pedersen.",
      "year" : 1997
    }, {
      "title" : "Hierarchical attention networks for document classification",
      "author" : [ "Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "Textual predictors of bill survival in congressional committees",
      "author" : [ "Tae Yano", "Noah A Smith", "John D Wilkerson." ],
      "venue" : "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan-",
      "citeRegEx" : "Yano et al\\.,? 2012",
      "shortCiteRegEx" : "Yano et al\\.",
      "year" : 2012
    }, {
      "title" : "Multi-level structured models for document sentiment classification",
      "author" : [ "Ainur Yessenalina", "Yisong Yue", "Claire Cardie." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Yessenalina et al\\.,? 2010",
      "shortCiteRegEx" : "Yessenalina et al\\.",
      "year" : 2010
    }, {
      "title" : "Linguistic structured sparsity in text categorization",
      "author" : [ "Dani Yogatama", "Noah A Smith." ],
      "venue" : "ACL.",
      "citeRegEx" : "Yogatama and Smith.,? 2014",
      "shortCiteRegEx" : "Yogatama and Smith.",
      "year" : 2014
    }, {
      "title" : "Dependency-based Discourse Parser for Single-Document Summarization",
      "author" : [ "Yasuhisa Yoshida", "Jun Suzuki", "Tsutomu Hirao", "Masaaki Nagata." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Yoshida et al\\.,? 2014",
      "shortCiteRegEx" : "Yoshida et al\\.",
      "year" : 2014
    }, {
      "title" : "Using “annotator rationales” to improve machine learning for text categorization",
      "author" : [ "Omar Zaidan", "Jason Eisner", "Christine D Piatko." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Zaidan et al\\.,? 2007",
      "shortCiteRegEx" : "Zaidan et al\\.",
      "year" : 2007
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 649–657.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Several past researchers have noticed that methods that reason about the relative saliency or importance of passages within a text can lead to improvements (Ko et al., 2004).",
      "startOffset" : 156,
      "endOffset" : 173
    }, {
      "referenceID" : 33,
      "context" : "Latent variables (Yessenalina et al., 2010), structured-sparse regularizers (Yogatama and Smith, 2014), and neural attention models (Yang et al.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 34,
      "context" : ", 2010), structured-sparse regularizers (Yogatama and Smith, 2014), and neural attention models (Yang et al.",
      "startOffset" : 40,
      "endOffset" : 66
    }, {
      "referenceID" : 31,
      "context" : ", 2010), structured-sparse regularizers (Yogatama and Smith, 2014), and neural attention models (Yang et al., 2016) have all been explored.",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 4,
      "context" : "Some promising results on sentiment classification tasks support this idea: Bhatia et al. (2015) and Hogenboom et al.",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "Some promising results on sentiment classification tasks support this idea: Bhatia et al. (2015) and Hogenboom et al. (2015) applied hand-crafted weighting schemes to the sentences in a document, based on its discourse structure, and showed benefit to sentiment polarity classification.",
      "startOffset" : 76,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "Figure 1: A manually constructed example of the RST (Mann and Thompson, 1988) discourse structure on a text.",
      "startOffset" : 52,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "automatically-derived document parse from a topperforming, open-source discourse parser, DPLP (Ji and Eisenstein, 2014).",
      "startOffset" : 94,
      "endOffset" : 119
    }, {
      "referenceID" : 17,
      "context" : "Rhetorical Structure Theory (RST; Mann and Thompson, 1988) is a theory of discourse that has enjoyed popularity in NLP.",
      "startOffset" : 28,
      "endOffset" : 58
    }, {
      "referenceID" : 18,
      "context" : "The information embedded in RST trees has motivated many applications in NLP research, including document summarization (Marcu, 1999), argumentation mining (Azar, 1999), and sentiment analysis (Bhatia et al.",
      "startOffset" : 120,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : "The information embedded in RST trees has motivated many applications in NLP research, including document summarization (Marcu, 1999), argumentation mining (Azar, 1999), and sentiment analysis (Bhatia et al.",
      "startOffset" : 156,
      "endOffset" : 168
    }, {
      "referenceID" : 4,
      "context" : "The information embedded in RST trees has motivated many applications in NLP research, including document summarization (Marcu, 1999), argumentation mining (Azar, 1999), and sentiment analysis (Bhatia et al., 2015).",
      "startOffset" : 193,
      "endOffset" : 214
    }, {
      "referenceID" : 10,
      "context" : "In this work, we use a state-of-the-art open-source RST-style discourse parser, DPLP (Ji and Eisenstein, 2014).",
      "startOffset" : 85,
      "endOffset" : 110
    }, {
      "referenceID" : 35,
      "context" : "2 We follow recent work that suggests transforming the RST tree into a dependency structure (Yoshida et al., 2014).",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 26,
      "context" : "Our approach is analogous to (and inspired by) the use of recursive neural networks on syntactic dependency trees, with word embeddings at the leaves (Socher et al., 2014).",
      "startOffset" : 150,
      "endOffset" : 171
    }, {
      "referenceID" : 23,
      "context" : "There is extensive recent work on architectures for embedding representations of sentences and other short pieces of text, including, for example, (bi)recursive neural networks (Paulus et al., 2014) and convolutional neural networks (Kalchbrenner et al.",
      "startOffset" : 177,
      "endOffset" : 198
    }, {
      "referenceID" : 13,
      "context" : ", 2014) and convolutional neural networks (Kalchbrenner et al., 2014).",
      "startOffset" : 42,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "Our attention mechanism differs from prior work (Bahdanau et al., 2015), in which attention weights are normalized to sum to one across competing candidates for attention.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 16,
      "context" : "It also differs from attention in composition functions used in syntactic parsing (Kuncoro et al., 2017), where attention can mimic head rules that follow from an endocentricity hypothesis of syntactic phrase representation.",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "This idea of using a weighting scheme along with discourse structure is explored in prior works (Bhatia et al., 2015; Hogenboom et al., 2015), although they are manually designed, rather than learned from training data.",
      "startOffset" : 96,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "This idea of using a weighting scheme along with discourse structure is explored in prior works (Bhatia et al., 2015; Hogenboom et al., 2015), although they are manually designed, rather than learned from training data.",
      "startOffset" : 96,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "This UNLABELED model is similar to the depth weighting scheme introduced by Bhatia et al. (2015), which also uses an unlabeled discourse dependency tree, but our attention weights are computed by a function whose parameters are learned.",
      "startOffset" : 76,
      "endOffset" : 97
    }, {
      "referenceID" : 4,
      "context" : "This UNLABELED model is similar to the depth weighting scheme introduced by Bhatia et al. (2015), which also uses an unlabeled discourse dependency tree, but our attention weights are computed by a function whose parameters are learned. This approach sits squarely between Bhatia et al. (2015) and the flat document structure used by Yang et al.",
      "startOffset" : 76,
      "endOffset" : 294
    }, {
      "referenceID" : 4,
      "context" : "This UNLABELED model is similar to the depth weighting scheme introduced by Bhatia et al. (2015), which also uses an unlabeled discourse dependency tree, but our attention weights are computed by a function whose parameters are learned. This approach sits squarely between Bhatia et al. (2015) and the flat document structure used by Yang et al. (2016); the UNLABELED model still uses discourse to bias the model toward some content (that which is closer to the tree’s root).",
      "startOffset" : 76,
      "endOffset" : 353
    }, {
      "referenceID" : 35,
      "context" : "This model variant is motivated by work on document summarization (Yoshida et al., 2014), where the",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 31,
      "context" : "i } are built in a different way from the work of Yang et al. (2016), this model is quite similar to their HN-AVE model on building document representations.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 21,
      "context" : "We implement our learning procedure with the DyNet package (Neubig et al., 2017).",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 7,
      "context" : "We used DPLP, the RST parser from Ji and Eisenstein (2014), which is one of the best discourse parsers on the RST discourse treebank benchmark (Carlson et al., 2001).",
      "startOffset" : 143,
      "endOffset" : 165
    }, {
      "referenceID" : 19,
      "context" : "DPLP as distributed is trained on 347 Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993).",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 9,
      "context" : "We used DPLP, the RST parser from Ji and Eisenstein (2014), which is one of the best discourse parsers on the RST discourse treebank benchmark (Carlson et al.",
      "startOffset" : 34,
      "endOffset" : 59
    }, {
      "referenceID" : 7,
      "context" : "We used DPLP, the RST parser from Ji and Eisenstein (2014), which is one of the best discourse parsers on the RST discourse treebank benchmark (Carlson et al., 2001). It employs a greedy decoding algorithm for parsing, producing 2,000 parses per minute on average on a single CPU. DPLP provides discourse segmentation, breaking a text into EDUs, typically clauses or sentences, based on syntactic parses provided by Stanford CoreNLP. RST trees are converted to dependencies following the method of Yoshida et al. (2014). DPLP as distributed is trained on 347 Wall Street Journal articles from the Penn Treebank (Marcus et al.",
      "startOffset" : 144,
      "endOffset" : 520
    }, {
      "referenceID" : 24,
      "context" : "In cases where there are 10,000 or fewer training examples, we used pretrained GloVe word embeddings (Pennington et al., 2014), following previous work on neural discourse processing (Ji and Eisenstein, 2015).",
      "startOffset" : 101,
      "endOffset" : 126
    }, {
      "referenceID" : 11,
      "context" : ", 2014), following previous work on neural discourse processing (Ji and Eisenstein, 2015).",
      "startOffset" : 64,
      "endOffset" : 89
    }, {
      "referenceID" : 31,
      "context" : "We compared with hierarchical attention networks (Yang et al., 2016), which use the normalized attention mechanism on both word and sentence layers with a flat document structure, and provide the state-of-the-art result on this corpus.",
      "startOffset" : 49,
      "endOffset" : 68
    }, {
      "referenceID" : 36,
      "context" : "We used the preprocessed dataset from Zhang et al. (2015), which has 650,000 training and 50,000 test examples.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "The Media Frames Corpus (MFC; Card et al., 2015) includes around 4,200 news articles about immigration from 13 U.",
      "startOffset" : 24,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "(2016), where they used logistic regression together with unigrams, bigrams and Bamman-style personas (Bamman et al., 2014) as features.",
      "startOffset" : 102,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "The best feature combination in their model alongside other hyperparameters was identified by a Bayesian optimization method (Bergstra et al., 2015).",
      "startOffset" : 125,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "Then, we report average accuracy across 10-fold cross validation as in (Card et al., 2016).",
      "startOffset" : 71,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "The state-of-the-art result on this corpus is from Card et al. (2016), where they used logistic regression together with unigrams, bigrams and Bamman-style personas (Bamman et al.",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 27,
      "context" : "The corpus was originally collected by Thomas et al. (2006), and the data split we used was constructed by Yessenalina et al.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 27,
      "context" : "The corpus was originally collected by Thomas et al. (2006), and the data split we used was constructed by Yessenalina et al. (2010). The goal is to predict the vote (“yea” or “nay”) for the speaker of each speech segment.",
      "startOffset" : 39,
      "endOffset" : 133
    }, {
      "referenceID" : 27,
      "context" : "The corpus was originally collected by Thomas et al. (2006), and the data split we used was constructed by Yessenalina et al. (2010). The goal is to predict the vote (“yea” or “nay”) for the speaker of each speech segment. The most recent work on this corpus is from Yogatama and Smith (2014), which proposed structured regularization methods based on linguistic components, e.",
      "startOffset" : 39,
      "endOffset" : 293
    }, {
      "referenceID" : 20,
      "context" : "This classic movie review corpus was constructed by Pang and Lee (2004) and includes 1,000 positive and 1,000 negative reviews.",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "We compared with the work from both Bhatia et al. (2015) and Hogenboom et al.",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "We compared with the work from both Bhatia et al. (2015) and Hogenboom et al. (2015), which are two recent works on discourse for sentiment analysis.",
      "startOffset" : 36,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "We compared with the work from both Bhatia et al. (2015) and Hogenboom et al. (2015), which are two recent works on discourse for sentiment analysis. Bhatia et al. (2015) used a hand-crafted weighting scheme to bias the bag-of-word representations on sentences.",
      "startOffset" : 36,
      "endOffset" : 171
    }, {
      "referenceID" : 4,
      "context" : "We compared with the work from both Bhatia et al. (2015) and Hogenboom et al. (2015), which are two recent works on discourse for sentiment analysis. Bhatia et al. (2015) used a hand-crafted weighting scheme to bias the bag-of-word representations on sentences. Hogenboom et al. (2015) also considered manually-designed weighting schemes and a lexicon-based model as classifier, achieving performance inferior to fully-supervised methods like Bhatia et al.",
      "startOffset" : 36,
      "endOffset" : 286
    }, {
      "referenceID" : 4,
      "context" : "We compared with the work from both Bhatia et al. (2015) and Hogenboom et al. (2015), which are two recent works on discourse for sentiment analysis. Bhatia et al. (2015) used a hand-crafted weighting scheme to bias the bag-of-word representations on sentences. Hogenboom et al. (2015) also considered manually-designed weighting schemes and a lexicon-based model as classifier, achieving performance inferior to fully-supervised methods like Bhatia et al. (2015) and ours.",
      "startOffset" : 36,
      "endOffset" : 464
    }, {
      "referenceID" : 32,
      "context" : "This corpus, collected by Yano et al. (2012), includes 51,762 legislative bills from the 103rd to 111th U.",
      "startOffset" : 26,
      "endOffset" : 45
    }, {
      "referenceID" : 32,
      "context" : "This corpus, collected by Yano et al. (2012), includes 51,762 legislative bills from the 103rd to 111th U.S. Congresses. The task is to predict whether a bill will survive based on its content. We randomly sampled 10% training examples as development data to search for the best hyperparameters. To our knowledge, the best published results are due to Yogatama and Smith (2014), which is the same baseline as for the congressional floor debates corpus.",
      "startOffset" : 26,
      "endOffset" : 378
    }, {
      "referenceID" : 27,
      "context" : "Yang et al. (2016) 71.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : "Card et al. (2016) — 56.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : "Card et al. (2016) — 56.8 — — — 3. Yogatama and Smith (2014) — — 74.",
      "startOffset" : 0,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "Bhatia et al. (2015) — — — 82.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "Bhatia et al. (2015) — — — 82.9 — 5. Hogenboom et al. (2015) — — — 71.",
      "startOffset" : 0,
      "endOffset" : 61
    }, {
      "referenceID" : 31,
      "context" : "Notably, if the RST discourse treebank were reduced to 25% of its size, our method would underperform the discourseignorant model of Yang et al. (2016). While we cannot extrapolate with certainty, these findings suggest that further improvements to discourse",
      "startOffset" : 133,
      "endOffset" : 152
    }, {
      "referenceID" : 1,
      "context" : "In section 3, we contrasted our new attention mechanism (Equation 2), which is inspired by RST’s lack of “competition” for salience among satellites, with the attention mechanism used in machine translation (Bahdanau et al., 2015).",
      "startOffset" : 207,
      "endOffset" : 230
    }, {
      "referenceID" : 30,
      "context" : "Early work on text categorization often treated text as a bag of words (e.g., Joachims, 1998; Yang and Pedersen, 1997).",
      "startOffset" : 71,
      "endOffset" : 118
    }, {
      "referenceID" : 8,
      "context" : "Representation learning, for example through matrix decomposition (Deerwester et al., 1990) or latent topic variables (Ramage et al.",
      "startOffset" : 66,
      "endOffset" : 91
    }, {
      "referenceID" : 25,
      "context" : ", 1990) or latent topic variables (Ramage et al., 2009), has been considered to avoid overfitting in the face of sparse data.",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "Representation learning, for example through matrix decomposition (Deerwester et al., 1990) or latent topic variables (Ramage et al., 2009), has been considered to avoid overfitting in the face of sparse data. The assumption that all parts of a text should influence categorization equally persists even as more powerful representation learners are considered. Zhang et al. (2015) treat a text as a sequence of characters, proposing to a deep convolutional neural network to build text representation.",
      "startOffset" : 67,
      "endOffset" : 381
    }, {
      "referenceID" : 8,
      "context" : "Representation learning, for example through matrix decomposition (Deerwester et al., 1990) or latent topic variables (Ramage et al., 2009), has been considered to avoid overfitting in the face of sparse data. The assumption that all parts of a text should influence categorization equally persists even as more powerful representation learners are considered. Zhang et al. (2015) treat a text as a sequence of characters, proposing to a deep convolutional neural network to build text representation. Xiao and Cho (2016) extended that architecture by inserting a recurrent neural network layer between the convolutional layer and the classification layer.",
      "startOffset" : 67,
      "endOffset" : 522
    }, {
      "referenceID" : 8,
      "context" : "Representation learning, for example through matrix decomposition (Deerwester et al., 1990) or latent topic variables (Ramage et al., 2009), has been considered to avoid overfitting in the face of sparse data. The assumption that all parts of a text should influence categorization equally persists even as more powerful representation learners are considered. Zhang et al. (2015) treat a text as a sequence of characters, proposing to a deep convolutional neural network to build text representation. Xiao and Cho (2016) extended that architecture by inserting a recurrent neural network layer between the convolutional layer and the classification layer. In contrast, our contributions follow Ko et al. (2004), who sought to weight the influence of different parts of an input text on the task.",
      "startOffset" : 67,
      "endOffset" : 712
    }, {
      "referenceID" : 8,
      "context" : "Representation learning, for example through matrix decomposition (Deerwester et al., 1990) or latent topic variables (Ramage et al., 2009), has been considered to avoid overfitting in the face of sparse data. The assumption that all parts of a text should influence categorization equally persists even as more powerful representation learners are considered. Zhang et al. (2015) treat a text as a sequence of characters, proposing to a deep convolutional neural network to build text representation. Xiao and Cho (2016) extended that architecture by inserting a recurrent neural network layer between the convolutional layer and the classification layer. In contrast, our contributions follow Ko et al. (2004), who sought to weight the influence of different parts of an input text on the task. Two works that sought to learn the importance of sentences in a document are Yessenalina et al. (2010) and Yang et al.",
      "startOffset" : 67,
      "endOffset" : 900
    }, {
      "referenceID" : 8,
      "context" : "Representation learning, for example through matrix decomposition (Deerwester et al., 1990) or latent topic variables (Ramage et al., 2009), has been considered to avoid overfitting in the face of sparse data. The assumption that all parts of a text should influence categorization equally persists even as more powerful representation learners are considered. Zhang et al. (2015) treat a text as a sequence of characters, proposing to a deep convolutional neural network to build text representation. Xiao and Cho (2016) extended that architecture by inserting a recurrent neural network layer between the convolutional layer and the classification layer. In contrast, our contributions follow Ko et al. (2004), who sought to weight the influence of different parts of an input text on the task. Two works that sought to learn the importance of sentences in a document are Yessenalina et al. (2010) and Yang et al. (2016). The former used a latent variable for the informativeness of each sentence, and the latter used a neural network to learn an attention function.",
      "startOffset" : 67,
      "endOffset" : 923
    }, {
      "referenceID" : 8,
      "context" : "Representation learning, for example through matrix decomposition (Deerwester et al., 1990) or latent topic variables (Ramage et al., 2009), has been considered to avoid overfitting in the face of sparse data. The assumption that all parts of a text should influence categorization equally persists even as more powerful representation learners are considered. Zhang et al. (2015) treat a text as a sequence of characters, proposing to a deep convolutional neural network to build text representation. Xiao and Cho (2016) extended that architecture by inserting a recurrent neural network layer between the convolutional layer and the classification layer. In contrast, our contributions follow Ko et al. (2004), who sought to weight the influence of different parts of an input text on the task. Two works that sought to learn the importance of sentences in a document are Yessenalina et al. (2010) and Yang et al. (2016). The former used a latent variable for the informativeness of each sentence, and the latter used a neural network to learn an attention function. Neither used any linguistic bias, relying only on task supervision to discover the latent variable distribution or attention function. Our work builds the neural network directly on a discourse dependency tree, favoring the most central EDUs over the others but giving the model the ability to overcome this bias. Another way to use linguistic information was presented by Yogatama and Smith (2014), who used a bag-of-words model.",
      "startOffset" : 67,
      "endOffset" : 1468
    }, {
      "referenceID" : 4,
      "context" : "Bhatia et al. (2015) proposed two discourse-motivated models for sentiment polarity prediction.",
      "startOffset" : 0,
      "endOffset" : 21
    } ],
    "year" : 2017,
    "abstractText" : "We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.",
    "creator" : "LaTeX with hyperref package"
  }
}