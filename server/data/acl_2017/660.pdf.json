{
  "name" : "660.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Automatically Generating Rhythmic Verse with Neural Networks",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\nWe propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machinegenerated poem to be the best amongst all evaluated."
    }, {
      "heading" : "1 Introduction",
      "text" : "Poetry is an advanced form of linguistic communication, in which a message is conveyed that satisfies both aesthetic and semantic constraints. As poetry is one of the most expressive forms of language, the automatic creation of texts recognisable as poetry is difficult. In addition to requiring an understanding of many aspects of language including phonetic patterns such as rhyme, rhythm and alliteration, poetry composition also requires a deep understanding of the meaning of language.\nPoetry generation can be divided into two subtasks, namely the problem of content, which is concerned with a poem’s semantics, and the problem of form, which is concerned with the aesthetic rules that a poem follows. These rules may describe aspects of the literary devices used, and are usually highly prescriptive. Examples of different forms of poetry are limericks, ballads and sonnets. Limericks, for example, are characterised by their strict rhyme scheme (AABBA), their rhythm (two unstressed syllables followed by one stressed syllable) and their shorter third and fourth lines. Creating such poetry requires not only an understanding of the language itself, but also of how it sounds when spoken aloud. Statistical text generation usually requires the construction of a generative language model that explicitly learns the probability of any given word given previous context. Neural language models (Schwenk and Gauvain, 2005; Bengio et al., 2006) have garnered signficant research interest for their ability to learn complex syntactic and semantic representations of natural language (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014; Kim et al., 2015). Poetry generation is an interesting application, since performing this task automatically requires the creation of models that not only focus on what is being written (content), but also on how it is being written (form). We experiment with two novel methodologies for solving this task. The first involves training a model to learn an implicit representation of content and form through the use of a phonological encoding. The second involves training a generative language model to represent content, which is then constrained by a discriminative pronunciation model, representing form. This second model is of particular interest because poetry with arbitrary rhyme, rhythm, repetition and themes can be generated by tuning the pronunciation model.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199"
    }, {
      "heading" : "2 Related Work",
      "text" : "Automatic poetry generation is an important task due to the significant challenges involved. Most systems that have been proposed can loosely be categorised as rule-based expert systems, or statistical approaches. Rule-based poetry generation attempts include case-based reasoning (Gervás, 2000), templatebased generation (Colton et al., 2012), constraint satisfaction (Toivanen et al., 2013) and text mining (Netzer et al., 2009). These approaches are often inspired by how humans might generate poetry. Statistical approaches, conversely, make no assumptions about the creative process. Instead, they attempt to extract statistical patterns from existing poetry corpora in order to construct a language model, which can then be used generate new poetic variants (Yi et al., 2016; Greene et al., 2010). The work of Zhang and Lapata (2014) is similar to ours, in that they make use of neural language models. For the task of automatic generation of classical Chinese poetry, they were able to outperform all other Chinese poetry generation systems with both manual and automatic evaluation."
    }, {
      "heading" : "3 Phonetic-level Model",
      "text" : "Our first model is a pure neural language model, trained on a phonetic encoding of poetry in order to represent both form and content. Phonetic encodings of language represent information as sequences of around 40 basic acoustic symbols. Training on phonetic symbols allows the model to learn effective representations of pronunciation, including rhyme and rhythm. However, just training on a large corpus of poetry data is not enough. Specifically, two problems need to be overcome. 1) Phonetic encoding results in information loss: words that have the same pronunciation (homophones) cannot be perfectly reconstructed from the corresponding phonemes. This means that we require an additional probabilistic model in order to determine the most likely word given a sequence of phonemes. 2) The variety of poetry and poetic devices one can use— e.g., rhyme, rhythm, repetition—means that poems sampled from a model trained on all poetry would be unlikely to maintain internal consistency of meter and rhyme. It is therefore important to train the model on poetry which has its own internal consistency.\nThus, the model comprises three steps: transliterating an ortographic sequence to its phonetic representation, training a neural language model on the phonetic encoding, and decoding the generated sequence back from phonemes to orthographic symbols.\nPhonetic encoding To solve the first step, we apply a combination of word lookups from the CMU pronunciation dictionary (Weide, 2005) with letter-to-sound rules for handling out-ofvocabulary words. These rules are based on the CART techniques described by Black et al. (1998), and are represented with a simple Finite State Transducer1. The number of letters and number of phones in a word are rarely a one-to-one match: letters may match with up to three phones. In addition, virtually all letters can, in some contexts, map to zero phones, which is known as ‘wild’ or epsilon. Expectation Maximisation is used to compute the probability of a single letter matching a single phone, which is maximised through the application of Dynamic Time Warping (Myers et al., 1980) to determine the most likely position of epsilon characters. Although this approach offers full coverage over the training corpus—even for abbreviated words like ask’d and archaic words like renewest—it has several limitations. Irregularities in the English language result in difficulty determining general letter-to-sound rules that can manage words with unusual pronunciations such as “colonel” and “receipt” 2.\nIn addition to transliterating words into phoneme sequences, we also represent word break characters as a specific symbol. This makes decipherment, when converting back into an orthographic representation, much easier. Phonetic transliteration allows us to construct a phonetic poetry corpus comprising 1,046,536 phonemes.\nNeural language model We train a Long-Short Term Memory Network (Hochreiter and Schmidhuber, 1997) on the phonetic representation of our poetry corpus. The model is trained using stochastic gradient descent to predict the next phoneme given a sequence of phonemes. Specifically, we\n1Implemented using FreeTTS (Walker et al., 2010) 2An evaluation of models in American English, British English, German and French was undertaken by Black et al. (1998), who reported an externally validated per token accuracy on British English as low as 67%. Although no experiments were carried out on corpora of early-modern English, it is likely that this accuracy would be significantly lower.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nmaximize a multinomial logistic regression objective over the final softmax prediction. Each phoneme is represented as a 256-dimensional embedding, and the model consists of two hidden layers of size 256. We apply backpropagationthrough-time (Werbos, 1990) for 150 timesteps, which roughly equates to four lines of poetry in sonnet form. This allows the network to learn features like rhyme even when spread over multiple lines. Training is preemptively stopped at 25 epochs to prevent overfitting.\nOrthographic decoding When decoding from phonemes back to orthographic symbols, the goal is to compute the most likely word corresponding to a sequence of phonemes. That is, we compute the most probable hypothesis word W given a phoneme sequence ρ:\nargmaxi P (Wi | ρ ) (1)\nWe can consider the phonetic encoding of plaintext to be a homophonic cipher; that is, a cipher in which each symbol can correspond to one or more possible decodings. The problem of homophonic decipherment has received significant research attention in the past; with approaches utilising Expectation Maximisation (Knight et al., 2006), Integer Programming (Ravi and Knight, 2009) and A* search (Corlett and Penn, 2010). Transliteration from phonetic to an orthographic representation is done by constructing a Hidden Markov Model using the CMU pronunciation dictionary (Weide, 2005) and an n-gram language model. We calculate the transition probabilities (using the n-gram model) and the emission matrix (using the CMU pronunciation dictionary) to determine pronunciations that correspond to a single word. All pronunciations are naively considered equiprobable. We perform Viterbi decoding to find the most likely sequence of words. This means finding the most likely word wt+1 given a previous word sequence (wt−n, ..., wt).\nargmaxwt+1 P ( wt+1 | w1, ... , wt ) (2)\nIf a phonetic sequence does not map to any word, we apply the heuristic of artificially breaking the sequence up into two subsequences at index n, such that nmaximises the n-gram frequency of the subsequences.\nAnd humble and their fit flees are wits size but that one made and made thy step me lies\n————————————— Cool light the golden dark in any way the birds a shade a laughter turn away\n————————————— Then adding wastes retreating white as thine\nShe watched what eyes are breathing awe what shine —————————————\nBut sometimes shines so covered how the beak Alone in pleasant skies no more to seek\nFigure 1: Example output of the phonetic-level model trained on Iambic Pentameter poetry (grammatical errors are emphasised).\nOutput A popular form of poetry with strict internal structure is the sonnet. Popularised in English by Shakespeare, the sonnet is characterised by a strict rhyme scheme and exactly fourteen lines of Iambic Pentameter (Greene et al., 2010). Since the 17,134 word tokens in Shakespeare’s 153 sonnets are insufficient to train an effective model, we augment this corpus with poetry taken from the website sonnets.org, yielding a training set of 288,326 words and 1,563,457 characters. An example of the output when training on this sonnets corpus is provided in Figure 1. Not only is it mostly in strict Iambic Pentameter, but the grammar of the output is mostly correct and the poetry contains rhyme."
    }, {
      "heading" : "4 Constrained Character-level Model",
      "text" : "As the example shows, phonetic-level language models are effective at learning poetic form, despite small training sets and relatively few parameters. However, the fact that they require training data with internal poetic consistency implies that they do not generalise to other forms of poetry. That is, in order to generate poetry in Dactylic Hexameter (for example), a phonetic model must be trained on a corpus of Dactylic poetry. Not only is this impractical, but in many cases no corpus of adequate size even exists. Even when such poetic corpora are available, a new model must be trained for each type of poetry. This precludes tweaking the form of the output, which is important when generating poetry automatically. We now explore an alternative approach. Instead of attempting to represent both form and content in a single model, we construct a pipeline containing a generative language model represent-\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\ning content, and a discriminative model representing form. This allows us to represent the problem of creating poetry as a constraint satisfaction problem, where we can modify constraints to restrict the types of poetry we generate.\nCharacter Language Model Rather than train a model on data representing features of both content and form, we now use a simple character-level model (Sutskever et al., 2011) focused solely on content. This approach offers several benefits over the word-level models that are prevalent in the literature. Namely, their more compact vocabulary allows for more efficient training; they can learn common prefixes and suffixes to allow us to sample words that are not present in the training corpus and can learn effective language representations from relatively small corpora; and they can handle archaic and incorrect spellings of words. As we no longer need the model to explicitly represent the form of generated poetry, we can loosen our constraints when choosing a training corpus. Instead of relying on poetry only in sonnet form, we can instead construct a generic corpus of poetry taken from online sources. This corpus is composed of 7.56 million words and 34.34 million characters, taken largely from 20th Century poetry books found online. The increase in corpus size facilitates a corresponding increase in the number of permissible model parameters. This allows us to train a 3-layer LSTM model with 2048- dimensional hidden layers. The model was trained to predict the next character given a sequence of characters, using stochastic gradient descent. We attenuate the learning rate over time, and by 20 epochs the model converges.\nRhythm Modeling Although a character-level language model trained on a corpus of generic poetry allows us to generate interesting text, internal irregularities and noise in the training data prevent the model from learning important features such as rhythm. Hence, we require an additional classifier to constrain our model by either accepting or rejecting sampled lines based on the presence or absence of these features. As the presence of meter (rhythm) is the most characteristic feature of poetry, it therefore must be our primary focus. Pronunciation dictionaries have often been used to determine the syllabic stresses of words (Colton et al., 2012; Manurung et al., 2000; Misztal and Indurkhya, 2014), but suffer from some limitations\nfor constructing a classifier. All word pronunciations are considered equiprobable, including archaic and uncommon pronunciations, and pronunciations are provided context free, despite the importance of context for pronunciation3. Furthermore, they are constructed from American English, meaning that British English may be misclassified. These issues are circumvented by applying lightly supervised learning to determine the contextual stress pattern of any word. That is, we exploit the latent structure in our corpus of sonnet poetry, namely, the fact that sonnets are composed of lines in rigid Iambic Pentameter, and are therefore exactly ten syllables long with alternating syllabic stress. This allows us to derive a syllablestress distribution. Although we use the sonnets corpus for this, it is important to note that any corpus with such a latent structure could be used. By representing a line as a cascade of Weighted Finite State Transducers (WFST), we can perform Expectation Maximisation over the poetry corpus to obtain a probabilistic classifier which enables us to determine the most likely stress patterns for each word. Every word is represented by a single transducer. Since weights can be assigned to state transitions, we can model the probability that a given input string maps to a particular output. In each cascade, a sequence of input words is mapped onto a sequence of stress patterns ⟨×, /⟩ where each pattern is between 1 and 5 syllables in length4. We initially set all transition probabilities equally, as we make no assumptions about the stress distributions in our training set. We then iterate over each line of the sonnet corpus, using Expectation Maximisation to train the cascades. In practice, there are several de facto variations of Iambic meter which are permissible, as shown in Figure 2. We train the rhythm classifier by converging the cascades to whatever output is the most likely given the line.\nConstraining the model To generate poetry using this model, we sample sequences of characters from the character-level language model. To impose rhythm constrains on the language model, we first represent these sampled characters at the word level and pool sampled characters into word\n3For example, the independent probability of stressing the single syllable word at is 40%, but this increases to 91%when the following word is the (Greene et al., 2010)\n4Words of more than 5 syllables comprise less than 0.1% of the lexicon (Aoyama and Constable, 1998).\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n× / × / × / × / × / / × × / × / × / × / × / × / × / × / × / × / × × / × / × / × / ×\nFigure 2: Permissible variations of Iambic Pentameter in Shakespeare’s sonnets.\ntokens in an intermediary buffer. We then apply the separately trained word-level WFSTs to construct a cascade of this buffer and perform Viterbi decoding over the cascade. This defines the distribution of stress-patterns over our word tokens. We can represent this cascade as a probabilistic classifier, and accept or reject the buffered output based on how closely it conforms to the desired meter. While sampling sequences of words from this model, the entire generated sequence is passed to the classifier each time a new word is sampled. The pronunciation model then returns the probability that the entire line is within the specified meter. If a new word is rejected by the classifier, the state of the network is rolled back to the state of the last formulaically acceptable line, removing the rejected word from memory. The constraint on rhythm can be controlled by adjusting the acceptability threshold of the classifier. By increasing the threshold, output focuses on form over content. Conversely, decreasing the criterion puts greater emphasis on content.\nGeneric poetry\nSonnet poetry\nLSTM\nWFST\nRhythmic Output\nTrained\nTrained\nBuffer"
    }, {
      "heading" : "4.1 Themes and Poetic devices",
      "text" : "It is important for any generative poetry model to include themes and poetic devices. One way to achieve this would be by constructing a corpus that exhibits the desired themes and devices. To create a themed corpus about ‘love’, for instance,\nThemed Training Set\nPoetry LSTM\nThemed Output\nTraining Set\nPoetry LSTM\nThemed Output\nThematic Boosting\nImplicit Explicit\nFigure 3: Two approaches for generating themed poetry.\nwe would aggregate love poetry to train the model, which would thus learn an implicit representation of love. However, this forces us to generate poetry according to discrete themes and styles from pretrained models, requiring a new training corpus for each model. In other words, we would suffer from similar limitations as with the phonetic-level model, in that we require a dedicated corpus. Alternatively, we can manipulate the language model by boosting character probabilities at sample time to increase the probability of sampling thematic words like ‘love’. This approach is more robust, and provides us with more control over the final output, including the capacity to vary the inclusion of poetic devices in the output.\nThemes In order to introduce thematic content, we heuristically boost the probability of sampling words that are semantically related to a theme word from the language model. First, we compile a list of similar words to a key theme word by retrieving its semantic neighbours from a distributional semantic model (Mikolov et al., 2013). For example, the theme winter might include thematic words frozen, cold, snow and frosty. We represent these semantic neighbours at the character level, and heuristically boost their probability by multiplying the sampling probability of such character strings by a function of their cosine similarity to the key word. Thus, the likelihood of sampling a thematically related word is artificially increased, while still constraining the model rhythmically.\nPoetic devices A similar method may be used for poetic devices such as assonance, consonance and alliteration. Since these devices can be orthographically described by the repetition of identical sequences of characters, we can apply the\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nErrors per line 1 2 3 4 Total\nPhonetic Model 11 2 3 1 28 Character Model + WFST 6 5 1 1 23 Character Model 3 8 7 7 68\nTable 1: Number of lines with n errors from a set of 50 lines generated by each of the three models.\nWord Line Coverage\nWikipedia 64.84% 83.35% 97.53% Sonnets 85.95% 80.32% 99.36%\nTable 2: Error when transliterating text into phonemes and reconstructing back into text.\nsame heuristic to boost the probability of sampling character strings that have previously been sampled. That is, to sample a line with many instances of alliteration (multiple words with the same initial sound) we record the historical frequencies of characters sampled at the beginning of each previous word. After a word break character, we boost the probability that those characters will be sampled again in the softmax. We only keep track of frequencies for a fixed number of time steps. By increasing or decreasing the size of this window, we can manipulate the prevalence of alliteration. Variations of this approach are applied to invoke consonance (by boosting intra-word consonants) and assonance (by boosting intra-word vowels). An example of two sampled lines with high degrees of alliteration, assonance and consonance is given in Figure 4c."
    }, {
      "heading" : "5 Evaluation",
      "text" : "In order to examine how effective our methodologies for generating poetry are, we evaluate the proposed models in two ways. First, we perform an intrinsic evaluation where we examine the quality of the models and the generated poetry. Second, we perform an extrinsic evaluation where we evaluate the generated output using human annotators, and compare it to human-generated poetry."
    }, {
      "heading" : "5.1 Intrinsic evaluation",
      "text" : "To evaluate the ability of both models to generate formulaic poetry that adheres to rhythmic rules, we compared sets of fifty sampled lines from each model. The first set was sampled from the phonetic-level model trained on Iambic poetry.\nThe second set was sampled from the characterlevel model, constrained to Iambic form. For comparison, and to act as a baseline, we also sampled from the unconstrained character model. We created gold-standard syllabic classifications by recording each line spoken-aloud, and marking each syllable as either stressed or unstressed. We then compared these observations to loose Iambic Pentameter (containing all four variants), to determine how many syllabic misclassifications existed on each line. This was done by speaking each line aloud, and noting where the speaker put stresses. As Table 1 shows, the constrained character level model generated the most formulaic poetry. Results from this model show that 70% of lines had zero mistakes, with frequency obeying an inverse power-law relationship with the number of errors. We can see that the phonetic model performed similarly, but produced more subtle mistakes than the constrained character model: many of the errors were single mistakes in an otherwise correct line of poetry. In order to investigate this further, we examined to what extent these errors are due to transliteration (i.e., the phonetic encoding and orthographic decoding steps). Table 2 shows the reconstruction accuracy per word and per line when transliterating either Wikipedia or Sonnets to phonemes using the CMU pronunciation dictionary and subsequently reconstructing English text using the ngram model5. Word accuracy reflects the frequency of perfect reconstruction, whereas per line tri-gram similarity (Kondrak, 2005) reflects the overall reconstruction. Coverage captures the percentage of in-vocabulary items. The relatively low per-word accuracy achieved on the Wikipedia corpus is likely due to the high frequency of out-ofvocabulary words. The results show that a significant number of errors in the phonetic-level model are likely to be caused by transliteration mistakes."
    }, {
      "heading" : "5.2 Extrinsic evaluation",
      "text" : "We conducted an indistinguishability study with a selection of automatically generated poetry and human poetry. As extrinsic evaluations are expensive and the phonetic model was unlikely to do well (as illustrated in Figure 4e: the model generates good Iambic form, but not very good English),\n5Obviously, calculating this value for the character-level model makes no sense, since no transliteration occurs in that case.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n(a) The crow crooked on more beautiful and free, He journeyed off into the quarter sea. his radiant ribs girdled empty and very - least beautiful as dignified to see.\n(c) Man with the broken blood blue glass and gold. Cheap chatter chants to be a lover do.\n(e) The son still streams and strength and spirit. The ridden souls of which the fills of.\n(b) Is that people like things (are the way we to figure it out) and I thought of you reading and then is your show or you know we will finish along will you play.\n(d) How dreary to be somebody, How public like a frog To tell one’s name the livelong day To an admiring bog.\nFigure 5: The experimental environment for asking participants to distinguish between automatically generated and human poetry.\nwe only evaluate on the constrained characterlevel model.\nThe aim of the study was to determine whether participants could distinguish between human and generated poetry, and if so to what extent. A set of 70 participants (of whom 61 were English native speakers) were each shown a selection of randomly chosen poetry segments, and were invited to classify them as either human or generated. Participants were recruited from friends and people within poetry communities, with an age range of 17 to 80, and a mean age of 29. Our participants were not financially incentivised, perceiving the evaluation as an intellectual challenge.\nIn addition to the classification task, each partic-\nipant was also invited to rate each poem on a 1-5 scale with respect to three criteria, namely readability, form and evocation (how much emotion did a poem elicit). We naively consider the overall quality of a poem to be the mean of these three measures. We used a custom web-based environment, built specifically for this evaluation6, which is illustrated in Figure 5. Based on human judgments, we can determine whether the models presented in this work can produce poetry of a similar quality to humans. To select appropriate human poetry that could be meaningfully compared with the machinegenerated poetry, we performed a comprehension test on all poems used in the evaluation, using the Dale-Chall readability formula (Dale and Chall, 1948). This formula represents readability as a function of the complexity of the input words. We selected nine machine-generated poems with a high readability score. The generated poems produced an average score of 7.11, indicating that readers over 15 years of age should easily be able to comprehend them. For our human poems, we focused explicitly on poetry where greater consideration is placed on prosodic elements like rhythm and rhyme than semantic content (known as “nonsense verse”). We randomly selected 30 poems belonging to that category from the website poetrysoup.com, of which\n6[URL-ANONYMIZED]\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nPoet Title Human Readability Emotion Form\nGenerated Best 0.66 0.60 -0.77 0.90 G. M. Hopkins Carrion Comfort 0.62 -1.09 1.39 -1.55\nJ. Thornton Delivery of Death 0.60 0.26 -1.38 -0.65\nGenerated All 0.54 -0.28 -0.30 0.23 M. Yvonne Intricate Weave 0.53 2.38 0.94 -1.67\nE. Dickinson I’m Nobody 0.52 -0.46 0.92 0.44\nG. M. Hopkins The Silver Jubilee 0.52 0.71 -0.33 0.65\nR. Dryden Mac Flecknoe 0.51 -0.01 0.35 -0.78\nA. Tennyson Beautiful City 0.48 -1.05 0.97 -1.26\nW. Shakespeare A Fairy Song 0.45 0.65 1.30 1.18\nTable 3: Proportion of people classifying each poem as ’human’, as well as the relative qualitative scores of each poem as deviations from the mean.\neight were selected for the final comparison based on their comparable readability score. The selected poems were segmented into passages of between four and six lines, to match the length of the generated poetry segments. An example of such a segment is shown in Figure 4d. The human poems had an average score of 7.52, requiring a similar level of English aptitude to the generated texts. The performance of each human poem, alongside the aggregated scores of the generated poems, is illustrated in Table 3. For the human poems, our group of participants guessed correctly that they were human 51.4% of the time. For the generated poems, our participants guessed correctly 46.2% of the time that they were machine generated. To determine whether our results were statistically significant, we performed a Chi2 test. This resulted in a p-value of 0.718. This indicates that our participants were unable to tell the difference between human and generated poetry in any significant way. Although our participants generally considered the human poems to be of marginally higher quality than our generated poetry, they were unable to effectively distinguish between them. Interestingly, our results seem to suggest that our participants consider the generated poems to be more ‘human-like’ than those actually written by humans. Furthermore, the poem with the highest overall quality rating is a machine generated one. This shows that our approach was effective at generating high-quality rhythmic verse. It should be noted that the poems that were most ‘human-like’, most aesthetic and most emotive re-\nspectively (though not the most readable) were all generated by the neural character model. Generally the set of poetry produced by the neural character model was slightly less readable and emotive than the human poetry, but had above average form. All generated poems included in this evaluation can be found in the supplementary material."
    }, {
      "heading" : "6 Conclusions",
      "text" : "Our contributions are twofold. First, we developed a neural language model trained on a phonetic transliteration of poetic form and content. Although example output looked promising, this model was limited by its inability to generalise to novel forms of verse. We then proposed a more robust model trained on unformed poetic text, whose output form is constrained at sample time. This approach offers greater control over the style of the generated poetry than the earlier method, and facilitates themes and poetic devices. An indistinguishability test, where participants were asked to classify a randomly selected set of human “nonsense verse” and machine-generated poetry, showed generated poetry to be indistinguishable from that written by humans. In addition, the poems that were deemed most ‘humanlike’, most aesthetic and most emotive, respectively, were all machine-generated. In future work, it would be useful to investigate models based on morphemes, rather than characters, which offers potentially superior performance for complex and rare words (Luong et al., 2013), which are common in poetry.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Word length frequency and distribution in english: Observations, theory, and implications for the construction of verse lines",
      "author" : [ "Hideaki Aoyama", "John Constable." ],
      "venue" : "arXiv preprint cmp-lg/9808004 .",
      "citeRegEx" : "Aoyama and Constable.,? 1998",
      "shortCiteRegEx" : "Aoyama and Constable.",
      "year" : 1998
    }, {
      "title" : "Neural probabilistic language models",
      "author" : [ "Yoshua Bengio", "Holger Schwenk", "Jean-Sébastien Senécal", "Fréderic Morin", "Jean-Luc Gauvain." ],
      "venue" : "Innovations in Machine Learning, Springer, pages 137–186.",
      "citeRegEx" : "Bengio et al\\.,? 2006",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2006
    }, {
      "title" : "Issues in building general letter to sound rules",
      "author" : [ "Alan W Black", "Kevin Lenzo", "Vincent Pagel" ],
      "venue" : null,
      "citeRegEx" : "Black et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Black et al\\.",
      "year" : 1998
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Full face poetry generation",
      "author" : [ "Simon Colton", "Jacob Goodwin", "Tony Veale." ],
      "venue" : "Proceedings of the Third International Conference on Computational Creativity. pages 95–102.",
      "citeRegEx" : "Colton et al\\.,? 2012",
      "shortCiteRegEx" : "Colton et al\\.",
      "year" : 2012
    }, {
      "title" : "An exact a* method for deciphering letter-substitution ciphers",
      "author" : [ "Eric Corlett", "Gerald Penn." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, pages 1040–",
      "citeRegEx" : "Corlett and Penn.,? 2010",
      "shortCiteRegEx" : "Corlett and Penn.",
      "year" : 2010
    }, {
      "title" : "A formula for predicting readability: Instructions",
      "author" : [ "Edgar Dale", "Jeanne S Chall." ],
      "venue" : "Educational research bulletin pages 37–54.",
      "citeRegEx" : "Dale and Chall.,? 1948",
      "shortCiteRegEx" : "Dale and Chall.",
      "year" : 1948
    }, {
      "title" : "Wasp: Evaluation of different strategies for the automatic generation of spanish verse",
      "author" : [ "Pablo Gervás." ],
      "venue" : "Proceedings of the AISB-00 Symposium on Creative & Cultural Aspects of AI. pages 93–100.",
      "citeRegEx" : "Gervás.,? 2000",
      "shortCiteRegEx" : "Gervás.",
      "year" : 2000
    }, {
      "title" : "Automatic analysis of rhythmic poetry with applications to generation and translation",
      "author" : [ "Erica Greene", "Tugba Bodrumlu", "Kevin Knight." ],
      "venue" : "Proceedings of the 2010 conference on empirical methods in natural language processing. Association for",
      "citeRegEx" : "Greene et al\\.,? 2010",
      "shortCiteRegEx" : "Greene et al\\.",
      "year" : 2010
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Character-aware neural language models",
      "author" : [ "Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush." ],
      "venue" : "arXiv preprint arXiv:1508.06615 .",
      "citeRegEx" : "Kim et al\\.,? 2015",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2015
    }, {
      "title" : "Unsupervised analysis for decipherment problems",
      "author" : [ "Kevin Knight", "Anish Nair", "Nishit Rathod", "Kenji Yamada." ],
      "venue" : "Proceedings of the COLING/ACL on Main conference poster sessions. Association for Computational Linguistics, pages 499–",
      "citeRegEx" : "Knight et al\\.,? 2006",
      "shortCiteRegEx" : "Knight et al\\.",
      "year" : 2006
    }, {
      "title" : "N-gram similarity and distance",
      "author" : [ "Grzegorz Kondrak." ],
      "venue" : "String processing and information retrieval. Springer, pages 115–126.",
      "citeRegEx" : "Kondrak.,? 2005",
      "shortCiteRegEx" : "Kondrak.",
      "year" : 2005
    }, {
      "title" : "Better word representations with recursive neural networks for morphology",
      "author" : [ "Thang Luong", "Richard Socher", "Christopher D Manning." ],
      "venue" : "CoNLL. pages 104–113.",
      "citeRegEx" : "Luong et al\\.,? 2013",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2013
    }, {
      "title" : "Towards a computational model of poetry generation",
      "author" : [ "Hisar Manurung", "Graeme Ritchie", "Henry Thompson." ],
      "venue" : "Technical report, The University of Edinburgh.",
      "citeRegEx" : "Manurung et al\\.,? 2000",
      "shortCiteRegEx" : "Manurung et al\\.",
      "year" : 2000
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781 .",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur." ],
      "venue" : "INTERSPEECH. volume 2, page 3.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Poetry generation system with an emotional personality",
      "author" : [ "Joanna Misztal", "Bipin Indurkhya." ],
      "venue" : "Proceedings of the Fourth International Conference on Computational Creativity.",
      "citeRegEx" : "Misztal and Indurkhya.,? 2014",
      "shortCiteRegEx" : "Misztal and Indurkhya.",
      "year" : 2014
    }, {
      "title" : "Performance tradeoffs in dynamic time warping algorithms for isolated word recognition",
      "author" : [ "Cory Myers", "Lawrence R Rabiner", "Aaron E Rosenberg." ],
      "venue" : "Acoustics, Speech and Signal Processing, IEEE Transactions on 28(6):623–635.",
      "citeRegEx" : "Myers et al\\.,? 1980",
      "shortCiteRegEx" : "Myers et al\\.",
      "year" : 1980
    }, {
      "title" : "Gaiku: Generating haiku with word associations norms",
      "author" : [ "Yael Netzer", "David Gabay", "Yoav Goldberg", "Michael Elhadad." ],
      "venue" : "Proceedings of the Workshop on Computational Approaches to Linguistic Creativity. Association for Computational Lin-",
      "citeRegEx" : "Netzer et al\\.,? 2009",
      "shortCiteRegEx" : "Netzer et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning phoneme mappings for transliteration without parallel data",
      "author" : [ "Sujith Ravi", "Kevin Knight." ],
      "venue" : "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Compu-",
      "citeRegEx" : "Ravi and Knight.,? 2009",
      "shortCiteRegEx" : "Ravi and Knight.",
      "year" : 2009
    }, {
      "title" : "Training neural network language models on very large corpora",
      "author" : [ "Holger Schwenk", "Jean-Luc Gauvain." ],
      "venue" : "Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing. Association for",
      "citeRegEx" : "Schwenk and Gauvain.,? 2005",
      "shortCiteRegEx" : "Schwenk and Gauvain.",
      "year" : 2005
    }, {
      "title" : "Generating text with recurrent neural networks",
      "author" : [ "Ilya Sutskever", "James Martens", "Geoffrey E Hinton." ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 1017–1024.",
      "citeRegEx" : "Sutskever et al\\.,? 2011",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2011
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems. pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Harnessing constraint programming for poetry composition",
      "author" : [ "Jukka M Toivanen", "Matti Järvisalo", "Hannu Toivonen" ],
      "venue" : "In Proceedings of the Fourth International Conference on Computational Creativity",
      "citeRegEx" : "Toivanen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Toivanen et al\\.",
      "year" : 2013
    }, {
      "title" : "Freetts 1.2: A speech synthesizer written entirely in the java programming language",
      "author" : [ "Willie Walker", "Paul Lamere", "Philip Kwok" ],
      "venue" : null,
      "citeRegEx" : "Walker et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Walker et al\\.",
      "year" : 2010
    }, {
      "title" : "The carnegie mellon pronouncing dictionary [cmudict",
      "author" : [ "R Weide." ],
      "venue" : "0.6].",
      "citeRegEx" : "Weide.,? 2005",
      "shortCiteRegEx" : "Weide.",
      "year" : 2005
    }, {
      "title" : "Backpropagation through time: what it does and how to do it",
      "author" : [ "Paul J Werbos." ],
      "venue" : "Proceedings of the IEEE 78(10):1550–1560.",
      "citeRegEx" : "Werbos.,? 1990",
      "shortCiteRegEx" : "Werbos.",
      "year" : 1990
    }, {
      "title" : "Generating chinese classical poems with rnn encoderdecoder",
      "author" : [ "Xiaoyuan Yi", "Ruoyu Li", "andMaosong Sun" ],
      "venue" : null,
      "citeRegEx" : "Yi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Yi et al\\.",
      "year" : 2016
    }, {
      "title" : "Chinese poetry generation with recurrent neural networks",
      "author" : [ "Xingxing Zhang", "Mirella Lapata." ],
      "venue" : "EMNLP. pages 670–680.",
      "citeRegEx" : "Zhang and Lapata.,? 2014",
      "shortCiteRegEx" : "Zhang and Lapata.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "Neural language models (Schwenk and Gauvain, 2005; Bengio et al., 2006) have garnered signficant research interest for their ability to learn complex syntactic and semantic representations of natural language (Mikolov et al.",
      "startOffset" : 23,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "Neural language models (Schwenk and Gauvain, 2005; Bengio et al., 2006) have garnered signficant research interest for their ability to learn complex syntactic and semantic representations of natural language (Mikolov et al.",
      "startOffset" : 23,
      "endOffset" : 71
    }, {
      "referenceID" : 16,
      "context" : ", 2006) have garnered signficant research interest for their ability to learn complex syntactic and semantic representations of natural language (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014; Kim et al., 2015).",
      "startOffset" : 145,
      "endOffset" : 227
    }, {
      "referenceID" : 23,
      "context" : ", 2006) have garnered signficant research interest for their ability to learn complex syntactic and semantic representations of natural language (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014; Kim et al., 2015).",
      "startOffset" : 145,
      "endOffset" : 227
    }, {
      "referenceID" : 3,
      "context" : ", 2006) have garnered signficant research interest for their ability to learn complex syntactic and semantic representations of natural language (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014; Kim et al., 2015).",
      "startOffset" : 145,
      "endOffset" : 227
    }, {
      "referenceID" : 10,
      "context" : ", 2006) have garnered signficant research interest for their ability to learn complex syntactic and semantic representations of natural language (Mikolov et al., 2010; Sutskever et al., 2014; Cho et al., 2014; Kim et al., 2015).",
      "startOffset" : 145,
      "endOffset" : 227
    }, {
      "referenceID" : 7,
      "context" : "Rule-based poetry generation attempts include case-based reasoning (Gervás, 2000), templatebased generation (Colton et al.",
      "startOffset" : 67,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "Rule-based poetry generation attempts include case-based reasoning (Gervás, 2000), templatebased generation (Colton et al., 2012), constraint satisfaction (Toivanen et al.",
      "startOffset" : 108,
      "endOffset" : 129
    }, {
      "referenceID" : 24,
      "context" : ", 2012), constraint satisfaction (Toivanen et al., 2013) and text mining (Netzer et al.",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 19,
      "context" : ", 2013) and text mining (Netzer et al., 2009).",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 28,
      "context" : "Instead, they attempt to extract statistical patterns from existing poetry corpora in order to construct a language model, which can then be used generate new poetic variants (Yi et al., 2016; Greene et al., 2010).",
      "startOffset" : 175,
      "endOffset" : 213
    }, {
      "referenceID" : 8,
      "context" : "Instead, they attempt to extract statistical patterns from existing poetry corpora in order to construct a language model, which can then be used generate new poetic variants (Yi et al., 2016; Greene et al., 2010).",
      "startOffset" : 175,
      "endOffset" : 213
    }, {
      "referenceID" : 4,
      "context" : "Rule-based poetry generation attempts include case-based reasoning (Gervás, 2000), templatebased generation (Colton et al., 2012), constraint satisfaction (Toivanen et al., 2013) and text mining (Netzer et al., 2009). These approaches are often inspired by how humans might generate poetry. Statistical approaches, conversely, make no assumptions about the creative process. Instead, they attempt to extract statistical patterns from existing poetry corpora in order to construct a language model, which can then be used generate new poetic variants (Yi et al., 2016; Greene et al., 2010). The work of Zhang and Lapata (2014) is similar to ours, in that they make use of neural language models.",
      "startOffset" : 109,
      "endOffset" : 626
    }, {
      "referenceID" : 26,
      "context" : "Phonetic encoding To solve the first step, we apply a combination of word lookups from the CMU pronunciation dictionary (Weide, 2005) with letter-to-sound rules for handling out-ofvocabulary words.",
      "startOffset" : 120,
      "endOffset" : 133
    }, {
      "referenceID" : 18,
      "context" : "Expectation Maximisation is used to compute the probability of a single letter matching a single phone, which is maximised through the application of Dynamic Time Warping (Myers et al., 1980) to determine the most likely position of epsilon characters.",
      "startOffset" : 171,
      "endOffset" : 191
    }, {
      "referenceID" : 2,
      "context" : "These rules are based on the CART techniques described by Black et al. (1998), and are represented with a simple Finite State Transducer1.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "Neural language model We train a Long-Short Term Memory Network (Hochreiter and Schmidhuber, 1997) on the phonetic representation of our poetry corpus.",
      "startOffset" : 64,
      "endOffset" : 98
    }, {
      "referenceID" : 25,
      "context" : "Specifically, we Implemented using FreeTTS (Walker et al., 2010) An evaluation of models in American English, British English, German and French was undertaken by Black et al.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : ", 2010) An evaluation of models in American English, British English, German and French was undertaken by Black et al. (1998), who reported an externally validated per token accuracy on British English as low as 67%.",
      "startOffset" : 106,
      "endOffset" : 126
    }, {
      "referenceID" : 27,
      "context" : "We apply backpropagationthrough-time (Werbos, 1990) for 150 timesteps, which roughly equates to four lines of poetry in sonnet form.",
      "startOffset" : 37,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "The problem of homophonic decipherment has received significant research attention in the past; with approaches utilising Expectation Maximisation (Knight et al., 2006), Integer Programming (Ravi and Knight, 2009) and A* search (Corlett and Penn, 2010).",
      "startOffset" : 147,
      "endOffset" : 168
    }, {
      "referenceID" : 20,
      "context" : ", 2006), Integer Programming (Ravi and Knight, 2009) and A* search (Corlett and Penn, 2010).",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 5,
      "context" : ", 2006), Integer Programming (Ravi and Knight, 2009) and A* search (Corlett and Penn, 2010).",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 26,
      "context" : "Transliteration from phonetic to an orthographic representation is done by constructing a Hidden Markov Model using the CMU pronunciation dictionary (Weide, 2005) and an n-gram language model.",
      "startOffset" : 149,
      "endOffset" : 162
    }, {
      "referenceID" : 8,
      "context" : "Popularised in English by Shakespeare, the sonnet is characterised by a strict rhyme scheme and exactly fourteen lines of Iambic Pentameter (Greene et al., 2010).",
      "startOffset" : 140,
      "endOffset" : 161
    }, {
      "referenceID" : 22,
      "context" : "Character Language Model Rather than train a model on data representing features of both content and form, we now use a simple character-level model (Sutskever et al., 2011) focused solely on content.",
      "startOffset" : 149,
      "endOffset" : 173
    }, {
      "referenceID" : 4,
      "context" : "Pronunciation dictionaries have often been used to determine the syllabic stresses of words (Colton et al., 2012; Manurung et al., 2000; Misztal and Indurkhya, 2014), but suffer from some limitations for constructing a classifier.",
      "startOffset" : 92,
      "endOffset" : 165
    }, {
      "referenceID" : 14,
      "context" : "Pronunciation dictionaries have often been used to determine the syllabic stresses of words (Colton et al., 2012; Manurung et al., 2000; Misztal and Indurkhya, 2014), but suffer from some limitations for constructing a classifier.",
      "startOffset" : 92,
      "endOffset" : 165
    }, {
      "referenceID" : 17,
      "context" : "Pronunciation dictionaries have often been used to determine the syllabic stresses of words (Colton et al., 2012; Manurung et al., 2000; Misztal and Indurkhya, 2014), but suffer from some limitations for constructing a classifier.",
      "startOffset" : 92,
      "endOffset" : 165
    }, {
      "referenceID" : 8,
      "context" : "To impose rhythm constrains on the language model, we first represent these sampled characters at the word level and pool sampled characters into word For example, the independent probability of stressing the single syllable word at is 40%, but this increases to 91%when the following word is the (Greene et al., 2010) Words of more than 5 syllables comprise less than 0.",
      "startOffset" : 297,
      "endOffset" : 318
    }, {
      "referenceID" : 0,
      "context" : "1% of the lexicon (Aoyama and Constable, 1998).",
      "startOffset" : 18,
      "endOffset" : 46
    }, {
      "referenceID" : 15,
      "context" : "First, we compile a list of similar words to a key theme word by retrieving its semantic neighbours from a distributional semantic model (Mikolov et al., 2013).",
      "startOffset" : 137,
      "endOffset" : 159
    }, {
      "referenceID" : 12,
      "context" : "Word accuracy reflects the frequency of perfect reconstruction, whereas per line tri-gram similarity (Kondrak, 2005) reflects the overall reconstruction.",
      "startOffset" : 101,
      "endOffset" : 116
    }, {
      "referenceID" : 6,
      "context" : "To select appropriate human poetry that could be meaningfully compared with the machinegenerated poetry, we performed a comprehension test on all poems used in the evaluation, using the Dale-Chall readability formula (Dale and Chall, 1948).",
      "startOffset" : 217,
      "endOffset" : 239
    }, {
      "referenceID" : 13,
      "context" : "In future work, it would be useful to investigate models based on morphemes, rather than characters, which offers potentially superior performance for complex and rare words (Luong et al., 2013), which are common in poetry.",
      "startOffset" : 174,
      "endOffset" : 194
    } ],
    "year" : 2017,
    "abstractText" : "We propose two novel methodologies for the automatic generation of rhythmic poetry in a variety of forms. The first approach uses a neural language model trained on a phonetic encoding to learn an implicit representation of both the form and content of English poetry. This model can effectively learn common poetic devices such as rhyme, rhythm and alliteration. The second approach considers poetry generation as a constraint satisfaction problem where a generative neural language model is tasked with learning a representation of content, and a discriminative weighted finite state machine constrains it on the basis of form. By manipulating the constraints of the latter model, we can generate coherent poetry with arbitrary forms and themes. A large-scale extrinsic evaluation demonstrated that participants consider machine-generated poems to be written by humans 54% of the time. In addition, participants rated a machinegenerated poem to be the best amongst all evaluated.",
    "creator" : "LaTeX with hyperref package"
  }
}