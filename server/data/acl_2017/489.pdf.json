{
  "name" : "489.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Combining distributional and referential information for naming objects through cross-modal mapping and direct word prediction",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Expressions referring to objects in visual scenes typically include a word naming the type of the object: E.g., “house” in Figure 1 (a), or, as a very general type, “thingy” in Figure 1 (d). Determining such a name is is a crucial step for referring expression generation (REG) systems, as many other decisions, concerning e.g. the selection of attributes, follow from it (Dale and Reiter,\n1995; Krahmer and Van Deemter, 2012). For a long time, however, research on REG mostly assumed the availability of symbolic representations of referent and scene, and sidestepped questions about how speakers actually choose these names, due to the lack of models capable of capturing what a word like house refers to in the real world.\nRecent advances in image processing promise to fill this gap, with state-of-the-art computer vision systems being able to classify images into thousands of different categories (eg. Szegedy et al. (2015)). However, classification is not naming (Ordonez et al., 2016). Classification schemes are typically designed to be “flat”, with labels being on the same ontological level and, ideally, having disjunct extensions. In contrast, humans seem to be more flexible as to the chosen level of generality. Depending on the prototypicality of the object to name, and possibly other visual properties, a general name might be more or less appropriate. For instance, a robin can be named “bird”, but a penguin is better referred to as “penguin” (Rosch, 1978); along the same lines, the rather unusual building in Figure 1 that is not easy to otherwise\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ncategorise was named “structure”. Other work at the intersection of image and language processing has investigated models that learn to directly associate visual objects with a representation of word meaning, for example through cross-modal transfer into distributional vector spaces. Under the assumption that such semantic spaces represent, in some form at least, taxonomic knowledge, this makes labels on different levels of specificity available for a given object. Moreover, if the mapping is sufficiently general, it should be able to map objects to an appropriate label, even if during training of the mapping this label has not been seen (zero-shot learning). While indeed performing with some promise on this task (Lazaridou et al., 2014), this approach does not generally outperform standard object classification with known categories (Frome et al., 2013; Norouzi et al., 2013).\nThis paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g. as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels. We extend upon work on learning models of referential word use from corpora of images paired with referring expressions (Schlangen et al., 2016; Anonymous, in press) that treats words as individual predictors capturing referential appropriateness. We explore different ways of linking these predictors to distributional knowledge, during application and during training. We find that these improve over direct cross-modal mapping and direct visual classification in a standard and a zero-shot setup of an object naming task, as they allow for a more flexible combination of lexical and visual information when modeling referential meaning."
    }, {
      "heading" : "2 Related Work",
      "text" : "Grounding and Reference An early example for work in REG that goes beyond Dale and Reiter (1995)’s dominant symbolic paradigm is Deb Roy’s work from the early 2000s (Roy et al., 2002; Roy, 2002, 2005). More recently, research on REG, which has traditionally been done on small toy data sets, is being scaled up to realworld images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Zarrieß and Schlangen, 2016; Mao et al., 2015). In this paper, we focus on a particular problem posed by REG on real-world images,\nnamely generating the appropriate head noun for a given object. Similarly, Ordonez et al. (2016) have studied the problem of deriving appropriate object names, or so-called entry-level categories, from the output of an object recognizer. Their approach focusses links abstract object categories in ImageNet to actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction.\nMulti-modal and cross-modal distributional semantics Distributional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks (Turney and Pantel, 2010; Mikolov et al., 2013; Erk, 2016). Recent work on multi-modal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation. More related to our work are cross-modal mapping models (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014), that learn to transfer a representation of an object or image in the visual space to a vector in a distributional space. When tested on standard object recognition tasks, transfer, however, comes at a price. Frome et al. (2013) and Norouzi et al. (2013) both find that it slightly degrades performance as compared to a plain object classification using standard accuracy metrics (called flat “hit @k metric” in their paper). Interestingly though, Frome et al. (2013) report better performance using “hierarchical precision”, which essentially means that transfer predicts words that are ontologically closer to the gold label and makes “semantically more reasonable errors”. To the best of our knowledge, this pattern has not been systematically investigated any further. Another known problem with cross-modal transfer is that it seems to generalize less well than expected, i.e. tends to reproduce word vectors observed during training (Lazaridou et al., 2015a). In this work, we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299"
    }, {
      "heading" : "3 Task and Data",
      "text" : "We define object naming as follows: Given an object x in an image, the task is to predict a word w that could be used as the head noun of a realistic referring expression. (Cf. discussion above: “bird” when naming a robin, but “penguin” when naming a penguin.) To get at this, we develop our approach using a corpus of referring expressions produced by human users under natural, interactive conditions (Kazemzadeh et al., 2014), and train and test on the corresponding head nouns in these REs. This is similar to picture naming setups used in psycholinguistic research (cf. Levelt et al. (1991)) and based on the simplifying assumption that the name used for referring to an object can be determined successfully without looking at other objects in the image.\nWe now summarise the details of our setup:\nCorpus We train and test on the REFERIT corpus (Kazemzadeh et al., 2014), which is based on the SAIAPR image collection (Grubinger et al., 2006) (99.5k image regions;120K REs). We follow (Schlangen et al., 2016) and select words with a minimum frequency of 40 in these two data sets, which gives us a vocabulary of 793 words.\nNames For most of our experiments, we only use a subset of this vocabulary, namely the set of object names. As the REs contain nouns that cannot be considered to be names (background, bottom, etc.), we extract from the semantically annotated portion of the REFERIT corpus a list of names which correspond to ‘entry-level’ nouns in terms of (Kazemzadeh et al., 2014). This gives us a list of 159 names. Thus, our experiments are on a smaller scale as compared to (Ordonez et al., 2016). Nevertheless, the data is challenging, as the corpus contains references to objects that fall outside of the object labeling scheme that available object recognition systems are typically optimized for, cf. Hu et al. (2015)’s discussion on “stuff” entities such “sky” or “grass” in the REFERIT data. For testing, we remove relational REs (containing a relational preposition such as ‘left of X’), because here we cannot be sure that the head noun of the target is fully informative; we also remove REs with more than one head noun from our list (i.e. these are mostly relational expressions as well such as ‘girl laughing at boy’). We pair each image region from the test set with its corresponding names from the remaining REs.\nImage and Word Embeddings Following Schlangen et al. (2016), we derive representations of our visual inputs with a convolutional neural network, ‘GoogleNet’ (Szegedy et al., 2015), which was trained on the ImageNet corpus (Deng et al., 2009), and extract the final fully-connected layer before the classification layer, to give us a 1024 dimensional representation of the region. We add 7 features that encode information about the region relative to the image, thus representing each object as a vector of 1031 features. As distributional word vectors, we use the word2vec representations provided by Baroni et al. (2014) (trained with CBOW, 5-word context window, 10 negative samples, 400 dimensions)."
    }, {
      "heading" : "4 Three Models of Interfacing Visual and",
      "text" : "Distributional Information"
    }, {
      "heading" : "4.1 Direct Cross-Modal Mapping",
      "text" : "Following e.g. Lazaridou et al. (2014), referential meaning can be represented as a translation function that projects visual representations of objects to linguistic representations of words in a distributional vector space. Thus, in contrast to standard object recognition systems or the other models we will use here, cross-modal mapping does not treat words as individual labels or classifiers, but learns to directly predict continuous representations of words in a vector space, such as the space defined by the word2vec embeddings that we use in this work. This model will be called TRANSFER below.\nDuring training, we pair each object with the distributional embedding of its name, and use standard Ridge regression for learning the transformation. Lazaridou et al. (2014) and Lazaridou et al. (2015a) test a range of technical tweaks and different algorithms for cross-modal mapping. For ease of comparison with other models, we stick with simple Ridge Regression in this work.\nFor decoding, we map an object into the distributional space, and retrieve the nearest neighbors of the predicted vector using cosine similarity. In theory, the model should generalize easily to words that it has not observed in a pair with an object during training as it can map an object anywhere in the distributional space."
    }, {
      "heading" : "4.2 Lexical Mapping Through Individual Word Classifiers",
      "text" : "Another approach is to keep visual and distributional information separate, by training a separate visual classifier for each word w in the vocabu-\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nlary. Predictions can then be mapped into distributional space during application time via the vectors of the predicted words. Here, we use Schlangen et al. (2016)’s WAC model, building the training set for each word w as follows: all visual objects in a corpus that have been referred to as w are used as positive instances, the remaining objects as negative instances. Thus, the classifiers learn to predict referential appropriateness for individual words based on the visual features of the objects they refer to, in isolation of other words.\nDuring decoding, we apply all word classifiers from the model’s vocabulary to the given object, and take the argmax over the individual word probabilities. The model can be used to predict names directly, without links into a distributional space.\nIn order to extend the model’s vocabulary for zero-shot learning, we follow Norouzi et al. (2013) and associate the top n words with their corresponding distributional vector and compute the convex combination of these vectors. Then, in parallel to cross-modal mapping, we retrieve the nearest neighbors of the combined embedding from the distributional space. Thus, with this model, we use two different modes of decoding: one that projects into distributional space, one that only applies the available word classifiers."
    }, {
      "heading" : "4.3 Word Prediction via Cross-Modal Similarity Mapping",
      "text" : "Finally, we implement an approach that combines ideas from cross-modal mapping with the WAC model: we train individual predictors for each word in the vocabulary, but, during training, we exploit lexical similarity relations encoded in a distributional space. Instead of treating a word as a binary classifier, we annotate its training instances with a fine-grained similarity signal according to their object names. When building the training set for such a word predictor w, instead of simply dividing objects into w and ¬w instances, we label each object with a real-valued similarity obtained from cosine similarity between w and v in a distributional vector space, where v is the word that was used to refer to the object. Thus, we task the model with jointly learning similarities and referential appropriateness, by training it with Ridge regression on a continuous output space. Object instances where v = w (i.e., the positive instances in the binary setup) have maximal similarity; the\nremaining instances have a lower value which is more or less close to maximal similarity. This is the SIM-WAP model, recently proposed in (Anonymous).\nImportantly, and going beyond (Anonymous), this model allows for an innovative treatment of words that only exist in a distributional space (without being paired with visual referents in the image corpus): as the predictors are trained on a continuous output space, no genuine positive instances of a word’s referent are needed. When training a predictor for such a word w, we use all available objects from our corpus and annotate them with the expected lexical similarity between w and the actual object names v, which for all objects will be below the maximal value that marks genuine positive instances. During decoding, this model does not need to project its predictions into a distributional space, but it simply applies all available predictors to the object, and takes the argmax over the predicted referential appropriateness scores."
    }, {
      "heading" : "5 Experiment 1: Naming Objects",
      "text" : "This Section reports on experiments in a standard setup of the object naming task where all object names are paired with visual instances of their referents during training. In a comparable task, i.e. object recognition with known object categories, cross-modal projection or transfer approaches have been reported to perform worse than standard object classification methods (Frome et al., 2013; Norouzi et al., 2013). This seems to suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the real world and that referential meaning should potentially be learned from visual object representation only."
    }, {
      "heading" : "5.1 Model comparison",
      "text" : "Setup We use the train/test split of REFERIT data as in (Schlangen et al., 2016). We consider image regions with non-relational referring expressions that contain at least one of the 159 head nouns from the list of entry-level nouns (see section 3). This amounts to 6208 image regions for testing and 73K instances for training.\nResults Table 1 shows accuracies in the object naming task for the TRANSFER, WAC and SIMWAP models according to their accuracies in the top n, including two variants of WAC where its top\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n5 and top 10 predictions are project into the distributional space. Overall, the differences in accuracy between the models are small, but the various models that link their predictions to word representations in the distributional space all perform slightly worse than the plain WAC model, i.e. individual word classifiers trained on visual features only. This suggests that referential meanings for a word are learned less accurately when mapping from visual to distributional space, which replicates results reported in the literature on standard object recognition benchmarks.\nhit @k(%) @1 @2 @5\ntransfer 48.34 60.49 74.89 wac 49.34 61.86 75.35 wac, project top5 48.73 61.10 74.07 wac, project top10 48.68 61.23 74.31 sim-wap 48.13 60.60 75.40\nTable 1: Accuracies in object naming"
    }, {
      "heading" : "5.2 Model combination",
      "text" : "In order to get more insight into why the TRANSFER and SIM-WAP models produce slightly worse results than individual visual word classifiers, we now test to what extent the different models are complementary and combine them by aggregating over their naming predictions. If the models are complementary, their combination should lead to more confident and accurate naming decisions.\nSetup We combine TRANSFER, SIM-WAP and WAC by aggregating the scores they predict for different object names for a given object. During testing, we apply all models to an image region and consider words ranked among the top 10. We first normalize the referential appropriateness scores in each top-10 list and then compute their sum. This aggregation scheme will give more weight to words that appear in the top 10 list of different models, and less weight to words that only get top-ranked by a single model. We test on the same data as in Section 5.1.\nhit @k(%) 1 5 10\nsim-wap + transfer 49.10 61.78 75.81 sim-wap + wac 51.10 63.45 77.92 transfer + wac 51.13 63.76 77.84 wac + transfer + sim-wap 52.19 64.71 78.40\nTable 2: Object naming acc., combined models\nResults Table 2 shows that all model combinations improve over the results of their isolated models in Table 1, suggesting that WAC, TRANSFER and SIM-WAP indeed do capture complementary aspects of referential word meaning. On their own, the distributionally informed models are less tuned to specific word occurrences than the visual word classifiers in the WAC model, but they can add useful information which leads to a clear overall improvement. We take this as a promising finding, supporting our initial hypothesis that knowledge on lexical distributional meaning should and can be exploited when learning how to use words for reference.\nAv. cosine distance among top k gold - top k\n5 10 5 10\ntransfer 0.68 0.73 0.72 0.75 wac 0.82 0.80 0.82 0.84 sim-wap 0.68 0.74 0.72 0.75\nTable 3: Cosine distances between word2vec embeddings of nouns generated in the top k"
    }, {
      "heading" : "5.3 Analysis",
      "text" : "Figure 2 illustrates objects from our test set where the combination of TRANSFER, SIM-WAP and WAC predicts an accurate name, whereas the models in isolation do not. These examples give some interesting insight into why the models capture different aspects of referential word use and meaning.\nWord Similarities Many of the examples in Figure 2 suggest that the object names ranked among the top 3 by the TRANSFER and SIMWAP model are semantically similar to each other, whereas WAC generates object names on top that describe very different underlying object categories, such as seal / rock in Figure 2(a), animal / lamp in Figure 2(g) or chair / shirt in Figure 2(c). To quantify this general impression, Table 3 shows cosine distances among words in the top n generated by our models, using their word2vec embeddings. The average cosine distance between words in our vocabulary is 0.83. The transfer and sim-wap model rank words on top that are clearly more similar to each other than word pairs on average, whereas words ranked top by the wac model are more dissimilar. This parallels findings by Frome et al. (2013), discussed in Section 2. Additional evaluation metrics, such as success rates\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nin a human evaluation (cf. Zarrieß and Schlangen (2016)), would be an interesting direction for more detailed investigation here.\nWord Use But even though the WAC classifiers lack knowledge on lexical similarities, they seem to able to detect relatively specific instances of word use such as hut in Figure 2(b), shirt in 2(c) or lamp in 2(h). Here, the combination with TRANSFER and SIM-WAP is helpful to give more weight to the object name that is taxonomically correct (sometimes pushing up words below the top-3 and hence not shown in Figure 2). In Figure 1(e), SIMWAP and TRANSFER give more weight to typical names for persons, whereas WAC top-ranks more unusual names, reflecting that the person is difficult to identify visually. Another observation is that the mapping models have difficulties dealing with object names in singular and plural. As these words have very similar representations in the distributional space, they are often predicted as likely variants among the top 10 by SIM-WAP and TRANSFER, whereas the WAC model seems to predict inappropriate plural words less often among the top 3. Such specific phenomena at the intersection of visual and semantic similarity have found very little attention in the literature. We will investigate them further in our Experiments on zeroshot naming in the following Section."
    }, {
      "heading" : "6 Zero-Shot Naming",
      "text" : "Zero-shot learning is an attractive prospect for REG from images, as it promises to overcome dependence on pairings of visual instances and natural names being available for all names, if visual/referential data can be generalised from other types of information. Previous work has looked at the feasibility of zero-shot learning as a function of semantic similarity or ontological closeness between unknown and known categories, and confirmed the intuition that the task is harder the less close unknown categories are to known ones (Frome et al., 2013; Norouzi et al., 2013).\nOur experiments on object naming in Section 5 suggest that lexical similarities encoded in a distributional space might not always fully carry over to referential meaning. This could constitute an additional challenge for zero-shot learning, as distributional similarities might be misleading when the model has to fully rely on them for learning referential word meanings. Therefore, the following experiments investigate the performance of\nour models in zero-shot naming as a function of the lexical relation between unknown and known object names, i.e. namely hypernyms and singular/plurals. Both relations are typically captured by distributional models of word meaning in terms of closeness in the vector space, but their visual and referential relation is clearly different."
    }, {
      "heading" : "6.1 Vocabulary Splits and Testsets",
      "text" : "Random As in previous work on zero-shot learning, we consider zero-shot naming for words of varying degrees of similarity in our vocabulary. We randomly split our 159 names from Experiment 1 into 10 subsets. We train the models on 90% of the nouns (and all their visual instances in the image corpus) and test on the set of image regions that are named with words which the model did not observe during training. Results reported in Table 4 on the random test set correspond to averaged scores from cross-validation over the 10 splits.\nHypernyms We manually split the model’s vocabulary into set of hypernyms (see Appendix A) and the remaining nouns. We train the models on those 84K image regions that where not named with a hypernym, and test on 8895 image regions that were named with a hypernym in the corpus. We checked that for each of these hypernyms, the vocabulary contains at least one or two names that can be considered as hyponyms, i.e. the model sees objects during training that are instances of vehicle for example, but never encounters actual uses of that name. This test set is particularly interesting from an REG perspective, as objects named with very general terms by human speakers are often difficult to describe with more common, but more specific terms, as is illustrated by the uses of structure and thingy in Figure 1.\nSingulars/Plurals We pick 68 words from our vocabulary that can be grouped into 34 singularplural noun pairs (see Appendix A). From each pair, we randomly include the singular or plural noun in the set of zero-shot nouns. Thus, we make sure that the model encounters singular and plural names during training, but it never encounters both variants of a name. This results in a more even training/test split, i.e. we train on 23K image regions and evaluate on 13825 instances.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n(a) wac: seal, rock, water sim-wap: side, rock,rocks transfer: rocks, rock, water combination: rock\n(c) wac: chair, shirt, guy sim-wap: woman, man, girl transfer: door, woman, window combination: shirt\n(e) wac: chick, person, guy sim-wap: man, person, woman transfer: man, guy, girl combination: person\n(g) wac: animal, lamp, table sim-wap: man, girl, person transfer: man, clouds, cloud combination: person\n(b) wac: cactus, hut, mountain sim-wap: side, rock, mountain transfer: mountain, rocks, rock combination: hut\n(d) wac: roof, house, building sim-wap: building, house, trees transfer: building, house, trees combination: house\n(f) wac: bush, bushes, tree sim-wap: trees, tree, grass transfer: trees, tree, bushes combination: bushes\n(h) wac: post, light, lamp sim-wap: tree, sky, pole transfer: tree, sky, trees combination: lamp\nFigure 2: Examples from object naming experiment where model combination is accurate\nZero-shot Model full vocab disjoint vocab names @1 @2 @5 @10 @1 @2\nRandom\ntransfer 0.05 2.38 16.57 35.71 41.49 62.34 wac, project top10 0.00 4.42 21.16 39.17 38.03 58.07 wac, project top5 0.00 4.39 21.63 40.01 37.46 57.36 sim-wap 3.71 13.13 36.49 54.44 42.28 64.26\nHypernyms\ntransfer 0.07 1.25 7.75 29.93 59.88 73.88 wac, project top10 0.00 3.01 15.55 36.99 50.51 66.33 wac, project top5 0.00 2.78 16.75 38.13 47.73 64.38 sim-wap 3.16 10.33 31.14 49.62 57.55 70.15\nSingulars/Plurals transfer 0.01 22.84 44.30 72.85 34.56 51.79 wac, project top10 0.00 22.21 43.43 68.95 31.46 48.76 wac, project top5 0.00 22.18 43.93 69.33 31.46 48.88 sim-wap 15.39 34.73 56.62 77.32 37.24 54.02\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799"
    }, {
      "heading" : "6.2 Evaluation",
      "text" : "Some previous work on zero-shot image labeling assumes additional components that first identify whether an image should be labelled by a known or unknown word (Frome et al., 2013). We follow Lazaridou et al. (2014) and let the model decide whether to refer to an object by a known or unknown name. Related to that, distinct evaluation procedures have been used in the literature on zero-shot learning:\nTesting on full vocabulary A realistic way to test zero-shot learning performance is to consider all words from a given vocabulary during testing, though the testset only contains instances of objects that have been named with a ‘zero-shot word’ (for which no visual instances were seen during training). Accuracies in this setup reflect how well the model is able to generalize, i.e. how often it decides to deviate from the words it was trained on, and (implicitly) predicts that the given object requires a “new” name. In case of the (i) hypernym and (ii) singular/plural test set, this accuracy also reflects to what extent the model is able to detect cases where (i) a more general or vague term is needed, where (ii) an unknown singular/plural counterpart of a known object type occurs.\nTesting on disjoint vocabulary Alternatively, the model’s vocabulary can be restricted during testing to zero-shot words only, such that names encountered during training and testing are disjoint, see e.g. (Lampert et al., 2009, 2013). This setup factors out the generalization problem, and assesses to what extent a model is able to capture the referential meaning of a word that does not have instances in the training data."
    }, {
      "heading" : "6.3 Results",
      "text" : "As compared to Experiment 1 where models achieved similar performance, differences are more pronounced in the zero-shot setup, as shown in Table 4. In particular, we find that the SIMWAP model which induces individual predictors for words that have not been observed in the training data is clearly more successful than TRANSFER or WAC that project predictions into the distributional space. When tested on the full vocabulary, we find that TRANSFER and WAC very rarely generate names whose referents were excluded from training, which is in line with observations made by Lazaridou et al. (2015a). The SIM-WAP\npredictors generalize much better, in particular on the singular/plural testset.\nAn interesting exception is the good performance of the TRANSFER model on the hypernym test set, when evaluated with a disjoint vocabulary. This corroborates evidence from Experiment 1, namely that the transfer model captures taxonomic aspects of object names better than the other models. Projection via individual word classifiers, on the other hand, seems to generalize better than TRANSFER, at least when looking at accuracies @2 ... @10. Thus, combining several vectors predicted by a model of referential word meaning can provide additional information, as compared to mapping an object to a single vector in distributional space. More work is needed to establish how these approaches can be integrated more effectively."
    }, {
      "heading" : "7 Discussion and Conclusion",
      "text" : "In this paper, we have investigated models of referential word meaning, using different ways of combining visual information about a word’s referent and distributional knowledge about its lexical similarities. Previous cross-modal mapping models essentially force semantically similar objects to be mapped into the same area in the semantic space regardless of their actual visual similarity. We found that cross-modal mapping produces semantically appropriate and mutually highly similar object names in its top-n list, but does not preserve differences in referential word use (e.g. appropriatness of person vs. woman) especially within the same semantic field. We have shown that it is beneficial for performance in standard and zeroshot object naming to treat words as individual predictors that capture referential appropriateness and are only indirectly linked to a distributional space, either through lexical mapping during application or through cross-modal similarity mapping during training. As we have tested these approaches on a rather small vocabulary, which may limit generality of conclusions, future work will be devoted to scaling up these findings to larger test sets, as e.g. recently collected through conversational agents (Das et al., 2016) that circumvent the need for human-human interaction data. Also from an REG perspective, various extensions of this approach are possible, such as the inclusion of contextual information during object naming and its combination with attribute selection.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Don’t count, predict! a systematic comparison of context-counting vs",
      "author" : [ "Marco Baroni", "Georgiana Dinu", "Germán Kruszewski." ],
      "venue" : "context-predicting semantic vectors. In ACL (1). pages 238–247.",
      "citeRegEx" : "Baroni et al\\.,? 2014",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2014
    }, {
      "title" : "Computational interpretations of the gricean maxims in the generation of referring expressions",
      "author" : [ "Robert Dale", "Ehud Reiter." ],
      "venue" : "Cognitive Science 19(2):233–263.",
      "citeRegEx" : "Dale and Reiter.,? 1995",
      "shortCiteRegEx" : "Dale and Reiter.",
      "year" : 1995
    }, {
      "title" : "Visual dialog",
      "author" : [ "Abhishek Das", "Satwik Kottur", "Khushi Gupta", "Avi Singh", "Deshraj Yadav", "José M.F. Moura", "Devi Parikh", "Dhruv Batra." ],
      "venue" : "CoRR abs/1611.08669. http://arxiv.org/abs/1611.08669.",
      "citeRegEx" : "Das et al\\.,? 2016",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2016
    }, {
      "title" : "ImageNet: A Large-Scale Hierarchical Image Database",
      "author" : [ "Jia Deng", "W. Dong", "Richard Socher", "L.-J. Li", "K. Li", "L. Fei-Fei." ],
      "venue" : "CVPR09.",
      "citeRegEx" : "Deng et al\\.,? 2009",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2009
    }, {
      "title" : "What do you know about an alligator when you know the company it keeps? Semantics and Pragmatics 9(17):1–63",
      "author" : [ "Katrin Erk." ],
      "venue" : "https://doi.org/10.3765/sp.9.17.",
      "citeRegEx" : "Erk.,? 2016",
      "shortCiteRegEx" : "Erk.",
      "year" : 2016
    }, {
      "title" : "Visual information in semantic representation",
      "author" : [ "Yansong Feng", "Mirella Lapata." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for",
      "citeRegEx" : "Feng and Lapata.,? 2010",
      "shortCiteRegEx" : "Feng and Lapata.",
      "year" : 2010
    }, {
      "title" : "Devise: A deep visualsemantic embedding model",
      "author" : [ "Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Marc Aurelio Ranzato", "Tomas Mikolov." ],
      "venue" : "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q.",
      "citeRegEx" : "Frome et al\\.,? 2013",
      "shortCiteRegEx" : "Frome et al\\.",
      "year" : 2013
    }, {
      "title" : "From the virtual to the real world: Referring to objects in real-world spatial scenes",
      "author" : [ "Dimitra Gkatzia", "Verena Rieser", "Phil Bartie", "William Mackaness." ],
      "venue" : "Proceedings of EMNLP 2015. Association for Computational Linguistics.",
      "citeRegEx" : "Gkatzia et al\\.,? 2015",
      "shortCiteRegEx" : "Gkatzia et al\\.",
      "year" : 2015
    }, {
      "title" : "The IAPR TC-12 benchmark: a new evaluation resource for visual information systems",
      "author" : [ "Michael Grubinger", "Paul Clough", "Henning Müller", "Thomas Deselaers." ],
      "venue" : "Proceedings of the International Conference on Language Resources and Evaluation",
      "citeRegEx" : "Grubinger et al\\.,? 2006",
      "shortCiteRegEx" : "Grubinger et al\\.",
      "year" : 2006
    }, {
      "title" : "Natural language object retrieval",
      "author" : [ "Ronghang Hu", "Huazhe Xu", "Marcus Rohrbach", "Jiashi Feng", "Kate Saenko", "Trevor Darrell." ],
      "venue" : "CoRR abs/1511.04164. http://arxiv.org/abs/1511.04164.",
      "citeRegEx" : "Hu et al\\.,? 2015",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics",
      "author" : [ "Douwe Kiela", "Léon Bottou." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-14).",
      "citeRegEx" : "Kiela and Bottou.,? 2014",
      "shortCiteRegEx" : "Kiela and Bottou.",
      "year" : 2014
    }, {
      "title" : "Visual word2vec (vis-w2v): Learning visually grounded word embeddings using abstract scenes",
      "author" : [ "Satwik Kottur", "Ramakrishna Vedantam", "José MF Moura", "Devi Parikh." ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern",
      "citeRegEx" : "Kottur et al\\.,? 2016",
      "shortCiteRegEx" : "Kottur et al\\.",
      "year" : 2016
    }, {
      "title" : "Computational generation of referring expressions: A survey",
      "author" : [ "Emiel Krahmer", "Kees Van Deemter." ],
      "venue" : "Computational Linguistics 38(1):173–218.",
      "citeRegEx" : "Krahmer and Deemter.,? 2012",
      "shortCiteRegEx" : "Krahmer and Deemter.",
      "year" : 2012
    }, {
      "title" : "Learning to detect unseen object classes by between-class attribute transfer",
      "author" : [ "Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling." ],
      "venue" : "IEEE Computer Vision and Pattern Recognition. IEEE, pages 951–958.",
      "citeRegEx" : "Lampert et al\\.,? 2009",
      "shortCiteRegEx" : "Lampert et al\\.",
      "year" : 2009
    }, {
      "title" : "Attribute-based classification for zero-shot visual object categorization",
      "author" : [ "Christoph H. Lampert", "Hannes Nickisch", "Stefan Harmeling." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence 36(3):453–465.",
      "citeRegEx" : "Lampert et al\\.,? 2013",
      "shortCiteRegEx" : "Lampert et al\\.",
      "year" : 2013
    }, {
      "title" : "Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world",
      "author" : [ "Angeliki Lazaridou", "Elia Bruni", "Marco Baroni." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Lazaridou et al\\.,? 2014",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2014
    }, {
      "title" : "Hubness and pollution: Delving into cross-space mapping for zero-shot learning",
      "author" : [ "Angeliki Lazaridou", "Georgiana Dinu", "Marco Baroni." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Lazaridou et al\\.,? 2015a",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2015
    }, {
      "title" : "Combining language and vision with a multimodal skip-gram model",
      "author" : [ "Angeliki Lazaridou", "Nghia The Pham", "Marco Baroni." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Lazaridou et al\\.,? 2015b",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2015
    }, {
      "title" : "The time course of lexical access",
      "author" : [ "Willem JM Levelt", "Herbert Schriefers", "Dirk Vorberg", "Antje S Meyer", "Thomas Pechmann", "Jaap Havinga" ],
      "venue" : null,
      "citeRegEx" : "Levelt et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Levelt et al\\.",
      "year" : 1991
    }, {
      "title" : "Generation and comprehension of unambiguous object descriptions",
      "author" : [ "Junhua Mao", "Jonathan Huang", "Alexander Toshev", "Oana Camburu", "Alan L. Yuille", "Kevin Murphy." ],
      "venue" : "ArXiv / CoRR abs/1511.02283. http://arxiv.org/abs/1511.02283.",
      "citeRegEx" : "Mao et al\\.,? 2015",
      "shortCiteRegEx" : "Mao et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Ad-",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Zero-shot learning by convex combination of semantic embeddings",
      "author" : [ "Mohammad Norouzi", "Tomas Mikolov", "Samy Bengio", "Yoram Singer", "Jonathon Shlens", "Andrea Frome", "Greg S Corrado", "Jeffrey Dean" ],
      "venue" : null,
      "citeRegEx" : "Norouzi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Norouzi et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning to name objects",
      "author" : [ "Vicente Ordonez", "Wei Liu", "Jia Deng", "Yejin Choi", "Alexander C. Berg", "Tamara L. Berg." ],
      "venue" : "Commun. ACM 59(3):108–115.",
      "citeRegEx" : "Ordonez et al\\.,? 2016",
      "shortCiteRegEx" : "Ordonez et al\\.",
      "year" : 2016
    }, {
      "title" : "Principles of Categorization",
      "author" : [ "Eleanor Rosch." ],
      "venue" : "Eleanor Rosch and Barbara B. Lloyd, editors, Cognition and Categorization, Lawrence Erlbaum, Hillsdale, N.J., USA, pages 27—-48.",
      "citeRegEx" : "Rosch.,? 1978",
      "shortCiteRegEx" : "Rosch.",
      "year" : 1978
    }, {
      "title" : "Grounding words in perception and action: Computational insights",
      "author" : [ "Deb Roy." ],
      "venue" : "Trends in Cognitive Sciene 9(8):389–396.",
      "citeRegEx" : "Roy.,? 2005",
      "shortCiteRegEx" : "Roy.",
      "year" : 2005
    }, {
      "title" : "A trainable spoken language understanding system for visual object selection",
      "author" : [ "Deb Roy", "Peter Gorniak", "Niloy Mukherjee", "Josh Juster." ],
      "venue" : "Proceedings of the International Conference on Speech and Language Processing 2002 (ICSLP 2002). Col-",
      "citeRegEx" : "Roy et al\\.,? 2002",
      "shortCiteRegEx" : "Roy et al\\.",
      "year" : 2002
    }, {
      "title" : "Learning visually-grounded words and syntax for a scene description task",
      "author" : [ "Deb K. Roy." ],
      "venue" : "Computer Speech and Language 16(3).",
      "citeRegEx" : "Roy.,? 2002",
      "shortCiteRegEx" : "Roy.",
      "year" : 2002
    }, {
      "title" : "Resolving references to objects in photographs using the words-as-classifiers model",
      "author" : [ "David Schlangen", "Sina Zarriess", "Casey Kennington." ],
      "venue" : "Proceedings of the 54rd Annual Meeting of the Association for Computational Linguistics (ACL 2016).",
      "citeRegEx" : "Schlangen et al\\.,? 2016",
      "shortCiteRegEx" : "Schlangen et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning grounded meaning representations with autoencoders",
      "author" : [ "Carina Silberer", "Mirella Lapata." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computa-",
      "citeRegEx" : "Silberer and Lapata.,? 2014",
      "shortCiteRegEx" : "Silberer and Lapata.",
      "year" : 2014
    }, {
      "title" : "Zero-shot learning through cross-modal transfer",
      "author" : [ "Richard Socher", "Milind Ganjoo", "Christopher D Manning", "Andrew Ng." ],
      "venue" : "Advances in neural information processing systems. pages 935–943.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich." ],
      "venue" : "CVPR 2015. Boston, MA, USA.",
      "citeRegEx" : "Szegedy et al\\.,? 2015",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics",
      "author" : [ "Peter D Turney", "Patrick Pantel." ],
      "venue" : "Journal of artificial intelligence research 37(1):141–188.",
      "citeRegEx" : "Turney and Pantel.,? 2010",
      "shortCiteRegEx" : "Turney and Pantel.",
      "year" : 2010
    }, {
      "title" : "Easy things first: Installments improve referring expression generation for objects in photographs",
      "author" : [ "Sina Zarrieß", "David Schlangen." ],
      "venue" : "Proceedings of ACL 2016. Berlin, Germany.",
      "citeRegEx" : "Zarrieß and Schlangen.,? 2016",
      "shortCiteRegEx" : "Zarrieß and Schlangen.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 22,
      "context" : "However, classification is not naming (Ordonez et al., 2016).",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 23,
      "context" : "For instance, a robin can be named “bird”, but a penguin is better referred to as “penguin” (Rosch, 1978); along the same lines, the rather unusual building in Figure 1 that is not easy to otherwise",
      "startOffset" : 92,
      "endOffset" : 105
    }, {
      "referenceID" : 28,
      "context" : "Szegedy et al. (2015)).",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 15,
      "context" : "While indeed performing with some promise on this task (Lazaridou et al., 2014), this approach does not generally outperform standard object classification with known categories (Frome et al.",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : ", 2014), this approach does not generally outperform standard object classification with known categories (Frome et al., 2013; Norouzi et al., 2013).",
      "startOffset" : 106,
      "endOffset" : 148
    }, {
      "referenceID" : 21,
      "context" : ", 2014), this approach does not generally outperform standard object classification with known categories (Frome et al., 2013; Norouzi et al., 2013).",
      "startOffset" : 106,
      "endOffset" : 148
    }, {
      "referenceID" : 25,
      "context" : "Grounding and Reference An early example for work in REG that goes beyond Dale and Reiter (1995)’s dominant symbolic paradigm is Deb Roy’s work from the early 2000s (Roy et al., 2002; Roy, 2002, 2005).",
      "startOffset" : 165,
      "endOffset" : 200
    }, {
      "referenceID" : 7,
      "context" : "More recently, research on REG, which has traditionally been done on small toy data sets, is being scaled up to realworld images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Zarrieß and Schlangen, 2016; Mao et al., 2015).",
      "startOffset" : 129,
      "endOffset" : 223
    }, {
      "referenceID" : 32,
      "context" : "More recently, research on REG, which has traditionally been done on small toy data sets, is being scaled up to realworld images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Zarrieß and Schlangen, 2016; Mao et al., 2015).",
      "startOffset" : 129,
      "endOffset" : 223
    }, {
      "referenceID" : 19,
      "context" : "More recently, research on REG, which has traditionally been done on small toy data sets, is being scaled up to realworld images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Zarrieß and Schlangen, 2016; Mao et al., 2015).",
      "startOffset" : 129,
      "endOffset" : 223
    }, {
      "referenceID" : 1,
      "context" : "Grounding and Reference An early example for work in REG that goes beyond Dale and Reiter (1995)’s dominant symbolic paradigm is Deb Roy’s work from the early 2000s (Roy et al.",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "Grounding and Reference An early example for work in REG that goes beyond Dale and Reiter (1995)’s dominant symbolic paradigm is Deb Roy’s work from the early 2000s (Roy et al., 2002; Roy, 2002, 2005). More recently, research on REG, which has traditionally been done on small toy data sets, is being scaled up to realworld images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Zarrieß and Schlangen, 2016; Mao et al., 2015). In this paper, we focus on a particular problem posed by REG on real-world images, namely generating the appropriate head noun for a given object. Similarly, Ordonez et al. (2016) have studied the problem of deriving appropriate object names, or so-called entry-level categories, from the output of an object recognizer.",
      "startOffset" : 74,
      "endOffset" : 607
    }, {
      "referenceID" : 31,
      "context" : "Multi-modal and cross-modal distributional semantics Distributional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks (Turney and Pantel, 2010; Mikolov et al., 2013; Erk, 2016).",
      "startOffset" : 165,
      "endOffset" : 223
    }, {
      "referenceID" : 20,
      "context" : "Multi-modal and cross-modal distributional semantics Distributional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks (Turney and Pantel, 2010; Mikolov et al., 2013; Erk, 2016).",
      "startOffset" : 165,
      "endOffset" : 223
    }, {
      "referenceID" : 4,
      "context" : "Multi-modal and cross-modal distributional semantics Distributional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks (Turney and Pantel, 2010; Mikolov et al., 2013; Erk, 2016).",
      "startOffset" : 165,
      "endOffset" : 223
    }, {
      "referenceID" : 5,
      "context" : "Recent work on multi-modal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation.",
      "startOffset" : 56,
      "endOffset" : 176
    }, {
      "referenceID" : 28,
      "context" : "Recent work on multi-modal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation.",
      "startOffset" : 56,
      "endOffset" : 176
    }, {
      "referenceID" : 10,
      "context" : "Recent work on multi-modal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation.",
      "startOffset" : 56,
      "endOffset" : 176
    }, {
      "referenceID" : 17,
      "context" : "Recent work on multi-modal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation.",
      "startOffset" : 56,
      "endOffset" : 176
    }, {
      "referenceID" : 11,
      "context" : "Recent work on multi-modal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation.",
      "startOffset" : 56,
      "endOffset" : 176
    }, {
      "referenceID" : 29,
      "context" : "More related to our work are cross-modal mapping models (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014), that learn to transfer a representation of an object or image in the visual space to a vector in a distributional space.",
      "startOffset" : 56,
      "endOffset" : 143
    }, {
      "referenceID" : 6,
      "context" : "More related to our work are cross-modal mapping models (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014), that learn to transfer a representation of an object or image in the visual space to a vector in a distributional space.",
      "startOffset" : 56,
      "endOffset" : 143
    }, {
      "referenceID" : 21,
      "context" : "More related to our work are cross-modal mapping models (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014), that learn to transfer a representation of an object or image in the visual space to a vector in a distributional space.",
      "startOffset" : 56,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : "More related to our work are cross-modal mapping models (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014), that learn to transfer a representation of an object or image in the visual space to a vector in a distributional space.",
      "startOffset" : 56,
      "endOffset" : 143
    }, {
      "referenceID" : 16,
      "context" : "tends to reproduce word vectors observed during training (Lazaridou et al., 2015a).",
      "startOffset" : 57,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : ", 2013; Erk, 2016). Recent work on multi-modal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation. More related to our work are cross-modal mapping models (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014), that learn to transfer a representation of an object or image in the visual space to a vector in a distributional space. When tested on standard object recognition tasks, transfer, however, comes at a price. Frome et al. (2013) and Norouzi et al.",
      "startOffset" : 8,
      "endOffset" : 767
    }, {
      "referenceID" : 4,
      "context" : ", 2013; Erk, 2016). Recent work on multi-modal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation. More related to our work are cross-modal mapping models (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014), that learn to transfer a representation of an object or image in the visual space to a vector in a distributional space. When tested on standard object recognition tasks, transfer, however, comes at a price. Frome et al. (2013) and Norouzi et al. (2013) both find that it slightly degrades performance as compared to a plain object classification using standard accuracy metrics (called flat “hit @k metric” in their paper).",
      "startOffset" : 8,
      "endOffset" : 793
    }, {
      "referenceID" : 4,
      "context" : ", 2013; Erk, 2016). Recent work on multi-modal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation. More related to our work are cross-modal mapping models (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014), that learn to transfer a representation of an object or image in the visual space to a vector in a distributional space. When tested on standard object recognition tasks, transfer, however, comes at a price. Frome et al. (2013) and Norouzi et al. (2013) both find that it slightly degrades performance as compared to a plain object classification using standard accuracy metrics (called flat “hit @k metric” in their paper). Interestingly though, Frome et al. (2013) report better performance using “hierarchical precision”, which essentially means that transfer predicts words that are ontologically closer to the gold label and makes “semantically more reasonable errors”.",
      "startOffset" : 8,
      "endOffset" : 1006
    }, {
      "referenceID" : 18,
      "context" : "Levelt et al. (1991)) and based on the simplifying assumption that the name used for referring to an object can be determined successfully without looking at other objects in the image.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : ", 2014), which is based on the SAIAPR image collection (Grubinger et al., 2006) (99.",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 27,
      "context" : "We follow (Schlangen et al., 2016) and select words with a minimum frequency of 40 in these two data sets, which gives us a vocabulary of 793 words.",
      "startOffset" : 10,
      "endOffset" : 34
    }, {
      "referenceID" : 22,
      "context" : "Thus, our experiments are on a smaller scale as compared to (Ordonez et al., 2016).",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 30,
      "context" : "(2016), we derive representations of our visual inputs with a convolutional neural network, ‘GoogleNet’ (Szegedy et al., 2015), which was trained on the ImageNet corpus (Deng et al.",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : ", 2015), which was trained on the ImageNet corpus (Deng et al., 2009), and extract the final fully-connected layer before the classification layer, to give us a 1024 dimensional representation of the region.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "Hu et al. (2015)’s discussion on “stuff” entities such “sky” or “grass” in the REFERIT data.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "Hu et al. (2015)’s discussion on “stuff” entities such “sky” or “grass” in the REFERIT data. For testing, we remove relational REs (containing a relational preposition such as ‘left of X’), because here we cannot be sure that the head noun of the target is fully informative; we also remove REs with more than one head noun from our list (i.e. these are mostly relational expressions as well such as ‘girl laughing at boy’). We pair each image region from the test set with its corresponding names from the remaining REs. Image and Word Embeddings Following Schlangen et al. (2016), we derive representations of our visual inputs with a convolutional neural network, ‘GoogleNet’ (Szegedy et al.",
      "startOffset" : 0,
      "endOffset" : 582
    }, {
      "referenceID" : 0,
      "context" : "As distributional word vectors, we use the word2vec representations provided by Baroni et al. (2014) (trained with CBOW, 5-word context window, 10 negative samples, 400 dimensions).",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 0,
      "context" : "As distributional word vectors, we use the word2vec representations provided by Baroni et al. (2014) (trained with CBOW, 5-word context window, 10 negative samples, 400 dimensions). 4 Three Models of Interfacing Visual and Distributional Information 4.1 Direct Cross-Modal Mapping Following e.g. Lazaridou et al. (2014), referential meaning can be represented as a translation function that projects visual representations of objects to linguistic representations of words in a distributional vector space.",
      "startOffset" : 80,
      "endOffset" : 320
    }, {
      "referenceID" : 0,
      "context" : "As distributional word vectors, we use the word2vec representations provided by Baroni et al. (2014) (trained with CBOW, 5-word context window, 10 negative samples, 400 dimensions). 4 Three Models of Interfacing Visual and Distributional Information 4.1 Direct Cross-Modal Mapping Following e.g. Lazaridou et al. (2014), referential meaning can be represented as a translation function that projects visual representations of objects to linguistic representations of words in a distributional vector space. Thus, in contrast to standard object recognition systems or the other models we will use here, cross-modal mapping does not treat words as individual labels or classifiers, but learns to directly predict continuous representations of words in a vector space, such as the space defined by the word2vec embeddings that we use in this work. This model will be called TRANSFER below. During training, we pair each object with the distributional embedding of its name, and use standard Ridge regression for learning the transformation. Lazaridou et al. (2014) and Lazaridou et al.",
      "startOffset" : 80,
      "endOffset" : 1062
    }, {
      "referenceID" : 0,
      "context" : "As distributional word vectors, we use the word2vec representations provided by Baroni et al. (2014) (trained with CBOW, 5-word context window, 10 negative samples, 400 dimensions). 4 Three Models of Interfacing Visual and Distributional Information 4.1 Direct Cross-Modal Mapping Following e.g. Lazaridou et al. (2014), referential meaning can be represented as a translation function that projects visual representations of objects to linguistic representations of words in a distributional vector space. Thus, in contrast to standard object recognition systems or the other models we will use here, cross-modal mapping does not treat words as individual labels or classifiers, but learns to directly predict continuous representations of words in a vector space, such as the space defined by the word2vec embeddings that we use in this work. This model will be called TRANSFER below. During training, we pair each object with the distributional embedding of its name, and use standard Ridge regression for learning the transformation. Lazaridou et al. (2014) and Lazaridou et al. (2015a) test a range of technical tweaks and different algorithms for cross-modal mapping.",
      "startOffset" : 80,
      "endOffset" : 1091
    }, {
      "referenceID" : 26,
      "context" : "Here, we use Schlangen et al. (2016)’s WAC model, building the training set for each word w as follows: all visual objects in a corpus that have been referred to as w are used as positive instances, the remaining objects as negative instances.",
      "startOffset" : 13,
      "endOffset" : 37
    }, {
      "referenceID" : 21,
      "context" : "In order to extend the model’s vocabulary for zero-shot learning, we follow Norouzi et al. (2013) and associate the top n words with their corresponding distributional vector and compute the convex combination of these vectors.",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 6,
      "context" : "object recognition with known object categories, cross-modal projection or transfer approaches have been reported to perform worse than standard object classification methods (Frome et al., 2013; Norouzi et al., 2013).",
      "startOffset" : 175,
      "endOffset" : 217
    }, {
      "referenceID" : 21,
      "context" : "object recognition with known object categories, cross-modal projection or transfer approaches have been reported to perform worse than standard object classification methods (Frome et al., 2013; Norouzi et al., 2013).",
      "startOffset" : 175,
      "endOffset" : 217
    }, {
      "referenceID" : 27,
      "context" : "1 Model comparison Setup We use the train/test split of REFERIT data as in (Schlangen et al., 2016).",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "This parallels findings by Frome et al. (2013), discussed in Section 2.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 32,
      "context" : "Zarrieß and Schlangen (2016)), would be an interesting direction for more detailed investigation here.",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 6,
      "context" : "Previous work has looked at the feasibility of zero-shot learning as a function of semantic similarity or ontological closeness between unknown and known categories, and confirmed the intuition that the task is harder the less close unknown categories are to known ones (Frome et al., 2013; Norouzi et al., 2013).",
      "startOffset" : 270,
      "endOffset" : 312
    }, {
      "referenceID" : 21,
      "context" : "Previous work has looked at the feasibility of zero-shot learning as a function of semantic similarity or ontological closeness between unknown and known categories, and confirmed the intuition that the task is harder the less close unknown categories are to known ones (Frome et al., 2013; Norouzi et al., 2013).",
      "startOffset" : 270,
      "endOffset" : 312
    }, {
      "referenceID" : 6,
      "context" : "2 Evaluation Some previous work on zero-shot image labeling assumes additional components that first identify whether an image should be labelled by a known or unknown word (Frome et al., 2013).",
      "startOffset" : 173,
      "endOffset" : 193
    }, {
      "referenceID" : 6,
      "context" : "2 Evaluation Some previous work on zero-shot image labeling assumes additional components that first identify whether an image should be labelled by a known or unknown word (Frome et al., 2013). We follow Lazaridou et al. (2014) and let the model decide whether to refer to an object by a known or unknown name.",
      "startOffset" : 174,
      "endOffset" : 229
    }, {
      "referenceID" : 15,
      "context" : "When tested on the full vocabulary, we find that TRANSFER and WAC very rarely generate names whose referents were excluded from training, which is in line with observations made by Lazaridou et al. (2015a). The SIM-WAP predictors generalize much better, in particular on the singular/plural testset.",
      "startOffset" : 181,
      "endOffset" : 206
    }, {
      "referenceID" : 2,
      "context" : "recently collected through conversational agents (Das et al., 2016) that circumvent the need for human-human interaction data.",
      "startOffset" : 49,
      "endOffset" : 67
    } ],
    "year" : 2017,
    "abstractText" : "We compare three recent models of referential word meaning that link visual object representations to lexical representations in a distributional vector space, either directly through cross-modal mapping or indirectly through visual predictors for individual words. We use these models to predict object names as they could be used in naturalistic referring expressions. We find that cross-modal mapping generally produces semantically appropriate and mutually highly similar object names in its topn list, but sometimes fails to make desired distinctions. Visual word predictors, on the other hand, can react to more subtle visual distinctions and select specific terms, but sometimes stray taxonomically very far from the correct one. Combination of the approaches improves over the individual predictions in a standard naming task. All approaches can be extended to the zero-shot naming case, where the correct name is one for which no instances were seen during training; again they show complementary strengths and weaknesses, depending on the setup and the lexical relation of the unattested object name to known ones.",
    "creator" : "TeX"
  }
}