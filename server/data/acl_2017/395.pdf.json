{
  "name" : "395.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "DRL-Sense: Deep Reinforcement Learning for Multi-Sense Word Representations",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recently, deep learning methodologies have dominated several research areas in natural language processing (NLP), such as machine translation, language understanding, and dialogue systems. However, most of applications usually utilize word-level embeddings to obtain semantics. Considering that natural language is highly ambiguous, the standard word embeddings may suffer from polysemy issues. Neelakantan et al. (2014) pointed out that, due to triangle inequality in vector space, if one word embedding has two different senses, the sum of the distance between the word embedding and its synonymous embedding in each sense would be larger or equal to the distance between these synonymous embed-\ndings1. Because this inaccurate distance measurement may degrade downstream NLP tasks, multisense word representations are proposed to address the ambiguity issue in a single word embedding scenario (Reisinger and Mooney, 2010; Huang et al., 2012)\nThis paper proposes DRL-Sense—a novel reinforcement learning based framework for learning multi-sense representations, which is composed by two main modules: a sense selection module for inferring the most probable sense for a word given its context, and a sense representation module for representing word senses in a continuous space. Our modular design implements pure sense-level representation learning while maintaining linear time sense selection. The proposed model is optimized by reinforcement learning and incorporates a non-parametric algorithms and a sense exploration mechanism without changing the network architecture2. Our contributions are four-fold: • We are among the first to study reinforcement\nlearning for sense embedding learning, taking account of the Markov property in sense selection given local contexts. • DRL-Sense is the first system that achieves\npure sense-level representation learning with linear time complexity on sense selection. • We develop non-parametric learning and\nsense exploration mechanisms for a general neural sense selection module to achieve better flexibility and robustness. • Our experimental results show the state-of-\nthe-art performance on contextual word similarities and comparable performance with word2vec (Mikolov et al., 2013b) using only 1/100 size of training data.\n1d(bank, finance) + d(bank, coast) ≥ d(finance, coast) 2The code will be released after the paper gets accepted.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199"
    }, {
      "heading" : "2 Related Work",
      "text" : "There are three dominant types of approaches for learning multi-sense word representations in the literature: 1) clustering methods, 2) probabilistic modeling methods, and 3) lexical ontology based methods. Our reinforcement learning based approach can be loosely connected to clustering methods and probabilistic modeling methods.\nReisinger and Mooney (2010) is the first work to propose multi-sense word representations on vector space based on clustering techniques. With the power of deep learning, some work exploit neural networks to learn embeddings with senseannotated corpus based on clustering (Huang et al., 2012; Neelakantan et al., 2014). Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model based on WordNet (Miller, 1995). Kågebäck et al. (2015) further exploited a weighting mechanism on context in the clustering procedure and evaluated their system on word sense induction. Vu and Parker (2016) proposed an iterative process on the twostage clustering-embedding learning framework. Moreover, Guo et al. (2014) leveraged bilingual resources to perform clustering. However, most of the above approaches separates the clustering procedure and the representation learning procedure, which may suffer from the error propagation issue. Instead, our reinforcement learning model utilizes a reward signal to propagate the statistical information from sense representations to optimize sense selection.\nIn order to make the model more flexible, probabilistic modeling methods are usually utilized for learning multi-sense emnbeddings, where Tian et al. (2014) and Jauhar et al. (2015) conducted probabilistic modeling with EM training. Li and Jurafsky (2015) exploited the Chinese Restaurant Process and demonstrates efficacy of multi-sense word representations on several downstream NLP tasks. Furthermore, Bartunov et al. (2016) developed a non-parametric Bayesian extension on the skip-gram model (Mikolov et al., 2013b). However, all above methods fail to maintain pure sense-level representation learning due to the complicated computation in their EM algorithms (Bartunov et al., 2016), where sense-level embeddings are trained with word-level embeddings.\nRecently, Qiu et al. (2016) proposed an EM algorithm to learn representations without wordlevel embeddings, where the computational cost\nis high when selecting the sense identity sequence because the time for searching all sense combination within a context window is exponential. In contrast, by exploiting embeddings in the sense selection module, our model performs linear time sense selection, while maintaining a pure senselevel representation learning module. Another difference between the prior work and ours is that, Qiu et al. (2016) used WordNet (Miller, 1995) to obtain the number of senses for words, and our solution supports non-parametric learning for automatically deciding the sense number of each word.\nFurthermore, most related work either directly exploited sense representations (Neelakantan et al., 2014; Qiu et al., 2016) or combined with online expectation sense counts (Li and Jurafsky, 2015; Jauhar et al., 2015) to select senses. The proposed approach incorporates a sense selection module, which can be learned together with the representation learning module through reinforcement learning.\nUnlike a lot of relevant work that requires additional resources such as the lexical ontology (Rothe and Schütze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al., 2014; Ettinger et al., 2016; Šuster et al., 2016), which may not be available in the target task, our model can be trained on the corpus only. Also, some prior work proposed to learn topical embeddings and word embeddings jointly in order to consider the contexts (Liu et al., 2015a,b), whereas this paper focuses on learning multi-sense word embeddings."
    }, {
      "heading" : "3 Proposed Approach: DRL-Sense",
      "text" : "This work proposes a framework to learn two key modules for multi-sense word representations: a sense selection module and a sense representation module. The sense selection module decides which sense to use given a text context, whereas the sense representation module learns meaningful representation for sense by their statistical characteristics.\nConsidering that the sense representation module requires the sense identity of each word from the sense selection module, and the sense selection module may also benefit from the semantics carried by sense representations, these two modules should be tangled. Hence, a naive two-stage algorithm or two separate learning algorithms proposed by prior work are not optimal. We propose\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n\uD835\uDC36\uD835\uDC61−\uD835\uDC5A \uD835\uDC36\uD835\uDC61+\uD835\uDC5A\uD835\uDC36\uD835\uDC61 = \uD835\uDC64\uD835\uDC56… …\n\uD835\uDC5E( ഥ\uD835\uDC36\uD835\uDC61 , \uD835\uDC67\uD835\uDC561) \uD835\uDC5E( ഥ\uD835\uDC36\uD835\uDC61, \uD835\uDC67\uD835\uDC562) \uD835\uDC5E( ഥ\uD835\uDC36\uD835\uDC61, \uD835\uDC67\uD835\uDC563)\n\uD835\uDC36\uD835\uDC61′−\uD835\uDC5A \uD835\uDC36\uD835\uDC61′+\uD835\uDC5A\uD835\uDC36\uD835\uDC61′ = \uD835\uDC64\uD835\uDC57… …\n\uD835\uDC5E(\uD835\uDC36\uD835\uDC61′, \uD835\uDC67\uD835\uDC571) \uD835\uDC5E(\uD835\uDC36\uD835\uDC61′, \uD835\uDC67\uD835\uDC572) \uD835\uDC5E(\uD835\uDC36\uD835\uDC61′, \uD835\uDC67\uD835\uDC573)\n\uD835\uDC67\uD835\uDC561\nSense Representation Module\n…\uD835\uDC43(\uD835\uDC67\uD835\uDC572|\uD835\uDC67\uD835\uDC561) \uD835\uDC43(\uD835\uDC67\uD835\uDC62\uD835\uDC63|\uD835\uDC67\uD835\uDC561)\n→ Q-learning reward signal\n← greedy sense selection\nSense Selection Module\n← Q -learn in g rew ard sign al → greed y sen se selectio n\nSense selection for target word \uD835\uDC36\uD835\uDC61 Sense selection for collocated word \uD835\uDC36\uD835\uDC61′\nnegative sampling\nmatrix \uD835\uDC44\uD835\uDC56\nmatrix \uD835\uDC49\nmatrix \uD835\uDC48 matrix \uD835\uDC43\nmatrix \uD835\uDC44\uD835\uDC57\nmatrix \uD835\uDC43\nsample collocation Sense Selection Module\nFigure 1: The RRL-Sense architecture illustration. Two shared-weight sense selection modules surround a sense representation module. The sense selection module infers the target word sense given its context. The selected senses are passed to the sense representation module for learning sense embeddings. Then, the sense embedding collocation is passed back to the sense selection modules for reinforcement learning.\nto learn two modules in a reinforcement learning manner by introducing a novel reward passing procedure to enable joint training. In addition, a nonparametric learning algorithm is incorporated to enable automatic sense induction, where the number of senses for each word is not predefined but automatically learned for practical usage. Also, simple mechanisms are applied to allow the sense exploration while incorporating the sense selection prior."
    }, {
      "heading" : "3.1 Model Architecture",
      "text" : "Our model architecture is illustrated in Figure 1 and detailed below."
    }, {
      "heading" : "3.1.1 Sense Selection Module",
      "text" : "The sense selection module decides the sense for a word given its context. Assuming that a word sense is determined by the local context, we use the Markov property to formulate the sense selection module as a Markov Decision Process (MDP) to infer the most probable sense based on its local context. Given a corpus C and a context window m, we extract a local context C̄t = {Ct−m, ..., Ct−1, Ct+1, ..., Ct+m} as a state St, and the selection of a sense zik ∈ Zi for a word wi = Ct given context C̄t as an action Aik given the state St, where Zi is the set of senses of word wi. However, this formulation lacks a central element of MDP: a reward signal for measuring the fitness of the selected sense zik. Therefore, if we have meaningful sense representations containing statistical estimation, we can use the estimation as a surrogate reward to simulate the reward signal in the sense selection module."
    }, {
      "heading" : "3.1.2 Sense Representation Module",
      "text" : "A successful sense selection module can be applied to each word in a corpus for obtaining its sense identity. Various techniques about word embeddings can be directly employed after mapping all words in a corpus to its sense identity. The typical method is to formulate the sense selection problem as a maximum likelihood estimation (MLE) problem for the collocation likelihood. In this paper, we use the skip-gram formulation (Mikolov et al., 2013b), because it is more light-weight and thus requires less training time: only two sense selections are required for stochastic training, while GloVe (Pennington et al., 2014) and continuous bag-of-words (CBOW) (Mikolov et al., 2013a) require the whole corpus and the whole context window for sense selection respectively.\nSpecifically, we first create input sense embedding matrixU and collocation estimation matrix V as the representations to be learned. Given a word wi and collocated word wj with corresponding local contexts, we map them to their sense identities as zik and zjl by the sense selection module, and maximize the sense collocation log likelihood logCo(·) using a negative sampling approximation (Mikolov et al., 2013b),\nlogCo(zik, zjl | U, V ) = log σ(UTzikVzjl) (1)\n+ M∑ v=1 Ezuv∼pneg(z)[log σ(−U T zik Vzuv)],\nwhere pneg(z) is the distribution over all senses for negative samples.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399"
    }, {
      "heading" : "3.2 Reinforcement Learning",
      "text" : "With the above two modules, we treat the sense collocation likelihood as a surrogate reward signal for the sense selection module, and then cast the whole formulation as a reinforcement learning task: An optimal policy πθ with parameters θ should select the sense that leads to an optimal collocation likelihood in (1). In order to estimate the sense collocation likelihood, two senses, zik and zjl, should be determined, which leads a two-step MDP: one for the target word wi and the other for the collocated word wj within a context window. A policy gradient algorithm (Sutton et al., 1999) can formulate the policies as decision probability distributions πiθ(zik|C̄t) and π j θ(zjl|C̄t′) for words wi, wj given their contexts C̄t, C̄t′ . Hence, a joint formulation can be written as follows,\nmax U,V,θ Ek∼πiθ(·)[El∼πjθ(·)[logCo(zik, zjl | U, V )]]. (2) The neat formulation is differentiable and supports stochastic optimization using doubly stochastic gradient (Lei et al., 2016). However, there are two major drawbacks of this formulation. First, it requires a policy πθ to fit a valid probability distribution, and this constraint underestimates the probability of senses that have not been selected yet during optimization. Second, if using stochastic gradient ascent (detailed in Appendix A) to optimize equation (2), it would actually lower the probability estimation for the selected sense because logCo(·) ≤ 0, which is not desirable if the model accurately select the right sense."
    }, {
      "heading" : "3.2.1 Q-Learning Formulation",
      "text" : "To address the above issue, a Q-learning algorithm (Mnih et al., 2013) with a novel reward passing procedure is applied to this task, because it optimizes the selected action (sense) independently to the other actions. Also, considering that the Qlearning algorithm is not constrained by a jointprobabilistic formulation, we reduce the range of the reward function to facilitate the training process by replacing the collocation log likelihood logCo ∈ (− inf, 0] with (its monotonous) likelihood Co ∈ [0, 1] as the reward function (Mnih et al., 2013).\nOur model utilizes a CBOW architecture to model the fitness for a sense zik of a wordwi under a local context C̄t. Specifically, given a word embedding matrix P , the local context can be modeled as the summation of word embeddings from\nAlgorithm 1: DRL-Sense Training Algorithm for wi = Ct ∈ C do\nsample wj = Ct′(0 < |t′ − t| ≤ m); k = arg maxk′ q(C̄t, zik′); l = arg maxl′ q(C̄t′ , zjl′); optimize U, V by (1) for the sense representation module; optimize P,Q by (4) for the sense selection module;\nits context C̄t. The fitness representation of the selected sense can be formulated with a 3-mode tensorQ, whose dimensions denote words, senses, and latent variables. Then we model the Q-value, q(C̄t, zik), for selecting the sense zik under the context C̄t as\nq(C̄t, zik) = σ(Q T ik ∑ j∈C̄t Pj), (3)\nwhere σ is a sigmoid function to form a valid probabilistic estimation."
    }, {
      "heading" : "3.2.2 DRL-Sense Training",
      "text" : "To jointly train sense selection and sense representation modules, we first sample a collocated word pair wi, wj with respective contexts C̄t, C̄t′ , and use the estimated Q-value to select the most probable senses zik, zjl. The selected senses are passed to the sense representation module to optimize the sense collocation likelihood. Afterwards, the estimated collocation likelihood is passed back as a reward signal to optimize the sense selection module. The loss function is defined as cross-entropy H(·) due to the probability distribution.\nmin P,Q\nH(Co(zik, zjl | U, V ), q(C̄t, zik))\n+H(Co(zik, zjl | U, V ), q(C̄t′ , zjl)). (4)\nAlgorithm 1 shows the proposed DRL-Sense model training procedure. There are two major contributions in our modular design. First, efficient sense selection with word embeddings and pure sense representation learning are simultaneously achieved. Second, reinforcement learning allows both modules to be jointly trained."
    }, {
      "heading" : "3.3 Non-Parametric Learning",
      "text" : "In order to automatically discover the sense number for each word, we incorporate word sense induction into our model (Song et al., 2016). Instead\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n\uD835\uDC36\uD835\uDC61−\uD835\uDC5A \uD835\uDC36\uD835\uDC61+\uD835\uDC5A\n\uD835\uDC5E( ഥ\uD835\uDC36\uD835\uDC61 , \uD835\uDC67\uD835\uDC561) 0.5 0.5 \uD835\uDC5E( ഥ\uD835\uDC36\uD835\uDC61 , \uD835\uDC67\uD835\uDC562)\nIf \uD835\uDC5E( ഥ\uD835\uDC36\uD835\uDC61, \uD835\uDC67\uD835\uDC561) < 0.5 If … < 0.5\nweight vector zero vector selected vector\n\uD835\uDC36\uD835\uDC61 = \uD835\uDC64\uD835\uDC56… … \uD835\uDC36\uD835\uDC61−\uD835\uDC5A \uD835\uDC36\uD835\uDC61+\uD835\uDC5A\n\uD835\uDC5E( ഥ\uD835\uDC36\uD835\uDC61 , \uD835\uDC67\uD835\uDC561) 0.5\n\uD835\uDC36\uD835\uDC61 = \uD835\uDC64\uD835\uDC56… …\nSense Selection Module\nFigure 2: The proposed non-parametric algorithm with initialization. The red block indicates the newly discovered sense zi2 for the target word wi based on the algorithm.\nof non-parametric clustering (Neelakantan et al., 2014) or the stochastic process (Li and Jurafsky, 2015) applied by prior work, this paper proposes a novel mechanism in a general neural model to conduct non-parametric sense discovery.\nBecause the formulated Q-value is a probabilistic estimation, the model allows the selected word to create a new sense when all Q-values of its existing senses are lower than 0.5. Therefore, the prediction layer is initialized to 0 so that all Qvalues remain σ(0) = 0.5 until having been selected due to independent updates in Q-learning. Figure 2 illustrates the sense discovery procedure, where the model expands a word’s sense set when its context has all Q-values of existing senses less than 0.5."
    }, {
      "heading" : "3.4 Word Sense Exploration",
      "text" : "Due to high ambiguity in natural language, a greedy sense selection strategy may not work well in the early training stage, because the sense selection module does not learn well. Such a drawback also exists in the literature using clustering algorithms (Neelakantan et al., 2014; Kågebäck et al., 2015) and hard-EM algorithms (Qiu et al., 2016; Jauhar et al., 2015). This issue is known as exploration-exploitation trade-off (Sutton and Barto, 1998). Li and Jurafsky (2015) proposed to sample on a Chinese Restaurant Process to introduce uncertainty. In our neural network architecture for computing Q-values, we perform dropout (Srivastava et al., 2014) in the hidden layer ∑ j∈C̄t Pj for our sense selection module to introduce uncertainty."
    }, {
      "heading" : "3.5 Sense Selection Prior",
      "text" : "Incorporating priors in sense selection is a key element for probabilistic models (Li and Juraf-\nsky, 2015; Jauhar et al., 2015), most previous work used online selection counts of each sense as its prior. As the key utility of the prior is to incorporate the sense selection preference for a word wi (disregarding its context {Ct−m, ..., Ct−1, Ct+1, ..., Ct+m}) in its sense selection process, we achieve similar prior utility by adding the target word wi into the input of the sense selection module as C̄t = {Ct−m, ..., Ct+m} such that the Q-values indicate the sense preference of the target word."
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluate our proposed DRL-Sense model and compare with other multi-sense word representation models for both quantitative and qualitative experiments."
    }, {
      "heading" : "4.1 Experimental Setup and Configuration",
      "text" : "Our model is trained on the April 2010 Wikipedia dump (Shaoul and Westbury, 2010), which contains approximately 1 billion tokens. For fair comparison, we adopt the same vocabulary set as Huang et al. (2012) and Neelakantan et al. (2014). For preprocessing, we convert all words to lower case, apply the Stanford tokenizer and the Stanford sentence tokenizer (Manning et al., 2014), and remove all sentences with less than 10 tokens.\nIn the experiments, the context window size is set to 6 (|C̄t| = 13). Subsampling technique introduced by word2vec (Mikolov et al., 2013b) is applied to accelerate the training process. The (upper bound of) number of senses per word in Q is set to 3 for fair comparison with prior work (Neelakantan et al., 2014). The learning rate is set to 0.025. Dropout is annealed from 0.5 to 0.0 within the first epoch. The embedding dimension is 300. We initialize Q and V as zeros, and P and U as the uniform distribution [− √ 1/100, √ 1/100] such that each embedding has unit length in expectation (Lei et al., 2015). For the negative sampling distribution in (1), we use 1/3 unigram of a word for each of its sense to compute the negative sampling probability using the 3/4rd power trick in Mikolov et al. (2013b). Our model uses 5 negative senses for negative sampling in (1).\nIn optimization, we conduct mini-batch training with 2048 batch size using the following procedure: 1) select senses in the batch; 2) optimize U, V using stochastic training within the batch for efficiency; 3) optimize P,Q using mini-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nMethod MaxSimC AvgSimC Huang et al. (2012) 26.1 65.7 Neelakantan et al. (2014) 60.1 68.6 Tian et al. (2014) 63.6 65.4 Li and Jurafsky (2015) 66.4 - Bartunov et al. (2016) 53.8 61.2 Qiu et al. (2016) 64.9 66.1 Proposed: DRL-Sense 66.6 65.2 - Non-Parametric 65.1 67.0 - Sense Exploration 66.2 64.5 - Sense Selection Prior 66.3 66.0\nTable 1: Spearman’s rank correlation ρ x100 on the SCWS dataset.\nbatch training for robustness. To further stabilize the reward signal for the sense selection module, we only use the collocated sense σ(UTzik , Vzjl) in (1) to approximate the collocation likelihood Co(zik, zjl | U, V ) for Q-learning in (4)."
    }, {
      "heading" : "4.2 Experiment 1: Contextual Word Similarity",
      "text" : "To evaluate the quality of sense embeddings, we compute the similarity score between each word pair given their respective local contexts and compare with the human-judged score using Stanford’s Contextual Word Similarities (SCWS) data (Huang et al., 2012). Specifically, given a list of word pairs with corresponding contexts, S = {(wi, C̄t, wj , C̄t′)}, we calculate the Spearman’s rank correlation ρ between human-judged similarity and model similarity estimations. Two major contextual similarity estimations are introduced by Reisinger and Mooney (2010): AvgSimC and MaxSimC (also referred to LocalSim in Neelakantan et al. (2014)). AvgSimC is a soft measurement that addresses the contextual information with a probability estimation:\nAvgSimC(wi, C̄t, wj , C̄t′) = Zi∑ k=1 Zj∑ l=1 p(zik|C̄t)p(zjl|C̄t′)d(zik, zjl),\nwhich weights the similarity measurement of each sense zik and zjl by their probability estimations. On the other hand, MaxSimC is a hard measurement that only evaluates on the most probable senses:\nMaxSimC(wi, C̄t, wj , C̄t′) = d(zik, zjl),\nzik = arg max k′ p(zik′ |C̄t), zjl = arg max l′ p(zjl′ |C̄t′).\nThe baselines for comparison include classic clustering methods (Huang et al., 2012; Neelakantan et al., 2014), EM algorithms (Tian et al., 2014; Qiu et al., 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015), where all approaches are trained on the same corpus except Li and Jurafsky (2015)3 and Qiu et al. (2016) used more recent Wikipedia dumps. The embedding sizes of all baselines are 300, except 50 in Huang et al. (2012). In our method, the probability assignment on each sense p(zik|C̄t) is calculated by replacing the sigmoid layer with a softmax layer in our sense selection module as:\n∑"
    }, {
      "heading" : "4.2.1 Embedding Quality Analysis",
      "text" : "Our DRL-Sense model achieves the state-of-theart performance on MaxSimC, demonstrating superior quality on independent sense embeddings. In real world applications, using multiple sense vectors for a word simultaneously may bring additionally computational overhead over conventional single word embedding scenarios, and also change the existing neural network architecture for word embeddings. In contrast, by simply replacing the most probable sense identity with each word identity (as in MaxSimC), the computation framework and cost for downstream NLP tasks remains the same as conventional word embedding approaches. Rothe and Schütze (2015) reported that the traditional single word representation model word2vec (Mikolov et al., 2013b), released by Google and trained on 100 billion tokens Google’s internal dataset, can achieved 66.6% similarity correlation on the SCWS dataset. Table 1 shows that our proposed DRL-Sense achieves comparable performance as word2vec using only 1/100 size of Google’s internal data, while using multi-sense representations in the single embedding scenario (MaxSimC).\n3We use Li and Jurafsky (2015)’s result on 1 billion tokens for fair comparison.\n4More detail is available in Appendix B.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nMethod ESL-50 RD-300 TOEFL-80 (1) Conventional Word Embedding GC 47.73 45.07 60.87 SG 52.08 55.66 66.67 (2) Word Sense Disambiguation IMS+SG 41.67 53.77 66.67 (3) Direct Learning from Corpus EM 27.08 33.96 40.00 NP-MSSG 45.24 50.00 81.16 DRL-Sense 54.76 60.71 72.46 (4) Retrofitting with WordNet Retro-GC 63.64 66.20 71.01 Retro-SG 56.25 65.09 73.33\nTable 2: Accuracy on synonym selection"
    }, {
      "heading" : "4.2.2 Probability Assignment Analysis",
      "text" : "To interpret the performance between MaxSimC and AvgSimC, we claim that an ideal model should have MaxSimC≥AvgSimC in Table 1. The reason is that, any sense selection model seeks a function f(zik | C̄t) to evaluate the fitness of each sense zik under a context C̄t, such as the cluster similarity in clustering methods, the likelihood or vanilla probability estimation in probabilistic models, and the Q-value in our model. Hence, to achieve the optimal expected fitness E[f(zik | C̄t)], the optimal probability assignment on senses is only selecting one and the optimal sense:\npopt(zik | C̄t) = { 1 if k = arg maxk′ f(zik′ |C̄t), 0 otherwise.\nNote that the above probability assignment makes AvgSimC=MaxSimC. Because the expected fitness E[f(zik | C̄t)] over any probability distribution would be no greater than the fitness value over the above distribution, the similar phenomenon (MaxSimC≥AvgSimC) can be observed in the robust fitness estimation (cluster similarity/likelihood/Q-value). Therefore, from Table 1, our DRL-Sense model demonstrates not only superior performance under a single sense embedding scenario but also reasonable and robust sense selection."
    }, {
      "heading" : "4.3 Experiment 2: Synonym Selection",
      "text" : "We further evaluate our model on synonym selection using multi-sense word representations (Jauhar et al., 2015). Three standard synonym selection datasets, ESL-50 (Turney, 2001), RD-300 (Jarmasz and Szpakowicz, 2004), and\nTOEFL-80 (Landauer and Dumais, 1997), are performed. In the datasets, each question consists of a question word wQ and four answer candidates {wA, wB, wC , wD}, and the goal is to select the most semantically synonymous choice among the four candidates. For example, in the TOEFL-80 dataset, a question shows {(Q) enormously, (a) appropriately, (b) uniquely, (c) tremendously, (d) decidedly}, and the answer is (c).\nIn the experiments, our model selects the synonym of the question word wQ by the collocation likelihood as a proxy of their semantic similarity. That is, for pairs of words (wQ, wT ), T ∈ {A,B,C,D}, we select the pair with the maximum bi-directional collocation likelihood,\nSim(wQ, wT )\n= max j∈ZQ,l∈ZT max(Co(zQj , zT l), Co(zT l, zQj))\n= max j∈ZQ,l∈ZT\nmax(σ(UTQjVT l), σ(U T T lVQj)).\nOur model is compared with following baselines: (1) conventional word embedding: global context vectors (GC) (Huang et al., 2012) and skip-gram (SG) (Mikolov et al., 2013b); (2) applying supervised word sense disambiguation (using the IMS system (Zhong and Ng, 2010)) and then applying SG on disambiguated corpus: IMS+SG; (3) direct training multi-sense word representations on corpus: EM algorithm (Jauhar et al., 2015), non-parametric multi-sense skipgram (NP-MSSG) (Neelakantan et al., 2014) and our DRL-Sense; (4) retrofitting existing word embeddings to sense embeddings using WordNet: retrofitting GC (Retro-GC) and retrofitting SG (Retro-SG) (Jauhar et al., 2015). Note that we do not compare with the methods in (4) on Table 1, because sense representations are generated without a sense selector, and thus cannot be evaluated using MaxSimC and AvgSimC. Also, the methods in (4) can be treated as an upperbound of the methods in (3) due to the usage of additional supervised information from WordNet.\nThe experimental results are shown on Table 2. Except for retrofitting methods and NP-MSSG in TOEFL-80, our DRL-Sense model significantly outperforms all baselines. In addition, our model also performs better than Retro-GC for TOEFL80, although Retro-GC uses the supervised knowledge from WordNet, showing that our model obtains good sense embeddings by learning with the robust sense selector. Moreover, our method out-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nSense k-NN Senses mercury-1 mercury comet apollo comets wnba mercury-2 mercury sulfide beryllium zirconium apple-1 apple blackberry macintosh ibm novell apple-2 avocado apple apricot blackberry prunus lucky-1 lucky gotti co-starring gravano starred lucky-2 beyonce singin beenie baby choo lil lucky-3 hilfiger starbucks t-bone puff armani\nTable 3: Sense representations with neighboring words using the collocation likelihood.\nperforms the EM method and NP-MSSG (Neelakantan et al., 2014), despite the highest score of AvgSimC on Table 1, echoing the superior quality of our sense vectors as suggested in the MaxSimC measurement."
    }, {
      "heading" : "4.4 Qualitative Analysis",
      "text" : "We qualitatively evaluate non-parametric learning and sense representation learning performance."
    }, {
      "heading" : "4.4.1 Non-Parametric Learning Effectiveness",
      "text" : "The largest challenge of multi-sense representation learning is that most words can be represented by a single embedding (non-polysemous or a single embedding can model multiple senses well). Hence, the feasible solution is to retain most words a single sense embedding, while leaving polysemous words multiple sense embeddings. Huang et al. (2012) and Neelakantan et al. (2014) only selected approximately 6,000 and 30,000 words from a vocabulary with size ≈ 100,000 for multisense representation learning. Qiu et al. (2016) utilized a coarse version (Navigli et al., 2007) of WordNet inventory (Miller, 1995) to determine the sense number for each word before training. The sense selection module in our learned DRL-Sense produces 71,194 words with a selected sense, 22,271 words with 2 senses, and 5,683 words with 3 senses based on the training data. Without any multi-sense determination procedure prior to training, our method automatically determines only about 30,000 polysemous words, where the number is close to Neelakantan et al. (2014)’s setting, demonstrating the effectiveness of our nonparametric learning algorithm."
    }, {
      "heading" : "4.4.2 Representation Learning Effectiveness",
      "text" : "To evaluate the quality of sense embeddings, we show the k-nearest neighbors (k-NN) for each sense. For example, given the first sense of the word “apple”, denoted as “apple-1”, we select\nFigure 3: The 2-D visualization using t-SNE (Maaten and Hinton, 2008) for the words “mercury” and “apple” on multi-sense word representations with L2 distance neighbors.\nk senses with the highest collocation likelihood from the sense representation module and list in\nTable 35. The learned sense embeddings of the words “mercury” and “apple” clearly correspond to different senses: “planet” (mercury-1), “metal element” (mercury-2), “computer” (apple-1), and “fruit” (apple-2) as visualized in Figure 3. Instead of actual senses, Table 3 also shows that our model is able to automatically detect a finegrained aspect division within a word “lucky” in an unsupervised manner. Specifically, it divides the semantics of “lucky” into three aspects: “starring” (lucky-1), “musician” (lucky-2), and “brands” (lucky-3), expanding the potential usage of the learned embeddings6."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We propose DRL-Sense—a novel deep reinforcement learning framework to jointly learn a word sense selection module and a sense representation module. Our model implements non-parametric learning for word sense induction and exploration for word sense selection. The experiments show that our DRL-Sense model achieves the state-ofthe-art performance for the benchmark contextual word similarity task and most of synonym selection datasets under the same setting. In the future, we plan to investigate reinforcement learning methods to incorporate multi-sense word representations for downstream NLP tasks.\n5For clarity purpose, we omit the sense identity in the kNN senses.\n6Note that the sense induction procedure highly depends on the training corpus, so it may fail to discover all senses in an existing sense inventory.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    }, {
      "heading" : "A Doubly Stochastic Gradient",
      "text" : "For clarity purpose, we abbreviate the collocation log likelihood logCo(ik, jl|U, V ) as L(·) in this section. To derive doubly stochastic gradient for equation (2), we first resolve the expectation form as:\nJ(U, V, θ)\n= Ek∼πiθ(·)[El∼πjθ(·)[L(·)]] = ∑ k ∑ l πiθ(zik|C̄t)π j θ(zjl|C̄t′)L(·).\nThe gradient of J with respect to Θ should be:\n∂J(U, V, θ)\n∂Θ\n= ∂\n∂Θ ∑ k ∑ l πiθ(zik|C̄t)π j θ(zjl|C̄t′)L(·)\n= ∑ k ∑ l πiθ(zik|C̄t)L(·) ∂πjθ(zjl|C̄t′) ∂Θ\n+ ∑ k ∑ l πjθ(zjl|C̄t′)L(·) ∂πiθ(zik|C̄t) ∂Θ\n= Ek∼πiθ(·)[ ∑ l L(·) ∂ log πjθ(zjl|C̄t′) ∂Θ πjθ(zjl|C̄t′)]\n+ E l∼πjθ(·) [ ∑ k L(·) ∂ log πiθ(zik|C̄t) ∂Θ πiθ(zik|C̄t)]\n= Ek∼πiθ(·)[El∼πjθ(·)[L(·) ∂\n∂Θ (log πiθ(zik|C̄t) + log π j θ(zjl|C̄t′))]]\nAccordingly, if we conduct typical stochastic gradient ascent training on J(U, V,Θ) with respect to Θ from samples k and l with a learning rate η, the update formula will be:\n∆Θ = ηL(·) ∂ ∂Θ (log πiθ(zik|C̄t) + log π j θ(zjl|C̄t′)).\nHowever, because the collocation likelihood is a valid probability distribution, the collocation log likelihood should always be non-positive: L(·) = logCo(zik, zjl|U, V ) ≤ 0. Therefore, as long as the collocation log likelihood L is negative, the update formula is to minimize the likelihood of choosing l and k, despite the fact that l and k may be good choices. On the other hand, if the log likelihood reaches 0, it also indicates that the likelihood reaches infinity and computational overflow on U and V ."
    }, {
      "heading" : "B Supplementary Experiments",
      "text" : "In this section, we conduct ablation experiments by removing each components separately in our system to test the efficacy of proposed mechanisms. Specifically, we test the following components:\n• Non-parametric: To remove the nonparametric learning mechanism in our model, we initialize the prediction layer Q in the sense selection module from uniform distribution [− √ 1/100, √ 1/100] as the\nword embedding P .\n• Sense exploration: To remove the word sense exploration mechanism in our model, we turn off the dropout mechanism during training.\n• Sense selection prior: To remove the sense selection prior mechanism in our model, we remove the target word wi in the input of the sense selection module as C̄t = {Ct−m, ..., Ct−1, Ct+1, ..., Ct+m}.\nIn addition, we also test the efficacy of specific module according to their complexity. We test the setting when the embedding dimension in the sense selection module is set to 50 (50D/300D), and the embedding dimensions in the both modules are set to 50 (50D/50D).\nThe experiment results are shown in Table 4. Consistent improvement in terms of MaxSimC from each component to the proposed DRL-Sense model can be observed. In addition, the improvement from increasing the complexity (embedding dimension) demonstrate the efficacy of both modules (50D-50D→50D-300D and 50D300D→DRL-Sense)."
    } ],
    "references" : [ {
      "title" : "Breaking sticks and ambiguities with adaptive skip-gram",
      "author" : [ "Sergey Bartunov", "Dmitry Kondrashkin", "Anton Osokin", "Dmitry Vetrov." ],
      "venue" : "Proceedings of the 19th International Conference on Artificial Intelligence and Statistics page 130138.",
      "citeRegEx" : "Bartunov et al\\.,? 2016",
      "shortCiteRegEx" : "Bartunov et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving distributed representation of word sense via wordnet gloss composition and context clustering",
      "author" : [ "Tao Chen", "Ruifeng Xu", "Yulan He", "Xuan Wang." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "A unified model for word sense representation and disambiguation",
      "author" : [ "Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "EMNLP. Citeseer, pages 1025– 1035.",
      "citeRegEx" : "Chen et al\\.,? 2014",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Retrofitting sense-specific word vectors using parallel text",
      "author" : [ "Allyson Ettinger", "Philip Resnik", "Marine Carpuat." ],
      "venue" : "Proceedings of NAACL-HLT . pages 1378–1383.",
      "citeRegEx" : "Ettinger et al\\.,? 2016",
      "shortCiteRegEx" : "Ettinger et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning sense-specific word embeddings by exploiting bilingual resources",
      "author" : [ "Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu." ],
      "venue" : "COLING. pages 497–507.",
      "citeRegEx" : "Guo et al\\.,? 2014",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving Word Representations via Global Context and Multiple Word Prototypes",
      "author" : [ "Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng." ],
      "venue" : "Annual Meeting of the Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Huang et al\\.,? 2012",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2012
    }, {
      "title" : "Sensembed: Learning sense embeddings for word and relational similarity",
      "author" : [ "Ignacio Iacobacci", "Mohammad Taher Pilehvar", "Roberto Navigli." ],
      "venue" : "ACL (1). pages 95–105.",
      "citeRegEx" : "Iacobacci et al\\.,? 2015",
      "shortCiteRegEx" : "Iacobacci et al\\.",
      "year" : 2015
    }, {
      "title" : "Rogets thesaurus and semantic similarity",
      "author" : [ "Mario Jarmasz", "Stan Szpakowicz." ],
      "venue" : "Recent Advances in Natural Language Processing III: Selected Papers from RANLP 2003:111.",
      "citeRegEx" : "Jarmasz and Szpakowicz.,? 2004",
      "shortCiteRegEx" : "Jarmasz and Szpakowicz.",
      "year" : 2004
    }, {
      "title" : "Ontologically grounded multi-sense representation learning for semantic vector space models",
      "author" : [ "Sujay Kumar Jauhar", "Chris Dyer", "Eduard H Hovy." ],
      "venue" : "HLT-NAACL. pages 683–693.",
      "citeRegEx" : "Jauhar et al\\.,? 2015",
      "shortCiteRegEx" : "Jauhar et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural context embeddings for automatic discovery of word senses",
      "author" : [ "Mikael Kågebäck", "Fredrik Johansson", "Richard Johansson", "Devdatt Dubhashi." ],
      "venue" : "Proceedings of NAACL-HLT . pages 25–32.",
      "citeRegEx" : "Kågebäck et al\\.,? 2015",
      "shortCiteRegEx" : "Kågebäck et al\\.",
      "year" : 2015
    }, {
      "title" : "A solution to plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge",
      "author" : [ "Thomas K Landauer", "Susan T Dumais." ],
      "venue" : "Psychological review 104(2):211.",
      "citeRegEx" : "Landauer and Dumais.,? 1997",
      "shortCiteRegEx" : "Landauer and Dumais.",
      "year" : 1997
    }, {
      "title" : "Molding cnns for text: non-linear, non-consecutive convolutions",
      "author" : [ "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "EMNLP .",
      "citeRegEx" : "Lei et al\\.,? 2015",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2015
    }, {
      "title" : "Rationalizing neural predictions",
      "author" : [ "Tao Lei", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing .",
      "citeRegEx" : "Lei et al\\.,? 2016",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2016
    }, {
      "title" : "Do multi-sense embeddings improve natural language understanding",
      "author" : [ "Jiwei Li", "Dan Jurafsky" ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Li and Jurafsky.,? \\Q2015\\E",
      "shortCiteRegEx" : "Li and Jurafsky.",
      "year" : 2015
    }, {
      "title" : "Learning context-sensitive word embeddings with neural tensor skip-gram model",
      "author" : [ "Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang." ],
      "venue" : "IJCAI. pages 1284–1290.",
      "citeRegEx" : "Liu et al\\.,? 2015a",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Topical word embeddings",
      "author" : [ "Yang Liu", "Zhiyuan Liu", "Tat-Seng Chua", "Maosong Sun." ],
      "venue" : "AAAI. pages 2418–2424.",
      "citeRegEx" : "Liu et al\\.,? 2015b",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Laurens van der Maaten", "Geoffrey Hinton." ],
      "venue" : "Journal of Machine Learning Research 9(Nov):2579–2605.",
      "citeRegEx" : "Maaten and Hinton.,? 2008",
      "shortCiteRegEx" : "Maaten and Hinton.",
      "year" : 2008
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky." ],
      "venue" : "Association for Computational Linguistics",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of Workshop at ICLR .",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A Miller." ],
      "venue" : "Communications of the ACM 38(11):39–",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller." ],
      "venue" : "NIPS Deep Learning Workshop .",
      "citeRegEx" : "Mnih et al\\.,? 2013",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2013
    }, {
      "title" : "Semeval-2007 task 07: Coarsegrained english all-words task",
      "author" : [ "Roberto Navigli", "Kenneth C Litkowski", "Orin Hargraves." ],
      "venue" : "Proceedings of the 4th International Workshop on Semantic Evaluations. Association for Computational Linguistics,",
      "citeRegEx" : "Navigli et al\\.,? 2007",
      "shortCiteRegEx" : "Navigli et al\\.",
      "year" : 2007
    }, {
      "title" : "Efficient nonparametric estimation of multiple embeddings per word in vector space",
      "author" : [ "Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Neelakantan et al\\.,? 2014",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2014
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "volume 14, pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Contextdependent sense embedding",
      "author" : [ "Lin Qiu", "Kewei Tu", "Yong Yu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Qiu et al\\.,? 2016",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2016
    }, {
      "title" : "Multi-prototype vector-space models of word meaning",
      "author" : [ "Joseph Reisinger", "Raymond J Mooney." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Reisinger and Mooney.,? 2010",
      "shortCiteRegEx" : "Reisinger and Mooney.",
      "year" : 2010
    }, {
      "title" : "Autoextend: Extending word embeddings to embeddings for synsets and lexemes",
      "author" : [ "Sascha Rothe", "Hinrich Schütze." ],
      "venue" : "arXiv preprint arXiv:1507.01127 .",
      "citeRegEx" : "Rothe and Schütze.,? 2015",
      "shortCiteRegEx" : "Rothe and Schütze.",
      "year" : 2015
    }, {
      "title" : "The westbury lab wikipedia",
      "author" : [ "Cyrus Shaoul", "Chris Westbury" ],
      "venue" : null,
      "citeRegEx" : "Shaoul and Westbury.,? \\Q2010\\E",
      "shortCiteRegEx" : "Shaoul and Westbury.",
      "year" : 2010
    }, {
      "title" : "Sense embedding learning for word sense induction",
      "author" : [ "Linfeng Song", "Zhiguo Wang", "Haitao Mi", "Daniel Gildea." ],
      "venue" : "arXiv preprint arXiv:1606.05409 .",
      "citeRegEx" : "Song et al\\.,? 2016",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2016
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "Journal of Machine Learning Research 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Bilingual learning of multi-sense embeddings with discrete autoencoders",
      "author" : [ "Simon Šuster", "Ivan Titov", "Gertjan van Noord." ],
      "venue" : "NAACL-HLT 2016 .",
      "citeRegEx" : "Šuster et al\\.,? 2016",
      "shortCiteRegEx" : "Šuster et al\\.",
      "year" : 2016
    }, {
      "title" : "Reinforcement learning: An introduction, volume 1",
      "author" : [ "Richard S Sutton", "Andrew G Barto." ],
      "venue" : "MIT press Cambridge.",
      "citeRegEx" : "Sutton and Barto.,? 1998",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Policy gradient methods for reinforcement learning with function approximation",
      "author" : [ "Richard S Sutton", "David A McAllester", "Satinder P Singh", "Yishay Mansour" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "A probabilistic model for learning multi-prototype word embeddings",
      "author" : [ "Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu." ],
      "venue" : "COLING. pages 151–160.",
      "citeRegEx" : "Tian et al\\.,? 2014",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2014
    }, {
      "title" : "Mining the web for synonyms: Pmi-ir versus lsa on toefl",
      "author" : [ "Peter D Turney." ],
      "venue" : "European Conference on Machine Learning. Springer, pages 491–502.",
      "citeRegEx" : "Turney.,? 2001",
      "shortCiteRegEx" : "Turney.",
      "year" : 2001
    }, {
      "title" : "K-embeddings: Learning conceptual embeddings for words using context",
      "author" : [ "Thuy Vu", "D Stott Parker." ],
      "venue" : "Proceedings of NAACL-HLT . pages 1262–1267.",
      "citeRegEx" : "Vu and Parker.,? 2016",
      "shortCiteRegEx" : "Vu and Parker.",
      "year" : 2016
    }, {
      "title" : "It makes sense: A wide-coverage word sense disambiguation system for free text",
      "author" : [ "Zhi Zhong", "Hwee Tou Ng." ],
      "venue" : "Proceedings of the ACL 2010 System Demonstrations. Association for Computational Linguistics, pages 78–83.",
      "citeRegEx" : "Zhong and Ng.,? 2010",
      "shortCiteRegEx" : "Zhong and Ng.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Because this inaccurate distance measurement may degrade downstream NLP tasks, multisense word representations are proposed to address the ambiguity issue in a single word embedding scenario (Reisinger and Mooney, 2010; Huang et al., 2012) This paper proposes DRL-Sense—a novel reinforcement learning based framework for learning multi-sense representations, which is composed by two main modules: a sense selection module for inferring the most probable sense for a word given its context, and a sense representation module for representing word senses in a continuous space.",
      "startOffset" : 191,
      "endOffset" : 239
    }, {
      "referenceID" : 5,
      "context" : "Because this inaccurate distance measurement may degrade downstream NLP tasks, multisense word representations are proposed to address the ambiguity issue in a single word embedding scenario (Reisinger and Mooney, 2010; Huang et al., 2012) This paper proposes DRL-Sense—a novel reinforcement learning based framework for learning multi-sense representations, which is composed by two main modules: a sense selection module for inferring the most probable sense for a word given its context, and a sense representation module for representing word senses in a continuous space.",
      "startOffset" : 191,
      "endOffset" : 239
    }, {
      "referenceID" : 19,
      "context" : "• Our experimental results show the state-ofthe-art performance on contextual word similarities and comparable performance with word2vec (Mikolov et al., 2013b) using only 1/100 size of training data.",
      "startOffset" : 137,
      "endOffset" : 160
    }, {
      "referenceID" : 20,
      "context" : "Neelakantan et al. (2014) pointed out that, due to triangle inequality in vector space, if one word embedding has two different senses, the sum of the distance between the word embedding and its synonymous embedding in each sense would be larger or equal to the distance between these synonymous embeddings1.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 5,
      "context" : "With the power of deep learning, some work exploit neural networks to learn embeddings with senseannotated corpus based on clustering (Huang et al., 2012; Neelakantan et al., 2014).",
      "startOffset" : 134,
      "endOffset" : 180
    }, {
      "referenceID" : 23,
      "context" : "With the power of deep learning, some work exploit neural networks to learn embeddings with senseannotated corpus based on clustering (Huang et al., 2012; Neelakantan et al., 2014).",
      "startOffset" : 134,
      "endOffset" : 180
    }, {
      "referenceID" : 20,
      "context" : "(2014) replaced the clustering procedure with a word sense disambiguation model based on WordNet (Miller, 1995).",
      "startOffset" : 97,
      "endOffset" : 111
    }, {
      "referenceID" : 19,
      "context" : "(2016) developed a non-parametric Bayesian extension on the skip-gram model (Mikolov et al., 2013b).",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "However, all above methods fail to maintain pure sense-level representation learning due to the complicated computation in their EM algorithms (Bartunov et al., 2016), where sense-level embeddings are trained with word-level embeddings.",
      "startOffset" : 143,
      "endOffset" : 166
    }, {
      "referenceID" : 20,
      "context" : "(2016) used WordNet (Miller, 1995) to obtain the number of senses for words, and our solution supports non-parametric learning for automatically deciding the sense number of each word.",
      "startOffset" : 20,
      "endOffset" : 34
    }, {
      "referenceID" : 23,
      "context" : "Furthermore, most related work either directly exploited sense representations (Neelakantan et al., 2014; Qiu et al., 2016) or combined with online expectation sense counts (Li and Jurafsky, 2015; Jauhar et al.",
      "startOffset" : 79,
      "endOffset" : 123
    }, {
      "referenceID" : 25,
      "context" : "Furthermore, most related work either directly exploited sense representations (Neelakantan et al., 2014; Qiu et al., 2016) or combined with online expectation sense counts (Li and Jurafsky, 2015; Jauhar et al.",
      "startOffset" : 79,
      "endOffset" : 123
    }, {
      "referenceID" : 13,
      "context" : ", 2016) or combined with online expectation sense counts (Li and Jurafsky, 2015; Jauhar et al., 2015) to select senses.",
      "startOffset" : 57,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : ", 2016) or combined with online expectation sense counts (Li and Jurafsky, 2015; Jauhar et al., 2015) to select senses.",
      "startOffset" : 57,
      "endOffset" : 101
    }, {
      "referenceID" : 27,
      "context" : "Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Rothe and Schütze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al.",
      "startOffset" : 94,
      "endOffset" : 183
    }, {
      "referenceID" : 8,
      "context" : "Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Rothe and Schütze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al.",
      "startOffset" : 94,
      "endOffset" : 183
    }, {
      "referenceID" : 1,
      "context" : "Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Rothe and Schütze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al.",
      "startOffset" : 94,
      "endOffset" : 183
    }, {
      "referenceID" : 6,
      "context" : "Unlike a lot of relevant work that requires additional resources such as the lexical ontology (Rothe and Schütze, 2015; Jauhar et al., 2015; Chen et al., 2015; Iacobacci et al., 2015) or bilingual data (Guo et al.",
      "startOffset" : 94,
      "endOffset" : 183
    }, {
      "referenceID" : 4,
      "context" : ", 2015) or bilingual data (Guo et al., 2014; Ettinger et al., 2016; Šuster et al., 2016), which may not be available in the target task, our model can be trained on the corpus only.",
      "startOffset" : 26,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : ", 2015) or bilingual data (Guo et al., 2014; Ettinger et al., 2016; Šuster et al., 2016), which may not be available in the target task, our model can be trained on the corpus only.",
      "startOffset" : 26,
      "endOffset" : 88
    }, {
      "referenceID" : 31,
      "context" : ", 2015) or bilingual data (Guo et al., 2014; Ettinger et al., 2016; Šuster et al., 2016), which may not be available in the target task, our model can be trained on the corpus only.",
      "startOffset" : 26,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "Reisinger and Mooney (2010) is the first work to propose multi-sense word representations on vector space based on clustering techniques.",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model based on WordNet (Miller, 1995).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 0,
      "context" : "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model based on WordNet (Miller, 1995). Kågebäck et al. (2015) further exploited a weighting mechanism on context in the clustering procedure and evaluated their system on word sense induction.",
      "startOffset" : 0,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model based on WordNet (Miller, 1995). Kågebäck et al. (2015) further exploited a weighting mechanism on context in the clustering procedure and evaluated their system on word sense induction. Vu and Parker (2016) proposed an iterative process on the twostage clustering-embedding learning framework.",
      "startOffset" : 0,
      "endOffset" : 300
    }, {
      "referenceID" : 0,
      "context" : "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model based on WordNet (Miller, 1995). Kågebäck et al. (2015) further exploited a weighting mechanism on context in the clustering procedure and evaluated their system on word sense induction. Vu and Parker (2016) proposed an iterative process on the twostage clustering-embedding learning framework. Moreover, Guo et al. (2014) leveraged bilingual resources to perform clustering.",
      "startOffset" : 0,
      "endOffset" : 415
    }, {
      "referenceID" : 0,
      "context" : "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model based on WordNet (Miller, 1995). Kågebäck et al. (2015) further exploited a weighting mechanism on context in the clustering procedure and evaluated their system on word sense induction. Vu and Parker (2016) proposed an iterative process on the twostage clustering-embedding learning framework. Moreover, Guo et al. (2014) leveraged bilingual resources to perform clustering. However, most of the above approaches separates the clustering procedure and the representation learning procedure, which may suffer from the error propagation issue. Instead, our reinforcement learning model utilizes a reward signal to propagate the statistical information from sense representations to optimize sense selection. In order to make the model more flexible, probabilistic modeling methods are usually utilized for learning multi-sense emnbeddings, where Tian et al. (2014) and Jauhar et al.",
      "startOffset" : 0,
      "endOffset" : 956
    }, {
      "referenceID" : 0,
      "context" : "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model based on WordNet (Miller, 1995). Kågebäck et al. (2015) further exploited a weighting mechanism on context in the clustering procedure and evaluated their system on word sense induction. Vu and Parker (2016) proposed an iterative process on the twostage clustering-embedding learning framework. Moreover, Guo et al. (2014) leveraged bilingual resources to perform clustering. However, most of the above approaches separates the clustering procedure and the representation learning procedure, which may suffer from the error propagation issue. Instead, our reinforcement learning model utilizes a reward signal to propagate the statistical information from sense representations to optimize sense selection. In order to make the model more flexible, probabilistic modeling methods are usually utilized for learning multi-sense emnbeddings, where Tian et al. (2014) and Jauhar et al. (2015) conducted probabilistic modeling with EM training.",
      "startOffset" : 0,
      "endOffset" : 981
    }, {
      "referenceID" : 0,
      "context" : "Chen et al. (2014) replaced the clustering procedure with a word sense disambiguation model based on WordNet (Miller, 1995). Kågebäck et al. (2015) further exploited a weighting mechanism on context in the clustering procedure and evaluated their system on word sense induction. Vu and Parker (2016) proposed an iterative process on the twostage clustering-embedding learning framework. Moreover, Guo et al. (2014) leveraged bilingual resources to perform clustering. However, most of the above approaches separates the clustering procedure and the representation learning procedure, which may suffer from the error propagation issue. Instead, our reinforcement learning model utilizes a reward signal to propagate the statistical information from sense representations to optimize sense selection. In order to make the model more flexible, probabilistic modeling methods are usually utilized for learning multi-sense emnbeddings, where Tian et al. (2014) and Jauhar et al. (2015) conducted probabilistic modeling with EM training. Li and Jurafsky (2015) exploited the Chinese Restaurant Process and demonstrates efficacy of multi-sense word representations on several downstream NLP tasks.",
      "startOffset" : 0,
      "endOffset" : 1055
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, Bartunov et al. (2016) developed a non-parametric Bayesian extension on the skip-gram model (Mikolov et al.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, Bartunov et al. (2016) developed a non-parametric Bayesian extension on the skip-gram model (Mikolov et al., 2013b). However, all above methods fail to maintain pure sense-level representation learning due to the complicated computation in their EM algorithms (Bartunov et al., 2016), where sense-level embeddings are trained with word-level embeddings. Recently, Qiu et al. (2016) proposed an EM algorithm to learn representations without wordlevel embeddings, where the computational cost is high when selecting the sense identity sequence because the time for searching all sense combination within a context window is exponential.",
      "startOffset" : 13,
      "endOffset" : 395
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, Bartunov et al. (2016) developed a non-parametric Bayesian extension on the skip-gram model (Mikolov et al., 2013b). However, all above methods fail to maintain pure sense-level representation learning due to the complicated computation in their EM algorithms (Bartunov et al., 2016), where sense-level embeddings are trained with word-level embeddings. Recently, Qiu et al. (2016) proposed an EM algorithm to learn representations without wordlevel embeddings, where the computational cost is high when selecting the sense identity sequence because the time for searching all sense combination within a context window is exponential. In contrast, by exploiting embeddings in the sense selection module, our model performs linear time sense selection, while maintaining a pure senselevel representation learning module. Another difference between the prior work and ours is that, Qiu et al. (2016) used WordNet (Miller, 1995) to obtain the number of senses for words, and our solution supports non-parametric learning for automatically deciding the sense number of each word.",
      "startOffset" : 13,
      "endOffset" : 911
    }, {
      "referenceID" : 19,
      "context" : "In this paper, we use the skip-gram formulation (Mikolov et al., 2013b), because it is more light-weight and thus requires less training time: only two sense selections are required for stochastic training, while GloVe (Pennington et al.",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 24,
      "context" : ", 2013b), because it is more light-weight and thus requires less training time: only two sense selections are required for stochastic training, while GloVe (Pennington et al., 2014) and continuous bag-of-words (CBOW) (Mikolov et al.",
      "startOffset" : 156,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : ", 2014) and continuous bag-of-words (CBOW) (Mikolov et al., 2013a) require the whole corpus and the whole context window for sense selection respectively.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 19,
      "context" : "Given a word wi and collocated word wj with corresponding local contexts, we map them to their sense identities as zik and zjl by the sense selection module, and maximize the sense collocation log likelihood logCo(·) using a negative sampling approximation (Mikolov et al., 2013b),",
      "startOffset" : 257,
      "endOffset" : 280
    }, {
      "referenceID" : 33,
      "context" : "A policy gradient algorithm (Sutton et al., 1999) can formulate the policies as decision probability distributions πi θ(zik|C̄t) and π j θ(zjl|C̄t′) for words wi, wj given their contexts C̄t, C̄t′ .",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 12,
      "context" : "(2) The neat formulation is differentiable and supports stochastic optimization using doubly stochastic gradient (Lei et al., 2016).",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 21,
      "context" : "1 Q-Learning Formulation To address the above issue, a Q-learning algorithm (Mnih et al., 2013) with a novel reward passing procedure is applied to this task, because it optimizes the selected action (sense) independently to the other actions.",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 21,
      "context" : "Also, considering that the Qlearning algorithm is not constrained by a jointprobabilistic formulation, we reduce the range of the reward function to facilitate the training process by replacing the collocation log likelihood logCo ∈ (− inf, 0] with (its monotonous) likelihood Co ∈ [0, 1] as the reward function (Mnih et al., 2013).",
      "startOffset" : 312,
      "endOffset" : 331
    }, {
      "referenceID" : 29,
      "context" : "3 Non-Parametric Learning In order to automatically discover the sense number for each word, we incorporate word sense induction into our model (Song et al., 2016).",
      "startOffset" : 144,
      "endOffset" : 163
    }, {
      "referenceID" : 23,
      "context" : "of non-parametric clustering (Neelakantan et al., 2014) or the stochastic process (Li and Jurafsky, 2015) applied by prior work, this paper proposes a novel mechanism in a general neural model to conduct non-parametric sense discovery.",
      "startOffset" : 29,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : ", 2014) or the stochastic process (Li and Jurafsky, 2015) applied by prior work, this paper proposes a novel mechanism in a general neural model to conduct non-parametric sense discovery.",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 23,
      "context" : "Such a drawback also exists in the literature using clustering algorithms (Neelakantan et al., 2014; Kågebäck et al., 2015) and hard-EM algorithms (Qiu et al.",
      "startOffset" : 74,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "Such a drawback also exists in the literature using clustering algorithms (Neelakantan et al., 2014; Kågebäck et al., 2015) and hard-EM algorithms (Qiu et al.",
      "startOffset" : 74,
      "endOffset" : 123
    }, {
      "referenceID" : 25,
      "context" : ", 2015) and hard-EM algorithms (Qiu et al., 2016; Jauhar et al., 2015).",
      "startOffset" : 31,
      "endOffset" : 70
    }, {
      "referenceID" : 8,
      "context" : ", 2015) and hard-EM algorithms (Qiu et al., 2016; Jauhar et al., 2015).",
      "startOffset" : 31,
      "endOffset" : 70
    }, {
      "referenceID" : 32,
      "context" : "This issue is known as exploration-exploitation trade-off (Sutton and Barto, 1998).",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 30,
      "context" : "In our neural network architecture for computing Q-values, we perform dropout (Srivastava et al., 2014) in the hidden layer ∑ j∈C̄t Pj for our sense selection module to introduce uncertainty.",
      "startOffset" : 78,
      "endOffset" : 103
    }, {
      "referenceID" : 8,
      "context" : ", 2016; Jauhar et al., 2015). This issue is known as exploration-exploitation trade-off (Sutton and Barto, 1998). Li and Jurafsky (2015) proposed to sample on a Chinese Restaurant Process to introduce uncertainty.",
      "startOffset" : 8,
      "endOffset" : 137
    }, {
      "referenceID" : 13,
      "context" : "5 Sense Selection Prior Incorporating priors in sense selection is a key element for probabilistic models (Li and Jurafsky, 2015; Jauhar et al., 2015), most previous work used online selection counts of each sense as its prior.",
      "startOffset" : 106,
      "endOffset" : 150
    }, {
      "referenceID" : 8,
      "context" : "5 Sense Selection Prior Incorporating priors in sense selection is a key element for probabilistic models (Li and Jurafsky, 2015; Jauhar et al., 2015), most previous work used online selection counts of each sense as its prior.",
      "startOffset" : 106,
      "endOffset" : 150
    }, {
      "referenceID" : 28,
      "context" : "Our model is trained on the April 2010 Wikipedia dump (Shaoul and Westbury, 2010), which contains approximately 1 billion tokens.",
      "startOffset" : 54,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "For preprocessing, we convert all words to lower case, apply the Stanford tokenizer and the Stanford sentence tokenizer (Manning et al., 2014), and remove all sentences with less than 10 tokens.",
      "startOffset" : 120,
      "endOffset" : 142
    }, {
      "referenceID" : 19,
      "context" : "Subsampling technique introduced by word2vec (Mikolov et al., 2013b) is applied to accelerate the training process.",
      "startOffset" : 45,
      "endOffset" : 68
    }, {
      "referenceID" : 23,
      "context" : "The (upper bound of) number of senses per word in Q is set to 3 for fair comparison with prior work (Neelakantan et al., 2014).",
      "startOffset" : 100,
      "endOffset" : 126
    }, {
      "referenceID" : 11,
      "context" : "We initialize Q and V as zeros, and P and U as the uniform distribution [− √ 1/100, √ 1/100] such that each embedding has unit length in expectation (Lei et al., 2015).",
      "startOffset" : 149,
      "endOffset" : 167
    }, {
      "referenceID" : 5,
      "context" : "For fair comparison, we adopt the same vocabulary set as Huang et al. (2012) and Neelakantan et al.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "For fair comparison, we adopt the same vocabulary set as Huang et al. (2012) and Neelakantan et al. (2014). For preprocessing, we convert all words to lower case, apply the Stanford tokenizer and the Stanford sentence tokenizer (Manning et al.",
      "startOffset" : 57,
      "endOffset" : 107
    }, {
      "referenceID" : 5,
      "context" : "For fair comparison, we adopt the same vocabulary set as Huang et al. (2012) and Neelakantan et al. (2014). For preprocessing, we convert all words to lower case, apply the Stanford tokenizer and the Stanford sentence tokenizer (Manning et al., 2014), and remove all sentences with less than 10 tokens. In the experiments, the context window size is set to 6 (|C̄t| = 13). Subsampling technique introduced by word2vec (Mikolov et al., 2013b) is applied to accelerate the training process. The (upper bound of) number of senses per word in Q is set to 3 for fair comparison with prior work (Neelakantan et al., 2014). The learning rate is set to 0.025. Dropout is annealed from 0.5 to 0.0 within the first epoch. The embedding dimension is 300. We initialize Q and V as zeros, and P and U as the uniform distribution [− √ 1/100, √ 1/100] such that each embedding has unit length in expectation (Lei et al., 2015). For the negative sampling distribution in (1), we use 1/3 unigram of a word for each of its sense to compute the negative sampling probability using the 3/4rd power trick in Mikolov et al. (2013b). Our model uses 5 negative senses for negative sampling in (1).",
      "startOffset" : 57,
      "endOffset" : 1110
    }, {
      "referenceID" : 4,
      "context" : "Method MaxSimC AvgSimC Huang et al. (2012) 26.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "Method MaxSimC AvgSimC Huang et al. (2012) 26.1 65.7 Neelakantan et al. (2014) 60.",
      "startOffset" : 23,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "Method MaxSimC AvgSimC Huang et al. (2012) 26.1 65.7 Neelakantan et al. (2014) 60.1 68.6 Tian et al. (2014) 63.",
      "startOffset" : 23,
      "endOffset" : 108
    }, {
      "referenceID" : 4,
      "context" : "Method MaxSimC AvgSimC Huang et al. (2012) 26.1 65.7 Neelakantan et al. (2014) 60.1 68.6 Tian et al. (2014) 63.6 65.4 Li and Jurafsky (2015) 66.",
      "startOffset" : 23,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "4 Bartunov et al. (2016) 53.",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "4 Bartunov et al. (2016) 53.8 61.2 Qiu et al. (2016) 64.",
      "startOffset" : 2,
      "endOffset" : 53
    }, {
      "referenceID" : 5,
      "context" : "2 Experiment 1: Contextual Word Similarity To evaluate the quality of sense embeddings, we compute the similarity score between each word pair given their respective local contexts and compare with the human-judged score using Stanford’s Contextual Word Similarities (SCWS) data (Huang et al., 2012).",
      "startOffset" : 279,
      "endOffset" : 299
    }, {
      "referenceID" : 5,
      "context" : "2 Experiment 1: Contextual Word Similarity To evaluate the quality of sense embeddings, we compute the similarity score between each word pair given their respective local contexts and compare with the human-judged score using Stanford’s Contextual Word Similarities (SCWS) data (Huang et al., 2012). Specifically, given a list of word pairs with corresponding contexts, S = {(wi, C̄t, wj , C̄t′)}, we calculate the Spearman’s rank correlation ρ between human-judged similarity and model similarity estimations. Two major contextual similarity estimations are introduced by Reisinger and Mooney (2010): AvgSimC and MaxSimC (also referred to LocalSim in Neelakantan et al.",
      "startOffset" : 280,
      "endOffset" : 602
    }, {
      "referenceID" : 5,
      "context" : "2 Experiment 1: Contextual Word Similarity To evaluate the quality of sense embeddings, we compute the similarity score between each word pair given their respective local contexts and compare with the human-judged score using Stanford’s Contextual Word Similarities (SCWS) data (Huang et al., 2012). Specifically, given a list of word pairs with corresponding contexts, S = {(wi, C̄t, wj , C̄t′)}, we calculate the Spearman’s rank correlation ρ between human-judged similarity and model similarity estimations. Two major contextual similarity estimations are introduced by Reisinger and Mooney (2010): AvgSimC and MaxSimC (also referred to LocalSim in Neelakantan et al. (2014)).",
      "startOffset" : 280,
      "endOffset" : 679
    }, {
      "referenceID" : 5,
      "context" : "The baselines for comparison include classic clustering methods (Huang et al., 2012; Neelakantan et al., 2014), EM algorithms (Tian et al.",
      "startOffset" : 64,
      "endOffset" : 110
    }, {
      "referenceID" : 23,
      "context" : "The baselines for comparison include classic clustering methods (Huang et al., 2012; Neelakantan et al., 2014), EM algorithms (Tian et al.",
      "startOffset" : 64,
      "endOffset" : 110
    }, {
      "referenceID" : 34,
      "context" : ", 2014), EM algorithms (Tian et al., 2014; Qiu et al., 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015), where all approaches are trained on the same corpus except Li and Jurafsky (2015)3 and Qiu et al.",
      "startOffset" : 23,
      "endOffset" : 83
    }, {
      "referenceID" : 25,
      "context" : ", 2014), EM algorithms (Tian et al., 2014; Qiu et al., 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015), where all approaches are trained on the same corpus except Li and Jurafsky (2015)3 and Qiu et al.",
      "startOffset" : 23,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : ", 2014), EM algorithms (Tian et al., 2014; Qiu et al., 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015), where all approaches are trained on the same corpus except Li and Jurafsky (2015)3 and Qiu et al.",
      "startOffset" : 23,
      "endOffset" : 83
    }, {
      "referenceID" : 13,
      "context" : ", 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015), where all approaches are trained on the same corpus except Li and Jurafsky (2015)3 and Qiu et al.",
      "startOffset" : 40,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : ", 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015), where all approaches are trained on the same corpus except Li and Jurafsky (2015)3 and Qiu et al.",
      "startOffset" : 8,
      "endOffset" : 170
    }, {
      "referenceID" : 0,
      "context" : ", 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015), where all approaches are trained on the same corpus except Li and Jurafsky (2015)3 and Qiu et al. (2016) used more recent Wikipedia dumps.",
      "startOffset" : 8,
      "endOffset" : 193
    }, {
      "referenceID" : 0,
      "context" : ", 2016; Bartunov et al., 2016), and Chinese Restaurant Process (Li and Jurafsky, 2015), where all approaches are trained on the same corpus except Li and Jurafsky (2015)3 and Qiu et al. (2016) used more recent Wikipedia dumps. The embedding sizes of all baselines are 300, except 50 in Huang et al. (2012). In our method, the probability assignment on each sense p(zik|C̄t) is calculated by replacing the sigmoid layer with a softmax layer in our sense selection module as:",
      "startOffset" : 8,
      "endOffset" : 306
    }, {
      "referenceID" : 19,
      "context" : "Rothe and Schütze (2015) reported that the traditional single word representation model word2vec (Mikolov et al., 2013b), released by Google and trained on 100 billion tokens Google’s internal dataset, can achieved 66.",
      "startOffset" : 97,
      "endOffset" : 120
    }, {
      "referenceID" : 25,
      "context" : "Rothe and Schütze (2015) reported that the traditional single word representation model word2vec (Mikolov et al.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : "We use Li and Jurafsky (2015)’s result on 1 billion tokens for fair comparison.",
      "startOffset" : 7,
      "endOffset" : 30
    }, {
      "referenceID" : 8,
      "context" : "3 Experiment 2: Synonym Selection We further evaluate our model on synonym selection using multi-sense word representations (Jauhar et al., 2015).",
      "startOffset" : 124,
      "endOffset" : 145
    }, {
      "referenceID" : 35,
      "context" : "Three standard synonym selection datasets, ESL-50 (Turney, 2001), RD-300 (Jarmasz and Szpakowicz, 2004), and TOEFL-80 (Landauer and Dumais, 1997), are performed.",
      "startOffset" : 50,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "Three standard synonym selection datasets, ESL-50 (Turney, 2001), RD-300 (Jarmasz and Szpakowicz, 2004), and TOEFL-80 (Landauer and Dumais, 1997), are performed.",
      "startOffset" : 73,
      "endOffset" : 103
    }, {
      "referenceID" : 10,
      "context" : "Three standard synonym selection datasets, ESL-50 (Turney, 2001), RD-300 (Jarmasz and Szpakowicz, 2004), and TOEFL-80 (Landauer and Dumais, 1997), are performed.",
      "startOffset" : 118,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "Our model is compared with following baselines: (1) conventional word embedding: global context vectors (GC) (Huang et al., 2012) and skip-gram (SG) (Mikolov et al.",
      "startOffset" : 109,
      "endOffset" : 129
    }, {
      "referenceID" : 19,
      "context" : ", 2012) and skip-gram (SG) (Mikolov et al., 2013b); (2) applying supervised word sense disambiguation (using the IMS system (Zhong and Ng, 2010)) and then applying SG on disambiguated corpus: IMS+SG; (3) direct training multi-sense word representations on corpus: EM algorithm (Jauhar et al.",
      "startOffset" : 27,
      "endOffset" : 50
    }, {
      "referenceID" : 37,
      "context" : ", 2013b); (2) applying supervised word sense disambiguation (using the IMS system (Zhong and Ng, 2010)) and then applying SG on disambiguated corpus: IMS+SG; (3) direct training multi-sense word representations on corpus: EM algorithm (Jauhar et al.",
      "startOffset" : 82,
      "endOffset" : 102
    }, {
      "referenceID" : 8,
      "context" : ", 2013b); (2) applying supervised word sense disambiguation (using the IMS system (Zhong and Ng, 2010)) and then applying SG on disambiguated corpus: IMS+SG; (3) direct training multi-sense word representations on corpus: EM algorithm (Jauhar et al., 2015), non-parametric multi-sense skipgram (NP-MSSG) (Neelakantan et al.",
      "startOffset" : 235,
      "endOffset" : 256
    }, {
      "referenceID" : 23,
      "context" : ", 2015), non-parametric multi-sense skipgram (NP-MSSG) (Neelakantan et al., 2014) and our DRL-Sense; (4) retrofitting existing word embeddings to sense embeddings using WordNet: retrofitting GC (Retro-GC) and retrofitting SG (Retro-SG) (Jauhar et al.",
      "startOffset" : 55,
      "endOffset" : 81
    }, {
      "referenceID" : 8,
      "context" : ", 2014) and our DRL-Sense; (4) retrofitting existing word embeddings to sense embeddings using WordNet: retrofitting GC (Retro-GC) and retrofitting SG (Retro-SG) (Jauhar et al., 2015).",
      "startOffset" : 162,
      "endOffset" : 183
    }, {
      "referenceID" : 23,
      "context" : "performs the EM method and NP-MSSG (Neelakantan et al., 2014), despite the highest score of AvgSimC on Table 1, echoing the superior quality of our sense vectors as suggested in the MaxSimC measurement.",
      "startOffset" : 35,
      "endOffset" : 61
    }, {
      "referenceID" : 22,
      "context" : "(2016) utilized a coarse version (Navigli et al., 2007) of WordNet inventory (Miller, 1995) to determine the sense number for each word before training.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : ", 2007) of WordNet inventory (Miller, 1995) to determine the sense number for each word before training.",
      "startOffset" : 29,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "Huang et al. (2012) and Neelakantan et al.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 5,
      "context" : "Huang et al. (2012) and Neelakantan et al. (2014) only selected approximately 6,000 and 30,000 words from a vocabulary with size ≈ 100,000 for multisense representation learning.",
      "startOffset" : 0,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "Huang et al. (2012) and Neelakantan et al. (2014) only selected approximately 6,000 and 30,000 words from a vocabulary with size ≈ 100,000 for multisense representation learning. Qiu et al. (2016) utilized a coarse version (Navigli et al.",
      "startOffset" : 0,
      "endOffset" : 197
    }, {
      "referenceID" : 5,
      "context" : "Huang et al. (2012) and Neelakantan et al. (2014) only selected approximately 6,000 and 30,000 words from a vocabulary with size ≈ 100,000 for multisense representation learning. Qiu et al. (2016) utilized a coarse version (Navigli et al., 2007) of WordNet inventory (Miller, 1995) to determine the sense number for each word before training. The sense selection module in our learned DRL-Sense produces 71,194 words with a selected sense, 22,271 words with 2 senses, and 5,683 words with 3 senses based on the training data. Without any multi-sense determination procedure prior to training, our method automatically determines only about 30,000 polysemous words, where the number is close to Neelakantan et al. (2014)’s setting, demonstrating the effectiveness of our nonparametric learning algorithm.",
      "startOffset" : 0,
      "endOffset" : 720
    }, {
      "referenceID" : 16,
      "context" : "For example, given the first sense of the word “apple”, denoted as “apple-1”, we select Figure 3: The 2-D visualization using t-SNE (Maaten and Hinton, 2008) for the words “mercury” and “apple” on multi-sense word representations with L2 distance neighbors.",
      "startOffset" : 132,
      "endOffset" : 157
    } ],
    "year" : 2017,
    "abstractText" : "This paper proposes DRL-Sense—a multisense word representation learning model, to address the word sense ambiguity issue, where a sense selection module and a sense representation module are jointly learned in a reinforcement learning fashion. A novel reward passing procedure is proposed to enable joint training on the selection and representation modules. The modular design implements pure senselevel representation learning with linear time sense selection (decoding). We further develop a non-parametric learning algorithm and a sense exploration mechanism for better flexibility and robustness. The experiments on benchmark data show that the proposed approach achieves the state-of-the-art performance on contextual word similarities and comparable performance with Google’s word2vec while using much less training data.",
    "creator" : "LaTeX with hyperref package"
  }
}