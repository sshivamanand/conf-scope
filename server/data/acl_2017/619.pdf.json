{
  "name" : "619.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Corpus of Annotated Revisions for Studying Argumentative Writing",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Much of the writing-related NLP research focuses on the analysis of single drafts. Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Brown and Yule, 1983; Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), as well as fine-grained phrasal-level error detection (Leacock et al., 2010; Grammarly, 2016). Less studied is the analysis of between drafts – a comparison of revisions and the properties of the differences. Research on the topic allows a variety of applications: revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014).\nAlthough there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011). In terms of revision anal-\nysis, the most relevant work are on Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of their revisions do not correspond well with other kinds of texts.\nThis work presents an annotated corpus to facilitate revision analysis for argumentative essays. The corpus consists of a collection of three drafts of essays written by college students; the drafts are manually aligned at the sentence level, and the purpose of each revision is manually coded, using a revision schema closely related to argument mining/discourse analysis. Within the domain of argumentative essays, the corpus may be used for research and application of argument mining techniques and argumentative revision analysis. Outside of the domain, the corpus may also be of interest to research on paraphrase comparisons, grammar error correction, and computational stylistics (Popescu and Dinu, 2008; Flekova et al., 2016). We expect the corpus to be useful for advanced writing comparison study (discourse level) that connects to the mainstream research on writing analysis (e.g., argument mining). In this paper, we present two applications of our corpus: 1) Rewriting behavior data analysis 2) Automatic revision purpose classification."
    }, {
      "heading" : "2 Corpus Design Decisions",
      "text" : "Consider this scenario: Alice begins her social science argumentative essay with the sentence: “Electronic communication allows people to make connections beyond physical limits.”\nAn analytical system might (rightly) identify the sentence as the thesis of her essay, and an evaluative system might give the essay a lower score due to this sentence’s vagueness and lack of evidence (though Alice may not know why she re-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nceived that score). Now suppose in a revised draft, Alice expanded the sentence: “Electronic communication allows people to make connections beyond physical limits location and enriches connections that would have been impossible to make otherwise.”\nAn analytical system would still identify the sentence as the thesis, and an evaluative system might raise the overall score a little higher. Alice may become satisfied with the increase and move on. However, there is an opportunity lost – neither systems addressed the quality of her revision.\nA revision analysis system might be helpful for Alice because it would link “limits” to “location and ...” and identify the reason why she made the change – perhaps adding precision. If Alice had intended her change as a way to add evidential support for her thesis, she would see that her attempt was not as successful as she hoped.\nThe above scenario highlights the application of a revision analysis system. This paper is about creating a corpus to enable the development of such systems. Because this is a relatively new problem, there are many possible ways for us to design the corpus. Here, we discuss some of our decisions.\nFirst, we need to define the unit of revision. In the example above illustrates a phrase-aligned revision. While this offers a more precise definition of the scope of a revision, it may be difficult to achieve consistent annotations. For example, the changes may not adhere to any syntactic linguistic unit. For this first corpus, we define our unit of revision to be at the sentence level. In other words, even if a pair of sentences contain multiple edits, the entire sentence pair will be annotated as one sentence revision.\nSecond, we need to define the quality we want to observe about the revision sentence pair. For this first corpus, we focus on recognizing the purpose of the revision, as in the example above. It is a useful property, and it has previously been studied by others in the literature. People have considered both binary purpose categories such as Content vs. Surface (Faigley and Witte, 1981) or Factual vs. Fluency (Bronner and Monz, 2012); and more fine-grained categories (Pfeil et al., 2006; Jones, 2008; Liu and Ram, 2009; Daxenberger and Gurevych, 2012; Zhang and Litman, 2015). Our corpus follows the two-tiered schema used by (Zhang and Litman, 2015) (see Section 3.2).\nThird, we not only have to decide on the an-\nnotation format, we also need to decide on how to obtain the raw text: argumentative essays with multiple drafts. We decided to sample from a population of predominantly college students, inclusive of both native and proficient non-native (aka L2) speakers. Comparing to high school students, college students are expected to have a better organization of the argument elements. Including native and L2 speakers allows for the exploration of possible rewriting differences between writers of varying backgrounds. We decided to give all subjects the same writing prompt and collect three drafts. The identical prompt minimizes the impact of topic difference for argumentation-related study. The collection of three drafts allows for a comparison of revision differences at different stages of rewriting.\nFinally, we need a method of eliciting two revised drafts from each writer. Ideally, an instructor would give formative feedback after each draft for each student, but we do not have the resources to carry out such an expensive project. We decided to simulate instructor feedback by asking students to add more examples after the first draft. To elicit a second revised draft, we tried two strategies: 1) show students a character-based comparison between their first and second draft,1; and 2) show students what an idealized revision analysis system might tell them."
    }, {
      "heading" : "3 The Argumentative Revision Corpus",
      "text" : "Based on the above design decisions, we have developed a corpus of argumentative essays with three drafts and detailed annotations for sentencealigned revisions between each consecutive pair of drafts. The main corpus has five elements; the relationships between them are shown in Figure 1;\n1Code derived from https://code.google.com/ p/google-diff-match-patch/ which implements Myers’ algorithm (Myers, 1986).\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n(a) Interface A. (b) Interface B.\nFigure 2: Screenshot of the interfaces. (a) Interface A with the annotated revision purposes, (b) Interface B with a streamlined character-based diff.\nSection 3.1 describes the procedure of obtaining them. Section 3.2 briefly describes the revision schema we used, and reports the inter-annotator agreement. Additionally, we have collected metadata from the participants who contributed to the corpus (discussed in Section 3.3); these data may be useful for user behavior analysis."
    }, {
      "heading" : "3.1 Corpus Development Procedure",
      "text" : "We have recruited 60 participants aged 18 years and older, among whom 40 were English native speakers and 20 were non-native speakers with sufficient English proficiency.2 The study is carried out in three 40-60 minute sessions over the duration of two weeks.\nDraft1 Each participant begins by completing a pre-study questionnaire (Section 3.3) and writing a short essay online. Participants are instructed to keep the essay around 400 words, making a single main point with two supporting examples. They are given the following prompt:\n“Suppose you’ve been asked to contribute a short op-ed piece for The New York Times. Argue whether the proliferation of electronic communications (e.g., email, text or other social media) enriches or hinders the development of interpersonal relationships.”\nDraft2 A few days after, participants are asked to revise their first draft online based on the following feedback: Strengthen the essay by adding one more example or reasoning for the claim; then\n2i.e., with a TOEFL score higher than 100.\nadd a rebuttal to an opposing idea; keep the essay at 400 words.\nAnnotated Revisions I (Rev12) The two drafts are semi-manually aligned at the sentence level.3 Then, the purpose of each pair of sentence revision is manually coded by a trained annotator, following the annotation guideline (see Section 3.2).\nDraft3 Participants perform their third revision in a lab environment. This time, they are not given additional instructional feedback. Instead, participants are shown a computer interface that highlights the differences between their first and second draft. They are asked to revise the third draft to improve the general quality of their essay. We experimented with two variations of elicitation. Chosen at random, half of the participants are shown Interface A, an interface that highlights the annotated differences between the drafts (Figure 2(a)); half of the participants are shown Interface B, a streamlined character-based diff (Figure 2(b)). Both groups are asked to read a tutorial about their respective interfaces before beginning to revise. Additionally, participants in group A are also asked to verify the manually annotated revision purposes between their first and second draft. After completing the final revision, all participants are given a post-study survey about their experiences (Section 3.3). Additionally, participants in group A are asked to verify the automatically predicted revision purposes between their second and\n3Sentences are first automatically aligned using the algorithm in Zhang and Litman (2014) and then manually corrected by human.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nDraft1 Revision Purpose\nDraft2 Revision Purpose\nDraft3\nThis world has no restriction on who one can talk to. Conventions/ Grammar/ Spelling This world has no restrictions on whom one can talk to.\nThis world has no restrictions on whom one can talk to.\nRebuttal/ Reservation\nUnfortunately, the younger users of digital communication cannot be entirely protected from the rhetoric of any outsider.\nWarrant/ Reasoning/ Backing Modern society is now faced with the issue of cyber bullying as a result.\nThe only aspects of communication that this new development improves are internet navigation and faux internet relatability. WordUsage/ Clarity\nThe only aspects of digital communication that this new development improves are internet navigation and faux internet relatability. WordUsage/ Clarity\nThe only aspects of digital communication that this new development improves are internet navigation and faux internet relationships.\nClaims/ Ideas\nBeing immersed in the sphere of new technologies can allow for complete isolation from the active, non-digital world.\nBeing immersed in the sphere of new technologies can allow for complete isolation from the active, non-digital world.\nTable 1: An example of the annotated corpus. The sentences were aligned across the drafts and the revision purposes were labeled on the aligned sentence pairs. From Draft1 to Draft2, there are two Modify revisions (Spelling and Clarity) and one Add revision. From Draft2 to Draft3, there are two Add revisions (Rebuttal and Reasoning) and one Modify revision (Clarity)\nthird draft (Section 4.2).\nAnnotated Revisions II (Rev23) Regardless of which interface the participant used, the second and third draft are compared and annotated by the trained annotator in the same process as before."
    }, {
      "heading" : "3.2 Revision Annotation Guideline",
      "text" : "Following prior codings, sentence revisions are first coarsely categorized as surface or contentbased changes (Faigley and Witte, 1981), depending on whether any informational content was modified; within each, we distinguish between several finer categories based on the argumentative and discourse writing literature (Kneupper, 1978; Faigley and Witte, 1981; Burstein et al., 2003). Our adapted schema has three Surface categories: Organization, Conventions/Grammar/Spelling, and Word-usage/Clarity; and five Content-based categories: Claim/Ideas, Warrant/Reasoning/Backing, Evidence, Rebuttal/Reservation, and General Content Development. Table 1 shows examples of the aligned sentences in the three collected drafts and their annotated revision categories.\nTwo annotators (one is experienced, and the other is newly trained) annotated the files separately and discussed on the disagreed annotations to remove possible annotation errors due to misun-\nderstanding. After clearing the possible errors, the Kappa on the held-out data is 0.84 on two highlevel categories of Surface vs. Content-based; and 0.71 on their eight low-level categories. The files labeled by the trained annotator were used as the gold standard annotation after the corrections."
    }, {
      "heading" : "3.3 Meta-Data",
      "text" : "In addition to the raw text and the annotations, this corpus release also includes meta-data collected about the participants. This includes: a pre-study survey, a post-study survey, and demographic statistics.\nPre-Study Survey The pre-study survey contains participant demographic information as well as their self-reported writing background, such as their confidence in their writing ability, the number of drafts they typically make, etc.\nPost-Study Survey The post-study survey contains questions about the participants’ in-lab revision experience, such as whether they found the computer interface helpful. All questions are answered on a scale of 5, ranging from “strongly disagree” to “strongly agree”.\nDemographic Statistics Among the 40 native speakers, there were 29 (72.5%) undergraduates, 6 (15%) graduate students, and 5 (12.5%) non-\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nstudents. Among the 20 L2 speakers, there were 4 (20%) undergraduates, and 16 (80%) graduate students; there were 9 Chinese, 2 Bengali, 2 Marathi, 2 Persian, 1 Arabic, 1 Korean, 1 Portuguese, 1 Spanish, and 1 Tamil. In terms of their disciplines, 33 participants (55.9%) were from the natural sciences, 24 (40.7%) from the social sciences, and 2 (3.4%) from the humanities."
    }, {
      "heading" : "3.4 Public Release",
      "text" : "This corpus will be freely available for research purposes, with the first release coordinated with the publication of this paper. This version release will include: the raw text plus revision annotations, and the meta-data. The revision annotations are stored as .xlsx files. There are 60 spreadsheet files for revisions from Draft1 to Draft2 and 60 more spreadsheet files for revisions from Draft2 to Draft3. Each spreadsheet file contains two sheets: Old Draft and New Draft. Each row in the sheet represents one sentence in the corresponding draft. The index of the aligned sentence row in the other draft and the type of the revision on the sentence are recorded. The meta-data are in .log text files. Information in the text files are stored as the JSON data format."
    }, {
      "heading" : "4 Applications of the Corpus",
      "text" : "While the development of a full fledged revision analysis system is outside the scope of this work, we demonstrate the potential applications of our corpus with two examples. The first performs statistical analyses on the collected revision data and meta-data to understand aspects of participant behaviors. The second uses the corpus to train a supervised classifier to automatically predict revision purposes."
    }, {
      "heading" : "4.1 Application: Student Revision Behavior Analysis",
      "text" : "While it is well-established that thoughtful revisions improve one’s writing, and while many college-level writing courses require students to submit multiple drafts on their writing assignments (Addison and McGee, 2010), instructors rarely monitor and provide feedback to students while they revise, partly due to time constraints, partly due to their uncertainty about how to support students revisions (Cole, 2014; Melzer, 2014). There is much we do not know about how to stimulate students to self-reflect and revise."
    }, {
      "heading" : "4.1.1 Hypotheses",
      "text" : "Using the Argumentative Revision Corpus, we can begin to ask and address some questions about student revision habits and behaviors. Our first question is: How do different types of revision feedback impact student revision? And relatedly: Does student background (native vs. L2) make a difference?\nWe mine the corpus to test the following hypotheses:\nH1. There is a difference in participants’ revising behaviors depending on which interface is used to elicit the third draft.\nH2. For participants who used Interface A, if the recognized revision purpose differs from the participants’ revision intention, participants will further modify their revision.\nH3. L2 and native speakers have different behaviors in making revisions.\nH1 and H2 address the first question; H3 addresses the second."
    }, {
      "heading" : "4.1.2 Methodology",
      "text" : "To test the hypotheses, we will use both subjective and objective measures. Subjective measures are based on participant post-study survey answers. Ideally, objective measures should be based on an assessment of improvements in the revised drafts; since we do not have evaluative data at this time, we approximate the degree of improvement with the number of revisions, since these two quantities were demonstrated to be positively correlated (Zhang and Litman, 2015). The objective measures are computed from Tables 2 and 3.\nTo compare differences between specific subgroups on the subjective and objective measures, we conduct ANOVA tests with two factors. For example, one factor is the native language of the participant, and another is the interface used. To\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nRevision Purpose Draft1 to Draft2 Draft2 to Draft3#Add #Delete #Modify #Add #Delete #Modify Content 294 179 33 320 27 16 Claims/Ideas 25 8 4 5 0 0 Warrant/Reasoning/Backing 166 83 7 191 13 3 Rebuttal/Reservation 23 1 0 13 0 0 General Content 50 80 18 86 13 13 Evidence 30 7 4 25 1 0 Surface 0 0 466 0 0 422 Word Usage/Clarity 0 0 362 0 0 357 Conventions/Grammar/Spelling 0 0 75 0 0 52 Organization 0 0 29 0 0 13\nTable 3: Number of revisions, by fine-grain revision purposes and edit types (add, delete, modify).\ndetermine correlation between quantitative measures, we conduct Pearson and Spearman correlation tests."
    }, {
      "heading" : "4.1.3 Results and Discussions",
      "text" : "Testing for H1 Comparing Group A and Group B participants, we observe some differences. First, we detect that Group A agrees with the statement “The system helps me to recognize the weakness of my essay” more so than Group B (Group A has a mean ratings of 3.97 (“Agree”) while Group B’s is 3.17 (“Neutral”), p < .003). Second, in Group A, there is a trending positive correlation between the number of revisions from Draft2 to Draft3 and the ratings for the statement “The system encourages me to make more revisions than I usually make” (ρ=.33 and p < .07); whereas there is no such correlation for Group B. Additional information about revision purposes may elicit a stronger self-reflection response in Group A participants. In contrast, in Group B, there is a significant negative correlation between the number of revisions from Draft1 to Draft2 and ratings for the statement “it is convenient to view my previous revisions with the system” (ρ=-.36 and p < .05). This suggests that the character-based interface is ineffective when participants have to reflect on many changes.\nOn the other hand, when comparing the number of revisions made by Group A and Group B on Rev23 (controlling for their Rev12 numbers), we did not find a significant difference.\nWith no main effect for interface group we cannot verify that H1 is true; possibly a larger pool of participants is needed; or possibly the assignment is not extensive enough (in length and in the number of drafts).\nTesting for H2 Focusing on the 30 participants from Group A, we check the impact of the feed-\nback Rev12 on how they subsequently revise (Rev23). We counted the number of times in which the participant disagrees with the revision purpose assigned by the annotator in Rev12. Of those, we then count the number of times the corresponding sentences were further revised (i.e., in Rev23). Of the 53 sentences where the participants disagreed with the annotator, 45 were further revised in the third draft. The ratio is 0.849, much higher than the overall ratio of general Rev12 revisions being further revised in Rev23 (161/394 = 0.408) and the ratio of the agreed Rev12 revisions being revised in Rev23 (67/341 = 0.196). In further analysis, a Pearson correlation test is conducted to check the correlation between the number of Rev23 and the number of disagreements for different types of agreement/disagreements, controlling for the number of Rev12. We find a negative correlation between Rev23 and the number of cases in which the revisions annotated as Content are verified by the participants; we also find a positive correlation between Rev23 and the number of cases in which the revisions annotated as Surface are intended to be Content revisions by the participants. Both findings are consistent with H2, which suggests that participants will revise further if they perceive that their intended revisions were not recognized.\nTesting for H3 We observe that native and L2 speakers exhibit different behaviors. First, we detect a significant difference in the number of Content and Surface revisions made by L2 and native speakers (p < .02 and p < .03). More specifically, native speakers tend to make more Content changes while the L2 speakers are likely to make more Surface changes. Second, there is also a significant interaction effect among two factors of Group and users’ native language (p < .021) on their ratings for the statement “the system helps\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nme to recognize the weakness of my essay”. Third, we observe a significant positive correlation in the native group between the number of content revisions in Rev23 and the ratings of the statement “the system encourages me to make more revisions than I usually make” (ρ=.4 and p < .009). This suggests that giving feedback (from either interface) encourages native speakers to make more content revisions. Finally, in the L2 group, there is a significant negative correlation between the number of surface revisions in Rev12 and the ratings for the statement “the system helps me to recognize the weakness of my essay” (ρ=-.57 and p < .008). This shows that giving feedback to L2 speakers is less helpful when they make more surface revisions. These results are consistent with H3.\nSummary Experimental findings over the three hypotheses suggest that feedback on revisions do impact how students review and rewrite their drafts. However, there are many factors at play, including the interface design and the students’ linguistic backgrounds."
    }, {
      "heading" : "4.2 Application: Automatic Revision Identification",
      "text" : "Another application of the corpus is to serve as the gold standard for training and evaluation of an automatic revision analysis system. One subtask of such a system is to predict the intention of the revision purpose. This task was previously investigated by Zhang and Litman (2015). They developed and reported the performance of a binary classifier for each individual revision category using features from the argument mining and discourse analysis researches. The availability of our corpus makes it possible for researchers to replicate their findings and conduct further studies."
    }, {
      "heading" : "4.2.1 Hypotheses",
      "text" : "In this paper, we repeat the experiment of Zhang and Litman (2015) under different settings to investigate three new hypotheses that can now be investigated given the features of our corpus:\nH4. The method used in Zhang and Litman (2015) for high school writings is also useful for the writings of college students.\nH5. The same revision classification method works differently for first revision attempts and second revision attempts.\nH6. The revision classification model trained\non L2 essays has a different preference from the model trained on native essays."
    }, {
      "heading" : "4.2.2 Methodology",
      "text" : "We followed the work in Zhang and Litman (2015), where the unigram feature was used as the baseline and the SVM classifier was used as the classifier. Besides the unigram feature, three groups of features that were used in argument mining and discourse analysis researches were extracted (Location, Textual and Language) (Burstein et al., 2001; Falakmasir et al., 2014). Unweighted average F-score for each category is reported.\nFor H4. 10-fold (participant) cross-validation is conducted on all the essays in the corpus. We compared the results using unigram features and the results using all the features. If H4 is true, we should expect a similar improvement over the unigram baseline using our corpus.\nFor H5. 10-fold cross-validation was conducted for the revisions from Draft1 to Draft2 and revisions from Draft2 to Draft3 separately. We compared the improvement ratio brought by the advanced features over the unigram baseline.\nFor H6. In this experiment we trained two classifiers separately with L2 and native essays with all the features. 20 native participants were first randomly selected as the test data. Afterwards classifiers were trained separately using the 20 L2 participants’ essays and the remaining 20 native participants’ essays. We would expect that the performance of the two trained classifiers is different on the same test data."
    }, {
      "heading" : "4.2.3 Results and Discussion",
      "text" : "The first two rows of Table 4 support H4. We observe that the method used in Zhang and Litman (2015) significantly improves performance (compared to a unigram baseline) for half of the classification tasks, which is similar to Zhang and Litman’s results on high school (primarily L1) writing. In our corpus, performance on Claim, Evidence, Rebuttal and Organization was not significantly better than the baseline, possibly due to the limited number of training samples for these categories (Table 3).\nFor H5, the four rows in the middle of Table 4 show the difference of the cross-validation results on first attempt revisions and second attempt revisions. The earlier results using all the revisions, versus now just using only Rev12 or Rev23 re-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nExperiments Text-based SurfaceClaim Warrant General Evidence Rebuttal Org. Word Conv 10fold + All Revs + Unigram 0.49 0.58 0.48 0.49 0.49 0.49 0.73 0.49 10fold + All Revs + All features 0.49 0.77∗ 0.55∗ 0.50 0.49 0.49 0.86∗ 0.62∗ 10fold + Rev12 + Unigram 0.50 0.58 0.47 0.50 0.50 0.50 0.57 0.62 10fold + Rev12 + All features 0.50 0.77∗ 0.56∗ 0.50 0.50 0.50 0.72∗ 0.72∗ 10fold + Rev23 + Unigram 0.50 0.46 0.53 0.49 0.50 0.50 0.58 0.46 10fold + Rev23 + All features 0.50 0.60∗ 0.65∗ 0.49 0.50 0.50 0.78∗ 0.50 20 L2 (train) + 20 Native (test) 0.50 0.72 0.48 0.49 0.50 0.50 0.83 0.63 20 Native (train) + 20 Native (test) 0.50 0.76 0.52 0.49 0.50 0.50 0.89 0.54\nTable 4: Average unweighted F-score for each binary classification task; The first 6 rows shows the average value of 10-fold cross-validation. ∗ indicates significantly better than unigram baseline (p < .05); The last 2 rows shows the F-value for training on L2/Native data and testing on Native data. Bold indicates larger than the number in the other row.\nvisions are similar, which provides little support for H5. With one exception, the features proposed in (Zhang and Litman, 2015) could again significantly improve the performance over the unigram baseline, for the same set of categories as when using all the revisions. However, for the Conventions/Grammar/Spelling category, we did not observe a significant improvement for revisions from Draft2 and Draft3. A possible explanation is that there is a bigger difference in the writers’ rewriting behavior from Draft2 to Draft3, which increases the difficulty of prediction.\nThe last two rows of Table 4 support H6. Interestingly, we observe a better performance on Warrant, General and Word Usage/Clarity with a classifier trained and tested using native essays. Perhaps essays of native speakers are more similar to each other when revised along these dimensions. For Conventions/Grammar/Spelling, in contrast, the classifier trained on L2 data yields a better performance on the same native test set. This may be because the L2 revisions usually include more spelling/grammar corrections."
    }, {
      "heading" : "5 Conclusion and Future Works",
      "text" : "In this paper we present a new corpus for writing comparison research. Currently the corpus focuses on revision analysis over essay revisions made by both native and L2 college students. In addition to three drafts of essays, we have compared and analyzed the drafts to align semantically identical sentences and assigned revision purposes for each aligned revision sentence pairs. We have also conducted two separate studies to demonstrate the application of the corpus for revision behavior analysis and for automatic revision purpose classification.\nWe plan to further augment the corpus to advance research on revision analysis in the future. Some potential augmentations include: more finegrained revision categories, other properties of the revisions, such as statement strength (Tan and Lee, 2014), sub-sentential scopes, and evaluative measures on the revisions."
    } ],
    "references" : [ {
      "title" : "Writing in high school/writing in college: Research trends and future directions",
      "author" : [ "Joanne Addison", "Sharon James McGee." ],
      "venue" : "College Composition and Communication pages 147–179.",
      "citeRegEx" : "Addison and McGee.,? 2010",
      "shortCiteRegEx" : "Addison and McGee.",
      "year" : 2010
    }, {
      "title" : "Automated essay scoring with e-rater R",
      "author" : [ "Yigal Attali", "Jill Burstein" ],
      "venue" : null,
      "citeRegEx" : "Attali and Burstein.,? \\Q2006\\E",
      "shortCiteRegEx" : "Attali and Burstein.",
      "year" : 2006
    }, {
      "title" : "User edits classification using document revision histories",
      "author" : [ "Amit Bronner", "Christof Monz." ],
      "venue" : "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,",
      "citeRegEx" : "Bronner and Monz.,? 2012",
      "shortCiteRegEx" : "Bronner and Monz.",
      "year" : 2012
    }, {
      "title" : "Discourse analysis",
      "author" : [ "Gillian Brown", "George Yule." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Brown and Yule.,? 1983",
      "shortCiteRegEx" : "Brown and Yule.",
      "year" : 1983
    }, {
      "title" : "Automated essay scoring for nonnative english speakers",
      "author" : [ "Jill Burstein", "Martin Chodorow." ],
      "venue" : "Proceedings of a Symposium on Computer Mediated Language Assessment and Evaluation in Natural Language Processing. Association for Compu-",
      "citeRegEx" : "Burstein and Chodorow.,? 1999",
      "shortCiteRegEx" : "Burstein and Chodorow.",
      "year" : 1999
    }, {
      "title" : "Towards automatic classification of discourse elements in essays",
      "author" : [ "Jill Burstein", "Daniel Marcu", "Slava Andreyev", "Martin Chodorow." ],
      "venue" : "Proceedings of the 39th annual Meeting on Association for Computational Linguistics. Association for Compu-",
      "citeRegEx" : "Burstein et al\\.,? 2001",
      "shortCiteRegEx" : "Burstein et al\\.",
      "year" : 2001
    }, {
      "title" : "Finding the WRITE stuff: Automatic identification of discourse structure in student essays",
      "author" : [ "Jill Burstein", "Daniel Marcu", "Kevin Knight." ],
      "venue" : "IEEE Intelligent Systems 18(1):32–39.",
      "citeRegEx" : "Burstein et al\\.,? 2003",
      "shortCiteRegEx" : "Burstein et al\\.",
      "year" : 2003
    }, {
      "title" : "What if the earth is flat? working with, not against, faculty concerns about grammar in student writing",
      "author" : [ "Daniel Cole." ],
      "venue" : "The WAC Journal 25:7–35.",
      "citeRegEx" : "Cole.,? 2014",
      "shortCiteRegEx" : "Cole.",
      "year" : 2014
    }, {
      "title" : "Building a large annotated corpus of learner English: The NUS corpus of learner English",
      "author" : [ "Daniel Dahlmeier", "Hwee Tou Ng", "Siew Mei Wu." ],
      "venue" : "Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications.",
      "citeRegEx" : "Dahlmeier et al\\.,? 2013",
      "shortCiteRegEx" : "Dahlmeier et al\\.",
      "year" : 2013
    }, {
      "title" : "A corpus-based study of edit categories in featured and non-featured wikipedia articles",
      "author" : [ "Johannes Daxenberger", "Iryna Gurevych." ],
      "venue" : "COLING. pages 711–726.",
      "citeRegEx" : "Daxenberger and Gurevych.,? 2012",
      "shortCiteRegEx" : "Daxenberger and Gurevych.",
      "year" : 2012
    }, {
      "title" : "Automatically classifying edit categories in Wikipedia revisions",
      "author" : [ "Johannes Daxenberger", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguis-",
      "citeRegEx" : "Daxenberger and Gurevych.,? 2013",
      "shortCiteRegEx" : "Daxenberger and Gurevych.",
      "year" : 2013
    }, {
      "title" : "Automatically constructing a corpus of sentential paraphrases",
      "author" : [ "William B Dolan", "Chris Brockett." ],
      "venue" : "Proc. of IWP.",
      "citeRegEx" : "Dolan and Brockett.,? 2005",
      "shortCiteRegEx" : "Dolan and Brockett.",
      "year" : 2005
    }, {
      "title" : "Analyzing revision",
      "author" : [ "Lester Faigley", "Stephen Witte." ],
      "venue" : "College composition and communication pages 400–414.",
      "citeRegEx" : "Faigley and Witte.,? 1981",
      "shortCiteRegEx" : "Faigley and Witte.",
      "year" : 1981
    }, {
      "title" : "Identifying thesis and conclusion statements in student essays to scaffold peer review",
      "author" : [ "Mohammad Hassan Falakmasir", "Kevin D Ashley", "Christian D Schunn", "Diane J Litman." ],
      "venue" : "International Conference on Intelligent Tutoring Systems.",
      "citeRegEx" : "Falakmasir et al\\.,? 2014",
      "shortCiteRegEx" : "Falakmasir et al\\.",
      "year" : 2014
    }, {
      "title" : "Exploring stylistic variation with age and income on twitter",
      "author" : [ "Lucie Flekova", "Daniel Preoţiuc-Pietro", "Lyle Ungar." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for",
      "citeRegEx" : "Flekova et al\\.,? 2016",
      "shortCiteRegEx" : "Flekova et al\\.",
      "year" : 2016
    }, {
      "title" : "Patterns of revision in online writing a study of wikipedia’s featured articles",
      "author" : [ "John Jones." ],
      "venue" : "Written Communication 25(2):262–289.",
      "citeRegEx" : "Jones.,? 2008",
      "shortCiteRegEx" : "Jones.",
      "year" : 2008
    }, {
      "title" : "Teaching argument: An introduction to the toulmin model",
      "author" : [ "Charles W Kneupper." ],
      "venue" : "College Composition and Communication 29(3):237–241.",
      "citeRegEx" : "Kneupper.,? 1978",
      "shortCiteRegEx" : "Kneupper.",
      "year" : 1978
    }, {
      "title" : "Automated grammatical error detection for language learners",
      "author" : [ "Claudia Leacock", "Martin Chodorow", "Michael Gamon", "Joel Tetreault." ],
      "venue" : "Synthesis lectures on human language technologies 3(1):1–134.",
      "citeRegEx" : "Leacock et al\\.,? 2010",
      "shortCiteRegEx" : "Leacock et al\\.",
      "year" : 2010
    }, {
      "title" : "Who does what: Collaboration patterns in the wikipedia and their impact on data quality",
      "author" : [ "Jun Liu", "Sudha Ram." ],
      "venue" : "19th Workshop on Information Technologies and Systems. pages 175–180.",
      "citeRegEx" : "Liu and Ram.,? 2009",
      "shortCiteRegEx" : "Liu and Ram.",
      "year" : 2009
    }, {
      "title" : "A generate and rank approach to sentence paraphrasing",
      "author" : [ "Prodromos Malakasiotis", "Ion Androutsopoulos." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
      "citeRegEx" : "Malakasiotis and Androutsopoulos.,? 2011",
      "shortCiteRegEx" : "Malakasiotis and Androutsopoulos.",
      "year" : 2011
    }, {
      "title" : "The connected curriculum: Designing a vertical transfer writing curriculum",
      "author" : [ "Dan Melzer." ],
      "venue" : "The WAC Journal 25:78–91.",
      "citeRegEx" : "Melzer.,? 2014",
      "shortCiteRegEx" : "Melzer.",
      "year" : 2014
    }, {
      "title" : "Ano (nd) difference algorithm and its variations",
      "author" : [ "Eugene W Myers." ],
      "venue" : "Algorithmica 1(1-4):251–266.",
      "citeRegEx" : "Myers.,? 1986",
      "shortCiteRegEx" : "Myers.",
      "year" : 1986
    }, {
      "title" : "End-to-end argumentation mining in student essays",
      "author" : [ "Isaac Persing", "Vincent Ng." ],
      "venue" : "Proceedings of NAACL-HLT . pages 1384–1394.",
      "citeRegEx" : "Persing and Ng.,? 2016",
      "shortCiteRegEx" : "Persing and Ng.",
      "year" : 2016
    }, {
      "title" : "Cultural differences in collaborative authoring of wikipedia",
      "author" : [ "Ulrike Pfeil", "Panayiotis Zaphiris", "Chee Siang Ang." ],
      "venue" : "Journal of Computer-Mediated Communication 12(1):88–113. https://doi.org/10.1111/j.1083-6101.2006.00316.x.",
      "citeRegEx" : "Pfeil et al\\.,? 2006",
      "shortCiteRegEx" : "Pfeil et al\\.",
      "year" : 2006
    }, {
      "title" : "Rank distance as a stylistic similarity",
      "author" : [ "Marius Popescu", "Liviu P. Dinu." ],
      "venue" : "Coling 2008: Companion volume: Posters. Coling 2008 Organizing Committee, Manchester, UK, pages 91–94. http://www.aclweb.org/anthology/C08-2023.",
      "citeRegEx" : "Popescu and Dinu.,? 2008",
      "shortCiteRegEx" : "Popescu and Dinu.",
      "year" : 2008
    }, {
      "title" : "Correction detection and error type selection as an esl educational aid",
      "author" : [ "Ben Swanson", "Elif Yamangil." ],
      "venue" : "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Swanson and Yamangil.,? 2012",
      "shortCiteRegEx" : "Swanson and Yamangil.",
      "year" : 2012
    }, {
      "title" : "A corpus of sentence-level revisions in academic writing: A step towards understanding statement strength in communication",
      "author" : [ "Chenhao Tan", "Lillian Lee." ],
      "venue" : "Proceedings of ACL (short paper).",
      "citeRegEx" : "Tan and Lee.,? 2014",
      "shortCiteRegEx" : "Tan and Lee.",
      "year" : 2014
    }, {
      "title" : "Improved correction detection in revised esl sentences",
      "author" : [ "Huichao Xue", "Rebecca Hwa." ],
      "venue" : "ACL (2). pages 599–604.",
      "citeRegEx" : "Xue and Hwa.,? 2014",
      "shortCiteRegEx" : "Xue and Hwa.",
      "year" : 2014
    }, {
      "title" : "A new dataset and method for automatically grading ESOL texts",
      "author" : [ "Helen Yannakoudakis", "Ted Briscoe", "Ben Medlock." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume",
      "citeRegEx" : "Yannakoudakis et al\\.,? 2011",
      "shortCiteRegEx" : "Yannakoudakis et al\\.",
      "year" : 2011
    }, {
      "title" : "Sentence-level rewriting detection",
      "author" : [ "Fan Zhang", "Diane Litman." ],
      "venue" : "Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications. Association for Computational Linguistics, Baltimore, Maryland, pages",
      "citeRegEx" : "Zhang and Litman.,? 2014",
      "shortCiteRegEx" : "Zhang and Litman.",
      "year" : 2014
    }, {
      "title" : "Annotation and classification of argumentative writing revisions",
      "author" : [ "Fan Zhang", "Diane Litman." ],
      "venue" : "Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications. Association for Computational",
      "citeRegEx" : "Zhang and Litman.,? 2015",
      "shortCiteRegEx" : "Zhang and Litman.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Brown and Yule, 1983; Burstein et al.",
      "startOffset" : 51,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Brown and Yule, 1983; Burstein et al.",
      "startOffset" : 51,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Brown and Yule, 1983; Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), as well as fine-grained phrasal-level error detection (Leacock et al.",
      "startOffset" : 145,
      "endOffset" : 237
    }, {
      "referenceID" : 6,
      "context" : "Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Brown and Yule, 1983; Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), as well as fine-grained phrasal-level error detection (Leacock et al.",
      "startOffset" : 145,
      "endOffset" : 237
    }, {
      "referenceID" : 13,
      "context" : "Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Brown and Yule, 1983; Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), as well as fine-grained phrasal-level error detection (Leacock et al.",
      "startOffset" : 145,
      "endOffset" : 237
    }, {
      "referenceID" : 22,
      "context" : "Examples include document-level quality assessment (Attali and Burstein, 2006; Burstein and Chodorow, 1999), discourse-level analysis and mining (Brown and Yule, 1983; Burstein et al., 2003; Falakmasir et al., 2014; Persing and Ng, 2016), as well as fine-grained phrasal-level error detection (Leacock et al.",
      "startOffset" : 145,
      "endOffset" : 237
    }, {
      "referenceID" : 17,
      "context" : ", 2014; Persing and Ng, 2016), as well as fine-grained phrasal-level error detection (Leacock et al., 2010; Grammarly, 2016).",
      "startOffset" : 85,
      "endOffset" : 124
    }, {
      "referenceID" : 30,
      "context" : "Research on the topic allows a variety of applications: revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014).",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 19,
      "context" : "Research on the topic allows a variety of applications: revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014).",
      "startOffset" : 111,
      "endOffset" : 151
    }, {
      "referenceID" : 25,
      "context" : "Research on the topic allows a variety of applications: revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014).",
      "startOffset" : 177,
      "endOffset" : 224
    }, {
      "referenceID" : 27,
      "context" : "Research on the topic allows a variety of applications: revision analysis (Zhang and Litman, 2015), paraphrase (Malakasiotis and Androutsopoulos, 2011) and correction detection (Swanson and Yamangil, 2012; Xue and Hwa, 2014).",
      "startOffset" : 177,
      "endOffset" : 224
    }, {
      "referenceID" : 11,
      "context" : "Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al.",
      "startOffset" : 176,
      "endOffset" : 221
    }, {
      "referenceID" : 26,
      "context" : "Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al.",
      "startOffset" : 176,
      "endOffset" : 221
    }, {
      "referenceID" : 8,
      "context" : "Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011).",
      "startOffset" : 250,
      "endOffset" : 302
    }, {
      "referenceID" : 28,
      "context" : "Although there are some corpora resources for NLP research on writing comparisons, most tend to be between individual sentences/phrases for tasks such as paraphrase comparison (Dolan and Brockett, 2005; Tan and Lee, 2014) or grammar error correction (Dahlmeier et al., 2013; Yannakoudakis et al., 2011).",
      "startOffset" : 250,
      "endOffset" : 302
    }, {
      "referenceID" : 10,
      "context" : "In terms of revision analysis, the most relevant work are on Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of their revisions do not correspond well with other kinds of texts.",
      "startOffset" : 81,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "In terms of revision analysis, the most relevant work are on Wikipedia revisions (Daxenberger and Gurevych, 2013; Bronner and Monz, 2012); however, the domain of Wikipedia is so specialized that the properties of their revisions do not correspond well with other kinds of texts.",
      "startOffset" : 81,
      "endOffset" : 137
    }, {
      "referenceID" : 24,
      "context" : "Outside of the domain, the corpus may also be of interest to research on paraphrase comparisons, grammar error correction, and computational stylistics (Popescu and Dinu, 2008; Flekova et al., 2016).",
      "startOffset" : 152,
      "endOffset" : 198
    }, {
      "referenceID" : 14,
      "context" : "Outside of the domain, the corpus may also be of interest to research on paraphrase comparisons, grammar error correction, and computational stylistics (Popescu and Dinu, 2008; Flekova et al., 2016).",
      "startOffset" : 152,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "Surface (Faigley and Witte, 1981) or Factual vs.",
      "startOffset" : 8,
      "endOffset" : 33
    }, {
      "referenceID" : 2,
      "context" : "Fluency (Bronner and Monz, 2012); and more fine-grained categories (Pfeil et al.",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 23,
      "context" : "Fluency (Bronner and Monz, 2012); and more fine-grained categories (Pfeil et al., 2006; Jones, 2008; Liu and Ram, 2009; Daxenberger and Gurevych, 2012; Zhang and Litman, 2015).",
      "startOffset" : 67,
      "endOffset" : 175
    }, {
      "referenceID" : 15,
      "context" : "Fluency (Bronner and Monz, 2012); and more fine-grained categories (Pfeil et al., 2006; Jones, 2008; Liu and Ram, 2009; Daxenberger and Gurevych, 2012; Zhang and Litman, 2015).",
      "startOffset" : 67,
      "endOffset" : 175
    }, {
      "referenceID" : 18,
      "context" : "Fluency (Bronner and Monz, 2012); and more fine-grained categories (Pfeil et al., 2006; Jones, 2008; Liu and Ram, 2009; Daxenberger and Gurevych, 2012; Zhang and Litman, 2015).",
      "startOffset" : 67,
      "endOffset" : 175
    }, {
      "referenceID" : 9,
      "context" : "Fluency (Bronner and Monz, 2012); and more fine-grained categories (Pfeil et al., 2006; Jones, 2008; Liu and Ram, 2009; Daxenberger and Gurevych, 2012; Zhang and Litman, 2015).",
      "startOffset" : 67,
      "endOffset" : 175
    }, {
      "referenceID" : 30,
      "context" : "Fluency (Bronner and Monz, 2012); and more fine-grained categories (Pfeil et al., 2006; Jones, 2008; Liu and Ram, 2009; Daxenberger and Gurevych, 2012; Zhang and Litman, 2015).",
      "startOffset" : 67,
      "endOffset" : 175
    }, {
      "referenceID" : 30,
      "context" : "Our corpus follows the two-tiered schema used by (Zhang and Litman, 2015) (see Section 3.",
      "startOffset" : 49,
      "endOffset" : 73
    }, {
      "referenceID" : 21,
      "context" : "com/ p/google-diff-match-patch/ which implements Myers’ algorithm (Myers, 1986).",
      "startOffset" : 66,
      "endOffset" : 79
    }, {
      "referenceID" : 29,
      "context" : "Sentences are first automatically aligned using the algorithm in Zhang and Litman (2014) and then manually corrected by human.",
      "startOffset" : 65,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : "Following prior codings, sentence revisions are first coarsely categorized as surface or contentbased changes (Faigley and Witte, 1981), depending on whether any informational content was modified; within each, we distinguish between several finer categories based on the argumentative and discourse writing literature (Kneupper, 1978; Faigley and Witte, 1981; Burstein et al.",
      "startOffset" : 110,
      "endOffset" : 135
    }, {
      "referenceID" : 16,
      "context" : "Following prior codings, sentence revisions are first coarsely categorized as surface or contentbased changes (Faigley and Witte, 1981), depending on whether any informational content was modified; within each, we distinguish between several finer categories based on the argumentative and discourse writing literature (Kneupper, 1978; Faigley and Witte, 1981; Burstein et al., 2003).",
      "startOffset" : 319,
      "endOffset" : 383
    }, {
      "referenceID" : 12,
      "context" : "Following prior codings, sentence revisions are first coarsely categorized as surface or contentbased changes (Faigley and Witte, 1981), depending on whether any informational content was modified; within each, we distinguish between several finer categories based on the argumentative and discourse writing literature (Kneupper, 1978; Faigley and Witte, 1981; Burstein et al., 2003).",
      "startOffset" : 319,
      "endOffset" : 383
    }, {
      "referenceID" : 6,
      "context" : "Following prior codings, sentence revisions are first coarsely categorized as surface or contentbased changes (Faigley and Witte, 1981), depending on whether any informational content was modified; within each, we distinguish between several finer categories based on the argumentative and discourse writing literature (Kneupper, 1978; Faigley and Witte, 1981; Burstein et al., 2003).",
      "startOffset" : 319,
      "endOffset" : 383
    }, {
      "referenceID" : 0,
      "context" : "While it is well-established that thoughtful revisions improve one’s writing, and while many college-level writing courses require students to submit multiple drafts on their writing assignments (Addison and McGee, 2010), instructors rarely monitor and provide feedback to students while they revise, partly due to time constraints, partly due to their uncertainty about how to support students revisions (Cole, 2014; Melzer, 2014).",
      "startOffset" : 195,
      "endOffset" : 220
    }, {
      "referenceID" : 7,
      "context" : "While it is well-established that thoughtful revisions improve one’s writing, and while many college-level writing courses require students to submit multiple drafts on their writing assignments (Addison and McGee, 2010), instructors rarely monitor and provide feedback to students while they revise, partly due to time constraints, partly due to their uncertainty about how to support students revisions (Cole, 2014; Melzer, 2014).",
      "startOffset" : 405,
      "endOffset" : 431
    }, {
      "referenceID" : 20,
      "context" : "While it is well-established that thoughtful revisions improve one’s writing, and while many college-level writing courses require students to submit multiple drafts on their writing assignments (Addison and McGee, 2010), instructors rarely monitor and provide feedback to students while they revise, partly due to time constraints, partly due to their uncertainty about how to support students revisions (Cole, 2014; Melzer, 2014).",
      "startOffset" : 405,
      "endOffset" : 431
    }, {
      "referenceID" : 30,
      "context" : "Ideally, objective measures should be based on an assessment of improvements in the revised drafts; since we do not have evaluative data at this time, we approximate the degree of improvement with the number of revisions, since these two quantities were demonstrated to be positively correlated (Zhang and Litman, 2015).",
      "startOffset" : 295,
      "endOffset" : 319
    }, {
      "referenceID" : 29,
      "context" : "This task was previously investigated by Zhang and Litman (2015). They developed and reported the performance of a binary classifier for each individual revision category using features from the argument mining and discourse analysis researches.",
      "startOffset" : 41,
      "endOffset" : 65
    }, {
      "referenceID" : 29,
      "context" : "1 Hypotheses In this paper, we repeat the experiment of Zhang and Litman (2015) under different settings to investigate three new hypotheses that can now be investigated given the features of our corpus: H4.",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 29,
      "context" : "1 Hypotheses In this paper, we repeat the experiment of Zhang and Litman (2015) under different settings to investigate three new hypotheses that can now be investigated given the features of our corpus: H4. The method used in Zhang and Litman (2015) for high school writings is also useful for the writings of college students.",
      "startOffset" : 56,
      "endOffset" : 251
    }, {
      "referenceID" : 5,
      "context" : "Besides the unigram feature, three groups of features that were used in argument mining and discourse analysis researches were extracted (Location, Textual and Language) (Burstein et al., 2001; Falakmasir et al., 2014).",
      "startOffset" : 170,
      "endOffset" : 218
    }, {
      "referenceID" : 13,
      "context" : "Besides the unigram feature, three groups of features that were used in argument mining and discourse analysis researches were extracted (Location, Textual and Language) (Burstein et al., 2001; Falakmasir et al., 2014).",
      "startOffset" : 170,
      "endOffset" : 218
    }, {
      "referenceID" : 26,
      "context" : "2 Methodology We followed the work in Zhang and Litman (2015), where the unigram feature was used as the baseline and the SVM classifier was used as the classifier.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 29,
      "context" : "We observe that the method used in Zhang and Litman (2015) significantly improves performance (compared to a unigram baseline) for half of the classification tasks, which is similar to Zhang and Litman’s results on high school (primarily L1) writing.",
      "startOffset" : 35,
      "endOffset" : 59
    }, {
      "referenceID" : 30,
      "context" : "With one exception, the features proposed in (Zhang and Litman, 2015) could again significantly improve the performance over the unigram baseline, for the same set of categories as when using all the revisions.",
      "startOffset" : 45,
      "endOffset" : 69
    }, {
      "referenceID" : 26,
      "context" : "Some potential augmentations include: more finegrained revision categories, other properties of the revisions, such as statement strength (Tan and Lee, 2014), sub-sentential scopes, and evaluative measures on the revisions.",
      "startOffset" : 138,
      "endOffset" : 157
    } ],
    "year" : 2017,
    "abstractText" : "This paper presents a new corpus of annotated revisions of argumentative essays. This corpus analyzes between-draft revisions in the context of the full essay. The writer’s intention for each revision is labeled with categories analogous to those used for argument mining and discourse analysis. The corpus enables more advanced research in writing comparison and revision analysis. Applications of the corpus are demonstrated via a study on student revision behaviors and a study on automatic revision intention prediction.",
    "creator" : "LaTeX with hyperref package"
  }
}