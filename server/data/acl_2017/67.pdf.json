{
  "name" : "67.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Constructing Semantic Hierarchies via Fusion Learning Architecture",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Ontologies and semantic thesauri (Miller, 1995; Suchanek et al., 2007) are significant for many natural language processing applications. The main components of ontologies and semantic thesauri are semantic hierarchies (see in Figure 1). In the WordNet, semantic hierarchies are organized in the form of “is-a” relations. For instance, the words “dog” and “canine” have such relation, and we call “canine” is a hypernym of “dog”. Conversely, “dog” is a hyponym of “canine”. The hypernym-hyponym (“is-a”) relation is the main relationship in semantic hierarchies. However, such manual semantic hierarchies construction as WordNet (Miller, 1995) and YAGO (Suchanek et al., 2007), the primary problem is the tradeoff between coverage scope and human labor. A number of papers have proposed some approach\nto extract semantic hierarchies automatically.\nhypernym-hyponym relation discovery is the key point of semantic hierarchies construction, also the major challenge. The usage of the context is a bottleneck in improving performance of hypernym-hyponym relation discovery. Several works focus on designing or learning lexical patterns (Hearst, 1992; Snow et al., 2004) via observing context of hypernym-hyponym relation, which suffer from covering a small proportion of complex linguistic circumstances. Besides, distributional inclusion hypothesis, which states that hypernyms tend to occur in a superset of contexts in which their hyponyms are found. In other words, hypernyms are semantically broader terms than their hyponyms (Kotlerman et al., 2010; Lenci and Benotto, 2012). However, it is not always rational. To acquire more contexts of words, Fu (2013) applies a web mining method to discover the hypernyms of Chinese entities from multiple sources, assuming that the hypernyms of an entity co-occur with it frequently. The method works well for named entities. But for class names with wider range of meanings, this assumption may fail.\nWord embedding is a kind of low-dimensional\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nand dense real-valued vector encoding context information. Inspired by Mikolov (2013b) who founded that word embeddings can capture a considerable amount of syntactic/semantic relations, and found that hypernym-hyponym relations are complicated and a uniform linear projection cannot fit all of the hypernym-hyponym word pairs, Fu (2014) proposed an architecture for learning semantic hierarchies via word embeddings with clustered hypernym-hyponym relation word pairs in advanced. But the method just focuses on linear transformation of word embeddings, using shallow level semantic of the representation. Besides, the method need clustering for hypernymhyponym relation word pairs in advanced.\nSince word embeddings can capture a considerable amount of syntactic/semantic relations (Mikolov et al., 2013b), we considered constructing a uniform architecture for semantic hierarchies learning based on nonlinear transformation of word embeddings. Inspired by advantages of discriminative model and generative model (see in Section 3), we fuse the two kind of models into one architecture. Considering word embeddings encode context information but ignore the lexical structures which contain some degree of semantic information, we integrate a very simple lexical structure rule into the previous fusion architecture aiming at building semantic hierarchies construction (see in Section 3.4).\nFor evaluation, the experimental results show that our method achieves an F1-score of 74.20% which outperforms the previous state-of-the-art methods. Moreover, and gets a much higher precision-value of 91.60%. Combining our method with the manually-built hierarchy can further improve F-score to 82.01%. The main contributions of our work are as follows:\n• We present a uniform fusion architecture which can learn semantic hierarchies via word embeddings without any background knowledge.\n• The method we proposed outperforms the state-of-the-art methods on a manually labeled test dataset especially with a good enough precision-value for application.\n• The fusion learning architecture is languageindependent which can be easily expanded to be suitable for other languages."
    }, {
      "heading" : "2 Related Work",
      "text" : "During the early phase of semantic hierarchies study, some focused on building manually-built semantic resources, WordNet (Miller, 1995) is a representative thesauri among them. Such manuallybuilt hierarchies have exact structure and high accuracy, but their coverage is limited, especially for fine-grained concepts and entities. Some researchers presented automatic approaches for supplementing manually-built semantic resources. Suchanek et al. (2007) linked the categories in Wikipedia onto WordNet in construction of YAGO. However, the coverage is still limited by the scope of Wikipedia.\nThe major challenge for building semantic hierarchies is the discovery of hypernym-hyponym relations automatically. The usage of the context is a bottleneck in improving performance of discovery of hypernym-hyponym relations. Some researchers proposed method based on lexical pattern abstracted from context manually or automatically to mine hypernym-hyponym relations. Hearst (1992) pointed out that certain lexical constructions linking two different noun phases (NPs) often imply hypernym-hyponym relation. A representative example is “such NP1 as NP2”. Considering time-consuming of manuallybuilt lexical patterns, Snow et al. (2004) proposed a automatic method extracting large numbers of lexico-syntactic patterns to detect hypernym relations from a large newswire corpus. But the method suffers from semantic drift of auto-extracted patterns. Generally speaking, these pattern-based methods often suffer from low recall-value or precision-value because of the coverage and quality of extracted patterns.\nSome measures rely on the assumption that hypernyms are semantically broader terms than their hyponyms. The assumption is a variation of the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009). The pioneer work by Kotlerman et al. (2010) designed a directional distributional measure to infer hypernym–hyponym relations. Differently from Kotlerman et al. (2010), Lenci and Benotto (2012) focus on applying directional, asymmetric similarity measures to identify hypernyms. However the broader semantics hypothesis may not always infer broader contexts (Fu et al., 2014).\nConsidering the behavior of a person explor-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\ning the meaning of an unknown entity, Fu (2013) applies a web mining method to discover the hypernyms of Chinese entities from multiple sources. The assumption is that the hypernyms will co-occur with its hyponym entity frequently. But the assumption maybe failed when involved with concept words which have boarder semantic compared with entities. Inspired by the fact (Mikolov et al., 2013b) that word embeddings can capture a considerable amount of syntactic/semantic relations(e.g. v(king) - v(queen) ≈ v(man) - v(woman), where v(w) is the word embedding of the word w), Fu (2014) present an approach to learn semantic hierarchies with clustered hypernym-hyponym relation word embedding pairs. However the method just focuses on linear transformation of word embeddings, using shallow level semantic of the representation. Besides, the method need clustering for hypernym-hyponym relation word pairs in advanced and the precision-value on test data is not good enough for practical application. Shwartz et al. (2016) included additional linguistic information for LSTM-based learning, but the method has co-occurrence requirements for hyponym-hypernym pairs in corpus.\nEnlightened from good properties of word embeddings for capturing semantic relationship between words in work of Fu (2014), we further explore capacity of word embedding for semantic hierarchies using neural networks based on a fusion learning architecture. Above all, the method we proposed do not need clustering for hypernymhyponym relation word pairs in advanced."
    }, {
      "heading" : "3 Method",
      "text" : "Given the hypernyms list of a word, our goal is building a semantic hierarchies construction of these hypernyms and the given word(Fu et al., 2014), the process is presented in Figure 2.\nThe whole information all we have to figure out whether there exists a hypernym-hyponym relation for the word pair is word embeddings representation. Two major kind of architectures for such problem in general, one is discriminative and another is generative.\nDiscriminative Architecture: Discriminative architecture regards the discovery of hypernymhypernym relation as a classification task, the process of learning semantic hierarchies equal to classify word pair into yes or no for whether exists\n乌头 aconite\n药品 medicine\n植物 plant\n毛茛科 Ranunculaceae\n植物药 medicine plant\n乌头属 Aconitum\n生物 organism\n乌头 aconite\n植物药 medicine plant 乌头属\nAconitum\n药品 medicine 毛茛科\nRanunculaceae\n植物 plant\n生物 organism\nFigure 2: An example of learning semantic hierarchies\nhypernym-hypernym relation. Generative Architecture: Generative architecture focus on generating the hypernym of a given hyponym directly. Direct generation is usual impractical, so generative method produces a fake target which is very similar with the true one.\nWe consider fusing these two models to discovery hypernym-hyponym relation much more precisely. We use Multilayer Perceptron (MLP) (Rosenblatt, 1961) to achieve a generative model and Recurrent Neural Network (RNN) (Mikolov et al., 2010) to implement a discriminative model."
    }, {
      "heading" : "3.1 Word Embedding Training",
      "text" : "Various methods for learning word embeddings have been proposed in the recent years, such as neural net language models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2013b) and spectral models (Dhillon et al., 2011). More recently, Mikolov et al. (2013a) propose two loglinear models, namely the Skip-gram and CBOW model, for inducing word embeddings efficiently on a large-scale corpus because of their low time complexity. Additionally, their experiment results have shown that the Skip-gram model performs best in identifying semantic relationship among words. For this reason, we employ the Skip-gram model for estimating word embeddings in this study."
    }, {
      "heading" : "3.2 Generative Architecture Based on MLP",
      "text" : "Multilayer Perceptron (MLP) (Rosenblatt, 1961) is a feedforward artificial neural network which maps inputs onto a set of appropriate outputs. An MLP consists of multiple layers connecting each layer fully connected to the next one. Except for the input nodes, each node is a neuron with a nonlinear activation function(e.g. sigmoid). MLP is a modification of the standard linear perceptron and\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\ncan distinguish data that are not linearly separable (Cybenko, 1989).\nFor single hidden layer MLP, there are input layer x, hidden layer h and output layer y range from bottom to top. The value of neurons in each layer is a nonlinear projection of the previous layer. Formalization denotation in formulas are as follows:\nh = fh(Whx+ bh) (1)\ny = fy(Wyh+ by) (2)\nWhere Wh,Wy are matrices for linear projection. And fh, fy are nonlinear activation functions for nonlinear transformation.\nIn our work, we use Multilayer Perceptron as the main component for generative architecture. The inputs of MLP is word embedding representation of hyponym and outputs a fake hypernym embedding which is very similar with the true hypernym vector. The model produces final result by calculating the distance between the fake hypernym and the candidate hypernym word in continuous space, subsequently, comparing the distance and a predefined threshold to give a judgment. By adjusting the threshold value of similarity, we expect MLP model obtain much higher precision compared with the discriminative one."
    }, {
      "heading" : "3.3 Discriminative Architecture Based on RNN",
      "text" : "Recurrent Neural Network (RNN) (Mikolov et al., 2010) is a kind of artificial neural network in which connections between neuron form a directed cycle, creating an internal state of the network which allows it to exhibit dynamic temporal behavior according to the history information. Unlike feedforward neural networks (e.g. MLP), RNNs can use their internal memory to process future inputs. The features of RNN makes them applicable to tasks such as unsegmented connected handwriting or speech recognition. There are some variants of original RNN, the most representative one of them is Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) which is capable of learning long-term dependencies. In this paper we use the original simple recurrent networks (SRN). There are two major classes of SRN, know as Elman networks (Elman, 1990) and Jordan networks (Jordan, 1997). In this paper, we use Elman networks as the main component for discriminative architecture (see in Figure 3).\n\uD835\uDC65\n\uD835\uDC66\nℎ\n\uD835\uDC49\n\uD835\uDC48\n\uD835\uDC4A\nUnfold\n\uD835\uDC65\uD835\uDC61−1\n\uD835\uDC66\uD835\uDC61−1\nℎ\uD835\uDC61−1 \uD835\uDC49\n\uD835\uDC4A \uD835\uDC4A\n\uD835\uDC65\uD835\uDC61\n\uD835\uDC66\uD835\uDC61\nℎ\uD835\uDC61 \uD835\uDC49\n\uD835\uDC4A\n\uD835\uDC65\uD835\uDC61+1\n\uD835\uDC66\uD835\uDC61+1\nℎ\uD835\uDC61+1 \uD835\uDC49\n\uD835\uDC4A\nFigure 3: Recurrent Neural Network (Elman networks)\nIn Elman networks, the main architecture is composed of three classes layers, namely, input layer x, hidden layer h, and output layer y. Different from feedforward neural networks, the hidden layer ht is depended on the previous time step hidden layer ht−1 and the current time step input layer xt−1. And the output layer yt is update by the current time step hidden layer ht . Formalization denotation in formulas are as follows:\nht = fh(Uxt +Wht−1 + bh) (3)\nyt = fy(V ht + by) (4)\nWhere U, V,W are matrices for linear projection. And fh, fy are nonlinear activation functions for nonlinear transformation.\nThe inputs of RNN is word embeddings representation of hyponym and candidate hypernym sequence. We regard the hyponym and candidate hypernym as a sequence, because that the judgment of candidate hypernym is depended on hyponym in discrimination process. Ignoring the outputs during recurrent process, we take the final output of the last input in the sequence as the result of discrimination."
    }, {
      "heading" : "3.4 Fusion Learning Architecture Combined with a Simple Lexical Structure Rule",
      "text" : "Generative architecture can get a very high precision by adjusting the threshold value of similarity, but will pay a high price for low recall-value. Compared with generative method, discriminative architecture can obtain a higher recall-value with low guarantee for precision-value.\nThe feature of discriminative architecture indicates that if it determines a candidate hypernymhyponym relation word pair as negative, then the word pair will have high probability for negative. We can use discriminative architecture to help the generative one to get rid of some false positive instance. For this reason, we fuse the generative and\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nDiscriminative Architecture ( RNN )\nGenerative Architecture ( based on MLP )\ndistance ∧ ∨\n洲际 弹道 导弹 Intercontinental Ballistic Missile\nLexical Structure Rule\n=\n: hyponym\n: candidate hypernym\n: similar with true hypernym\n: Boolean value. 0 for unrelated; 1 for hypernym-hyponym relation.\nFigure 4: Fusion Learning Architecture.\ndiscriminative architectures together by applying Boolean operator “AND” to the results outputted by the two architectures. Excepting a much higher precision-value than the precious two models and almost the same recall-value as the generative one.\nBy combining discriminative and generative architectures, the fusion architecture can discovery hypernym-hyponym much more precisely but becomes only focusing deep level semantic and ignoring the lexical stricture information which is very useful for discovery of hypernym-hyponym relationship, especially for compound nouns (CNs), for instance, “洲际 弹道导弹(Intercontinental Ballistic Missile)”. The root word of CN often indicates a hypernym relation, like the word “导 弹(Missile)” is the hypernyms of the precious CN. Root word of a CN can be obtained via using syntax dependency parsing or semantic dependency parsing of CN. Due to the word formation rule of Chinese, the root word is usually the last word in CN segmentation result. To supplement the capacity of learning semantic hierarchy from lexical structure, we use the simple lexical structure rule to assist previous fusion model.\nThe final fusion learning architecture (showed in Figure 4) is composed of three parts, namely generative architecture, discriminative architecture and lexical structure rule module."
    }, {
      "heading" : "4 Experiments",
      "text" : "In the experimental stage, we implement our fusion architecture for learning semantic hierarchies. To the end of this, we first introduce the preparation of experimental setup. Next, we report the\nperformance of fusion architecture and its components. Subsequently, we compare the performance of our method to those of several previous methods in different aspects and give an example for construction of semantic hierarchies."
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : ""
    }, {
      "heading" : "Pre-trained Word Embeddings",
      "text" : "We use a Chinese encyclopedia corpus named Baidubaike1 to learn word embeddings, which contains about 30 million sentences (about 780 million words). The Chinese segmentation technology is provided by the open-source Chinese language processing platform LTP2 (Che et al., 2010). Then, we employ the Skip-gram method (Section 3.1) to train word embeddings for the further experiment. We obtain the embedding vectors of 0.56 million words in total."
    }, {
      "heading" : "Dataset and Evaluation Metrics",
      "text" : "The training data for learning semantic hierarchies is collected from CilinE3 which contains 100,093 Chinese words and organized as a hierarchy of five levels, in which the words are linked by hypernym–hyponym relations. Finally, we obtain 15,242 word pairs of hypernym–hyponym relation for positive instances and constructed 15,242 negative instances for training.\nFor comparability we use the same test dataset as Fu et al. (2014) in evaluation stage. They obtain the hypernyms for 418 entities, which are se-\n1Baidubaike (https://baike.baidu.com/) is one of the largest Chinese encyclopedias.\n2http://www.ltp-cloud.com/demo/ 3http://www.ltp-cloud.com/download/\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nWord Dimension\n# of neutrons in hidden layer(RNN)\n# of neutrons in hidden layer(MLP)\nBatch size Adadelta parameter\ndw = 300 nh = 800 nh = 500 b = 20 ρ = 0.95, ε = 1e −6\nTable 1: Parameters used in our experiments.\nlected randomly from Baidubaike, following their previous work (Fu et al., 2013). The final data set was manually labeled and measured the interannotator agreement by using the kappa coefficient (Siegel and Castellan, 1981). The kappa value is 0.96, which indicates a good strength of agreement. Training data and test data are showed in Table 2.\nRelation # of word pairsTraining Test hypernym-hyponym 15,242 1,079 hyponym-hypernym 7,621 1,079 unrelated 7,621 3,250 Total 30,484 5,408\nTable 2: The Experimental Data.\nWe use precision-value, recall-value, and F1score as metrics to evaluate the performances of the methods. Since the discovery of hypernymhyponym relation is a binary classification task, we only report the performance of the positive instances recognition in the experiments."
    }, {
      "heading" : "Parameter Settings and Training",
      "text" : "In our fusion architecture, there are MLP for generation (see in Section 3.2) and RNN for discrimination (see in Section 3.3) need to be trained. We experimentally study the effects of several hyperparameters on this two neural networks: the number of neutrons in hidden layer, the selection of activation function. Table 1 shows all parameters used in the experiments. We use Adadelta (Zeiler, 2012) in the update procedure, which relies on two main parameters, ρ and ε, which do not significantly affect the performance. Following Zeiler (2012), we choose 0.95 and 1e−6, respectively, as the values of these two parameters.\nIn the training stage, we train the discriminative architecture and generative architecture respectively. For training discriminative architecture based on RNN, we use the whole training data. But for training generative architecture based on MLP, we only use the positive instances in train-\ning data as Fu (2014) for generating positive hypernym vectors.\n0.00%\n10.00%\n20.00%\n30.00%\n40.00%\n50.00%\n60.00%\n70.00%\n0.7 0.8 0.9 1 1.1 1.2 1.3 1.4 1.5 Distance Threshold\n63.20%\n63.60%\n64.00%\n64.40%\n64.80%\n65.20%\n65.60%\n0.5 0.6 0.7 0.8 0.9 1\nThreshold for Probability\n(a) generative architecture (b) discriminative architecture\nFigure 5: The effects on F1-score of threshold for generative and discriminative architecture.\n70.00%\n70.50%\n71.00%\n71.50%\n72.00%\n72.50%\n73.00%\n73.50%\n74.00%\n74.50%\n0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1\nF1\nThreshold for Probability\nFigure 6: The effects on F1-score of threshold for fusion architecture."
    }, {
      "heading" : "4.2 Performance of Fusion Architecture",
      "text" : "In the test stage, we tune the performance of architecture by changing the thresholds for components of fusion architecture. There two kinds of thresholds need to be tuned: the similar distance of generative architecture and the probability for positive instances of discriminative architecture.\nWe first experimentally study the effects of semantic distance threshold on generative architecture based on MLP neural network. Selecting the semantic distance threshold of best F1-score for generative architecture and fixing it, we tun the probability threshold for positive instances of discriminative architecture, observing the F1-score\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\ncurve of discriminative architecture and the fusion learning architecture (see in Figure 5, Figure 6)."
    }, {
      "heading" : "4.3 Comparison with Previous Work",
      "text" : "In this section, we compare the proposed method with previous methods, including pairwise hypernym-hyponym relation extraction based on patterns, word distributions, web mining, and based on word embeddings(see in Section 2). Results are shown in Table 3.\nMethod P(%) R(%) F1(%) MPttern 97.47 21.41 35.11 MSnow 60.88 25.67 36.11 MbalApinc 54.96 53.38 54.16 MinvCL 49.63 62.84 55.46 MWeb 87.40 48.19 62.13 MEmb 80.54 67.99 73.74 MlexicalRule 100.0 16.88 28.88 MMLPgen. 77.96 53.51 63.46 MRNNdis. 55.97 77.40 64.96 MMLP+RNN 90.00 51.48 65.50 MFusion 91.60 62.36 74.20\nTable 3: Comparison of the proposed method with existing methods in the test set."
    }, {
      "heading" : "Overall Comparison",
      "text" : "MPattern refers to the pattern-based method (Hearst, 1992). The method uses the Chinese Hearst-style patterns (Fu et al., 2013). The result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in the fixed patterns, and most of them are expressed in highly flexible manners. MSnow originally proposed by Snow et al. (2004). This method relies on an accurate syntactic parser, and the quality of the automatically extracted patterns is difficult to guarantee.\nThere are two previous distributional methods MbalApinc (Kotlerman et al., 2010) and MinvCL (Lenci and Benotto, 2012). Each word is represented as a feature vector in which each dimension is the point-wise mutual information (PMI) value (Geffet and Dagan, 2005; ZhitomirskyGeffet and Dagan, 2009) of the word and its context words (see in Section 2). MFu refers to a web mining method proposed by Fu et al. (2013) which mines hypernyms of a given word w from multiple sources returning a\nranked list of the hypernyms. MEmb refers to a novel method based on word embeddings achieving the best F1-value among previous methods. MlexicalRule refers to using lexical structure rule to discover hypernym-hyponym relations (see in Section 3.4). The 100% precision-value on test data indicates the lexical rule is correct for most compound nouns (CNs). However the rule only takes effect for CNs which are minority. MMLPgen. represents the generative architecture based on MLP neural network (see in Section 3.2), the method get a higher F1-score than most of previous semantic hierarchy discovery method except MEmb. MRNNdis. represents the discriminative architecture based on RNN neural network (see in Section 3.3) which obtains the highest recall-value in comparison of the proposed methods. MMLP+RNN combines these two architectures based on MLP and RNN (see in Section 3.4), getting a much higher precision-value than any components and a comparable recall-value with MMLPgen. . MFusion refers to a fusion learning architecture composed of discriminative and generative architectures and assisted with lexical structure rule (see in Section 3.4). Assisted with the simple lexical structure rule, the fusion learning architecture get a better F1-score than all of the previous methods do and significantly improve the precisionvalue over the state-of-the-art method MEmb.\nFurther, we experimentally study the effects of lexical rule for MEmb. The results show that the method MEmb+lexicalRule does not improve the F1-score compared with the original MEmb. The reason maybe that MEmb already recall the compound nouns (CNs) hypernym-hyponym relations (see in Table 4). The results also indicate that the improvement is mainly caused by discriminative generative architecture.\nP(%) R(%) F1(%) MEmb+lexicalRule 80.54 67.99 73.74 MFusion 91.60 62.36 74.20\nTable 4: Comparison of the MEmb assisted with lexical structure rule."
    }, {
      "heading" : "Comparison on the Out-of-CilinE Data",
      "text" : "Since the training data is extracted from CilinE, we are greatly interested in the performance of our method on the hypernym-hyponym relations out-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nside of CilinE. We assume that as long as there is one word in the pair not existing in CilinE, the word pair is outside of CilinE. In our test data, about 61% word pairs are outside of CilinE.\nP(%) R(%) F1(%) MWiki+CilinE 80.39 19.29 31.12 MEmb 65.85 44.47 53.09 MFusion 79.92 44.94 57.53\nTable 5: Comparison on the Out-of-CilinE Data.\nTable 5 shows the performances of the baseline method MWiki+CilinE , previous state-of-theart methodMEmb and our methodMFusion on the out-of-CilinE data. In comparison, MWiki+CilinE has the highest precision-value but has a lowest recall-value, MEmb significantly improve recallvalue and F1-score. By contrast, our method MFusion can discover a little bit more hypernym–hyponym relations than MEmb with achieving a more than 14% precision-value improvement. And our method can get an F1-score of 57.53%, which is a new state-of-the-art result on the Outof-CilinE Data."
    }, {
      "heading" : "Combined with Manually-Built Hierarchies",
      "text" : "For further exploration, we combine our method MFusion with the existing manually-built hierarchies in Wikipedia and CilinE. The combination strategy is to simply merge all positive results from the two methods together, and then to infer new relations based on the transitivity of hypernym–hyponym relations. The same manner allied to precious method MEmb to be compared. The comparison is showed in Table 6. Combining our fusion method MFusion with manually-built hierarchies Wikipedia and CilinE can further improve F1-score to 82.01%, gets an about 1.7% improvement compared with the same manners on MEmb.\nP(%) R(%) F1(%) MEmb 80.54 67.99 73.74 MFusion 91.60 62.36 74.20 MEmd+CilinE 80.59 72.42 76.29 MFusion+CilinE 91.64 70.76 79.85 MEmd+Wiki+CilinE 79.78 80.81 80.29 MFusion+Wiki+CilinE 91.00 74.63 82.01\nTable 6: Comparison ofMFusion andMEmb combined with manually-built hierarchies."
    }, {
      "heading" : "4.4 Example of Learning Semantic Hierarchies",
      "text" : "In Figure 7, there is an example of learning semantic hierarchies based on our fusion architecture (MFusion) and combined method using manuallybuilt hierarchies (MFusion+Wiki+CilinE). From the results, we can see that our method can actually learn the semantic hierarchies for a given word and its hypernyms list relatively precisely. NBA球员 NBA player 篮球运动员 basketball player 球员 ball player 运动员 athlete 体育人物 sportsman 人物 person\n(b) \uD835\uDC40\uD835\uDC39\uD835\uDC62\uD835\uDC60\uD835\uDC56\uD835\uDC5C\uD835\uDC5B+\uD835\uDC4A\uD835\uDC3C\uD835\uDC58\uD835\uDC56+\uD835\uDC36\uD835\uDC56\uD835\uDC59\uD835\uDC56\uD835\uDC5B\uD835\uDC38\nNBA球员 NBA player\n篮球运动员 basketball play r\n球员 ball player\n运动员 athlete\n体育人物 sportsman\n人物 person\n(a) \uD835\uDC40\uD835\uDC39\uD835\uDC62\uD835\uDC60\uD835\uDC56\uD835\uDC5C\uD835\uDC5B\n乌头 aconite\n药品 medicine\n植物 plant\n毛茛科 Ranunculaceae\n植物药 medicine plant 乌头属\nAconitum\n生物 organism\n(b) \uD835\uDC40\uD835\uDC39\uD835\uDC62\uD835\uDC60\uD835\uDC56\uD835\uDC5C\uD835\uDC5B+\uD835\uDC4A\uD835\uDC3C\uD835\uDC58\uD835\uDC56+\uD835\uDC36\uD835\uDC56\uD835\uDC59\uD835\uDC56\uD835\uDC5B\uD835\uDC38\n乌头 aconite\n药品 medicine\n植物 plant\n毛茛科 Ranunculaceae\n植物药 medicine plant 乌头属\nAconitum\n生物 organism\n(a) \uD835\uDC40\uD835\uDC39\uD835\uDC62\uD835\uDC60\uD835\uDC56\uD835\uDC5C\uD835\uDC5B\nHyponym List of Hypernyms\nNBA球员 (NBA player) 球员 (ball player) 篮球运动员 (basketball player) 运动员 (athlete) 体育人物 (sportsman) 人物 (person)\nHyponym List of Hypernyms\n乌头 (conite) 植物药 (medicine plant) 乌头属 (Aconitum) 药品 (medicine) 毛茛科 (Ranunculaceae) 植物 (plant) 生物 (organism)\nFigure 7: Example of Learning Semantic Hierarchies.\nThe dashed line frames in Figure 7(a) refers to the losing hypernym-hyponym relations words. For instance, our method fail to learn the semantic hierarchies between “乌头属(aconite)” and “毛茛科(Ranunculaceae)”. The reason maybe that their semantic similarity effects representations close to each other in the embedding space and our method can not find suitable projection for these pairs. By combining our method with manually-built hierarchies, we can improve the capacity of learning semantic hierarchies. In this case, the combined method can build the semantic hierarchies correctly (see in Figure 7(b))."
    }, {
      "heading" : "5 Conclusion",
      "text" : "This paper proposes a novel method for learning semantic hierarchies based on discriminative generative fusion architecture combined with a very simple lexical structure rule. The fusion architecture method can be easily expanded to be suitable for other languages. In experiments, the proposed method achieves the best F1-score of 74.20% on a manually labeled test dataset outperforming stateof-the-art methods with a much higher precisionvalue of 91.60% for application. Further experiments show that our method is complementary with some manually-built hierarchies to learn semantic hierarchy construction more precisely.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin." ],
      "venue" : "Journal of machine learning research 3(Feb):1137–1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Ltp: A chinese language technology platform",
      "author" : [ "Wanxiang Che", "Zhenghua Li", "Ting Liu." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics: Demonstrations. Association for Computational Linguistics, pages 13–16.",
      "citeRegEx" : "Che et al\\.,? 2010",
      "shortCiteRegEx" : "Che et al\\.",
      "year" : 2010
    }, {
      "title" : "Approximation by superpositions of a sigmoidal function",
      "author" : [ "George Cybenko." ],
      "venue" : "Mathematics of Control, Signals, and Systems (MCSS) 2(4):303–314.",
      "citeRegEx" : "Cybenko.,? 1989",
      "shortCiteRegEx" : "Cybenko.",
      "year" : 1989
    }, {
      "title" : "Multi-view learning of word embeddings via cca",
      "author" : [ "Paramveer Dhillon", "Dean P Foster", "Lyle H Ungar." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 199–207.",
      "citeRegEx" : "Dhillon et al\\.,? 2011",
      "shortCiteRegEx" : "Dhillon et al\\.",
      "year" : 2011
    }, {
      "title" : "Finding structure in time",
      "author" : [ "Jeffrey L Elman." ],
      "venue" : "Cognitive science 14(2):179–211.",
      "citeRegEx" : "Elman.,? 1990",
      "shortCiteRegEx" : "Elman.",
      "year" : 1990
    }, {
      "title" : "Learning semantic hierarchies via word embeddings",
      "author" : [ "Ruiji Fu", "Jiang Guo", "Bing Qin", "Wanxiang Che", "Haifeng Wang", "Ting Liu." ],
      "venue" : "ACL (1). pages 1199–1209.",
      "citeRegEx" : "Fu et al\\.,? 2014",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2014
    }, {
      "title" : "Exploiting multiple sources for open-domain hypernym discovery",
      "author" : [ "Ruiji Fu", "Bing Qin", "Ting Liu." ],
      "venue" : "EMNLP. pages 1224–1234.",
      "citeRegEx" : "Fu et al\\.,? 2013",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2013
    }, {
      "title" : "The distributional inclusion hypotheses and lexical entailment",
      "author" : [ "Maayan Geffet", "Ido Dagan." ],
      "venue" : "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 107–114.",
      "citeRegEx" : "Geffet and Dagan.,? 2005",
      "shortCiteRegEx" : "Geffet and Dagan.",
      "year" : 2005
    }, {
      "title" : "Automatic acquisition of hyponyms from large text corpora",
      "author" : [ "Marti A Hearst." ],
      "venue" : "Proceedings of the 14th conference on Computational linguisticsVolume 2. Association for Computational Linguistics, pages 539–545.",
      "citeRegEx" : "Hearst.,? 1992",
      "shortCiteRegEx" : "Hearst.",
      "year" : 1992
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Serial order: A parallel distributed processing approach",
      "author" : [ "Michael I Jordan." ],
      "venue" : "Advances in psychology 121:471–495.",
      "citeRegEx" : "Jordan.,? 1997",
      "shortCiteRegEx" : "Jordan.",
      "year" : 1997
    }, {
      "title" : "Directional distributional similarity for lexical inference",
      "author" : [ "Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet." ],
      "venue" : "Natural Language Engineering 16(04):359–389.",
      "citeRegEx" : "Kotlerman et al\\.,? 2010",
      "shortCiteRegEx" : "Kotlerman et al\\.",
      "year" : 2010
    }, {
      "title" : "Identifying hypernyms in distributional semantic spaces",
      "author" : [ "Alessandro Lenci", "Giulia Benotto." ],
      "venue" : "Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and",
      "citeRegEx" : "Lenci and Benotto.,? 2012",
      "shortCiteRegEx" : "Lenci and Benotto.",
      "year" : 2012
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781 .",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur." ],
      "venue" : "Interspeech. volume 2, page 3.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig." ],
      "venue" : "HLT-NAACL. volume 13, pages 746–751.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A Miller." ],
      "venue" : "Communications of the ACM 38(11):39–",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "A scalable hierarchical distributed language model",
      "author" : [ "Andriy Mnih", "Geoffrey E Hinton." ],
      "venue" : "Advances in neural information processing systems. pages 1081–1088.",
      "citeRegEx" : "Mnih and Hinton.,? 2009",
      "shortCiteRegEx" : "Mnih and Hinton.",
      "year" : 2009
    }, {
      "title" : "Principles of neurodynamics",
      "author" : [ "Frank Rosenblatt." ],
      "venue" : "perceptrons and the theory of brain mechanisms. Technical report, DTIC Document.",
      "citeRegEx" : "Rosenblatt.,? 1961",
      "shortCiteRegEx" : "Rosenblatt.",
      "year" : 1961
    }, {
      "title" : "Improving hypernymy detection with an integrated path-based and distributional method",
      "author" : [ "Vered Shwartz", "Yoav Goldberg", "Ido Dagan." ],
      "venue" : "arXiv preprint arXiv:1603.06076 .",
      "citeRegEx" : "Shwartz et al\\.,? 2016",
      "shortCiteRegEx" : "Shwartz et al\\.",
      "year" : 2016
    }, {
      "title" : "J.(1988). nonparametric statistics for the behavioral sciences",
      "author" : [ "Sidney Siegel", "N John Castellan" ],
      "venue" : null,
      "citeRegEx" : "Siegel and Castellan.,? \\Q1981\\E",
      "shortCiteRegEx" : "Siegel and Castellan.",
      "year" : 1981
    }, {
      "title" : "Learning syntactic patterns for automatic hypernym discovery",
      "author" : [ "Rion Snow", "Daniel Jurafsky", "Andrew Y Ng." ],
      "venue" : "Advances in Neural Information Processing Systems 17 .",
      "citeRegEx" : "Snow et al\\.,? 2004",
      "shortCiteRegEx" : "Snow et al\\.",
      "year" : 2004
    }, {
      "title" : "Yago: a core of semantic knowledge",
      "author" : [ "Fabian M Suchanek", "Gjergji Kasneci", "Gerhard Weikum." ],
      "venue" : "Proceedings of the 16th international conference on World Wide Web. ACM, pages 697–706.",
      "citeRegEx" : "Suchanek et al\\.,? 2007",
      "shortCiteRegEx" : "Suchanek et al\\.",
      "year" : 2007
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "Matthew D Zeiler." ],
      "venue" : "arXiv preprint arXiv:1212.5701 .",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    }, {
      "title" : "Bootstrapping distributional feature vector quality",
      "author" : [ "Maayan Zhitomirsky-Geffet", "Ido Dagan." ],
      "venue" : "Computational linguistics 35(3):435–461.",
      "citeRegEx" : "Zhitomirsky.Geffet and Dagan.,? 2009",
      "shortCiteRegEx" : "Zhitomirsky.Geffet and Dagan.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Ontologies and semantic thesauri (Miller, 1995; Suchanek et al., 2007) are significant for many natural language processing applications.",
      "startOffset" : 33,
      "endOffset" : 70
    }, {
      "referenceID" : 22,
      "context" : "Ontologies and semantic thesauri (Miller, 1995; Suchanek et al., 2007) are significant for many natural language processing applications.",
      "startOffset" : 33,
      "endOffset" : 70
    }, {
      "referenceID" : 16,
      "context" : "However, such manual semantic hierarchies construction as WordNet (Miller, 1995) and YAGO (Suchanek et al.",
      "startOffset" : 66,
      "endOffset" : 80
    }, {
      "referenceID" : 22,
      "context" : "However, such manual semantic hierarchies construction as WordNet (Miller, 1995) and YAGO (Suchanek et al., 2007), the primary problem is the tradeoff between coverage scope and human labor.",
      "startOffset" : 90,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "Several works focus on designing or learning lexical patterns (Hearst, 1992; Snow et al., 2004) via observing context of hypernym-hyponym relation, which suffer from covering a small proportion of complex linguistic circumstances.",
      "startOffset" : 62,
      "endOffset" : 95
    }, {
      "referenceID" : 21,
      "context" : "Several works focus on designing or learning lexical patterns (Hearst, 1992; Snow et al., 2004) via observing context of hypernym-hyponym relation, which suffer from covering a small proportion of complex linguistic circumstances.",
      "startOffset" : 62,
      "endOffset" : 95
    }, {
      "referenceID" : 11,
      "context" : "In other words, hypernyms are semantically broader terms than their hyponyms (Kotlerman et al., 2010; Lenci and Benotto, 2012).",
      "startOffset" : 77,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "In other words, hypernyms are semantically broader terms than their hyponyms (Kotlerman et al., 2010; Lenci and Benotto, 2012).",
      "startOffset" : 77,
      "endOffset" : 126
    }, {
      "referenceID" : 8,
      "context" : "Several works focus on designing or learning lexical patterns (Hearst, 1992; Snow et al., 2004) via observing context of hypernym-hyponym relation, which suffer from covering a small proportion of complex linguistic circumstances. Besides, distributional inclusion hypothesis, which states that hypernyms tend to occur in a superset of contexts in which their hyponyms are found. In other words, hypernyms are semantically broader terms than their hyponyms (Kotlerman et al., 2010; Lenci and Benotto, 2012). However, it is not always rational. To acquire more contexts of words, Fu (2013) applies a web mining method to discover the hypernyms of Chinese entities from multiple sources, assuming that the hypernyms of an entity co-occur with it frequently.",
      "startOffset" : 63,
      "endOffset" : 589
    }, {
      "referenceID" : 15,
      "context" : "Since word embeddings can capture a considerable amount of syntactic/semantic relations (Mikolov et al., 2013b), we considered constructing a uniform architecture for semantic hierarchies learning based on nonlinear transformation of word embeddings.",
      "startOffset" : 88,
      "endOffset" : 111
    }, {
      "referenceID" : 16,
      "context" : "During the early phase of semantic hierarchies study, some focused on building manually-built semantic resources, WordNet (Miller, 1995) is a representative thesauri among them.",
      "startOffset" : 122,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : "The assumption is a variation of the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009).",
      "startOffset" : 73,
      "endOffset" : 133
    }, {
      "referenceID" : 24,
      "context" : "The assumption is a variation of the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009).",
      "startOffset" : 73,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "However the broader semantics hypothesis may not always infer broader contexts (Fu et al., 2014).",
      "startOffset" : 79,
      "endOffset" : 96
    }, {
      "referenceID" : 10,
      "context" : "During the early phase of semantic hierarchies study, some focused on building manually-built semantic resources, WordNet (Miller, 1995) is a representative thesauri among them. Such manuallybuilt hierarchies have exact structure and high accuracy, but their coverage is limited, especially for fine-grained concepts and entities. Some researchers presented automatic approaches for supplementing manually-built semantic resources. Suchanek et al. (2007) linked the categories in Wikipedia onto WordNet in construction of YAGO.",
      "startOffset" : 123,
      "endOffset" : 455
    }, {
      "referenceID" : 5,
      "context" : "Hearst (1992) pointed out that certain lexical constructions linking two different noun phases (NPs) often imply hypernym-hyponym relation.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 5,
      "context" : "Hearst (1992) pointed out that certain lexical constructions linking two different noun phases (NPs) often imply hypernym-hyponym relation. A representative example is “such NP1 as NP2”. Considering time-consuming of manuallybuilt lexical patterns, Snow et al. (2004) proposed a automatic method extracting large numbers of lexico-syntactic patterns to detect hypernym relations from a large newswire corpus.",
      "startOffset" : 0,
      "endOffset" : 268
    }, {
      "referenceID" : 5,
      "context" : "The assumption is a variation of the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009). The pioneer work by Kotlerman et al. (2010) designed a directional distributional measure to infer hypernym–hyponym relations.",
      "startOffset" : 74,
      "endOffset" : 179
    }, {
      "referenceID" : 5,
      "context" : "The assumption is a variation of the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009). The pioneer work by Kotlerman et al. (2010) designed a directional distributional measure to infer hypernym–hyponym relations. Differently from Kotlerman et al. (2010), Lenci and Benotto (2012) focus on applying directional, asymmetric similarity measures to identify hypernyms.",
      "startOffset" : 74,
      "endOffset" : 303
    }, {
      "referenceID" : 5,
      "context" : "The assumption is a variation of the Distributional Inclusion Hypothesis (Geffet and Dagan, 2005; Zhitomirsky-Geffet and Dagan, 2009). The pioneer work by Kotlerman et al. (2010) designed a directional distributional measure to infer hypernym–hyponym relations. Differently from Kotlerman et al. (2010), Lenci and Benotto (2012) focus on applying directional, asymmetric similarity measures to identify hypernyms.",
      "startOffset" : 74,
      "endOffset" : 329
    }, {
      "referenceID" : 15,
      "context" : "Inspired by the fact (Mikolov et al., 2013b) that word embeddings can capture a considerable amount of syntactic/semantic relations(e.",
      "startOffset" : 21,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "Inspired by the fact (Mikolov et al., 2013b) that word embeddings can capture a considerable amount of syntactic/semantic relations(e.g. v(king) v(queen) ≈ v(man) - v(woman), where v(w) is the word embedding of the word w), Fu (2014) present an approach to learn semantic hierarchies with clustered hypernym-hyponym relation word embedding pairs.",
      "startOffset" : 22,
      "endOffset" : 234
    }, {
      "referenceID" : 13,
      "context" : "Inspired by the fact (Mikolov et al., 2013b) that word embeddings can capture a considerable amount of syntactic/semantic relations(e.g. v(king) v(queen) ≈ v(man) - v(woman), where v(w) is the word embedding of the word w), Fu (2014) present an approach to learn semantic hierarchies with clustered hypernym-hyponym relation word embedding pairs. However the method just focuses on linear transformation of word embeddings, using shallow level semantic of the representation. Besides, the method need clustering for hypernym-hyponym relation word pairs in advanced and the precision-value on test data is not good enough for practical application. Shwartz et al. (2016) included additional linguistic information for LSTM-based learning, but the method has co-occurrence requirements for hyponym-hypernym pairs in corpus.",
      "startOffset" : 22,
      "endOffset" : 670
    }, {
      "referenceID" : 13,
      "context" : "Inspired by the fact (Mikolov et al., 2013b) that word embeddings can capture a considerable amount of syntactic/semantic relations(e.g. v(king) v(queen) ≈ v(man) - v(woman), where v(w) is the word embedding of the word w), Fu (2014) present an approach to learn semantic hierarchies with clustered hypernym-hyponym relation word embedding pairs. However the method just focuses on linear transformation of word embeddings, using shallow level semantic of the representation. Besides, the method need clustering for hypernym-hyponym relation word pairs in advanced and the precision-value on test data is not good enough for practical application. Shwartz et al. (2016) included additional linguistic information for LSTM-based learning, but the method has co-occurrence requirements for hyponym-hypernym pairs in corpus. Enlightened from good properties of word embeddings for capturing semantic relationship between words in work of Fu (2014), we further explore capacity of word embedding for semantic hierarchies using neural networks based on a fusion learning architecture.",
      "startOffset" : 22,
      "endOffset" : 945
    }, {
      "referenceID" : 5,
      "context" : "Given the hypernyms list of a word, our goal is building a semantic hierarchies construction of these hypernyms and the given word(Fu et al., 2014), the process is presented in Figure 2.",
      "startOffset" : 130,
      "endOffset" : 147
    }, {
      "referenceID" : 18,
      "context" : "We use Multilayer Perceptron (MLP) (Rosenblatt, 1961) to achieve a generative model and Recurrent Neural Network (RNN) (Mikolov et al.",
      "startOffset" : 35,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "We use Multilayer Perceptron (MLP) (Rosenblatt, 1961) to achieve a generative model and Recurrent Neural Network (RNN) (Mikolov et al., 2010) to implement a discriminative model.",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : "1 Word Embedding Training Various methods for learning word embeddings have been proposed in the recent years, such as neural net language models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2013b) and spectral models (Dhillon et al.",
      "startOffset" : 146,
      "endOffset" : 213
    }, {
      "referenceID" : 17,
      "context" : "1 Word Embedding Training Various methods for learning word embeddings have been proposed in the recent years, such as neural net language models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2013b) and spectral models (Dhillon et al.",
      "startOffset" : 146,
      "endOffset" : 213
    }, {
      "referenceID" : 15,
      "context" : "1 Word Embedding Training Various methods for learning word embeddings have been proposed in the recent years, such as neural net language models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2013b) and spectral models (Dhillon et al.",
      "startOffset" : 146,
      "endOffset" : 213
    }, {
      "referenceID" : 3,
      "context" : ", 2013b) and spectral models (Dhillon et al., 2011).",
      "startOffset" : 29,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "1 Word Embedding Training Various methods for learning word embeddings have been proposed in the recent years, such as neural net language models (Bengio et al., 2003; Mnih and Hinton, 2009; Mikolov et al., 2013b) and spectral models (Dhillon et al., 2011). More recently, Mikolov et al. (2013a) propose two loglinear models, namely the Skip-gram and CBOW model, for inducing word embeddings efficiently on a large-scale corpus because of their low time complexity.",
      "startOffset" : 147,
      "endOffset" : 296
    }, {
      "referenceID" : 18,
      "context" : "2 Generative Architecture Based on MLP Multilayer Perceptron (MLP) (Rosenblatt, 1961) is a feedforward artificial neural network which maps inputs onto a set of appropriate outputs.",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "can distinguish data that are not linearly separable (Cybenko, 1989).",
      "startOffset" : 53,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : "3 Discriminative Architecture Based on RNN Recurrent Neural Network (RNN) (Mikolov et al., 2010) is a kind of artificial neural network in which connections between neuron form a directed cycle, creating an internal state of the network which allows it to exhibit dynamic temporal behavior according to the history information.",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 9,
      "context" : "There are some variants of original RNN, the most representative one of them is Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) which is capable of learning long-term dependencies.",
      "startOffset" : 110,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "There are two major classes of SRN, know as Elman networks (Elman, 1990) and Jordan networks (Jordan, 1997).",
      "startOffset" : 59,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : "There are two major classes of SRN, know as Elman networks (Elman, 1990) and Jordan networks (Jordan, 1997).",
      "startOffset" : 93,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "The Chinese segmentation technology is provided by the open-source Chinese language processing platform LTP2 (Che et al., 2010).",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "For comparability we use the same test dataset as Fu et al. (2014) in evaluation stage.",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "lected randomly from Baidubaike, following their previous work (Fu et al., 2013).",
      "startOffset" : 63,
      "endOffset" : 80
    }, {
      "referenceID" : 20,
      "context" : "The final data set was manually labeled and measured the interannotator agreement by using the kappa coefficient (Siegel and Castellan, 1981).",
      "startOffset" : 113,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "We use Adadelta (Zeiler, 2012) in the update procedure, which relies on two main parameters, ρ and ε, which do not significantly affect the performance.",
      "startOffset" : 16,
      "endOffset" : 30
    }, {
      "referenceID" : 23,
      "context" : "We use Adadelta (Zeiler, 2012) in the update procedure, which relies on two main parameters, ρ and ε, which do not significantly affect the performance. Following Zeiler (2012), we choose 0.",
      "startOffset" : 17,
      "endOffset" : 177
    }, {
      "referenceID" : 23,
      "context" : "We use Adadelta (Zeiler, 2012) in the update procedure, which relies on two main parameters, ρ and ε, which do not significantly affect the performance. Following Zeiler (2012), we choose 0.95 and 1e−6, respectively, as the values of these two parameters. In the training stage, we train the discriminative architecture and generative architecture respectively. For training discriminative architecture based on RNN, we use the whole training data. But for training generative architecture based on MLP, we only use the positive instances in training data as Fu (2014) for generating positive hypernym vectors.",
      "startOffset" : 17,
      "endOffset" : 569
    }, {
      "referenceID" : 8,
      "context" : "Overall Comparison MPattern refers to the pattern-based method (Hearst, 1992).",
      "startOffset" : 63,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "The method uses the Chinese Hearst-style patterns (Fu et al., 2013).",
      "startOffset" : 50,
      "endOffset" : 67
    }, {
      "referenceID" : 11,
      "context" : "There are two previous distributional methods MbalApinc (Kotlerman et al., 2010) and MinvCL (Lenci and Benotto, 2012).",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 12,
      "context" : ", 2010) and MinvCL (Lenci and Benotto, 2012).",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 7,
      "context" : "Each word is represented as a feature vector in which each dimension is the point-wise mutual information (PMI) value (Geffet and Dagan, 2005; ZhitomirskyGeffet and Dagan, 2009) of the word and its context words (see in Section 2).",
      "startOffset" : 118,
      "endOffset" : 177
    }, {
      "referenceID" : 5,
      "context" : "The method uses the Chinese Hearst-style patterns (Fu et al., 2013). The result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in the fixed patterns, and most of them are expressed in highly flexible manners. MSnow originally proposed by Snow et al. (2004). This method relies on an accurate syntactic parser, and the quality of the automatically extracted patterns is difficult to guarantee.",
      "startOffset" : 51,
      "endOffset" : 349
    }, {
      "referenceID" : 5,
      "context" : "The method uses the Chinese Hearst-style patterns (Fu et al., 2013). The result shows that only a small part of the hypernyms can be extracted based on these patterns because only a few hypernym relations are expressed in the fixed patterns, and most of them are expressed in highly flexible manners. MSnow originally proposed by Snow et al. (2004). This method relies on an accurate syntactic parser, and the quality of the automatically extracted patterns is difficult to guarantee. There are two previous distributional methods MbalApinc (Kotlerman et al., 2010) and MinvCL (Lenci and Benotto, 2012). Each word is represented as a feature vector in which each dimension is the point-wise mutual information (PMI) value (Geffet and Dagan, 2005; ZhitomirskyGeffet and Dagan, 2009) of the word and its context words (see in Section 2). MFu refers to a web mining method proposed by Fu et al. (2013) which mines hypernyms of a given word w from multiple sources returning a ranked list of the hypernyms.",
      "startOffset" : 51,
      "endOffset" : 899
    } ],
    "year" : 2017,
    "abstractText" : "Semantic hierarchies construction means to build structure of concepts linked by hypernym-hyponym (“is-a”) relations. A major challenge for this task is the automatic discovery of hypernym-hyponym (“is-a”) relations. We propose a fusion learning architecture based on word embeddings for constructing semantic hierarchies, composed of discriminative generative fusion architecture and a very simple lexical structure rule for assisting, getting an F1-score of 74.20% with 91.60% precision-value, outperforming the stateof-the-art methods on a manually labeled test dataset. Subsequently, combining our method with manually-built hierarchies can further improve F1-score to 82.01%. Besides, the fusion learning architecture is language-independent.",
    "creator" : "LaTeX with hyperref package"
  }
}