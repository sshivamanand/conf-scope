{
  "name" : "355.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Neural Modeling of Multi-Predicate Interactions for Japanese Predicate Argument Structure Analysis",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Predicate argument structure (PAS) analysis is a basic semantic analysis task, in which systems are required to identify semantic units of a sentence, such as who did what to whom. In pro-drop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues of PAS analysis (Iida and Poesio, 2011; Sasano and Kurohashi, 2011).\nAs an approach to the argument omission problem, in Japanese PAS analysis, joint modeling of interactions between multiple predicates has been gaining popularity and achieved the state-of-theart result (Ouchi et al., 2015; Shibata et al., 2016). This approach is based on the linguistic intuition that the predicates in a sentence are semantically related to each other and the interaction information can be a clue for PAS analysis. However, to model such multi-predicate interactions, this ap-\nproach heavily relies on syntactic information predicted by parsers and suffers from the error propagation caused by the pipeline processing. To remedy this problem, we propose a neural model which automatically induces features sensitive to multi-predicate interactions from word sequence information of a sentence. This model takes as input all predicates and their argument candidates in a sentence at a time, and captures the interactions using grid-type recurrent neural networks (Grid-RNN) without syntactic information. In this paper, we firstly introduce a basic model using RNNs, which independently estimates arguments of each predicate without considering the multi-predicate interactions (Sec. 3). Then, extending this model, we propose a neural model using Grid-RNNs (Sec. 4). Performing experiments on the NAIST Text Corpus (Iida et al., 2007), we demonstrate that our neural models exceed the accuracy of the stateof-the-art Japanese PAS analyzer (Ouchi et al., 2015). In particular, the neural model using GridRNNs achieves the best result, which suggests that our grid-type neural architecture effectively captures multi-predicate interactions. 1\n1Our source code is publicly available at http:xxx\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nFigure 2: An overview of the neural models: (i) single-sequence and (ii) multi-sequence models."
    }, {
      "heading" : "2 Japanese Predicate Argument Structure Analysis",
      "text" : ""
    }, {
      "heading" : "2.1 Task Description",
      "text" : "In Japanese PAS analysis, we identify arguments taking part in the three major case roles, nominative (NOM), accusative (ACC) and dative (DAT) cases, for each predicate. Arguments can be divided into the following three categories according to the positions relative to their predicates (Hayashibe et al., 2011; Ouchi et al., 2015):\nDep: The arguments that have direct syntactic dependency with the predicate.\nZero: The arguments referred to by zero pronouns within the same sentence, which have no direct syntactic dependency with the predicate.\nInter-Zero: The arguments referred to by zero pronouns out of the same sentence.\nFor example, in Figure 1, the nominative argument “警察 (police)” for the predicate “逮捕した (arrested)” is regarded as a Dep argument since the argument has a direct syntactic dependency with the predicate. In contrast, the nominative argument “男 i (mani)” for the predicate “逃走した (escaped)” is regarded as a Zero argument since the argument has no direct syntactic dependency with the predicate.\nIn this paper, we focus on the analysis for these intra-sentential arguments, i.e., Dep and Zero. In order to identify inter-sentential arguments (Inter-Zero), it is required to search a much broader space, such as the whole document, resulting in a much harder analysis than intrasentential arguments.2 Thus, Ouchi et al. (2015)\n2The F-measure remains 10-20% (Taira et al., 2008; Imamura et al., 2009; Sasano and Kurohashi, 2011).\nand Shibata et al. (2016) focused on only intrasentential argument analysis. Following this trend, we focus on intra-sentential argument analysis."
    }, {
      "heading" : "2.2 Challenging Problem",
      "text" : "Arguments are often omitted in Japanese sentences. In Figure 1, ϕi represents the omitted argument, called zero pronoun. This zero pronoun ϕi refers to “男 i (mani)”. In Japanese PAS analysis, when an argument of the target predicate is omitted, we have to identify the antecedent of the omitted argument (Zero argument). The analysis for such Zero arguments is much more difficult than that forDep arguments because of the lack of direct syntactic dependencies. For Dep arguments, the syntactic dependency between an argument and its predicate is a strong clue. In the sentence in Figure 1, for the predicate “逮捕し た (arrested)”, the nominative argument is “警察 (police)”. This argument can easily be identified by relying on the syntactic dependency. In contrast, since the nominative argument “男 i (mani)” has no syntactic dependency with its predicate “逃 走した (escaped)”, we have to use other information for such zero argument identification. As an solution to this problem, we exploit two kinds of information: (i) context in the entire sentence and (ii) multi-predicate interactions. For the former, we introduce single-sequence model, which induces context-sensitive representations from a sequence of argument candidates of a predicate. For the latter, we introduce multi-sequence model, which induces predicate-sensitive representations from multiple sequences of argument candidates of all predicates in a sentence (shown in Figure 2).\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nFigure 3: The overall architecture of the single sequence model. This model consists of three components: (i) Input Layer, (ii) RNN Layer and (iii) Output Layer."
    }, {
      "heading" : "3 Single-Sequence Model",
      "text" : "The single-sequence model exploits stacked bidirectional RNNs (Bi-RNN) (Schuster and Paliwal, 1997; Graves et al., 2005, 2013; Zhou and Xu, 2015). Figure 3 shows the overall architecture, which consists of the following three components:\nInput Layer: Map each word to a feature vector representation.\nRNN Layer: Produce high-level feature vectors using Bi-RNNs.\nOutput Layer: Compute the probability of each case label for each word using the softmax function.\nIn the following subsections, we describe each of them in more detail."
    }, {
      "heading" : "3.1 Input Layer",
      "text" : "Given an input sentence w1:T = (w1, · · · , wT) and a predicate p, each word wt is mapped to a feature representation xt, which is the concatenation (⊕) of three types of vectors:\nxt = x arg t ⊕ x pred t ⊕ xmarkt (1)\nwhere each vector is based on the following atomic features inspired by Zhou and Xu (2015):\nARG: Word index of each word. PRED: Word index of the target predicate and\nwords around the predicate.\nMARK: Binary index that represents whether the word is the predicate or not.\nFigure 4: An example of the feature extraction. The underlined word is the target predicate. From the sentence “彼女はパンを食べた。(She ate a bread.)”, the three types of features are extracted for the target predicate “食べた (ate)”.\nFigure 5: An example of the process of creating a feature vector. The extracted features are mapped to each vector, and all the vectors are concatenated into one feature vector.\nFigure 4 presents an example of the atomic features. As the ARG feature, we extract a word index xword ∈ V of each word. Similarly, as the PRED feature, we extract each word index xword of the C words taking the target predicate at the center, where C is the window size. The MARK feature xmark ∈ {0, 1} is a binary value that represents whether the word is the predicate or not. Then, using feature indices, we extract feature vector representations from each embedding matrix. Figure 5 shows the process of creating the feature vector x1 for the word w1 “彼女 (she)”. We set two embedding matrices: (i) word embedding matrix Eword ∈ Rdword×|V|, and (ii) mark embedding matrixEmark ∈ Rdmark×2. From each embedding matrix, we extract corresponding column vectors and concatenate them as a feature vector representation xt based on Eq. 1.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nEach feature vector xt is multiplied with a parameter matrix Wx:\nh (0) t = Wx xt (2)\nThe vector h(0)t is given to the first layer of the RNN layers as input."
    }, {
      "heading" : "3.2 RNN Layer",
      "text" : "In the RNN layers, feature vectors are updated recurrently using Bi-RNNs. Bi-RNNs process an input sequence from the left-to-right manner in odd-numbered layers and the opposite in evennumbered layers. By stacking these layers, we can construct the deeper network structures.\nStacked Bi-RNNs consist of L layers, and the hidden state in the layer ℓ ∈ (1, · · · , L) is calculated as follows:\nh (ℓ) t =\n{ g(ℓ)(h\n(ℓ−1) t , h (ℓ) t−1) (ℓ = odd)\ng(ℓ)(h (ℓ−1) t , h (ℓ) t+1) (ℓ = even)\n(3)\nBoth of the odd and even-numbered layers receive h(ℓ−1)t , the t-th hidden state of the ℓ − 1 layer, as the first input of the function g(ℓ), which is an arbitrary function 3. As the second input of g(ℓ), odd-numbered layers receive h(ℓ)t−1 while even-numbered layers receive h(ℓ)t+1. By calculating the hidden states until the L-th layer, we obtain a hidden state sequence h(L)1:T = (h (L) 1 , · · · ,h (L) T ). Using each vector h(L)t , we calculate the probability of case labels for each word in the output layer."
    }, {
      "heading" : "3.3 Output Layer",
      "text" : "In the output layer, multi-class classification is performed using the softmax function:\nyt = softmax(Wy h (L) t )\nwhere h(L)t is a vector representation propagated from the last RNN layer (Fig 3). Each element of yt is a probability value corresponding to each label. The label with the maximum probability among them is output as a result. In this task, there are five labels: NOM, ACC, DAT, PRED, null. The labels NOM, ACC and DAT indicate the nominative, accusative and dative case, respectively. PRED is the label for the predicate. null represents a word that does not play any case role.\n3In this work, we use the Gated Recurrent Unit (GRU) (Cho et al., 2014) as the function g(ℓ)."
    }, {
      "heading" : "4 Multi-Sequence Model",
      "text" : "While the single-sequence model assumes the independence between predicates, the multisequence model assumes the multi-predicate interactions. To capture such interactions between all predicates in a sentence, we extend the singlesequence model to the multi-sequence model using Grid-RNNs (Graves and Schmidhuber, 2009; Kalchbrenner et al., 2016). Figure 6 presents the overall architecture of the multi-sequence model, which consists of three components:\nInput Layer: Map words to M sequences of feature vectors for M predicates.\nGrid Layer: Update the hidden states over different sequences using Grid-RNNs.\nOutput Layer: Compute the probability of each case label for each word using the softmax function.\nIn the following subsections, we describe them in more detail."
    }, {
      "heading" : "4.1 Input Layer",
      "text" : "The multi-sequence model takes as input a sentence w1:T = (w1, · · · , wT) and all predicates {pm}M1 in the sentence. For each predicate pm, the input layer creates a sequence of feature vectors Xm = (xm,1, · · · ,xm,T) by mapping each input word wt to a feature vector xm,t based on Eq 1. That is, for M predicates, M sequences of feature vectors {Xm}M1 are created. Then, using Eq. 2, each feature vector xm,t is mapped to h(0)m,t, and a feature sequence is created for a predicate pm, i.e.,H (0) m = (h (0) m,1, · · · ,h (0) m,T). Consequently, forM predicates, we obtainM feature sequences {H(0)m }M1 ."
    }, {
      "heading" : "4.2 Grid Layer",
      "text" : ""
    }, {
      "heading" : "Inter-Sequence Connections",
      "text" : "In the grid layers, we use Grid-RNNs to propagate the feature information over the different sequences (inter-sequence connections). The right figure in Figure 6 shows an odd-numbered layer of the Grid layers. The hidden state is recurrently calculated from upper-left (m = 1, t = 1) to lowerright (m = M, t = T). Formally, in the ℓ-th layer, the hidden state h(ℓ)m,t is calculated as follows:\nh (ℓ) m,t=\n{ g(ℓ)(h\n(ℓ−1) m,t ⊕ h (ℓ) m−1,t,h (ℓ) m,t−1) (ℓ = odd)\ng(ℓ)(h (ℓ−1) m,t ⊕ h (ℓ) m+1,t,h (ℓ) m,t+1) (ℓ = even)\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nThis equation is similar to Eq. 3. The main difference is that the hidden state of a neighboring sequence, h(ℓ)m−1,t (or h (ℓ) m+1,t), is concatenated (⊕) with the hidden state of the previous (ℓ− 1) layer, h (ℓ−1) m,t , and is taken as input of the function g\n(ℓ). In the right figure in Figure 6, the blue curve lines represent the inter-sequence connections. Taking as input the hidden states of neighboring sequences, the network propagates feature information over multiple sequences (predicates). By calculating the hidden states until the L-th layer, we obtain M sequences of the hidden states, i.e., {H(L)m }M1 , in which H (L) m = (h (L) m,1, · · · ,h (L) m,T)."
    }, {
      "heading" : "Residual Connections",
      "text" : "As more layers are stacked, it gets more difficult to learn the model parameters due to some problems such as gradient vanishment (Pascanu et al., 2013). In this work, we integrate residual connections (He et al., 2015;Wu et al., 2016) with our networks to connect between layers. Specifically, the input vector h(ℓ−1)m,t of the ℓ-th layer is added to the output vector h(ℓ)m,t. Residual connections can also be applied to the single-sequence model, and thus we perform the experiments on both models with/without residual connections."
    }, {
      "heading" : "4.3 Output Layer",
      "text" : "Like the single-sequence model, using the softmax function, we calculate the probability of case labels of each word wt for each predicate pm:\nym,t = softmax(Wy h (L) m,t)\nwhere h(L)m,t is a hidden state vector calculated in the last Grid Layer."
    }, {
      "heading" : "5 Related Work",
      "text" : ""
    }, {
      "heading" : "5.1 Japanese PAS Analysis Approaches",
      "text" : "Existing approaches for Japanese PAS analysis are divided into two categories: (i) pointwise approach and (ii) joint approach. In the pointwise approach, we estimate the score of each argument candidate for one predicate, and select the argument candidate with the maximum score as an argument (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida et al., 2016). In the joint approach, we select the score of all the predicate-argument combinations in one sentence, and select the combination with the highest score (Yoshikawa et al., 2011; Sasano and Kurohashi, 2011; Ouchi et al., 2015; Shibata et al., 2016). Compared with the pointwise approach, the joint approach has achieved better results."
    }, {
      "heading" : "5.2 Multi-Predicate Interactions",
      "text" : "Ouchi et al. (2015) reported that it is beneficial to Japanese PAS analysis to capture the interactions between all predicates in a sentence. This is based on the linguistic intuition that the predicates in a sentence are semantically related to each other, and the interaction information can be a clue for PAS analysis. Similarly, in semantic role labeling (SRL), Yang and Zong (2014) also reported that their reranking model capturing the multi-predicate interactions is effective for the English constituentbased SRL task (Carreras and Màrquez, 2005). Taking a step further in this direction, we propose the neural architecture that effectively models the multi-predicate interactions.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599"
    }, {
      "heading" : "5.3 Neural Approaches",
      "text" : ""
    }, {
      "heading" : "Japanese PAS",
      "text" : "In recent years, several attempts have been made to apply neural networks for Japanese PAS analysis (Shibata et al., 2016; Iida et al., 2016)4. Shibata et al. (2016) used a feed-forward neural network for the score calculation part of the joint model proposed by Ouchi et al. (2015). Iida et al. (2016) used multi-column convolutional neural networks for the zero anaphora resolution task.\nBoth models exploited syntactic and selectional preference information as atomic features of neural networks. Using neural networks, the good performance was realized with mitigating the cost of manually designing combination features. In this work, we demonstrate that even without such syntactic information, our neural models realize the state-of-the-art performance by using word sequence information of a sentence."
    }, {
      "heading" : "English SRL",
      "text" : "Some neural models achieved high performance without syntactic information in English SRL. Collobert et al. (2011) worked on the English constituent-based SRL task (Carreras and Màrquez, 2005). Their model exploited a convolutional neural network and achieved 74.15% in F-measure without syntactic information. Zhou and Xu (2015) also worked on the same task using bidirectional RNNs with CRF and achieved the state-of-the-art result, 81.07% in F-measure. Our models can be regarded as an extension from their model.\nThe main differences between Zhou and Xu (2015) and our work are: (i) the constituent vs dependency-based argument identification and (ii) the multi-predicate consideration. In the constituent-based SRL, since systems are required to identify the spans of arguments for each predicate, Zhou and Xu (2015) used CRF to capture the IOB label dependencies. In contrast, in Japanese dependency-based PAS analysis, since arguments are infrequently adjacent to each other, we replaced the CRF with the softmax function. Also, while the model of Zhou and Xu (2015) predicts arguments for each predicate independently, our multi-sequence model jointly predicts arguments for all predicates in a sentence at a time by considering the multi-predicate interactions.\n4These previous studies used unpublished datasets and evaluated the performance with different experimental settings, so we cannot compare their models with ours."
    }, {
      "heading" : "6 Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Experimental Settings",
      "text" : ""
    }, {
      "heading" : "Dataset",
      "text" : "We use the NAIST Text Corpus 1.5, which consists of 40,000 sentences of Japanese newspaper text (Iida et al., 2007). In the experiments, we adopt the standard data splits (Taira et al., 2008; Imamura et al., 2009; Ouchi et al., 2015):\nTrain: Articles: Jan 1-11, Editorials: Jan-Aug Dev: Articles: Jan 12-13, Editorials: Sept Test: Articles: Jan 14-17, Editorials: Oct-Dec\nWe use the word boundaries annotated to the NAIST Text Corpus and the target predicates that have at least one argument in the same sentence. We do not use any external resources."
    }, {
      "heading" : "Learning",
      "text" : "We train the model parameters by minimizing the cross-entropy loss function:\nL(θ) = − ∑ n ∑ t logP (yt|xt) + λ 2 ||θ||2\nwhere θ is a set of model parameters, and the hyper-parameter λ is the coefficient governing the L2 weight decay."
    }, {
      "heading" : "Implementation Details",
      "text" : "We implement our neural models using Theano (Bastien et al., 2012). The number of epochs is set to 50, and we report the result of the test set in the epoch with the best F-measure of the development set. Parameter optimization is done by stochastic gradient descent method (SGD) using mini-batch, whose size is selected from {2, 4, 8}. The learning rate is automatically adjusted using Adam (Kingma and Ba, 2014). For the L2 weight decay, the hyper-parameter λ in Eq. 4 is selected from {0.001, 0.0005, 0.0001}. In the neural models, the number of the RNN and Grid layers are selected from {2, 4, 6, 8}. The window size C for the PRED feature (Sec. 3.1) is set to 5. Words with frequency 2 or more are mapped to each word index, and the remaining words are mapped to the unknown word index. The dimensions dword and dmark of the embeddings are set to 32. In the single-sequence model, the parameters of GRUs are set to 32× 32. In the multi-sequence model, the parameters of GRUs related to the input values are set to 64 × 32, and\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nDep Zero All Imamura+ 09 85.06 41.65 78.15\nOuchi+ 15 86.07 44.09 79.23 Single-Seq 88.10 46.10 81.15 Multi-Seq 88.17 † 47.12 † 81.42 †\nTable 1: F-measures in the test set. Single-Seq is the single-sequence model, and Multi-Seq is the multi-sequence model. Imamura+ 09 is the model of Imamura et al. (2009) reimplemented by Ouchi et al. (2015), and Ouchi+ 15 is the ALLCases Joint Model of Ouchi et al. (2015). The mark † denotes the significantly better results with the significance level p < 0.05 comparing between Single-Seq and Multi-Seq.\nothers are 32× 32. The initial values of all the parameters are sampled according to the uniform distribution from [− √ 6√\nrow+col ,\n√ 6√\nrow+col ], where row\nand col are the number of rows and columns of each matrix, respectively."
    }, {
      "heading" : "Baseline Models",
      "text" : "We compare our models with the models in the previous works (Sec. 5.1) that use the NAIST Text Corpus 1.5. As a baseline of the pointwise approach, we use the pointwise model5 of Imamura et al. (2009). In addition, as a baseline of the joint approach, we use the model of Ouchi et al. (2015), which achieved the best result on the NAIST Text Corpus 1.5."
    }, {
      "heading" : "6.2 Results",
      "text" : ""
    }, {
      "heading" : "Neural Models vs Baseline Models",
      "text" : "Table 1 presents F-measures of our neural sequence models with 8 RNN or Grid layers and the baseline models on the test set, in which as the significent test, we used the bootstrap resampling method. In all the metrics, both of the single-sequence (Single-Seq) and multisequence model (Multi-Seq) outperformed the baseline models. This confirms that our neural sequence models realize high-performance even without syntactic information by learning contextual information effective for PAS analysis from a word sequence of the sentence.\nIn particular, for zero arguments (Zero), our models achieved a considerable improvement compared with the state-of-the-art model of\n5We compared the results of the model reimplemented by Ouchi et al. (2015).\nOuchi et al. (2015), i.e., the single model improved around 2.0 points and the multi-sequence model improved around 3.0 points in F-measure. These results suggest that it is beneficial to Japanese PAS analysis, particularly to the zero argument identification, to model the context in the entire sentence using RNNs."
    }, {
      "heading" : "Effects of Multiple Predicate Consideration",
      "text" : "As Table 1 shows, the multi-sequence model significantly outperformed the single-sequence model in F-measure in total (81.42% vs 81.15%). This result demonstrates that the grid-type neural architecture can effectively capture the multipredicate interactions by connecting between the sequences of the argument candidates for all predicates in a sentence. Compared with the single-sequence model for different argument types, the multi-sequence model achieved slightly but significantly better result for the direct dependency arguments (Dep) (88.10% vs 88.17%). In addition, for zero arguments (Zero), which have no syntactic dependency with its predicate, the multi-sequence model significantly outperformed the single-sequence model by around 1.0 points in F-measure (46.10% vs 47.12%). This shows that capturing the multipredicate interactions is particularly effective for zero arguments, which is consistent with the results of Ouchi et al. (2015).\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nDep Zero NOM ACC DAT NOM ACC DAT\nNAIST Text Corpus 1.5 Imamura+ 09 86.50 92.84 30.97 45.56 21.38 0.83\nOuchi+ 15 88.13 92.74 38.39 48.11 24.43 4.80 Single-Seq 88.32 93.89 65.91 49.51 35.07 9.83 Multi-Seq 88.75 93.68 64.38 50.65 32.35 7.52\nNAIST Text Corpus 1.4β Taira+ 08* 75.53 88.20 89.51 30.15 11.41 3.66\nImamura+ 09* 87.0 93.9 80.8 50.0 30.8 0.0 Sasano+ 11* - - - 39.5 17.5 8.9\nTable 3: Performance comparison for different case roles on the test set in F-measures. NOM, ACC or DAT is the nominal, accusative or dative case, respectively. The mark * indicates that the model uses external resources."
    }, {
      "heading" : "Effects of Network Depth",
      "text" : "Table 2 presents F-measures of the neural sequence models with different network depths and with/without residual connections. The performance tends to get better as the RNN or Grid layers get deeper with residual connections. In particular, the two models with 8 layers and residual connections achieved considerable improvements of around 1.0 point in F-measure compared the models without residual connections, which means that the residual connections contribute to the effective parameter learning of deeper models."
    }, {
      "heading" : "Comparison per Case Role",
      "text" : "Table 3 shows F-measures for each case role. For reference, we show the results of the previous studies using NAIST Text Corpus 1.4β with external resources as well.6\nComparing between the models using the NAIST Text Corpus 1.5, the single-sequence and multi-sequence models outperformed the baseline models in all the metrics. In particular, for the dative case, the two neural models achieved much higher results by around 30 points. This suggests that although dative arguments appear infrequently compared with the other two case arguments, the neural models can robustly learn it.\nIn addition, for zero arguments (Zero), the neural models achieved better results than the baseline models. Especially, for zero arguments of\n6The major difference between NAIST Text Corpus 1.4β and 1.5 is the revision of the annotation criterion for the dative case (DAT) (corresponding to Japanese case marker “に”). Argument and adjunct usages of the case marker “に” are not distinguished in 1.4β, making the identification of the dative case seemingly easy (Ouchi et al., 2015).\nthe nominative case (NOM), the multi-sequence model achieved a considerable improvement of around 2.5 points in F-measure compared with the state-of-the-art model of Ouchi et al. (2015). To achieve high accuracies for the analysis of such zero arguments, it is necessary to capture long distance dependencies (Iida et al., 2005; Sasano and Kurohashi, 2011; Iida et al., 2015). Therefore, the improvements of the results suggest that the neural models effectively capture long distance dependencies using RNNs that can encode the context in the entire sentence."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this work, we introduced neural sequence models that automatically induce effective feature representations from word sequence information of a sentence for Japanese PAS analysis. The experiments on NAIST Text Corpus 1.5 demonstrated that the models achieve the state-of-the-art result without syntactic information. In particular, our multi-sequence model improved the performance for zero argument identification, one of the problematic issues in Japanese PAS analysis, by considering the multi-predicate interactions using Grid-RNNs. Since our neural models are applicable to SRL, applying our models for multilingual SRL tasks is an interesting line of the future research. In addition, in this work, the model parameters were learned without any external resources. For future work, we plan to explore effective methods for exploiting large-scale unlabeled data to learn the neural models.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "Frédéric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio." ],
      "venue" : "Deep Learning and Unsupervised Feature Learning",
      "citeRegEx" : "Bastien et al\\.,? 2012",
      "shortCiteRegEx" : "Bastien et al\\.",
      "year" : 2012
    }, {
      "title" : "Introduction to the conll-2005 shared task: Semantic role labeling",
      "author" : [ "Xavier Carreras", "Lluı́s Màrquez" ],
      "venue" : "In Proceedings of CoNLL",
      "citeRegEx" : "Carreras and Màrquez.,? \\Q2005\\E",
      "shortCiteRegEx" : "Carreras and Màrquez.",
      "year" : 2005
    }, {
      "title" : "Learning phrase representations using rnn encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of Machine Learning Research .",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Hybrid speech recognition with deep bidirectional lstm",
      "author" : [ "Alan Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed." ],
      "venue" : "Proceedings of Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop.",
      "citeRegEx" : "Graves et al\\.,? 2013",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2013
    }, {
      "title" : "Bidirectional lstm networks for improved phoneme classification and recognition",
      "author" : [ "Alex Graves", "Santiago Fernández", "Jürgen Schmidhuber." ],
      "venue" : "Proceedings of International Conference on Artificial Neural Networks. pages 799–804.",
      "citeRegEx" : "Graves et al\\.,? 2005",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2005
    }, {
      "title" : "Offline handwriting recognition with multidimensional recurrent neural networks",
      "author" : [ "Alex Graves", "Jürgen Schmidhuber." ],
      "venue" : "Proceedings of NIPS. pages 545–552.",
      "citeRegEx" : "Graves and Schmidhuber.,? 2009",
      "shortCiteRegEx" : "Graves and Schmidhuber.",
      "year" : 2009
    }, {
      "title" : "Japanese predicate argument structure analysis exploiting argument position and type",
      "author" : [ "Yuta Hayashibe", "Mamoru Komachi", "Yuji Matsumoto." ],
      "venue" : "Proceedings of IJCNLP. pages 201–209.",
      "citeRegEx" : "Hayashibe et al\\.,? 2011",
      "shortCiteRegEx" : "Hayashibe et al\\.",
      "year" : 2011
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "arXiv preprint arXiv:1512.03385 .",
      "citeRegEx" : "He et al\\.,? 2015",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Anaphora resolution by antecedent identification followed by anaphoricity determination",
      "author" : [ "Ryu Iida", "Kentaro Inui", "Yuji Matsumoto." ],
      "venue" : "ACM Transactions on Asian Language Information Processing (TALIP) 4(4):417–434.",
      "citeRegEx" : "Iida et al\\.,? 2005",
      "shortCiteRegEx" : "Iida et al\\.",
      "year" : 2005
    }, {
      "title" : "Annotating a japanese text corpus with predicate-argument and coreference relations",
      "author" : [ "Ryu Iida", "Mamoru Komachi", "Kentaro Inui", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the Linguistic Annotation Workshop. pages 132–139.",
      "citeRegEx" : "Iida et al\\.,? 2007",
      "shortCiteRegEx" : "Iida et al\\.",
      "year" : 2007
    }, {
      "title" : "A cross-lingual ilp solution to zero anaphora resolution",
      "author" : [ "Ryu Iida", "Massimo Poesio." ],
      "venue" : "Proceedings of ACL-HLT . pages 804–813.",
      "citeRegEx" : "Iida and Poesio.,? 2011",
      "shortCiteRegEx" : "Iida and Poesio.",
      "year" : 2011
    }, {
      "title" : "Intrasentential zero anaphora resolution using subject sharing recognition",
      "author" : [ "Ryu Iida", "Kentaro Torisawa", "Chikara Hashimoto", "JongHoon Oh", "Julien Kloetzer." ],
      "venue" : "Proceedings of EMNLP. pages 2179–2189.",
      "citeRegEx" : "Iida et al\\.,? 2015",
      "shortCiteRegEx" : "Iida et al\\.",
      "year" : 2015
    }, {
      "title" : "Intrasentential subject zero anaphora resolution using multi-column convolutional neural network",
      "author" : [ "Ryu Iida", "Kentaro Torisawa", "Jong-Hoon Oh", "Canasai Kruengkrai", "Julien Kloetzer." ],
      "venue" : "Proceedings of EMNLP. pages 1244–1254.",
      "citeRegEx" : "Iida et al\\.,? 2016",
      "shortCiteRegEx" : "Iida et al\\.",
      "year" : 2016
    }, {
      "title" : "Discriminative approach to predicateargument structure analysis with zero-anaphora resolution",
      "author" : [ "Kenji Imamura", "Kuniko Saito", "Tomoko Izumi." ],
      "venue" : "Proceedings of ACL-IJCNLP. pages 85–",
      "citeRegEx" : "Imamura et al\\.,? 2009",
      "shortCiteRegEx" : "Imamura et al\\.",
      "year" : 2009
    }, {
      "title" : "Grid long short-term memory",
      "author" : [ "Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Kalchbrenner et al\\.,? 2016",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "J. Ba." ],
      "venue" : "arXiv preprint arXiv: 1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Joint case argument identification for japanese predicate argument structure analysis",
      "author" : [ "Hiroki Ouchi", "Hiroyuki Shindo", "Kevin Duh", "Yuji Matsumoto." ],
      "venue" : "Proceedings of ACL-IJCNLP. pages 961– 970.",
      "citeRegEx" : "Ouchi et al\\.,? 2015",
      "shortCiteRegEx" : "Ouchi et al\\.",
      "year" : 2015
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Pascanu et al\\.,? 2013",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2013
    }, {
      "title" : "A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames",
      "author" : [ "Ryohei Sasano", "Sadao Kurohashi." ],
      "venue" : "Proceedings of IJCNLP. pages 758–766.",
      "citeRegEx" : "Sasano and Kurohashi.,? 2011",
      "shortCiteRegEx" : "Sasano and Kurohashi.",
      "year" : 2011
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "Mike Schuster", "Kuldip K Paliwal." ],
      "venue" : "IEEE Transactions on Signal Processing pages 2673–2681.",
      "citeRegEx" : "Schuster and Paliwal.,? 1997",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "Neural network-based model for japanese predicate argument structure analysis",
      "author" : [ "Tomohide Shibata", "Daisuke Kawahara", "Sadao Kurohashi." ],
      "venue" : "Proceedings of ACL. pages 1235–1244.",
      "citeRegEx" : "Shibata et al\\.,? 2016",
      "shortCiteRegEx" : "Shibata et al\\.",
      "year" : 2016
    }, {
      "title" : "A japanese predicate argument structure analysis using decision lists",
      "author" : [ "Hirotoshi Taira", "Sanae Fujita", "Masaaki Nagata." ],
      "venue" : "Proceedings of EMNLP. pages 523–532.",
      "citeRegEx" : "Taira et al\\.,? 2008",
      "shortCiteRegEx" : "Taira et al\\.",
      "year" : 2008
    }, {
      "title" : "Multipredicate semantic role labeling",
      "author" : [ "Haitong Yang", "Chengqing Zong." ],
      "venue" : "Proceedings of EMNLP. pages 363–373.",
      "citeRegEx" : "Yang and Zong.,? 2014",
      "shortCiteRegEx" : "Yang and Zong.",
      "year" : 2014
    }, {
      "title" : "Jointly extracting japanese predicate-argument relation with markov logic",
      "author" : [ "Katsumasa Yoshikawa", "Masayuki Asahara", "Yuji Matsumoto." ],
      "venue" : "Proceedings of IJCNLP. pages 1125–1133.",
      "citeRegEx" : "Yoshikawa et al\\.,? 2011",
      "shortCiteRegEx" : "Yoshikawa et al\\.",
      "year" : 2011
    }, {
      "title" : "End-to-end learning of semantic role labeling using recurrent neural networks",
      "author" : [ "Jie Zhou", "Wei Xu." ],
      "venue" : "Proceedings of ACL-IJCNLP.",
      "citeRegEx" : "Zhou and Xu.,? 2015",
      "shortCiteRegEx" : "Zhou and Xu.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "In pro-drop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues of PAS analysis (Iida and Poesio, 2011; Sasano and Kurohashi, 2011).",
      "startOffset" : 191,
      "endOffset" : 242
    }, {
      "referenceID" : 19,
      "context" : "In pro-drop languages such as Japanese, Chinese and Italian, arguments are often omitted in text, and such argument omission is regarded as one of the most problematic issues of PAS analysis (Iida and Poesio, 2011; Sasano and Kurohashi, 2011).",
      "startOffset" : 191,
      "endOffset" : 242
    }, {
      "referenceID" : 17,
      "context" : "As an approach to the argument omission problem, in Japanese PAS analysis, joint modeling of interactions between multiple predicates has been gaining popularity and achieved the state-of-theart result (Ouchi et al., 2015; Shibata et al., 2016).",
      "startOffset" : 202,
      "endOffset" : 244
    }, {
      "referenceID" : 21,
      "context" : "As an approach to the argument omission problem, in Japanese PAS analysis, joint modeling of interactions between multiple predicates has been gaining popularity and achieved the state-of-theart result (Ouchi et al., 2015; Shibata et al., 2016).",
      "startOffset" : 202,
      "endOffset" : 244
    }, {
      "referenceID" : 10,
      "context" : "Performing experiments on the NAIST Text Corpus (Iida et al., 2007), we demonstrate that our neural models exceed the accuracy of the stateof-the-art Japanese PAS analyzer (Ouchi et al.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 17,
      "context" : ", 2007), we demonstrate that our neural models exceed the accuracy of the stateof-the-art Japanese PAS analyzer (Ouchi et al., 2015).",
      "startOffset" : 112,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "Arguments can be divided into the following three categories according to the positions relative to their predicates (Hayashibe et al., 2011; Ouchi et al., 2015):",
      "startOffset" : 117,
      "endOffset" : 161
    }, {
      "referenceID" : 17,
      "context" : "Arguments can be divided into the following three categories according to the positions relative to their predicates (Hayashibe et al., 2011; Ouchi et al., 2015):",
      "startOffset" : 117,
      "endOffset" : 161
    }, {
      "referenceID" : 22,
      "context" : "(2015) The F-measure remains 10-20% (Taira et al., 2008; Imamura et al., 2009; Sasano and Kurohashi, 2011).",
      "startOffset" : 36,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "(2015) The F-measure remains 10-20% (Taira et al., 2008; Imamura et al., 2009; Sasano and Kurohashi, 2011).",
      "startOffset" : 36,
      "endOffset" : 106
    }, {
      "referenceID" : 19,
      "context" : "(2015) The F-measure remains 10-20% (Taira et al., 2008; Imamura et al., 2009; Sasano and Kurohashi, 2011).",
      "startOffset" : 36,
      "endOffset" : 106
    }, {
      "referenceID" : 16,
      "context" : "2 Thus, Ouchi et al. (2015) The F-measure remains 10-20% (Taira et al.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : ", 2008; Imamura et al., 2009; Sasano and Kurohashi, 2011). and Shibata et al. (2016) focused on only intrasentential argument analysis.",
      "startOffset" : 8,
      "endOffset" : 85
    }, {
      "referenceID" : 20,
      "context" : "The single-sequence model exploits stacked bidirectional RNNs (Bi-RNN) (Schuster and Paliwal, 1997; Graves et al., 2005, 2013; Zhou and Xu, 2015).",
      "startOffset" : 71,
      "endOffset" : 145
    }, {
      "referenceID" : 25,
      "context" : "The single-sequence model exploits stacked bidirectional RNNs (Bi-RNN) (Schuster and Paliwal, 1997; Graves et al., 2005, 2013; Zhou and Xu, 2015).",
      "startOffset" : 71,
      "endOffset" : 145
    }, {
      "referenceID" : 25,
      "context" : "where each vector is based on the following atomic features inspired by Zhou and Xu (2015):",
      "startOffset" : 72,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "In this work, we use the Gated Recurrent Unit (GRU) (Cho et al., 2014) as the function g.",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "To capture such interactions between all predicates in a sentence, we extend the singlesequence model to the multi-sequence model using Grid-RNNs (Graves and Schmidhuber, 2009; Kalchbrenner et al., 2016).",
      "startOffset" : 146,
      "endOffset" : 203
    }, {
      "referenceID" : 15,
      "context" : "To capture such interactions between all predicates in a sentence, we extend the singlesequence model to the multi-sequence model using Grid-RNNs (Graves and Schmidhuber, 2009; Kalchbrenner et al., 2016).",
      "startOffset" : 146,
      "endOffset" : 203
    }, {
      "referenceID" : 18,
      "context" : "Residual Connections As more layers are stacked, it gets more difficult to learn the model parameters due to some problems such as gradient vanishment (Pascanu et al., 2013).",
      "startOffset" : 151,
      "endOffset" : 173
    }, {
      "referenceID" : 8,
      "context" : "In this work, we integrate residual connections (He et al., 2015;Wu et al., 2016) with our networks to connect between layers.",
      "startOffset" : 48,
      "endOffset" : 81
    }, {
      "referenceID" : 22,
      "context" : "In the pointwise approach, we estimate the score of each argument candidate for one predicate, and select the argument candidate with the maximum score as an argument (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida et al., 2016).",
      "startOffset" : 167,
      "endOffset" : 252
    }, {
      "referenceID" : 14,
      "context" : "In the pointwise approach, we estimate the score of each argument candidate for one predicate, and select the argument candidate with the maximum score as an argument (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida et al., 2016).",
      "startOffset" : 167,
      "endOffset" : 252
    }, {
      "referenceID" : 7,
      "context" : "In the pointwise approach, we estimate the score of each argument candidate for one predicate, and select the argument candidate with the maximum score as an argument (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida et al., 2016).",
      "startOffset" : 167,
      "endOffset" : 252
    }, {
      "referenceID" : 13,
      "context" : "In the pointwise approach, we estimate the score of each argument candidate for one predicate, and select the argument candidate with the maximum score as an argument (Taira et al., 2008; Imamura et al., 2009; Hayashibe et al., 2011; Iida et al., 2016).",
      "startOffset" : 167,
      "endOffset" : 252
    }, {
      "referenceID" : 24,
      "context" : "In the joint approach, we select the score of all the predicate-argument combinations in one sentence, and select the combination with the highest score (Yoshikawa et al., 2011; Sasano and Kurohashi, 2011; Ouchi et al., 2015; Shibata et al., 2016).",
      "startOffset" : 153,
      "endOffset" : 247
    }, {
      "referenceID" : 19,
      "context" : "In the joint approach, we select the score of all the predicate-argument combinations in one sentence, and select the combination with the highest score (Yoshikawa et al., 2011; Sasano and Kurohashi, 2011; Ouchi et al., 2015; Shibata et al., 2016).",
      "startOffset" : 153,
      "endOffset" : 247
    }, {
      "referenceID" : 17,
      "context" : "In the joint approach, we select the score of all the predicate-argument combinations in one sentence, and select the combination with the highest score (Yoshikawa et al., 2011; Sasano and Kurohashi, 2011; Ouchi et al., 2015; Shibata et al., 2016).",
      "startOffset" : 153,
      "endOffset" : 247
    }, {
      "referenceID" : 21,
      "context" : "In the joint approach, we select the score of all the predicate-argument combinations in one sentence, and select the combination with the highest score (Yoshikawa et al., 2011; Sasano and Kurohashi, 2011; Ouchi et al., 2015; Shibata et al., 2016).",
      "startOffset" : 153,
      "endOffset" : 247
    }, {
      "referenceID" : 1,
      "context" : "Similarly, in semantic role labeling (SRL), Yang and Zong (2014) also reported that their reranking model capturing the multi-predicate interactions is effective for the English constituentbased SRL task (Carreras and Màrquez, 2005).",
      "startOffset" : 204,
      "endOffset" : 232
    }, {
      "referenceID" : 21,
      "context" : "3 Neural Approaches Japanese PAS In recent years, several attempts have been made to apply neural networks for Japanese PAS analysis (Shibata et al., 2016; Iida et al., 2016)4.",
      "startOffset" : 133,
      "endOffset" : 174
    }, {
      "referenceID" : 13,
      "context" : "3 Neural Approaches Japanese PAS In recent years, several attempts have been made to apply neural networks for Japanese PAS analysis (Shibata et al., 2016; Iida et al., 2016)4.",
      "startOffset" : 133,
      "endOffset" : 174
    }, {
      "referenceID" : 9,
      "context" : ", 2016; Iida et al., 2016)4. Shibata et al. (2016) used a feed-forward neural network for the score calculation part of the joint model proposed by Ouchi et al.",
      "startOffset" : 8,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : ", 2016; Iida et al., 2016)4. Shibata et al. (2016) used a feed-forward neural network for the score calculation part of the joint model proposed by Ouchi et al. (2015). Iida et al.",
      "startOffset" : 8,
      "endOffset" : 168
    }, {
      "referenceID" : 9,
      "context" : ", 2016; Iida et al., 2016)4. Shibata et al. (2016) used a feed-forward neural network for the score calculation part of the joint model proposed by Ouchi et al. (2015). Iida et al. (2016) used multi-column convolutional neural networks for the zero anaphora resolution task.",
      "startOffset" : 8,
      "endOffset" : 188
    }, {
      "referenceID" : 1,
      "context" : "(2011) worked on the English constituent-based SRL task (Carreras and Màrquez, 2005).",
      "startOffset" : 56,
      "endOffset" : 84
    }, {
      "referenceID" : 2,
      "context" : "Collobert et al. (2011) worked on the English constituent-based SRL task (Carreras and Màrquez, 2005).",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : "(2011) worked on the English constituent-based SRL task (Carreras and Màrquez, 2005). Their model exploited a convolutional neural network and achieved 74.15% in F-measure without syntactic information. Zhou and Xu (2015) also worked on the same task using bidirectional RNNs with CRF and achieved the state-of-the-art result, 81.",
      "startOffset" : 57,
      "endOffset" : 222
    }, {
      "referenceID" : 1,
      "context" : "(2011) worked on the English constituent-based SRL task (Carreras and Màrquez, 2005). Their model exploited a convolutional neural network and achieved 74.15% in F-measure without syntactic information. Zhou and Xu (2015) also worked on the same task using bidirectional RNNs with CRF and achieved the state-of-the-art result, 81.07% in F-measure. Our models can be regarded as an extension from their model. The main differences between Zhou and Xu (2015) and our work are: (i) the constituent vs dependency-based argument identification and (ii) the multi-predicate consideration.",
      "startOffset" : 57,
      "endOffset" : 457
    }, {
      "referenceID" : 1,
      "context" : "(2011) worked on the English constituent-based SRL task (Carreras and Màrquez, 2005). Their model exploited a convolutional neural network and achieved 74.15% in F-measure without syntactic information. Zhou and Xu (2015) also worked on the same task using bidirectional RNNs with CRF and achieved the state-of-the-art result, 81.07% in F-measure. Our models can be regarded as an extension from their model. The main differences between Zhou and Xu (2015) and our work are: (i) the constituent vs dependency-based argument identification and (ii) the multi-predicate consideration. In the constituent-based SRL, since systems are required to identify the spans of arguments for each predicate, Zhou and Xu (2015) used CRF to capture the IOB label dependencies.",
      "startOffset" : 57,
      "endOffset" : 714
    }, {
      "referenceID" : 1,
      "context" : "(2011) worked on the English constituent-based SRL task (Carreras and Màrquez, 2005). Their model exploited a convolutional neural network and achieved 74.15% in F-measure without syntactic information. Zhou and Xu (2015) also worked on the same task using bidirectional RNNs with CRF and achieved the state-of-the-art result, 81.07% in F-measure. Our models can be regarded as an extension from their model. The main differences between Zhou and Xu (2015) and our work are: (i) the constituent vs dependency-based argument identification and (ii) the multi-predicate consideration. In the constituent-based SRL, since systems are required to identify the spans of arguments for each predicate, Zhou and Xu (2015) used CRF to capture the IOB label dependencies. In contrast, in Japanese dependency-based PAS analysis, since arguments are infrequently adjacent to each other, we replaced the CRF with the softmax function. Also, while the model of Zhou and Xu (2015) predicts arguments for each predicate independently, our multi-sequence model jointly predicts arguments for all predicates in a sentence at a time by considering the multi-predicate interactions.",
      "startOffset" : 57,
      "endOffset" : 966
    }, {
      "referenceID" : 10,
      "context" : "5, which consists of 40,000 sentences of Japanese newspaper text (Iida et al., 2007).",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 22,
      "context" : "In the experiments, we adopt the standard data splits (Taira et al., 2008; Imamura et al., 2009; Ouchi et al., 2015):",
      "startOffset" : 54,
      "endOffset" : 116
    }, {
      "referenceID" : 14,
      "context" : "In the experiments, we adopt the standard data splits (Taira et al., 2008; Imamura et al., 2009; Ouchi et al., 2015):",
      "startOffset" : 54,
      "endOffset" : 116
    }, {
      "referenceID" : 17,
      "context" : "In the experiments, we adopt the standard data splits (Taira et al., 2008; Imamura et al., 2009; Ouchi et al., 2015):",
      "startOffset" : 54,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : "Implementation Details We implement our neural models using Theano (Bastien et al., 2012).",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 16,
      "context" : "The learning rate is automatically adjusted using Adam (Kingma and Ba, 2014).",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "Imamura+ 09 is the model of Imamura et al. (2009) reimplemented by Ouchi et al.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 14,
      "context" : "Imamura+ 09 is the model of Imamura et al. (2009) reimplemented by Ouchi et al. (2015), and Ouchi+ 15 is the ALLCases Joint Model of Ouchi et al.",
      "startOffset" : 28,
      "endOffset" : 87
    }, {
      "referenceID" : 14,
      "context" : "Imamura+ 09 is the model of Imamura et al. (2009) reimplemented by Ouchi et al. (2015), and Ouchi+ 15 is the ALLCases Joint Model of Ouchi et al. (2015). The mark † denotes the significantly better results with the significance level p < 0.",
      "startOffset" : 28,
      "endOffset" : 153
    }, {
      "referenceID" : 14,
      "context" : "As a baseline of the pointwise approach, we use the pointwise model5 of Imamura et al. (2009). In addition, as a baseline of the joint approach, we use the model of Ouchi et al.",
      "startOffset" : 72,
      "endOffset" : 94
    }, {
      "referenceID" : 14,
      "context" : "As a baseline of the pointwise approach, we use the pointwise model5 of Imamura et al. (2009). In addition, as a baseline of the joint approach, we use the model of Ouchi et al. (2015), which achieved the best result on the NAIST Text Corpus 1.",
      "startOffset" : 72,
      "endOffset" : 185
    }, {
      "referenceID" : 17,
      "context" : "In particular, for zero arguments (Zero), our models achieved a considerable improvement compared with the state-of-the-art model of We compared the results of the model reimplemented by Ouchi et al. (2015). Single-Seq Multi-Seq L +res.",
      "startOffset" : 187,
      "endOffset" : 207
    }, {
      "referenceID" : 17,
      "context" : "This shows that capturing the multipredicate interactions is particularly effective for zero arguments, which is consistent with the results of Ouchi et al. (2015).",
      "startOffset" : 144,
      "endOffset" : 164
    }, {
      "referenceID" : 17,
      "context" : "4β, making the identification of the dative case seemingly easy (Ouchi et al., 2015).",
      "startOffset" : 64,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "To achieve high accuracies for the analysis of such zero arguments, it is necessary to capture long distance dependencies (Iida et al., 2005; Sasano and Kurohashi, 2011; Iida et al., 2015).",
      "startOffset" : 122,
      "endOffset" : 188
    }, {
      "referenceID" : 19,
      "context" : "To achieve high accuracies for the analysis of such zero arguments, it is necessary to capture long distance dependencies (Iida et al., 2005; Sasano and Kurohashi, 2011; Iida et al., 2015).",
      "startOffset" : 122,
      "endOffset" : 188
    }, {
      "referenceID" : 12,
      "context" : "To achieve high accuracies for the analysis of such zero arguments, it is necessary to capture long distance dependencies (Iida et al., 2005; Sasano and Kurohashi, 2011; Iida et al., 2015).",
      "startOffset" : 122,
      "endOffset" : 188
    }, {
      "referenceID" : 13,
      "context" : "4β, making the identification of the dative case seemingly easy (Ouchi et al., 2015). the nominative case (NOM), the multi-sequence model achieved a considerable improvement of around 2.5 points in F-measure compared with the state-of-the-art model of Ouchi et al. (2015). To achieve high accuracies for the analysis of such zero arguments, it is necessary to capture long distance dependencies (Iida et al.",
      "startOffset" : 65,
      "endOffset" : 272
    } ],
    "year" : 2017,
    "abstractText" : "The accuracy of Japanese predicate argument structure (PAS) analysis has improved in recent years thanks to joint modeling of interactions between multiple predicates. However, this approach heavily relies on syntactic information predicted by parsers, and suffers from the error propagation. To remedy this problem, we introduce a model using grid-type recurrent neural networks (Grid-RNN), which automatically induces features sensitive to multi-predicate interactions from word sequence information of a sentence. The experiments on the NAIST Text Corpus show that our model exceeds the accuracy of the state-of-the-art Japanese PAS analyzer without syntactic information.",
    "creator" : " TeX output 2017.02.07:2041"
  }
}