{
  "name" : "792.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "LSTMEMBED: a Lexical and SemanTic Model of Embeddings with a bidirectional LSTM",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recurrent neural networks (RNNs) with Long Short-Term Memory (LSTMs) have recently gained considerable popularity. Introduced by Hochreiter and Schmidhuber (1997), LSTMs are a special kind of RNN capable of learning long-term dependencies on problems related with sequential data. RNNs, and particularly LSTMs have been working extremely well on a large variety of problems in NLP, such as machine translation (Cho et al., 2014), lexical substitution (Melamud et al., 2016), word sense disambiguation, (Kågebäck and Salomonsson, 2016; Yuan et al., 2016), syntactic parsing (Dyer et al., 2015), among others.\nEmbeddings represent lexical and semantic items in a low-dimensional continuous space. The resulting vectors capture useful syntactic and semantic information, such as regularities in language, where relationships are characterized by a relation-specific vector offset. Recent approaches, such as word2vec (Mikolov et al., 2013), and GloVe (Pennington et al., 2014) are efficient for\nlearning embeddings, but they do not take into account word ordering. Vice versa, RNNs take order into account but they are not competitive in terms of speed or quality (Mikolov et al., 2010; Mikolov and Zweig, 2012; Mesnil et al., 2013).\nThe recently celebrated LSTMs appear to be perfect for learning sequence representations, like phrases (Hill et al., 2016) and contexts (Melamud et al., 2016). However, when dealing with large vocabularies, LSTMs involve time-intensive matrix-matrix multiplications, making them prohibitively expensive. This issue was addressed by Jean et al. (2015), who proposed an approximate training algorithm based on sampling, and Zoph et al. (2016), who introduced an adaptation for GPUs of the noise-contrastive estimation algorithm, a method that avoids repeated summations by training the model to correctly separate generated noise samples from words observed in the training data. However, both these approaches speed up the training process at the cost of lowering the performance.\nOur contributions are twofold:\n• We introduce LSTMEmbed, an RNN model based on a bidirectional LSTM for training word and sense embeddings that which outperforms classical approaches such as word2vec and GloVe.\n• We present an innovative idea for enriching these representations with semantic knowledge from large corpora and vocabularies, while at the same time speeding up the training process.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199"
    }, {
      "heading" : "2 Vector Space Models for words and senses",
      "text" : ""
    }, {
      "heading" : "2.1 Word Embeddings",
      "text" : "Embeddings represent lexical and semantic items as real-valued continuous vectors. These representations are extremely good at capturing syntactic and semantic regularities in language as well as relationships among items. Recent advances (Mikolov et al., 2013, word2vec) showed that word representations learnt with a neural network trained with raw text show relationships such as the male/female relationship, e.g. the induced vector representations of king −man + woman resulted very close to the induced vector of queen. GloVe, an alternative approach trained on aggregated global word-word co-occurrences, reached similar results. While these embeddings are surprisingly good for monosemous words, they fail to represent properly the non-dominant senses of words. For instance, the representations of bar and pub should be similar, as well as those of bar and stick, but having similar representations of pub and stick is undesired. Several approaches were proposed to address this problem: Yu and Dredze (2014) presented an alternative way to train word embeddings by using, in addition to common features, words having some relation in a semantic resource, like PPDB1 (Ganitkevitch et al., 2013) or WordNet (Miller, 1995). Faruqui et al. (2015) presented a technique applicable to pre-processed embeddings, in which vectors are updated (i.e. retrofitted) in order to make them more similar to those which share a word type and less similar those which do not. The word types were extracted from diverse semantic resources such as PPDB, WordNet and FrameNet (Baker et al., 1998). Finally, Melamud et al. (2016) introduced context2vec, a model based on a bidirectional LSTM for learning word embeddings. They use large raw text corpora to learn a neural model that embeds entire sentential contexts and target words in the same low-dimensional space."
    }, {
      "heading" : "2.2 Sense Embeddings",
      "text" : "In contrast with the above approaches, which aim to learn representations of single words, sense embeddings represent individual word senses as separate vectors. Sense embeddings can learned with the main approaches: (1) supervised, which rely\n1www.paraphrase.org/#/download\non a predefined sense inventory such as WordNet, Wikipedia, BabelNet2 (Navigli and Ponzetto, 2012) or Freebase (Bollacker et al., 2008), and (2) unsupervised, which rely on contextual information from parallel corpora to discriminate word senses among different occurrences. From the first group we can highlight successful approaches: SENSEMBED (Iacobacci et al., 2015) used Babelfy3, a state-of-the-art tool for Word Sense Disambiguation and Entity linking, to build a sense-annotated corpus which was in turn used to train a vector space model for word senses with word2vec. SENSEMBED exploits the structured knowledge of BabelNet’s sense inventory along with the distributional information gathered from text corpora. An important limitation of this approach was the inability to train both word and sense embeddings. The only word embeddings that the model was able to learn were the representations of words which were not annotated by the disambiguation tool. Those representations were of poor quality due to the fact that they are learnt from the occurrences in a ambiguous or unclear context, else the disambiguation should be able to annotate them. In addition, owing to the fact that it is based on word2vec, this approach suffers from the lack of word ordering. AutoExtend (Rothe and Schütze, 2015) is a system that learns embeddings for lexemes, senses and synsets from WordNet in a shared space. The synset/lexeme embeddings live in the same vector space as the word embeddings, given the constraint that words are sums of their lexemes and synsets are sums of their lexemes. AutoExtend is based on an autoencoder, a network that mimics the input and output vectors. Finally, Mancini et al. (2016) presented SW2V, an extension of word2vec which learns word and sense embeddings jointly and represents them in a unified vector space. The model was built by exploiting large corpora and knowledge obtained from WordNet and BabelNet. Their basic idea was to extend the CBOW architecture of word2vec to represent both words and senses as different inputs and train the model in order to predict the word and the sense in the middle. In contrast to AutoExtend, the model learns word and sense embeddings in a shared space as an emerging feature, rather than via constraints on both representations. Nevertheless, being based also\n2http://www.babelnet.org/ 3http://babelfy.org/\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nFigure 1: The LSTMEmbed architecture Figure 2: The LSTMEmbedSW extension\non word2vec, SW2V also lacks word order. In marked contrast, LSTMEmbed aims to learn representations for both words and senses in a shared emerging space, handling word ordering, and improving those representations by injecting semantic knowledge via pretrained embeddings."
    }, {
      "heading" : "3 LSTMEmbed",
      "text" : ""
    }, {
      "heading" : "3.1 Model Overview",
      "text" : "At the core of LSTMEmbed is a bidirectional LSTM, a special kind of recurrent neural network (RNN). An RNN is a type of neural network architecture particularly suited for learning time series. The simplest RNN is called Elman network. This network is composed of an input, a hidden and an output layer, just as regular feed-forward networks, with the difference that the hidden layer takes in addition to the input vector xt ∈ Rn, its previous state ht−1 ∈ Rm. Mathematically, the computation follows the formula:\nht = f (Wxt +Uht−1 + b) (1)\nwhere W ∈ Rm×n is the weights matrix from the input to the hidden layer, U ∈ Rm×m is the weights matrix from the previous hidden state to the current one, b ∈ Rm is the bias and f is the activation function. In practice this architecture has several problems in learning long-term dependencies (Hochreiter, 1991; Bengio et al., 1994). To cope the latter, we adopt the Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) architecture which augments the RNN with a series of connections called gates: three gates are added namely the input (it), the forget (ft) and the output (ot) gates. These gates control the flow of information between states. This is achieved\nby adding an extra vector, ct ∈ Rn: in each step, the LSTM takes xt, ht−1 and ct−1 as input and outputs ht and ct according to the following calculations:\nit = σ ( Wixt +U iht−1 + b i )\nft = σ ( Wfxt +U fht−1 + b f )\not = σ (W oxt +U oht−1 + b o) gt = tanh (W gxt +U ght−1 + b g)\nct = ft ct−1 + it gt ht = ot tanh(ct)\n(2)\nwhere σ is the sigmoid function, tanh is the hyperbolic tangent and is the element-wise multiplication operator. The bidirectional LSTM (Graves and Schmidhuber, 2005, BLSTM) is a variant of the original LSTM especially designed for temporal problems where the input direction is relevant. The state at each time step in a BLSTM consists of the state of two LSTMs, one going left and one going right.\nLSTMEmbed, however, is not merely a standard BLSTM: it produces word and sense embeddings jointly based on the following innovations:\n• A single lookup table, shared between both left and right LSTM, learned in the same architecture that represents both words and senses.\n• Using a sense-annotated corpus which includes both words and senses for learning the embeddings.\n• A new way of learning using a set of pretrained embeddings as the objective for injecting semantic information while speeding up the training.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399"
    }, {
      "heading" : "3.2 Definition",
      "text" : "As we mentioned before, our model uses a bidirectional LSTM to obtain word and sense representations. Given a sense-annotated corpus and a set of pretrained embeddings, the objective is to predict the embedding of a single word or sense (embedding given by the pretrained set) given its context. The context is defined by a fixed window W of words and senses on each side.\nFigure 1 illustrates the model architecture. Given a sentence and the position of the target token i (word or sense), the input to our model are two sequences of tokens si−W , ..., si−1, the preceding context, and si+1, ..., si+W , the posterior context. Each token is represented by its corresponding embedding vector v(sj) ∈ Rn, given by a shared table which allows to learn representations taking into account the contextual information on both sides of the sentence. Then, the BLSTM reads both sequences, the LSTM that reads the preceding context from left to right and the LSTM that reads the posterior context, from right to left:\nol = lstml(v(si−W ), ...,v(si−1))\nor = lstmr(v(si+1), ...,v(si+W )) (3)\nThe output of the model is given by a perceptron with sigmoid as its activation function, applied to the concatenation of the outputs of both LSTMs:\noutLSTMEmbed = σ (W o(ol ⊕ or)) (4)\nwhere Wo ∈ R2m×m is the weights matrix and m the dimension of the LSTM. Then, the model compares outLSTMEmbed with emb(si), where emb(si) is the embedding vector of the target token given by the set of pretrained embeddings. The weights of the network are modified in order to maximize the similarity between outLSTMEmbed and emb(si). For error calculation the comparison is in terms of cosine similarity.\nsimilarity = S(~v1, ~v2) = ~v1 · ~v2 ‖~v1‖‖~v2‖\n(5)\nOnce the training is over, the embedding vector of an item s that our model learns is given by v(s)."
    }, {
      "heading" : "3.3 Modeling words and senses together",
      "text" : "The vanilla version of LSTMEmbed is able to learn either word embeddings or sense embeddings. In the following we describe an extension of LSTMEmbed, LSTMEmbedSW (Figure 2), which learns both word and sense embeddings jointly in a shared vector space. Given a sentence and the position of the target item i, the input to our model are four sequences, two that represent words and senses on the left side of the context (wi−W , ..., wi−1 and si−W , ..., si−1) and two sequences that represent words and senses from the right side of the context (wi+1, ..., wi+W and si+1, ..., si+W ).\nThe architecture is augmented by adding an extra layer that takes the sum of the embedding vectors from the word and the sense with the same index and projects them via a linear perceptron into the corresponding LSTM:\npi = W (v(si) + v(wi)) (6)\nwhere W ∈ Rn×n is the weights matrix. In the same way as explained above, the BLSTM reads the two resulting sequences:\nol = lstml(pi−W , ..., pi−1)\nor = lstmr(pi+1, ..., pi+W ) (7)"
    }, {
      "heading" : "3.4 Input alternatives & Objective embeddings",
      "text" : "Corpus type Pretrained Representations\nEmbeddings words senses\nraw text words X – senses – –\nsense-annotated words X X senses X X\nTable 1: LSTMEmbed configuration alternatives.\nOur model can exploit both raw and senseannotated corpora and different types of pretrained embeddings. Given a raw-text training corpus and a set of pretrained word embeddings (i.e. word and sense embeddings), our model is only capable to learn word embeddings, while the combination of raw text and sense embeddings is invalid (see Table 1). With a sense annotated corpus the model can exploit either word or sense embeddings and also it is able to learn word or sense embeddings.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nbarn1 pubn1 barn3 stickn1\ncafen1 coffee shop2n cage5n washer2n grilln1 public bar1n rigid frame1n strap3n mugn4 grilln1 nut3n flap1n\ndoughnutsn2 barn1 coupler1n twine1n pantry1n wine barn1 tappet1n buckle1v buffet2n dinern1 puley1n piece of leather1n\ncoffee shop2n walletn1 stampings1n threaded5v\nTable 2: Closest senses of ambiguous nouns."
    }, {
      "heading" : "3.5 Inspiration from Language Models",
      "text" : "LSTMEmbed is inspired by the architecture or recurrent neural network language models (RNNLM) (Mikolov et al., 2010; Sutskever et al., 2011; Sundermeyer et al., 2012; Zaremba et al., 2014). Language modelling aims to learn a function which defines probability distributions over sequences of words. While RNN-LMs learns to predict target words based on their context, our model learns to predict the embedding of the target word, and uses the learning process for deriving useful representations of words and senses."
    }, {
      "heading" : "4 Similarity Measurement",
      "text" : "In the following we describe how we leverage our representations for the computation of word similarity and analogy tasks."
    }, {
      "heading" : "4.1 Word similarity",
      "text" : "As was claimed by Rubenstein and Goodenough (1965) the proportion of words common to the contexts of two words A and B is a function of the degree in which A and B are similar in meaning. LSTMEmbed is intended for improving word and sense representations by learning from their contexts. While measuring semantic similarity between words we will refer to the conventional approach of Resnik (1999) where the similarity bewteen two words is given by the similarity of their closest senses:\nSim (w1, w2) = max s1∈Sw1 s2∈Sw2\nS (~s1, ~s2) (8)\nwhere Swi is the set of senses associated with the word wi."
    }, {
      "heading" : "4.2 Word Analogy",
      "text" : "Given two pairs of words (wi1, wi2) and (wj1, wj2), the degree of similarity between the\nrelation among wi1 and wi2, and wj1 and wj2 suggests by what degree the two relations are analogous. For measuring this similarity we follow Iacobacci et al. (2015):\nAn(wi1, wi2, , wj1, wj2) = S(v(wi1)− v(wi2), v(wj1)− v(wj2))\n(9)"
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Evaluation Methods",
      "text" : "We evaluate our semantic representations of words and senses with different configurations on tasks of word similarity, synonym identification and word analogy.\nWord Similarity. We evaluate LSTMEmbed on standard word similarity and relatedness datasets: the RG65 (Rubenstein and Goodenough, 1965) and the WordSim-353 (Finkelstein et al., 2002, WS353) datasets. We also include the proposed split made by Agirre and Soroa (2009) who divided WS353 in pairs that measured the degree of similarity (WSSim) and pairs that measured the degree of relatedness (WSRel). We include two large datasets: SimLex999 (Hill et al., 2015), which puts especially focus on representing antonyms as completely unrelated words, and the MEN dataset introduced by Bruni et al. (2014) composed by 3000 pairs. Two datasets especially made for analyzing verb similarity are also included in our experiments: YP130, created by Yang and Powers (2005), based on the taxonomy of WordNet, and SimVerb3500 (Gerz et al., 2016) a newer and much larger dataset extracted from the USF norms data set4 (Nelson et al., 2004) and VerbNet5 (Kipper et al., 2008). Finally, we include evaluations on Stanford Contextual Word Similarities (SCWS), a dataset for measuring word-incontext similarity (Huang et al., 2012). We only report the MaxSim measurement, that is, without taking into account the contextual sentences.\nSynonym Identification. For synonym identification we include two datasets. The first one, introduced by Landauer and Dumais (1997, TOEFL), is extracted directly from the synonym questions of the TOEFL (Test of English as a Foreign Language). The test is composed by\n4http://w3.usf.edu/FreeAssociation/ 5http://verbs.colorado.edu/verb-index\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nModel WS353 WSSim WSRel RG65 MEN SimLex SCWS YP130 SimVerb TOEFL ESL SemEval-2012 MaxDiff Spearman\nword2vec 0.620 0.699 0.478 0.791 0.650 0.441 0.547 0.694 0.433 0.875 0.640 0.404 0.242 GloVe 0.575 0.684 0.426 0.761 0.675 0.478 0.532 0.658 0.399 0.875 0.620 0.420 0.265\nLSTMEmbed 0.634 0.710 0.510 0.719 0.690 0.456 0.552 0.705 0.436 0.878 0.600 0.401 0.216 LSTMEmbedSW 0.561 0.626 0.478 0.785 0.644 0.450 0.529 0.600 0.414 0.869 0.560 0.358 0.139\nTable 3: Spearman correlation for Word Similarity and Synonym identification between sense-based representations.\n80 multiple-choice synonym questions with four choices per question. The second one, introduced by Turney (2001), provide a set of questions extracted, in this case, from the synonym questions of the ESL (English as a Second Language). Similarly to TOEFL, it is composed by 50 multiplechoice synonym questions with four choices per question.\nWord Analogy. For word analogy we experimented with the SemEval-2012 task on Measuring Degrees of Relational Similarity (Jurgens et al., 2012). The task provides a dataset comprising 79 graded word relations, 10 of which are used for training and the rest for test. The task includes two different evaluations, one based on Spearman correlation and a second one based on MaxDiff score (Louviere, 1991)."
    }, {
      "heading" : "5.2 Corpora and Training Details",
      "text" : "Corpora. We compare several sense-annotated corpora: SemCor (Miller et al., 1994) is a manually sense-tagged corpus created by the WordNet project team at Princeton University. The corpus is a subset of the English Brown Corpus and comprises around 360,000 words, providing annotations for more than 200K content words. The Semantically Enriched Wikipedia (Raganato et al., 2016, SEW) is a automatically-constructed corpus which exploits the hyperlink structure of Wikipedia and the wide-coverage sense inventory of BabelNet. Finally Scozzafava et al. (2015, BabelWiki) presented a sense-annotated corpus of the English and Italian Wikipedia, annotated automatically with Babelfy (Moro et al., 2014), a tool for Word Sense Disambiguation on the BabelNet semantic network. We unify the sense annotations of all corpora used to learn our models into a single sense inventory. To this end we choose the last available version of BabelNet (3.7). BabelNet is both a multilingual encyclopedic dictionary, with lexicographic and encyclopedic cover-\nage of terms, and a semantic network which connects concepts and named entities in a network of semantic relations. The resource is a merger of multiple lexical semantic resources such as WordNet and Wikipedia."
    }, {
      "heading" : "5.3 Hyperparameter selection",
      "text" : "For chosing the hyperparameters, we trained LSTMEmbed on a raw corpus6. We evaluated the resulting word representations on WS353, one of the word similarity datasets included in our experiments:\nWe varied the window size between 1 and 15 and we averaged the results across datasets. As in visible in Figure 3, we found that taking a window size longer than 3 words was approximately equivalent across experiments. For our experiments we used a windows size of 10 as that is the recommended window size for word2vec."
    }, {
      "heading" : "5.4 Learning embeddings",
      "text" : "LSTMEmbed was built on the Keras7 library using Theano (Theano Development Team, 2016)\n6http://mattmahoney.net/dc/text8.zip 7https://keras.io\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nas backend. Keras is a neural networks library written in Python and an output of the ONEIROS project (Open-ended Neuro-Electronic Intelligent Robot Operating System). We trained our models with a Nvidia Titan X Pascal GPU on a computer equipped with an eight core Intel Core i73820 running at 3.60GHz with 64GB or RAM."
    }, {
      "heading" : "6 Results",
      "text" : ""
    }, {
      "heading" : "6.1 Corpus Selection",
      "text" : "In Figure 4 we can see the progression of the Spearman correlation while our model learns a larger portion of the training corpus. We can see that the BabelWiki corpus achieves better representations than SEW.\nFigure 4: Spearman correlation in WS353 against learning progression on BabelWiki and SEW."
    }, {
      "heading" : "6.2 Word Similarity with BabelWiki",
      "text" : "In Table 3 we show the results of our model compared with other approaches in the tasks of word similarity and synonym identification. The representations were all trained on the same senseannotated corpus, i.e. BabelWiki. For a fair comparison we trained representations using the same configuration across algorithms: 50-dimension embeddings, window size of 10 words, and a vocabulary of 1M most frequent tokens. For handling frequent words, we discarded the 1000 most frequent tokens. We chose 50 as the LSTM dimensionality, the same dimension than our trained embeddings. We set the batch size in 128 and the training was set only for one epoch. For both word2vec and GloVe, the remaining parameters were chosen following the default configurations. For word2vec we use the Skip-gram with negative sampling set on 10, sub-sampling of frequent words set to 10−3. For GloVe we set the maximum iterations in 15, and the x max in 10.\nModel Dim WSSim RG65 MEN SimLex\nSW2V 300 0.710 0.740 0.760 0.470 AutoExtend 300 - - 0.750 0.450 SensEmbed 400 - - 0.700 0.390\nLSTMEmbed 50 0.710 0.719 0.690 0.456 LSTMEmbedSW 50 0.626 0.785 0.644 0.450\nThe LSTMEmbed architecture obtains slightly but consistently better performance across task. The LSTMEmbedSW extension is consistently worse, with the exception of the RG65 dataset, but is the only approach that offers a joint space of words and sense. Contrary to our expectations, our approach does not appear to be competitive in the task on word analogy. This may be due to the fact that LSTMEmbed is a much more complex model than word2vec and GloVe. The relations in our model might be represented by a higher degree relationship."
    }, {
      "heading" : "6.3 Comparison with other Systems",
      "text" : "We compare the performance of LSTMEmbed against alternative approaches able to obtain sense embeddings: SW2V, AutoExtend and SENSEMBED. Table 4 shows that our approach, although providing the embeddings with smallest dimensionality, achieves competitive results against larger embeddings from the other approaches."
    }, {
      "heading" : "6.4 Word Similarity with SemCor",
      "text" : "In Table 5 we show word similarity experiments when training our model with the SemCor corpus. We performed two classes of experiments: 1) taking the SemCor corpus with all its sense annotations and 2) using a raw version of it, e.g. excluding all the annotations. The training was perform using a batch size of 32 and taking into account all the different tokens, 91349 tokens for the sense-annotated SemCor and 46797 for the runs of the raw version of SemCor–. In addition to the model trained with word2vec, we include a model, identified as LSTM, which is identical to LSTMEmbed but usese softmax as output layer as regular language models. As we can see, training with sense-annotated data outperforms the configurations based only on raw text. The LSTM model is competitive in the experiments considering only\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nModel Sense RG65 WS353 WSRel WSSim YP130 MEN SCWS SimLex\nword2vec - 0.288 -0.009 0.027 -0.009 0.490 0.166 0.270 0.218 X 0.397 0.158 0.125 0.201 0.268 0.305 0.325 0.225\nLSTM - 0.358 0.077 0.018 0.125 0.563 0.145 0.189 0.238 X 0.462 0.071 -0.020 0.128 0.532 0.108 0.292 0.247\nLSTMEmbed - 0.382 0.067 0.007 0.122 0.492 0.180 0.332 0.309 X 0.434 0.175 0.102 0.211 0.650 0.229 0.351 0.275\nLSTMEmbedSW - 0.508 0.081 -0.017 0.164 0.566 0.168 0.322 0.249 X 0.445 0.125 0.009 0.210 0.508 0.214 0.350 0.269\nTable 5: Spearman correlation for Word Similarity trained with SemCor.\nEmb Dim WS353\nword2vec - - 0.488 GloVe - - 0.557\nLSTMEmbed\nword2vec 50 0.573 word2ve + retro 50 0.569 GoogleNews 300 0.574\nSensEmbed 400 0.612\nTable 6: LSTMEmbed with different pretrained embeddings compared with word2vec and GloVe.\nraw text but unlike the other configurations, it does not seems to take advantage of the sense annotated data. We hypothesize that the increment of the vocabulary, with the corresponding increment of parameters to learn, is responsible for this behavior. Additionally, the learning time of this configuration was substantally larger than the time spent by LSTMEmbed, which is the reason why we did not include it in the experiments with large corpora. Similarly to the experiments on the BabelWiki corpus, LSTMEmbed provides good results, while the LSTMEmbedSW extension provides less performing results with the exception of the RG65 dataset."
    }, {
      "heading" : "6.5 Improving Representations via richer Embeddings",
      "text" : "In this section we study how we can inject semantic information through the set of pretrained embeddings. Our assumption is that richer embeddings should enhance the representation delivered by our model. In Table 6 we show the behavior of the LSTMEmbed architecture in the task of word similarity with the WS353 dataset. We perform our experiments in a reduced set, i.e. a 10% sample, of our chosen sense-annotated corpus. We compared four sets of pretrained embeddings. The first set is a 50-dimension embeddings, trained with word2vec Skip-gram with the\ndefault configuration. The second set consists of the same vectors, retrofitted with PPDB using the default configuration. The third set is the wellknown GoogleNews set of pretrained embeddings dataset8. Finally, we tested our model with the pretrained embeddings of SensEmbed9. As we can see, using richer pretrained embeddings improves the resulting representations given by our model. All the representations reach better results compared with the representations from word2vec and GloVe trained on the same corpus. The sense embeddings from SensEmbed, a priori the richest set of pretrained embeddings, achieved in fact the best performance."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this paper we introduced LSTMEmbed, a new model based on a bidirectional LSTM for learning embeddings of words and senses. We draw four main findings: First, we have shown that our approach is able to learn word embeddings which consistently outperform the representations given by classical algorithms like word2vec and GloVe on the same training data. Second, the introduction of an output layer which predicts pre-trained embeddings allow us to inject more semantic information while speed up the training. Third, we have shown that using semantically richer embeddings as enriches the resulting representations- And finally, our sense-based representations also outperform previous representations based on word2vec and GloVe, consistently with the results shown for the word-based representations.\n8https://goo.gl/p4RXac 9http://lcl.uniroma1.it/sensembed/\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Personalizing pagerank for word sense disambiguation",
      "author" : [ "Eneko Agirre", "Aitor Soroa." ],
      "venue" : "Proceedings of the 12th Conference of the EACL. Athens, Greece, pages 33–41.",
      "citeRegEx" : "Agirre and Soroa.,? 2009",
      "shortCiteRegEx" : "Agirre and Soroa.",
      "year" : 2009
    }, {
      "title" : "The Berkeley FrameNet Project",
      "author" : [ "Collin F. Baker", "Charles J. Fillmore", "John B. Lowe." ],
      "venue" : "Proceedings of the 36th ACL. Montreal, Quebec, Canada, pages 86–90.",
      "citeRegEx" : "Baker et al\\.,? 1998",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 1998
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Y. Bengio", "P. Simard", "P. Frasconi." ],
      "venue" : "IEEE Transactions on Neural Networks 5(2):157–166.",
      "citeRegEx" : "Bengio et al\\.,? 1994",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "Freebase: A collaboratively created graph database for structuring human knowledge",
      "author" : [ "Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor." ],
      "venue" : "Proceedings of the 2008 ACM SIGMOD International Conference on Management",
      "citeRegEx" : "Bollacker et al\\.,? 2008",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Multimodal Distributional Semantics",
      "author" : [ "Elia Bruni", "Nam Khanh Tran", "Marco Baroni." ],
      "venue" : "Journal of Artificial Intelligence Research (JAIR) 49(1):1–47.",
      "citeRegEx" : "Bruni et al\\.,? 2014",
      "shortCiteRegEx" : "Bruni et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning Phrase Representations using RNN Encoder– Decoder for Statistical Machine Translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Çalar Gülçehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Pro-",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "TransitionBased Dependency Parsing with Stack Long ShortTerm Memory",
      "author" : [ "Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith." ],
      "venue" : "Proceedings of the 53rd ACL. Beijing, China, pages 334–343.",
      "citeRegEx" : "Dyer et al\\.,? 2015",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Retrofitting Word Vectors to Semantic Lexicons",
      "author" : [ "Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith." ],
      "venue" : "Proceedings of the 2015 NAACL. Denver, Colorado, pages 1606–1615.",
      "citeRegEx" : "Faruqui et al\\.,? 2015",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2015
    }, {
      "title" : "Community evaluation and exchange of word vectors at wordvectors.org",
      "author" : [ "Manaal Faruqui", "Chris Dyer" ],
      "venue" : "In Proceedings of the 52nd ACL. Association for Computational Linguistics,",
      "citeRegEx" : "Faruqui and Dyer.,? \\Q2014\\E",
      "shortCiteRegEx" : "Faruqui and Dyer.",
      "year" : 2014
    }, {
      "title" : "Placing Search in Context: The Concept Revisited",
      "author" : [ "Lev Finkelstein", "Gabrilovich Evgeniy", "Matias Yossi", "Rivlin Ehud", "Solan Zach", "Wolfman Gadi", "Ruppin Eytan." ],
      "venue" : "ACM Transactions on Information Systems 20(1):116–131.",
      "citeRegEx" : "Finkelstein et al\\.,? 2002",
      "shortCiteRegEx" : "Finkelstein et al\\.",
      "year" : 2002
    }, {
      "title" : "PPDB: The Paraphrase Database",
      "author" : [ "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 2013 NAACL. Atlanta, Georgia, pages 758–764.",
      "citeRegEx" : "Ganitkevitch et al\\.,? 2013",
      "shortCiteRegEx" : "Ganitkevitch et al\\.",
      "year" : 2013
    }, {
      "title" : "SimVerb-3500: A LargeScale Evaluation Set of Verb Similarity",
      "author" : [ "Daniela Gerz", "Ivan Vulić", "Felix Hill", "Roi Reichart", "Anna Korhonen." ],
      "venue" : "Proceedings of the 2016 EMNLP. Austin, Texas, pages 2173–2182.",
      "citeRegEx" : "Gerz et al\\.,? 2016",
      "shortCiteRegEx" : "Gerz et al\\.",
      "year" : 2016
    }, {
      "title" : "Framewise phoneme classification with bidirectional lstm and other neural network architectures",
      "author" : [ "Alex Graves", "Jürgen Schmidhuber." ],
      "venue" : "Neural Networks 18(5):602–610.",
      "citeRegEx" : "Graves and Schmidhuber.,? 2005",
      "shortCiteRegEx" : "Graves and Schmidhuber.",
      "year" : 2005
    }, {
      "title" : "Learning to Understand Phrases by Embedding the Dictionary",
      "author" : [ "Felix Hill", "KyungHyun Cho", "Anna Korhonen", "Yoshua Bengio." ],
      "venue" : "Transactions of the ACL 4:17–30.",
      "citeRegEx" : "Hill et al\\.,? 2016",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2016
    }, {
      "title" : "SimLex-999: Evaluating semantic models with (genuine) similarity estimation",
      "author" : [ "Felix Hill", "Roi Reichart", "Anna Korhonen." ],
      "venue" : "Computational Linguistics 41(4):665–695.",
      "citeRegEx" : "Hill et al\\.,? 2015",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2015
    }, {
      "title" : "Untersuchungen zu dynamischen neuronalen Netzen",
      "author" : [ "Sepp Hochreiter." ],
      "venue" : "Ph.D. thesis, diploma thesis, institut für informatik, lehrstuhl prof. brauer, technische universität münchen.",
      "citeRegEx" : "Hochreiter.,? 1991",
      "shortCiteRegEx" : "Hochreiter.",
      "year" : 1991
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "SensEmbed: Learning Sense Embeddings for Word and Relational Similarity",
      "author" : [ "Ignacio Iacobacci", "Mohammad Taher Pilehvar", "Roberto Navigli." ],
      "venue" : "Proceedings of the 53rd ACL. Beijing, China, volume 1, pages 95–105.",
      "citeRegEx" : "Iacobacci et al\\.,? 2015",
      "shortCiteRegEx" : "Iacobacci et al\\.",
      "year" : 2015
    }, {
      "title" : "On Using Very Large Target Vocabulary for Neural Machine Translation",
      "author" : [ "Sébastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 53rd ACL. Beijing, China, volume 1, pages 1–10.",
      "citeRegEx" : "Jean et al\\.,? 2015",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Semeval-2012 task 2: Measuring degrees of relational similarity",
      "author" : [ "David A. Jurgens", "Peter D. Turney", "Saif M. Mohammad", "Keith J. Holyoak." ],
      "venue" : "Proceedings of SemEval-2012. pages 356–364.",
      "citeRegEx" : "Jurgens et al\\.,? 2012",
      "shortCiteRegEx" : "Jurgens et al\\.",
      "year" : 2012
    }, {
      "title" : "Word Sense Disambiguation using a Bidirectional LSTM",
      "author" : [ "Mikael Kågebäck", "Hans Salomonsson." ],
      "venue" : "5th Workshop on Cognitive Aspects of the Lexicon (CogALex). Osaka, Japan.",
      "citeRegEx" : "Kågebäck and Salomonsson.,? 2016",
      "shortCiteRegEx" : "Kågebäck and Salomonsson.",
      "year" : 2016
    }, {
      "title" : "A large-scale classification of english verbs",
      "author" : [ "Karin Kipper", "Anna Korhonen", "Neville Ryant", "Martha Palmer." ],
      "venue" : "Language Resources and Evaluation 42(1):21–40.",
      "citeRegEx" : "Kipper et al\\.,? 2008",
      "shortCiteRegEx" : "Kipper et al\\.",
      "year" : 2008
    }, {
      "title" : "A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge",
      "author" : [ "Thomas K. Landauer", "Susan T. Dumais." ],
      "venue" : "Psychological review 104(2):211.",
      "citeRegEx" : "Landauer and Dumais.,? 1997",
      "shortCiteRegEx" : "Landauer and Dumais.",
      "year" : 1997
    }, {
      "title" : "Best-Worst Scaling: A Model for the Largest Difference Judgments",
      "author" : [ "Jordan Louviere." ],
      "venue" : "Working paper, University of Alberta.",
      "citeRegEx" : "Louviere.,? 1991",
      "shortCiteRegEx" : "Louviere.",
      "year" : 1991
    }, {
      "title" : "Embedding words and senses together via joint knowledgeenhanced training",
      "author" : [ "Massimiliano Mancini", "Jose Camacho-Collados", "Ignacio Iacobacci", "Roberto Navigli." ],
      "venue" : "CoRR abs/1612.02703.",
      "citeRegEx" : "Mancini et al\\.,? 2016",
      "shortCiteRegEx" : "Mancini et al\\.",
      "year" : 2016
    }, {
      "title" : "context2vec: Learning generic context embedding with bidirectional LSTM",
      "author" : [ "Oren Melamud", "Jacob Goldberger", "Ido Dagan." ],
      "venue" : "Proceedings of the 20th CONLL. Berlin, Germany.",
      "citeRegEx" : "Melamud et al\\.,? 2016",
      "shortCiteRegEx" : "Melamud et al\\.",
      "year" : 2016
    }, {
      "title" : "Investigation of Recurrent-neuralnetwork Architectures and Learning Methods for Spoken Language Understanding",
      "author" : [ "Grégoire Mesnil", "Xiaodong He", "Li Deng", "Yoshua Bengio." ],
      "venue" : "Proceedings of Interspeech 2013. Lyon, France, pages 3771–3775.",
      "citeRegEx" : "Mesnil et al\\.,? 2013",
      "shortCiteRegEx" : "Mesnil et al\\.",
      "year" : 2013
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur." ],
      "venue" : "Proceedings of Interspeech 2010. Makuhari, Japan, volume 2, page 3.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Exploiting Similarities among Languages for Machine Translation",
      "author" : [ "Tomas Mikolov", "Quoc V. Le", "Ilya Sutskever." ],
      "venue" : "CoRR abs/1309.4168.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Context dependent recurrent neural network language model",
      "author" : [ "Tomas Mikolov", "Geoffrey Zweig." ],
      "venue" : "IEEE Workshop on Spoken Language Technology. Miami, Florida, pages 234–239.",
      "citeRegEx" : "Mikolov and Zweig.,? 2012",
      "shortCiteRegEx" : "Mikolov and Zweig.",
      "year" : 2012
    }, {
      "title" : "WordNet: A Lexical Database for English",
      "author" : [ "George A. Miller." ],
      "venue" : "Communications of the ACM 38(11):39–41.",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Using a Semantic Concordance for Sense Identification",
      "author" : [ "George A. Miller", "Martin Chodorow", "Shari Landes", "Claudia Leacock", "Robert G Thomas." ],
      "venue" : "Proceedings of the Workshop on HLT . Plainsboro, New Jersey, pages 240–243.",
      "citeRegEx" : "Miller et al\\.,? 1994",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 1994
    }, {
      "title" : "Entity Linking meets Word Sense Disambiguation: a Unified Approach",
      "author" : [ "Andrea Moro", "Alessandro Raganato", "Roberto Navigli." ],
      "venue" : "Transactions of the ACL 2:231–244.",
      "citeRegEx" : "Moro et al\\.,? 2014",
      "shortCiteRegEx" : "Moro et al\\.",
      "year" : 2014
    }, {
      "title" : "BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network",
      "author" : [ "Roberto Navigli", "Simone Paolo Ponzetto." ],
      "venue" : "Artificial Intelligence 193:217– 250.",
      "citeRegEx" : "Navigli and Ponzetto.,? 2012",
      "shortCiteRegEx" : "Navigli and Ponzetto.",
      "year" : 2012
    }, {
      "title" : "The university of south florida free association, rhyme, and word fragment norms",
      "author" : [ "Douglas L. Nelson", "Cathy L. McEvoy", "Thomas A. Schreiber." ],
      "venue" : "Behavior Research Methods, Instruments, & Computers 36(3):402–407.",
      "citeRegEx" : "Nelson et al\\.,? 2004",
      "shortCiteRegEx" : "Nelson et al\\.",
      "year" : 2004
    }, {
      "title" : "GloVe: Global Vectors for Word Representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2014 EMNLP. Doha, Qatar, pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Automatic Construction and Evaluation of a Large Semantically Enriched Wikipedia",
      "author" : [ "Alessandro Raganato", "Claudio Delli Bovi", "Roberto Navigli." ],
      "venue" : "In",
      "citeRegEx" : "Raganato et al\\.,? 2016",
      "shortCiteRegEx" : "Raganato et al\\.",
      "year" : 2016
    }, {
      "title" : "Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language",
      "author" : [ "Philip Resnik." ],
      "venue" : "Journal of Artificial Intelligence Research (JAIR) 11:95–130.",
      "citeRegEx" : "Resnik.,? 1999",
      "shortCiteRegEx" : "Resnik.",
      "year" : 1999
    }, {
      "title" : "Autoextend: Extending word embeddings to embeddings for synsets and lexemes",
      "author" : [ "Sascha Rothe", "Hinrich Schütze." ],
      "venue" : "Proceedings of the 53rd ACL. Beijing, China, volume 1, pages 1793–1803.",
      "citeRegEx" : "Rothe and Schütze.,? 2015",
      "shortCiteRegEx" : "Rothe and Schütze.",
      "year" : 2015
    }, {
      "title" : "Contextual Correlates of Synonymy",
      "author" : [ "Herbert Rubenstein", "John B. Goodenough." ],
      "venue" : "Communications of the ACM 8(10):627–633.",
      "citeRegEx" : "Rubenstein and Goodenough.,? 1965",
      "shortCiteRegEx" : "Rubenstein and Goodenough.",
      "year" : 1965
    }, {
      "title" : "Automatic identification and disambiguation of concepts and named entities in the multilingual wikipedia",
      "author" : [ "Federico Scozzafava", "Alessandro Raganato", "Andrea Moro", "Roberto Navigli." ],
      "venue" : "Congress of the Italian Association for Artificial Intelligence",
      "citeRegEx" : "Scozzafava et al\\.,? 2015",
      "shortCiteRegEx" : "Scozzafava et al\\.",
      "year" : 2015
    }, {
      "title" : "LSTM Neural Networks for Language Modeling",
      "author" : [ "Martin Sundermeyer", "Ralf Schlüter", "Hermann Ney." ],
      "venue" : "Proceedings of Interspeech 2012. Portland, Oregon, pages 194–197.",
      "citeRegEx" : "Sundermeyer et al\\.,? 2012",
      "shortCiteRegEx" : "Sundermeyer et al\\.",
      "year" : 2012
    }, {
      "title" : "Generating text with recurrent neural networks",
      "author" : [ "Ilya Sutskever", "James Martens", "Geoffrey E Hinton." ],
      "venue" : "Proceedings of the 28th ICML. Bellevue, Washington, pages 1017–1024.",
      "citeRegEx" : "Sutskever et al\\.,? 2011",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2011
    }, {
      "title" : "Theano: A Python framework for fast computation of mathematical expressions",
      "author" : [ "Theano Development Team." ],
      "venue" : "arXiv e-prints abs/1605.02688.",
      "citeRegEx" : "Team.,? 2016",
      "shortCiteRegEx" : "Team.",
      "year" : 2016
    }, {
      "title" : "Mining the web for synonyms: Pmi-ir versus lsa on toefl",
      "author" : [ "Peter D. Turney." ],
      "venue" : "Proceedings of the 12th ECML. Freiburg, Germany, pages 491–502.",
      "citeRegEx" : "Turney.,? 2001",
      "shortCiteRegEx" : "Turney.",
      "year" : 2001
    }, {
      "title" : "Measuring Semantic Similarity in the Taxonomy of WordNet",
      "author" : [ "Dongqiang Yang", "David M.W. Powers." ],
      "venue" : "Proceedings of the 28th ACSC. Newcastle, NSW, Australia, volume 38, pages 315–322.",
      "citeRegEx" : "Yang and Powers.,? 2005",
      "shortCiteRegEx" : "Yang and Powers.",
      "year" : 2005
    }, {
      "title" : "Improving Lexical Embeddings with Semantic Knowledge",
      "author" : [ "Mo Yu", "Mark Dredze." ],
      "venue" : "Proceedings of the 52nd ACL. Baltimore, Maryland, volume 2, pages 545–550.",
      "citeRegEx" : "Yu and Dredze.,? 2014",
      "shortCiteRegEx" : "Yu and Dredze.",
      "year" : 2014
    }, {
      "title" : "Semi-supervised word sense disambiguation with neural models",
      "author" : [ "Dayu Yuan", "Julian Richardson", "Ryan Doherty", "Colin Evans", "Eric Altendorf." ],
      "venue" : "Proceedings of COLING 2016. Osaka, Japan, pages 1374–1385.",
      "citeRegEx" : "Yuan et al\\.,? 2016",
      "shortCiteRegEx" : "Yuan et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1409.2329 .",
      "citeRegEx" : "Zaremba et al\\.,? 2014",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    }, {
      "title" : "Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies",
      "author" : [ "Barret Zoph", "Ashish Vaswani", "Jonathan May", "Kevin Knight." ],
      "venue" : "Proceedings of the 2016 NAACL. San Diego, California, pages 1217–1222.",
      "citeRegEx" : "Zoph et al\\.,? 2016",
      "shortCiteRegEx" : "Zoph et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "RNNs, and particularly LSTMs have been working extremely well on a large variety of problems in NLP, such as machine translation (Cho et al., 2014), lexical substitution (Melamud et al.",
      "startOffset" : 129,
      "endOffset" : 147
    }, {
      "referenceID" : 25,
      "context" : ", 2014), lexical substitution (Melamud et al., 2016), word sense disambiguation, (Kågebäck and Salomonsson, 2016; Yuan et al.",
      "startOffset" : 30,
      "endOffset" : 52
    }, {
      "referenceID" : 20,
      "context" : ", 2016), word sense disambiguation, (Kågebäck and Salomonsson, 2016; Yuan et al., 2016), syntactic parsing (Dyer et al.",
      "startOffset" : 36,
      "endOffset" : 87
    }, {
      "referenceID" : 47,
      "context" : ", 2016), word sense disambiguation, (Kågebäck and Salomonsson, 2016; Yuan et al., 2016), syntactic parsing (Dyer et al.",
      "startOffset" : 36,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : ", 2016), syntactic parsing (Dyer et al., 2015), among others.",
      "startOffset" : 27,
      "endOffset" : 46
    }, {
      "referenceID" : 28,
      "context" : "Recent approaches, such as word2vec (Mikolov et al., 2013), and GloVe (Pennington et al.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 35,
      "context" : ", 2013), and GloVe (Pennington et al., 2014) are efficient for learning embeddings, but they do not take into account word ordering.",
      "startOffset" : 19,
      "endOffset" : 44
    }, {
      "referenceID" : 27,
      "context" : "Vice versa, RNNs take order into account but they are not competitive in terms of speed or quality (Mikolov et al., 2010; Mikolov and Zweig, 2012; Mesnil et al., 2013).",
      "startOffset" : 99,
      "endOffset" : 167
    }, {
      "referenceID" : 29,
      "context" : "Vice versa, RNNs take order into account but they are not competitive in terms of speed or quality (Mikolov et al., 2010; Mikolov and Zweig, 2012; Mesnil et al., 2013).",
      "startOffset" : 99,
      "endOffset" : 167
    }, {
      "referenceID" : 26,
      "context" : "Vice versa, RNNs take order into account but they are not competitive in terms of speed or quality (Mikolov et al., 2010; Mikolov and Zweig, 2012; Mesnil et al., 2013).",
      "startOffset" : 99,
      "endOffset" : 167
    }, {
      "referenceID" : 13,
      "context" : "Introduced by Hochreiter and Schmidhuber (1997), LSTMs are a special kind of RNN capable of learning long-term dependencies on problems related with sequential data.",
      "startOffset" : 14,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : "The recently celebrated LSTMs appear to be perfect for learning sequence representations, like phrases (Hill et al., 2016) and contexts (Melamud et al.",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 25,
      "context" : ", 2016) and contexts (Melamud et al., 2016).",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "The recently celebrated LSTMs appear to be perfect for learning sequence representations, like phrases (Hill et al., 2016) and contexts (Melamud et al., 2016). However, when dealing with large vocabularies, LSTMs involve time-intensive matrix-matrix multiplications, making them prohibitively expensive. This issue was addressed by Jean et al. (2015), who proposed an approximate training algorithm based on sampling, and Zoph et al.",
      "startOffset" : 104,
      "endOffset" : 351
    }, {
      "referenceID" : 13,
      "context" : "The recently celebrated LSTMs appear to be perfect for learning sequence representations, like phrases (Hill et al., 2016) and contexts (Melamud et al., 2016). However, when dealing with large vocabularies, LSTMs involve time-intensive matrix-matrix multiplications, making them prohibitively expensive. This issue was addressed by Jean et al. (2015), who proposed an approximate training algorithm based on sampling, and Zoph et al. (2016), who introduced an adaptation for GPUs of the noise-contrastive estimation algorithm, a method that avoids repeated summations by training the model to correctly separate generated noise samples from words observed in the training data.",
      "startOffset" : 104,
      "endOffset" : 441
    }, {
      "referenceID" : 10,
      "context" : "Several approaches were proposed to address this problem: Yu and Dredze (2014) presented an alternative way to train word embeddings by using, in addition to common features, words having some relation in a semantic resource, like PPDB1 (Ganitkevitch et al., 2013) or WordNet (Miller, 1995).",
      "startOffset" : 237,
      "endOffset" : 264
    }, {
      "referenceID" : 30,
      "context" : ", 2013) or WordNet (Miller, 1995).",
      "startOffset" : 19,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "The word types were extracted from diverse semantic resources such as PPDB, WordNet and FrameNet (Baker et al., 1998).",
      "startOffset" : 97,
      "endOffset" : 117
    }, {
      "referenceID" : 23,
      "context" : "Recent advances (Mikolov et al., 2013, word2vec) showed that word representations learnt with a neural network trained with raw text show relationships such as the male/female relationship, e.g. the induced vector representations of king −man + woman resulted very close to the induced vector of queen. GloVe, an alternative approach trained on aggregated global word-word co-occurrences, reached similar results. While these embeddings are surprisingly good for monosemous words, they fail to represent properly the non-dominant senses of words. For instance, the representations of bar and pub should be similar, as well as those of bar and stick, but having similar representations of pub and stick is undesired. Several approaches were proposed to address this problem: Yu and Dredze (2014) presented an alternative way to train word embeddings by using, in addition to common features, words having some relation in a semantic resource, like PPDB1 (Ganitkevitch et al.",
      "startOffset" : 17,
      "endOffset" : 795
    }, {
      "referenceID" : 6,
      "context" : "Faruqui et al. (2015) presented a technique applicable to pre-processed embeddings, in which vectors are updated (i.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 1,
      "context" : "The word types were extracted from diverse semantic resources such as PPDB, WordNet and FrameNet (Baker et al., 1998). Finally, Melamud et al. (2016) introduced context2vec, a model based on a bidirectional LSTM for learning word embeddings.",
      "startOffset" : 98,
      "endOffset" : 150
    }, {
      "referenceID" : 33,
      "context" : "org/#/download on a predefined sense inventory such as WordNet, Wikipedia, BabelNet2 (Navigli and Ponzetto, 2012) or Freebase (Bollacker et al.",
      "startOffset" : 85,
      "endOffset" : 113
    }, {
      "referenceID" : 3,
      "context" : "org/#/download on a predefined sense inventory such as WordNet, Wikipedia, BabelNet2 (Navigli and Ponzetto, 2012) or Freebase (Bollacker et al., 2008), and (2) unsupervised, which rely on contextual information from parallel corpora to discriminate word senses among different occurrences.",
      "startOffset" : 126,
      "endOffset" : 150
    }, {
      "referenceID" : 17,
      "context" : "From the first group we can highlight successful approaches: SENSEMBED (Iacobacci et al., 2015) used Babelfy3, a state-of-the-art tool for Word Sense Disambiguation and Entity linking, to build a sense-annotated corpus which was in turn used to train a vector space model for word senses with word2vec.",
      "startOffset" : 71,
      "endOffset" : 95
    }, {
      "referenceID" : 38,
      "context" : "AutoExtend (Rothe and Schütze, 2015) is a system that learns embeddings for lexemes, senses and synsets from WordNet in a shared space.",
      "startOffset" : 11,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : "org/#/download on a predefined sense inventory such as WordNet, Wikipedia, BabelNet2 (Navigli and Ponzetto, 2012) or Freebase (Bollacker et al., 2008), and (2) unsupervised, which rely on contextual information from parallel corpora to discriminate word senses among different occurrences. From the first group we can highlight successful approaches: SENSEMBED (Iacobacci et al., 2015) used Babelfy3, a state-of-the-art tool for Word Sense Disambiguation and Entity linking, to build a sense-annotated corpus which was in turn used to train a vector space model for word senses with word2vec. SENSEMBED exploits the structured knowledge of BabelNet’s sense inventory along with the distributional information gathered from text corpora. An important limitation of this approach was the inability to train both word and sense embeddings. The only word embeddings that the model was able to learn were the representations of words which were not annotated by the disambiguation tool. Those representations were of poor quality due to the fact that they are learnt from the occurrences in a ambiguous or unclear context, else the disambiguation should be able to annotate them. In addition, owing to the fact that it is based on word2vec, this approach suffers from the lack of word ordering. AutoExtend (Rothe and Schütze, 2015) is a system that learns embeddings for lexemes, senses and synsets from WordNet in a shared space. The synset/lexeme embeddings live in the same vector space as the word embeddings, given the constraint that words are sums of their lexemes and synsets are sums of their lexemes. AutoExtend is based on an autoencoder, a network that mimics the input and output vectors. Finally, Mancini et al. (2016) presented SW2V, an extension of word2vec which learns word and sense embeddings jointly and represents them in a unified vector space.",
      "startOffset" : 127,
      "endOffset" : 1728
    }, {
      "referenceID" : 15,
      "context" : "In practice this architecture has several problems in learning long-term dependencies (Hochreiter, 1991; Bengio et al., 1994).",
      "startOffset" : 86,
      "endOffset" : 125
    }, {
      "referenceID" : 2,
      "context" : "In practice this architecture has several problems in learning long-term dependencies (Hochreiter, 1991; Bengio et al., 1994).",
      "startOffset" : 86,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "To cope the latter, we adopt the Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) architecture which augments the RNN with a series of connections called gates: three gates are added namely the input (it), the forget (ft) and the output (ot) gates.",
      "startOffset" : 63,
      "endOffset" : 97
    }, {
      "referenceID" : 27,
      "context" : "LSTMEmbed is inspired by the architecture or recurrent neural network language models (RNNLM) (Mikolov et al., 2010; Sutskever et al., 2011; Sundermeyer et al., 2012; Zaremba et al., 2014).",
      "startOffset" : 94,
      "endOffset" : 188
    }, {
      "referenceID" : 42,
      "context" : "LSTMEmbed is inspired by the architecture or recurrent neural network language models (RNNLM) (Mikolov et al., 2010; Sutskever et al., 2011; Sundermeyer et al., 2012; Zaremba et al., 2014).",
      "startOffset" : 94,
      "endOffset" : 188
    }, {
      "referenceID" : 41,
      "context" : "LSTMEmbed is inspired by the architecture or recurrent neural network language models (RNNLM) (Mikolov et al., 2010; Sutskever et al., 2011; Sundermeyer et al., 2012; Zaremba et al., 2014).",
      "startOffset" : 94,
      "endOffset" : 188
    }, {
      "referenceID" : 48,
      "context" : "LSTMEmbed is inspired by the architecture or recurrent neural network language models (RNNLM) (Mikolov et al., 2010; Sutskever et al., 2011; Sundermeyer et al., 2012; Zaremba et al., 2014).",
      "startOffset" : 94,
      "endOffset" : 188
    }, {
      "referenceID" : 38,
      "context" : "As was claimed by Rubenstein and Goodenough (1965) the proportion of words common to the contexts of two words A and B is a function of the degree in which A and B are similar in meaning.",
      "startOffset" : 18,
      "endOffset" : 51
    }, {
      "referenceID" : 37,
      "context" : "While measuring semantic similarity between words we will refer to the conventional approach of Resnik (1999) where the similarity bewteen two words is given by the similarity of their closest senses:",
      "startOffset" : 96,
      "endOffset" : 110
    }, {
      "referenceID" : 17,
      "context" : "For measuring this similarity we follow Iacobacci et al. (2015):",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 39,
      "context" : "We evaluate LSTMEmbed on standard word similarity and relatedness datasets: the RG65 (Rubenstein and Goodenough, 1965) and the WordSim-353 (Finkelstein et al.",
      "startOffset" : 85,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "We include two large datasets: SimLex999 (Hill et al., 2015), which puts especially focus on representing antonyms as completely unrelated words, and the MEN dataset introduced by Bruni et al.",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : "Two datasets especially made for analyzing verb similarity are also included in our experiments: YP130, created by Yang and Powers (2005), based on the taxonomy of WordNet, and SimVerb3500 (Gerz et al., 2016) a newer and much larger dataset extracted from the USF norms data set4 (Nelson et al.",
      "startOffset" : 189,
      "endOffset" : 208
    }, {
      "referenceID" : 34,
      "context" : ", 2016) a newer and much larger dataset extracted from the USF norms data set4 (Nelson et al., 2004) and VerbNet5 (Kipper et al.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 21,
      "context" : ", 2004) and VerbNet5 (Kipper et al., 2008).",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "We also include the proposed split made by Agirre and Soroa (2009) who divided WS353 in pairs that measured the degree of similarity (WSSim) and pairs that measured the degree of relatedness (WSRel).",
      "startOffset" : 43,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "We also include the proposed split made by Agirre and Soroa (2009) who divided WS353 in pairs that measured the degree of similarity (WSSim) and pairs that measured the degree of relatedness (WSRel). We include two large datasets: SimLex999 (Hill et al., 2015), which puts especially focus on representing antonyms as completely unrelated words, and the MEN dataset introduced by Bruni et al. (2014) composed by 3000 pairs.",
      "startOffset" : 43,
      "endOffset" : 400
    }, {
      "referenceID" : 0,
      "context" : "We also include the proposed split made by Agirre and Soroa (2009) who divided WS353 in pairs that measured the degree of similarity (WSSim) and pairs that measured the degree of relatedness (WSRel). We include two large datasets: SimLex999 (Hill et al., 2015), which puts especially focus on representing antonyms as completely unrelated words, and the MEN dataset introduced by Bruni et al. (2014) composed by 3000 pairs. Two datasets especially made for analyzing verb similarity are also included in our experiments: YP130, created by Yang and Powers (2005), based on the taxonomy of WordNet, and SimVerb3500 (Gerz et al.",
      "startOffset" : 43,
      "endOffset" : 562
    }, {
      "referenceID" : 44,
      "context" : "The second one, introduced by Turney (2001), provide a set of questions extracted, in this case, from the synonym questions of the ESL (English as a Second Language).",
      "startOffset" : 30,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : "For word analogy we experimented with the SemEval-2012 task on Measuring Degrees of Relational Similarity (Jurgens et al., 2012).",
      "startOffset" : 106,
      "endOffset" : 128
    }, {
      "referenceID" : 23,
      "context" : "The task includes two different evaluations, one based on Spearman correlation and a second one based on MaxDiff score (Louviere, 1991).",
      "startOffset" : 119,
      "endOffset" : 135
    }, {
      "referenceID" : 31,
      "context" : "We compare several sense-annotated corpora: SemCor (Miller et al., 1994) is a manually sense-tagged corpus created by the WordNet project team at Princeton University.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 32,
      "context" : "(2015, BabelWiki) presented a sense-annotated corpus of the English and Italian Wikipedia, annotated automatically with Babelfy (Moro et al., 2014), a tool for Word Sense Disambiguation on the BabelNet semantic network.",
      "startOffset" : 128,
      "endOffset" : 147
    } ],
    "year" : 2017,
    "abstractText" : "The Long Short-Term Memory (LSTM) architecture for recurrent neural networks has become the state-of-the-art model for a range of different Natural Language Processing (NLP) tasks, especially in language modeling and sequence to sequence learning. In this paper we leverage a bidirectional LSTM while at the same time taking advantage of other semantic resources in order to create a vector space model for words and senses that outperforms most popular algorithms for learning embeddings. We evaluate our approach on the most well-known benchmarks on vector space representations.",
    "creator" : "LaTeX with hyperref package"
  }
}