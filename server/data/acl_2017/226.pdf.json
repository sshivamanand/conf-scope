{
  "name" : "226.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\nThe paper presents a procedure of building an evaluation dataset for validation of compositional distributional semantics models estimated for languages other than English. The procedure generally builds on steps designed to assemble the SICK corpus which contains pairs of English sentences annotated for semantic relatedness and entailment, as we aim at building a comparable dataset. However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for an investigated language and the demand for language-specific transformation rules. The designed procedure is verified on Polish, a fusional language with a relatively free word order, and contributes to building a Polish evaluation dataset. The resource consists of 10K sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for evaluation of compositional distributional semantics models of Polish."
    }, {
      "heading" : "1 Introduction and related work",
      "text" : "Can you imagine a straightforward answer to the question How to automatically analyse semantics of a natural language and to represent the meaning of phrases (or even sentences) in this language in an accessible way? A few years ago most of us would probably answer I don’t know! And now, in the era of high-speed and high-performance computing, everybody seems to know it – with distributional semantics models."
    }, {
      "heading" : "1.1 Distributional semantics",
      "text" : "The basic idea of distributional semantics, i.e. determining the meaning of a word based on its co-occurrence with other words, is derived from the empiricists – Harris (1954) and Firth (1957). John R. Firth drew attention to the contextdependent nature of meaning especially with his famous maxim “You shall know a word by the company it keeps” (Firth, 1957, p. 11).\nNowadays, distributional semantics models are estimated with various methods, e.g. word embedding techniques (Bengio et al., 2003, 2006; Mikolov et al., 2013). To ascertain the purport of a word, e.g. bath, you can use the context of other words that surround it. If we assume that the meaning of this word expressed by its lexical context is associated with a distributional vector, the distance between distributional vectors of two semantically similar words, e.g bath and shower, should be smaller than between vectors representing semantically distinct words, e.g. bath and tree."
    }, {
      "heading" : "1.2 Compositional distributional semantics",
      "text" : "Based on empirical observations that distributional vectors encode certain aspects of the word meaning, it is expected that similar aspects of the meaning of phrases and sentences can also be represented with vectors obtained via composition of distributional word vectors. The idea of semantic composition is not new. It is well known as principle of compositionality:1 “The meaning of a compound expression is a function of the meaning of its parts and of the way they are syntactically combined.” (Janssen, 2012, p. 19).\nModelling the meaning of textual units larger than words using compositional and distributional information is the main subject of compositional\n1As the principle of compositionality is attributed to Gottlob Frege, it is often called Frege’s principle.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ndistributional semantics (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012, to name a few studies). Fundamental principles of compositional distributional semantics, henceforth referred to as CDS, are mainly propagated with papers written on the topic. Apart from papers it was the SemEval-2014 Shared Task 1 (Marelli et al., 2014) which essentially contributed to the expansion of CDS and increased an interest in this domain. The goal of the task was to evaluate CDS models of English in terms of semantic relatedness and entailment on proper sentences from the SICK corpus."
    }, {
      "heading" : "1.3 The SICK corpus",
      "text" : "The SICK corpus (Bentivogli et al., 2014) consists of 10K pairs of English sentences containing multiple lexical, syntactic, and semantic phenomena. It builds on two external data sources – the 8K ImageFlickr dataset (Rashtchian et al., 2010) and SemEval-2012 Semantic Textual Similarity dataset (Agirre et al., 2012). Each sentence pair is human-annotated for relatedness in meaning and entailment.\nThe relatedness score corresponds to the degree of semantic relatedness between two sentences and is calculated as the average of ten human ratings collected for this sentence pair on the 5-point Likert scale. This score indicates the extent to which the meanings of two sentences are related.\nThe entailment relation between two sentences, in turn, is labelled with entailment, contradiction, or neutral. According to the SICK guidelines, the label assigned by the majority of human annotators is selected as the valid entailment label."
    }, {
      "heading" : "1.4 Motivation and organisation of the paper",
      "text" : "Studying approaches to various natural language processing (henceforth NLP) problems, we have observed that the availability of language resources (e.g. training or testing data) stimulates development of NLP tools and estimation of NLP models. Hence, we aim at building datasets for evaluation of CDS models in languages other than English, which are often under-resourced. We therefore assume that availability of test data will encourage development of CDS models in these languages.\nWe start with a high-quality dataset for Polish, which is a completely different language than English in at least two dimensions. First, it is\na rather under-resourced language in contrast to the resource-rich English. Second, it is a fusional language with a relatively free word order in contrast to the isolated English with a relatively fixed word order.\nThe procedure of building an evaluation dataset for validating compositional distributional semantics models of Polish generally builds on steps designed to assemble the SICK corpus (described in Section 1.3), as we aim at building an evaluation dataset which is comparable to the SICK corpus. However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for Polish (see Section 2.1) and demand for Polishspecific transformation rules (see Section 2.2). Furthermore, rules of arranging sentences into pairs (see Section 2.3) are defined anew taking into account the characteristic of data and bidirectional entailment annotations, since an entailment relation between two sentences must not be symmetric. Even if our assumptions of annotating sentence pairs coincide with the SICK principles to a certain extent (see Section 3.1), the annotation process differs from the SICK procedure, in particular by introducing an element of human verification of correctness of automatically transformed sentences (see Section 3.2) and some additional post-corrections (see Section 3.3). Finally, a summary of the dataset is provided in Section 4.1 and the dataset evaluation is given in Section 4.2."
    }, {
      "heading" : "2 Procedure of collecting data",
      "text" : ""
    }, {
      "heading" : "2.1 Selection and description of images",
      "text" : "The first step of building the SICK corpus consisted in the random selection of English sentence pairs from existing datasets (Rashtchian et al., 2010; Agirre et al., 2012). Since we are not aware of accessibility of analogous resources for Polish, we have to select images first and then describe the selected images.\nImages are selected from the 8K ImageFlickr dataset (Rashtchian et al., 2010). At first we wanted to take only these images the descriptions of which were selected for the SICK corpus. However, a cursory check shows that these images are quite homogeneous, with a predominant number of dogs depictions. Therefore, we independently extract 1K images and split them into 46 thematic groups (e.g. children, musical instruments, motor-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nbikes, football, dogs). Numbers of images within individual thematic groups vary from 6 images in the volleyball and telephoning groups to 94 images in the various people group. The second largest groups are children and dogs with 50 images each.\nThe chosen images are given to two authors who independently of each other formulate their descriptions based on a short instruction. The authors are instructed to write one single sentence (with a sentence predicate) describing the action on a displayed image. They should not describe an imaginable context or an interpretation of what may lie behind the scene on the picture. If some details on the picture are not obvious, they should not be described too. Furthermore, the authors should avoid multiword expressions, such as idioms, metaphors, and named entities, as those are not compositional linguistic phenomena. Finally, descriptions should contain Polish diacritics and proper punctuation."
    }, {
      "heading" : "2.2 Transformation of descriptions",
      "text" : "The second step of building the SICK corpus consisted in pre-processing extracted sentences, i.e. normalisation and expansion (Bentivogli et al., 2014, p. 3–4). Since the authors of Polish descriptions are asked to follow the guidelines (presented in Section 2.1), the normalisation step is not essential for our data. The expansion step, in turn, is implemented and the sentences provided by the authors are lexically and syntactically transformed in order to obtain derivative sentences with similar, contrastive, or neutral meanings. The following transformations are implemented:\n1. dropping conjunction concerns sentences with coordinated predicates sharing a subject, e.g. Rowerzysta odpoczywa i obserwuje morze. (Eng. ‘A cyclist is resting and watching the sea.’). The finite form of one of the coordinated predicates is transformed into:\n• an active adjectival participle, e.g. Odpoczywający rowerzysta obserwuje morze. (Eng. ‘A resting cyclist is watching the sea.’) or Obserwujący morze rowerzysta odpoczywa. (Eng. ‘A cyclist, who is watching the sea, is resting.’), • a contemporary adverbial participle,\ne.g. Rowerzysta, odpoczywając, obserwuje morze. (Eng. ‘A cyclist is watching the sea, while resting.’) or Rowerzysta odpoczywa, obserwując morze.\n(Eng. ‘A cyclist is resting, while watching the sea.’).\n2. removing conjunct in adjuncts, i.e. the deletion of one of coordinated elements of an adjunct, e.g. Mały, ale zwinny kot miauczy. (Eng. ‘A small but agile cat miaows.’) can be changed into either Mały kot miauczy. (Eng. ‘A small cat miaows.’) or Zwinny kot miauczy. (Eng. ‘An agile cat miaows.’).\n3. passivisation, e.g. Człowiek ujeżdża byka. (Eng. ‘A man is breaking a bull in.’) can be transformed into Byk jest ujeżdżany przez człowieka. (Eng. ‘A bull is being broken in by a man.’).\n4. removing adjuncts, e.g. Dwa białe króliki siedzą na trawie. (Eng. ‘Two small rabbits are sitting on the grass.’) can be changed into Króliki siedzą. (Eng. ‘The rabbits are sitting.’).\n5. swapping relative clause for participles, i.e. a relative clause swaps with a participle (and vice versa), e.g. Kobieta przytula psa, którego trzyma na smyczy. (Eng. ‘A woman hugs a dog which she keeps on a leash.’). The relative clause is interchanged for a participle construction, e.g. Kobieta przytula trzymanego na smyczy psa. (Eng. ‘A woman hugs a dog kept on a leash.’).\n6. negation, e.g. Mężczyźni w turbanach na głowie siedzą na słoniach. (Eng. ‘Men in turbans on their heads are sitting on elephants.’) can be transformed into Nikt nie siedzi na słoniach. (Eng. ‘Nobody is sitting on elephants.’), Żadni mężczyźni w turbanach na głowie nie siedzą na słoniach. (Eng. ‘No men in turbans on their heads are sitting on elephants.’), and Mężczyźni w turbanach na głowie nie siedzą na słoniach. (Eng. ‘Men in turbans on their heads are not sitting on elephants.’).\n7. constrained mixing of dependents from various sentences, e.g. Dwoje dzieci siedzi na wielbłądach w pobliżu wysokich gór. (Eng. ‘Two children are sitting on camels near high mountains.’) can be changed into Dwoje dzieci siedzi przy zastawionym stole w pobliżu wysokich gór. (Eng. ‘Two children\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nare sitting at the table laden with food near high mountains.’).\nThe first five transformations are designed to produce sentences with a similar meaning, the sixth transformation outputs sentences with a contradictory meaning, and the seventh transformation should generate sentences with a neutral (or unrelated) meaning. All transformations are performed on dependency structures of input sentences (Wróblewska, 2014).\nSome of the transformations are very productive (e.g. mixing dependents). Other, in turn, are sparsely represented in the output (e.g. dropping conjunction). The number of transformed sentences randomly selected to build the dataset is in the second column of Table 1.\ntransformation selected dropping conjunction 139 2.0% removing conjunct in adjunct 485 6.9% passivisation 893 12.8% removing adjuncts 1013 14.5% swapping rc↔ptcp 1291 18.4% negation 1304 18.6% mixing dependents 1878 26.8%\nTable 1: Numbers of transformed sentences selected for annotation."
    }, {
      "heading" : "2.3 Data ensemble",
      "text" : "The final step of building the SICK corpus consisted in arranging normalised and expanded sentences into pairs. As our data diverges from SICK data, the process of arranging Polish sentences into pairs also differs from pairing in the SICK corpus. Apart from pairs connecting two sentences originally written by humans (as described in Section 2.1), there are also pairs in which an original sentence is connected with a transformed sentence. For each of the 1K images, the following 10 pairs are constructed (for A being the set of all sentences originally written by the first author, B being the set of all sentences originally written by the second author, a ∈ A and b ∈ B being the original descriptions of the picture):\n1. (a,b),\n2. (a,a1), where a1 ∈ t(a), for t(a) being the set of all transformations of the sentence a,\n3. (b,b1), where b1 ∈ t(b),\n4. (a,b2), where b2 ∈ t(b),\n5. (b,a2), where a2 ∈ t(a),\n6. (a,a3), where a3 ∈ t(a′),a′ ∈ A, T (a′) = T (a),a′ 6= a, for T (a) being the thematic group of a,\n7. (b,b3), where b3 ∈ t(b′),b′ ∈ B, T (b′) = T (b),b′ 6= b,\n8. (a,a4), where a4 ∈ A, T (a4) 6= T (a),\n9. (b,b4), where b4 ∈ B, T (b4) 6= T (b),\n10. (a,a5), where a5 ∈ t(a),a5 6= a1 for 50% images, (b,b5) (analogously) for other 50%.\nFor each sentence pair (a,b) created according to this procedure, its reverse (b,a) is also included in our corpus. As a result, the working set consists of 20K sentence pairs."
    }, {
      "heading" : "3 Corpus annotation",
      "text" : ""
    }, {
      "heading" : "3.1 Annotation assumptions",
      "text" : "The degree of semantic relatedness between two sentences is calculated as the average of all human ratings on the Likert scale with the range from 0 to 5. Since we do not want to excessively influence the annotations, the guidelines given to annotators are mainly example-based:\n• 5 (very related): Kot siedzi na płocie. (Eng. ‘A cat is sitting on the fence.’) vs. Na płocie jest duży kot. (Eng. ‘There is a large cat on the fence.’),\n• 1–4 (more or less related): Kot siedzi na płocie. (Eng. ‘A cat is sitting on the fence.’) vs. Kot nie siedzi na płocie. (Eng. ‘A cat is not sitting on the fence.’); Kot siedzi na płocie. (Eng. ‘A cat is sitting on the fence.’) vs. Właściciel dał kotu chrupki. (Eng. ‘The owner gave kibble to his cat.’); Kot siedzi na płocie. (Eng. ‘A cat is sitting on the fence.’) vs. Kot miauczy pod płotem. (Eng. ‘A cat miaows under the fence.’).\n• 0 (unrelated): Kot siedzi na płocie. (Eng. ‘A cat is sitting on the fence.’) vs. Zaczął padać deszcz. (Eng. ‘It starts raining.’).\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nApart from these examples, there is a note in the annotation guidelines indicating that the degree of semantic relatedness is not the same thing as the degree of semantic similarity. Since semantic similarity is only a special case of semantic relatedness, semantic relatedness is thus a more general term than the other one.\nPolish entailment labels correspond directly to the SICK labels (i.e. entailment, contradiction, neutral). The entailment label assigned by the majority of human judges is selected as the gold label. The entailment labels are defined as follows:\n• a wynika z b (b entails a) – if a situation or an event described by the sentence b occurs, it is recognised that a situation or an event described by a occurs as well, i.e. a and b refer to the same event or the same situation,\n• a jest zaprzeczeniem b (a is the negation of b) – if a situation or an event described by b occurs, it is recognised that a situation or an event described by a may not occur at the same time,\n• a jest neutralne wobec b (a is neutral to b) – the truth of a situation described by a cannot be determined on the basis of b."
    }, {
      "heading" : "3.2 Annotation procedure",
      "text" : "Similarly as in the SICK corpus, each Polish sentence pair is human annotated for semantic relatedness and entailment by 3 human judges experienced in Polish linguistics.2 Since for each annotated pair (a,b), its reverse (b,a) is also subject to annotation, the entailment relation is in practice determined ‘in both directions’ for 10K sentence pairs. For the task of relatedness annotation, the order of sentences within pairs seems to be irrelevant, we can thus assume to obtain 6 relatedness scores for 10K unique pairs.\nSince the transformation process is fully automatic and to a certain extent based on imperfect dependency parsing, we cannot ignore errors in the transformed sentences. In order to avoid annotating erroneous sentences, the annotation process is divided into two stages:\n1. a sentence pair is sent to a judge with the leader role, who is expected to edit and\n2Our annotators have relatively strong linguistic background. Five of them have PhD in linguistics, five are PhD students, one is a graduate, and one is an undergraduate.\nto correct the transformed sentence from this pair before annotation, if necessary,\n2. the verified and possibly enhanced sentence pair is sent to the other two judges, who can only annotate it.\nThe leader judges should correct incomprehensible and ungrammatical sentences with a minimal number of necessary changes. Unusual sentences which are generally accepted by Polish speakers should not be modified. Moreover, the modified sentence may not be identical with the other sentence in the pair."
    }, {
      "heading" : "3.3 Impromptu post-corrections",
      "text" : "During the annotation process it came out that sentences accepted by some human annotators are unacceptable for other annotators. We thus decided to garner annotators’ comments and suggestions for improving sentences. After validation of these suggestions by an experienced linguist, it turns out that most of these proposals concern punctuation errors and typos in 228 distinct sentences. These errors are fixed directly in the corpus, as they should not impact annotations of sentence pairs. The other suggestions concern more significant changes in 141 distinct sentences. Annotations of pairs with modified sentences are resent to the annotators so that they can verify and update them."
    }, {
      "heading" : "4 Corpus summary and evaluation",
      "text" : ""
    }, {
      "heading" : "4.1 Corpus statistics",
      "text" : "Tables 2 and 3 summarise the annotations of the resulting 10K sentence pairs corpus. Table 2 aggregates the occurrences of 6 possible relatedness scores, calculated as the mean of all 6 individual annotations, rounded to an integer.\nrelatedness # of pairs 0 1977 1 1430 2 1081 3 2165 4 2384 5 963\nTable 2: Final relatedness scores rounded to integers (total: 10K pairs).\nTable 3 shows the number of the particular entailment labels in the corpus. Since each sentence\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\npair is annotated for entailment in both directions, the final entailment label is actually a pair of two labels:\n• entailment+neutral points to ‘one-way’ entailment,\n• contradiction+neutral points to ‘one-way’ contradiction,\n• entailment+entailment, contradiction+contradiction, and neutral+neutral point to equivalence.\nWhile the actual corpus labels are ordered in the sense that there is a difference between, e.g. entailment+neutral and neutral+entailment (the entailment occurs in different directions), we treat all labels as unordered for the purpose of this summary (e.g. entailment+neutral covers neutral+entailment as well, representing the same type of relation between two sentences). The X label corresponds to pairs for which a majority label could not be established.\nentailment # of pairs neutral+neutral 6477 entailment+neutral 1741 entailment+entailment 932 contradiction+contradiction 717 contradiction+neutral 113 X 20\nTable 3: Final entailment labels (total: 10K pairs)."
    }, {
      "heading" : "4.2 Inter-annotator agreement",
      "text" : "The standard measure of inter-annotator agreement in various natural language labelling tasks is Cohen’s kappa (Cohen, 1960). However, this coefficient is designed to measure agreement between two annotators only. As there are three annotators of each pair of ordered sentences, we decided to apply Fleiss’ kappa3 (Fleiss, 1971) designed for measuring agreement between multiple raters who give categorical ratings to a fixed number of items. An additional advantage of this measure is that different items can be rated by different human judges, which doesn’t impact measurement. The normalised Fleiss’ measure of inter-annotator agreement is:\n3As Fleiss’ kappa is actually generalisation of Scott’s π (Scott, 1955), it is sometimes referred to as Fleiss’ multi-π, cf. Artstein and Poesio (2008).\nκ = P̄ − P̄e 1− P̄e\nwhere the quantity P̄ − P̄e measures the degree of agreement actually attained in excess of chance, while “[t]he quantity 1 − P̄e measures the degree of agreement attainable over and above what would be predicted by chance” (Fleiss, 1971, p. 379).\nWe recognise Fleiss’ kappa as particularly useful for measuring inter-annotator agreement with respect to entailment labelling in our evaluation dataset. First, there are more then two raters. Second, entailment labels are categorial. Measured with Fleiss’ kappa, there is inter-annotator agreement of κ = 0.732 for entailment labels in Polish evaluation dataset, which is quite satisfactory as for a semantic labelling task.\nRelative to semantic relatedness, the distinction in meaning of two sentences made by human judges is often very subtle. This is also reflected in the inter-annotator agreement scores measured with Fleiss’ kappa. Inter-annotator agreement measured for six semantic relatedness groups corresponding to points on the Likert scale is quite low: κ = 0.336. If we measure interannotator agreement for three classes corresponding to the three relatedness groups from the annotation guidelines (see Section 3.1), i.e. <0>, <1, 2, 3, 4>, and <5>, the Fleiss’ score is significantly higher: κ = 0.543. Hence, we conclude that Fleiss’ kappa is not a reliable measure of inter-annotator agreement in relation to relatedness scores. Therefore, we decided to use Krippendorff’s α instead.\nKrippendorff’s α (Krippendorff, 1980, 2013) is a coefficient appropriate for measuring interannotator agreement of a dataset which is annotated with multiple judges and characterised by different magnitudes of disagreement and missing values. Krippendorff (1980; 2013) offers distance metrics suitable for various scales: binary, nominal, interval, ordinal, and ratio. In ordinal measurement4 the attributes can be rank-ordered, but\n4Nominal measurement is useless for measuring agreement between relatedness scores (α = 0.336 is the identical value as Fleiss’ kappa, since all disagreements are considered equal). We also test interval measurement, in which the distance between the attributes does have meaning and an average of an interval variable is computed. The interval score measured for relatedness annotations is quite high α = 0.785, but we doubt whether the distance between relatedness scores is meaningful in this case.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\ndistances between them do not have any meaning. Measured with Krippendorff’s α, there is interannotator agreement of α = 0.780 for relatedness scores in the Polish evaluation dataset, which is quite satisfactory as well. Hence, we conclude that our dataset is a reliable resource for purpose of evaluating compositional distributional semantics model of Polish."
    }, {
      "heading" : "5 Conclusions",
      "text" : "The goal of this paper is to present the procedure of building the Polish evaluation dataset for validation of compositional distributional semantics models. As we aim at building an evaluation dataset which is comparable to the SICK corpus, the general assumptions of our procedure correspond to the design principles of the SICK corpus. However, the procedure of building the SICK corpus cannot be adapted without modifications. First, the Polish seed-sentences have to be written based on the images which are selected from 8K ImageFlickr dataset and split into thematic groups, since usable datasets are not publicly available. Second, as the process of transforming sentences seems to be language-specific, the linguistic transformation rules appropriate for Polish have to be defined from scratch. Third, the process of arranging Polish sentences into pairs is defined anew taking into account the data characteristic and bidirectional entailment annotations. The discrepancies relative to the SICK procedure also concern the annotation process itself. Since an entailment relation between two sentences must not be symmetric, each sentence pair is annotated for entailment in both directions. Furthermore, we introduce an element of human verification of correctness of automatically transformed sentences and some additional post-corrections.\nThe presented procedure results in building the Polish test corpus of relatively high quality. There are inter-annotator agreement on entailment labels of κ = 0.732 measured with Fleiss’ kappa and inter-annotator agreement on relatedness scores of α = 0.78 measured with Krippendorff’s alpha. The evaluation dataset will be made publicly available upon publication of this paper."
    } ],
    "references" : [ {
      "title" : "Semeval-2012 task 6: A pilot on semantic textual similarity",
      "author" : [ "Eneko Agirre", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre." ],
      "venue" : "Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM). pages 385–393.",
      "citeRegEx" : "Agirre et al\\.,? 2012",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2012
    }, {
      "title" : "Inter-Coder Agreement for Computational Linguistics",
      "author" : [ "Ron Artstein", "Massimo Poesio." ],
      "venue" : "Computational Linguistics 34:557–596.",
      "citeRegEx" : "Artstein and Poesio.,? 2008",
      "shortCiteRegEx" : "Artstein and Poesio.",
      "year" : 2008
    }, {
      "title" : "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space",
      "author" : [ "Marco Baroni", "Roberto Zamparelli." ],
      "venue" : "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. pages",
      "citeRegEx" : "Baroni and Zamparelli.,? 2010",
      "shortCiteRegEx" : "Baroni and Zamparelli.",
      "year" : 2010
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin." ],
      "venue" : "Journal of Machine Learning Research 3:1137–1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Neural probabilistic language models",
      "author" : [ "Yoshua Bengio", "Holger Schwenk", "Jean-Sébastien Senécal", "Fréderic Morin", "Jean-Luc Gauvain." ],
      "venue" : "D.E. Holmes and L.C. Jain, editors, Innovations in Machine Learning. Theory and Applications,",
      "citeRegEx" : "Bengio et al\\.,? 2006",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2006
    }, {
      "title" : "Sick through the semeval glasses",
      "author" : [ "Luisa Bentivogli", "Raffaella Bernardi", "Marco Marelli", "Stefano Menini", "Marco Baroni", "Roberto Zamparelli." ],
      "venue" : "lesson learned from the evaluation of compositional distributional semantic models on full sen-",
      "citeRegEx" : "Bentivogli et al\\.,? 2014",
      "shortCiteRegEx" : "Bentivogli et al\\.",
      "year" : 2014
    }, {
      "title" : "A coefficient of agreement for nominal scales",
      "author" : [ "Jacob Cohen." ],
      "venue" : "Educational and Psychological Measurement 20:37–46.",
      "citeRegEx" : "Cohen.,? 1960",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1960
    }, {
      "title" : "A synopsis of linguistic theory, 1930-1955",
      "author" : [ "John Rupert Firth." ],
      "venue" : "Studies in Linguistic Analysis. Special volume of the Philological Society pages 1–32.",
      "citeRegEx" : "Firth.,? 1957",
      "shortCiteRegEx" : "Firth.",
      "year" : 1957
    }, {
      "title" : "Measuring nominal scale agreement among many raters",
      "author" : [ "Joseph L. Fleiss." ],
      "venue" : "Psychological Bulletin 75:378–382.",
      "citeRegEx" : "Fleiss.,? 1971",
      "shortCiteRegEx" : "Fleiss.",
      "year" : 1971
    }, {
      "title" : "Experimental support for a categorical compositional distributional model of meaning",
      "author" : [ "Edward Grefenstette", "Mehrnoosh Sadrzadeh." ],
      "venue" : "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011).",
      "citeRegEx" : "Grefenstette and Sadrzadeh.,? 2011",
      "shortCiteRegEx" : "Grefenstette and Sadrzadeh.",
      "year" : 2011
    }, {
      "title" : "Distributional structure",
      "author" : [ "Zellig Harris." ],
      "venue" : "Word 10:146–162.",
      "citeRegEx" : "Harris.,? 1954",
      "shortCiteRegEx" : "Harris.",
      "year" : 1954
    }, {
      "title" : "Compositionality: its historic context",
      "author" : [ "Theo M.V. Janssen." ],
      "venue" : "Wolfram Hinzen, Edouard Machery, and Markus Werning, editors, The Oxford Handbook of Compositionality, Oxford University Press, Studies in Fuzziness and Soft Computing, pages 19–",
      "citeRegEx" : "Janssen.,? 2012",
      "shortCiteRegEx" : "Janssen.",
      "year" : 2012
    }, {
      "title" : "Content Analysis: An Introduction to Its Methodology",
      "author" : [ "Klaus Krippendorff." ],
      "venue" : "Sage Publications, Beverly Hills.",
      "citeRegEx" : "Krippendorff.,? 1980",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 1980
    }, {
      "title" : "Content Analysis: An Introduction to Its Methodology",
      "author" : [ "Klaus Krippendorff." ],
      "venue" : "Sage Publication, Thousand Oaks, 3rd edition.",
      "citeRegEx" : "Krippendorff.,? 2013",
      "shortCiteRegEx" : "Krippendorff.",
      "year" : 2013
    }, {
      "title" : "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual",
      "author" : [ "Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli" ],
      "venue" : null,
      "citeRegEx" : "Marelli et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Marelli et al\\.",
      "year" : 2014
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Advances in neural information processing systems pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Composition in distributional models of semantics",
      "author" : [ "Jeff Mitchell", "Mirella Lapata." ],
      "venue" : "Cognitive Science 34:1388–1429.",
      "citeRegEx" : "Mitchell and Lapata.,? 2010",
      "shortCiteRegEx" : "Mitchell and Lapata.",
      "year" : 2010
    }, {
      "title" : "Collecting image annotations using amazon’s mechanical turk",
      "author" : [ "Cyrus Rashtchian", "Peter Young", "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechan-",
      "citeRegEx" : "Rashtchian et al\\.,? 2010",
      "shortCiteRegEx" : "Rashtchian et al\\.",
      "year" : 2010
    }, {
      "title" : "Reliability of content analysis: The case of nominal scale coding",
      "author" : [ "William A. Scott." ],
      "venue" : "Public Opinion Quarterly 19:321–325.",
      "citeRegEx" : "Scott.,? 1955",
      "shortCiteRegEx" : "Scott.",
      "year" : 1955
    }, {
      "title" : "Semantic compositionality through recursive matrix-vector spaces",
      "author" : [ "Richerd Socher", "Brody Huval", "Christopher Manning", "Andrew Ng." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and",
      "citeRegEx" : "Socher et al\\.,? 2012",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2012
    }, {
      "title" : "Polish Dependency Parser Trained on an Automatically Induced Dependency Bank",
      "author" : [ "Alina Wróblewska." ],
      "venue" : "Ph.D. dissertation, Institute of Computer Science, Polish Academy of Sciences, Warsaw.",
      "citeRegEx" : "Wróblewska.,? 2014",
      "shortCiteRegEx" : "Wróblewska.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "word embedding techniques (Bengio et al., 2003, 2006; Mikolov et al., 2013).",
      "startOffset" : 26,
      "endOffset" : 75
    }, {
      "referenceID" : 7,
      "context" : "determining the meaning of a word based on its co-occurrence with other words, is derived from the empiricists – Harris (1954) and Firth (1957).",
      "startOffset" : 113,
      "endOffset" : 127
    }, {
      "referenceID" : 5,
      "context" : "determining the meaning of a word based on its co-occurrence with other words, is derived from the empiricists – Harris (1954) and Firth (1957). John R.",
      "startOffset" : 131,
      "endOffset" : 144
    }, {
      "referenceID" : 14,
      "context" : "Apart from papers it was the SemEval-2014 Shared Task 1 (Marelli et al., 2014) which essentially contributed to the expansion of CDS and increased an interest in this domain.",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "3 The SICK corpus The SICK corpus (Bentivogli et al., 2014) consists of 10K pairs of English sentences containing multiple lexical, syntactic, and semantic phenomena.",
      "startOffset" : 34,
      "endOffset" : 59
    }, {
      "referenceID" : 17,
      "context" : "It builds on two external data sources – the 8K ImageFlickr dataset (Rashtchian et al., 2010) and SemEval-2012 Semantic Textual Similarity dataset (Agirre et al.",
      "startOffset" : 68,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : ", 2010) and SemEval-2012 Semantic Textual Similarity dataset (Agirre et al., 2012).",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : "1 Selection and description of images The first step of building the SICK corpus consisted in the random selection of English sentence pairs from existing datasets (Rashtchian et al., 2010; Agirre et al., 2012).",
      "startOffset" : 164,
      "endOffset" : 210
    }, {
      "referenceID" : 0,
      "context" : "1 Selection and description of images The first step of building the SICK corpus consisted in the random selection of English sentence pairs from existing datasets (Rashtchian et al., 2010; Agirre et al., 2012).",
      "startOffset" : 164,
      "endOffset" : 210
    }, {
      "referenceID" : 17,
      "context" : "Images are selected from the 8K ImageFlickr dataset (Rashtchian et al., 2010).",
      "startOffset" : 52,
      "endOffset" : 77
    }, {
      "referenceID" : 20,
      "context" : "All transformations are performed on dependency structures of input sentences (Wróblewska, 2014).",
      "startOffset" : 78,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "2 Inter-annotator agreement The standard measure of inter-annotator agreement in various natural language labelling tasks is Cohen’s kappa (Cohen, 1960).",
      "startOffset" : 139,
      "endOffset" : 152
    }, {
      "referenceID" : 8,
      "context" : "As there are three annotators of each pair of ordered sentences, we decided to apply Fleiss’ kappa3 (Fleiss, 1971) designed for measuring agreement between multiple raters who give categorical ratings to a fixed number of items.",
      "startOffset" : 100,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : "The normalised Fleiss’ measure of inter-annotator agreement is: As Fleiss’ kappa is actually generalisation of Scott’s π (Scott, 1955), it is sometimes referred to as Fleiss’ multi-π, cf.",
      "startOffset" : 121,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "Artstein and Poesio (2008). κ = P̄ − P̄e 1− P̄e where the quantity P̄ − P̄e measures the degree of agreement actually attained in excess of chance, while “[t]he quantity 1 − P̄e measures the degree of agreement attainable over and above what would be predicted by chance” (Fleiss, 1971, p.",
      "startOffset" : 0,
      "endOffset" : 27
    } ],
    "year" : 2017,
    "abstractText" : "The paper presents a procedure of building an evaluation dataset for validation of compositional distributional semantics models estimated for languages other than English. The procedure generally builds on steps designed to assemble the SICK corpus which contains pairs of English sentences annotated for semantic relatedness and entailment, as we aim at building a comparable dataset. However, the implementation of particular building steps significantly differs from the original SICK design assumptions, which is caused by both lack of necessary extraneous resources for an investigated language and the demand for language-specific transformation rules. The designed procedure is verified on Polish, a fusional language with a relatively free word order, and contributes to building a Polish evaluation dataset. The resource consists of 10K sentence pairs which are human-annotated for semantic relatedness and entailment. The dataset may be used for evaluation of compositional distributional semantics models of Polish.",
    "creator" : "LaTeX with hyperref package"
  }
}