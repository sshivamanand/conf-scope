{
  "name" : "222.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Joint extraction of entities and relations is to detect entity mentions and recognize their semantic relations simultaneously from unstructured text, as Figure 1 shows. Different from open information extraction (Open IE) (Banko et al., 2007) whose relation words are extracted from the given sentence, in this task, relation words are extracted from a predefined relation set which may not appear in the given sentence. It is an important issue in knowledge extraction and automatic construction of knowledge base.\nTraditional methods handle this task in a pipelined manner, i.e., extracting the entities (Nadeau and Sekine, 2007) first and then recognizing their relations (Rink, 2010). This separated framework makes the task easy to deal with, and each component can be more flexible. But it neglects the relevance between these two sub-tasks\nand each subtask is an independent model. The results of entity recognition may affect the performance of relation classification and lead to erroneous delivery (Li and Ji, 2014).\nDifferent from the pipelined methods, joint learning framework is to extract entities together with relations using a single model. It can effectively integrate the information of entities and relations, and it has been shown to achieve better results in this task. However, most existing joint methods are feature-based structured systems (Li and Ji, 2014; Miwa and Sasaki, 2014; Yu and Lam, 2010; Ren et al., 2017). They need complicated feature engineering and heavily rely on the other NLP toolkits, which might also lead to error propagation. In order to reduce the manual work in feature extraction, recently, (Miwa and Bansal, 2016) presents a neural networkbased method for the end-to-end entities and relations extraction. Although the joint models can represent both entities and relations with shared parameters in a single model, they also extract the entities and relations separately and produce redundant information. For instance, the sentence in Figure 1 contains three entities: “United States”, “Obama” and “Apple Inc”. But only “United States” and “Obama” hold a fix relation “CountryPresident”. Entity “Apple Inc” has no obvious relationship with the other entities in this sentence. Hence, the extracted result from this sentence is {United Statese1, Country-Presidentr,\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nObamae2}, which called triplet here. In this paper, we focus on the extraction of triplets that are composed of two entities and one relation between these two entities. Therefore, we can model the triplets directly, rather than extracting the entities and relations separately. Based on the motivations, we propose a tagging scheme accompanied with the end-to-end model to settle this problem. We design a kind of novel tags which contain the information of entities and the relationships they hold. Based on this tagging scheme, the joint extraction of entities and relations can be transformed into a tagging problem. In this way, we can also easily use neural networks to model the task without complicated feature engineering.\nRecently, end-to-end models based on LSTM (Hochreiter and Schmidhuber, 1997) have been successfully applied to various tagging tasks: Named Entity Recognition (Lample et al., 2016), CCG Supertagging (Vaswani et al., 2016), Chunking (Zhai et al., 2017) et al. LSTM is capable of learning long-term dependencies, which is beneficial to sequence modeling tasks. Therefore, based on our tagging scheme, we investigate different kinds of LSTM-based end-to-end models to jointly extract the entities and relations. We also modify the decoding method by adding a bias loss to make it more suitable for our special tags.\nThe method we proposed is a supervised learning algorithm. In reality, however, the process of manually labeling a training set with a large number of entity and relation is too expensive and error-prone. Therefore, we conduct experiments on a public dataset1 which is produced by distant supervision method (Ren et al., 2017) to validate our approach. The experimental results show that our tagging scheme is effective in this task. In addition, our end-to-end model can achieve the best results on the public dataset.\nThe major contributions of this paper are: (1) A novel tagging scheme is proposed to jointly extract entities and relations, which can easily transform the extraction problem into a tagging task. It is the first work to solve the problem by a tagging manner. (2) Based on our tagging scheme, we study different kinds of end-to-end models to settle the problem. The tagging-based methods are better than most of the existing pipelined and joint learning methods. (3) Furthermore, we also develop an end-to-end model with a bias objective\n1https://github.com/shanzhenren/CoType\nfunction to suit for the novel tags. It can enhance the association between related entities."
    }, {
      "heading" : "2 Related Works",
      "text" : "Entities and relations extraction is an important step to construct a knowledge base, which can be benefit for many NLP tasks (Zou et al., 2014). Two main frameworks have been widely used to solve the problem of extracting entity and their relationships. One is the pipelined method and the other is the joint learning method. The pipelined method treats this task as two separated tasks, i.e., named entity recognition (NER) (Nadeau and Sekine, 2007) and relation classification (RC) (Rink, 2010). Classical NER models are linear statistical models, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Passos et al., 2014; Luo et al., 2015). Recently, several neural network architectures (Chiu and Nichols, 2015; Huang et al., 2015; Lample et al., 2016) have been successfully applied to NER, which is regarded as a sequential token tagging task. Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al., 2016; Zeng, 2014; Xu, 2015b; dos Santos, 2015). While joint models extract entities and relations using a single model. Most of the joint methods are feature-based structured systems (Ren et al., 2017; Yang and Cardie, 2013; Singh et al., 2013; Miwa and Sasaki, 2014; Li and Ji, 2014). Recently, (Miwa and Bansal, 2016) uses a LSTM-based model to extract entities and relations, which can reduce the manual work.\nDifferent from the above methods, the method proposed in this paper is based on a special tagging manner, so that we can easily use end-toend model to extract results without NER and RC. end-to-end method is to map the input sentence into meaningful vectors and then back to produce a sequence. It is widely used in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and sequence tagging tasks (Lample et al., 2016; Vaswani et al., 2016). Most methods apply bidirectional LSTM to encode the input sentences, but the decoding methods are always different. For examples, (Lample et al., 2016) use a CRF layers to decode the tag sequence, while (Vaswani et al., 2016; Katiyar and Cardie, 2016) apply LSTM layer to produce the tag sequence.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nInput Sentence: The United States President Obama will visit the Apple Inc founded by Steven Paul Jobs\n{Apple Inc, Company-Founder, Steven Paul Jobs}Final Results:\nTags: O B-CP-1 E-CP-1 O S-CP-2 O O O B-CF-1 E-CF-1 O O B-CF-2 I-CF-2 E-CF-2\n{United States, Country-President, Obama}\nFigure 2: Gold standard annotation for an example sentence based on our tagging scheme, where “CP” is short for “Country-President” and “CF” is short for “Company-Founder”."
    }, {
      "heading" : "3 Method",
      "text" : "We propose a novel tagging scheme and an end-toend model with biased objective function to jointly extract entities and their relations. In this section, we firstly introduce how to change the extraction problem to a tagging problem based on our tagging method. Then we detail the model we used to extract results."
    }, {
      "heading" : "3.1 The Tagging Scheme",
      "text" : "Figure 2 is an example of how the results are tagged. Each word is assigned a label that contributes to extract the results. Tag “O” represents the “Other” tag, which means that the corresponding word is independent of the extracted results. In addition to “O”, the other tags consist of three parts: the word position in the entity, the relation type, and the relation role. We use the “BIES” (Begin, Inside, End,Single) signs to represent the position information of a word in the entity. The relation type information is obtained from a predefined set of relations and the relation role information is represented by the numbers “1” and “2”. An extracted result is represented by a triplet: (Entity1, RelationType,Entity2). “1” means that the word belongs to the first entity in the triplet, while “2” belongs to second entity that behind the relation type. Thus, the total number of tags is Nt = 2 ∗ 4 ∗ |R|+ 1, where |R| is the size of the predefined relation set.\nFigure 2 is an example illustrating our tagging method. The input sentence contains two triplets: {United States, CountryPresident, Obama} and {Apple Inc, CompanyFounder, Steven Paul Jobs}, where “CountryPresident” and “Company-Founder” are the predefined relation types. The words “United”,“States”,“Obama”,“Apple”,“Inc” ,“Steven”, “Paul” and “Jobs” are all related to the final extracted results. Thus they are tagged based\non our special tags. For example, the word of “United” is the first word of entity “United States” and is related to the relation “Country-President”, so its tag is “B-CP-1”. The other entity “Obama”, which is corresponding to “United States”, is labeled as “S-CP-2”. Besides, the other words irrelevant to the final result are labeled as “O”."
    }, {
      "heading" : "3.2 From Tag Sequence To Extracted Results",
      "text" : "From the tag sequence in Figure 2, we know that “Obama” and “United States” share the same relation type “Country-President”, “Apple Inc” and “Steven Paul Jobs” share the same relation type “Company-Founder”. We combine entities with the same relation type into a triplet to get the final result. Accordingly, “Obama” and “United States” can be combined into a triplet whose relation type is “Country-President”. Because, the relation role of “Obama” is “2” and “United States” is “1”, the final result is {United States, CountryPresident, Obama}. The same applies to {Apple Inc, Company-Founder, Steven Paul Jobs}.\nBesides, if a sentence contains two or more triplets with the same relation type, we combine every two entities into a triplet based on the nearest principle. For example, if the relation type “Country-President” in Figure 2 is “CompanyFounder”, then there will be four entities in the given sentence with the same relation type. “United States” is closest to entity “Obama” and the “Apple Inc” is closest to “Jobs”, so the results will be {United States, Company-Founder, Obama} and {Apple Inc, Company-Founder, Steven Paul Jobs}. In this paper, we only consider the situation where an entity belongs to a triplet, and we leave identification of overlapping relations for future work."
    }, {
      "heading" : "3.3 The end-to-end Model",
      "text" : "In recent years, end-to-end model based on neural network is been widely used in sequence tagging\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\ntask. In this paper, we investigate an end-to-end Model to produce the tags sequence. It contains a bi-directional Long Short Term Memory (BiLSTM) layer to encode the input sentence and a LSTM decoding layer with bias loss. The bias loss can enhance the relevance of entity tags. The Bi-LSTM Encoding Layer. In sequence tagging problems, the Bi-LSTM encoding layer has been shown the effectiveness to capture the semantic information of each word. It contains word embedding layer, forward lstm layer, backward lstm layer and the concatenate layer. The word embedding layer converts the word with 1- hot representation to an embedding vector. Hence, a sequence of words can be represented as W = {w1, ...wt, wt+1...wn}, where wt ∈ Rd is the ddimensional word vector corresponding to the t-th word in the sentence and n is the length of the given sentence. After word embedding layer, there are two parallel LSTM layers: forward lstm layer and backward lstm layer. For each word wt, the forward layer will encode wt by considering the contextual information from wordw1 towt, which is marked as −→ ht . In the similar way, the backward layer will encodewt based on the contextual information from wn to wt, which is marked as ←− ht .\nThe LSTM architecture consists of a set of recurrently connected subnets, known as memory blocks. Each time-step is a LSTM memory block, which is used to compute current hidden vector ht based on the previous hidden vector ht−1, the previous cell vector ct−1 and the current input word embedding wt. It can be shortly denoted as: −→ ht = lstm( −−→ ht−1,\n−−→ct−1, wt) and←− ht = lstm( ←−− ht+1,\n←−−ct+1, wt). The detail operations are defined as follows:\nit = δ(Wxixt +Whiht−1 +Wcict−1 + bi), (1)\nft = δ(Wxfxt +Whfht−1 +Wcfct−1 + bf ), (2)\nzt = tanh(Wxcxt +Whcht−1 + bc), (3)\nct = ftct−1 + itzt, (4)\not = δ(Wxoxt +Whoht−1 +Wcoct + bo), (5)\nht = ottanh(ct), (6)\nwhere i, f and o are the input gate, forget gate and output gate respectively, b is the bias term, c is the cell memory, · denotes element-wise multiplication and W(.) are the parameters. Finally, we\nconcatenate ←− ht and −→ ht to represent word t’s encoding information, denoted as ht = [ ←− ht , −→ ht ]. The LSTM Decoding Layer. We also adopt a LSTM structure to produce the tag sequence. When detecting the tag of word wt, the inputs of decoding layer are: ht obtained from Bi-LSTM encoding layer, former predicted tag vector Tt−1, and the former hidden state of decoding LSTM st−1. Each unit of the decoding LSTM is the same as the encoding lstm memory block except for the input gate, which can be rewritten as:\nit = δ(Wxiht +Whist−1 +WtiTt−1 + bi), (7)\nwhere the tag embedding T is transformed from the hidden state s as follows:\nTt = Wtsst + bts. (8)\nThe final softmax layer computes normalized entity tag probabilities based on the tag predicted vector Tt:\nyt = WyTt + by, (9)\npit = exp(yit) Nt∑ j=1 exp(yjt ) , (10)\nwhere Wy is the softmax matrix, Nt is the total number of tags. Because the T is similar to tag embedding and LSTM is capable of learning longterm dependencies, the decoding manner can model tag interactions. The Bias Objective Function. We train our model to maximize the log-likelihood of the data and the optimization method we used is RMSprop proposed by Hinton in (Tieleman and Hinton, 2012). The objective function can be defined as:\nL =max |D|∑ j=1 Lj∑ t=1 (log(p (j) t = y (j) t |xj ,Θ) · I(O)\n+α · log(p(j)t = y (j) t |xj ,Θ) · (1− I(O))),\nwhere |D| is the size of training set, Lj is the length of sentence xj , y (j) t is the label of word t in sentence xj and p (j) t is the normalized probabilities of tags which defined in Formula 10. Besides, I(O) is a switching function to distinguish the loss of tag ’O’ and relational tags that can indicate the results. It is defined as follows:\nI(O) =\n{ 1, if tag = ′O′\n0, if tag 6= ′O′.\nα is the bias weight. The larger α is, the greater influence of relational tags on the model.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental setting",
      "text" : "Dataset To evaluate the performance of our methods, we use the public dataset NYT 2 which is produced by distant supervision method (Ren et al., 2017). A large amount of training data can be obtained by means of distant supervision methods without manually labeling. While the test set is manually labeled to ensure its quality. In total, the training data contains 1.18M sentences samples from New York Times News with 353k triplets. The test set contains 395 manually labeling sentences with 3, 880 triplets. Besides, the size of relation set is 24. Evaluation We adopt standard Precision (Prec), Recall (Rec) and F1 score to evaluate the results. A triplet is regarded as correct when its relation type and the head offsets of two corresponding entities are both correct. Besides, the ground-truth relation mentions are given and “None” label is excluded as (Ren et al., 2017; Li and Ji, 2014; Miwa and Bansal, 2016) did. We create a validation set by randomly sampling 10% data from test set and use the remaining data as evaluation based on (Ren et al., 2017)’s suggestion. We run 10 times for each experiment then report the average results and their standard deviation as Table 1 shows. Hyperparameters Our model consists of a BiLSTM encoding layer and a LSTM decoding layer with bias objective function. The word embeddings used in the encoding part are initialed by running word2vec3 (Mikolov et al., 2013) on NYT training corpus. The dimension of the word embeddings is d = 300. We regularize our network using dropout on embedding layer and the dropout ratio is 0.5. The number of lstm units in encoding layer is 300 and the number in decoding layer is 600. The bias parameter α corresponding to the results in Table 1 is 10. Baselines We compare our method with several classical triplet extraction methods, which can be divided into the following categories: the pipelined methods, the jointly extracting method-\n2The dataset can be downloaded at: https://github.com/shanzhenren/CoType. There are three data sets in the public resource and we only use the NYT dataset. Because more than 50% of the data in BioInfer has overlapping relations which is beyond the scope of this paper. As for dataset Wiki-KBP, the number of relation type in the test set is more than that of the train set, which is also not suitable for a supervised training method.\n3https://code.google.com/archive/p/word2vec/\ns and the end-to-end methods based our tagging scheme.\nFor the pipelined methods, we follow (Ren et al., 2017)’s settings: The NER results are obtained by CoType (Ren et al., 2017) then several classical relation classification methods are applied to detect the relations. These methods are: (1) DS-logistic (Mintz et al., 2009) is a distant supervised and feature based method, which combines the advantages of supervised IE and unsupervised IE features; (2) LINE (Tang et al., 2015) is a network embedding method, which is suitable for arbitrary types of information networks; (3) FCM (Gormley et al., 2015) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction.\nThe jointly extracting methods used in this paper are listed as follows: (4) DS-Joint (Li and Ji, 2014) is a supervised method, which jointly extracts entities and relations using structured perceptron on human-annotated dataset; (5) MultiR (Hoffmann et al., 2011) is a typical distant supervised method based on multi-instance learning algorithms to combat the noisy training data; (6) CoType (Ren et al., 2017) is a domain independent framework by jointly embedding entity mentions, relation mentions, text features and type labels into meaningful representations.\nIn addition, we also compare our method with two classical end-to-end tagging models: LSTMCRF (Lample et al., 2016) and LSTM-LSTM (Vaswani et al., 2016). LSTM-CRF is proposed for entity recognition by using a bidirectional LSTM to encode input sentence and a conditional random fields to predict the entity tag sequence. Different from LSTM-CRF, LSTM-LSTM uses a LSTM layer to decode the tag sequence instead of CRF. They are used for the first time to jointly extract entities and relations based on our tagging scheme."
    }, {
      "heading" : "4.2 Experimental Results",
      "text" : "We report the results of different methods as shown in Table 1. It can be seen that our method, LSTM-LSTM-Bias, outperforms all other methods in F1 score and achieves a 3% improvement in F1 over the best method CoType (Ren et al., 2017). It shows the effectiveness of our proposed method. Furthermore, from Table 1, we also can see that the jointly extracting methods are better than pipelined methods, and the tagging methods\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nMethods Prec. Rec. F1 FCM 0.553 0.154 0.240 DS+logistic 0.258 0.393 0.311 LINE 0.335 0.329 0.332\nMultiR 0.338 0.327 0.333 DS-Joint 0.574 0.256 0.354 CoType 0.423 0.511 0.463\nLSTM-CRF 0.693± 0.008 0.310± 0.007 0.428± 0.008 LSTM-LSTM 0.682± 0.007 0.320± 0.006 0.436± 0.006 LSTM-LSTM-Bias 0.615± 0.008 0.414± 0.005 0.495± 0.006\nTable 1: The predicted results of different methods on extracting both entities and their relations. The first part (from row 1 to row 3) is the pipelined methods and the second part (row 4 to 6) is the jointly extracting methods. Our tagging methods are shown in part three (row 7 to 9). In this part, we not only report the results of precision, recall and F1, we also compute their standard deviation.\nElements E1 E2 (E1,E2) PRF Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1\nLSTM-CRF 0.596 0.325 0.420 0.605 0.325 0.423 0.724 0.341 0.465 LSTM-LSTM 0.593 0.342 0.434 0.619 0.334 0.434 0.705 0.340 0.458 LSTM-LSTM-Bias 0.590 0.479 0.529 0.597 0.451 0.514 0.645 0.437 0.520\nTable 2: The predicted results of triplet’s elements based on our tagging scheme.\nare better than most of the jointly extracting methods. It also validates the validity of our tagging scheme for the task of jointly extracting entities and relations.\nWhen compared with the traditional methods, the precisions of the end-to-end models are significantly improved. But only LSTM-LSTM-Bias can be better to balance the precision and recall. The reason may be that these end-to-end models all use a Bi-LSTM encoding input sentence and different neural networks to decode the results. The methods based on neural networks can well fit the data. Therefore, they can learn the common features of the training set well and may lead to the lower expansibility. We also find that the LSTM-LSTM model is better than LSTM-CRF model based on our tagging scheme. Because, LSTM is capable of learning long-term dependencies and CRF (Lafferty et al., 2001) is good at capturing the joint probability of the entire sequence of labels. The related tags may have a long distance from each other. Hence, LSTM decoding manner is a little better than CRF. LSTM-LSTM-Bias adds a bias weight to enhance the effect of entity tags and weaken the effect of invalid tag. Therefore, in this tagging scheme, our method can be better than the common LSTM-decoding methods."
    }, {
      "heading" : "5 Analysis and Discussion",
      "text" : ""
    }, {
      "heading" : "5.1 Error Analysis",
      "text" : "In this paper, we focus on extracting triplets composed of two entities and a relation. Table 1 has shown the predict results of the task. It treats an triplet is correct only when the relation type and the head offsets of two corresponding entities are both correct. In order to find out the factors that affect the results of end-to-end models, we analyze the performance on predicting each element in the triplet as Table 2 shows. E1 and E2 represent the performance on predicting each entity, respectively. If the head offset of the first entity is correct, then the instance of E1 is correct, the same to E2. Regardless of relation type, if the head offsets of two corresponding entities are both correct, the instance of (E1, E2) is correct.\nAs shown in Table 2, (E1, E2) has higher precision when compared with E1 and E2. But its recall result is lower than E1 and E2. It means that some of the predicted entities do not form a pair. They only obtain E1 and do not find its corresponding E2, or obtain E2 and do not find its corresponding E1. Thus it leads to the prediction of more single E and less (E1, E2) pairs. Therefore, entity pair (E1, E2) has higher precision and lower recall than single E. Besides, the predict-\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n691\n692\n693\n694\n695\n696\n697\n698\n699\ned results of (E1, E2) in Table 2 have about 3% improvement when compared predicted results in Table 1, which means that 3% of the test data is predicted to be wrong because the relation type is predicted to be wrong."
    }, {
      "heading" : "5.2 Analysis of Bias",
      "text" : "Different from LSTM-CRF and LSTM-LSTM, our approach is biased towards relational labels to enhance links between entities. In order to further analyze the effect of the bias objective function, we visualize the ratio of predicted single entities for each end-to-end method as Figure 3. The single entities refer to those who cannot find their corresponding entities. Figure 3 shows whether it is E1 or E2, our method can get a relatively low ratio on the single entities. It means that our method can effectively associate two entities when compared LSTM-CRF and LSTM-LSTM which pay little attention to the relational tags.\nSingle E1 Single E2 0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nT h e R\na ti\no o\nf S in\ng le\nE\n0.178 0.186\n0.151 0.167\n0.135\n0.101\nLSTM-CRF LSTM-LSTM LSTM-LSTM-Bias\nFigure 3: The ratio of predicted single entities for each method. The higher of the ratio the more entities are left.\nBesides, we also change the Bias Parameter α from 1 to 20, and the predicted results are shown in Figure 4. If α is too large, it will affect the accuracy of prediction and if α is too small, the recall will decline. When α = 10, LSTM-LSTMBias can balance the precision and recall, and can achieve the best F1 scores."
    }, {
      "heading" : "5.3 The Applicability of Our Methods",
      "text" : "Although LSTM-LSTM-Bias can achieve a maximum F1 value, in practice we may pay more attention to the precision or recall. Because, LSTMLSTM and LSTM-LSTM-Bias use the softmax\nfunction to select the maximum probability label as the predicted tag for each word. We reset the prediction method as follows: if the maximum predicted probability is smaller than the threshold, the predicted tag is ’O’, else the result is the maximum probability label. Therefore, we tune the threshold for LSTM-LSTM and LSTM-LSTMBias to obtain their Precision-Recall curves that can reflect the predictive ability of models. Figure 5 shows that LSTM-LSTM-Bias is larger than LSTM-LSTM in both precision and recall, which means a more broad applicability."
    }, {
      "heading" : "5.4 Case Study",
      "text" : "In this section, we observe the prediction results of end-to-end methods, and then select several representative examples to illustrate the advantages and\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nStandard S1 [Panama City Beach]E2contain has condos , but the area was one of only two in [Florida]E1contain where sales rose in March , compared with a year earlier.\nLSTM-LSTM Panama City Beach has condos , but the area was one of only two in [Florida]E1contain where sales rose in March , compared with a year earlier.\nLSTM-LSTM-Bias [Panama City Beach]E2contain has condos , but the area was one of only two in [Florida]E1contain where sales rose in March , compared with a year earlier.\nStandard S2 All came from [Nuremberg]E2contain , [Germany]E1contain , a center of brass production since the Middle Ages. LSTM-LSTM All came from Nuremberg , [Germany]E1contain , a center of brass production since the [Middle Ages]E2contain. LSTM-LSTM-Bias All came from Nuremberg , [Germany]E1contain , a center of brass production since the [Middle Ages]E2contain.\nStandard S3 [Stephen A.]E2CF , the co-founder of the [Blackstone Group]E1CF , which is in the process of going public , made $ 400 million last year. LSTM-LSTM [Stephen A.]E1CF , the co-founder of the [Blackstone Group]E1CF , which is in the process of going public , made $ 400 million last year. LSTM-LSTM-Bias [Stephen A.]E1CF , the co-founder of the [Blackstone Group]E2CF , which is in the process of going public , made $ 400 million last year.\nTable 3: Output from different models. Standard Si represents the gold standard of sentence i. The blue part is the correct result, and the red one is the wrong one. E1CF in case ’3’ is short for E1Company−Founder.\ndisadvantages of the methods. Each example contains three row, the first row is the gold standard, the second and the third rows are the extracted results of model LSTM-LSTM and LSTM-LSTMBias respectively. Example S1 represents the situation that the distance between the two interrelated entities is far away from each other, which is more difficult to detect their relationships. When compared with LSTM-LSTM, LSTM-LSTM-Bias uses a bias objective function which enhance the relevance between entities. Therefore, in this example, LSTM-LSTM-Bias can extract two related entities, while LSTM-LSTM can only extract one entity of “Florida” and can not detect entity “Panama City Beach”.\nS2 is a negative example that shows these methods may mistakenly predict one of the entity. There are no indicative words between entities Nuremberg and Germany. Besides, the patten “a * of *” between Germany and MiddleAges may be easy to mislead the models that there exists a relation of “Contains” between them. The problem can be solved by adding some samples of this kind of expression patterns to the training data.\nS3 is a case that models can predict the entities’ head offset right, but the relational role is\nwrong. LSTM-LSTM treats both “Stephen A. Schwarzman” and “Blackstone Group” as entity E1, and can not find its corresponding E2. Although, LSTM-LSMT–Bias can find the entities pair (E1, E2), it reverses the roles of “Stephen A. Schwarzman” and “Blackstone Group”. It shows that LSTM-LSTM-Bias is able to better on predicting entities pair, but it remains to be improved in distinguishing the relationship between the two entities."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose a novel tagging scheme and investigate the end-to-end models to jointly extract entities and relations. The experimental results show the effectiveness of our proposed method. But it still has shortcoming on the identification of the overlapping relations. In the future work, we will replace the softmax function in the output layer with multiple classifier, so that a word can has multiple tags. In this way, a word can appear in multiple triplet results, which can solve the problem of overlapping relations. Although, our model can enhance the effect of entity tags, the association between two corresponding entities still requires refinement in next works.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Open information extraction from the web",
      "author" : [ "Michele Banko", "Michael J Cafarella", "Stephen Soderland", "Matthew Broadhead", "Oren Etzioni." ],
      "venue" : "IJCAI. volume 7, pages 2670–2676.",
      "citeRegEx" : "Banko et al\\.,? 2007",
      "shortCiteRegEx" : "Banko et al\\.",
      "year" : 2007
    }, {
      "title" : "Named entity recognition with bidirectional lstm-cnns",
      "author" : [ "Jason PC Chiu", "Eric Nichols." ],
      "venue" : "Processings of Transactions of the Association for Computational Linguistics.",
      "citeRegEx" : "Chiu and Nichols.,? 2015",
      "shortCiteRegEx" : "Chiu and Nichols.",
      "year" : 2015
    }, {
      "title" : "Classifying relations by ranking with convolutional neural networks",
      "author" : [ "Cıcero Nogueira et al. dos Santos." ],
      "venue" : "Proceedings of the 53th ACL international conference. volume 1, pages 626–634.",
      "citeRegEx" : "Santos.,? 2015",
      "shortCiteRegEx" : "Santos.",
      "year" : 2015
    }, {
      "title" : "Improved relation extraction with feature-rich compositional embedding models",
      "author" : [ "Matthew R Gormley", "Mo Yu", "Mark Dredze." ],
      "venue" : "Proceedings of the EMNLP.",
      "citeRegEx" : "Gormley et al\\.,? 2015",
      "shortCiteRegEx" : "Gormley et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Knowledgebased weak supervision for information extraction of overlapping relations",
      "author" : [ "Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computation-",
      "citeRegEx" : "Hoffmann et al\\.,? 2011",
      "shortCiteRegEx" : "Hoffmann et al\\.",
      "year" : 2011
    }, {
      "title" : "Bidirectional lstm-crf models for sequence tagging",
      "author" : [ "Zhiheng Huang", "Wei Xu", "Kai Yu." ],
      "venue" : "arXiv preprint arXiv:1508.01991 .",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "EMNLP. volume 3, page 413.",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations",
      "author" : [ "Nanda Kambhatla." ],
      "venue" : "Proceedings of the 43th ACL international conference. page 22.",
      "citeRegEx" : "Kambhatla.,? 2004",
      "shortCiteRegEx" : "Kambhatla.",
      "year" : 2004
    }, {
      "title" : "Investigating lstms for joint extraction of opinion entities and relations",
      "author" : [ "Arzoo Katiyar", "Claire Cardie." ],
      "venue" : "Proceedings of the 54th ACL international conference.",
      "citeRegEx" : "Katiyar and Cardie.,? 2016",
      "shortCiteRegEx" : "Katiyar and Cardie.",
      "year" : 2016
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John Lafferty", "Andrew McCallum", "Fernando Pereira" ],
      "venue" : "In Proceedings of the eighteenth international conference on machine learning, ICML",
      "citeRegEx" : "Lafferty et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of the NAACL international conference.",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Incremental joint extraction of entity mentions and relations",
      "author" : [ "Qi Li", "Heng Ji." ],
      "venue" : "Proceedings of the 52rd Annual Meeting of the Association for Computational Linguistics. pages 402–412.",
      "citeRegEx" : "Li and Ji.,? 2014",
      "shortCiteRegEx" : "Li and Ji.",
      "year" : 2014
    }, {
      "title" : "Joint entity recognition and disambiguation",
      "author" : [ "Gang Luo", "Xiaojiang Huang", "Chin-Yew Lin", "Zaiqing Nie." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing. pages 879–888.",
      "citeRegEx" : "Luo et al\\.,? 2015",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distant supervision for relation extraction without labeled data",
      "author" : [ "Mike Mintz", "Steven Bills", "Rion Snow", "Dan Jurafsky." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL. Association for Computational Linguistics,",
      "citeRegEx" : "Mintz et al\\.,? 2009",
      "shortCiteRegEx" : "Mintz et al\\.",
      "year" : 2009
    }, {
      "title" : "End-to-end relation extraction using lstms on sequences and tree structures",
      "author" : [ "Makoto Miwa", "Mohit Bansal." ],
      "venue" : "Proceedings of the 54rd Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Miwa and Bansal.,? 2016",
      "shortCiteRegEx" : "Miwa and Bansal.",
      "year" : 2016
    }, {
      "title" : "Modeling joint entity and relation extraction with table representation",
      "author" : [ "Makoto Miwa", "Yutaka Sasaki." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. pages 1858–1869.",
      "citeRegEx" : "Miwa and Sasaki.,? 2014",
      "shortCiteRegEx" : "Miwa and Sasaki.",
      "year" : 2014
    }, {
      "title" : "A survey of named entity recognition and classification",
      "author" : [ "David Nadeau", "Satoshi Sekine." ],
      "venue" : "Lingvisticae Investigationes 30(1):3–26.",
      "citeRegEx" : "Nadeau and Sekine.,? 2007",
      "shortCiteRegEx" : "Nadeau and Sekine.",
      "year" : 2007
    }, {
      "title" : "Lexicon infused phrase embeddings for named entity resolution",
      "author" : [ "Alexandre Passos", "Vineet Kumar", "Andrew McCallum." ],
      "venue" : "International Conference on Computational Linguistics. pages 78–86.",
      "citeRegEx" : "Passos et al\\.,? 2014",
      "shortCiteRegEx" : "Passos et al\\.",
      "year" : 2014
    }, {
      "title" : "Cotype: Joint extraction of typed entities and relations with knowledge bases",
      "author" : [ "Xiang Ren", "Zeqiu Wu", "Wenqi He", "Meng Qu", "Clare R Voss", "Heng Ji", "Tarek F Abdelzaher", "Jiawei Han." ],
      "venue" : "Proceedings of the 26th WWW international conference.",
      "citeRegEx" : "Ren et al\\.,? 2017",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2017
    }, {
      "title" : "Utd: Classifying semantic relations by combining lexical and semantic resources",
      "author" : [ "Bryan et al. Rink." ],
      "venue" : "Proceedings of the 5th International Workshop on Semantic Evaluation. pages 256–259.",
      "citeRegEx" : "Rink.,? 2010",
      "shortCiteRegEx" : "Rink.",
      "year" : 2010
    }, {
      "title" : "Joint inference of entities, relations, and coreference",
      "author" : [ "Sameer Singh", "Sebastian Riedel", "Brian Martin", "Jiaping Zheng", "Andrew McCallum." ],
      "venue" : "Proceedings of the 2013 workshop on Automated knowledge base construction. ACM, pages 1–6.",
      "citeRegEx" : "Singh et al\\.,? 2013",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2013
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems. pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Line: Large-scale information network embedding",
      "author" : [ "Jian Tang", "Meng Qu", "Mingzhe Wang", "Ming Zhang", "Jun Yan", "Qiaozhu Mei." ],
      "venue" : "Proceedings of the 24th International Conference on World Wide Web. ACM, pages 1067–1077.",
      "citeRegEx" : "Tang et al\\.,? 2015",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2015
    }, {
      "title" : "Supertagging with lstms",
      "author" : [ "Ashish Vaswani", "Yonatan Bisk", "Kenji Sagae", "Ryan Musa." ],
      "venue" : "Proceedings of the NAACL international conference. pages 232–237.",
      "citeRegEx" : "Vaswani et al\\.,? 2016",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2016
    }, {
      "title" : "Semantic relation classification via convolutional neural networks with simple negative sampling",
      "author" : [ "Kun et al. Xu." ],
      "venue" : "Proceedings of the EMNLP.",
      "citeRegEx" : "Xu.,? 2015a",
      "shortCiteRegEx" : "Xu.",
      "year" : 2015
    }, {
      "title" : "Classifying relations via long short term memory networks along shortest dependency paths",
      "author" : [ "Yan et al. Xu." ],
      "venue" : "Proceedings of EMNLP international conference.",
      "citeRegEx" : "Xu.,? 2015b",
      "shortCiteRegEx" : "Xu.",
      "year" : 2015
    }, {
      "title" : "Joint inference for fine-grained opinion extraction",
      "author" : [ "Bishan Yang", "Claire Cardie." ],
      "venue" : "Proceedings of the 51rd Annual Meeting of the Association for Computational Linguistics. pages 1640–1649.",
      "citeRegEx" : "Yang and Cardie.,? 2013",
      "shortCiteRegEx" : "Yang and Cardie.",
      "year" : 2013
    }, {
      "title" : "Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach",
      "author" : [ "Xiaofeng Yu", "Wai Lam." ],
      "venue" : "Proceedings of the 21th COLING international conference. pages 1399–1407.",
      "citeRegEx" : "Yu and Lam.,? 2010",
      "shortCiteRegEx" : "Yu and Lam.",
      "year" : 2010
    }, {
      "title" : "Relation classification via convolutional deep neural network",
      "author" : [ "Daojian et al. Zeng." ],
      "venue" : "Proceedings of the 25th COLING international conference. pages 2335–2344.",
      "citeRegEx" : "Zeng.,? 2014",
      "shortCiteRegEx" : "Zeng.",
      "year" : 2014
    }, {
      "title" : "Neural models for sequence chunking",
      "author" : [ "Feifei Zhai", "Saloni Potdar", "Bing Xiang", "Bowen Zhou." ],
      "venue" : "Proceedings of the AAAI international conference.",
      "citeRegEx" : "Zhai et al\\.,? 2017",
      "shortCiteRegEx" : "Zhai et al\\.",
      "year" : 2017
    }, {
      "title" : "A neural network framework for relation extraction: Learning entity semantic and relation pattern",
      "author" : [ "Suncong Zheng", "Jiaming Xu", "Peng Zhou", "Hongyun Bao", "Zhenyu Qi", "Bo Xu." ],
      "venue" : "KnowledgeBased Systems 114:12–23.",
      "citeRegEx" : "Zheng et al\\.,? 2016",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2016
    }, {
      "title" : "Natural language question answering over rdf: a graph data driven approach",
      "author" : [ "Lei Zou", "Ruizhe Huang", "Haixun Wang", "Jeffrey Xu Yu", "Wenqiang He", "Dongyan Zhao." ],
      "venue" : "Proceedings of the 2014 ACM SIGMOD international conference on Management",
      "citeRegEx" : "Zou et al\\.,? 2014",
      "shortCiteRegEx" : "Zou et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Different from open information extraction (Open IE) (Banko et al., 2007) whose relation words are extracted from the given sentence, in this task, relation words are extracted from a predefined relation set which may not appear in the given sentence.",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 18,
      "context" : ", extracting the entities (Nadeau and Sekine, 2007) first and then recognizing their relations (Rink, 2010).",
      "startOffset" : 26,
      "endOffset" : 51
    }, {
      "referenceID" : 21,
      "context" : ", extracting the entities (Nadeau and Sekine, 2007) first and then recognizing their relations (Rink, 2010).",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "The results of entity recognition may affect the performance of relation classification and lead to erroneous delivery (Li and Ji, 2014).",
      "startOffset" : 119,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "However, most existing joint methods are feature-based structured systems (Li and Ji, 2014; Miwa and Sasaki, 2014; Yu and Lam, 2010; Ren et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 150
    }, {
      "referenceID" : 17,
      "context" : "However, most existing joint methods are feature-based structured systems (Li and Ji, 2014; Miwa and Sasaki, 2014; Yu and Lam, 2010; Ren et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 150
    }, {
      "referenceID" : 29,
      "context" : "However, most existing joint methods are feature-based structured systems (Li and Ji, 2014; Miwa and Sasaki, 2014; Yu and Lam, 2010; Ren et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 150
    }, {
      "referenceID" : 20,
      "context" : "However, most existing joint methods are feature-based structured systems (Li and Ji, 2014; Miwa and Sasaki, 2014; Yu and Lam, 2010; Ren et al., 2017).",
      "startOffset" : 74,
      "endOffset" : 150
    }, {
      "referenceID" : 16,
      "context" : "In order to reduce the manual work in feature extraction, recently, (Miwa and Bansal, 2016) presents a neural networkbased method for the end-to-end entities and relations extraction.",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 4,
      "context" : "Recently, end-to-end models based on LSTM (Hochreiter and Schmidhuber, 1997) have been successfully applied to various tagging tasks: Named Entity Recognition (Lample et al.",
      "startOffset" : 42,
      "endOffset" : 76
    }, {
      "referenceID" : 11,
      "context" : "Recently, end-to-end models based on LSTM (Hochreiter and Schmidhuber, 1997) have been successfully applied to various tagging tasks: Named Entity Recognition (Lample et al., 2016), CCG Supertagging (Vaswani et al.",
      "startOffset" : 159,
      "endOffset" : 180
    }, {
      "referenceID" : 25,
      "context" : ", 2016), CCG Supertagging (Vaswani et al., 2016), Chunking (Zhai et al.",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 31,
      "context" : ", 2016), Chunking (Zhai et al., 2017) et al.",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 20,
      "context" : "Therefore, we conduct experiments on a public dataset1 which is produced by distant supervision method (Ren et al., 2017) to validate our approach.",
      "startOffset" : 103,
      "endOffset" : 121
    }, {
      "referenceID" : 33,
      "context" : "Entities and relations extraction is an important step to construct a knowledge base, which can be benefit for many NLP tasks (Zou et al., 2014).",
      "startOffset" : 126,
      "endOffset" : 144
    }, {
      "referenceID" : 18,
      "context" : ", named entity recognition (NER) (Nadeau and Sekine, 2007) and relation classification (RC) (Rink, 2010).",
      "startOffset" : 33,
      "endOffset" : 58
    }, {
      "referenceID" : 21,
      "context" : ", named entity recognition (NER) (Nadeau and Sekine, 2007) and relation classification (RC) (Rink, 2010).",
      "startOffset" : 92,
      "endOffset" : 104
    }, {
      "referenceID" : 19,
      "context" : "Classical NER models are linear statistical models, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Passos et al., 2014; Luo et al., 2015).",
      "startOffset" : 123,
      "endOffset" : 162
    }, {
      "referenceID" : 13,
      "context" : "Classical NER models are linear statistical models, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Passos et al., 2014; Luo et al., 2015).",
      "startOffset" : 123,
      "endOffset" : 162
    }, {
      "referenceID" : 1,
      "context" : "Recently, several neural network architectures (Chiu and Nichols, 2015; Huang et al., 2015; Lample et al., 2016) have been successfully applied to NER, which is regarded as a sequential token tagging task.",
      "startOffset" : 47,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "Recently, several neural network architectures (Chiu and Nichols, 2015; Huang et al., 2015; Lample et al., 2016) have been successfully applied to NER, which is regarded as a sequential token tagging task.",
      "startOffset" : 47,
      "endOffset" : 112
    }, {
      "referenceID" : 11,
      "context" : "Recently, several neural network architectures (Chiu and Nichols, 2015; Huang et al., 2015; Lample et al., 2016) have been successfully applied to NER, which is regarded as a sequential token tagging task.",
      "startOffset" : 47,
      "endOffset" : 112
    }, {
      "referenceID" : 21,
      "context" : "Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al.",
      "startOffset" : 104,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : "Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al.",
      "startOffset" : 104,
      "endOffset" : 133
    }, {
      "referenceID" : 26,
      "context" : "Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al., 2016; Zeng, 2014; Xu, 2015b; dos Santos, 2015).",
      "startOffset" : 167,
      "endOffset" : 239
    }, {
      "referenceID" : 32,
      "context" : "Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al., 2016; Zeng, 2014; Xu, 2015b; dos Santos, 2015).",
      "startOffset" : 167,
      "endOffset" : 239
    }, {
      "referenceID" : 30,
      "context" : "Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al., 2016; Zeng, 2014; Xu, 2015b; dos Santos, 2015).",
      "startOffset" : 167,
      "endOffset" : 239
    }, {
      "referenceID" : 27,
      "context" : "Existing methods for relation classification can also be divided into handcrafted feature based methods (Rink, 2010; Kambhatla, 2004) and neural network based methods (Xu, 2015a; Zheng et al., 2016; Zeng, 2014; Xu, 2015b; dos Santos, 2015).",
      "startOffset" : 167,
      "endOffset" : 239
    }, {
      "referenceID" : 20,
      "context" : "Most of the joint methods are feature-based structured systems (Ren et al., 2017; Yang and Cardie, 2013; Singh et al., 2013; Miwa and Sasaki, 2014; Li and Ji, 2014).",
      "startOffset" : 63,
      "endOffset" : 164
    }, {
      "referenceID" : 28,
      "context" : "Most of the joint methods are feature-based structured systems (Ren et al., 2017; Yang and Cardie, 2013; Singh et al., 2013; Miwa and Sasaki, 2014; Li and Ji, 2014).",
      "startOffset" : 63,
      "endOffset" : 164
    }, {
      "referenceID" : 22,
      "context" : "Most of the joint methods are feature-based structured systems (Ren et al., 2017; Yang and Cardie, 2013; Singh et al., 2013; Miwa and Sasaki, 2014; Li and Ji, 2014).",
      "startOffset" : 63,
      "endOffset" : 164
    }, {
      "referenceID" : 17,
      "context" : "Most of the joint methods are feature-based structured systems (Ren et al., 2017; Yang and Cardie, 2013; Singh et al., 2013; Miwa and Sasaki, 2014; Li and Ji, 2014).",
      "startOffset" : 63,
      "endOffset" : 164
    }, {
      "referenceID" : 12,
      "context" : "Most of the joint methods are feature-based structured systems (Ren et al., 2017; Yang and Cardie, 2013; Singh et al., 2013; Miwa and Sasaki, 2014; Li and Ji, 2014).",
      "startOffset" : 63,
      "endOffset" : 164
    }, {
      "referenceID" : 16,
      "context" : "Recently, (Miwa and Bansal, 2016) uses a LSTM-based model to extract entities and relations, which can reduce the manual work.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 7,
      "context" : "It is widely used in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and sequence tagging tasks (Lample et al.",
      "startOffset" : 41,
      "endOffset" : 97
    }, {
      "referenceID" : 23,
      "context" : "It is widely used in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) and sequence tagging tasks (Lample et al.",
      "startOffset" : 41,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : ", 2014) and sequence tagging tasks (Lample et al., 2016; Vaswani et al., 2016).",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 25,
      "context" : ", 2014) and sequence tagging tasks (Lample et al., 2016; Vaswani et al., 2016).",
      "startOffset" : 35,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "For examples, (Lample et al., 2016) use a CRF layers to decode the tag sequence, while (Vaswani et al.",
      "startOffset" : 14,
      "endOffset" : 35
    }, {
      "referenceID" : 25,
      "context" : ", 2016) use a CRF layers to decode the tag sequence, while (Vaswani et al., 2016; Katiyar and Cardie, 2016) apply LSTM layer to produce the tag sequence.",
      "startOffset" : 59,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : ", 2016) use a CRF layers to decode the tag sequence, while (Vaswani et al., 2016; Katiyar and Cardie, 2016) apply LSTM layer to produce the tag sequence.",
      "startOffset" : 59,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "1 Experimental setting Dataset To evaluate the performance of our methods, we use the public dataset NYT 2 which is produced by distant supervision method (Ren et al., 2017).",
      "startOffset" : 155,
      "endOffset" : 173
    }, {
      "referenceID" : 20,
      "context" : "Besides, the ground-truth relation mentions are given and “None” label is excluded as (Ren et al., 2017; Li and Ji, 2014; Miwa and Bansal, 2016) did.",
      "startOffset" : 86,
      "endOffset" : 144
    }, {
      "referenceID" : 12,
      "context" : "Besides, the ground-truth relation mentions are given and “None” label is excluded as (Ren et al., 2017; Li and Ji, 2014; Miwa and Bansal, 2016) did.",
      "startOffset" : 86,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : "Besides, the ground-truth relation mentions are given and “None” label is excluded as (Ren et al., 2017; Li and Ji, 2014; Miwa and Bansal, 2016) did.",
      "startOffset" : 86,
      "endOffset" : 144
    }, {
      "referenceID" : 20,
      "context" : "We create a validation set by randomly sampling 10% data from test set and use the remaining data as evaluation based on (Ren et al., 2017)’s suggestion.",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 14,
      "context" : "The word embeddings used in the encoding part are initialed by running word2vec3 (Mikolov et al., 2013) on NYT training corpus.",
      "startOffset" : 81,
      "endOffset" : 103
    }, {
      "referenceID" : 20,
      "context" : "For the pipelined methods, we follow (Ren et al., 2017)’s settings: The NER results are obtained by CoType (Ren et al.",
      "startOffset" : 37,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : ", 2017)’s settings: The NER results are obtained by CoType (Ren et al., 2017) then several classical relation classification methods are applied to detect the relations.",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 15,
      "context" : "These methods are: (1) DS-logistic (Mintz et al., 2009) is a distant supervised and feature based method, which combines the advantages of supervised IE and unsupervised IE features; (2) LINE (Tang et al.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 24,
      "context" : ", 2009) is a distant supervised and feature based method, which combines the advantages of supervised IE and unsupervised IE features; (2) LINE (Tang et al., 2015) is a network embedding method, which is suitable for arbitrary types of information networks; (3) FCM (Gormley et al.",
      "startOffset" : 144,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : ", 2015) is a network embedding method, which is suitable for arbitrary types of information networks; (3) FCM (Gormley et al., 2015) is a compositional model that combines lexicalized linguistic context and word embeddings for relation extraction.",
      "startOffset" : 110,
      "endOffset" : 132
    }, {
      "referenceID" : 12,
      "context" : "The jointly extracting methods used in this paper are listed as follows: (4) DS-Joint (Li and Ji, 2014) is a supervised method, which jointly extracts entities and relations using structured perceptron on human-annotated dataset; (5) MultiR (Hoffmann et al.",
      "startOffset" : 86,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "The jointly extracting methods used in this paper are listed as follows: (4) DS-Joint (Li and Ji, 2014) is a supervised method, which jointly extracts entities and relations using structured perceptron on human-annotated dataset; (5) MultiR (Hoffmann et al., 2011) is a typical distant supervised method based on multi-instance learning algorithms to combat the noisy training data; (6) CoType (Ren et al.",
      "startOffset" : 241,
      "endOffset" : 264
    }, {
      "referenceID" : 20,
      "context" : ", 2011) is a typical distant supervised method based on multi-instance learning algorithms to combat the noisy training data; (6) CoType (Ren et al., 2017) is a domain independent framework by jointly embedding entity mentions, relation mentions, text features and type labels into meaningful representations.",
      "startOffset" : 137,
      "endOffset" : 155
    }, {
      "referenceID" : 11,
      "context" : "In addition, we also compare our method with two classical end-to-end tagging models: LSTMCRF (Lample et al., 2016) and LSTM-LSTM (Vaswani et al.",
      "startOffset" : 94,
      "endOffset" : 115
    }, {
      "referenceID" : 25,
      "context" : ", 2016) and LSTM-LSTM (Vaswani et al., 2016).",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 20,
      "context" : "It can be seen that our method, LSTM-LSTM-Bias, outperforms all other methods in F1 score and achieves a 3% improvement in F1 over the best method CoType (Ren et al., 2017).",
      "startOffset" : 154,
      "endOffset" : 172
    }, {
      "referenceID" : 10,
      "context" : "Because, LSTM is capable of learning long-term dependencies and CRF (Lafferty et al., 2001) is good at capturing the joint probability of the entire sequence of labels.",
      "startOffset" : 68,
      "endOffset" : 91
    } ],
    "year" : 2017,
    "abstractText" : "Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem. Then, based on our tagging scheme, we study different end-toend models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What’s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.",
    "creator" : "LaTeX with hyperref package"
  }
}