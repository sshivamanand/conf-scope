{
  "name" : "578.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Robust Incremental Neural Semantic Graph Parsing",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049\n050\n051\n052\n053\n054\n055\n056\n057\n058\n059\n060\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\nRobust Incremental Neural Semantic Graph Parsing\nAnonymous ACL submission\nAbstract\nParsing sentences to linguisticallyexpressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focussed almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation."
    }, {
      "heading" : "1 Introduction",
      "text" : "An important goal of Natural Language Understanding (NLU) is to parse sentences to structured, interpretable meaning representations that can be used for query execution, inference and reasoning. While it has recently been shown that end-to-end models outperform traditional pipeline approaches using syntactic or semantic parsers on many NLU tasks, those parses were frequently relatively shallow, e.g. restricted to projective bilexical dependencies.\nIn this paper we focus on the robust parsing of linguistically deep semantic representations. The main representation that we use is Minimal Recursion Semantics (MRS) (Copestake et al., 1995, 2005), which serves as the semantic representation of the English Resource Grammar (ERG) (Flickinger, 2000). The only previous approach to parsing and disambiguating full MRS structures (as opposed to bilexical semantic graphs derived from, but simplifying MRS) were based on the ERG (Toutanova et al., 2005); this approach has high precision but incomplete coverage.\nOur main contribution is to develop a fast and robust parser for full MRS-based semantic graphs. We exploit the power of global conditioning enabled by deep learning to predict linguistically deep graphs incrementally. The model does not have access to the underlying ERG or syntactic structures from which the MRS analyses were originally derived. We develop parsers for two graph-based conversions of MRS, Elementary Dependency Structure (EDS) (Oepen and Lønning, 2006) and Dependency MRS (DMRS) (Copestake, 2009), of which the latter is inter-convertible with MRS.\nAbstract Meaning Representation (AMR) (Banarescu et al., 2013) is a graph-based semantic representation with similar goals to that of MRS. Apart from differences in the choice of which linguistic phenomena are annotated, MRS is a compositional representation explicitly coupled with the syntactic structure of the sentence, while AMR does not assume compositionality or alignment with the sentence structure. AMR parsing has recently received a lot of attention, but the size of the available training data is still relatively small, and inter-annotator agreement has been shown to be relatively low, placing on upper bound of 83% F1 on the expected parser performance. We apply our model to AMR parsing by introducing struc-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nture (alignments and distinguishing between lexical and non-lexical concepts) that is present explicitly in MRS but not in AMR.\nParsers based on RNNs have achieved state-ofthe-art performance for dependency parsing (Dyer et al., 2015; Kiperwasser and Goldberg, 2016) and constituency parsing (Vinyals et al., 2015b; Dyer et al., 2016; Cross and Huang, 2016b). However, one of the main advantages of deep learning is the ability to make predictions conditioned on the unbounded contexts encoded with RNNs; this enables us to predict more complex structures than have been possible previously, without increasing algorithmic complexity. One of the main reasons for the prevalence of dependency parsing, including semantic dependency parsing (Ivanova et al., 2013), is that it can be performed with efficient and well-understood algorithms. Therefore deep learning gives us the opportunity to perform robust, linguistically deep parsing.\nOur parser is a transition-based model for parsing semantic graphs. However, instead of generating arcs over an ordered, fixed set of nodes (the words in the sentence), we generate the nodes, including their labels and alignments to the input tokens, jointly with the transition actions. We use a variant of the arc-eager transition-system that is able to parse graphs and non-planar dependencies. The sentence is encoded with a bidirectional RNN. The transition sequence, seen as a graph linearization, can be predicted with any encoder-decoder model, but we show that using hard attention, predicting the alignment with a pointer network and conditioning explicitly on stack-based features improves performance. In order to deal with data sparsity candidate lemmas are predicted as a preprocessing step, so that the RNN decoder predicts unlexicalized predicates.\nWe evaluate our parser on DMRS, EDS and AMR graphs. We show that our model architecture improves performance from 79.68% to 84.16% F1 over an attention-based encoderdecoder baseline. Although the model is less accurate that a high-precision grammar-based parser on a test set of sentences parsable by that grammar, our model is an order of magnitude faster due to incremental prediction and a GPU batch processing implementation of the transition system. On AMR parsing our model obtains 60.11% Smatch, an improvement of 8% over an existing neural AMR parser."
    }, {
      "heading" : "2 Deep Meaning Representations",
      "text" : "We define a common framework for semantic graphs, in which we can place both MRSbased graph representations (DMRS and EDS) and AMR. In this framework sentence meaning is represented with rooted, labelled, connected, directed graphs. An example graph is visualized in Figure 1. Node labels are referred to as predicates (concepts in AMR) and edge labels as arguments (AMR relations). In addition, a special class of node modifiers, constants, are used to denote the string values of named entities and numbers (including date and time expressions). Every node is aligned to a token or a continuous span of tokens in the sentence the graph corresponds to.\nMinimal Recursion Semantics (MRS) is a framework for computational semantics that can be used for parsing or generation (Copestake et al., 2005). The main units of MRS are elementary predications (EPs). An EP consists of a relation (referred to as a predicate), usually corresponding to a single lexeme, and its arguments. Quantification is expressed by relations, not logical operations like ∃ or ∀. One of the distinguishing characteristics of MRS its support for scope underspecification; multiple scope-resolved logical representations can be derived from one MRS structure. MRS was designed to be integrated\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nwith feature-based grammars, and has been implemented in English within the framework of Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) in the English Resource Grammar (ERG) (Flickinger, 2000).\nMRS can be converted without loss to variablefree dependency graphs, called Dependency MRS (DMRS) (Copestake, 2009; Copestake et al., 2016). A similar graph-based conversion is Elementary Dependency Structures (EDS) (Oepen and Lønning, 2006), which drops the scopeunderspecification machinery, primarily simplifying edge labels. Figure 1 illustrates an EDS graph.\nMRS makes an explicit the distinction between lexical and non-lexical predicates (lexical predicates are prefixed by an underscore). Lexical predicates consist of a lemma followed by a coarse part-of-speech tag and an optional sense label. Predicates absent from the ERG lexicon are represented by their surface forms, POS tags and an unknown sense label. All predicates are annotated with an alignment to the character-span of the (untokenized) input sentence. We convert the character-level spans given by MRS to token-level spans. Lexical predicates usually align with the span of the token(s) they represent, while nonlexical predicates can span longer segments. In full MRS predicates are annotated with a set of morphosyntactic features consisting of attributevalue pairs, but we do not currently model these features.\nAMR (Banarescu et al., 2013) graphs can be represented in the same framework, despite a number of linguistic differences with MRS. However, information annotated explicitly in MRS is considered as latent in AMR. This include alignments, as well as distinguishing between lexical and nonlexical concepts. AMR predicates are based on PropBank (Palmer et al., 2005), annotated as lemmas plus sense labels, but they form only a subset of concepts. Other concepts are either English words or special keywords, and can correspond to overt lexemes in some cases but not others."
    }, {
      "heading" : "3 Incremental Graph Parsing",
      "text" : "We parse sentences to their meaning representations by incrementally predicting semantic graphs together with their alignments. Let e = e1, e2, . . . , eI be a tokenized English sentence, t = t1, t2, . . . , tJ a sequential representation of its graph derivation and a = a1, a2, . . . , aJ an align-\n:root( <1> _v_1 :ARG1( <0> named_CARG :BV-of( <0> proper_q ) )\n:ARG2 <5> _v_1 :ARG1( <3> _n_1\n:BV-of ( <2> _q ) ) :ARG2( <5> pron\n:BV-of ( <5> pronoun_q ) ) )\nFigure 2: A top-down linearization of the EDS graph in Figure 1, using unlexicalized predicates.\nment sequence consisting of integers in the range 1, . . . , I . We model the conditional distribution p(t,a|e) which decomposes as\nJ∏ j=1 p(aj |(a, t)1:j−1, e)p(tj |a1:j , t1:j−1, e).\nWe also predict the end-of-span alignments as a seperate sequence a(e)."
    }, {
      "heading" : "3.1 Top-down linearization",
      "text" : "We now consider how to linearize the semantic graphs, before defining the neural models to parameterize the parser in section 4. The first approach is to linearize a graph as the pre-order traversal of its spanning tree, starting at a designated root node (see Figure 2). Variants of this approach has been proposed for neural constituency parsing (Vinyals et al., 2015b), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016) and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017).\nIn the linearization, labels of edges whose direction are reversed in the spanning tree are marked by adding -of. Edges not included in the spanning tree, referred to as reentrancies, are represented with special with edges whose dependents are dummy nodes pointing back to the original nodes. Our potentially lossy representation represents the edge by repeating the label and alignment of the dependen node, which is recovered heuristically. The alignment does not influence the order of the nodes in this linearization."
    }, {
      "heading" : "3.2 Arc-eager parsing",
      "text" : "Figure 1 shows that the semantic graphs we work with can also be interpreted as dependency graphs, as nodes are aligned to sentence tokens. An approach to predicting dependency graphs incrementally that has been used extensively is transition-\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nbased parsing (Nivre, 2008). We apply a variant of the arc-eager transition system that has been proposed for graph (as opposed to tree) parsing (Titov et al., 2009; Gómez-Rodrı́guez and Nivre, 2010) to derive a transition-based parser for deep semantic graphs. In dependency parsing the sentence tokens also act as nodes in the graph, but here we need to generate the nodes incrementally as the transitionsystem proceeds, conditioning the generation on the given sentence. Damonte et al. (2016) proposed an arc-eager AMR parser, but the definition of their system is more narrowly defined for AMR graphs.\nThe transition system consists of a stack of graph nodes being processed and a buffer, holding a single node at a time. The transition actions are shift, left-arc, right-arc and reduce. An example transition sequence is given in Figure 3, together with the stack and buffer after each step. The shift transition moves the element on the buffer to the top of the stack, and generates a predicate and its alignment as the next node on the buffer. Left-arc and right-arc actions add labeled arcs between the buffer and stack top (for DMRS a transition for undirected arcs is included), but do not change the state of the stack or buffer. Finally, reduce pops the top element from the stack, and predicts its end-ofspan alignment if included in the representation. To predict non-planar arcs, we add another transition, which we call cross-arc, which first predicts the stack index of a node which is not on top of the stack, adding an arc between the head of the buffer and that node. Another special transition designates the buffer node as the root.\nTo derive an oracle for this transition system, it is necessary to determine the order in which the nodes are generated. We consider two different orderings. The first is that of an in-order traversal of the spanning tree, where the node order is determined by the alignment. This leads to a linearization where the only non-planar arcs are reentrancies. The second is to make the ordering nondecreasing with respect to the alignments, while for nodes with the same alignment following the in-order ordering. In an arc-eager oracle arcs are added greedily, while a reduce action can either be performed as soon as the stack top node has been connected to all its dependents, or delayed until it has to reduce to allow the correct parse tree to be formed. In our model the oracle delays reduce, where possible, until the end alignment of\nthe stack top node spans the node on the buffer. As the span end alignments often cover phrases that they head (e.g. for quantifiers) this gives a natural interpretation to predicting the span end together with the reduce action."
    }, {
      "heading" : "3.3 Lemma prediction",
      "text" : "Lexical predicates in MRS consist of a lemma followed by a sense label. Therefore if we can predict the alignment of a graph node as well as the lemma of the aligned word, the prediction required by the decoder can be simplified to only predicting the sense label. We extract a dictionary mapping words to lemmas from the ERG lexicon, which map words to their possible predicates. In combination with a lemmatizer, this can be used to predict a candidate lemma for each token. The same approach is applied to predict constants, together with additional normalizations, e.g. to map numbers to digit strings.\nWe use the Stanford CoreNLP toolkit (Manning et al., 2014) to tokenize and lemmatize sentences, and tag tokens with the Stanford Named Entity Recognizer (Finkel et al., 2005). For AMR parsing there is not annotated distinction between lexical and non-lexical tokens, so we automaticallyobtained alignments and the graph structure to classifiy concepts as lexical or non-lexical. The lexicon is restricted to Propbank (Palmer et al., 2005) predicates, so for other concepts we extract a lexicon from the training data."
    }, {
      "heading" : "4 Encoder-Decoder Models",
      "text" : ""
    }, {
      "heading" : "4.1 Sentence encoder",
      "text" : "The sentence e is encoded with a bidirectional RNN. We use a standard LSTM architecture without peephole connections (Jozefowicz et al., 2015). For every token e we embed its word, POS tag and named entity (NE) tag as vectors xw, xt and xn, respectively.\nThe embeddings are concatenated and passed through a linear transformation,\ng(e) =W (x)[xw;xt;xn] + b x\nsuch that g(e) has the same dimension as the LSTM. We don’t embed word lemmas separately for the unlexicalized representations. Each input position i is represented by a hidden state hi, which is the concatenation of its forward and backward LSTM state vectors.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nAction Stack Buffer Arc added init(0, named CARG) [ ] (0, 0, named CARG) - sh(0, proper q) [(0, 0, named CARG)] (1, 0, proper q) - la(BV) [(0, 0, named CARG)] (1, 0, proper q) (1, BV, 0) sh(1, v 1) [(0, 0, named CARG), (1, 0, proper q)] (2, 1, v 1) - re [(0, 0, named CARG)] (2, 1, v 1) - la(ARG1) [(0, 0, named CARG)] (2, 1, v 1) (2, ARG1, 0)\nFigure 3: First part of the transition sequence for parsing the graph in Figure 1. The transitions are shift (sh), reduce (re), left arc (la) and right arc (ra). The action taken at each step is given, along with the state of the stack and buffer after the action is applied, and any arcs added. Shift transitions generate the alignment and predicate of the next graph node. Items on the stack and buffer have the form (node index, alignment, predicate label), and arcs are of the form (head index, argument label, dependent index)."
    }, {
      "heading" : "4.2 Hard attention decoder",
      "text" : "We model the alignment of graph nodes to sentence tokens, a, as a random variable. For the arceager model, aj corresponds to the alignment of the node of the buffer after action tj is executed. The distribution of tj is over all transitions and predicate predictions (for shifts), predicted with a single softmax.\nLet sj be the RNN decoder hidden state at output position j. We initialize s0 with the final state of the backward encoder LSTM. The alignment is predicted with a pointer network (Vinyals et al., 2015a).\nThe logits are computed with an MLP scoring the decoder hidden state against each of the encoder hidden states (for i = 1, . . . , I),\nuij = v T tanh(W (1)hi +W (2)sj).\nThe alignment distribution is then estimated by\np(aj = i|a1:j−1, t1:j−1, e) = softmax(uij).\nTo predict the next transition ti, the output vector is conditioned on the encoder state vector haj , corresponding to the alignment:\noj =W (3)sj +W (4)haj vj = R (d)oj + b (d),\nwhere R(d) and b(d) are the output representation matrix and bias vector, respectively.\nThe transition distribution is then given by\np(tj |a1:j , t1:j−1, e) = softmax(vj).\nLet v(t) be the embedding of decoder symbol t. The RNN state at the next time-step is computed\nas\ndj+1 =W (5)v(tj) +W (6)haj\nsj+1 = RNN(dj+1, sj).\nThe end-of-span alignment a(e)j for MRS-based graphs is predicted with another pointer network. The end alignment of a token is predicted only when a node is reduced from the stack, therefore this alignment is not observed at each time-step; it is also not fed back into the model.\nThe hard attention approach, based on supervised alignments, can be contrasted to soft attention, which learns to attend over the input without supervision. The attention is computed as with hard attention, as αij = softmax(u i j). However instead of making a hard selection, a weighted average over the encoder vectors is computed as qj = ∑i=I i=1 α i jhi. This vector is used instead of haj for prediction and feeding to the next timestep."
    }, {
      "heading" : "4.3 Stack-based model",
      "text" : "We extend the hard attention model to include features based on the transition system stack. Elements on the stack can be represented by the encoder biLSTM representations corresponding to the tokens they are aligned to. We include vectors for the top of the stack and the buffer, the latter which is predicted by the hard attention. This approach is similar to the features proposed by Kiperwasser and Goldberg (2016) and Cross and Huang (2016a) for dependency parsing. The layer that computes the output vector is extended to\noj =W (3)sj +W (4)haj +W (7)hst0\nwhere st0 is the sentence alignment index of the top element on the buffer. The input layer to the\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nnext RNN time-step is similarly extended to\ndj+1 =W (5)v(tj) +W (6)hbuf +W (8)hst0 ,\nwhere buf is the buffer alignment after tj is executed.\nOur implementation of the stack-based model enables batch processing in static computation graphs, similar to Bowman et al. (2016). We maintain a stack of alignment indexes for each element in the batch, which is updated inside the computation graph after each parsing action.\nWe perform greedy decoding. For the stackbased model we ensure that if the stack is empty, the next transition predicted has to be shift. For the other models we ensure that the output is wellformed during post-processing by robustly skipping over out-of-place symbols or inserting missing ones."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Data",
      "text" : "DeepBank (Flickinger et al., 2012) is HPSG and MRS annotation of the Penn Treebank Wall Street Journal (WSJ) corpus text, developed following an approach known as dynamic treebanking (Oepen et al., 2004), that couples treebank annotation with grammar development, in the case of the ERG. This approach has been shown to lead to high inter-annotator agreement: 0.94 against 0.71 for AMR (Bender et al., 2015). Parses are only provided for sentences for which the ERG has an analysis acceptable to the annotator – this means that we cannot evaluate parsing accuracy for sentences which the ERG cannot parse (approximately 15% of the original corpus).\nWe use Deepbank version 1.1, corresponding to ERG 12141, following the suggested split of sections 0 to 19 as training data data, 20 for development and 21 for testing. The gold-annotated training data consists of 35,315 sentences. We use the pyDelphin library2 and software provided with the ERG to extract DMRS and EDS graphs.\nFor AMR parsing we use LDC2015E86, the dataset released for the SemEval 2016 AMR parsing Shared Task (May, 2016). This data includes newswire, weblog and discussion forum text. The training set has 16,144 sentences. We obtain align-\n1http://svn.delph-in.net/erg/tags/ 1214/\n2https://github.com/delph-in/pydelphin\nments using the rule-based JAMR aligner (Flanigan et al., 2014)."
    }, {
      "heading" : "5.2 Evaluation",
      "text" : "Dridan and Oepen (2011) proposed an evaluation metric called Elementary Dependency Matching (EDM) for MRS graphs. EDM computes the F1scores of tuples of predicates and arguments. A predicate tuples consist of the label and character span of a predicate, while an argument tuple consist of the character spans of the head and dependent of the relation, together with the argument label. In order to tolerate tokenization differences with respect to punctuation, we allow span pairs whose ends differ by 1 character to be matched.\nEDM can be contrasted with the Smatch metric (Cai and Knight, 2013) for evaluating AMR graphs. This evaluation does not rely on sentence alignments; instead it performs inference over graph alignments to estimate the maximum F1-score obtainable from a 1-1 matching between the predicted and gold graph nodes."
    }, {
      "heading" : "5.3 Model Setup",
      "text" : "Our models are implemented in TensorFlow (Abadi et al., 2015). For training we use Adam (Kingma and Ba, 2015) with learning rate 0.01 and batch-size 64. Gradients norms are clipped to 5.0 (Pascanu et al., 2013). We use single-layer LSTMs with dropout of 0.3 (tuned on the development set) on input and output connections. We use encoder and decoder embeddings of size 256, and POS and NE tag embeddings of size 32, For DMRS and EDS graphs the hidden units size is set to 256, for AMR it is 128. This configuration, found using grid search and heuristic search within the range of models that fit into a single GPU, gave the best performance on the development set under multiple graph linearizations. Encoder word embeddings are initialized (in the first 100 dimensions) with pre-trained order-sensitive embeddings (Ling et al., 2015). Singletons in the encoder input is replaced with an unknown word symbol with probability 0.5 for each iteration."
    }, {
      "heading" : "5.4 MRS parsing results",
      "text" : "We compare different linearizations and model architectures for parsing DMRS on the development data, showing that our approach is more accurate than baseline neural approaches. We report\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nModel EDM EDM-Pred EDM-Arg TD lex 81.44 85.20 76.87 TD unlex 81.72 85.59 77.04 AE lex 81.35 85.79 76.02 AE delex 82.56 86.76 77.54\nTable 1: DMRS development set results for attention-based encoder-decoder models with alignments encoded in the linearization, for topdown (TD) and arc-eager (AE) linearizations, and lexicalized and unlexicalized predicate prediction.\nModel EDM EDM-Pred EDM-Arg TD soft 81.53 85.32 76.94 TD hard 82.75 86.37 78.37 AE hard 84.65 87.77 80.85 AE stack 85.28 88.38 81.51\nTable 2: DMRS development set results of pointer-augmented encoder-decoder models with hard and soft attention architectures.\nEDM F1 scores, as well as EDM scores for predicate (EDM-Pred) and argument (EDM-Arg) prediction.\nFirst we report results using a vanilla encoderdecoder model (Table 1). We compare the topdown and arc-eager linearizations, as well as the effect of delexicalizing the predicates (factorizing lemmas out of of the predicate labels and predicting them separately.) In both cases constants are predicted with a dictionary lookup based on the predicted spans; for predicates not in the lexicon an unknown-label is predicted and the words and POS tags are recovered during post-processing.\nThe arc-eager unlexicalized linearization gives the best performance, down linearization, even though the model has to learn to model the transition stack inside the recurrent states without any supervision of the semantics of transition actions. The unlexicalized models are more accurate, mostly due to their ability to generalize to sparse or unseen predicates occurring in the lexicon. For the arc-eager representation, the oracle EDM is 99% for the lexicalized representation and 98.06% for the delexicalized representation. The remaining errors are due to discrepancies between the tokenization used by our system and the ERG tokenization. The unlexicalized models are also faster to train, as the decoder’s output vocabulary is much smaller, reducing the expense of computing softmaxes over large vocabularies.\nNext we consider models that predict the alignments with pointer networks, contrasting soft and hard attention models (Table 2). The results show that the arc-eager models performs better than those based on top-down representation. For the arc-eager model we use hard attention, due to the natural interpretation of the alignment prediction corresponding to the transition system. The arc-eager stack-based architecture improves further over the model that purely relies on the hard attention.\nWe compared the effect of different orderings of the predicates for the arc-eager model: The monotone ordering (with non-decreasing alignments) performs 0.44 EDM better that the in-order ordering, despite having to parse more non-planar dependencies. We also compare the predicate prediction against a hard attention model that only predicts predicates (in monotone order) together with their start spans. This model obtains 91.36% F1 on predicates together with their start spans with the delexicalized model, compared to 88.22% for lexicalized predicates and 91.65% for the full parsing model.\nWe present test set results for various metrics in Table 3. We compare the performance of our neural baseline and stack-based decoders with the ACE3 ERG-based parser. Another approach to robust MRS parsing has previously been proposed (Zhang et al., 2014), but no comparable results or implementation is available.\nDespite the promising performance of the model there is still a gap between the accuracy of our parser and ACE. One reason for this is that the test set sentences will arguably be easier for ACE to parse as their choice was restricted by the grammar that ACE uses. EDM metrics excluding endspan prediction (-Start) show that our parser has relatively more difficulty in parsing end-span predictions than the grammar-based parser.\n3http://sweaglesw.org/linguistics/ace/\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nModel ArcEager ACE EDM 85.48 89.58 Smatch 86.50 93.52 EDM-Pred 88.14 91.82 EDM-Arg 82.20 86.92\nTable 4: EDS parsing test set results.\nModel Concept F1 Smatch TD no point 65.55 57.95 TD soft 66.44 59.36 TD soft delex 67.07 59.88 AE hard delex 72.86 59.83 AE stack delex 73.69 61.21\nTable 5: Development set results for AMR parsing. All the models are pointer-based, except where indicated otherwise.\nWe also evaluate the speed of our model compared with ACE. For the unbatched version of our model, the stack-based parser parses 41.63 tokens per second, while the baseline with linearized spans parses 31.65 tokens per second. The batched (stack-based) implementation parses 529.42 tokens per second, using a batch size of 128. In comparison, the setting of ACE for which we reported accuracies parses 7.47 tokens per second. By restricting the memory usage of ACE, which restricts its coverage, we see that ACE can parse 11.07 tokens per second at 87.7% coverage, and 15.11 tokens per second at 77.8% coverage.\nFinally we report results for parsing EDS (Table 4) The EDS parsing task is slightly simpler than DMRS, due to the absence of label and structural annotations that allows for the recovery of MRS, including scope underspecification. These additional labels are harder to predict without access to a grammar."
    }, {
      "heading" : "5.5 AMR parsing",
      "text" : "We now apply the same approach to AMR parsing. Results on the development set are given in Table 5. The arc-eager-based models again give better performance, mainly due to improved concept prediction accuracy. However, concept prediction remains a weakness of the model; Damonte et al. (2016) reports that state-of-the-art AMR parsers score between 79% and 83% on concept prediction. In contrast to DMRS parsing, the in-order linearization gives better results than the monotone one. We hypothesize that this is due to the\nnon-compositional nature of AMR. We show that even our neural baseline outperforms existing results for neural AMR parsing. The arc-eager model does not perform better than the top-down linearization, which we hypothesize is due to noise in the automatically-obtained alignments.\nWe report test set results on LDC2015E86 test set (Table 6). Our best neural model outperforms the baseline JAMR parser (Flanigan et al., 2014), but still lags behind the performance of state-ofthe-art AMR parsers such as CAMR (Wang et al., 2016). However these models make extensive use of external resources, including syntax trees and semantic role labelling. We see that our attentionbased encoder-decoder model already performs better than previous sequence-to-sequence AMR parsers (Barzdins and Gosko, 2016; Peng et al., 2017), and the arc-eager model boost accuracy further. Our model also outperforms a Synchronous Hyperedge Replacement Grammar model (Peng and Gildea, 2016) which is comparable as it does not make extensive use of external resources."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper we advance the state of parsing by employing deep learning techniques to parse sentence to linguistically expressive semantic representations that have not previously been parsed in an end-to-end fashion. We presented a robust, wide-coverage parser for MRS that is faster than existing parsers and amenable to batch processing. We believe that there are many future avenues to explore to further increase the accuracy of such parsers, including different training objectives, more structured architectures and semisupervised learning.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org",
      "author" : [ "sudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "sudevan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "sudevan et al\\.",
      "year" : 2015
    }, {
      "title" : "Abstract meaning representation for sembanking",
      "author" : [ "Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider." ],
      "venue" : "Proceedings of the",
      "citeRegEx" : "Banarescu et al\\.,? 2013",
      "shortCiteRegEx" : "Banarescu et al\\.",
      "year" : 2013
    }, {
      "title" : "Riga at semeval-2016 task 8: Impact of smatch extensions and character-level neural translation on AMR parsing accuracy",
      "author" : [ "Guntis Barzdins", "Didzis Gosko." ],
      "venue" : "Proceedings of SemEval.",
      "citeRegEx" : "Barzdins and Gosko.,? 2016",
      "shortCiteRegEx" : "Barzdins and Gosko.",
      "year" : 2016
    }, {
      "title" : "Layers of interpretation: On grammar and compositionality",
      "author" : [ "Emily M Bender", "Dan Flickinger", "Stephan Oepen", "Woodley Packard", "Ann Copestake." ],
      "venue" : "Proceedings of the 11th International Conference on Computational Semantics. pages 239–",
      "citeRegEx" : "Bender et al\\.,? 2015",
      "shortCiteRegEx" : "Bender et al\\.",
      "year" : 2015
    }, {
      "title" : "A fast unified model for parsing and sentence understanding",
      "author" : [ "Samuel R. Bowman", "Jon Gauthier", "Abhinav Rastogi", "Raghav Gupta", "Christopher D. Manning", "Christopher Potts." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the As-",
      "citeRegEx" : "Bowman et al\\.,? 2016",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2016
    }, {
      "title" : "Smatch: An evaluation metric for semantic feature structures",
      "author" : [ "Shu Cai", "Kevin Knight." ],
      "venue" : "ACL (2).",
      "citeRegEx" : "Cai and Knight.,? 2013",
      "shortCiteRegEx" : "Cai and Knight.",
      "year" : 2013
    }, {
      "title" : "Invited talk: Slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go",
      "author" : [ "Ann Copestake." ],
      "venue" : "Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009). Association for",
      "citeRegEx" : "Copestake.,? 2009",
      "shortCiteRegEx" : "Copestake.",
      "year" : 2009
    }, {
      "title" : "Resources for building applications with dependency minimal recursion semantics",
      "author" : [ "Muszyska." ],
      "venue" : "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016).",
      "citeRegEx" : "Muszyska.,? 2016",
      "shortCiteRegEx" : "Muszyska.",
      "year" : 2016
    }, {
      "title" : "Translation using minimal recursion semantics",
      "author" : [ "Ann Copestake", "Dan Flickinger", "Rob Malouf", "Susanne Riehemann", "Ivan Sag." ],
      "venue" : "In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation.",
      "citeRegEx" : "Copestake et al\\.,? 1995",
      "shortCiteRegEx" : "Copestake et al\\.",
      "year" : 1995
    }, {
      "title" : "Minimal recursion semantics: An introduction",
      "author" : [ "Ann Copestake", "Dan Flickinger", "Carl Pollard", "Ivan A Sag." ],
      "venue" : "Research on Language and Computation 3(2-3):281–332.",
      "citeRegEx" : "Copestake et al\\.,? 2005",
      "shortCiteRegEx" : "Copestake et al\\.",
      "year" : 2005
    }, {
      "title" : "Incremental parsing with minimal features using bi-directional lstm",
      "author" : [ "James Cross", "Liang Huang." ],
      "venue" : "The 54th Annual Meeting of the Association for Computational Linguistics. page 32.",
      "citeRegEx" : "Cross and Huang.,? 2016a",
      "shortCiteRegEx" : "Cross and Huang.",
      "year" : 2016
    }, {
      "title" : "Span-based constituency parsing with a structure-label system and provably optimal dynamic oracles",
      "author" : [ "James Cross", "Liang Huang." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association",
      "citeRegEx" : "Cross and Huang.,? 2016b",
      "shortCiteRegEx" : "Cross and Huang.",
      "year" : 2016
    }, {
      "title" : "An incremental parser for abstract meaning representation",
      "author" : [ "Marco Damonte", "Shay B. Cohen", "Giorgio Satta." ],
      "venue" : "CoRR abs/1608.06111. http://arxiv.org/abs/1608.06111.",
      "citeRegEx" : "Damonte et al\\.,? 2016",
      "shortCiteRegEx" : "Damonte et al\\.",
      "year" : 2016
    }, {
      "title" : "Language to logical form with neural attention",
      "author" : [ "Li Dong", "Mirella Lapata." ],
      "venue" : "CoRR abs/1601.01280. http://arxiv.org/abs/1601.01280.",
      "citeRegEx" : "Dong and Lapata.,? 2016",
      "shortCiteRegEx" : "Dong and Lapata.",
      "year" : 2016
    }, {
      "title" : "Parser evaluation using elementary dependency matching",
      "author" : [ "Rebecca Dridan", "Stephan Oepen." ],
      "venue" : "Proceedings of the 12th International Conference on Parsing Technologies. Association for Computational Linguistics, pages 225–230.",
      "citeRegEx" : "Dridan and Oepen.,? 2011",
      "shortCiteRegEx" : "Dridan and Oepen.",
      "year" : 2011
    }, {
      "title" : "Transition-based dependency parsing with stack long short-term memory",
      "author" : [ "Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith." ],
      "venue" : "Proceedings of ACL. Association for Computational",
      "citeRegEx" : "Dyer et al\\.,? 2015",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent neural network grammars",
      "author" : [ "Chris Dyer", "Adhiguna Kuncoro", "Miguel Ballesteros", "Noah A. Smith." ],
      "venue" : "Proceedings of NAACL.",
      "citeRegEx" : "Dyer et al\\.,? 2016",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2016
    }, {
      "title" : "Incorporating non-local information into information extraction systems by Gibbs sampling",
      "author" : [ "Jenny Rose Finkel", "Trond Grenager", "Christopher Manning." ],
      "venue" : "Proceedings of ACL. Association for Computational Linguis-",
      "citeRegEx" : "Finkel et al\\.,? 2005",
      "shortCiteRegEx" : "Finkel et al\\.",
      "year" : 2005
    }, {
      "title" : "A discriminative graph-based parser for the abstract meaning representation",
      "author" : [ "Jeffrey Flanigan", "Sam Thomson", "Jaime G. Carbonell", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proceedings of ACL. pages 1426– 1436. http://aclweb.org/anthology/P/P14/P14-",
      "citeRegEx" : "Flanigan et al\\.,? 2014",
      "shortCiteRegEx" : "Flanigan et al\\.",
      "year" : 2014
    }, {
      "title" : "On building a more effcient grammar by exploiting types",
      "author" : [ "Dan Flickinger." ],
      "venue" : "Natural Language Engineering 6(01):15–28.",
      "citeRegEx" : "Flickinger.,? 2000",
      "shortCiteRegEx" : "Flickinger.",
      "year" : 2000
    }, {
      "title" : "Deepbank",
      "author" : [ "Dan Flickinger", "Yi Zhang", "Valia Kordoni." ],
      "venue" : "a dynamically annotated treebank of the wall street journal. In Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories. pages 85–96.",
      "citeRegEx" : "Flickinger et al\\.,? 2012",
      "shortCiteRegEx" : "Flickinger et al\\.",
      "year" : 2012
    }, {
      "title" : "A transition-based parser for 2-planar dependency structures",
      "author" : [ "Carlos Gómez-Rodrı́guez", "Joakim Nivre" ],
      "venue" : "In Proceedings of the 48th Annual Meeting of the Association",
      "citeRegEx" : "Gómez.Rodrı́guez and Nivre.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gómez.Rodrı́guez and Nivre.",
      "year" : 2010
    }, {
      "title" : "On different approaches to syntactic analysis into bi-lexical dependencies",
      "author" : [ "Angelina Ivanova", "Stephan Oepen", "Rebecca Dridan", "Dan Flickinger", "Lilja Øvrelid." ],
      "venue" : "an empirical comparison of direct, PCFG-based, and HPSG-based parsers. In Proceed-",
      "citeRegEx" : "Ivanova et al\\.,? 2013",
      "shortCiteRegEx" : "Ivanova et al\\.",
      "year" : 2013
    }, {
      "title" : "Data recombination for neural semantic parsing",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computa-",
      "citeRegEx" : "Jia and Liang.,? 2016",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2016
    }, {
      "title" : "An empirical exploration of recurrent network architectures",
      "author" : [ "Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever." ],
      "venue" : "Proceedings of the 32nd International Conference on Machine Learning (ICML-15). pages 2342–2350.",
      "citeRegEx" : "Jozefowicz et al\\.,? 2015",
      "shortCiteRegEx" : "Jozefowicz et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of ICLR. http://arxiv.org/abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Simple and accurate dependency parsing using bidirectional lstm feature representations",
      "author" : [ "Eliyahu Kiperwasser", "Yoav Goldberg." ],
      "venue" : "Transactions of the Association for Computational Linguistics 4:313–327.",
      "citeRegEx" : "Kiperwasser and Goldberg.,? 2016",
      "shortCiteRegEx" : "Kiperwasser and Goldberg.",
      "year" : 2016
    }, {
      "title" : "Two/too simple adaptations of word2vec for syntax problems",
      "author" : [ "Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso." ],
      "venue" : "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "citeRegEx" : "Ling et al\\.,? 2015",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "Christopher D. Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky." ],
      "venue" : "Association for Computational Linguistics",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "Semeval-2016 task 8: Meaning representation parsing",
      "author" : [ "Jonathan May." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). Association for Computational Linguistics, San Diego, California, pages",
      "citeRegEx" : "May.,? 2016",
      "shortCiteRegEx" : "May.",
      "year" : 2016
    }, {
      "title" : "Algorithms for deterministic incremental dependency parsing",
      "author" : [ "Joakim Nivre." ],
      "venue" : "Computational Linguistics 34(4):513–553.",
      "citeRegEx" : "Nivre.,? 2008",
      "shortCiteRegEx" : "Nivre.",
      "year" : 2008
    }, {
      "title" : "Lingo redwoods",
      "author" : [ "Stephan Oepen", "Dan Flickinger", "Kristina Toutanova", "Christopher D. Manning." ],
      "venue" : "Research on Language and Computation 2(4):575–596. https://doi.org/10.1007/s11168-0047430-4.",
      "citeRegEx" : "Oepen et al\\.,? 2004",
      "shortCiteRegEx" : "Oepen et al\\.",
      "year" : 2004
    }, {
      "title" : "Discriminant-based MRS banking",
      "author" : [ "Stephan Oepen", "Jan Tore Lønning." ],
      "venue" : "Proceedings of the 5th International Conference on Language Resources and Evaluation. pages 1250–1255.",
      "citeRegEx" : "Oepen and Lønning.,? 2006",
      "shortCiteRegEx" : "Oepen and Lønning.",
      "year" : 2006
    }, {
      "title" : "The proposition bank: An annotated corpus of semantic roles",
      "author" : [ "Martha Palmer", "Daniel Gildea", "Paul Kingsbury." ],
      "venue" : "Computational linguistics 31(1):71– 106.",
      "citeRegEx" : "Palmer et al\\.,? 2005",
      "shortCiteRegEx" : "Palmer et al\\.",
      "year" : 2005
    }, {
      "title" : "On the difficulty of training recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio." ],
      "venue" : "ICML (3) 28:1310–1318.",
      "citeRegEx" : "Pascanu et al\\.,? 2013",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2013
    }, {
      "title" : "Uofr at semeval-2016 task 8: Learning synchronous hyperedge replacement grammar for amr parsing",
      "author" : [ "Xiaochang Peng", "Daniel Gildea." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-",
      "citeRegEx" : "Peng and Gildea.,? 2016",
      "shortCiteRegEx" : "Peng and Gildea.",
      "year" : 2016
    }, {
      "title" : "Addressing the data sparsity issue in neural amr parsing",
      "author" : [ "Xiaochang Peng", "Chuan Wang", "Daniel Gildea", "Nianwen Xue." ],
      "venue" : "Proceedings of EACL. Preprint. http://www.cs.brandeis.edu/ cwang24/files/eacl17.pdf.",
      "citeRegEx" : "Peng et al\\.,? 2017",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2017
    }, {
      "title" : "Head-driven phrase structure grammar",
      "author" : [ "Carl Pollard", "Ivan A Sag." ],
      "venue" : "University of Chicago Press.",
      "citeRegEx" : "Pollard and Sag.,? 1994",
      "shortCiteRegEx" : "Pollard and Sag.",
      "year" : 1994
    }, {
      "title" : "Online graph planarisation for synchronous parsing of semantic and syntactic dependencies",
      "author" : [ "Ivan Titov", "James Henderson", "Paola Merlo", "Gabriele Musillo." ],
      "venue" : "IJCAI. pages 1562–1567.",
      "citeRegEx" : "Titov et al\\.,? 2009",
      "shortCiteRegEx" : "Titov et al\\.",
      "year" : 2009
    }, {
      "title" : "Stochastic HPSG parse disambiguation using the redwoods corpus",
      "author" : [ "Kristina Toutanova", "Christopher D. Manning", "Dan Flickinger", "Stephan Oepen." ],
      "venue" : "Research on Language and Computation 3(1):83–105.",
      "citeRegEx" : "Toutanova et al\\.,? 2005",
      "shortCiteRegEx" : "Toutanova et al\\.",
      "year" : 2005
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, Curran Associates, Inc.,",
      "citeRegEx" : "Vinyals et al\\.,? 2015a",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Oriol Vinyals", "Łukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 2755–2763.",
      "citeRegEx" : "Vinyals et al\\.,? 2015b",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Camr at semeval2016 task 8: An extended transition-based amr parser",
      "author" : [ "Chuan Wang", "Sameer Pradhan", "Xiaoman Pan", "Heng Ji", "Nianwen Xue." ],
      "venue" : "Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Robust parsing, meaning composition, and evaluation: Integrating grammar approximation, default unification, and elementary",
      "author" : [ "Yi Zhang", "Stephan Oepen", "Rebecca Dridan", "Dan Flickinger", "Hans-Ulrich Krieger" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : ", 1995, 2005), which serves as the semantic representation of the English Resource Grammar (ERG) (Flickinger, 2000).",
      "startOffset" : 97,
      "endOffset" : 115
    }, {
      "referenceID" : 39,
      "context" : "The only previous approach to parsing and disambiguating full MRS structures (as opposed to bilexical semantic graphs derived from, but simplifying MRS) were based on the ERG (Toutanova et al., 2005); this approach has high precision but incomplete coverage.",
      "startOffset" : 175,
      "endOffset" : 199
    }, {
      "referenceID" : 32,
      "context" : "We develop parsers for two graph-based conversions of MRS, Elementary Dependency Structure (EDS) (Oepen and Lønning, 2006) and Dependency MRS (DMRS) (Copestake, 2009), of which the latter is inter-convertible with MRS.",
      "startOffset" : 97,
      "endOffset" : 122
    }, {
      "referenceID" : 6,
      "context" : "We develop parsers for two graph-based conversions of MRS, Elementary Dependency Structure (EDS) (Oepen and Lønning, 2006) and Dependency MRS (DMRS) (Copestake, 2009), of which the latter is inter-convertible with MRS.",
      "startOffset" : 149,
      "endOffset" : 166
    }, {
      "referenceID" : 1,
      "context" : "Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a graph-based semantic representation with similar goals to that of MRS.",
      "startOffset" : 38,
      "endOffset" : 62
    }, {
      "referenceID" : 15,
      "context" : "Parsers based on RNNs have achieved state-ofthe-art performance for dependency parsing (Dyer et al., 2015; Kiperwasser and Goldberg, 2016) and constituency parsing (Vinyals et al.",
      "startOffset" : 87,
      "endOffset" : 138
    }, {
      "referenceID" : 26,
      "context" : "Parsers based on RNNs have achieved state-ofthe-art performance for dependency parsing (Dyer et al., 2015; Kiperwasser and Goldberg, 2016) and constituency parsing (Vinyals et al.",
      "startOffset" : 87,
      "endOffset" : 138
    }, {
      "referenceID" : 41,
      "context" : ", 2015; Kiperwasser and Goldberg, 2016) and constituency parsing (Vinyals et al., 2015b; Dyer et al., 2016; Cross and Huang, 2016b).",
      "startOffset" : 65,
      "endOffset" : 131
    }, {
      "referenceID" : 16,
      "context" : ", 2015; Kiperwasser and Goldberg, 2016) and constituency parsing (Vinyals et al., 2015b; Dyer et al., 2016; Cross and Huang, 2016b).",
      "startOffset" : 65,
      "endOffset" : 131
    }, {
      "referenceID" : 11,
      "context" : ", 2015; Kiperwasser and Goldberg, 2016) and constituency parsing (Vinyals et al., 2015b; Dyer et al., 2016; Cross and Huang, 2016b).",
      "startOffset" : 65,
      "endOffset" : 131
    }, {
      "referenceID" : 22,
      "context" : "One of the main reasons for the prevalence of dependency parsing, including semantic dependency parsing (Ivanova et al., 2013), is that it can be performed with efficient and well-understood algorithms.",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 9,
      "context" : "Minimal Recursion Semantics (MRS) is a framework for computational semantics that can be used for parsing or generation (Copestake et al., 2005).",
      "startOffset" : 120,
      "endOffset" : 144
    }, {
      "referenceID" : 37,
      "context" : "with feature-based grammars, and has been implemented in English within the framework of Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) in the English Resource Grammar (ERG) (Flickinger, 2000).",
      "startOffset" : 132,
      "endOffset" : 155
    }, {
      "referenceID" : 19,
      "context" : "with feature-based grammars, and has been implemented in English within the framework of Headdriven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) in the English Resource Grammar (ERG) (Flickinger, 2000).",
      "startOffset" : 194,
      "endOffset" : 212
    }, {
      "referenceID" : 6,
      "context" : "MRS can be converted without loss to variablefree dependency graphs, called Dependency MRS (DMRS) (Copestake, 2009; Copestake et al., 2016).",
      "startOffset" : 98,
      "endOffset" : 139
    }, {
      "referenceID" : 32,
      "context" : "A similar graph-based conversion is Elementary Dependency Structures (EDS) (Oepen and Lønning, 2006), which drops the scopeunderspecification machinery, primarily simplifying edge labels.",
      "startOffset" : 75,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "AMR (Banarescu et al., 2013) graphs can be represented in the same framework, despite a number of linguistic differences with MRS.",
      "startOffset" : 4,
      "endOffset" : 28
    }, {
      "referenceID" : 33,
      "context" : "AMR predicates are based on PropBank (Palmer et al., 2005), annotated as lemmas plus sense labels, but they form only a subset of concepts.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 41,
      "context" : "Variants of this approach has been proposed for neural constituency parsing (Vinyals et al., 2015b), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016) and AMR parsing (Barzdins and Gosko, 2016; Peng et al.",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 13,
      "context" : ", 2015b), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016) and AMR parsing (Barzdins and Gosko, 2016; Peng et al.",
      "startOffset" : 34,
      "endOffset" : 78
    }, {
      "referenceID" : 23,
      "context" : ", 2015b), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016) and AMR parsing (Barzdins and Gosko, 2016; Peng et al.",
      "startOffset" : 34,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : ", 2015b), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016) and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017).",
      "startOffset" : 95,
      "endOffset" : 140
    }, {
      "referenceID" : 36,
      "context" : ", 2015b), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016) and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017).",
      "startOffset" : 95,
      "endOffset" : 140
    }, {
      "referenceID" : 30,
      "context" : "based parsing (Nivre, 2008).",
      "startOffset" : 14,
      "endOffset" : 27
    }, {
      "referenceID" : 38,
      "context" : "We apply a variant of the arc-eager transition system that has been proposed for graph (as opposed to tree) parsing (Titov et al., 2009; Gómez-Rodrı́guez and Nivre, 2010) to derive a transition-based parser for deep semantic graphs.",
      "startOffset" : 116,
      "endOffset" : 170
    }, {
      "referenceID" : 21,
      "context" : "We apply a variant of the arc-eager transition system that has been proposed for graph (as opposed to tree) parsing (Titov et al., 2009; Gómez-Rodrı́guez and Nivre, 2010) to derive a transition-based parser for deep semantic graphs.",
      "startOffset" : 116,
      "endOffset" : 170
    }, {
      "referenceID" : 12,
      "context" : "Damonte et al. (2016) proposed an arc-eager AMR parser, but the definition of their system is more narrowly defined for AMR graphs.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 28,
      "context" : "We use the Stanford CoreNLP toolkit (Manning et al., 2014) to tokenize and lemmatize sentences, and tag tokens with the Stanford Named Entity Recognizer (Finkel et al.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 17,
      "context" : ", 2014) to tokenize and lemmatize sentences, and tag tokens with the Stanford Named Entity Recognizer (Finkel et al., 2005).",
      "startOffset" : 102,
      "endOffset" : 123
    }, {
      "referenceID" : 33,
      "context" : "The lexicon is restricted to Propbank (Palmer et al., 2005) predicates, so for other concepts we extract a lexicon from the training data.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 24,
      "context" : "We use a standard LSTM architecture without peephole connections (Jozefowicz et al., 2015).",
      "startOffset" : 65,
      "endOffset" : 90
    }, {
      "referenceID" : 40,
      "context" : "The alignment is predicted with a pointer network (Vinyals et al., 2015a).",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 24,
      "context" : "This approach is similar to the features proposed by Kiperwasser and Goldberg (2016) and Cross and Huang (2016a) for dependency parsing.",
      "startOffset" : 53,
      "endOffset" : 85
    }, {
      "referenceID" : 10,
      "context" : "This approach is similar to the features proposed by Kiperwasser and Goldberg (2016) and Cross and Huang (2016a) for dependency parsing.",
      "startOffset" : 89,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "Our implementation of the stack-based model enables batch processing in static computation graphs, similar to Bowman et al. (2016). We maintain a stack of alignment indexes for each element in the batch, which is updated inside the computation graph after each parsing action.",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 20,
      "context" : "1 Data DeepBank (Flickinger et al., 2012) is HPSG and MRS annotation of the Penn Treebank Wall Street Journal (WSJ) corpus text, developed following an approach known as dynamic treebanking (Oepen et al.",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 31,
      "context" : ", 2012) is HPSG and MRS annotation of the Penn Treebank Wall Street Journal (WSJ) corpus text, developed following an approach known as dynamic treebanking (Oepen et al., 2004), that couples treebank annotation with grammar development, in the case of the ERG.",
      "startOffset" : 156,
      "endOffset" : 176
    }, {
      "referenceID" : 3,
      "context" : "71 for AMR (Bender et al., 2015).",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 29,
      "context" : "For AMR parsing we use LDC2015E86, the dataset released for the SemEval 2016 AMR parsing Shared Task (May, 2016).",
      "startOffset" : 101,
      "endOffset" : 112
    }, {
      "referenceID" : 18,
      "context" : "com/delph-in/pydelphin ments using the rule-based JAMR aligner (Flanigan et al., 2014).",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : "EDM can be contrasted with the Smatch metric (Cai and Knight, 2013) for evaluating AMR graphs.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 25,
      "context" : "For training we use Adam (Kingma and Ba, 2015) with learning rate 0.",
      "startOffset" : 25,
      "endOffset" : 46
    }, {
      "referenceID" : 34,
      "context" : "0 (Pascanu et al., 2013).",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 27,
      "context" : "Encoder word embeddings are initialized (in the first 100 dimensions) with pre-trained order-sensitive embeddings (Ling et al., 2015).",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 43,
      "context" : "Another approach to robust MRS parsing has previously been proposed (Zhang et al., 2014), but no comparable results or implementation is available.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "However, concept prediction remains a weakness of the model; Damonte et al. (2016) reports that state-of-the-art AMR parsers score between 79% and 83% on concept prediction.",
      "startOffset" : 61,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "However, concept prediction remains a weakness of the model; Damonte et al. (2016) reports that state-of-the-art AMR parsers score between 79% and 83% on concept prediction. In contrast to DMRS parsing, the in-order linearization gives better results than the monotone one. We hypothesize that this is due to the Model LDC JAMR 56 Wang et al. (2016) 66.",
      "startOffset" : 61,
      "endOffset" : 350
    }, {
      "referenceID" : 11,
      "context" : "However, concept prediction remains a weakness of the model; Damonte et al. (2016) reports that state-of-the-art AMR parsers score between 79% and 83% on concept prediction. In contrast to DMRS parsing, the in-order linearization gives better results than the monotone one. We hypothesize that this is due to the Model LDC JAMR 56 Wang et al. (2016) 66.54 Damonte et al. (2016) 64 Peng and Gildea (2016) 55 Peng et al.",
      "startOffset" : 61,
      "endOffset" : 378
    }, {
      "referenceID" : 11,
      "context" : "However, concept prediction remains a weakness of the model; Damonte et al. (2016) reports that state-of-the-art AMR parsers score between 79% and 83% on concept prediction. In contrast to DMRS parsing, the in-order linearization gives better results than the monotone one. We hypothesize that this is due to the Model LDC JAMR 56 Wang et al. (2016) 66.54 Damonte et al. (2016) 64 Peng and Gildea (2016) 55 Peng et al.",
      "startOffset" : 61,
      "endOffset" : 404
    }, {
      "referenceID" : 11,
      "context" : "However, concept prediction remains a weakness of the model; Damonte et al. (2016) reports that state-of-the-art AMR parsers score between 79% and 83% on concept prediction. In contrast to DMRS parsing, the in-order linearization gives better results than the monotone one. We hypothesize that this is due to the Model LDC JAMR 56 Wang et al. (2016) 66.54 Damonte et al. (2016) 64 Peng and Gildea (2016) 55 Peng et al. (2017) 52 Barzdins and Gosko (2016) 43.",
      "startOffset" : 61,
      "endOffset" : 426
    }, {
      "referenceID" : 2,
      "context" : "(2017) 52 Barzdins and Gosko (2016) 43.",
      "startOffset" : 10,
      "endOffset" : 36
    }, {
      "referenceID" : 18,
      "context" : "Our best neural model outperforms the baseline JAMR parser (Flanigan et al., 2014), but still lags behind the performance of state-ofthe-art AMR parsers such as CAMR (Wang et al.",
      "startOffset" : 59,
      "endOffset" : 82
    }, {
      "referenceID" : 42,
      "context" : ", 2014), but still lags behind the performance of state-ofthe-art AMR parsers such as CAMR (Wang et al., 2016).",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "We see that our attentionbased encoder-decoder model already performs better than previous sequence-to-sequence AMR parsers (Barzdins and Gosko, 2016; Peng et al., 2017), and the arc-eager model boost accuracy further.",
      "startOffset" : 124,
      "endOffset" : 169
    }, {
      "referenceID" : 36,
      "context" : "We see that our attentionbased encoder-decoder model already performs better than previous sequence-to-sequence AMR parsers (Barzdins and Gosko, 2016; Peng et al., 2017), and the arc-eager model boost accuracy further.",
      "startOffset" : 124,
      "endOffset" : 169
    }, {
      "referenceID" : 35,
      "context" : "Our model also outperforms a Synchronous Hyperedge Replacement Grammar model (Peng and Gildea, 2016) which is comparable as it does not make extensive use of external resources.",
      "startOffset" : 77,
      "endOffset" : 100
    } ],
    "year" : 2017,
    "abstractText" : "Parsing sentences to linguisticallyexpressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focussed almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.",
    "creator" : "LaTeX with hyperref package"
  }
}