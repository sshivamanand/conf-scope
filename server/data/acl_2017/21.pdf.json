{
  "name" : "21.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Transductive Non-linear Learning for Chinese Hypernym Prediction",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "A hypernym of an entity characterizes the type or the class of the entity. For example, the word country is the hypernym of the entity Canada. The accurate prediction of hypernyms benefits a variety of NLP tasks, such as taxonomy learning (Wu et al., 2012; Fu et al., 2014), fine-grained entity categorization (Ren et al., 2016), knowledge base construction (Suchanek et al., 2007), etc.\nIn previous work, the detection of hypernyms requires lexical, syntactic and/or semantic analysis of relations between entities and their respective hypernyms from a language-specific knowledge source. For example, Hearst (1992) is the pioneer work to extract is-a relations from a text corpus based on handcraft patterns. The followingup work mostly focuses on is-a relation extraction using automatically generated patterns (Snow et al., 2004; Ritter et al., 2009; Sang and Hof-\nmann, 2009; Kozareva and Hovy, 2010) and relation inference based on distributional similarity measures (Kotlerman et al., 2010; Lenci and Benotto, 2012; Shwartz et al., 2016).\nWhile these approaches have relatively high precision over English corpora, extracting hypernyms for entities is still challenging for Chinese. From the linguistic perspective, Chinese is a lower-resourced language with very flexible expressions and grammatical rules (Wang et al., 2015). For instance, there are no word spaces, explicit tenses and voices, and distinctions between singular and plural forms in Chinese. The order of words can be changed flexibly in sentences. Hence, as previous research indicates, hypernym extraction methods for English are not necessarily suitable for the Chinese language (Fu et al., 2014; Wang et al., 2015; Wang and He, 2016).\nBased on such conditions, several classification methods are proposed to distinguish is-a and notis-a relations based on Chinese encyclopedias (Lu et al., 2015; Li et al., 2015). Similar to Princeton WordNet, a few Chinese wordnets have also been developed (Huang et al., 2004; Xu et al., 2008; Wang and Bond, 2013). The most recent approaches for Chinese is-a relation extraction (Fu et al., 2014; Wang and He, 2016) use word embedding based linear projection models to map embeddings of hyponyms to those of their hypernyms, which outperform previous algorithms.\nHowever, we argue that these projection-based methods may have three potential limitations: (i) Only positive is-a relations are used for projection learning. The distinctions between is-a and not-is-a relations in the embedding space are not modeled. (ii) These methods lack the capacity to encode linguistic rules, which are designed by linguists and usually have high precision. (iii) It assumes that the linguistic regularities of is-a relations can be solely captured by single or multiple\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nlinear projection models. In this paper, we address these limitations by a two-stage transductive learning approach. It distinguishes is-a and not-is-a relations given a Chinese word/phrase pair as input. In the initial stage, we train linear projection models on positive and negative training data separately and predict isa relations jointly. In the transductive learning stage, the initial prediction results, linguistic rules and the non-linear mappings from entities to hypernyms are optimized simultaneously in a unified framework. This optimization problem can be efficiently solved by blockwise gradient descent. We evaluate our method over two public datasets and show that it outperforms state-of-the-art approaches for Chinese hypernym prediction.1\nThe rest of this paper is organized as follows. We summarize the related work in Section 2. Our approach is introduced in Section 3. Experimental results are presented in Section 4. We conclude our paper in Section 5.2"
    }, {
      "heading" : "2 Related Work",
      "text" : "In this section, we overview the related work on hypernym prediction and discuss the challenges of Chinese hypernym detection.\nPattern based methods identify is-a relations from texts by handcraft or automatically generated patterns. Hearst patterns (Hearst, 1992) are lexical patterns in English that are employed to extract isa relations for taxonomy construction (Wu et al., 2012). Automatic approaches mostly use iterative learning paradigms such that the system learns new is-a relations and patterns simultaneously. A few relevant studies can be found in (Caraballo, 1999; Etzioni et al., 2004; Sang, 2007; Pantel and Pennacchiotti, 2006; Kozareva and Hovy, 2010). To avoid “semantic drift” in iterations, Snow et al. (2004) train a hypernym classifier based on syntactic features based on parse trees. Carlson et al. (2010) exploit multiple learners to extract relations via coupled learning. These approaches are\n1While there is abundant research on hypernym prediction for English with high precision, we mostly focus on the Chinese language in this paper. However, the proposed method is not entirely language-specific and has the potential to be adapted to other languages. In Section 4, we provide additional experiments to show that our approach also outperforms several existing methods for hypernym prediction in the English environment. We also discuss some potential applications of our method.\n2Additionally, we present more details on algorithms and experimental settings and a prototype system for taxonomy visualization in the supplementary notes.\nnot effective for Chinese for two reasons: i) Chinese is-a relations are expressed in a highly flexible manner (Fu et al., 2014) and ii) the accuracy of basic NLP tasks such as dependency parsing still need improvement for Chinese (Li et al., 2013).\nInference based methods take advantage of distributional similarity measures (DSM) to infer relations between words. They assume that a hypernym may appear in all contexts of the hyponyms and a hyponym can only appear in part of the contexts of its hypernyms. In previous work, Kotlerman et al. (2010) design directional DSMs to model the asymmetric property of is-a relations. Other DSMs are introduced in (Bhagat et al., 2007; Szpektor et al., 2007; Lenci and Benotto, 2012; Santus et al., 2014). Shwartz et al. (2016) combine dependency parsing and DSM to improve the performance of hypernymy detection. The reason why DSM is not effective for Chinese is that the contexts of entities in Chinese are flexible and sparse.\nEncyclopedia based methods take encyclopedias as knowledge sources to construct taxonomies. Ponzetto and Strube (2007) design features from multiple aspects to predict is-a relations between entities and categories in English Wikipedia. The taxonomy in YAGO (Suchanek et al., 2007) is constructed by linking conceptual categories in Wikipedia to WordNet synsets (Miller, 1995). For Chinese, Li et al. (2015) propose an SVM-based approach to build a large Chinese taxonomy from Wikipedia. Similar classification based algorithms are presented in (Fu et al., 2013; Lu et al., 2015). Due to the lack of Chinese version of WordNet, several Chinese semantic dictionaries have been conducted, such as Sinica BOW (Huang et al., 2004), SEW (Xu et al., 2008), COW (Wang and Bond, 2013), etc. These approaches have higher accuracy than mining hypernym relations from texts directly. However, they heavily rely on existing knowledge sources and are difficult to extend to different domains.\nTo tackle these challenges, word embedding based methods directly model the task of hypernym prediction as learning a mapping from entity vectors to their respective hypernym vectors in the embedding space. The vectors can be pretrained by neural language models (Mikolov et al., 2013). For the Chinese language, Fu et al. (2014) train piecewise linear projection models based on a Chinese thesaurus. The state-of-the-art method\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n(Wang and He, 2016) combines an iterative learning procedure and Chinese Hearst-style patterns to improve the performance of projection models. They can reduce data noise by avoiding direct parsing of Chinese texts, but still capture the linguistic regularities of is-a relations based on word embeddings. Additionally, several work aims to study how to combine word embeddings for relation classification, such as (Mirza and Tonelli, 2016). In our paper, we extend these approaches by modeling non-linear mappings from entities to hypernyms and adding linguistic rules via a unified transductive learning framework."
    }, {
      "heading" : "3 Proposed Approach",
      "text" : "This section begins with a brief overview of our approach. After that, the detailed steps and the learning algorithm are introduced in detail."
    }, {
      "heading" : "3.1 Overview",
      "text" : "Given a word/phase pair (xi, yi), the goal of our task is to learn a classification model to predict whether yi is the hypernym of xi.\nAs illustrated in Figure 1, our approach has two stages: initial stage and transductive learning stage. The input is a positive is-a set D+, a negative is-a set D− and an unlabeled set DU , all of which are the collections of word/phrase pairs.\nDenote xi as the embedding vector of word xi, pre-trained and stored in a lookup table. In the initial stage, we train a linear projection model over D+ such that for each (xi, yi) ∈ D+, a projection matrix maps the entity vector xi to its hypernym vector yi. A similar model is also trained over D−. Based on the two models, we estimate the prediction score and the confidence score for each (xi, yi) ∈ DU . In the transductive learning stage, a joint optimization problem is formed to learn the final prediction score for each (xi, yi) ∈ DU . It aims to minimize the prediction errors based on the human labeled data, the initial model prediction and linguistic rules. It also employs nonlinear mappings to capture linguistic regularities of is-a relations other than linear projections."
    }, {
      "heading" : "3.2 Initial Model Training",
      "text" : "The initial stage models how entities are translated to their hypernyms or non-hypernyms by projection learning. We first train a Skip-gram model (Mikolov et al., 2013) to learn word embeddings over a large text corpus. Inspired by\n(Fu et al., 2014; Wang and He, 2016), for each (xi, yi) ∈ D+, we assume there is a positive projection model such that M+xi ≈ yi where M+ is an |xi| × |xi| projection matrix3. However, this model does not capture the semantics of not-isa relations. Thus, we learn a negative projection model M−xi ≈ yi where (xi, yi) ∈ D−. This approach is equivalent to learning two separate translation models within the same semantic space. For parameter estimation, we minimize the two following objectives:\nJ(M+) = 1\n2 ∑ (xi,yi)∈D+ ‖M+xi−yi‖22+ λ 2 ‖M+‖2F\nJ(M−) = 1\n2 ∑ (xi,yi)∈D− ‖M−xi−yi‖22+ λ 2 ‖M−‖2F\nwhere λ > 0 is a Tikhonov regularization parameter (Golub et al., 1999).\nIn the testing phase, for each (xi, yi) ∈ DU , denote d+(xi, yi) = ‖M+xi − yi‖2 and d−(xi, yi) = ‖M−xi−yi‖2. The prediction score is defined as:\nscore(xi, yi) = tanh(d −(xi, yi)− d+(xi, yi))\nwhere score(xi, yi) ∈ (−1, 1). Higher prediction score indicates there is a larger probability of an is-a relation between xi and yi. We choose the hyperbolic tangent function rather than the sigmoid function to avoid the widespread saturation of sigmoid function (Menon et al., 1996).\nThe difference between d+(xi, yi) and d−(xi, yi) can be also used to indicate whether the models are confident enough to make a prediction. In this paper, we calculate the confidence score as:\nconf(xi, yi) = |d+(xi, yi)− d−(xi, yi)| max{d+(xi, yi), d−(xi, yi)} 3We have also examined piecewise linear projection models proposed in (Fu et al., 2014; Wang and He, 2016) as the initial models for transductive learning. However, we found that this practice is less efficient and the performance does not improve significantly.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nwhere conf(xi, yi) ∈ (0, 1). Higher confidence score means that there is a larger probability that the models can predict whether there is an is-a relation between xi and yi correctly. This score gives different data instances different weights in the transductive learning stage."
    }, {
      "heading" : "3.3 Transductive Non-linear Learning",
      "text" : "Although linear projection methods are effective for Chinese hypernym prediction, it does not encode non-linear transformation and only leverages the positive data. We present an optimization framework for non-linear mapping utilizing both labeled and unlabeled data and linguistic rules by transductive learning (Gammerman et al., 1998; Chapelle et al., 2006).\nLet Fi be the final prediction score of the word/phrase pair (xi, yi). In the initialization stage of our algorithm, we set Fi = 1 if (xi, yi) ∈ D+, Fi = −1 if (xi, yi) ∈ D− and set Fi randomly in (−1, 1) if (xi, yi) ∈ DU . In matrix representation, denote F as the m× 1 final prediction vector where m = |D+| + |D−| + |DU |. Fi is the ith element in F. The three components in our transductive learning model are as follows:"
    }, {
      "heading" : "3.3.1 Initial Prediction",
      "text" : "Denote S as an m×1 initial prediction vector. We set Si = 1 if (xi, yi) ∈ D+, Si = −1 if (xi, yi) ∈ D− and Si = score(xi, yi) if (xi, yi) ∈ DU . In order to encode the confidence of model prediction, we define W as an m ×m diagonal weight matrix. The element in the ith row and the jth column of W is set as follows:\nWi,j =  conf(xi, yi) i = j, (xi, yi) ∈ DU 1 i = j, (xi, yi) ∈ D+ ∪D−\n0 Otherwise\nThe objective function is defined as: Os = ‖W(F−S)‖22, which encodes the hypothesis that the final prediction should be similar to the initial prediction for unlabeled data or human labeling for training data. The weight matrix W gives the largest weight (i.e., 1) to all the pairs in D+ ∪D− and a larger weight to the pair (xi, yi) ∈ DU if the initial prediction is more confident."
    }, {
      "heading" : "3.3.2 Linguistic Rules",
      "text" : "Although linguistic rules can only cover a few circumstances, they are effective to guide the learning process. For Chinese hypernym prediction,\nLi et al. (2015) study the word formation of conceptual categories in Chinese Wikipedia. In our model, let C be the collection of linguistic rules. γi is the true positive (or negative) rate with respect to the respective positive (or negative) rule ci ∈ C, estimated over the training set. Considering the word formation of Chinese entities and hypernyms, we design one positive rule (i.e., P1) and two negative rules (i.e., N1 and N2), shown in Table 1.\nLet R be an m × 1 linguistic rule vector and Ri is the ith element in R. For training data, we set Ri = 1 if (xi, yi) ∈ D+ and Ri = −1 if (xi, yi) ∈ D−, which follows the same settings as those in S. For unlabeled pairs that do not match any linguistic rules in C, we update Ri = Fi in each iteration of the learning process, meaning no loss for errors imposed in this part.\nFor other conditions, denote C(xi,yi) ⊆ C as the collection of rules that (xi, yi) matches. If C(xi,yi) are positive rules, we set Ri as follows:\nRi = max{Fi, max cj∈C(xi,yi) γj}\nSimilarly, if C(xi,yi) are negative rules, we have:\nRi = −max{−Fi, max cj∈C(xi,yi) γj}\nwhich means Fi receives a penalty only if Fi < maxcj∈C(xi,yi) γj for pairs that match positive rules or Fi > −maxcj∈C(xi,yi) γj for negative rules4. The objective function is: Or = ‖F−R‖22. In this way, our model can integrate arbitrary “soft” constraints, making it robust to false positives or negatives introduced by these rules."
    }, {
      "heading" : "3.3.3 Non-linear Learning",
      "text" : "TransLP is a transductive label propagation framework (Liu and Yang, 2015) for link prediction, previously used for applications such as text classification (Xu et al., 2016). In our work, we extend their work for our task, modeling non-linear mappings from entities to hypernyms.\nFor is-a relations, we find that if y is the hypernym of x, it is likely that y is the hypernym of entities that are semantically close to x. For example,\n4We do not consider the cases where a pair matches both positive and negative rules because such cases are very rare, and even non-existent in our datasets. However, our method can deal with these cases by using some simple heuristics. For example, we can update Ri using either of the following two ways: i) Ri = Fi and ii) Ri = Fi + ∑ cj∈C(xi,yi) γj .\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nP1 The head word of the entity x matches that of the candidate hypernym y. For example,动物 (Animal) is the correct hypernym of哺乳动物 (Mammal). N1 The head word of the entity x matches the non-head word of the candidate hypernym y. For example,动物学 (Zoology) is not a hypernym of哺乳动物 (Mammal). N2 The head word of the candidate hypernym y matches an entry in a Chinese lexicon extended based on the lexicon used in Li et al., (2015). It consists of 184 non-taxonomic, thematic words such as政治(Politics),军事(Military), etc.\nTable 1: Three linguistic rules used in our work for Chinese hypernym prediction.\nif we know United States is a country, we can infer country is the hypernym of similar entities such as Canada, Australia, etc. This intuition can be encoded in the similarity of the two pairs pi = (xi, yi) and pj = (xj , yj):\nsim(pi, pj) =\n{ cos(xi,xj) yi = yj\n0 otherwise (1)\nwhere xi is the embedding vector of xi5. This similarity indicates there exists a nonlinear mapping from entities to hypernyms, which can not be encoded in linear projection based methods (Fu et al., 2014; Wang and He, 2016). Based on TransLP (Liu and Yang, 2015), this intuition can be model as propagating class labels (is-a or not-is-a) of labeled word/phrase pairs to similar unlabeled ones based on Eq. (1). For example, the score of is-a relations between United State and country will propagate to pairs such as (Canada, country) and (Australia, country) by random walks.\nDenote F∗ as the optimal solution of the problem min Os + Or. Inspired by (Liu and Yang, 2015; Xu et al., 2016), we can add a Gaussian prior N(F∗,Σ) to F where Σ is the covariance matrix and Σi,j = sim(pi, pj). Hence the optimization objective of this part is defined as: On = FTΣ−1F which is linearly proportional to the negative likelihood of the Gaussian random field prior. This means we minimize the training error and encourage F to have a smooth propagation with respect to the similarities among pairs defined by Eq. (1) at the same time."
    }, {
      "heading" : "3.3.4 Joint Optimization",
      "text" : "Based on the three components, we minimize the following function:\nJ(F) = Os + Or + µ1 2 On + µ2 2 ‖F‖22 (2)\n5We only consider the similarity between entities and not candidate hypernyms because the similar rule for candidate hypernyms is not true. For example, nouns close to country in our Skip-gram model are region, department, etc. They are not all correct hypernyms of United States, Canada, Australia, etc.\nwhere ‖F‖22 imposes an additional smooth l2regularization on F. µ1 and µ2 are tuning parameters.\nA basic approach to learn the optimal values of F is via gradient descent. Based on Eq. (2), the derivative of F with respect to J(F) can be computed as follows:\ndJ(F) dF = W2(F−S)+(F−R)+µ1Σ−1F+µ2F\nOptimizing Eq. (2) is computationally expensive when m is large. After W2, S, R and Σ−1 are pre-computed, the runtime complexity of the loop of gradient descent is O(tm2) where t is the number of iterations.\nTo speed up the learning process, we introduce a blockwise gradient descent technique. From the definition of Eq. (2), we can see that the optimal values of Fi and Fj with respect to (xi, yi) and (xj , yj) are irrelevant if yi 6= yj . Therefore, the original optimization problem can be decomposed and solved separately according to different candidate hypernyms.\nLetH be the collection of candidate hypernyms in DU . For each h ∈ H , denote Dh as the collection of word/phase pairs in D+ ∪ D− ∪ DU that share the same candidate hypernym h. The original problem can be decomposed into |H| optimization subproblems over Dh for each h ∈ H . The runtime complexity is O( ∑ h∈Dh th|Dh|\n2) where th is the number of iterations to solve the subproblem over Dh. Although we do not know the upper bounds on the numbers of iterations of these two learning techniques, the runtime complexity can be reduced by blockwise gradient descent for two reasons: i) ∑ h∈Dh |Dh| ≤ m and ii) th has a large probability to be smaller than t due to the smaller number of data instances. This technique can be also viewed as optimizing Eq. (2) based on blockwise matrix computation.\nFinally, for each (xi, yi) ∈ DU , we predict that yi is a hypernym of xi if Fi > θ where θ ∈ (−1, 1) is a threshold tuned on the development set.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599"
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we conduct experiments to evaluate our method. Section 4.1 to Section 4.5 report the experimental steps on Chinese datasets. We present the performance on English datasets in Section 4.6 and a discussion in Section 4.7."
    }, {
      "heading" : "4.1 Experimental Data",
      "text" : "The Chinese text corpus is extracted from the contents of 1.2M entity pages from Baidu Baike6, a Chinese online encyclopedia. It contains approximately 1.1B words. We use the open source toolkit Ansj7 for Chinese word segmentation.\nWe have two collections of Chinese word/phase pairs as ground truth datasets. Each pair is labeled with an is-a or not-is-a tag. The first one (denoted as FD) is from Fu et al., (2014), containing 1,391 is-a pairs and 4,294 not-is-a pairs, which is the first publicly available dataset to evaluate this task. The second one (denoted as BK) is larger in size and crawled from Baidu Baike by ourselves, consisting of <entity, category> pairs. For each pair in BK, we ask multiple human annotators to label the tag and discard the pair with inconsistent labels by different annotators. In total, it contains 3,870 is-a pairs and 3,582 not-is-a pairs8.\nIn the following experiments, we use 60% of the data for training, 20% for development and 20% for testing, partitioned randomly. By rotating the 5-fold subsets of the datasets, we report the performance of each method on average."
    }, {
      "heading" : "4.2 Parameter Analysis",
      "text" : "The word embeddings are pre-trained by ourselves on the Chinese corpus. In total, we obtain the 100- dimensional embedding vectors of 5.8M distinct words. The regularization parameters are set to λ = 10−3 and µ1 = µ2 = 10−4, fine tuned on the development set.\nThe choice of θ reflects the precision-recall trade-off in our model. A larger value of θ means we pay more attention to precision rather than recall. Figure 2 illustrates the precision-recall curves on both datasets. It can be seen that the performance of our method is generally better in BK than FD. The most probable cause is that BK is a large dataset with more “balanced” numbers of positive\n6https://baike.baidu.com/ 7https://github.com/NLPchina/ansj seg/ 8Our dataset is publicly available. See submitted dataset.\n0.0 0.2 0.4 0.6 0.8 1.0 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nPr ec is io n\n(a) Dataset: FD\n0.0 0.2 0.4 0.6 0.8 1.0 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRecall\nPr ec is io n\n(b) Dataset: BK\nFigure 2: Precision-recall curve with respect to the tuning of θ on development sets (%).\nand negative data. Finally, θ is set to 0.05 on FD and 0.1 on BK."
    }, {
      "heading" : "4.3 Performance",
      "text" : "In a series of previous work (Fu et al., 2013, 2014; Wang and He, 2016), several pattern-based, inference-based and encyclopedia-based is-a relation extraction methods for English have been implemented for the Chinese language. As their experiments show, these methods achieve the Fmeasure of lower than 60% in most cases, which are not suggested to be strong baselines for Chinese hypernym prediction. Interested readers may refer to their papers for the experimental results.\nTo make the convincing conclusion, we employ two recent state-of-the-art approaches for Chinese is-a relation identification (Fu et al., 2014; Wang and He, 2016) as baselines. We also take the word embedding based classification approach (Mirza and Tonelli, 2016)9 and Chinese Wikipedia based SVM model (Li et al., 2015) as baselines to predict is-a relations between words10. The experimental results are illustrated in Table 3.\nFor Fu et al., (2014), we test the performance using a linear projection model (denoted as S in Table 3) and piecewise projection models (P). It shows that the semantics of is-a relations are better modeled by multiple projection models, with a slightly improvement in F-measure. By combining iterative projection models and pattern-based validation, the most recent approach (Wang and He, 2016) increases the F-measure by 4% and 2% in two datasets. In this method, the patternbased statistics are calculated using the same cor-\n9Although the experiments in their paper are mostly related to temporal relations, the method can be applied to is-a relations without modification.\n10Previously, these methods used different knowledge sources to train models and thus the results in their papers are not directly comparable with ours. To make fair comparison, we take the training data as the same knowledge source to train models for all methods.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nCandidate Hypernym P T Candidate Hypernym P T Entity: 乙烯(Ethylene) Entity: 孙燕姿(Stefanie Sun) 化学品(Chemical) √ √ 歌手(Singer) √ √ 有机化学(Organic Chemistry) × × 明星(Star) √ √ 有机物(Organics) √ √ 人物(Person) √ √ 气体(Gas) √ √ 金曲奖 (Golden Melody Award) √\n× 自然科学(Natural Science) × × 音乐人(Musician) √ √ Entity: 显卡(Graphics Card) Entity: 核反应堆(Nuclear Reactor) 硬件(Hardware) √ √ 建筑学(Architecture) × × 电子产品(Electronic Product) √ √\n核科学(Nuclear Science) × × 电脑硬件(Computer Hardware) √ √ 核能 (Nuclear Energy) √ × 数码(Digit) × × 自然科学(Natural Science) × ×\nTable 2: Examples of model prediction. (P: prediction result, T: ground truth, √ : positive, ×: negative)\nDataset FD Method P R F Fu et al., (2014) (S) 64.1 56.0 59.8 Fu et al., (2014) (P) 66.4 59.3 62.6 Li et al., (2015) 54.3 38.4 45.0 Mirza and Tonelli, (2016) (C) 67.7 75.2 69.7 Mirza and Tonelli, (2016) (A) 65.3 60.7 62.9 Mirza and Tonelli, (2016) (S) 71.9 60.6 65.7 Wang and He, (2016) 69.3 64.5 66.9 Ours (Initial) 70.7 69.2 69.9 Ours 72.8 70.5 71.6 Dataset BK Method P R F Fu et al., (2014) (S) 71.4 64.8 67.9 Fu et al., (2014) (P) 72.7 67.5 70.0 Li et al., (2015) 61.2 47.5 53.5 Mirza and Tonelli, (2016) (C) 80.3 75.9 78.0 Mirza and Tonelli, (2016) (A) 72.7 65.6 68.9 Mirza and Tonelli, (2016) (S) 78.4 60.7 68.4 Wang and He, (2016) 73.9 69.8 71.8 Ours (Initial) 81.7 78.5 80.0 Ours 83.6 80.6 82.1\nTable 3: Performance comparison on test sets for Chinese hypernym prediction (%).\npus over which we train word embedding models. The main reason of the improvement may be that the projection models have a better generalization power by applying an iterative learning paradigm.\nMirza and Tonelli, (2016) is implemented using three different strategies in combining the word vectors of a pair: i) concatenation xi ⊕ yi (denoted as C), ii) addition xi + yi (A) and iii) subtraction xi − yi (S). As seen, the classification models using addition and subtraction have similar performance in two datasets, while the concatenation strategy outperforms previous two approaches. Although Li et al., (2015) achieve a high performance in their dataset, this method does not perform well in ours. The most case is that the features in that work are designed specifically for the Chinese Wikipedia category system. Our initial model has a higher accuracy than all the base-\nTP/TN Rate Rule P1 Rule N1 Rule N2 Dataset FD 98.6 92.3 94.1 Dataset BK 97.6 96.8 97.3\nTable 4: TP/TN rates of three linguistic rules (%).\nlines. By utilizing the transductive learning framework, we boost the F-measure by 1.7% and 2.1%, respectively. Therefore, our method is effective to predict hypernyms of Chinese entities."
    }, {
      "heading" : "4.4 Effectiveness of Linguistic Rules",
      "text" : "To illustrate the effectiveness of linguistic rules, we present the true positive (or negative) rate by using one positive (or negative) rule solely, shown in Table 4. These values serve as γis in the transductive learning stage. The results indicate that these rules have high precision (over 90%) over both datasets for our task.\nWe state that currently we only use a few handcraft linguistic rules in our work. The proposed approach is a general framework that can encode arbitrary numbers of rules of any kind and in any language."
    }, {
      "heading" : "4.5 Error Analysis and Case Studies",
      "text" : "We analyze correct and error cases in the experiments. Some examples of prediction results are shown in Table 2. We can see that our method is generally effective. However, some mistakes occur mostly because it is difficult to distinguish strict is-a and topic-of relations. For example, the entity Nuclear Reactor is semantically close to Nuclear Energy. The error statistics show that such kind of errors account for approximately 80.2% and 78.6% in two test sets, respectively.\nBased on the literature study, we find that such problem has been also reported in (Fu et al., 2013; Wang and He, 2016). To reduce such errors, we employ the Chinese thematic lexicon based on Li\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\net al., (2015) in the transductive learning stage but the coverage is still limited. Two possible solutions are: i) adding more negative training data of this kind; and ii) constructing a large-scale thematic lexicon automatically from the Web."
    }, {
      "heading" : "4.6 Experiments on English Datasets",
      "text" : "To examine how our method can benefit hypernym prediction for the English language, we use two standard datasets in this paper. The first one is a benchmark dataset for distributional semantic evaluation, i.e., BLESS (Baroni and Lenci, 2011). Because the number of pairs in BLESS is relatively small, we also use the Shwartz (Shwartz et al., 2016) dataset. In the experiments, we treat the HYPER relations as positive data (1,337 pairs) and randomly sample 30% of the RANDOM relations as negative data (3,754 pairs) in BLESS. To create a relatively balanced dataset, we take the random split of Shwartz as input and use only 30% of the negative pairs. The dataset contains 14,135 positive pairs and 16,956 negative pairs. The text corpus is English Wikipedia and the embedding vectors are set to 100 dimensions.\nFor comparison, we test all the baselines over English datasets except Li et al., (2015). This is because most features in Li et al., (2015) can only be used in the Chinese environment. To implement Wang and He., (2016) for English, we use the original Hearst patterns (Hearst, 1992) to perform relation selection and do not consider not-isa patterns. We also take two recent DSM based approaches (Lenci and Benotto, 2012; Santus et al., 2014) as baselines. As for our own method, we do not use linguistic rules in Table 1 for English. The results are illustrated in Table 5. As seen, our method is superior to all the baselines over BLESS, with an F-measure of 81.9%. In Shwartz, while the approach (Mirza and Tonelli, 2016) has the highest F-measure of 80.1%, our method is generally comparable to theirs and outperforms others. The results suggest that although our method is not necessarily the state-of-the-art for English hypernym prediction, it has several potential applications. Refer to Section 4.7 for discussion."
    }, {
      "heading" : "4.7 Discussion",
      "text" : "From the experiments, we can see that the proposed approach outperforms the state-of-the-art methods for Chinese hypernym prediction. Although the English language is not our focus, our approach still has relatively high performance.\nDataset BLESS Method P R F Lenci and Benotto, (2012) 42.8 38.6 40.6 Santus et al., (2014) 59.2 52.3 55.4 Fu et al., (2014) (S) 65.3 62.4 63.8 Fu et al., (2014) (P) 68.1 64.2 66.1 Mirza and Tonelli, (2016) (C) 79.4 84.1 81.7 Mirza and Tonelli, (2016) (A) 80.7 72.3 76.3 Mirza and Tonelli, (2016) (S) 78.0 81.2 79.6 Wang and He, (2016) 76.2 75.4 75.8 Ours (Initial) 79.3 76.3 77.7 Ours 84.4 79.5 81.9 Dataset Shwartz Method P R F Lenci and Benotto, (2012) 38.5 50.1 43.5 Santus et al., (2014) 51.2 71.5 59.6 Fu et al., (2014) (S) 65.6 66.1 65.8 Fu et al., (2014) (P) 62.3 71.9 67.3 Mirza and Tonelli, (2016) (C) 79.3 80.9 80.1 Mirza and Tonelli, (2016) (A) 79.1 79.6 79.4 Mirza and Tonelli, (2016) (S) 80.5 77.5 79.0 Wang and He, (2016) 75.1 76.3 75.6 Ours (Initial) 77.2 76.8 77.0 Ours 79.1 77.5 78.3\nTable 5: Performance comparison on test sets for English hypernym prediction (%).\nAdditionally, our work has potential values for the following applications:11\nDomain-specific or Context-sparse Relation Extraction. If the task is to predict relations between words when it is related to a specific domain or the contexts are sparse, even for English, traditional pattern-based methods are likely to fail. Our method can predict the existence of relations without explicit textual patterns and requires a relatively small amount of pairs as training data.\nUnder-resourced Language Learning. Our method can be adapted for relation extraction in languages with flexible expressions, few knowledge resources and/or low-performance NLP tools. This is because it does not require deep NLP parsing of sentences in a text corpus."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In summary, this paper introduces a transuctive learning approach for Chinese hypernym prediction. By modeling linear projection models, linguistic rules and non-linear mappings, our method is able to identify Chinese hypernyms with high accuracy. Experiments show that the performance of our method outperforms previous approaches. We also discuss the potential applications of our method besides Chinese hypernym prediction.\n11The implementations of these applications are beyond the scope of this paper.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "How we blessed distributional semantic evaluation",
      "author" : [ "Marco Baroni", "Alessandro Lenci." ],
      "venue" : "Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics. pages 1—-10.",
      "citeRegEx" : "Baroni and Lenci.,? 2011",
      "shortCiteRegEx" : "Baroni and Lenci.",
      "year" : 2011
    }, {
      "title" : "LEDIR: an unsupervised algorithm for learning directionality of inference rules",
      "author" : [ "Rahul Bhagat", "Patrick Pantel", "Eduard H. Hovy." ],
      "venue" : "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational",
      "citeRegEx" : "Bhagat et al\\.,? 2007",
      "shortCiteRegEx" : "Bhagat et al\\.",
      "year" : 2007
    }, {
      "title" : "Automatic construction of a hypernym-labeled noun hierarchy from text",
      "author" : [ "Sharon A. Caraballo." ],
      "venue" : "27th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Caraballo.,? 1999",
      "shortCiteRegEx" : "Caraballo.",
      "year" : 1999
    }, {
      "title" : "Coupled semi-supervised learning for information extraction",
      "author" : [ "Andrew Carlson", "Justin Betteridge", "Richard C. Wang", "Estevam R. Hruschka Jr.", "Tom M. Mitchell." ],
      "venue" : "Proceedings of the Third International Conference on Web Search and Web Data",
      "citeRegEx" : "Carlson et al\\.,? 2010",
      "shortCiteRegEx" : "Carlson et al\\.",
      "year" : 2010
    }, {
      "title" : "Transductive Inference and SemiSupervised Learning",
      "author" : [ "Olivier Chapelle", "Bernhard Schölkopf", "Alexander Zien." ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Chapelle et al\\.,? 2006",
      "shortCiteRegEx" : "Chapelle et al\\.",
      "year" : 2006
    }, {
      "title" : "Web-scale information extraction in knowitall: (preliminary results)",
      "author" : [ "Oren Etzioni", "Michael J. Cafarella", "Doug Downey", "Stanley Kok", "Ana-Maria Popescu", "Tal Shaked", "Stephen Soderland", "Daniel S. Weld", "Alexander Yates." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Etzioni et al\\.,? 2004",
      "shortCiteRegEx" : "Etzioni et al\\.",
      "year" : 2004
    }, {
      "title" : "Learning semantic hierarchies via word embeddings",
      "author" : [ "Ruiji Fu", "Jiang Guo", "Bing Qin", "Wanxiang Che", "Haifeng Wang", "Ting Liu." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. pages 1199–1209.",
      "citeRegEx" : "Fu et al\\.,? 2014",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2014
    }, {
      "title" : "Exploiting multiple sources for open-domain hypernym discovery",
      "author" : [ "Ruiji Fu", "Bing Qin", "Ting Liu." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. pages 1224–1234.",
      "citeRegEx" : "Fu et al\\.,? 2013",
      "shortCiteRegEx" : "Fu et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning by transduction",
      "author" : [ "Alexander Gammerman", "Katy S. Azoury", "Vladimir Vapnik." ],
      "venue" : "Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence. pages 148–155.",
      "citeRegEx" : "Gammerman et al\\.,? 1998",
      "shortCiteRegEx" : "Gammerman et al\\.",
      "year" : 1998
    }, {
      "title" : "Tikhonov regularization and total least squares",
      "author" : [ "Gene H. Golub", "Per Christian Hansen", "Dianne P. O’Leary" ],
      "venue" : "SIAM J. Matrix Analysis Applications 21(1):185–194",
      "citeRegEx" : "Golub et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Golub et al\\.",
      "year" : 1999
    }, {
      "title" : "Automatic acquisition of hyponyms from large text corpora",
      "author" : [ "Marti A. Hearst." ],
      "venue" : "14th International Conference on Computational Linguistics. pages 539–545.",
      "citeRegEx" : "Hearst.,? 1992",
      "shortCiteRegEx" : "Hearst.",
      "year" : 1992
    }, {
      "title" : "Sinica BOW (bilingual ontological wordnet): Integration of bilingual wordnet and SUMO",
      "author" : [ "Chu-Ren Huang", "Ru-Yng Chang", "Hshiang-Pin Lee." ],
      "venue" : "Proceedings of the Fourth International Conference on Language Resources and Evaluation.",
      "citeRegEx" : "Huang et al\\.,? 2004",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2004
    }, {
      "title" : "Directional distributional similarity for lexical inference",
      "author" : [ "Lili Kotlerman", "Ido Dagan", "Idan Szpektor", "Maayan Zhitomirsky-Geffet." ],
      "venue" : "Natural Language Engineering 16(4):359–389.",
      "citeRegEx" : "Kotlerman et al\\.,? 2010",
      "shortCiteRegEx" : "Kotlerman et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning arguments and supertypes of semantic relations using recursive patterns",
      "author" : [ "Zornitsa Kozareva", "Eduard H. Hovy." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. pages 1482–1491.",
      "citeRegEx" : "Kozareva and Hovy.,? 2010",
      "shortCiteRegEx" : "Kozareva and Hovy.",
      "year" : 2010
    }, {
      "title" : "Identifying hypernyms in distributional semantic spaces",
      "author" : [ "Alessandro Lenci", "Giulia Benotto." ],
      "venue" : "Proceedings of the Sixth International Workshop on Semantic Evaluation. pages 543–546.",
      "citeRegEx" : "Lenci and Benotto.,? 2012",
      "shortCiteRegEx" : "Lenci and Benotto.",
      "year" : 2012
    }, {
      "title" : "A relation extraction method of chinese named entities based on location and semantic features",
      "author" : [ "Hai-Guang Li", "Xindong Wu", "Zhao Li", "Gong-Qing Wu." ],
      "venue" : "Appl. Intell. 38(1):1–15.",
      "citeRegEx" : "Li et al\\.,? 2013",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2013
    }, {
      "title" : "User generated content oriented chinese taxonomy construction",
      "author" : [ "Jinyang Li", "Chengyu Wang", "Xiaofeng He", "Rong Zhang", "Ming Gao." ],
      "venue" : "Web Technologies and Applications - 17th Asia-Pacific Web Conference. pages 623–634.",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Bipartite edge prediction via transductive learning over product graphs",
      "author" : [ "Hanxiao Liu", "Yiming Yang." ],
      "venue" : "Proceedings of the 32nd International Conference on Machine Learning. pages 1880– 1888.",
      "citeRegEx" : "Liu and Yang.,? 2015",
      "shortCiteRegEx" : "Liu and Yang.",
      "year" : 2015
    }, {
      "title" : "Taxonomy induction from chinese encyclopedias by combinatorial optimization",
      "author" : [ "Weiming Lu", "Renjie Lou", "Hao Dai", "Zhenyu Zhang", "Shansong Yang", "Baogang Wei." ],
      "venue" : "Proceedings of the 4th CCF Conference on Natural Language Processing",
      "citeRegEx" : "Lu et al\\.,? 2015",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2015
    }, {
      "title" : "Characterization of a class of sigmoid functions with applications to neural networks",
      "author" : [ "Anil Menon", "Kishan Mehrotra", "Chilukuri K. Mohan", "Sanjay Ranka." ],
      "venue" : "Neural Networks 9(5):819–835.",
      "citeRegEx" : "Menon et al\\.,? 1996",
      "shortCiteRegEx" : "Menon et al\\.",
      "year" : 1996
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "CoRR abs/1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A. Miller." ],
      "venue" : "Communications of the Acm 38(11):39–",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "On the contribution of word embeddings to temporal relation classification",
      "author" : [ "Paramita Mirza", "Sara Tonelli." ],
      "venue" : "Proceedings of the 26th International Conference on Computational Linguistics. pages 2818–2828.",
      "citeRegEx" : "Mirza and Tonelli.,? 2016",
      "shortCiteRegEx" : "Mirza and Tonelli.",
      "year" : 2016
    }, {
      "title" : "Espresso: Leveraging generic patterns for automatically harvesting semantic relations",
      "author" : [ "Patrick Pantel", "Marco Pennacchiotti." ],
      "venue" : "Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the",
      "citeRegEx" : "Pantel and Pennacchiotti.,? 2006",
      "shortCiteRegEx" : "Pantel and Pennacchiotti.",
      "year" : 2006
    }, {
      "title" : "Deriving a large-scale taxonomy from wikipedia",
      "author" : [ "Simone Paolo Ponzetto", "Michael Strube." ],
      "venue" : "Proceedings of the Twenty-Second AAAI Conference on Artificial Intelligence. pages 1440–1445.",
      "citeRegEx" : "Ponzetto and Strube.,? 2007",
      "shortCiteRegEx" : "Ponzetto and Strube.",
      "year" : 2007
    }, {
      "title" : "AFET: automatic finegrained entity typing by hierarchical partial-label embedding",
      "author" : [ "Xiang Ren", "Wenqi He", "Meng Qu", "Lifu Huang", "Heng Ji", "Jiawei Han." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Pro-",
      "citeRegEx" : "Ren et al\\.,? 2016",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2016
    }, {
      "title" : "What is this, anyway: Automatic hypernym discovery",
      "author" : [ "Alan Ritter", "Stephen Soderland", "Oren Etzioni." ],
      "venue" : "Learning by Reading and Learning to Read, the 2009 AAAI Spring Symposium. pages 88–",
      "citeRegEx" : "Ritter et al\\.,? 2009",
      "shortCiteRegEx" : "Ritter et al\\.",
      "year" : 2009
    }, {
      "title" : "Extracting hypernym pairs from the web",
      "author" : [ "Erik F. Tjong Kim Sang." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Sang.,? 2007",
      "shortCiteRegEx" : "Sang.",
      "year" : 2007
    }, {
      "title" : "Lexical patterns or dependency patterns: Which is better for hypernym extraction? In Proceedings of the Thirteenth Conference on Computational Natural Language Learning",
      "author" : [ "Erik F. Tjong Kim Sang", "Katja Hofmann." ],
      "venue" : "pages 174–182.",
      "citeRegEx" : "Sang and Hofmann.,? 2009",
      "shortCiteRegEx" : "Sang and Hofmann.",
      "year" : 2009
    }, {
      "title" : "Chasing hypernyms in vector spaces with entropy",
      "author" : [ "Enrico Santus", "Alessandro Lenci", "Qin Lu", "Sabine Schulte im Walde." ],
      "venue" : "Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Santus et al\\.,? 2014",
      "shortCiteRegEx" : "Santus et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving hypernymy detection with an integrated path-based and distributional method",
      "author" : [ "Vered Shwartz", "Yoav Goldberg", "Ido Dagan." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics.",
      "citeRegEx" : "Shwartz et al\\.,? 2016",
      "shortCiteRegEx" : "Shwartz et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning syntactic patterns for automatic hypernym discovery",
      "author" : [ "Rion Snow", "Daniel Jurafsky", "Andrew Y. Ng." ],
      "venue" : "Advances in Neural Information Processing Systems 17, NIPS 2004. pages 1297–1304.",
      "citeRegEx" : "Snow et al\\.,? 2004",
      "shortCiteRegEx" : "Snow et al\\.",
      "year" : 2004
    }, {
      "title" : "Yago: a core of semantic knowledge",
      "author" : [ "Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum." ],
      "venue" : "Proceedings of the 16th International Conference on World Wide Web. pages 697–706.",
      "citeRegEx" : "Suchanek et al\\.,? 2007",
      "shortCiteRegEx" : "Suchanek et al\\.",
      "year" : 2007
    }, {
      "title" : "Instance-based evaluation of entailment rule acquisition",
      "author" : [ "Idan Szpektor", "Eyal Shnarch", "Ido Dagan." ],
      "venue" : "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics. page 456–463.",
      "citeRegEx" : "Szpektor et al\\.,? 2007",
      "shortCiteRegEx" : "Szpektor et al\\.",
      "year" : 2007
    }, {
      "title" : "Challenges in chinese knowledge graph construction",
      "author" : [ "Chengyu Wang", "Ming Gao", "Xiaofeng He", "Rong Zhang." ],
      "venue" : "Proceedings of the 31st IEEE International Conference on Data Engineering Workshops. pages 59–61.",
      "citeRegEx" : "Wang et al\\.,? 2015",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Chinese hypernym-hyponym extraction from user generated categories",
      "author" : [ "Chengyu Wang", "Xiaofeng He." ],
      "venue" : "Proceedings of the 26th International Conference on Computational Linguistics. pages 1350–1361.",
      "citeRegEx" : "Wang and He.,? 2016",
      "shortCiteRegEx" : "Wang and He.",
      "year" : 2016
    }, {
      "title" : "Cbuilding the chinese open wordnet (cow): Starting from core synsets",
      "author" : [ "Shan Wang", "Francis Bond." ],
      "venue" : "Proceedings of the 11th Workshop on Asian Language Resources: ALR-2013 a Workshop of The 6th International Joint Conference on Natu-",
      "citeRegEx" : "Wang and Bond.,? 2013",
      "shortCiteRegEx" : "Wang and Bond.",
      "year" : 2013
    }, {
      "title" : "Probase: a probabilistic taxonomy for text understanding",
      "author" : [ "Wentao Wu", "Hongsong Li", "Haixun Wang", "Kenny Qili Zhu." ],
      "venue" : "Proceedings of the ACM SIGMOD International Conference on Management of Data. pages 481–492.",
      "citeRegEx" : "Wu et al\\.,? 2012",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2012
    }, {
      "title" : "An integrated approach for automatic construction of bilingual chinese-english wordnet",
      "author" : [ "Renjie Xu", "Zhiqiang Gao", "Yingji Pan", "Yuzhong Qu", "Zhisheng Huang." ],
      "venue" : "The Semantic Web, Proceedings of the 3rd Asian Semantic Web Conference. pages 302–",
      "citeRegEx" : "Xu et al\\.,? 2008",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2008
    }, {
      "title" : "Cross-lingual text classification via model translation with limited dictionaries",
      "author" : [ "Ruochen Xu", "Yiming Yang", "Hanxiao Liu", "Andrew Hsi." ],
      "venue" : "Proceedings of the 25th ACM International on Conference on Information and Knowledge Management.",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 37,
      "context" : "The accurate prediction of hypernyms benefits a variety of NLP tasks, such as taxonomy learning (Wu et al., 2012; Fu et al., 2014), fine-grained entity categorization (Ren et al.",
      "startOffset" : 96,
      "endOffset" : 130
    }, {
      "referenceID" : 6,
      "context" : "The accurate prediction of hypernyms benefits a variety of NLP tasks, such as taxonomy learning (Wu et al., 2012; Fu et al., 2014), fine-grained entity categorization (Ren et al.",
      "startOffset" : 96,
      "endOffset" : 130
    }, {
      "referenceID" : 25,
      "context" : ", 2014), fine-grained entity categorization (Ren et al., 2016), knowledge base construction (Suchanek et al.",
      "startOffset" : 44,
      "endOffset" : 62
    }, {
      "referenceID" : 32,
      "context" : ", 2016), knowledge base construction (Suchanek et al., 2007), etc.",
      "startOffset" : 37,
      "endOffset" : 60
    }, {
      "referenceID" : 31,
      "context" : "The followingup work mostly focuses on is-a relation extraction using automatically generated patterns (Snow et al., 2004; Ritter et al., 2009; Sang and Hofmann, 2009; Kozareva and Hovy, 2010) and relation inference based on distributional similarity measures (Kotlerman et al.",
      "startOffset" : 103,
      "endOffset" : 192
    }, {
      "referenceID" : 26,
      "context" : "The followingup work mostly focuses on is-a relation extraction using automatically generated patterns (Snow et al., 2004; Ritter et al., 2009; Sang and Hofmann, 2009; Kozareva and Hovy, 2010) and relation inference based on distributional similarity measures (Kotlerman et al.",
      "startOffset" : 103,
      "endOffset" : 192
    }, {
      "referenceID" : 28,
      "context" : "The followingup work mostly focuses on is-a relation extraction using automatically generated patterns (Snow et al., 2004; Ritter et al., 2009; Sang and Hofmann, 2009; Kozareva and Hovy, 2010) and relation inference based on distributional similarity measures (Kotlerman et al.",
      "startOffset" : 103,
      "endOffset" : 192
    }, {
      "referenceID" : 13,
      "context" : "The followingup work mostly focuses on is-a relation extraction using automatically generated patterns (Snow et al., 2004; Ritter et al., 2009; Sang and Hofmann, 2009; Kozareva and Hovy, 2010) and relation inference based on distributional similarity measures (Kotlerman et al.",
      "startOffset" : 103,
      "endOffset" : 192
    }, {
      "referenceID" : 12,
      "context" : ", 2009; Sang and Hofmann, 2009; Kozareva and Hovy, 2010) and relation inference based on distributional similarity measures (Kotlerman et al., 2010; Lenci and Benotto, 2012; Shwartz et al., 2016).",
      "startOffset" : 124,
      "endOffset" : 195
    }, {
      "referenceID" : 14,
      "context" : ", 2009; Sang and Hofmann, 2009; Kozareva and Hovy, 2010) and relation inference based on distributional similarity measures (Kotlerman et al., 2010; Lenci and Benotto, 2012; Shwartz et al., 2016).",
      "startOffset" : 124,
      "endOffset" : 195
    }, {
      "referenceID" : 30,
      "context" : ", 2009; Sang and Hofmann, 2009; Kozareva and Hovy, 2010) and relation inference based on distributional similarity measures (Kotlerman et al., 2010; Lenci and Benotto, 2012; Shwartz et al., 2016).",
      "startOffset" : 124,
      "endOffset" : 195
    }, {
      "referenceID" : 34,
      "context" : "From the linguistic perspective, Chinese is a lower-resourced language with very flexible expressions and grammatical rules (Wang et al., 2015).",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 6,
      "context" : "Hence, as previous research indicates, hypernym extraction methods for English are not necessarily suitable for the Chinese language (Fu et al., 2014; Wang et al., 2015; Wang and He, 2016).",
      "startOffset" : 133,
      "endOffset" : 188
    }, {
      "referenceID" : 34,
      "context" : "Hence, as previous research indicates, hypernym extraction methods for English are not necessarily suitable for the Chinese language (Fu et al., 2014; Wang et al., 2015; Wang and He, 2016).",
      "startOffset" : 133,
      "endOffset" : 188
    }, {
      "referenceID" : 35,
      "context" : "Hence, as previous research indicates, hypernym extraction methods for English are not necessarily suitable for the Chinese language (Fu et al., 2014; Wang et al., 2015; Wang and He, 2016).",
      "startOffset" : 133,
      "endOffset" : 188
    }, {
      "referenceID" : 18,
      "context" : "Based on such conditions, several classification methods are proposed to distinguish is-a and notis-a relations based on Chinese encyclopedias (Lu et al., 2015; Li et al., 2015).",
      "startOffset" : 143,
      "endOffset" : 177
    }, {
      "referenceID" : 16,
      "context" : "Based on such conditions, several classification methods are proposed to distinguish is-a and notis-a relations based on Chinese encyclopedias (Lu et al., 2015; Li et al., 2015).",
      "startOffset" : 143,
      "endOffset" : 177
    }, {
      "referenceID" : 11,
      "context" : "Similar to Princeton WordNet, a few Chinese wordnets have also been developed (Huang et al., 2004; Xu et al., 2008; Wang and Bond, 2013).",
      "startOffset" : 78,
      "endOffset" : 136
    }, {
      "referenceID" : 38,
      "context" : "Similar to Princeton WordNet, a few Chinese wordnets have also been developed (Huang et al., 2004; Xu et al., 2008; Wang and Bond, 2013).",
      "startOffset" : 78,
      "endOffset" : 136
    }, {
      "referenceID" : 36,
      "context" : "Similar to Princeton WordNet, a few Chinese wordnets have also been developed (Huang et al., 2004; Xu et al., 2008; Wang and Bond, 2013).",
      "startOffset" : 78,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "The most recent approaches for Chinese is-a relation extraction (Fu et al., 2014; Wang and He, 2016) use word embedding based linear projection models to map embeddings of hyponyms to those of their hypernyms, which outperform previous algorithms.",
      "startOffset" : 64,
      "endOffset" : 100
    }, {
      "referenceID" : 35,
      "context" : "The most recent approaches for Chinese is-a relation extraction (Fu et al., 2014; Wang and He, 2016) use word embedding based linear projection models to map embeddings of hyponyms to those of their hypernyms, which outperform previous algorithms.",
      "startOffset" : 64,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : ", 2012; Fu et al., 2014), fine-grained entity categorization (Ren et al., 2016), knowledge base construction (Suchanek et al., 2007), etc. In previous work, the detection of hypernyms requires lexical, syntactic and/or semantic analysis of relations between entities and their respective hypernyms from a language-specific knowledge source. For example, Hearst (1992) is the pioneer work to extract is-a relations from a text corpus based on handcraft patterns.",
      "startOffset" : 8,
      "endOffset" : 368
    }, {
      "referenceID" : 10,
      "context" : "Hearst patterns (Hearst, 1992) are lexical patterns in English that are employed to extract isa relations for taxonomy construction (Wu et al.",
      "startOffset" : 16,
      "endOffset" : 30
    }, {
      "referenceID" : 37,
      "context" : "Hearst patterns (Hearst, 1992) are lexical patterns in English that are employed to extract isa relations for taxonomy construction (Wu et al., 2012).",
      "startOffset" : 132,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "A few relevant studies can be found in (Caraballo, 1999; Etzioni et al., 2004; Sang, 2007; Pantel and Pennacchiotti, 2006; Kozareva and Hovy, 2010).",
      "startOffset" : 39,
      "endOffset" : 147
    }, {
      "referenceID" : 5,
      "context" : "A few relevant studies can be found in (Caraballo, 1999; Etzioni et al., 2004; Sang, 2007; Pantel and Pennacchiotti, 2006; Kozareva and Hovy, 2010).",
      "startOffset" : 39,
      "endOffset" : 147
    }, {
      "referenceID" : 27,
      "context" : "A few relevant studies can be found in (Caraballo, 1999; Etzioni et al., 2004; Sang, 2007; Pantel and Pennacchiotti, 2006; Kozareva and Hovy, 2010).",
      "startOffset" : 39,
      "endOffset" : 147
    }, {
      "referenceID" : 23,
      "context" : "A few relevant studies can be found in (Caraballo, 1999; Etzioni et al., 2004; Sang, 2007; Pantel and Pennacchiotti, 2006; Kozareva and Hovy, 2010).",
      "startOffset" : 39,
      "endOffset" : 147
    }, {
      "referenceID" : 13,
      "context" : "A few relevant studies can be found in (Caraballo, 1999; Etzioni et al., 2004; Sang, 2007; Pantel and Pennacchiotti, 2006; Kozareva and Hovy, 2010).",
      "startOffset" : 39,
      "endOffset" : 147
    }, {
      "referenceID" : 6,
      "context" : "not effective for Chinese for two reasons: i) Chinese is-a relations are expressed in a highly flexible manner (Fu et al., 2014) and ii) the accuracy of basic NLP tasks such as dependency parsing still need improvement for Chinese (Li et al.",
      "startOffset" : 111,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : ", 2014) and ii) the accuracy of basic NLP tasks such as dependency parsing still need improvement for Chinese (Li et al., 2013).",
      "startOffset" : 110,
      "endOffset" : 127
    }, {
      "referenceID" : 1,
      "context" : "Other DSMs are introduced in (Bhagat et al., 2007; Szpektor et al., 2007; Lenci and Benotto, 2012; Santus et al., 2014).",
      "startOffset" : 29,
      "endOffset" : 119
    }, {
      "referenceID" : 33,
      "context" : "Other DSMs are introduced in (Bhagat et al., 2007; Szpektor et al., 2007; Lenci and Benotto, 2012; Santus et al., 2014).",
      "startOffset" : 29,
      "endOffset" : 119
    }, {
      "referenceID" : 14,
      "context" : "Other DSMs are introduced in (Bhagat et al., 2007; Szpektor et al., 2007; Lenci and Benotto, 2012; Santus et al., 2014).",
      "startOffset" : 29,
      "endOffset" : 119
    }, {
      "referenceID" : 29,
      "context" : "Other DSMs are introduced in (Bhagat et al., 2007; Szpektor et al., 2007; Lenci and Benotto, 2012; Santus et al., 2014).",
      "startOffset" : 29,
      "endOffset" : 119
    }, {
      "referenceID" : 32,
      "context" : "The taxonomy in YAGO (Suchanek et al., 2007) is constructed by linking conceptual categories in Wikipedia to WordNet synsets (Miller, 1995).",
      "startOffset" : 21,
      "endOffset" : 44
    }, {
      "referenceID" : 21,
      "context" : ", 2007) is constructed by linking conceptual categories in Wikipedia to WordNet synsets (Miller, 1995).",
      "startOffset" : 88,
      "endOffset" : 102
    }, {
      "referenceID" : 7,
      "context" : "Similar classification based algorithms are presented in (Fu et al., 2013; Lu et al., 2015).",
      "startOffset" : 57,
      "endOffset" : 91
    }, {
      "referenceID" : 18,
      "context" : "Similar classification based algorithms are presented in (Fu et al., 2013; Lu et al., 2015).",
      "startOffset" : 57,
      "endOffset" : 91
    }, {
      "referenceID" : 11,
      "context" : "Due to the lack of Chinese version of WordNet, several Chinese semantic dictionaries have been conducted, such as Sinica BOW (Huang et al., 2004), SEW (Xu et al.",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 38,
      "context" : ", 2004), SEW (Xu et al., 2008), COW (Wang and Bond, 2013), etc.",
      "startOffset" : 13,
      "endOffset" : 30
    }, {
      "referenceID" : 36,
      "context" : ", 2008), COW (Wang and Bond, 2013), etc.",
      "startOffset" : 13,
      "endOffset" : 34
    }, {
      "referenceID" : 20,
      "context" : "The vectors can be pretrained by neural language models (Mikolov et al., 2013).",
      "startOffset" : 56,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "A few relevant studies can be found in (Caraballo, 1999; Etzioni et al., 2004; Sang, 2007; Pantel and Pennacchiotti, 2006; Kozareva and Hovy, 2010). To avoid “semantic drift” in iterations, Snow et al. (2004) train a hypernym classifier based on syntactic features based on parse trees.",
      "startOffset" : 40,
      "endOffset" : 209
    }, {
      "referenceID" : 1,
      "context" : "A few relevant studies can be found in (Caraballo, 1999; Etzioni et al., 2004; Sang, 2007; Pantel and Pennacchiotti, 2006; Kozareva and Hovy, 2010). To avoid “semantic drift” in iterations, Snow et al. (2004) train a hypernym classifier based on syntactic features based on parse trees. Carlson et al. (2010) exploit multiple learners to extract relations via coupled learning.",
      "startOffset" : 40,
      "endOffset" : 309
    }, {
      "referenceID" : 1,
      "context" : "A few relevant studies can be found in (Caraballo, 1999; Etzioni et al., 2004; Sang, 2007; Pantel and Pennacchiotti, 2006; Kozareva and Hovy, 2010). To avoid “semantic drift” in iterations, Snow et al. (2004) train a hypernym classifier based on syntactic features based on parse trees. Carlson et al. (2010) exploit multiple learners to extract relations via coupled learning. These approaches are While there is abundant research on hypernym prediction for English with high precision, we mostly focus on the Chinese language in this paper. However, the proposed method is not entirely language-specific and has the potential to be adapted to other languages. In Section 4, we provide additional experiments to show that our approach also outperforms several existing methods for hypernym prediction in the English environment. We also discuss some potential applications of our method. Additionally, we present more details on algorithms and experimental settings and a prototype system for taxonomy visualization in the supplementary notes. not effective for Chinese for two reasons: i) Chinese is-a relations are expressed in a highly flexible manner (Fu et al., 2014) and ii) the accuracy of basic NLP tasks such as dependency parsing still need improvement for Chinese (Li et al., 2013). Inference based methods take advantage of distributional similarity measures (DSM) to infer relations between words. They assume that a hypernym may appear in all contexts of the hyponyms and a hyponym can only appear in part of the contexts of its hypernyms. In previous work, Kotlerman et al. (2010) design directional DSMs to model the asymmetric property of is-a relations.",
      "startOffset" : 40,
      "endOffset" : 1597
    }, {
      "referenceID" : 1,
      "context" : "Other DSMs are introduced in (Bhagat et al., 2007; Szpektor et al., 2007; Lenci and Benotto, 2012; Santus et al., 2014). Shwartz et al. (2016) combine dependency parsing and DSM to improve the performance of hypernymy detection.",
      "startOffset" : 30,
      "endOffset" : 143
    }, {
      "referenceID" : 1,
      "context" : "Other DSMs are introduced in (Bhagat et al., 2007; Szpektor et al., 2007; Lenci and Benotto, 2012; Santus et al., 2014). Shwartz et al. (2016) combine dependency parsing and DSM to improve the performance of hypernymy detection. The reason why DSM is not effective for Chinese is that the contexts of entities in Chinese are flexible and sparse. Encyclopedia based methods take encyclopedias as knowledge sources to construct taxonomies. Ponzetto and Strube (2007) design features from multiple aspects to predict is-a relations between entities and categories in English Wikipedia.",
      "startOffset" : 30,
      "endOffset" : 465
    }, {
      "referenceID" : 1,
      "context" : "Other DSMs are introduced in (Bhagat et al., 2007; Szpektor et al., 2007; Lenci and Benotto, 2012; Santus et al., 2014). Shwartz et al. (2016) combine dependency parsing and DSM to improve the performance of hypernymy detection. The reason why DSM is not effective for Chinese is that the contexts of entities in Chinese are flexible and sparse. Encyclopedia based methods take encyclopedias as knowledge sources to construct taxonomies. Ponzetto and Strube (2007) design features from multiple aspects to predict is-a relations between entities and categories in English Wikipedia. The taxonomy in YAGO (Suchanek et al., 2007) is constructed by linking conceptual categories in Wikipedia to WordNet synsets (Miller, 1995). For Chinese, Li et al. (2015) propose an SVM-based approach to build a large Chinese taxonomy from Wikipedia.",
      "startOffset" : 30,
      "endOffset" : 754
    }, {
      "referenceID" : 1,
      "context" : "Other DSMs are introduced in (Bhagat et al., 2007; Szpektor et al., 2007; Lenci and Benotto, 2012; Santus et al., 2014). Shwartz et al. (2016) combine dependency parsing and DSM to improve the performance of hypernymy detection. The reason why DSM is not effective for Chinese is that the contexts of entities in Chinese are flexible and sparse. Encyclopedia based methods take encyclopedias as knowledge sources to construct taxonomies. Ponzetto and Strube (2007) design features from multiple aspects to predict is-a relations between entities and categories in English Wikipedia. The taxonomy in YAGO (Suchanek et al., 2007) is constructed by linking conceptual categories in Wikipedia to WordNet synsets (Miller, 1995). For Chinese, Li et al. (2015) propose an SVM-based approach to build a large Chinese taxonomy from Wikipedia. Similar classification based algorithms are presented in (Fu et al., 2013; Lu et al., 2015). Due to the lack of Chinese version of WordNet, several Chinese semantic dictionaries have been conducted, such as Sinica BOW (Huang et al., 2004), SEW (Xu et al., 2008), COW (Wang and Bond, 2013), etc. These approaches have higher accuracy than mining hypernym relations from texts directly. However, they heavily rely on existing knowledge sources and are difficult to extend to different domains. To tackle these challenges, word embedding based methods directly model the task of hypernym prediction as learning a mapping from entity vectors to their respective hypernym vectors in the embedding space. The vectors can be pretrained by neural language models (Mikolov et al., 2013). For the Chinese language, Fu et al. (2014) train piecewise linear projection models based on a Chinese thesaurus.",
      "startOffset" : 30,
      "endOffset" : 1656
    }, {
      "referenceID" : 35,
      "context" : "(Wang and He, 2016) combines an iterative learning procedure and Chinese Hearst-style patterns to improve the performance of projection models.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 22,
      "context" : "Additionally, several work aims to study how to combine word embeddings for relation classification, such as (Mirza and Tonelli, 2016).",
      "startOffset" : 109,
      "endOffset" : 134
    }, {
      "referenceID" : 20,
      "context" : "We first train a Skip-gram model (Mikolov et al., 2013) to learn word embeddings over a large text corpus.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "(Fu et al., 2014; Wang and He, 2016), for each (xi, yi) ∈ D+, we assume there is a positive projection model such that Mxi ≈ yi where M+ is an |xi| × |xi| projection matrix3.",
      "startOffset" : 0,
      "endOffset" : 36
    }, {
      "referenceID" : 35,
      "context" : "(Fu et al., 2014; Wang and He, 2016), for each (xi, yi) ∈ D+, we assume there is a positive projection model such that Mxi ≈ yi where M+ is an |xi| × |xi| projection matrix3.",
      "startOffset" : 0,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "where λ > 0 is a Tikhonov regularization parameter (Golub et al., 1999).",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 19,
      "context" : "We choose the hyperbolic tangent function rather than the sigmoid function to avoid the widespread saturation of sigmoid function (Menon et al., 1996).",
      "startOffset" : 130,
      "endOffset" : 150
    }, {
      "referenceID" : 6,
      "context" : "conf(xi, yi) = |d(xi, yi)− d(xi, yi)| max{d(xi, yi), d−(xi, yi)} We have also examined piecewise linear projection models proposed in (Fu et al., 2014; Wang and He, 2016) as the initial models for transductive learning.",
      "startOffset" : 134,
      "endOffset" : 170
    }, {
      "referenceID" : 35,
      "context" : "conf(xi, yi) = |d(xi, yi)− d(xi, yi)| max{d(xi, yi), d−(xi, yi)} We have also examined piecewise linear projection models proposed in (Fu et al., 2014; Wang and He, 2016) as the initial models for transductive learning.",
      "startOffset" : 134,
      "endOffset" : 170
    }, {
      "referenceID" : 8,
      "context" : "We present an optimization framework for non-linear mapping utilizing both labeled and unlabeled data and linguistic rules by transductive learning (Gammerman et al., 1998; Chapelle et al., 2006).",
      "startOffset" : 148,
      "endOffset" : 195
    }, {
      "referenceID" : 4,
      "context" : "We present an optimization framework for non-linear mapping utilizing both labeled and unlabeled data and linguistic rules by transductive learning (Gammerman et al., 1998; Chapelle et al., 2006).",
      "startOffset" : 148,
      "endOffset" : 195
    }, {
      "referenceID" : 15,
      "context" : "For Chinese hypernym prediction, Li et al. (2015) study the word formation of conceptual categories in Chinese Wikipedia.",
      "startOffset" : 33,
      "endOffset" : 50
    }, {
      "referenceID" : 17,
      "context" : "3 Non-linear Learning TransLP is a transductive label propagation framework (Liu and Yang, 2015) for link prediction, previously used for applications such as text classification (Xu et al.",
      "startOffset" : 76,
      "endOffset" : 96
    }, {
      "referenceID" : 39,
      "context" : "3 Non-linear Learning TransLP is a transductive label propagation framework (Liu and Yang, 2015) for link prediction, previously used for applications such as text classification (Xu et al., 2016).",
      "startOffset" : 179,
      "endOffset" : 196
    }, {
      "referenceID" : 15,
      "context" : "N2 The head word of the candidate hypernym y matches an entry in a Chinese lexicon extended based on the lexicon used in Li et al., (2015). It consists of 184 non-taxonomic, thematic words such as政治(Politics),军事(Military), etc.",
      "startOffset" : 121,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "This similarity indicates there exists a nonlinear mapping from entities to hypernyms, which can not be encoded in linear projection based methods (Fu et al., 2014; Wang and He, 2016).",
      "startOffset" : 147,
      "endOffset" : 183
    }, {
      "referenceID" : 35,
      "context" : "This similarity indicates there exists a nonlinear mapping from entities to hypernyms, which can not be encoded in linear projection based methods (Fu et al., 2014; Wang and He, 2016).",
      "startOffset" : 147,
      "endOffset" : 183
    }, {
      "referenceID" : 17,
      "context" : "Based on TransLP (Liu and Yang, 2015), this intuition can be model as propagating class labels (is-a or not-is-a) of labeled word/phrase pairs to similar unlabeled ones based on Eq.",
      "startOffset" : 17,
      "endOffset" : 37
    }, {
      "referenceID" : 17,
      "context" : "Inspired by (Liu and Yang, 2015; Xu et al., 2016), we can add a Gaussian prior N(F∗,Σ) to F where Σ is the covariance matrix and Σi,j = sim(pi, pj).",
      "startOffset" : 12,
      "endOffset" : 49
    }, {
      "referenceID" : 39,
      "context" : "Inspired by (Liu and Yang, 2015; Xu et al., 2016), we can add a Gaussian prior N(F∗,Σ) to F where Σ is the covariance matrix and Σi,j = sim(pi, pj).",
      "startOffset" : 12,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "The first one (denoted as FD) is from Fu et al., (2014), containing 1,391 is-a pairs and 4,294 not-is-a pairs, which is the first publicly available dataset to evaluate this task.",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 35,
      "context" : "In a series of previous work (Fu et al., 2013, 2014; Wang and He, 2016), several pattern-based, inference-based and encyclopedia-based is-a relation extraction methods for English have been implemented for the Chinese language.",
      "startOffset" : 29,
      "endOffset" : 71
    }, {
      "referenceID" : 6,
      "context" : "To make the convincing conclusion, we employ two recent state-of-the-art approaches for Chinese is-a relation identification (Fu et al., 2014; Wang and He, 2016) as baselines.",
      "startOffset" : 125,
      "endOffset" : 161
    }, {
      "referenceID" : 35,
      "context" : "To make the convincing conclusion, we employ two recent state-of-the-art approaches for Chinese is-a relation identification (Fu et al., 2014; Wang and He, 2016) as baselines.",
      "startOffset" : 125,
      "endOffset" : 161
    }, {
      "referenceID" : 22,
      "context" : "We also take the word embedding based classification approach (Mirza and Tonelli, 2016)9 and Chinese Wikipedia based SVM model (Li et al.",
      "startOffset" : 62,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "We also take the word embedding based classification approach (Mirza and Tonelli, 2016)9 and Chinese Wikipedia based SVM model (Li et al., 2015) as baselines to predict is-a relations between words10.",
      "startOffset" : 127,
      "endOffset" : 144
    }, {
      "referenceID" : 35,
      "context" : "By combining iterative projection models and pattern-based validation, the most recent approach (Wang and He, 2016) increases the F-measure by 4% and 2% in two datasets.",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 6,
      "context" : "In a series of previous work (Fu et al., 2013, 2014; Wang and He, 2016), several pattern-based, inference-based and encyclopedia-based is-a relation extraction methods for English have been implemented for the Chinese language. As their experiments show, these methods achieve the Fmeasure of lower than 60% in most cases, which are not suggested to be strong baselines for Chinese hypernym prediction. Interested readers may refer to their papers for the experimental results. To make the convincing conclusion, we employ two recent state-of-the-art approaches for Chinese is-a relation identification (Fu et al., 2014; Wang and He, 2016) as baselines. We also take the word embedding based classification approach (Mirza and Tonelli, 2016)9 and Chinese Wikipedia based SVM model (Li et al., 2015) as baselines to predict is-a relations between words10. The experimental results are illustrated in Table 3. For Fu et al., (2014), we test the performance using a linear projection model (denoted as S in Table 3) and piecewise projection models (P).",
      "startOffset" : 30,
      "endOffset" : 930
    }, {
      "referenceID" : 6,
      "context" : "Dataset FD Method P R F Fu et al., (2014) (S) 64.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 6,
      "context" : "Dataset FD Method P R F Fu et al., (2014) (S) 64.1 56.0 59.8 Fu et al., (2014) (P) 66.",
      "startOffset" : 24,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "Dataset FD Method P R F Fu et al., (2014) (S) 64.1 56.0 59.8 Fu et al., (2014) (P) 66.4 59.3 62.6 Li et al., (2015) 54.",
      "startOffset" : 24,
      "endOffset" : 116
    }, {
      "referenceID" : 6,
      "context" : "Dataset FD Method P R F Fu et al., (2014) (S) 64.1 56.0 59.8 Fu et al., (2014) (P) 66.4 59.3 62.6 Li et al., (2015) 54.3 38.4 45.0 Mirza and Tonelli, (2016) (C) 67.",
      "startOffset" : 24,
      "endOffset" : 157
    }, {
      "referenceID" : 6,
      "context" : "Dataset FD Method P R F Fu et al., (2014) (S) 64.1 56.0 59.8 Fu et al., (2014) (P) 66.4 59.3 62.6 Li et al., (2015) 54.3 38.4 45.0 Mirza and Tonelli, (2016) (C) 67.7 75.2 69.7 Mirza and Tonelli, (2016) (A) 65.",
      "startOffset" : 24,
      "endOffset" : 202
    }, {
      "referenceID" : 6,
      "context" : "Dataset FD Method P R F Fu et al., (2014) (S) 64.1 56.0 59.8 Fu et al., (2014) (P) 66.4 59.3 62.6 Li et al., (2015) 54.3 38.4 45.0 Mirza and Tonelli, (2016) (C) 67.7 75.2 69.7 Mirza and Tonelli, (2016) (A) 65.3 60.7 62.9 Mirza and Tonelli, (2016) (S) 71.",
      "startOffset" : 24,
      "endOffset" : 247
    }, {
      "referenceID" : 6,
      "context" : "Dataset FD Method P R F Fu et al., (2014) (S) 64.1 56.0 59.8 Fu et al., (2014) (P) 66.4 59.3 62.6 Li et al., (2015) 54.3 38.4 45.0 Mirza and Tonelli, (2016) (C) 67.7 75.2 69.7 Mirza and Tonelli, (2016) (A) 65.3 60.7 62.9 Mirza and Tonelli, (2016) (S) 71.9 60.6 65.7 Wang and He, (2016) 69.",
      "startOffset" : 24,
      "endOffset" : 286
    }, {
      "referenceID" : 6,
      "context" : "Dataset FD Method P R F Fu et al., (2014) (S) 64.1 56.0 59.8 Fu et al., (2014) (P) 66.4 59.3 62.6 Li et al., (2015) 54.3 38.4 45.0 Mirza and Tonelli, (2016) (C) 67.7 75.2 69.7 Mirza and Tonelli, (2016) (A) 65.3 60.7 62.9 Mirza and Tonelli, (2016) (S) 71.9 60.6 65.7 Wang and He, (2016) 69.3 64.5 66.9 Ours (Initial) 70.7 69.2 69.9 Ours 72.8 70.5 71.6 Dataset BK Method P R F Fu et al., (2014) (S) 71.",
      "startOffset" : 24,
      "endOffset" : 393
    }, {
      "referenceID" : 6,
      "context" : "Dataset FD Method P R F Fu et al., (2014) (S) 64.1 56.0 59.8 Fu et al., (2014) (P) 66.4 59.3 62.6 Li et al., (2015) 54.3 38.4 45.0 Mirza and Tonelli, (2016) (C) 67.7 75.2 69.7 Mirza and Tonelli, (2016) (A) 65.3 60.7 62.9 Mirza and Tonelli, (2016) (S) 71.9 60.6 65.7 Wang and He, (2016) 69.3 64.5 66.9 Ours (Initial) 70.7 69.2 69.9 Ours 72.8 70.5 71.6 Dataset BK Method P R F Fu et al., (2014) (S) 71.4 64.8 67.9 Fu et al., (2014) (P) 72.",
      "startOffset" : 24,
      "endOffset" : 430
    }, {
      "referenceID" : 6,
      "context" : "Dataset FD Method P R F Fu et al., (2014) (S) 64.1 56.0 59.8 Fu et al., (2014) (P) 66.4 59.3 62.6 Li et al., (2015) 54.3 38.4 45.0 Mirza and Tonelli, (2016) (C) 67.7 75.2 69.7 Mirza and Tonelli, (2016) (A) 65.3 60.7 62.9 Mirza and Tonelli, (2016) (S) 71.9 60.6 65.7 Wang and He, (2016) 69.3 64.5 66.9 Ours (Initial) 70.7 69.2 69.9 Ours 72.8 70.5 71.6 Dataset BK Method P R F Fu et al., (2014) (S) 71.4 64.8 67.9 Fu et al., (2014) (P) 72.7 67.5 70.0 Li et al., (2015) 61.",
      "startOffset" : 24,
      "endOffset" : 467
    }, {
      "referenceID" : 6,
      "context" : "Dataset FD Method P R F Fu et al., (2014) (S) 64.1 56.0 59.8 Fu et al., (2014) (P) 66.4 59.3 62.6 Li et al., (2015) 54.3 38.4 45.0 Mirza and Tonelli, (2016) (C) 67.7 75.2 69.7 Mirza and Tonelli, (2016) (A) 65.3 60.7 62.9 Mirza and Tonelli, (2016) (S) 71.9 60.6 65.7 Wang and He, (2016) 69.3 64.5 66.9 Ours (Initial) 70.7 69.2 69.9 Ours 72.8 70.5 71.6 Dataset BK Method P R F Fu et al., (2014) (S) 71.4 64.8 67.9 Fu et al., (2014) (P) 72.7 67.5 70.0 Li et al., (2015) 61.2 47.5 53.5 Mirza and Tonelli, (2016) (C) 80.",
      "startOffset" : 24,
      "endOffset" : 508
    }, {
      "referenceID" : 6,
      "context" : "Dataset FD Method P R F Fu et al., (2014) (S) 64.1 56.0 59.8 Fu et al., (2014) (P) 66.4 59.3 62.6 Li et al., (2015) 54.3 38.4 45.0 Mirza and Tonelli, (2016) (C) 67.7 75.2 69.7 Mirza and Tonelli, (2016) (A) 65.3 60.7 62.9 Mirza and Tonelli, (2016) (S) 71.9 60.6 65.7 Wang and He, (2016) 69.3 64.5 66.9 Ours (Initial) 70.7 69.2 69.9 Ours 72.8 70.5 71.6 Dataset BK Method P R F Fu et al., (2014) (S) 71.4 64.8 67.9 Fu et al., (2014) (P) 72.7 67.5 70.0 Li et al., (2015) 61.2 47.5 53.5 Mirza and Tonelli, (2016) (C) 80.3 75.9 78.0 Mirza and Tonelli, (2016) (A) 72.",
      "startOffset" : 24,
      "endOffset" : 553
    }, {
      "referenceID" : 6,
      "context" : "Dataset FD Method P R F Fu et al., (2014) (S) 64.1 56.0 59.8 Fu et al., (2014) (P) 66.4 59.3 62.6 Li et al., (2015) 54.3 38.4 45.0 Mirza and Tonelli, (2016) (C) 67.7 75.2 69.7 Mirza and Tonelli, (2016) (A) 65.3 60.7 62.9 Mirza and Tonelli, (2016) (S) 71.9 60.6 65.7 Wang and He, (2016) 69.3 64.5 66.9 Ours (Initial) 70.7 69.2 69.9 Ours 72.8 70.5 71.6 Dataset BK Method P R F Fu et al., (2014) (S) 71.4 64.8 67.9 Fu et al., (2014) (P) 72.7 67.5 70.0 Li et al., (2015) 61.2 47.5 53.5 Mirza and Tonelli, (2016) (C) 80.3 75.9 78.0 Mirza and Tonelli, (2016) (A) 72.7 65.6 68.9 Mirza and Tonelli, (2016) (S) 78.",
      "startOffset" : 24,
      "endOffset" : 598
    }, {
      "referenceID" : 6,
      "context" : "Dataset FD Method P R F Fu et al., (2014) (S) 64.1 56.0 59.8 Fu et al., (2014) (P) 66.4 59.3 62.6 Li et al., (2015) 54.3 38.4 45.0 Mirza and Tonelli, (2016) (C) 67.7 75.2 69.7 Mirza and Tonelli, (2016) (A) 65.3 60.7 62.9 Mirza and Tonelli, (2016) (S) 71.9 60.6 65.7 Wang and He, (2016) 69.3 64.5 66.9 Ours (Initial) 70.7 69.2 69.9 Ours 72.8 70.5 71.6 Dataset BK Method P R F Fu et al., (2014) (S) 71.4 64.8 67.9 Fu et al., (2014) (P) 72.7 67.5 70.0 Li et al., (2015) 61.2 47.5 53.5 Mirza and Tonelli, (2016) (C) 80.3 75.9 78.0 Mirza and Tonelli, (2016) (A) 72.7 65.6 68.9 Mirza and Tonelli, (2016) (S) 78.4 60.7 68.4 Wang and He, (2016) 73.",
      "startOffset" : 24,
      "endOffset" : 637
    }, {
      "referenceID" : 20,
      "context" : "Mirza and Tonelli, (2016) is implemented using three different strategies in combining the word vectors of a pair: i) concatenation xi ⊕ yi (denoted as C), ii) addition xi + yi (A) and iii) subtraction xi − yi (S).",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 15,
      "context" : "Although Li et al., (2015) achieve a high performance in their dataset, this method does not perform well in ours.",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 7,
      "context" : "Based on the literature study, we find that such problem has been also reported in (Fu et al., 2013; Wang and He, 2016).",
      "startOffset" : 83,
      "endOffset" : 119
    }, {
      "referenceID" : 35,
      "context" : "Based on the literature study, we find that such problem has been also reported in (Fu et al., 2013; Wang and He, 2016).",
      "startOffset" : 83,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : ", BLESS (Baroni and Lenci, 2011).",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 30,
      "context" : "Because the number of pairs in BLESS is relatively small, we also use the Shwartz (Shwartz et al., 2016) dataset.",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 10,
      "context" : ", (2016) for English, we use the original Hearst patterns (Hearst, 1992) to perform relation selection and do not consider not-isa patterns.",
      "startOffset" : 58,
      "endOffset" : 72
    }, {
      "referenceID" : 14,
      "context" : "We also take two recent DSM based approaches (Lenci and Benotto, 2012; Santus et al., 2014) as baselines.",
      "startOffset" : 45,
      "endOffset" : 91
    }, {
      "referenceID" : 29,
      "context" : "We also take two recent DSM based approaches (Lenci and Benotto, 2012; Santus et al., 2014) as baselines.",
      "startOffset" : 45,
      "endOffset" : 91
    }, {
      "referenceID" : 22,
      "context" : "In Shwartz, while the approach (Mirza and Tonelli, 2016) has the highest F-measure of 80.",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : ", BLESS (Baroni and Lenci, 2011). Because the number of pairs in BLESS is relatively small, we also use the Shwartz (Shwartz et al., 2016) dataset. In the experiments, we treat the HYPER relations as positive data (1,337 pairs) and randomly sample 30% of the RANDOM relations as negative data (3,754 pairs) in BLESS. To create a relatively balanced dataset, we take the random split of Shwartz as input and use only 30% of the negative pairs. The dataset contains 14,135 positive pairs and 16,956 negative pairs. The text corpus is English Wikipedia and the embedding vectors are set to 100 dimensions. For comparison, we test all the baselines over English datasets except Li et al., (2015). This is because most features in Li et al.",
      "startOffset" : 9,
      "endOffset" : 692
    }, {
      "referenceID" : 0,
      "context" : ", BLESS (Baroni and Lenci, 2011). Because the number of pairs in BLESS is relatively small, we also use the Shwartz (Shwartz et al., 2016) dataset. In the experiments, we treat the HYPER relations as positive data (1,337 pairs) and randomly sample 30% of the RANDOM relations as negative data (3,754 pairs) in BLESS. To create a relatively balanced dataset, we take the random split of Shwartz as input and use only 30% of the negative pairs. The dataset contains 14,135 positive pairs and 16,956 negative pairs. The text corpus is English Wikipedia and the embedding vectors are set to 100 dimensions. For comparison, we test all the baselines over English datasets except Li et al., (2015). This is because most features in Li et al., (2015) can only be used in the Chinese environment.",
      "startOffset" : 9,
      "endOffset" : 744
    }, {
      "referenceID" : 0,
      "context" : ", BLESS (Baroni and Lenci, 2011). Because the number of pairs in BLESS is relatively small, we also use the Shwartz (Shwartz et al., 2016) dataset. In the experiments, we treat the HYPER relations as positive data (1,337 pairs) and randomly sample 30% of the RANDOM relations as negative data (3,754 pairs) in BLESS. To create a relatively balanced dataset, we take the random split of Shwartz as input and use only 30% of the negative pairs. The dataset contains 14,135 positive pairs and 16,956 negative pairs. The text corpus is English Wikipedia and the embedding vectors are set to 100 dimensions. For comparison, we test all the baselines over English datasets except Li et al., (2015). This is because most features in Li et al., (2015) can only be used in the Chinese environment. To implement Wang and He., (2016) for English, we use the original Hearst patterns (Hearst, 1992) to perform relation selection and do not consider not-isa patterns.",
      "startOffset" : 9,
      "endOffset" : 823
    }, {
      "referenceID" : 12,
      "context" : "Dataset BLESS Method P R F Lenci and Benotto, (2012) 42.",
      "startOffset" : 27,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "Dataset BLESS Method P R F Lenci and Benotto, (2012) 42.8 38.6 40.6 Santus et al., (2014) 59.",
      "startOffset" : 27,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "4 Fu et al., (2014) (S) 65.",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "4 Fu et al., (2014) (S) 65.3 62.4 63.8 Fu et al., (2014) (P) 68.",
      "startOffset" : 2,
      "endOffset" : 57
    }, {
      "referenceID" : 6,
      "context" : "4 Fu et al., (2014) (S) 65.3 62.4 63.8 Fu et al., (2014) (P) 68.1 64.2 66.1 Mirza and Tonelli, (2016) (C) 79.",
      "startOffset" : 2,
      "endOffset" : 102
    }, {
      "referenceID" : 6,
      "context" : "4 Fu et al., (2014) (S) 65.3 62.4 63.8 Fu et al., (2014) (P) 68.1 64.2 66.1 Mirza and Tonelli, (2016) (C) 79.4 84.1 81.7 Mirza and Tonelli, (2016) (A) 80.",
      "startOffset" : 2,
      "endOffset" : 147
    }, {
      "referenceID" : 6,
      "context" : "4 Fu et al., (2014) (S) 65.3 62.4 63.8 Fu et al., (2014) (P) 68.1 64.2 66.1 Mirza and Tonelli, (2016) (C) 79.4 84.1 81.7 Mirza and Tonelli, (2016) (A) 80.7 72.3 76.3 Mirza and Tonelli, (2016) (S) 78.",
      "startOffset" : 2,
      "endOffset" : 192
    }, {
      "referenceID" : 6,
      "context" : "4 Fu et al., (2014) (S) 65.3 62.4 63.8 Fu et al., (2014) (P) 68.1 64.2 66.1 Mirza and Tonelli, (2016) (C) 79.4 84.1 81.7 Mirza and Tonelli, (2016) (A) 80.7 72.3 76.3 Mirza and Tonelli, (2016) (S) 78.0 81.2 79.6 Wang and He, (2016) 76.",
      "startOffset" : 2,
      "endOffset" : 231
    }, {
      "referenceID" : 6,
      "context" : "4 Fu et al., (2014) (S) 65.3 62.4 63.8 Fu et al., (2014) (P) 68.1 64.2 66.1 Mirza and Tonelli, (2016) (C) 79.4 84.1 81.7 Mirza and Tonelli, (2016) (A) 80.7 72.3 76.3 Mirza and Tonelli, (2016) (S) 78.0 81.2 79.6 Wang and He, (2016) 76.2 75.4 75.8 Ours (Initial) 79.3 76.3 77.7 Ours 84.4 79.5 81.9 Dataset Shwartz Method P R F Lenci and Benotto, (2012) 38.",
      "startOffset" : 2,
      "endOffset" : 351
    }, {
      "referenceID" : 6,
      "context" : "4 Fu et al., (2014) (S) 65.3 62.4 63.8 Fu et al., (2014) (P) 68.1 64.2 66.1 Mirza and Tonelli, (2016) (C) 79.4 84.1 81.7 Mirza and Tonelli, (2016) (A) 80.7 72.3 76.3 Mirza and Tonelli, (2016) (S) 78.0 81.2 79.6 Wang and He, (2016) 76.2 75.4 75.8 Ours (Initial) 79.3 76.3 77.7 Ours 84.4 79.5 81.9 Dataset Shwartz Method P R F Lenci and Benotto, (2012) 38.5 50.1 43.5 Santus et al., (2014) 51.",
      "startOffset" : 2,
      "endOffset" : 388
    }, {
      "referenceID" : 6,
      "context" : "4 Fu et al., (2014) (S) 65.3 62.4 63.8 Fu et al., (2014) (P) 68.1 64.2 66.1 Mirza and Tonelli, (2016) (C) 79.4 84.1 81.7 Mirza and Tonelli, (2016) (A) 80.7 72.3 76.3 Mirza and Tonelli, (2016) (S) 78.0 81.2 79.6 Wang and He, (2016) 76.2 75.4 75.8 Ours (Initial) 79.3 76.3 77.7 Ours 84.4 79.5 81.9 Dataset Shwartz Method P R F Lenci and Benotto, (2012) 38.5 50.1 43.5 Santus et al., (2014) 51.2 71.5 59.6 Fu et al., (2014) (S) 65.",
      "startOffset" : 2,
      "endOffset" : 421
    }, {
      "referenceID" : 6,
      "context" : "4 Fu et al., (2014) (S) 65.3 62.4 63.8 Fu et al., (2014) (P) 68.1 64.2 66.1 Mirza and Tonelli, (2016) (C) 79.4 84.1 81.7 Mirza and Tonelli, (2016) (A) 80.7 72.3 76.3 Mirza and Tonelli, (2016) (S) 78.0 81.2 79.6 Wang and He, (2016) 76.2 75.4 75.8 Ours (Initial) 79.3 76.3 77.7 Ours 84.4 79.5 81.9 Dataset Shwartz Method P R F Lenci and Benotto, (2012) 38.5 50.1 43.5 Santus et al., (2014) 51.2 71.5 59.6 Fu et al., (2014) (S) 65.6 66.1 65.8 Fu et al., (2014) (P) 62.",
      "startOffset" : 2,
      "endOffset" : 458
    }, {
      "referenceID" : 6,
      "context" : "4 Fu et al., (2014) (S) 65.3 62.4 63.8 Fu et al., (2014) (P) 68.1 64.2 66.1 Mirza and Tonelli, (2016) (C) 79.4 84.1 81.7 Mirza and Tonelli, (2016) (A) 80.7 72.3 76.3 Mirza and Tonelli, (2016) (S) 78.0 81.2 79.6 Wang and He, (2016) 76.2 75.4 75.8 Ours (Initial) 79.3 76.3 77.7 Ours 84.4 79.5 81.9 Dataset Shwartz Method P R F Lenci and Benotto, (2012) 38.5 50.1 43.5 Santus et al., (2014) 51.2 71.5 59.6 Fu et al., (2014) (S) 65.6 66.1 65.8 Fu et al., (2014) (P) 62.3 71.9 67.3 Mirza and Tonelli, (2016) (C) 79.",
      "startOffset" : 2,
      "endOffset" : 503
    }, {
      "referenceID" : 6,
      "context" : "4 Fu et al., (2014) (S) 65.3 62.4 63.8 Fu et al., (2014) (P) 68.1 64.2 66.1 Mirza and Tonelli, (2016) (C) 79.4 84.1 81.7 Mirza and Tonelli, (2016) (A) 80.7 72.3 76.3 Mirza and Tonelli, (2016) (S) 78.0 81.2 79.6 Wang and He, (2016) 76.2 75.4 75.8 Ours (Initial) 79.3 76.3 77.7 Ours 84.4 79.5 81.9 Dataset Shwartz Method P R F Lenci and Benotto, (2012) 38.5 50.1 43.5 Santus et al., (2014) 51.2 71.5 59.6 Fu et al., (2014) (S) 65.6 66.1 65.8 Fu et al., (2014) (P) 62.3 71.9 67.3 Mirza and Tonelli, (2016) (C) 79.3 80.9 80.1 Mirza and Tonelli, (2016) (A) 79.",
      "startOffset" : 2,
      "endOffset" : 548
    }, {
      "referenceID" : 6,
      "context" : "4 Fu et al., (2014) (S) 65.3 62.4 63.8 Fu et al., (2014) (P) 68.1 64.2 66.1 Mirza and Tonelli, (2016) (C) 79.4 84.1 81.7 Mirza and Tonelli, (2016) (A) 80.7 72.3 76.3 Mirza and Tonelli, (2016) (S) 78.0 81.2 79.6 Wang and He, (2016) 76.2 75.4 75.8 Ours (Initial) 79.3 76.3 77.7 Ours 84.4 79.5 81.9 Dataset Shwartz Method P R F Lenci and Benotto, (2012) 38.5 50.1 43.5 Santus et al., (2014) 51.2 71.5 59.6 Fu et al., (2014) (S) 65.6 66.1 65.8 Fu et al., (2014) (P) 62.3 71.9 67.3 Mirza and Tonelli, (2016) (C) 79.3 80.9 80.1 Mirza and Tonelli, (2016) (A) 79.1 79.6 79.4 Mirza and Tonelli, (2016) (S) 80.",
      "startOffset" : 2,
      "endOffset" : 593
    }, {
      "referenceID" : 6,
      "context" : "4 Fu et al., (2014) (S) 65.3 62.4 63.8 Fu et al., (2014) (P) 68.1 64.2 66.1 Mirza and Tonelli, (2016) (C) 79.4 84.1 81.7 Mirza and Tonelli, (2016) (A) 80.7 72.3 76.3 Mirza and Tonelli, (2016) (S) 78.0 81.2 79.6 Wang and He, (2016) 76.2 75.4 75.8 Ours (Initial) 79.3 76.3 77.7 Ours 84.4 79.5 81.9 Dataset Shwartz Method P R F Lenci and Benotto, (2012) 38.5 50.1 43.5 Santus et al., (2014) 51.2 71.5 59.6 Fu et al., (2014) (S) 65.6 66.1 65.8 Fu et al., (2014) (P) 62.3 71.9 67.3 Mirza and Tonelli, (2016) (C) 79.3 80.9 80.1 Mirza and Tonelli, (2016) (A) 79.1 79.6 79.4 Mirza and Tonelli, (2016) (S) 80.5 77.5 79.0 Wang and He, (2016) 75.",
      "startOffset" : 2,
      "endOffset" : 632
    } ],
    "year" : 2017,
    "abstractText" : "Finding the correct hypernyms for entities is essential for taxonomy learning, finegrained entity categorization, knowledge base construction, etc. Due to the flexibility of the Chinese language, it is challenging to identify hypernyms in Chinese accurately. Rather than extracting hypernyms from texts, in this paper, we present a transductive learning approach to establish mappings from entities to hypernyms in the embedding space directly. It combines linear and non-linear embedding projection models, with the capacity of encoding arbitrary language-specific rules. Experiments on real-world datasets illustrate that our approach outperforms previous methods for Chinese hypernym prediction.",
    "creator" : "LaTeX with hyperref package"
  }
}