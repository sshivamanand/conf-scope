{
  "name" : "483.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Here’s My Point: Argumentation Mining with Pointer Networks",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Computational approaches to argument mining/understanding have become very popular (Persing and Ng, 2016; Cano-Basave and He, 2016; Wei et al., 2016; Ghosh et al., 2016; Palau and Moens, 2009; Habernal and Gurevych, 2016). An important avenue in this work is to understand the structure in argumentative text (Persing and Ng, 2016; Peldszus and Stede, 2015; Stab\nand Gurevych, 2016; Nguyen and Litman, 2016). One fundamental assumption when working with argumentative text is the presence of Arguments Components (ACs). The types of ACs are generally characterized as a claim or a premise (Govier, 2013), with premises acting as support (or possibly attack) units for claims. To model more complex structures of arguments, some annotation schemes also include a major claim AC type (Stab and Gurevych, 2016, 2014b).\nGenerally, the task of processing argument structure encapsulates four distinct subtasks: 1) Given a sequence of tokens that represents an entire argumentative text, determine the token subsequences that constitute non-intersecting ACs; 2) Given an AC, determine the type of AC (claim, premise, etc.); 3) Given a set/list of ACs, determine which ACs have directed links that encapsulate overall argument structure; 4) Given two linked ACs, determine whether the link is a supporting or attacking relation. In this work, we focus on subtasks 2 and 3.\nThere are two key assumptions our work makes going forward. First, we assume subtask 1 has been completed, i.e. ACs have already been identified. Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau and Moens, 2009; Cohen, 1987; Peldszus and Stede, 2015; Stab and Gurevych, 2016). Specifically, a given AC can only have a single outgoing link, but can have numerous incoming links. Furthermore, there is a ‘head’ component that has no outgoing link (the top of the tree). However, we note that in a broader sense, depending on the corpus (see Section 4), an argument structure can be either a single tree or a forest, consisting of multiple trees. Figure 1 shows an example that we will use throughout the paper to concretely explain how our approach works. First, the left side of the figure presents the raw text of a paragraph in a per-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nFirst, [ ::::::: cloning ::::: will ::: be :::::::::: beneficial :::: for ::::: many ::::::: people :::: who :::: are ::: in ::::: need ::: of ::::: organ ::::::::: transplants]AC1. In addition, [it shortens the healing process]AC2. Usually, [it is very rare to find an appropriate organ donor]AC3 and [by using cloning in order to raise required organs the waiting time can be shortened tremendously]AC4.\nAC1 Claim\nAC2 Premise\nAC4 Premise AC3 Premise\nFigure 1: An example of argument structure with four ACs. The left side shows raw text that has been annotated for the presence of ACs. Squiggly and straight underlining means an AC is a claim or premise, respectively. The ACs in the text have also been annotated for links to other ACs, which is show in the right figure. ACs 3 and 4 are premises that link to another premise, AC2. Finally, AC2 links to a claim, AC1. AC1 therefore acts as the central argumentative component.\nsuasive essay (Stab and Gurevych, 2016), with the ACs contained in square brackets. Squiggly verse straight underlining differentiates between claims and premises, respectively. The ACs have been annotated as to how they are linked, and the right side of the figure reflects this structure. The argument structure with four ACs forms a tree, where AC2 has two incoming links, and AC1 acts as the head, with no outgoing links. We also specify the type of AC, with the head AC marked as a claim and the remaining ACs marked as premises. Lastly, we note that the order of arguments components can be a strong indicator of how components should related. Linking to the first argument component can provide a competitive baseline heuristic (Peldszus and Stede, 2015; Stab and Gurevych, 2016).\nGiven the task at hand, we propose a modification of a Pointer Network (PN) (Vinyals et al., 2015b). A PN is a sequence-to-sequence model that outputs a distribution over the encoding indices at each decoding timestep. The PN is a promising model for link extraction in argumentative text because it inherently possesses three important characteristics: 1) it is able to model the sequential nature of ACs; 2) it constrains ACs to have a single outgoing link, thus partly enforcing the tree structure; 3) the hidden representations learned by the model can be used for jointly predicting multiple subtasks. We also note that since a PN is a type of sequence-to-sequence model (Sutskever et al., 2014), it allows the entire sequence to be seen before making prediction. This is important because if the problem were to be approached as standard sequence modeling (Graves and Schmidhuber, 2009; Robinson, 1994), mak-\ning predictions at each forward timestep, it would only allow links to ACs that have already been seen. This is equivalent to only allowing backward links. We note that we do test a simplified model that only uses hidden states from an encoding network to make predictions, as opposed to the sequence-to-sequence architecture present in the PN (see Section 5).\nPNs were originally proposed to allow a variable length decoding sequence (Vinyals et al., 2015b). Alternatively, the PN we implement differs from the original model in that we decode for the same number of timesteps as there are input components. We also propose a joint PN for both extracting links between ACs and predicting the type of AC. The model uses the hidden representation of ACs produced during the encoding step (see Section 3.4). Aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the AC types or connectivity, unlike the work of (Peldszus, 2014). Lastly, in respect to the broad task of parsing, our model is flexible because it can easily handle non-projective, multiroot dependencies. We evaluate our models on the corpora of (Stab and Gurevych, 2016) and (Peldszus, 2014), and compare our results with the results of the aformentioned authors."
    }, {
      "heading" : "2 Related Work",
      "text" : "In this section, we discuss previous argumentation mining work that deals with the subtask pipeline we discussed in the previous section. This can be labeled as a ‘micro’ approach to argumentation mining (Stab and Gurevych, 2016) (in con-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\ntrast, there has been a number of efforts to identify argument structure at a higher level (Boltuzic and Šnajder, 2014; Ghosh et al., 2014; Cabrio and Villata, 2012)). Palau and Moens (2009) is an early work in this area, using a hand-crafted ContextFree Grammar to determine the structure of ACs in a corpus of legal texts. Lawrence et al. (2014) leverage a topic modeling-based AC similarity to uncover tree-structured arguments in philosophical texts. Unlike these methodologies, recent work offers data-driven approaches to the task of predicting links between ACs. Stab and Gurevych (2014b) approach the task as a binary classification problem. The authors train an SVM with various semantic and structural features. Peldszus and Stede have also used classification models for predicting the presence of links (2015).\nVarious authors have also proposed to jointly model link extraction with other subtasks from the argumentation mining pipeline, using either a Integer Linear Programming (ILP) framework (Persing and Ng, 2016; Stab and Gurevych, 2016) or directly feeding previous subtask predictions into a tree-based parser. The former joint approaches are evaluated on annotated corpora of persuasive essays (Stab and Gurevych, 2014a, 2016), and the latter on a corpus of microtexts (Peldszus, 2014). The ILP framework is effective in enforcing a tree structure between ACs when predictions are made from otherwise naive base classifiers.\nUnrelated to argumentation mining specifically, recurrent neural networks have previously been proposed to model tree/graph structures in a linear manner. (Vinyals et al., 2015c) use a sequenceto-sequence model for the task of syntactic parsing. The authors linearize input parse graphs using a depth-first search, allowing it to be consumed as a sequence, achieving state-of-the-art results on several syntactic parsing datasets. (Bowman et al., 2015) experiment on an artificial entailment dataset that is specifically engineered to capture recursive logic (Bowman et al., 2014). The text is annotated with brackets, in an original attempt to provide easy input into a recursive neural network. However, standard recurrent neural networks can take in complete sentence sequences, brackets included, and perform competitively with a recursive neural network."
    }, {
      "heading" : "3 Pointer Network for Link Extraction",
      "text" : "In this section we will describe how we use a PN for the problem of extracting links between ACs. We begin by giving a general description of the PN model."
    }, {
      "heading" : "3.1 Pointer Network",
      "text" : "A PN is a sequence-to-sequence model (Sutskever et al., 2014) with attention (Bahdanau et al., 2014) that was proposed to handle decoding sequences over the encoding inputs, and can be extended to arbitrary sets (Vinyals et al., 2015a). The original motivation for a pointer network was to allow networks to learn solutions to algorithmic problems, such as the traveling salesperson and convex hull, where the solution is a sequence over candidate points. The PN model is trained on input/output sequence pairs (E,D), where E is the source and D is the target (our choice of E,D is meant to represent the encoding, decoding steps of the sequence-to-sequence model). Given model parameters Θ, we apply the chain rule to determine the probability of a single training example:\np(D|E; Θ) = m(E)∏ i=1 p(Di|D1, ..., Di−1, E; Θ)\n(1) where the function m signifies that the number of decoding timesteps is a function of each individual training example. We will discuss shortly why we need to modify the original definition of m for our application. By taking the log-likelihood of Equation 1, we arrive at the optimization objective:\nΘ∗ = arg max Θ ∑ E,D log p(D|E; Θ) (2)\nwhich is the sum over all training example pairs. The PN uses Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) for sequential modeling, which produces a hidden layer h at each encoding/decoding timestep. In practice, the PN has two separate LSTMs, one for encoding and one for decoding. Thus, we refer to encoding hidden layers as e, and decoding hidden layers as d.\nThe PN uses a form of content-based attention (Bahdanau et al., 2014) to allow the model to produce a distribution over input elements. This can also be thought of as a distribution over input indices, wherein a decoding\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nComponent 1 Component 2 Component 3 Component 4\nLSTM D1\nLSTM D2\nLSTM D3\nLSTM D4 LSTM E1 LSTM E2 LSTM E3 LSTM E4\nFigure 2: Applying a Pointer Network to the example paragraph in Figure 1 with LSTMs unrolled over time.\nstep ‘points’ to the input. Formally, given encoding hidden states (e1, ..., en), the model calculates p(Di|D1, ..., Di−1, E) as follows:\nuij = v T tanh(W1ej +W2di) (3)\np(Di|D1, ..., Dj−1, E) = softmax(ui) (4)\nwhere matrices W1, W2 and vector v are parameters of the model (along with the LSTM parameters used for encoding and decoding). In Equation 3, prior to taking the dot product with v, the resulting transformation can be thought of as creating a joint, hidden representation of inputs i and j. Vector ui in equation 4 is of length n, and index j corresponds to input element j. Therefore, by taking the softmax of ui, we are able to create a distribution over the input."
    }, {
      "heading" : "3.2 Link Extraction as Sequence Modeling",
      "text" : "A given piece of text has a set of ACs, which occur in a specific order in the text, (C1, ..., Cn). Therefore, at encoding timestep i, the model is fed a representation of Ci. Since the representation is large and sparse (see Section 3.3 for details on how we represent ACs), we add a fully-connected layer before the LSTM input. Given a representation Ri for AC Ci the LSTM input Ai becomes:\nAi = σ(WrepRi + brep) (5)\nwhere Wrep, brep in turn become model parameters, and σ is the sigmoid function1. Similarly, the decoding network applies a fully-connected layer with sigmoid activation to its inputs, see Figure 3. At encoding step i, the encoding LSTM produces hidden layer ei, which can be thought of as a hidden representation of AC Ci.\n1We also experimented with relu and elu activations, but found sigmoid to yield the best performance.\nIn order to make the PN applicable to the problem of link extraction, we explicitly set the number of decoding timesteps to be equal to the number of input components. Using notation from Equation 1, the decoding sequence length for an encoding sequence E is simply m(E) = |{C1, ..., Cn}|, which is trivially equal to n. By constructing the decoding sequence in this manner, we can associate decoding timestep i with AC Ci.\nFrom Equation 4, decoding timestep Di will output a distribution over input indices. The result of this distribution will indicate to which AC component Ci links. Recall there is a possibility that an AC has no outgoing link, such as if it’s the root of the tree. In this case, we state that if ACCi does not have an outgoing link, decoding step Di will output index i. Conversely, if Di outputs index j, such that j is not equal to i, this implies thatCi has an outgoing link to Cj . For the argument structure in Figure 1, the corresponding decoding sequence is (1, 1, 2, 2). The topology of this decoding sequence is illustrated in Figure 2. Observe how C1 points to itself since it has no outgoing link.\nFinally, we note that we modify the PN structure to have a Bidirectional LSTM as the encoder. Thus, ei is the concatenation of forward and backward hidden states −→e i and←−e n−i+1, produced by two separate LSTMs. The decoder remains a standard forward LSTM."
    }, {
      "heading" : "3.3 Representing Argument Components",
      "text" : "At each timestep of the decoder, the network takes in the representation of an AC. Each AC is itself a sequence of tokens, similar to the recently proposed Question-Answering dataset (Weston et al., 2015). We follow the work of (Stab and Gurevych, 2016) and focus on three different types of features to represent our ACs: 1) Bag-of-Words of the AC; 2) Embedding representation based on GloVe embeddings (Pennington et al., 2014); 3) Structural features: Whether or not the AC is the first AC in\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nBi-LSTM E1\nFC1 FC1 FC1 FC1\nBidirectional LSTM Encoder\nComponent 1 Component 2 Component 3 Component 4\nFC2 FC2 FC2 FC2\nClaim Premise Premise Premise\nLSTM D1\nLSTM D2\nLSTM D3\nBi-LSTM E2 Bi-LSTM E3 Bi-LSTM E4\nFC3 FC3 FC3\na paragraph, and Whether the AC is in an opening, body, or closing paragraph. See Section 6 for an ablation study of the proposed features."
    }, {
      "heading" : "3.4 Joint Neural Model",
      "text" : "Up to this point, we focused on the task of extracting links between ACs. However, recent work has shown that joint models that simultaneously try to complete multiple aspects of the subtask pipeline outperform models that focus on a single subtask (Persing and Ng, 2016; Stab and Gurevych, 2014b; Peldszus and Stede, 2015). Therefore, we will modify the architecture we proposed in Section 3 so that it would allow us to perform AC classification (Kwon et al., 2007; Rooney et al., 2012) together with link prediction. Knowledge of an individual subtask’s predictions can aid in other subtasks. For example, claims do not have an outgoing link, so knowing the type of AC can aid in the link prediction task. This can be seen as a way of regularizing the hidden representations from the encoding component (Che et al., 2015).\nPredicting AC type is a straightforward classification task: given AC Ci, we need to predict whether it is a claim or premise. Some annotation schemes also include the class major claim (Stab and Gurevych, 2014a), which means this can be a multi-class classification task. For encoding timestep i, the model creates hidden representation ei. This can be thought of as a representation of AC Ci. Therefore, our joint model will simply pass this representation through a fully connected layer as follows:\nzi = Wclsei + bcls (6)\nwhere Wcls, bcls become elements of the model\nparameters, Θ. The dimensionality of Wcls, bcls is determined by the number of classes. Lastly, we use softmax to form a distribution over the possible classes.\nConsequently, the probability of predicting component type at timestep i is defined as:\np(Ci) = p(Ei| −→ E i, ←− E i; Θ) (7)\np(Ei| −→ E i, ←− E i; Θ) = softmax(zi) (8)\nFinally, combining this new prediction task with Equation 2, we arrive at the new training objective:\nΘ∗ = arg max Θ α ∑ E,D log p(D|E; Θ)\n+(1− α) ∑ E log p(E|Θ) (9)\nwhich simply sums the costs of the individual prediction tasks, and the second summation is the cost for the new task of predicting AC type. α ∈ [0, 1] is a hyperparameter that specifies how we weight the two prediction tasks in our cost function. The architecture of the joint model, applied to our ongoing example, is illustrated in Figure 3."
    }, {
      "heading" : "4 Experimental Design",
      "text" : "As we have previously mentioned, our work assumes that ACs have already been identified. That is, the token sequence that comprises a given AC is already known. The order of ACs corresponds directly to the order in which the ACs appear in the text. Since ACs are non-overlapping, there is no\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nType prediction Link prediction Model Macro f1 MC f1 Cl f1 Pr f1 Macro f1 Link f1 No Link f1 Base Classifier .794 .891 .611 .879 .717 .508 .917 ILP Joint Model .826 .891 .682 .903 .751 .585 .918 BLSTM .810 .830 .688 .912 .754 .589 .919 PN - - - - .709 .511 .906 Joint Model No FC Input .791 .826 .642 .906 .708 .514 .901 Joint Model .849 .894 .732 .921 .767 .608 .925\nTable 1: Results on the Persuasive Essay corpus.\nType prediction Link prediction Model Macro f1 Cl f1 Pr f1 Macro f1 Link f1 No Link f1 Simple .817 - - .663 .478 .848 Best EG .869 - - .693 .502 .884 MP+p .831 - - .720 .546 .894 Base Classifier .830 .712 .937 .650 .446 .841 ILP Joint Model .857 .770 .943 .683 .486 .881 PN .813 .692 .934 .740 .577 .903\nTable 2: Results on the Microtext corpus.\nambiguity in this ordering. We test the effectiveness of our proposed model on a dataset of persuasive essays (Stab and Gurevych, 2016), as well as a dataset of microtexts (Peldszus, 2014). The feature space for the persuasive essay corpus has roughly 3,000 dimensions, and the microtext corpus feature space has between 2,500 and 3,000 dimensions, depending on the data split (see below).\nThe persuasive essay corpus contains a total of 402 essays, with a frozen set of 80 essays held out for testing. There are three AC types in this corpus: major claim, claim, and premise. Also, in this corpus individual structures can be either trees or forests. We follow the creators of the corpus and only evaluate ACs within a given paragraph. That is, each training/test example is a sequence of ACs from a paragraph. This results in a 1,405/144 training/test split.\nThe microtext corpus contains 112 short texts. Unlike the persuasive essay corpus, each text in this corpus is itself a complete example, as well as a single tree. Since the dataset is small, the authors have created 10 sets of 5-fold cross-validation, reporting the the average across all splits for final model evaluation. This corpus contains only two types of ACs: claim and premise. The annotation of argument structure of the microtext corpus varies from the persuasive essay corpus; ACs can be linked to other links, as opposed to ACs. Therefore, if AC Ci is annotated to be linked to link l,\nwe create a link to the source AC of l. On average, this corpus has 5.14 ACs per text. Lastly, we note that predicting the presence of links is directional (ordered): predicting a link between the pair Ci, Cj(i 6= j) is different than Cj , Ci.\nWe implement our models in TensorFlow (Abadi et al., 2015). Our model has the following parameters: hidden input dimension size 512, hidden layer size 256 for the bidirectional LSTMs, hidden layer size 512 for the LSTM decoder, α equal to 0.5, and dropout (Srivastava et al., 2014) of 0.9. We believe the need for such high dropout is due to the small amounts of training data (Zarrella and Marsh, 2016), particularly in the Microtext corpus. All models are trained with Adam optimizer (Kingma and Ba, 2014) with a batch size of 16. For a given training set, we randomly select 10% to become the validation set. Training occurs for 4,000 epochs. Once training is completed, we select the model with the highest validation accuracy (on the link prediction task) and evaluate it on the held-out test set. At test time, we take a greedy approach and select the index of the probability distribution (whether link or type prediction) with the highest value."
    }, {
      "heading" : "5 Results",
      "text" : "The results of our experiments are presented in Tables 1 and 2. For each corpus, we present f1 scores\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nType prediction Link prediction Model Macro f1 MC f1 Cl f1 Pr f1 Macro f1 Link f1 No Link f1 No structural .808 .824 .694 .907 .760 .598 .922 No BOW .796 .833 .652 .902 .728 .543 .912 No Embeddings .827 .874 .695 .911 .750 .581 .918 Only Avg Emb* .832 .873 .717 .917 .751 .583 .918 Only Max Emb* .843 .874 .732 .923 .766 .608 .924 Only Min Emb* .838 .878 .719 .918 .763 .602 .924 All features .849 .894 .732 .921 .767 .608 .925\nTable 3: Feature ablation study. * indicates that both BOW and Structural are present, as well as the stated embedding type.\nType prediction Link prediction Bin Macro f1 MC f1 Cl f1 Pr f1 Macro f1 Link f1 No Link f1\n1 ≤ len < 4 .863 .902 .798 .889 .918 .866 .969 4 ≤ len < 8 .680 .444 .675 .920 .749 .586 .912 8 ≤ len < 12 .862* .000* .762 .961 .742 .542 .941\nTable 4: Results of binning test data by length of AC sequence. * indicates that this bin does not contain any major claim labels, and this average only applies to claim and premise classes. However, we do not disable the model from predicting this class: the model was able to avoid predicting this class on its own.\nfor the AC type classification experiment, with a macro-averaged score of the individual class f1 scores. We also present the f1 scores for predicting the presence/absence of links between ACs, as well as the associated macro-average between these two values.\nWe implement and compare four types of neural models: 1) The previously described PN-based model depicted in Figure 3 (called Joint Model in the tables); 2) The same as 1), but without the fully-connected input layers (called Joint Model No FC Input in the table); 3) The same as 1), but the model only predicts the link task, and is therefore not optimized for type prediction (called PN in the tables); 4) A non-sequence-to-sequence model that uses the hidden layers produced by the BLSTM encoder with the same type of attention as the PN (called BLSTM in the table). That is, di in Equation 3 is replaced by ei.\nIn both corpora we compare against the following previously proposed models: Base Classifier (Stab and Gurevych, 2016) is a feature-rich, taskspecific (AC type or link extraction) SVM classifier. Neither of these classifiers enforce structural or global constraints. Conversely, the ILP Joint Model (Stab and Gurevych, 2016) provides constraints by sharing prediction information between the base classifiers. For example, the model attempts to enforce a tree structure among ACs\nwithin a given paragraph, as well as using incoming link predictions to better predict the type class claim. For the microtext corpus only, we have the following comparative models: Simple (Peldszus and Stede, 2015) is a feature-rich logistic regression classifier. Best EG (Peldszus and Stede, 2015) creates an Evidence Graph (EG) from the predictions of a set of base classifiers. The EG models the potential argument structure, and offers a global optimization objective that the base classifiers attempt to optimize by adjusting their individual weights. Lastly, MP+p (Peldszus and Stede, 2015) combines predictions from base classifiers with a Minimum Spanning Tree Parser (MSTParser)."
    }, {
      "heading" : "6 Discussion",
      "text" : "First, we point out that the Joint Model achieves state-of-the-art on 10 of the 13 metrics in Tables 1 and 2, including the highest results in all metrics on the Persuasive Essay corpus, as well as link prediction on the Microtext corpus. The performance on the Microtext corpus is very encouraging for several reasons. First, the fact that the model can perform so well with only a hundred training examples is rather remarkable. Second, although we motivate the use of a PN due to the fact that it partially enforces the tree structure in argumentation, other models we compare against explic-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nitly contain further constraints. For example, only premises can have outgoing links, and there can be only one claim in an AC. Moreover, the MP+p model directly enforces the tree constraint unique to the microtext corpus. Even though the Joint Model does not have the tree constraint directly encoded, it able to learn the structure effectively from the training examples so that it can outperform the Mp+p model for link prediction. As for the other neural models, the BLSTM model performs competitively with the ILP Joint Model on the persuasive essay corpus, but trails the performance of the Joint Model. We believe this is because the Joint Model is able to create two different representations for each AC, one each in the encoding/decoding state, which benefits performance in the dual tasks. Conversely, the BLSTM model must encode information relating to type as well as link prediction in a single hidden representation. On one hand, the BLSTM model outperforms the ILP model on link prediction, yet it is not able to match the ILP Joint Model’s performance on type prediction, primarily due to the BLSTM’s poor performance on predicting the major claim class. Another interesting outcome is the importance of the fully-connected layer before the LSTM input. The results show that this extra layer of depth is crucial for good performance on this task. Without it, the Joint Model is only able to perform competitively with the Base Classifier. The results dictate that even a simple fullyconnected layer with sigmoid activation can provide a useful dimensionality reduction for feature representation. Finally, the PN model, only optimized for link prediction, suffers a large drop in performance, conveying that the dual optimization of the Joint Model is crucial for high performance in the link prediction task.\nTable 3 shows the results of an ablation study for AC feature representation. Regarding link prediction, BOW features are clearly the most important, as their absence results in the highest drop in performance. Conversely, the presence of structural features provides the smallest boost in performance, as the model is still able to record stateof-the-art results compared to the ILP Joint Model. This shows that the Joint Model is able to capture structural cues through sequence modeling and semantics. When considering type prediction, both BOW and structural features are important, and it is the embedding features that provide the least\nbenefit. The Ablation results also provide an interesting insight into the effectiveness of different ‘pooling’ strategies for using individual token embeddings to create a multi-word embedding. The popular method of averaging embeddings (which is used by (Stab and Gurevych, 2016) in their system) is in fact the worst method, although its performance is still competitive with the previous state-of-the-art. Conversely, max pooling results are on par with the Joint Model results from Table 1.\nTable 4 shows the results on the Persuasive Essay test set with the test examples binned by sequence length. First, it is not surprising to see that the model performs best when the sequences are the shortest. As the sequence length increases, the accuracy on link prediction drops. This is possibly due to the fact that as the length increases, a given AC has more possibilities as to which other AC it can link to, making the task more difficult. Conversely, there is actually a rise in no link prediction accuracy from the second to third row. This is likely due to the fact that since the model predicts at most one outgoing link, it indirectly predicts no link for the remaining ACs in the sequence. Since the chance probability is low for having a link between a given AC in a long sequence, the no link performance is actually better in longer sequences."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper we have proposed how to use a modified PN (Vinyals et al., 2015b) to extract links between ACs in argumentative text. We evaluate our models on two corpora: a corpus of persuasive essays (Stab and Gurevych, 2016), and a corpus of microtexts (Peldszus, 2014). The Joint Model records state-of-the-art results on the persuasive essay corpus, as well as achieving state-of-the-art results for link prediction on the microtext corpusThe results show that jointly modeling the two prediction tasks is crucial for high performance. Future work can attempt to learn the AC representations themselves, such as in (Kumar et al., 2015). Lastly, future work can integrate subtasks 1 and 4 into the model. The representations produced by Equation 3 could potentially be used to predict the type of link connecting ACs, i.e. supporting or attacking; this is the fourth subtask in the pipeline. In addition, a segmenting technique, such as the one proposed by (Weston et al., 2014), can accomplish subtask 1.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org",
      "author" : [ "sudevan", "Fernanda Viégas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng" ],
      "venue" : null,
      "citeRegEx" : "sudevan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "sudevan et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473 .",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Back up your stance: Recognizing arguments in online discussions",
      "author" : [ "Filip Boltuzic", "Jan Šnajder." ],
      "venue" : "Proceedings of the First Workshop on Argumentation Mining. Citeseer, pages 49–58.",
      "citeRegEx" : "Boltuzic and Šnajder.,? 2014",
      "shortCiteRegEx" : "Boltuzic and Šnajder.",
      "year" : 2014
    }, {
      "title" : "Tree-structured composition in neural networks without tree-structured architectures",
      "author" : [ "Samuel R Bowman", "Christopher D Manning", "Christopher Potts." ],
      "venue" : "arXiv preprint arXiv:1506.04834 .",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Recursive neural networks can learn logical semantics",
      "author" : [ "Samuel R Bowman", "Christopher Potts", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1406.1827 .",
      "citeRegEx" : "Bowman et al\\.,? 2014",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2014
    }, {
      "title" : "Natural language arguments: A combined approach",
      "author" : [ "Elena Cabrio", "Serena Villata." ],
      "venue" : "ECAI. volume 242, pages 205–210.",
      "citeRegEx" : "Cabrio and Villata.,? 2012",
      "shortCiteRegEx" : "Cabrio and Villata.",
      "year" : 2012
    }, {
      "title" : "A study of the impact of persuasive argumentation in political debates",
      "author" : [ "Amparo Elizabeth Cano-Basave", "Yulan He." ],
      "venue" : "Proceedings of NAACL-HLT. pages 1405–1413.",
      "citeRegEx" : "Cano.Basave and He.,? 2016",
      "shortCiteRegEx" : "Cano.Basave and He.",
      "year" : 2016
    }, {
      "title" : "Deep computational phenotyping",
      "author" : [ "Zhengping Che", "David Kale", "Wenzhe Li", "Mohammad Taha Bahadori", "Yan Liu." ],
      "venue" : "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,",
      "citeRegEx" : "Che et al\\.,? 2015",
      "shortCiteRegEx" : "Che et al\\.",
      "year" : 2015
    }, {
      "title" : "Analyzing the structure of argumentative discourse",
      "author" : [ "Robin Cohen." ],
      "venue" : "Computational linguistics 13(1-2):11–24.",
      "citeRegEx" : "Cohen.,? 1987",
      "shortCiteRegEx" : "Cohen.",
      "year" : 1987
    }, {
      "title" : "Coarse-grained argumentation features for scoring persuasive essays",
      "author" : [ "Debanjan Ghosh", "Aquila Khanam", "Yubo Han", "Smaranda Muresan." ],
      "venue" : "The 54th Annual Meeting of the Association for Computational Linguistics. page 549.",
      "citeRegEx" : "Ghosh et al\\.,? 2016",
      "shortCiteRegEx" : "Ghosh et al\\.",
      "year" : 2016
    }, {
      "title" : "Analyzing argumentative discourse units in online interactions",
      "author" : [ "Debanjan Ghosh", "Smaranda Muresan", "Nina Wacholder", "Mark Aakhus", "Matthew Mitsui." ],
      "venue" : "Proceedings of the First Workshop on Argumentation Mining. pages 39–48.",
      "citeRegEx" : "Ghosh et al\\.,? 2014",
      "shortCiteRegEx" : "Ghosh et al\\.",
      "year" : 2014
    }, {
      "title" : "A practical study of argument",
      "author" : [ "Trudy Govier." ],
      "venue" : "Cengage Learning.",
      "citeRegEx" : "Govier.,? 2013",
      "shortCiteRegEx" : "Govier.",
      "year" : 2013
    }, {
      "title" : "Offline handwriting recognition with multidimensional recurrent neural networks",
      "author" : [ "Alex Graves", "Jürgen Schmidhuber." ],
      "venue" : "Advances in neural information processing systems. pages 545–552.",
      "citeRegEx" : "Graves and Schmidhuber.,? 2009",
      "shortCiteRegEx" : "Graves and Schmidhuber.",
      "year" : 2009
    }, {
      "title" : "Which argument is more convincing? analyzing and predicting convincingness of web arguments using bidirectional lstm",
      "author" : [ "Ivan Habernal", "Iryna Gurevych." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational",
      "citeRegEx" : "Habernal and Gurevych.,? 2016",
      "shortCiteRegEx" : "Habernal and Gurevych.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980 .",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Ask me anything: Dynamic memory networks for natural language processing",
      "author" : [ "Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Kumar et al\\.,? 2015",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2015
    }, {
      "title" : "Identifying and classifying subjective claims",
      "author" : [ "Namhee Kwon", "Liang Zhou", "Eduard Hovy", "Stuart W Shulman." ],
      "venue" : "Proceedings of the 8th annual international conference on Digital government research: bridging disciplines & domains. Digital",
      "citeRegEx" : "Kwon et al\\.,? 2007",
      "shortCiteRegEx" : "Kwon et al\\.",
      "year" : 2007
    }, {
      "title" : "Mining arguments from 19th century philosophical texts using topic based modelling",
      "author" : [ "John Lawrence", "Chris Reed", "Colin Allen", "Simon McAlister", "Andrew Ravenscroft", "David Bourget." ],
      "venue" : "Proceedings of the First Workshop on",
      "citeRegEx" : "Lawrence et al\\.,? 2014",
      "shortCiteRegEx" : "Lawrence et al\\.",
      "year" : 2014
    }, {
      "title" : "Contextaware argumentative relation mining",
      "author" : [ "Huy V Nguyen", "Diane J Litman" ],
      "venue" : null,
      "citeRegEx" : "Nguyen and Litman.,? \\Q2016\\E",
      "shortCiteRegEx" : "Nguyen and Litman.",
      "year" : 2016
    }, {
      "title" : "Argumentation mining: the detection, classification and structure of arguments in text",
      "author" : [ "Raquel Mochales Palau", "Marie-Francine Moens." ],
      "venue" : "Proceedings of the 12th international conference on artificial intelligence and law. ACM, pages 98–107.",
      "citeRegEx" : "Palau and Moens.,? 2009",
      "shortCiteRegEx" : "Palau and Moens.",
      "year" : 2009
    }, {
      "title" : "Towards segment-based recognition of argumentation structure in short texts",
      "author" : [ "Andreas Peldszus." ],
      "venue" : "ACL 2014 page 88.",
      "citeRegEx" : "Peldszus.,? 2014",
      "shortCiteRegEx" : "Peldszus.",
      "year" : 2014
    }, {
      "title" : "Joint prediction in mst-style discourse parsing for argumentation mining",
      "author" : [ "Andreas Peldszus", "Manfred Stede." ],
      "venue" : "Proc. of the Conference on Empirical Methods in Natural Language Processing. pages 938–948.",
      "citeRegEx" : "Peldszus and Stede.,? 2015",
      "shortCiteRegEx" : "Peldszus and Stede.",
      "year" : 2015
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "EMNLP. volume 14, pages 1532–",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "End-to-end argumentation mining in student essays",
      "author" : [ "Isaac Persing", "Vincent Ng." ],
      "venue" : "Proceedings of NAACL-HLT. pages 1384–1394.",
      "citeRegEx" : "Persing and Ng.,? 2016",
      "shortCiteRegEx" : "Persing and Ng.",
      "year" : 2016
    }, {
      "title" : "An application of recurrent nets to phone probability estimation",
      "author" : [ "Anthony J Robinson." ],
      "venue" : "IEEE transactions on Neural Networks 5(2):298–305.",
      "citeRegEx" : "Robinson.,? 1994",
      "shortCiteRegEx" : "Robinson.",
      "year" : 1994
    }, {
      "title" : "Applying kernel methods to argumentation mining",
      "author" : [ "Niall Rooney", "Hui Wang", "Fiona Browne." ],
      "venue" : "FLAIRS Conference.",
      "citeRegEx" : "Rooney et al\\.,? 2012",
      "shortCiteRegEx" : "Rooney et al\\.",
      "year" : 2012
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "Journal of Machine Learning Research 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Annotating argument components and relations in persuasive essays",
      "author" : [ "Christian Stab", "Iryna Gurevych." ],
      "venue" : "COLING. pages 1501–1510.",
      "citeRegEx" : "Stab and Gurevych.,? 2014a",
      "shortCiteRegEx" : "Stab and Gurevych.",
      "year" : 2014
    }, {
      "title" : "Identifying argumentative discourse structures in persuasive essays",
      "author" : [ "Christian Stab", "Iryna Gurevych." ],
      "venue" : "EMNLP. pages 46–56.",
      "citeRegEx" : "Stab and Gurevych.,? 2014b",
      "shortCiteRegEx" : "Stab and Gurevych.",
      "year" : 2014
    }, {
      "title" : "Parsing argumentation structures in persuasive essays",
      "author" : [ "Christian Stab", "Iryna Gurevych." ],
      "venue" : "arXiv preprint arXiv:1604.07370 .",
      "citeRegEx" : "Stab and Gurevych.,? 2016",
      "shortCiteRegEx" : "Stab and Gurevych.",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems. pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Order matters: Sequence to sequence for sets",
      "author" : [ "Oriol Vinyals", "Samy Bengio", "Manjunath Kudlur." ],
      "venue" : "arXiv preprint arXiv:1511.06391 .",
      "citeRegEx" : "Vinyals et al\\.,? 2015a",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 2692–2700.",
      "citeRegEx" : "Vinyals et al\\.,? 2015b",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Oriol Vinyals", "Łukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 2773–2781.",
      "citeRegEx" : "Vinyals et al\\.,? 2015c",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Is this post persuasive? ranking argumentative comments in the online forum",
      "author" : [ "Zhongyu Wei", "Yang Liu", "Yi Li." ],
      "venue" : "The 54th Annual Meeting of the Association for Computational Linguistics. page 195.",
      "citeRegEx" : "Wei et al\\.,? 2016",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards ai-complete question answering: A set of prerequisite toy tasks",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Alexander M Rush", "Bart van Merriënboer", "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "arXiv preprint arXiv:1502.05698 .",
      "citeRegEx" : "Weston et al\\.,? 2015",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    }, {
      "title" : "Memory networks",
      "author" : [ "Jason Weston", "Sumit Chopra", "Antoine Bordes." ],
      "venue" : "arXiv preprint arXiv:1410.3916 .",
      "citeRegEx" : "Weston et al\\.,? 2014",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2014
    }, {
      "title" : "Mitre at semeval-2016 task 6: Transfer learning for stance detection",
      "author" : [ "Guido Zarrella", "Amy Marsh." ],
      "venue" : "arXiv preprint arXiv:1606.03784 .",
      "citeRegEx" : "Zarrella and Marsh.,? 2016",
      "shortCiteRegEx" : "Zarrella and Marsh.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "Computational approaches to argument mining/understanding have become very popular (Persing and Ng, 2016; Cano-Basave and He, 2016; Wei et al., 2016; Ghosh et al., 2016; Palau and Moens, 2009; Habernal and Gurevych, 2016).",
      "startOffset" : 83,
      "endOffset" : 221
    }, {
      "referenceID" : 6,
      "context" : "Computational approaches to argument mining/understanding have become very popular (Persing and Ng, 2016; Cano-Basave and He, 2016; Wei et al., 2016; Ghosh et al., 2016; Palau and Moens, 2009; Habernal and Gurevych, 2016).",
      "startOffset" : 83,
      "endOffset" : 221
    }, {
      "referenceID" : 35,
      "context" : "Computational approaches to argument mining/understanding have become very popular (Persing and Ng, 2016; Cano-Basave and He, 2016; Wei et al., 2016; Ghosh et al., 2016; Palau and Moens, 2009; Habernal and Gurevych, 2016).",
      "startOffset" : 83,
      "endOffset" : 221
    }, {
      "referenceID" : 9,
      "context" : "Computational approaches to argument mining/understanding have become very popular (Persing and Ng, 2016; Cano-Basave and He, 2016; Wei et al., 2016; Ghosh et al., 2016; Palau and Moens, 2009; Habernal and Gurevych, 2016).",
      "startOffset" : 83,
      "endOffset" : 221
    }, {
      "referenceID" : 20,
      "context" : "Computational approaches to argument mining/understanding have become very popular (Persing and Ng, 2016; Cano-Basave and He, 2016; Wei et al., 2016; Ghosh et al., 2016; Palau and Moens, 2009; Habernal and Gurevych, 2016).",
      "startOffset" : 83,
      "endOffset" : 221
    }, {
      "referenceID" : 13,
      "context" : "Computational approaches to argument mining/understanding have become very popular (Persing and Ng, 2016; Cano-Basave and He, 2016; Wei et al., 2016; Ghosh et al., 2016; Palau and Moens, 2009; Habernal and Gurevych, 2016).",
      "startOffset" : 83,
      "endOffset" : 221
    }, {
      "referenceID" : 24,
      "context" : "An important avenue in this work is to understand the structure in argumentative text (Persing and Ng, 2016; Peldszus and Stede, 2015; Stab and Gurevych, 2016; Nguyen and Litman, 2016).",
      "startOffset" : 86,
      "endOffset" : 184
    }, {
      "referenceID" : 22,
      "context" : "An important avenue in this work is to understand the structure in argumentative text (Persing and Ng, 2016; Peldszus and Stede, 2015; Stab and Gurevych, 2016; Nguyen and Litman, 2016).",
      "startOffset" : 86,
      "endOffset" : 184
    }, {
      "referenceID" : 30,
      "context" : "An important avenue in this work is to understand the structure in argumentative text (Persing and Ng, 2016; Peldszus and Stede, 2015; Stab and Gurevych, 2016; Nguyen and Litman, 2016).",
      "startOffset" : 86,
      "endOffset" : 184
    }, {
      "referenceID" : 19,
      "context" : "An important avenue in this work is to understand the structure in argumentative text (Persing and Ng, 2016; Peldszus and Stede, 2015; Stab and Gurevych, 2016; Nguyen and Litman, 2016).",
      "startOffset" : 86,
      "endOffset" : 184
    }, {
      "referenceID" : 11,
      "context" : "The types of ACs are generally characterized as a claim or a premise (Govier, 2013), with premises acting as support (or possibly attack) units for claims.",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau and Moens, 2009; Cohen, 1987; Peldszus and Stede, 2015; Stab and Gurevych, 2016).",
      "startOffset" : 85,
      "endOffset" : 172
    }, {
      "referenceID" : 8,
      "context" : "Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau and Moens, 2009; Cohen, 1987; Peldszus and Stede, 2015; Stab and Gurevych, 2016).",
      "startOffset" : 85,
      "endOffset" : 172
    }, {
      "referenceID" : 22,
      "context" : "Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau and Moens, 2009; Cohen, 1987; Peldszus and Stede, 2015; Stab and Gurevych, 2016).",
      "startOffset" : 85,
      "endOffset" : 172
    }, {
      "referenceID" : 30,
      "context" : "Second, we follow previous work that assumes a tree structure for the linking of ACs (Palau and Moens, 2009; Cohen, 1987; Peldszus and Stede, 2015; Stab and Gurevych, 2016).",
      "startOffset" : 85,
      "endOffset" : 172
    }, {
      "referenceID" : 30,
      "context" : "suasive essay (Stab and Gurevych, 2016), with the ACs contained in square brackets.",
      "startOffset" : 14,
      "endOffset" : 39
    }, {
      "referenceID" : 22,
      "context" : "Linking to the first argument component can provide a competitive baseline heuristic (Peldszus and Stede, 2015; Stab and Gurevych, 2016).",
      "startOffset" : 85,
      "endOffset" : 136
    }, {
      "referenceID" : 30,
      "context" : "Linking to the first argument component can provide a competitive baseline heuristic (Peldszus and Stede, 2015; Stab and Gurevych, 2016).",
      "startOffset" : 85,
      "endOffset" : 136
    }, {
      "referenceID" : 33,
      "context" : "Given the task at hand, we propose a modification of a Pointer Network (PN) (Vinyals et al., 2015b).",
      "startOffset" : 76,
      "endOffset" : 99
    }, {
      "referenceID" : 31,
      "context" : "We also note that since a PN is a type of sequence-to-sequence model (Sutskever et al., 2014), it allows the entire sequence to be seen before making prediction.",
      "startOffset" : 69,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "This is important because if the problem were to be approached as standard sequence modeling (Graves and Schmidhuber, 2009; Robinson, 1994), making predictions at each forward timestep, it would only allow links to ACs that have already been seen.",
      "startOffset" : 93,
      "endOffset" : 139
    }, {
      "referenceID" : 25,
      "context" : "This is important because if the problem were to be approached as standard sequence modeling (Graves and Schmidhuber, 2009; Robinson, 1994), making predictions at each forward timestep, it would only allow links to ACs that have already been seen.",
      "startOffset" : 93,
      "endOffset" : 139
    }, {
      "referenceID" : 33,
      "context" : "PNs were originally proposed to allow a variable length decoding sequence (Vinyals et al., 2015b).",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 21,
      "context" : "Aside from the partial assumption of tree structure in the argumentative text, our models do not make any additional assumptions about the AC types or connectivity, unlike the work of (Peldszus, 2014).",
      "startOffset" : 184,
      "endOffset" : 200
    }, {
      "referenceID" : 30,
      "context" : "We evaluate our models on the corpora of (Stab and Gurevych, 2016) and (Peldszus, 2014), and compare our results with the results of the aformentioned authors.",
      "startOffset" : 41,
      "endOffset" : 66
    }, {
      "referenceID" : 21,
      "context" : "We evaluate our models on the corpora of (Stab and Gurevych, 2016) and (Peldszus, 2014), and compare our results with the results of the aformentioned authors.",
      "startOffset" : 71,
      "endOffset" : 87
    }, {
      "referenceID" : 30,
      "context" : "This can be labeled as a ‘micro’ approach to argumentation mining (Stab and Gurevych, 2016) (in con-",
      "startOffset" : 66,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "trast, there has been a number of efforts to identify argument structure at a higher level (Boltuzic and Šnajder, 2014; Ghosh et al., 2014; Cabrio and Villata, 2012)).",
      "startOffset" : 91,
      "endOffset" : 165
    }, {
      "referenceID" : 10,
      "context" : "trast, there has been a number of efforts to identify argument structure at a higher level (Boltuzic and Šnajder, 2014; Ghosh et al., 2014; Cabrio and Villata, 2012)).",
      "startOffset" : 91,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "trast, there has been a number of efforts to identify argument structure at a higher level (Boltuzic and Šnajder, 2014; Ghosh et al., 2014; Cabrio and Villata, 2012)).",
      "startOffset" : 91,
      "endOffset" : 165
    }, {
      "referenceID" : 2,
      "context" : "trast, there has been a number of efforts to identify argument structure at a higher level (Boltuzic and Šnajder, 2014; Ghosh et al., 2014; Cabrio and Villata, 2012)). Palau and Moens (2009) is an early work in this area, using a hand-crafted ContextFree Grammar to determine the structure of ACs in a corpus of legal texts.",
      "startOffset" : 92,
      "endOffset" : 191
    }, {
      "referenceID" : 2,
      "context" : "trast, there has been a number of efforts to identify argument structure at a higher level (Boltuzic and Šnajder, 2014; Ghosh et al., 2014; Cabrio and Villata, 2012)). Palau and Moens (2009) is an early work in this area, using a hand-crafted ContextFree Grammar to determine the structure of ACs in a corpus of legal texts. Lawrence et al. (2014) leverage a topic modeling-based AC similarity to uncover tree-structured arguments in philosophical texts.",
      "startOffset" : 92,
      "endOffset" : 348
    }, {
      "referenceID" : 2,
      "context" : "trast, there has been a number of efforts to identify argument structure at a higher level (Boltuzic and Šnajder, 2014; Ghosh et al., 2014; Cabrio and Villata, 2012)). Palau and Moens (2009) is an early work in this area, using a hand-crafted ContextFree Grammar to determine the structure of ACs in a corpus of legal texts. Lawrence et al. (2014) leverage a topic modeling-based AC similarity to uncover tree-structured arguments in philosophical texts. Unlike these methodologies, recent work offers data-driven approaches to the task of predicting links between ACs. Stab and Gurevych (2014b) approach the task as a binary classification problem.",
      "startOffset" : 92,
      "endOffset" : 596
    }, {
      "referenceID" : 2,
      "context" : "trast, there has been a number of efforts to identify argument structure at a higher level (Boltuzic and Šnajder, 2014; Ghosh et al., 2014; Cabrio and Villata, 2012)). Palau and Moens (2009) is an early work in this area, using a hand-crafted ContextFree Grammar to determine the structure of ACs in a corpus of legal texts. Lawrence et al. (2014) leverage a topic modeling-based AC similarity to uncover tree-structured arguments in philosophical texts. Unlike these methodologies, recent work offers data-driven approaches to the task of predicting links between ACs. Stab and Gurevych (2014b) approach the task as a binary classification problem. The authors train an SVM with various semantic and structural features. Peldszus and Stede have also used classification models for predicting the presence of links (2015).",
      "startOffset" : 92,
      "endOffset" : 822
    }, {
      "referenceID" : 24,
      "context" : "Various authors have also proposed to jointly model link extraction with other subtasks from the argumentation mining pipeline, using either a Integer Linear Programming (ILP) framework (Persing and Ng, 2016; Stab and Gurevych, 2016) or directly feeding previous subtask predictions into a tree-based parser.",
      "startOffset" : 186,
      "endOffset" : 233
    }, {
      "referenceID" : 30,
      "context" : "Various authors have also proposed to jointly model link extraction with other subtasks from the argumentation mining pipeline, using either a Integer Linear Programming (ILP) framework (Persing and Ng, 2016; Stab and Gurevych, 2016) or directly feeding previous subtask predictions into a tree-based parser.",
      "startOffset" : 186,
      "endOffset" : 233
    }, {
      "referenceID" : 21,
      "context" : "The former joint approaches are evaluated on annotated corpora of persuasive essays (Stab and Gurevych, 2014a, 2016), and the latter on a corpus of microtexts (Peldszus, 2014).",
      "startOffset" : 159,
      "endOffset" : 175
    }, {
      "referenceID" : 34,
      "context" : "(Vinyals et al., 2015c) use a sequenceto-sequence model for the task of syntactic parsing.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "(Bowman et al., 2015) experiment on an artificial entailment dataset that is specifically engineered to capture recursive logic (Bowman et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : ", 2015) experiment on an artificial entailment dataset that is specifically engineered to capture recursive logic (Bowman et al., 2014).",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 31,
      "context" : "1 Pointer Network A PN is a sequence-to-sequence model (Sutskever et al., 2014) with attention (Bahdanau et al.",
      "startOffset" : 55,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : ", 2014) with attention (Bahdanau et al., 2014) that was proposed to handle decoding sequences over the encoding inputs, and can be extended to arbitrary sets (Vinyals et al.",
      "startOffset" : 23,
      "endOffset" : 46
    }, {
      "referenceID" : 32,
      "context" : ", 2014) that was proposed to handle decoding sequences over the encoding inputs, and can be extended to arbitrary sets (Vinyals et al., 2015a).",
      "startOffset" : 119,
      "endOffset" : 142
    }, {
      "referenceID" : 14,
      "context" : "The PN uses Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) for sequential modeling, which produces a hidden layer h at each encoding/decoding timestep.",
      "startOffset" : 42,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "The PN uses a form of content-based attention (Bahdanau et al., 2014) to allow the model to produce a distribution over input elements.",
      "startOffset" : 46,
      "endOffset" : 69
    }, {
      "referenceID" : 36,
      "context" : "Each AC is itself a sequence of tokens, similar to the recently proposed Question-Answering dataset (Weston et al., 2015).",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 30,
      "context" : "We follow the work of (Stab and Gurevych, 2016) and focus on three different types of features to represent our ACs: 1) Bag-of-Words of the AC; 2) Embedding representation based on GloVe embeddings (Pennington et al.",
      "startOffset" : 22,
      "endOffset" : 47
    }, {
      "referenceID" : 23,
      "context" : "We follow the work of (Stab and Gurevych, 2016) and focus on three different types of features to represent our ACs: 1) Bag-of-Words of the AC; 2) Embedding representation based on GloVe embeddings (Pennington et al., 2014); 3) Structural features: Whether or not the AC is the first AC in",
      "startOffset" : 198,
      "endOffset" : 223
    }, {
      "referenceID" : 24,
      "context" : "However, recent work has shown that joint models that simultaneously try to complete multiple aspects of the subtask pipeline outperform models that focus on a single subtask (Persing and Ng, 2016; Stab and Gurevych, 2014b; Peldszus and Stede, 2015).",
      "startOffset" : 175,
      "endOffset" : 249
    }, {
      "referenceID" : 29,
      "context" : "However, recent work has shown that joint models that simultaneously try to complete multiple aspects of the subtask pipeline outperform models that focus on a single subtask (Persing and Ng, 2016; Stab and Gurevych, 2014b; Peldszus and Stede, 2015).",
      "startOffset" : 175,
      "endOffset" : 249
    }, {
      "referenceID" : 22,
      "context" : "However, recent work has shown that joint models that simultaneously try to complete multiple aspects of the subtask pipeline outperform models that focus on a single subtask (Persing and Ng, 2016; Stab and Gurevych, 2014b; Peldszus and Stede, 2015).",
      "startOffset" : 175,
      "endOffset" : 249
    }, {
      "referenceID" : 17,
      "context" : "Therefore, we will modify the architecture we proposed in Section 3 so that it would allow us to perform AC classification (Kwon et al., 2007; Rooney et al., 2012) together with link prediction.",
      "startOffset" : 123,
      "endOffset" : 163
    }, {
      "referenceID" : 26,
      "context" : "Therefore, we will modify the architecture we proposed in Section 3 so that it would allow us to perform AC classification (Kwon et al., 2007; Rooney et al., 2012) together with link prediction.",
      "startOffset" : 123,
      "endOffset" : 163
    }, {
      "referenceID" : 7,
      "context" : "This can be seen as a way of regularizing the hidden representations from the encoding component (Che et al., 2015).",
      "startOffset" : 97,
      "endOffset" : 115
    }, {
      "referenceID" : 28,
      "context" : "Some annotation schemes also include the class major claim (Stab and Gurevych, 2014a), which means this can be a multi-class classification task.",
      "startOffset" : 59,
      "endOffset" : 85
    }, {
      "referenceID" : 30,
      "context" : "We test the effectiveness of our proposed model on a dataset of persuasive essays (Stab and Gurevych, 2016), as well as a dataset of microtexts (Peldszus, 2014).",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : "We test the effectiveness of our proposed model on a dataset of persuasive essays (Stab and Gurevych, 2016), as well as a dataset of microtexts (Peldszus, 2014).",
      "startOffset" : 144,
      "endOffset" : 160
    }, {
      "referenceID" : 27,
      "context" : "5, and dropout (Srivastava et al., 2014) of 0.",
      "startOffset" : 15,
      "endOffset" : 40
    }, {
      "referenceID" : 38,
      "context" : "We believe the need for such high dropout is due to the small amounts of training data (Zarrella and Marsh, 2016), particularly in the Microtext corpus.",
      "startOffset" : 87,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "All models are trained with Adam optimizer (Kingma and Ba, 2014) with a batch size of 16.",
      "startOffset" : 43,
      "endOffset" : 64
    }, {
      "referenceID" : 30,
      "context" : "In both corpora we compare against the following previously proposed models: Base Classifier (Stab and Gurevych, 2016) is a feature-rich, taskspecific (AC type or link extraction) SVM classifier.",
      "startOffset" : 93,
      "endOffset" : 118
    }, {
      "referenceID" : 30,
      "context" : "Conversely, the ILP Joint Model (Stab and Gurevych, 2016) provides constraints by sharing prediction information between the base classifiers.",
      "startOffset" : 32,
      "endOffset" : 57
    }, {
      "referenceID" : 22,
      "context" : "For the microtext corpus only, we have the following comparative models: Simple (Peldszus and Stede, 2015) is a feature-rich logistic regression classifier.",
      "startOffset" : 80,
      "endOffset" : 106
    }, {
      "referenceID" : 22,
      "context" : "Best EG (Peldszus and Stede, 2015) creates an Evidence Graph (EG) from the predictions of a set of base classifiers.",
      "startOffset" : 8,
      "endOffset" : 34
    }, {
      "referenceID" : 22,
      "context" : "Lastly, MP+p (Peldszus and Stede, 2015) combines predictions from base classifiers with a Minimum Spanning Tree Parser (MSTParser).",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 30,
      "context" : "The popular method of averaging embeddings (which is used by (Stab and Gurevych, 2016) in their system) is in fact the worst method, although its performance is still competitive with the previous state-of-the-art.",
      "startOffset" : 61,
      "endOffset" : 86
    }, {
      "referenceID" : 33,
      "context" : "In this paper we have proposed how to use a modified PN (Vinyals et al., 2015b) to extract links between ACs in argumentative text.",
      "startOffset" : 56,
      "endOffset" : 79
    }, {
      "referenceID" : 30,
      "context" : "We evaluate our models on two corpora: a corpus of persuasive essays (Stab and Gurevych, 2016), and a corpus of microtexts (Peldszus, 2014).",
      "startOffset" : 69,
      "endOffset" : 94
    }, {
      "referenceID" : 21,
      "context" : "We evaluate our models on two corpora: a corpus of persuasive essays (Stab and Gurevych, 2016), and a corpus of microtexts (Peldszus, 2014).",
      "startOffset" : 123,
      "endOffset" : 139
    }, {
      "referenceID" : 16,
      "context" : "Future work can attempt to learn the AC representations themselves, such as in (Kumar et al., 2015).",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 37,
      "context" : "In addition, a segmenting technique, such as the one proposed by (Weston et al., 2014), can accomplish subtask 1.",
      "startOffset" : 65,
      "endOffset" : 86
    } ],
    "year" : 2017,
    "abstractText" : "Argumentation mining seeks to uncover the argument structure present in argumentative text. In order to determine this structure, one must understand how different individual components of the overall argument are linked. General consensus in this field dictates that the argument components form a hierarchy of persuasion, which manifests itself in a tree structure. This work provides the first neural network-based approach to argumentation mining, focusing on the dual tasks of extracting links between argument components, and classifying types of argument components. We propose to use a joint model based on a Pointer Network architecture to simultaneously solve these tasks. In doing so, we construct a joint model that simultaneously attempts to learn the type of argument component, as well as continuing to predict links between argument components. The proposed joint model achieves state-of-the-art results on two separate evaluation corpora, achieving far superior performance than a regular Pointer Network model. Our results show that optimizing for both tasks is crucial for high performance.",
    "creator" : "LaTeX with hyperref package"
  }
}