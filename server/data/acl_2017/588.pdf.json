{
  "name" : "588.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Rare Entity Prediction: Language Understanding with External Knowledge using Hierarchical LSTMs",
    "authors" : [ "Peter Ackroyd" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "One of the preeminent problems in natural language processing is the design of models that can acquire knowledge of the world through language. Such knowledge can be obtained in at least two ways: through unstructured texts such as news articles or blogs, and through structured knowledge bases such as WordNet (Miller, 1995) and Freebase (Bollacker et al., 2008).\nA natural setting for testing the abilities of models to acquire knowledge through natural language is reading comprehension, where models must answer questions about a given document. Many existing reading comprehension tasks, however, can be solved using basic language modelling, and re-\nContext [...] , who lived from 1757 to 1827, was admired by a small group of intellectuals and artists in his day, but never gained general recognition as either a poet or painter. [...]\nCandidate Entities Peter Ackroyd: Peter Ackroyd is an English biographer, novelist and critic with a particular interest in the history and culture of London. [...] William Blake: William Blake was an English poet, painter, and printmaker. [...] Emanuel Swedenborg: Emanuel Swedenborg was a Swedish scientist, philosopher, theologian, revelator, and mystic. [...]\nquire little reasoning. This is the case for the Daily Mail/ CNN dataset (Hermann et al., 2015), for example, which has been shown to have a small gap between machine and human performance (Chen et al., 2016). Existing reading comprehension tasks target reasoning about generic concepts, while accounting for syntax, lexical semantics, and/or discourse. In this work, we aim to move towards reasoning about specific instances of entities in context. This is a very difficult problem, as we have very few training samples per instance; thus, we demonstrate that we cannot simply rely on language modelling, and must leverage external sources of knowledge.\nRecent efforts have been made to integrate different sources of knowledge, for example com-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nbining distributional and relational semantics for building word embeddings (Fried and Duh, 2014; Long et al., 2016). These models show performance improvements on tasks such as knowledge base completion. However, such ideas are rarely examined for reading comprehension tasks.\nIn this paper, we propose a novel task called rare entity prediction, where models must predict missing entities (e.g. proper names or places) in web articles by leveraging the available lexical resources. Using lexical resources is necessary as there are a large number of rare entities in the dataset (see Table 2); thus, it is much more difficult to obtain information about these entities using the unstructured texts alone. For this task, we provide a significantly enhanced version of the Wikilinks dataset, with entity descriptions extracted from Freebase serving as the lexical resources, which we call the Wikilinks Entity Prediction dataset. An example from the Wikilinks Entity Prediction dataset is shown in Table 1.\nIn addition to the task and dataset, we introduce several models that can be used to solve this task, using distributional semantics and recurrent neural networks. We show that language modelling baseline models that do not consider the Freebase descriptions are unable to achieve good performance on the task. RNN models leveraging the external knowledge perform much better; in particular, our hierarchical double encoder model (HIERENC) achieves a 17% increase in accuracy over the language model baseline.\nRare entity prediction task is unique because it is difficult for models that heavily rely on word cooccurrence statistics to achieve good performance. To solve the task, models need to have the abilities to incorporate external a priori knowledge with an understanding of the unstructured natural language. Some questions can be challenging to answer even for humans without proper prior knowledge about the candidate entities. Table 1 provides a good example of such scenarios. We believe that being able to integrate multiple sources of knowledge is crucial for not only reading comprehension, but also other NLP systems."
    }, {
      "heading" : "2 Related Work",
      "text" : "Related to our work is the task of entity prediction, also called link prediction or knowledge base completion, in the domain of multi-relational data. Multi-relational datasets like WordNet (Miller,\n1995) and Freebase (Bollacker et al., 2008) consist of entity-relation triples of the form (head, relation, tail), indicating a relationship between the head and tail entity as described by the relation. In entity prediction, either the head or tail entity is removed, and the model has to predict the missing entity. While this task necessitates understanding structured information in the form of relationships between objects, it does not require the processing of natural language text, which is crucial in rare entity prediction.\nAlso related to our approach is the task of script learning or script induction, in which a system is asked to predict an event, or a (verb, dependency) pair, that is held out from a document (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012). For example, the event ‘X orders Y ’ could predict the event ‘X eats Y ’. Approaches to this problem have used relational atoms similar to relation triples to represent events (Pichotta and Mooney, 2014), or have used recurrent neural networks (Pichotta and Mooney, 2016). Similarly, the task of discourse coherence modeling involves determining text relatedness at different levels of granularity (Grosz et al., 1995). While these tasks involve understanding unstructured natural language, and there are entity-based approaches for solving them (Barzilay and Lapata, 2008), they differ from our task as they do not explicitly require the use of structured knowledge sources such as Freebase. Events can be represented, for example, as relational atoms; for example, the event ‘X eats Y ’ could be represented as eat(X,Y ).\nRare entity prediction is clearly distinct from tasks such as entity tagging and recognition (Ritter et al., 2011), as models are provided with the actual name of the entity in question, and only have to match the entity with related concepts and tags. It is more closely related to the machine reading literature from e.g. Etzioni et al. (2006); however, the authors define machine reading as primarily unsupervised, whereas our task is supervised.\nA similar supervised reading comprehension task was proposed by Hermann et al. (2015) using news articles from CNN and the Daily Mail. Given an article, models are tasked with filling in blanks of one-sentence summaries of the article. The original dataset was found to have a low ceiling for machine improvement (Chen et al., 2016); thus, alternative datasets have been proposed that consist of more difficult questions (Trischler et al.,\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n2016; Rajpurkar et al., 2016). A dataset with a similar task was also proposed by Hill et al. (2015a), where models must answer questions about short children’s stories. While these tasks require the understanding of unstructured natural language, it does not require integration with external knowledge sources.\nHill et al. (2015b) propose a method of combining distributional semantics with an external knowledge source in the form of dictionary definitions. The purpose of their model is to obtain more accurate word and phrase embeddings by combining lexical and phrasal semantics, and they achieve fairly good performance on a reverse dictionary and crossword QA tasks.\nPerhaps the most related approach to our work is the one concurrently developed by Ahn et al. (2016). The authors propose a WikiFacts dataset where Wikipedia descriptions are aligned with Freebase facts. While they also aim to integrate external knowledge with unstructured natural language, their task differs from ours in that it is primarily a language modeling problem.\nFinally, methods that address factoid question answering (QA) using Freebase (Bordes et al., 2014a; Yao and Van Durme, 2014; Serban et al., 2016) must also combine knowledge from a structured knowledge base with an understanding of the question that’s being asked. However, the nature of the unstructured data is different: in factoid QA, questions are often short and relate directly to the relations of a given entity, while rare entity prediction requires learning much longer-term dependencies in natural language, where relations between entities are not specifically referred to."
    }, {
      "heading" : "3 Rare Entity Prediction",
      "text" : "3.1 The Wikilinks Dataset\nThe Wikilinks dataset (Singh et al., 2012) is a large dataset originally designed for cross-document coreference resolution, the task of grouping entity mentions from a set of documents into clusters that represent a single entity. The dataset consists of a list of non-Wikipedia web pages (discovered using the Google search index) that contain hyperlinks to Wikipedia, such as random blog posts or news articles. Every token with a hyperlink to Wikipedia is then marked and considered an entity mention in the dataset. Each entity mention is also linked back to a knowledge base through their corresponding Freebase IDs.\nIn order to ensure the hyperlinks refer to the correct Wikipedia pages, additional filtering is conducted to ensure that either (1) at least one token in the hyperlink (or anchor) matches a token in the title of the Wikipedia page, or (2) the anchor text matches exactly an anchor from the Wikipedia page text, which can be considered an alias of the page. As many near-duplicate copies of Wikipedia pages can be found online, any web pages where more than 70% of the sentences match those from their linked Wikipedia pages are discarded. To ensure the quality of data, the authors performed manual inspection on a small subset of the web pages. No incorrect hyperlinks were found.\n3.2 The Wikilinks Rare Entity Prediction Dataset\nWe provide a significantly pre-processed and augmented version of the Wikilinks dataset for the purpose of entity prediction, which we called the Wikilinks Rare Entity Prediction dataset1. In particular, we parse the HTML texts of the web pages and extract their page contents to form our corpus. Entity mentions with hyperlinks to Wikipedia are marked and replaced by a special token (**blank**), serving as the placeholder for missing entities that we would like the models to predict. The correct missing entity ẽ is preserved as a target. Additionally, we extract the lexical definitions of all entities that are marked in the corpus from Freebase using their Freebase IDs, which are available for all entities in the Wikilinks dataset. These lexical definitions will serve as the external knowledge to our models.\nTable 2 above shows some basic statistics of our dataset. As we can see, unlike the Children’s\n1We will make this dataset publicly available upon acceptance of the paper.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nBook dataset, which has 50k candidate entities for almost 700k context and query pairs (Hill et al., 2015a), the number of unique entities found in our dataset is in the same order of magnitude as the number of documents available.\nMoreover, the majority of entities appears a relatively small number of times, with 92.8% observed less than or equal to 10 times across the entire corpus. This suggests that models that only rely on the surrounding context information may not be able to correctly predict the missing entities. This further motivates us to incorporate additional information into the decision process to improve the performance. In the experiments section, we show that the lexical resources are indeed necessary to achieve better results.\n3.3 Task Definition2\nHere, we formalize the task definition of the entity prediction problem. Given a document D in the corpus, we split it into an ordered list of contexts C = {C1, ...,Cn} where each context Ci (1 ≤ i ≤ n) is a single sentence (w1, ..., wm) where the special token **blank** is found.3 Let E be the set of candidate entities. For each missing entity, we want the model to select the correct entity e ∈ E to fill the blank slot. In our problem\n2 On notation; we use A to denote sequences, A to denote sets, a to denote words/entities, a to denote vectors, A to denote matrices.\n3Note that we restrict each context to only have one **blank** token.\nsetting, the model also has access to the lexical resource L = {Le | e ∈ E} where Le = (le1, ..., lek) is the lexical definition of entity e extracted from the knowledge base. Thus, the task of the model is to predict the correct missing entities for each empty slot in D.\nThere are several possible ways to specify the candidate set E . For instance, we could define E so that it includes all entities found in the corpus. However, given the extremely large amount of unique entities found in the dataset, this would render the task difficult to solve from both a practical and computational perspective. We present a simpler version of the task where E is the set of entities that are present in the document D. Note that we can make the task arbitrarily more difficult by randomly sampling other entities from the entity vocabulary and adding them to the candidate set.\nWe show an example from the Wikilinks Entity Prediction dataset, along with a visual guide to the notation from this section, in Figure 1."
    }, {
      "heading" : "4 Model Architectures",
      "text" : "In this section, we present two models that use the lexical definitions of entities to solve the proposed rare entity prediction problem. The basic building blocks of our models are recurrent neural networks (RNN) with long short-term memory (LSTM) units. An RNN is a neural network with feedback connections that allows information from the past to be encoded in the hidden layer\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\ne→ Le :\nCi: de\nhei\nw1 ... ... wm\nle1 ... lek\nP (e = ẽ|Ci, Le) = σ((hei )TWde + b)\nFigure 2: Our double encoder architecture. Each entity e has an associated lexical definition Le = (le1, le2..., lek), which is fed through the lexical encoder g (orange squares) to provide an encoding de. This definition embedding is then fed as the blank input token of context Ci to the context encoder f (blue circles), which provides a context embedding hei .\nh̄i = 1 |E| ∑ e′∈E h e′ ih̄i−1h̄i−2\n...\nP (e = ẽ|C1..i, Le) = σ((hei )TWde + rTi V + b) ri\n... de′ ... de\nheiCi\nFigure 3: Our hierarchical encoder architecture. Each entity e is encoded as de, at each timestep, hei is computed for each e. h̄i is the average encoding, which is fed as input to the temporal model r (green diamonds). The temporal model produces ri, which is used to compute P (e = ẽ|C1..i, Le).\nrepresentation, thus is ideal for modelling sequential data (Dietterich, 2002) and most language related problems.\nLSTMs are an extension of RNNs which include a memory cell ct alongside their hidden state representation ht. Reads and writes to the memory cell are controlled by a set of three gates that allow the model to either keep or discard information from the past and update their state with the current input (Hochreiter and Schmidhuber, 1997). This allows LSTMs to model potentially longer dependencies and at the same time mitigate the vanishing and exploding gradient problems, which are quite common among regular RNNs (Hochreiter, 1991; Bengio et al., 1994). In our experiments, we use LSTMs augmented with peephole connections (Gers et al., 2002).\nWe denote the output (i.e. the last hidden state) of an RNN f operating on a sequence S as f(S), and subscript the t-th hidden state as ft(S)."
    }, {
      "heading" : "4.1 Double Encoder (DOUBENC)",
      "text" : "We use two jointly trained recurrent models, a lexical encoder g(.) and a context encoder f(.), and a\nlogistic predictor P (see Figure 2). The lexical encoder converts the definition of an entity into a vector embedding, while the context encoder repeats the same process for a given context to obtain its context embedding. These two embeddings are then used by P to predict if the given entity-context pair is correct. Additionally, the blank in the context sentence is replaced by the encoded definition embedding to provide more information to f .\nFor an entity e in the candidate set E of document D, we retrieve its corresponding lexical definition Le, itself a sequence of words, to compute its encoding g(Le) ≡ de.\nFor a given context Ci, we replace the embedding of the blank token with de. Thus Ci = (w1, ..., wblank, ..., wm) becomes Cei = (w1, ...,de, ..., wm)\n4. We then compute the context embedding of the new Cei , f(C e i ) ≡ hei\nAfter getting hei and de, we wish to compute the probability of candidate e being the correct entity\n4We mix the word/vector notation here since each word w is replaced by its corresponding word embedding vector.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nẽ missing in context Ci. This probability is the output of the predictor:\nP (e = ẽ|Ci, Le) = σ((hei )TWde + b)\nwhere σ is the sigmoid function, W and b are model parameters. The cross term (hei )\nTWde is a dot product between hei and de that weighs the dimensions differently based on the learned parametersW . A similar prediction method has been used successfully for question answering (Bordes et al., 2014b; Yu et al., 2014) and dialogue response retrieval (Lowe et al., 2015). Note that while hei is a function of de, using de in the cross term (hei )\nTWde provides a much shorter gradient path from the loss to the lexical encoder through de.\nGiven a context, the model outputs a probability for each entity e ∈ E . Entities in the candidate set are then ranked against each other according to their predicted probabilities. The entity with the highest probability is considered as the most plausible answer for the missing entity in the current context. We consider the model to make an error if that entity is not ẽ."
    }, {
      "heading" : "4.2 Hierarchical Double Encoder",
      "text" : "(HIERENC)\nThe double encoder model above considers each context independently. However, since each document consists of a sequence of contexts, the knowledge carried by other contexts in C could also provide useful information for the decision process of Ci.\nTo that end, we propose a hierarchical model structure by adding a LSTM network, which we call the temporal network r (see Figure 3), on top of the double encoder architecture. Since a document is a sequence of Cis, each timestep of this network consists of one such context, and thus is indexed with i.\nSince we already have a context encoder f , we reuse the output of f(Cei ) as the input of r at timestep i. More specifically, we combine the embeddings generated by f into a single one via averaging: h̄i = 1|E| ∑ e′∈E h e′ i , which then serves as the input to the temporal network for context Ci. Note that alternatively, one could aggregate information about the past predictions through other means like policies or soft attention. However, this would introduce extra complexities to the learning process. As such, we use averaging to that end.\nFinally, at each timestep i, the temporal network outputs an embedding ri(C1, ...,Cn) ≡ ri. We use\nthis temporal embedding to predict the probability of the context-entity pair using slightly altered P :\nP (e = ẽ|C1..i, Le) = σ((hei )TWde + rTi V + b)\nwhere W , V and b are model parameters. The entities in candidate set are again ranked against each other based on their probabilities."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Experiment Setup",
      "text" : "We randomly partition our dataset into training, validation and test sets. The training set consists of approximately 80% of the total dataset, the validation and test sets comprise about 10% each.\nIn our experiments, the contexts are defined as the sentences where the special **blank** tokens are found. The lexical definitions for each entity are the first sentences of their Freebase descriptions. We have experimented with different configurations of defining contexts and entity definitions, such as expanding the context window by including the sentences before and after the blank, as well as taking more than one sentence out of the entity description. However, results on the validation set show that increasing the context window size and definition size has very little impact on the evaluation metrics and drastically increases the training time of all models.\nTo train our models, we use the correct missing entity for each blank as the positive sample and all other entities in the candidate set as the negative samples. During the testing phase, we present each entity in the candidate set to our models and record the probabilities output by the models. The entity with the highest probability is chosen as the model prediction. For all gradient-based methods, including both baseline models and our proposed models, the learning objective is to minimize the binary cross-entropy of the training data.\nWe measure the performance on our entity prediction task using the accuracy; that is, the number of correct entity predictions made by the model divided by the total number of predictions. This is equivalent to the metric of Recall@1 that is often used in information retrieval. All our models are implemented using Theano (Theano Development Team, 2016)."
    }, {
      "heading" : "5.2 Baselines",
      "text" : "In order to demonstrate the effects of using lexical resources as external knowledge for solving\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nthe task, we present here three sets of baselines: one set of simple baselines (RANDOM and FREQENT), one LSTM-based model that only relies on the contexts and does not utilize the definitions (CONTENC), and another set of models that do make use of the entity definitions but in a relatively simple fashion (TF-IDF + COS and AVGEMB + COS).\nRANDOM For each context in a given document, this baseline simply selects an entity from the candidate set uniformly at random as its prediction.\nFREQENT Under this baseline, we rank all entities in the candidate set by the number of times that they appear in the document. For each blank in the document, we always choose the entity with the highest frequency in that document as the prediction. Note that this model has access to extra information compared to the other models, in particular the total number of times each entity appears in the document.\nCONTENC Instead of using their definitions, entities are treated as regular tokens in our vocabulary. Thus for a particular entity e, the context sequence Ci = [w1, ..., wblank, ..., wm] becomes [w1, ..., we, ..., wm]. We feed the sequence Ci into the context encoder and as usual take the last hidden state as the context embedding hei . Thus given Ci and e ∈ E, the probability of e being the correct missing entity is:\nP (e = ẽ|Ci) = σ((hei )TW + b)\nwhere again σ is the sigmoid function, W and b are model parameters. This model is essentially a language model baseline, that does not make use of the external a priori knowledge.\nTF-IDF + COS This method takes the term frequency-inverse document frequency (TF-IDF) vectors of the context and the entity definition as their corresponding embeddings. The aggregations of contexts and definitions are treated as their own corpora, and two separate TF-IDF transformers are fitted. Candidate entities are ranked by the cosine similarity between their definition vectors and the context vector. The entity with the highest cosine similarity score is chosen as the prediction.\nAVGEMB + COS This baseline computes the context embedding by taking the average of some\nAccuracy Model Valid Test Fixed baselines RANDOM 0.294 0.301 FREQENT 0.329 0.331 Without external knowledge CONTENC 0.393 0.396 With external knowledge TF-IDF + COS 0.292 0.300 AVGEMB + COS 0.355 0.359 DOUBENC 0.547 0.540 HIERENC 0.573 0.566\nTable 3: Results on the Wikilinks Entity Prediction dataset for all proposed baselines and models.\npre-trained word embeddings. The entities’ embeddings are computed in the same way. In our experiments, we choose to use the published GloVe (Pennington et al., 2014) pre-trained word embeddings. Same as above, the prediction is made by considering the cosine similarity between the context embedding and the entity embeddings."
    }, {
      "heading" : "5.3 Results",
      "text" : "Empirical results are shown in Table 3. We test our two proposed model architectures (detailed in Section 4), in addition to baselines described in Section 5.2.\nFor the CONTENC baseline, we choose 300 as the size of hidden state for the encoder. For the DOUBENC and the HIERENC models, the size of hidden state for both the context encoder and the lexical encoder is set to 300. Additionally, an RNN with 200 LSTM units is used as the temporal network in the hierarchical case. All three models are trained with stochastic gradient descent with Adam (Kingma and Ba, 2014) as our optimizer, with a learning rate of 0.0005 used for CONTENC and 0.0001 used for DOUBENC as well as HIERENC. Models with the best performance on validation set are saved and used to test on the test set.\nIt is clear from Table 3 that models that only use contextual knowledge give relatively poor performance compared to the ones that utilize lexical resources. The large discrepancy between the context encoder and the double encoder shows that lexical resources play a crucial role in solving the task. The best result is achieved by the hierarchical double encoder, which confirms that knowing about previous contexts is indeed beneficial to the prediction at the current timestep.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nContext & Prediction [...] We heard from Audrey Bomse, who is with the Free Gaza movement. She was in , Cyprus. [...]\nCONTENC: Istanbul HIERENC: Larnaca\nCandidate Set Istanbul: Istanbul is the most populous city in Turkey, and the country’s economic, cultural, and historical center. Larnaca: Larnaca is a city on the southern coast of Cyprus and the capital of the eponymous district. Ben Macintyre: Ben Macintyre is a British author, historian, reviewer and columnist writing for The Times newspaper. (Other candidates......)\nTable 4: An example from the test set, with the predictions made by CONTENC and HIERENC. HIERENC was able to successfully predict the correct missing entity, Larnaca.\nHowever, even our best performing model, the hierarchical double encoder, is still a long way from solving this task, achieving an accuracy of 56.6%. This reflects the challenging nature of the task, as most of the predictions are about entities that have been seen fewer than 5 times in the entire corpus. Note that, since Singh et al. (2012) manually verified the quality of the original Wikilinks dataset, there are virtually no entities that are incorrectly labeled, and thus this is not a significant source of error."
    }, {
      "heading" : "6 Discussion",
      "text" : "Table 4 shows an example found in the test set, along with the predictions from CONTENC and HIERENC. Although the context encoder was able to identify that the missing entity should be a city, it incorrectly predicted Istanbul. This is likely because Istanbul appears 86 times in the dataset, whereas Larnaca appears only twice. It seems that, although the context encoder was able to derive a strong association between Istanbul and Middle Eastern geolocations, such knowledge was not learned for Larnaca because of the lack of examples. Conversely, the hierarchical double encoder was able to take both the context and the external knowledge into account and successfully predicted the correct missing city.\nOne interesting observation is the margin of difference in accuracy between the context encoder\nand the embedding average baseline. The context encoder, which is a relatively sophisticated context-only model, only slightly outperforms the simple embedding average baseline that has no learning component. This suggests that the lexical definitions are valuable in solving such tasks even when it is used in a rather simplistic way.\nAnother interesting observation is that, in our experiments we found that using a large context window size (including the sentences before and after the sentence where the blank is found) does not have any significant positive impact on the results. This implies that the words that are most informative about the missing entity in the blank are generally found in the vicinity of the blank. It is likely that more sophisticated models will be able to use the surrounding context information more effectively, leading to greater performance increases."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this paper, we examine the use of external knowledge in the form of lexical resources for solving reading comprehension problems. Specifically, we propose the problem of rare entity prediction. In our Wikilinks Rare Entity Prediction dataset, the majority of the entities have very low frequencies across the entire corpus; thus, models that solely rely on co-occurrence statistics tend to underperform. We show that models leveraging the Freebase descriptions achieve large performance increases, particularly when this information is incorporated intelligently as in our double encoder-based models.\nFor future work, it would be interesting to examine the effects of other knowledge sources. In this paper, we use entity definitions as the source of external knowledge. However, Freebase also contains other types of valuable information, such as relational information between entities. Thus, one potential direction for future work would be to incorporate the relational information, alongside the lexical definitions, to achieve better results.\nWe have seen the crucial role that external knowledge plays in solving tasks with many rare entities. Thus we believe that incorporating external knowledge into other NLP systems, such as dialogue agents, should also see similar positive results. We plan to explore the idea of external knowledge integration further in our future research.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "A neural knowledge language model",
      "author" : [ "Sungjin Ahn", "Heeyoul Choi", "Tanel Pärnamaa", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1608.00318 .",
      "citeRegEx" : "Ahn et al\\.,? 2016",
      "shortCiteRegEx" : "Ahn et al\\.",
      "year" : 2016
    }, {
      "title" : "Modeling local coherence: An entity-based approach",
      "author" : [ "Regina Barzilay", "Mirella Lapata." ],
      "venue" : "Computational Linguistics 34(1):1–34.",
      "citeRegEx" : "Barzilay and Lapata.,? 2008",
      "shortCiteRegEx" : "Barzilay and Lapata.",
      "year" : 2008
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Yoshua Bengio", "Patrice Simard", "Paolo Frasconi." ],
      "venue" : "IEEE transactions on neural networks 5(2):157–166.",
      "citeRegEx" : "Bengio et al\\.,? 1994",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "Freebase: a collaboratively created graph database for structuring human knowledge",
      "author" : [ "Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor." ],
      "venue" : "Proceedings of the 2008 ACM SIGMOD international conference on Management",
      "citeRegEx" : "Bollacker et al\\.,? 2008",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Question answering with subgraph embeddings",
      "author" : [ "Antoine Bordes", "Sumit Chopra", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1406.3676 .",
      "citeRegEx" : "Bordes et al\\.,? 2014a",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2014
    }, {
      "title" : "Open question answering with weakly supervised embedding models",
      "author" : [ "Antoine Bordes", "Jason Weston", "Nicolas Usunier." ],
      "venue" : "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, pages 165–180.",
      "citeRegEx" : "Bordes et al\\.,? 2014b",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2014
    }, {
      "title" : "Unsupervised learning of narrative schemas and their participants",
      "author" : [ "Nathanael Chambers", "Dan Jurafsky." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language",
      "citeRegEx" : "Chambers and Jurafsky.,? 2009",
      "shortCiteRegEx" : "Chambers and Jurafsky.",
      "year" : 2009
    }, {
      "title" : "Unsupervised learning of narrative event chains",
      "author" : [ "Nathanael Chambers", "Daniel Jurafsky." ],
      "venue" : "ACL. Citeseer, volume 94305, pages 789–797.",
      "citeRegEx" : "Chambers and Jurafsky.,? 2008",
      "shortCiteRegEx" : "Chambers and Jurafsky.",
      "year" : 2008
    }, {
      "title" : "A thorough examination of the cnn/daily mail reading comprehension task",
      "author" : [ "Danqi Chen", "Jason Bolton", "Christopher D Manning." ],
      "venue" : "arXiv preprint arXiv:1606.02858 .",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Machine learning for sequential data: A review",
      "author" : [ "Thomas G Dietterich." ],
      "venue" : "Structural, syntactic, and statistical pattern recognition, Springer, pages 15–30.",
      "citeRegEx" : "Dietterich.,? 2002",
      "shortCiteRegEx" : "Dietterich.",
      "year" : 2002
    }, {
      "title" : "Machine reading",
      "author" : [ "Oren Etzioni", "Michele Banko", "Michael J Cafarella." ],
      "venue" : "AAAI. volume 6, pages 1517–1519.",
      "citeRegEx" : "Etzioni et al\\.,? 2006",
      "shortCiteRegEx" : "Etzioni et al\\.",
      "year" : 2006
    }, {
      "title" : "Incorporating both distributional and relational semantics in word representations",
      "author" : [ "Daniel Fried", "Kevin Duh." ],
      "venue" : "arXiv preprint arXiv:1412.4369 .",
      "citeRegEx" : "Fried and Duh.,? 2014",
      "shortCiteRegEx" : "Fried and Duh.",
      "year" : 2014
    }, {
      "title" : "Learning precise timing with lstm recurrent networks",
      "author" : [ "Felix A Gers", "Nicol N Schraudolph", "Jürgen Schmidhuber." ],
      "venue" : "Journal of machine learning research 3(Aug):115–143.",
      "citeRegEx" : "Gers et al\\.,? 2002",
      "shortCiteRegEx" : "Gers et al\\.",
      "year" : 2002
    }, {
      "title" : "Centering: A framework for modeling the local coherence of discourse",
      "author" : [ "Barbara J Grosz", "Scott Weinstein", "Aravind K Joshi." ],
      "venue" : "Computational linguistics 21(2):203–225.",
      "citeRegEx" : "Grosz et al\\.,? 1995",
      "shortCiteRegEx" : "Grosz et al\\.",
      "year" : 1995
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 1684–",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "The goldilocks principle: Reading children’s books with explicit memory representations",
      "author" : [ "Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston." ],
      "venue" : "arXiv preprint arXiv:1511.02301 .",
      "citeRegEx" : "Hill et al\\.,? 2015a",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to understand phrases by embedding the dictionary",
      "author" : [ "Felix Hill", "Kyunghyun Cho", "Anna Korhonen", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1504.00548 .",
      "citeRegEx" : "Hill et al\\.,? 2015b",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2015
    }, {
      "title" : "Untersuchungen zu dynamischen neuronalen Netzen",
      "author" : [ "Sepp Hochreiter." ],
      "venue" : "Ph.D. thesis, diploma thesis, institut für informatik, lehrstuhl prof. brauer, technische universität münchen.",
      "citeRegEx" : "Hochreiter.,? 1991",
      "shortCiteRegEx" : "Hochreiter.",
      "year" : 1991
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Skip n-grams and ranking functions for predicting script events",
      "author" : [ "Bram Jans", "Steven Bethard", "Ivan Vulić", "Marie Francine Moens." ],
      "venue" : "Proceedings of the 13th Conference of the European Chapter of the Association for Computa-",
      "citeRegEx" : "Jans et al\\.,? 2012",
      "shortCiteRegEx" : "Jans et al\\.",
      "year" : 2012
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980 .",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Leveraging lexical resources for learning entity embeddings in multi-relational data",
      "author" : [ "Teng Long", "Ryan Lowe", "Jackie Chi Kit Cheung", "Doina Precup." ],
      "venue" : "arXiv preprint arXiv:1605.05416 .",
      "citeRegEx" : "Long et al\\.,? 2016",
      "shortCiteRegEx" : "Long et al\\.",
      "year" : 2016
    }, {
      "title" : "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
      "author" : [ "Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau." ],
      "venue" : "arXiv preprint arXiv:1506.08909 .",
      "citeRegEx" : "Lowe et al\\.,? 2015",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2015
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A Miller." ],
      "venue" : "Communications of the ACM 38(11):39–",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "EMNLP. volume 14, pages 1532– 1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Statistical script learning with multi-argument events",
      "author" : [ "Karl Pichotta", "Raymond J Mooney." ],
      "venue" : "EACL. volume 14, pages 220–229.",
      "citeRegEx" : "Pichotta and Mooney.,? 2014",
      "shortCiteRegEx" : "Pichotta and Mooney.",
      "year" : 2014
    }, {
      "title" : "Learning statistical scripts with lstm recurrent neural networks",
      "author" : [ "Karl Pichotta", "Raymond J Mooney." ],
      "venue" : "Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16).",
      "citeRegEx" : "Pichotta and Mooney.,? 2016",
      "shortCiteRegEx" : "Pichotta and Mooney.",
      "year" : 2016
    }, {
      "title" : "Squad: 100,000+ questions for machine comprehension of text",
      "author" : [ "Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang." ],
      "venue" : "arXiv preprint arXiv:1606.05250 .",
      "citeRegEx" : "Rajpurkar et al\\.,? 2016",
      "shortCiteRegEx" : "Rajpurkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Named entity recognition in tweets: an experimental study",
      "author" : [ "Alan Ritter", "Sam Clark", "Oren Etzioni" ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
      "citeRegEx" : "Ritter et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ritter et al\\.",
      "year" : 2011
    }, {
      "title" : "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus",
      "author" : [ "Iulian Vlad Serban", "Alberto Garcı́a-Durán", "Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Aaron Courville", "Yoshua Bengio" ],
      "venue" : null,
      "citeRegEx" : "Serban et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "Wikilinks: A large-scale cross-document coreference corpus labeled via links to Wikipedia",
      "author" : [ "Sameer Singh", "Amarnag Subramanya", "Fernando Pereira", "Andrew McCallum." ],
      "venue" : "Technical Report UMCS-2012-015.",
      "citeRegEx" : "Singh et al\\.,? 2012",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2012
    }, {
      "title" : "Theano: A Python framework for fast computation of mathematical expressions",
      "author" : [ "Theano Development Team." ],
      "venue" : "arXiv e-prints abs/1605.02688. http://arxiv.org/abs/1605.02688.",
      "citeRegEx" : "Team.,? 2016",
      "shortCiteRegEx" : "Team.",
      "year" : 2016
    }, {
      "title" : "Newsqa: A machine comprehension dataset",
      "author" : [ "Adam Trischler", "Tong Wang", "Xingdi Yuan", "Justin Harris", "Alessandro Sordoni", "Philip Bachman", "Kaheer Suleman." ],
      "venue" : "arXiv preprint arXiv:1611.09830 .",
      "citeRegEx" : "Trischler et al\\.,? 2016",
      "shortCiteRegEx" : "Trischler et al\\.",
      "year" : 2016
    }, {
      "title" : "Information extraction over structured data: Question answering with freebase",
      "author" : [ "Xuchen Yao", "Benjamin Van Durme." ],
      "venue" : "ACL (1). Citeseer, pages 956–966.",
      "citeRegEx" : "Yao and Durme.,? 2014",
      "shortCiteRegEx" : "Yao and Durme.",
      "year" : 2014
    }, {
      "title" : "Deep learning for answer sentence selection",
      "author" : [ "Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman." ],
      "venue" : "arXiv preprint arXiv:1412.1632 .",
      "citeRegEx" : "Yu et al\\.,? 2014",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Such knowledge can be obtained in at least two ways: through unstructured texts such as news articles or blogs, and through structured knowledge bases such as WordNet (Miller, 1995) and Freebase (Bollacker et al.",
      "startOffset" : 167,
      "endOffset" : 181
    }, {
      "referenceID" : 3,
      "context" : "Such knowledge can be obtained in at least two ways: through unstructured texts such as news articles or blogs, and through structured knowledge bases such as WordNet (Miller, 1995) and Freebase (Bollacker et al., 2008).",
      "startOffset" : 195,
      "endOffset" : 219
    }, {
      "referenceID" : 14,
      "context" : "This is the case for the Daily Mail/ CNN dataset (Hermann et al., 2015), for example, which has been shown to have a small gap between machine and human performance (Chen et al.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : ", 2015), for example, which has been shown to have a small gap between machine and human performance (Chen et al., 2016).",
      "startOffset" : 101,
      "endOffset" : 120
    }, {
      "referenceID" : 11,
      "context" : "bining distributional and relational semantics for building word embeddings (Fried and Duh, 2014; Long et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "bining distributional and relational semantics for building word embeddings (Fried and Duh, 2014; Long et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 116
    }, {
      "referenceID" : 23,
      "context" : "Multi-relational datasets like WordNet (Miller, 1995) and Freebase (Bollacker et al.",
      "startOffset" : 39,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "Multi-relational datasets like WordNet (Miller, 1995) and Freebase (Bollacker et al., 2008) consist of entity-relation triples of the form (head, relation, tail), indicating a relationship between the head and tail entity as described by the relation.",
      "startOffset" : 67,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : "Also related to our approach is the task of script learning or script induction, in which a system is asked to predict an event, or a (verb, dependency) pair, that is held out from a document (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012).",
      "startOffset" : 192,
      "endOffset" : 246
    }, {
      "referenceID" : 25,
      "context" : "Approaches to this problem have used relational atoms similar to relation triples to represent events (Pichotta and Mooney, 2014), or have used recurrent neural networks (Pichotta and Mooney, 2016).",
      "startOffset" : 102,
      "endOffset" : 129
    }, {
      "referenceID" : 26,
      "context" : "Approaches to this problem have used relational atoms similar to relation triples to represent events (Pichotta and Mooney, 2014), or have used recurrent neural networks (Pichotta and Mooney, 2016).",
      "startOffset" : 170,
      "endOffset" : 197
    }, {
      "referenceID" : 13,
      "context" : "Similarly, the task of discourse coherence modeling involves determining text relatedness at different levels of granularity (Grosz et al., 1995).",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "While these tasks involve understanding unstructured natural language, and there are entity-based approaches for solving them (Barzilay and Lapata, 2008), they differ from our task as they do not explicitly require the use of structured knowledge sources such as Freebase.",
      "startOffset" : 126,
      "endOffset" : 153
    }, {
      "referenceID" : 28,
      "context" : "Rare entity prediction is clearly distinct from tasks such as entity tagging and recognition (Ritter et al., 2011), as models are provided with the actual name of the entity in question, and only have to match the entity with related concepts and tags.",
      "startOffset" : 93,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "The original dataset was found to have a low ceiling for machine improvement (Chen et al., 2016); thus, alternative datasets have been proposed that consist of more difficult questions (Trischler et al.",
      "startOffset" : 77,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "While these tasks involve understanding unstructured natural language, and there are entity-based approaches for solving them (Barzilay and Lapata, 2008), they differ from our task as they do not explicitly require the use of structured knowledge sources such as Freebase. Events can be represented, for example, as relational atoms; for example, the event ‘X eats Y ’ could be represented as eat(X,Y ). Rare entity prediction is clearly distinct from tasks such as entity tagging and recognition (Ritter et al., 2011), as models are provided with the actual name of the entity in question, and only have to match the entity with related concepts and tags. It is more closely related to the machine reading literature from e.g. Etzioni et al. (2006); however, the authors define machine reading as primarily unsupervised, whereas our task is supervised.",
      "startOffset" : 127,
      "endOffset" : 750
    }, {
      "referenceID" : 1,
      "context" : "While these tasks involve understanding unstructured natural language, and there are entity-based approaches for solving them (Barzilay and Lapata, 2008), they differ from our task as they do not explicitly require the use of structured knowledge sources such as Freebase. Events can be represented, for example, as relational atoms; for example, the event ‘X eats Y ’ could be represented as eat(X,Y ). Rare entity prediction is clearly distinct from tasks such as entity tagging and recognition (Ritter et al., 2011), as models are provided with the actual name of the entity in question, and only have to match the entity with related concepts and tags. It is more closely related to the machine reading literature from e.g. Etzioni et al. (2006); however, the authors define machine reading as primarily unsupervised, whereas our task is supervised. A similar supervised reading comprehension task was proposed by Hermann et al. (2015) using news articles from CNN and the Daily Mail.",
      "startOffset" : 127,
      "endOffset" : 940
    }, {
      "referenceID" : 4,
      "context" : "Finally, methods that address factoid question answering (QA) using Freebase (Bordes et al., 2014a; Yao and Van Durme, 2014; Serban et al., 2016) must also combine knowledge from a structured knowledge base with an understanding of the question that’s being asked.",
      "startOffset" : 77,
      "endOffset" : 145
    }, {
      "referenceID" : 29,
      "context" : "Finally, methods that address factoid question answering (QA) using Freebase (Bordes et al., 2014a; Yao and Van Durme, 2014; Serban et al., 2016) must also combine knowledge from a structured knowledge base with an understanding of the question that’s being asked.",
      "startOffset" : 77,
      "endOffset" : 145
    }, {
      "referenceID" : 12,
      "context" : "A dataset with a similar task was also proposed by Hill et al. (2015a), where models must answer questions about short children’s stories.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "A dataset with a similar task was also proposed by Hill et al. (2015a), where models must answer questions about short children’s stories. While these tasks require the understanding of unstructured natural language, it does not require integration with external knowledge sources. Hill et al. (2015b) propose a method of combining distributional semantics with an external knowledge source in the form of dictionary definitions.",
      "startOffset" : 51,
      "endOffset" : 302
    }, {
      "referenceID" : 0,
      "context" : "Perhaps the most related approach to our work is the one concurrently developed by Ahn et al. (2016). The authors propose a WikiFacts dataset where Wikipedia descriptions are aligned with Freebase facts.",
      "startOffset" : 83,
      "endOffset" : 101
    }, {
      "referenceID" : 30,
      "context" : "1 The Wikilinks Dataset The Wikilinks dataset (Singh et al., 2012) is a large dataset originally designed for cross-document coreference resolution, the task of grouping entity mentions from a set of documents into clusters that represent a single entity.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 15,
      "context" : "Book dataset, which has 50k candidate entities for almost 700k context and query pairs (Hill et al., 2015a), the number of unique entities found in our dataset is in the same order of magnitude as the number of documents available.",
      "startOffset" : 87,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "representation, thus is ideal for modelling sequential data (Dietterich, 2002) and most language related problems.",
      "startOffset" : 60,
      "endOffset" : 78
    }, {
      "referenceID" : 18,
      "context" : "Reads and writes to the memory cell are controlled by a set of three gates that allow the model to either keep or discard information from the past and update their state with the current input (Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 194,
      "endOffset" : 228
    }, {
      "referenceID" : 17,
      "context" : "This allows LSTMs to model potentially longer dependencies and at the same time mitigate the vanishing and exploding gradient problems, which are quite common among regular RNNs (Hochreiter, 1991; Bengio et al., 1994).",
      "startOffset" : 178,
      "endOffset" : 217
    }, {
      "referenceID" : 2,
      "context" : "This allows LSTMs to model potentially longer dependencies and at the same time mitigate the vanishing and exploding gradient problems, which are quite common among regular RNNs (Hochreiter, 1991; Bengio et al., 1994).",
      "startOffset" : 178,
      "endOffset" : 217
    }, {
      "referenceID" : 12,
      "context" : "In our experiments, we use LSTMs augmented with peephole connections (Gers et al., 2002).",
      "startOffset" : 69,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "A similar prediction method has been used successfully for question answering (Bordes et al., 2014b; Yu et al., 2014) and dialogue response retrieval (Lowe et al.",
      "startOffset" : 78,
      "endOffset" : 117
    }, {
      "referenceID" : 34,
      "context" : "A similar prediction method has been used successfully for question answering (Bordes et al., 2014b; Yu et al., 2014) and dialogue response retrieval (Lowe et al.",
      "startOffset" : 78,
      "endOffset" : 117
    }, {
      "referenceID" : 22,
      "context" : ", 2014) and dialogue response retrieval (Lowe et al., 2015).",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 24,
      "context" : "In our experiments, we choose to use the published GloVe (Pennington et al., 2014) pre-trained word embeddings.",
      "startOffset" : 57,
      "endOffset" : 82
    }, {
      "referenceID" : 20,
      "context" : "All three models are trained with stochastic gradient descent with Adam (Kingma and Ba, 2014) as our optimizer, with a learning rate of 0.",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 30,
      "context" : "Note that, since Singh et al. (2012) manually verified the quality of the original Wikilinks dataset, there are virtually no entities that are incorrectly labeled, and thus this is not a significant source of error.",
      "startOffset" : 17,
      "endOffset" : 37
    } ],
    "year" : 2017,
    "abstractText" : "Reading comprehension in NLP refers to the ability of models to answer any question about a passage accurately. An important open problem is how to effectively use external knowledge to answer such questions. In this paper, we introduce a new task and derive new models to drive progress towards this goal. In particular, we propose the task of rare entity prediction: given a web document with several entities removed, models are tasked with predicting the correct missing entities conditioned on the document context and the lexical resources. This task is challenging due to the diversity of language styles and the extremely large number of rare entities. Our experiments show that models that make use of external knowledge in the form of lexical resources, particularly our model using hierarchical LSTMs, perform significantly better at rare entity prediction than those that do not.",
    "creator" : "LaTeX with hyperref package"
  }
}