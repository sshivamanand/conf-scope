{
  "name" : "56.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Ngram2vec: Learning Improved Word Representations from Ngram Co-occurrence Statistics",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recently, deep learning approaches have achieved state-of-the-art results on a range of NLP tasks. One of the most fundamental work in this field is word embedding, where low-dimensional word representations are learned from unlabeled corpus. The trained word embeddings reflect semantic and syntactic information of words. They are not only useful in tasks of lexical semantics (e.g. finding similar words), but also widely used as the input of the downstream tasks, such as text classification (Kim, 2014) and tagging (Collobert et al., 2011; Pennington et al., 2014).\nWord2vec is one of the most popular word embedding models (Mikolov et al., 2013b,a). It is trained upon <word, context> pairs in the local window. Word2vec gains the reputation by its amazing effectiveness and efficiency. It achieves state-of-the-art results on a range of linguistic tasks with only a fraction of time compared with\nprevious techniques. A challenger of word2vec is GloVe (Pennington et al., 2014). Instead of training on <word, context> pairs, GloVe directly uses word co-occurrence matrix. They claim that the change brings the improvement over word2vec on both accuracy and speed. Levy and Goldberg (2014b) further reveal that the attractive properties observed in word embeddings are not restricted to neural models. They use traditional bag-ofcontexts (concretely, PPMI matrix) to represent words, and achieve comparable results with the above neural embedding models.\nThe relationships among different representation methods are intricate. A preliminary conclusion is obtained in (Levy et al., 2015), which states that none of them consistently outperform the other methods. One should not feel surprised with the conclusion, because these methods all exploit word co-occurrence statistics as the information source and no one goes beyond that. To learn improved word representations, we extend the information source from co-occurrence of ‘wordword’ type to co-occurrence of ‘ngram-ngram’ type. The idea of using ngrams is well supported by language modeling, one of the oldest problems studied in statistical NLP. In language models, co-occurrence of words and ngrams is used to predict the next word (Kneser and Ney, 1995; Katz, 1987). Actually, the idea of word embedding models roots in language models. They are closely related but with different purposes. Word embedding models aim at learning word representations instead of word prediction. Since ngram is a vital part in language modeling, we are inspired to integrate ngram statistical information into recent word representation methods for better performance.\nThe idea of using ngram is intuitive. However, there is still rare work using ngrams in recent representation methods. In this paper, we in-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ntroduce ngrams into SGNS, GloVe, PPMI and its SVD factorization. To evaluate the ngram-based models, comprehensive experiments are conducted on word analogy and similarity tasks. The results demonstrate that the improved word representations are learned from ngram co-occurrence statistics. Besides that, we qualitatively evaluate the trained ngram representations. We show that they are able to reflect ngrams’ meanings and syntactic patterns (e.g. ‘be + past participle’ pattern). The high-quality ngram representations are useful in many ways. For example, ngrams in negative form (e.g. ‘not interesting’) can be used for finding antonyms (e.g. ‘boring’).\nFinally, a novel method is proposed to build ngram co-occurrence matrix. Our method reduces the disk I/O as much as possible, largely alleviating the costs brought by ngrams. We unify different representation methods in a pipeline. The source code is organized as ngram2vec toolkit and released at http://github.com/."
    }, {
      "heading" : "2 Related Work",
      "text" : "SGNS, GloVe, PPMI and its SVD factorization are used as baselines. The information used by them does not go beyond word co-occurrence statistics. However, their approaches to using the information are totally different. We review these methods in the following 3 sections. In section 2.4, we revisit the use of ngram in deep learning context."
    }, {
      "heading" : "2.1 SGNS",
      "text" : "Skip-gram with negative sampling (SGNS) is a model in the word2vec toolkit (Mikolov et al., 2013b,a). Its training procedure follows the majority of neural embedding models (Bengio et al., 2003): (1) Scan the corpus and use <word, context> pairs in the local window as training samples. (2) Train the models to make words useful for predicting contexts (or in reverse). The details of SGNS is discussed in Section 3.1. Compared to previous neural embedding models, SGNS speeds up the training process, reducing the training time from days or weeks to hours. Also, the trained embeddings possess valuable properties. They are able to reflect relations between the two words accurately, which is evaluated by a fancy task called word analogy.\nDue to the above advantages, many models are proposed on the basis of word2vec. For example, Faruqui et al. (2015) introduce knowledge in lexi-\ncal resources into the word2vec. Zhao et al. (2016) extend the contexts from the local window to the entire texts. Li et al. (2015) use supervised information to guide the training. Dependency parsetree is used for defining context in (Levy and Goldberg, 2014a). Sub-word information is considered in (Sun et al., 2016; Soricut and Och, 2015)."
    }, {
      "heading" : "2.2 GloVe",
      "text" : "Different from typical neural embedding models which are trained on <word, context> pairs, GloVe learns word representation on the basis of co-occurrence matrix (Pennington et al., 2014). GloVe breaks traditional ‘words predict contexts’ paradigm. Its objective is to reconstruct non-zero values in the matrix. The direct use of matrix is reported to bring more improved results and higher speed. However, There is still dispute about the advantages of GloVe over word2vec (Levy et al., 2015; Schnabel et al., 2015). GloVe and other embedding models are essentially based on word cooccurrence statistics of the corpus. The <word, context> pairs and matrix can be converted to each other. Suzuki and Nagata (2015) try to unify GloVe and SGNS in one framework."
    }, {
      "heading" : "2.3 PPMI & SVD",
      "text" : "When we are satisfied with the embeddings’ fancy properties, a natural question is raised: where the properties come from. One conjecture is that it’s due to the neural networks. However, Levy and Goldberg (2014c) reveal that SGNS is just factoring PMI matrix implicitly. The experiments also confirm this idea. Levy and Goldberg (2014b) show that positive PMI (PPMI) matrix still rivals the newly proposed embedding models on a range of linguistic tasks. Properties like word analogy are not restricted to neural models. At last, we factorize PPMI matrix via SVD factorization, a classic method for generating low-dimensional vectors from sparse matrix (Deerwester et al., 1990)."
    }, {
      "heading" : "2.4 Ngram in Deep Learning",
      "text" : "In the deep learning literature, ngram has shown to be useful in generating text representations. Recently, convolutional neural networks (CNNs) are reported to perform well on a range of NLP tasks (Blunsom et al., 2014; Hu et al., 2014; Severyn and Moschitti, 2015). CNNs are essentially using ngram information to represent texts. They use 1-D convolutional layers to extract ngram features and the distinct features are selected by max-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\npooling layers. In (Li et al., 2016), ngram embedding is introduced into Paragraph Vector model, where text embedding is trained to be useful to predict ngrams in the text. In the word embedding literature, a work that is related to ngram is from (Mikolov et al., 2013b), where phrases are embedded into vectors. It should be noted that phrases are different from ngrams. Phrases have clear semantics and the number of phrases is much less than the number of ngrams. Using phrase embedding has little impact on word embedding’s quality."
    }, {
      "heading" : "3 Model",
      "text" : "In this section, we introduce ngrams into SGNS, GloVe, PPMI and SVD. Section 3.1 reviews the SGNS. Section 3.2 and 3.3 show the details of introducing ngrams into SGNS. In section 3.4, we show the way of using ngrams in GloVe, PPMI and SVD, and propose a novel way of building ngram co-occurrence matrix."
    }, {
      "heading" : "3.1 Word Predicts Word: the Revisit of SGNS",
      "text" : "First we establish some notations. The raw input is the corpus T = {w1,w2,......,w|T |}. W and C denote the word and context vocabulary. θ is the parameters to be optimized. SGNS’s parameters involve two parts: word embedding matrix and context embedding matrix. With embedding ~w ∈ Rd, the total number of parameters is (|W|+|C|)*d.\nThe SGNS’s objective is to maximize the conditional probabilities of contexts given center words:\n|T |∑ t=1 [ ∑ c∈C(wt) log p(c|wt; θ) ] (1)\nwhere C(wt) = {wi, t− win ≤ i ≤ t+ win and i 6= t} and win denotes the window size. As illustrated in figure 1, the center word ‘written’ predicts its surrounding words ‘Potter’, ‘is’, ‘by’ and ‘J.K.’. In this paper, negative sampling (Mikolov et al., 2013b) is used to approximate the conditional probability:\np(c|w) = σ(~wT~c) k∏\nj=1\nE cj∼Pn(C)\nσ(−~wT ~cj) (2)\nwhere σ is sigmoid function. k samples (from c1 to ck) are drawn from context distribution raised to the n power.\nFigure 1: Illustration of ‘word predicts word’."
    }, {
      "heading" : "3.2 Word Predicts Ngram",
      "text" : "In this section, we introduce ngrams into context vocabulary. We treat each ngram as a normal word and give it a unique embedding. During the training, the center word should not only predict its surrounding words, but also predict its surrounding ngrams. As shown in figure 2, center word ‘written’ predicts the bi-grams in the local window such as ‘by J.K.’. The objective of ‘word predicts ngram’ is similar with original SGNS. The only difference is the definition of the C(w). In ngram case, C(w) is formally defined as follows:\nC(wt) = N⋃\nn=1\n{wi:i+n|wi:i+n is not wt AND\nt− win ≤ i ≤ t+ win− n+ 1} (3)\nwhere wi:i+n denotes the ngram wiwi+1...wi+n−1 and N is the order of context ngram. Two points need to be noticed from the above definition. The first is how to determine the distance between word and ngram. In this paper, we use the distance between word and ngram’s far-end word. As show in figure 2, the distance between ‘written’ and ‘Harry Potter’ is 3. As a result, ‘Harry Potter’ is not included in the center word’s context. This distance definition ensures that the ngram models don’t use the information beyond the pre-specified window, which guarantees the fair comparisons with the baselines. Another point is whether the overlap of word and ngram is allowed or not. In the overlap situation, ngrams are used as context even they contain the center word. As the example in figure 2 shows, ngram ‘is written’ and ‘written by’ are predicted by the center word ‘written’. In the non-overlap case, these ngrams are excluded. The properties of word embeddings are different when overlap is allowed or not, which is discussed in experiments section."
    }, {
      "heading" : "3.3 Ngram Predicts Ngram",
      "text" : "We further extend the model to introduce ngrams into center word vocabulary. During the training, center ngrams (including words) predict their\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nFigure 2: Illustration of ‘word predicts ngram’.\nFigure 3: Illustration of ‘ngram predicts ngram’.\nsurrounding ngrams. As shown in figure 3, center bi-gram ‘is written’ predicts its surrounding words and bi-grams. The objective of ‘ngram predicts ngram’ is as follows:\n|T |∑ t=1 Nw∑ nw=1 [ ∑ c∈C(wt:t+nw ) log p(c|wt:t+nw ; θ) ] (4)\nwhere Nw is the order of center ngram. The definition of C(wt:t+nw) is as follows:\nNc⋃ nc=1 {wi:i+nc |wi:i+nc is not wt:t+nw AND\nt− win+ nw − 1 ≤ i ≤ t+ win− nc + 1} (5)\nwhere Nc is the order of context ngram. To this end, the word embeddings are not only affected by the ngrams in the context, but also indirectly affected by co-occurrence statistics of ‘ngramngram’ type in the corpus.\nSGNS is proven to be equivalent with factorizing pointwise mutual information (PMI) matrix (Levy and Goldberg, 2014c). Following their work, we can easily show that models in section 3.2 and 3.3 are implicitly factoring PMI of ‘wordngram’ and ‘ngram-ngram’ type. They are just the representation method discussed in the following section, where ngrams are introduced into positive PMI (PPMI) matrix."
    }, {
      "heading" : "3.4 Co-occurrence Matrix Construction",
      "text" : "Introducing ngrams into GloVe, PPMI and SVD is straightforward: the only change is to replace word co-occurrence matrices with ngram ones. In the above three sections, we have discussed the\nway of taking out <ngram, context> pairs from the corpus. Afterwards, we build co-occurrence matrix upon these pairs. The rest steps are identical with the original baseline models.\nHowever, building co-occurrence matrix is not an easy task as it apparently looks like. The introduction of ngrams brings huge burdens on the hardware. The matrix construction cost is closely related to the number of pairs (#Pairs). Table 1 shows the detailed statistics of corpus wiki2010 1. We can observe that #Pairs is huge when bi-grams are considered.\nTo speed up the process of building ngram co-occurrence matrix, we take advantages of ‘mixture’ strategy (Pennington et al., 2014) and ‘stripes’ strategy (Dyer et al., 2008; Lin, 2008). The two strategies optimize the process in different aspects. Computational cost is reduced significantly when they are used together.\nWhen the words (or ngrams) are sorted in descending order by frequency, the matrix’s topleft corner is dense while the rest part is sparse. Based on this observation, the ‘mixture’ of two data structures are used for storing matrix. Elements in the top-left corner are stored in a 2D array, which stays in memory. The rest of the elements are stored in the form of <ngram, H> (‘stripes’ strategy), where H<context, count> is an associative array recording the times the ngram and context co-occurs. Compared with storing <ngram, context> pairs explicitly, the ‘stripes’ strategy provides more opportunities to aggregate pairs outside of the top-left corner.\nAlgorithm 1 shows the way of using the ‘mixture’ and ‘stripes’ strategies together. In the first\n1http://nlp.stanford.edu/data/WestburyLab.wikicorp.201004.txt.bz2\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nstage, pairs are stored in different data structures according to topLeft function. Intermediate results are written to temporary files when memory is full. In the second stage, we merge these sorted temporary files to generate co-occurrence matrix. The getSmallest function takes out the pair <ngram, H> with the smallest key from temporary files. In practice, algorithm 1 is efficient. Instead of using computer clusters (Lin, 2008), we can build the matrix of ‘bi bi’ type even in a laptop. It only requires 12GB to store temporary files (win=2, subsampling=0, memory size=4GB), which is much smaller than the implementations in (Pennington et al., 2014; Levy et al., 2015) . More detailed analysis about these strategies can be found in the ngram2vec toolkit and the attachment along with our paper 2.\nAlgorithm 1: An algorithm for building ngram co-occurrence matrix\nInput : Pairs P , Sorted vocabulary V Output: Sorted and aggregated pairs\n1 The 2D array A[ ][ ]; 2 The dictionary D < ngram,H >; 3 The temporary files array tfs[ ]; fid=1; 4 for pair p < n, c > in P do 5 if topLeft(n, c) == 1 then 6 A[getId(n)][getId(c)] += 1; 7 else 8 D{n}{c} += 1; 9 if Memory is full or P is empty then\n10 Sort D by key (ngram); 11 Write D to tfs[fid]; 12 fid += 1; 13 end 14 end 15 end 16 Write A to tfs[0] in the form of < ngram,H >; 17 old = getSmallest(tfs) ; 18 while !(All files in tfs are empty) do 19 new = getSmallest(tfs) ; 20 if old.ngram == new.ngram then 21 old = < old.ngram,merge(old.H, new.H) >; 22 else 23 Write old to disk; 24 old = new 25 end 26 end"
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "The datasets used in this paper is the same with (Levy et al., 2015), including six similarity and two analogy datasets. In similarity task, a scalar\n2We upload the implementation of algorithm 1 along with the paper\n(e.g. a score from 0 to 10) is used to measure the relation between the two words. The scalar only reflects the strength of the relation, not the type. For example, the ‘train, car’ pair is given the score of 6.31. In our view, relations are too complex to be reflected by a single score. Suppose that when one is asked to describe the relations between ‘Athens’ and ‘Greece’, rather than giving a score, a reasonable answer is probably ‘they are both place names and Athens is the capital of Greece’. Besides that, we have to compare pairs with different types in similarity task. Considering the two pairs: ‘train, car’ and ‘movie, star’. It is hard to say which pair has closer relations (higher score) (Schnabel et al., 2015).\nIn analogy task, relations between the two words are reflected by a vector, which is usually obtained by the difference between word embeddings. Different from scalar, the vector provides more accurate descriptions of relations. For example, capital-country relation is encoded in vec(Athens)-vec(Greece), vec(Tokyo)vec(Japan) and so on. More concretely, the questions in the analogy task are in the form of ‘a is to b as c is to d’. ‘d’ is an unknown word in the test phase. To correctly answer the questions, the models should embed the two relations, vec(a)vec(b) and vec(c)-vec(d), into similar positions in the space. Following (Levy and Goldberg, 2014b), both additive (add) and multiplicative (mul) functions are used for finding word ‘d’. The latter one is more suitable for sparse representation."
    }, {
      "heading" : "4.2 Pipeline and Hyper-parameter Setting",
      "text" : "We implement SGNS, GloVe, PPMI and SVD in a pipeline, allowing the reuse of code and intermediate results. Figure 4 illustrates the overview of the pipeline. Firstly, <ngram, context> pairs are extracted from the corpus as the input of SGNS. Afterwards, we build the co-occurrence ma-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nWin Type Google Tot. / Sem. / Syn. MSRAdd Mul Add Mul\n2\nuni uni .488 / .425 / .540 .513 / .457 / .559 .477 .498 overlap uni bi .579 / .640 / .528 .610 / .670 / .560 .465 .496bi bi .645 / .729 / .576 .665 / .739 / .603 .527 .550 non-overlap uni bi .589 / .606 / .575 .610 / .625 / .598 .469 .505bi bi .651 / .708 / .604 .682 / .725 / .646 .525 .557\n5\nuni uni .587 / .591 / .585 .607 / .611 / .604 .445 .463 overlap uni bi .635 / .697 / .583 .638 / .696 / .590 .448 .420bi bi .649 / .711 / .598 .660 / .714 / .616 .463 .480 non-overlap uni bi .653 / .666 / .643 .670 / .676 / .665 .491 .516bi bi .661 / .687 / .639 .671 / .690 / .656 .512 .536\nTable 2: Performance of (ngram) SGNS on analogy datasets.\nWin Type Sim. Rel. Bruni Radinsky Luong Hill\n2 uni uni .707 .565 .696 .585 .450 .405 uni bi .714 .568 .688 .616 .471 .428 bi bi .735 .594 .698 .644 .468 .413 5 uni uni .739 .637 .771 .651 .457 .396 uni bi .763 .624 .773 .663 .471 .416 bi bi .778 .647 .767 .664 .473 .413\nTable 3: Performance of (ngram) SGNS on similarity datasets.\ntrix upon the pairs. GloVe and PPMI learn word representations on the basis of co-occurrence matrix. SVD factorizes the PPMI matrix to obtain low-dimensional representation.\nMost hyper-parameters come from ‘corpus2pairs’ part and four representation models. ‘corpus2pairs’ part determines the source of information for the subsequent models. We basically follow the default setting in (Levy et al., 2015): low-frequency words (ngrams) are removed with a threshold of 100. High-frequency words (ngrams) are removed with sub-sampling at the degree of 1e-5 3. Window size is set to 2 and 5. Clean strategy (Levy et al., 2015) is used to ensure no information beyond pre-specified window is included. Non-overlap setting is used in default since it produces much less pairs. For hyper-parameters in the models, we use the embeddings of 300 dimensions. The rest strictly follow the baseline models 4.\nWe only consider uni-grams (words) and bigrams in our experiments. Bi-grams are introduced into context (uni bi) or both context and center ‘word’ (bi bi). We suspect that the higherorder grams are so sparse that may deteriorate the performance. The implementation of higher-order\n3Sub-sampling is not used in GloVe, which follows its original setting.\n4http://bitbucket.org/omerlevy/ hyperwords for SGNS, PPMI and SVD; http://nlp.stanford.edu/projects/glove/ for GloVe.\nmodels and their results will be released with ngram2vec toolkit."
    }, {
      "heading" : "4.3 Ngrams on SGNS",
      "text" : "SGNS is a highly popular word embedding model. Even compared with its challengers such as GloVe, SGNS is reported to have more robust performance with faster training speed. Table 2 lists the results on analogy datasets. We can observe that the introduction of bi-grams gives the huge promotions at different hyper-parameter settings. The SGNS of ‘bi bi’ type provides the highest results. Bi-grams are very effective for capturing semantic information. Around 20 and 10 percent improvements are witnessed on semantic questions at window sizes of 2 and 5. For syntactic questions, the improvements are still significant. Around 5 percent improvements are achieved on Google (syntactic) and MSR datasets. In traditional language models, ngram is the vital part for predicting next words. Our results confirm the effectiveness of ngrams again on recent word embedding models and more advanced analogy task.\nThe effect of overlap is large on analogy datasets. Semantic questions prefer the overlap setting. Around 3 percent increase is witnessed compared with non-overlap setting. While in syntactic case, non-overlap setting performs better by a margin of around 5 percent.\nTable 3 illustrates the SGNS’s performance on similarity task. The use of bi-grams is effective on\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nWin Type Google MSR Sim. Rel. Bruni Radinsky Luong HillAdd Mul Add Mul\n2 uni uni .488 / .571 / .418 .626 / .739 / .533 .276 .441 .681 .559 .670 .621 .404 .366uni bi .464 / .580 / .367 .733 / .833 / .649 .286 .553 .673 .532 .681 .635 .401 .377 5 uni uni .508 / .635 / .403 .588 / .757 / .447 .238 .332 .732 .680 .722 .626 .421 .329uni bi .509 / .682 / .365 .650 / .819 / .508 .239 .416 .750 .695 .708 .658 .411 .371\nTable 4: Performance of (ngram) PPMI on analogy and similarity datasets.\nWin Type Google MSR Sim. Rel. Bruni Radinsky Luong HillAdd Mul Add Mul\n2 uni uni .535 / .599 / .482 .540 / .610 / .481 .444 .445 .681 .529 .698 .608 .381 .351uni bi .543 / .601 / .493 .549 / .612 / .496 .464 .472 .686 .545 .695 .631 .389 .352 5 uni uni .625 / .689 / .572 .626 / .696 / .568 .476 .490 .747 .600 .735 .657 .389 .347uni bi .631 / .699 / .575 .633 / .703 / .574 .477 .504 .752 .610 .737 .631 .395 .342\nTable 5: Performance of (ngram) GloVe on analogy and similarity datasets.\nWin Type Google MSR Sim. Rel. Bruni Radinsky Luong HillAdd Mul Add Mul\n2 uni uni .429 / .385 / .465 .448 / .392 / .495 .345 .375 .729 .611 .732 .637 .499 .369uni bi .410 / .338 / .471 .446 / .364 / .514 .370 .411 .731 .608 .721 .637 .513 .378 5 uni uni .436 / .415 / .454 .461 / .438 / .481 .328 .332 .744 .629 .756 .634 .501 .337uni bi .486 / .425 / .536 .514 / .460 / .559 .349 .391 .761 .642 .751 .626 .522 .357\nTable 6: Performance of (ngram) SVD on analogy and similarity datasets.\nmost cases. However, the improvements brought by bi-grams are not as big as the case in analogy task. The SGNS of ‘uni bi’ type outperforms the ‘bi bi’ type on most datasets."
    }, {
      "heading" : "4.4 Ngrams on GloVe, PPMI, SVD",
      "text" : "In this section, we only report the results of models of ‘uni uni’ and ‘uni bi’ types. Using ngram-ngram co-occurrence statistics bring little improvements but immense costs. Levy and Goldberg (2014b) demonstrate that sparse representation (PPMI matrix) can still achieve competitive results on many linguistic tasks, challenging the dominance of neural embedding models. Table 4 lists the results of PPMI. PPMI prefers Multiplicative (Mul) evalution. To this end, we focus on analyzing the results on Mul columns. When bi-grams are used, significant improvements are witnessed on analogy task. On Google dataset, bi-grams bring around 8 percent increase on the total accuracies. At window size 2, the accuracies even reach 73.3/83.3/64.9. They are pretty high numbers considering only default setting is used. On MSR dataset, around 10 percent improvements are achieved. Though bi-grams are very effective on analogy task, the improvements on similarity task are not witnessed. The PPMI of ‘uni bi’ type improves the results of 3 in 6 datasets.\nTable 5 and 6 list the GloVe and SVD’s results. In GloVe, consistent but minor improvements are achieved on analogy task. Little improvement is witnessed on similarity task. In SVD, bi-grams\nsometimes lead to worse results in both analogy and similarity tasks. That’s not what we expected before the experiments. We thought ngram would be very effective in GloVe and SVD just like the cases in SGNS, since these models use exactly the same source of information. Our preliminary conjecture is that the default hyper-parameter setting should be blamed. We strictly follow the hyper-parameters used in baseline models, making no adjustments to cater to the introduction of ngrams. Besides that, some common techniques such as dynamic window, decreasing weighting function, dirty sub-sampling are discarded. The relationships between ngrams and various hyperparameters require further exploration. Though trivial, it may lead to much better results and give researchers better understanding of different representation methods. That will be the focus of our future work."
    }, {
      "heading" : "4.5 Qualitative Evaluations of Ngram Embedding",
      "text" : "In this section, we analyze the properties of ngram embeddings trained on SGNS of ‘bi bi’ type. Ideally, the trained ngram embeddings should reflect ngrams’ semantic meanings. For example, the vec(wasn’t able) should be closed to vec(unable). The vec(is written) should be closed to vec(write) and vec(book). Also, the trained ngram embeddings should preserve the ngrams’ syntactic patterns. For example, ‘was written’ is in the form of ‘be + past participle’ and the nearest neighbors\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nPattern Target Word Bi-gram\nNegative Form wasn’t able unable(.745), couldn’t(.723), didn’t(.680) was unable(.832), didn’t manage(.799), never managed(.786) don’t need don’t(.773), dont(.751), needn’t(.715) dont need(.790), don’t have(.785), dont want(.769) not enough enough(.708), insufficient(.701), sufficient(.629) not sufficient(.750), wasn’t enough(.729), was insufficient(0.685) Adj. Modifier heavy rain torrential(.844), downpours(.780), rain(.766) torrential rain(.829), heavy rainfall(.799), heavy snow(.799)\nstrong supporter supporter(.828), proponent(.733), admirer(.602) staunch supporter(.870), vocal supporter(.810), a supporter(.808) high quality high-quality(.867), quality(.744), inexpensive(.622) good quality(.813), top quality(.751), superior quality(.736)\nPassive Voice was written written(.793), penned(.675), co-written(.629) were written(.785), is written(.744), written by(.739)\nwas sent sent(.844), dispatched(.661), went(.630) then sent(.779), later sent(.776), was dispatched(.774) was pulled pulled(.730), yanked(.629), limped(.593) were pulled(.706), pulled from(.691), was ripped(.682)\nPerfect Tense has achieved achieved(.683), achieves(.680), achieving(.625) has attained(.775), has enjoyed(.741), has gained(.733) has impacted interconnectedness(.679), pervade(.676) have impacted(.838), is affecting(.773), have shaped(.772) has published authored(.722), publishes(.705), coauthored(.791) has authored(.852), has edited(.795), has written(.791) Phrasal Verb give off exude(.796), fluoresce(.789), emit(.754) gave off(.837), giving off(.820), and emit(.816)\nmake up comprise(.726), constitute(.616), make(.541) makes up(.705), making up(.702), comprise the(.672) picked up picked(.870), snagged(.544), scooped(.538) later picked(.712), and picked(.682), then picked(.681)\nCommon Sense highest mountain muztagh(.669), prokletije(.664), cadair(.658) highest peak(.873), tallest mountain(.857), highest summit(.830) avian influenza h5n1(.870), zoonotic(.812), adenovirus(.806) avian flu(.885), the h5n1(.870), flu virus(.868) computer vision human-computer(.789), holography(.767) image processing(.850), object recognition(.818)\nTable 7: Target bi-grams and their nearest neighbours associated with similarity scores.\nshould possess similar patterns, such as ‘is written’ and ‘was transcribed’.\nTable 7 lists the target ngrams and their top nearest neighbours. We divide the target ngrams into six groups according to their patterns. We can observe that the returned words and ngrams are very intuitive. As might be expected, synonyms of the target ngrams are returned in top positions (e.g. ‘give off’ and ‘emit’; ‘heavy rain’ and ‘downpours’). Besides that, from the results of the first group, it can be observed that bi-gram in negative form ‘not X’ is useful for finding the antonym of word ‘X’. The trained n-gram embeddings also preserve some common sense. For example, the returned results of ‘highest mountain’ is a list of mountain names (with a few exceptions such as ‘unclimbed’). In terms of syntactic patterns, we can observe that in most cases, the returned ngrams are in the similar form of target ngrams. In general, the trained embeddings basically reflect the semantic meanings and syntactic patterns of ngrams.\nWith high-quality ngram embeddings, we have the opportunity to do more interesting things in our future work. For example, we will construct a antonym dataset to evaluate ngram embeddings systematically. Besides that, we will find more scenarios for using ngram embeddings. In our view, ngram embeddings may be useful in many NLP tasks. For example, Johnson and Zhang (2015) use one-hot ngram representation as the input of CNN. Li et al. (2016) use the ngram embeddings to represent texts. Intuitively, initializing these models with pre-trained ngram embeddings may further\nimprove the accuracies."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We introduce ngrams into four representation methods. The experimental results demonstrate ngrams’ effectiveness for learning improved word representations. In addition, we find that the trained ngram embeddings are able to reflect their semantic meanings and syntactic patterns. To alleviate the costs brought by ngrams, we propose a novel way of building co-occurrence matrix, enabling the ngram-based models to run on cheap hardware."
    } ],
    "references" : [ {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin." ],
      "venue" : "Journal of Machine Learning Research 3:1137–1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Phil Blunsom", "Edward Grefenstette", "Nal Kalchbrenner." ],
      "venue" : "Proceedings of ACL 2014.",
      "citeRegEx" : "Blunsom et al\\.,? 2014",
      "shortCiteRegEx" : "Blunsom et al\\.",
      "year" : 2014
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa." ],
      "venue" : "Journal of Machine Learning Research 12:2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Indexing by latent semantic analysis",
      "author" : [ "Scott C. Deerwester", "Susan T. Dumais", "Thomas K. Landauer", "George W. Furnas", "Richard A. Harshman." ],
      "venue" : "JASIS 41(6):391–407.",
      "citeRegEx" : "Deerwester et al\\.,? 1990",
      "shortCiteRegEx" : "Deerwester et al\\.",
      "year" : 1990
    }, {
      "title" : "Fast, easy, and cheap: Construction of statistical machine translation models with",
      "author" : [ "Christopher Dyer", "Aaron Cordova", "Alex Mont", "Jimmy Lin" ],
      "venue" : null,
      "citeRegEx" : "Dyer et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2008
    }, {
      "title" : "Retrofitting word vectors to semantic lexicons",
      "author" : [ "Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard H. Hovy", "Noah A. Smith." ],
      "venue" : "Proceedings of NAACL 2015. pages 1606– 1615.",
      "citeRegEx" : "Faruqui et al\\.,? 2015",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2015
    }, {
      "title" : "Convolutional neural network architectures for matching natural language sentences",
      "author" : [ "Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen." ],
      "venue" : "Proceedings of NIPS 2014. pages 2042–2050.",
      "citeRegEx" : "Hu et al\\.,? 2014",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2014
    }, {
      "title" : "Effective use of word order for text categorization with convolutional neural networks",
      "author" : [ "Rie Johnson", "Tong Zhang." ],
      "venue" : "Proceedings of NAACL 2015. pages 103–112.",
      "citeRegEx" : "Johnson and Zhang.,? 2015",
      "shortCiteRegEx" : "Johnson and Zhang.",
      "year" : 2015
    }, {
      "title" : "Estimation of probabilities from sparse data for the language model component of a speech recognizer",
      "author" : [ "Slava M. Katz." ],
      "venue" : "IEEE Trans. Acoustics, Speech, and Signal Processing 35(3):400–401.",
      "citeRegEx" : "Katz.,? 1987",
      "shortCiteRegEx" : "Katz.",
      "year" : 1987
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "Proceedings of EMNLP 2014. pages 1746–1751.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Improved backing-off for m-gram language modeling",
      "author" : [ "Reinhard Kneser", "Hermann Ney." ],
      "venue" : "Proceedings of ICASSP 1995. pages 181–184.",
      "citeRegEx" : "Kneser and Ney.,? 1995",
      "shortCiteRegEx" : "Kneser and Ney.",
      "year" : 1995
    }, {
      "title" : "Dependencybased word embeddings",
      "author" : [ "Omer Levy", "Yoav Goldberg." ],
      "venue" : "ACL (2). pages 302– 308.",
      "citeRegEx" : "Levy and Goldberg.,? 2014a",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Linguistic regularities in sparse and explicit word representations",
      "author" : [ "Omer Levy", "Yoav Goldberg." ],
      "venue" : "Proceedings of CoNLL 2014. pages 171–180.",
      "citeRegEx" : "Levy and Goldberg.,? 2014b",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Neural word embedding as implicit matrix factorization",
      "author" : [ "Omer Levy", "Yoav Goldberg." ],
      "venue" : "Proceedings of NIPS 2014. pages 2177–2185.",
      "citeRegEx" : "Levy and Goldberg.,? 2014c",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Improving distributional similarity with lessons learned from word embeddings",
      "author" : [ "Omer Levy", "Yoav Goldberg", "Ido Dagan." ],
      "venue" : "TACL 3:211–225.",
      "citeRegEx" : "Levy et al\\.,? 2015",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2015
    }, {
      "title" : "Weighted neural bag-of-n-grams model: New baselines for text classification",
      "author" : [ "Bofang Li", "Zhe Zhao", "Tao Liu", "Puwei Wang", "Xiaoyong Du." ],
      "venue" : "Proceedings of COLING 2016. pages 1591–1600.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Word embedding revisited: A new representation learning and explicit matrix factorization perspective",
      "author" : [ "Yitan Li", "Linli Xu", "Fei Tian", "Liang Jiang", "Xiaowei Zhong", "Enhong Chen." ],
      "venue" : "Proceedings of IJCAI 2015. pages 3650–3656.",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Scalable language processing algorithms for the masses: A case study in computing word co-occurrence matrices with mapreduce",
      "author" : [ "Jimmy J. Lin." ],
      "venue" : "Proceedings of EMNLP 2008. pages 419–428.",
      "citeRegEx" : "Lin.,? 2008",
      "shortCiteRegEx" : "Lin.",
      "year" : 2008
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "CoRR abs/1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of NIPS 2013. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proceedings of EMNLP 2014. pages 1532–1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Evaluation methods for unsupervised word embeddings",
      "author" : [ "Tobias Schnabel", "Igor Labutov", "David M. Mimno", "Thorsten Joachims." ],
      "venue" : "Proceedings of EMNLP 2015. pages 298–307.",
      "citeRegEx" : "Schnabel et al\\.,? 2015",
      "shortCiteRegEx" : "Schnabel et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to rank short text pairs with convolutional deep neural networks",
      "author" : [ "Aliaksei Severyn", "Alessandro Moschitti." ],
      "venue" : "Proceedings of SIGIR 2015. pages 373–382.",
      "citeRegEx" : "Severyn and Moschitti.,? 2015",
      "shortCiteRegEx" : "Severyn and Moschitti.",
      "year" : 2015
    }, {
      "title" : "Unsupervised morphology induction using word embeddings",
      "author" : [ "Radu Soricut", "Franz Josef Och." ],
      "venue" : "Proceedings of NAACL 2015. pages 1627– 1637.",
      "citeRegEx" : "Soricut and Och.,? 2015",
      "shortCiteRegEx" : "Soricut and Och.",
      "year" : 2015
    }, {
      "title" : "Inside out: Two jointly predictive models for word representations and phrase representations",
      "author" : [ "Fei Sun", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Xueqi Cheng." ],
      "venue" : "Proceedings of AAAI 2016. pages 2821–2827.",
      "citeRegEx" : "Sun et al\\.,? 2016",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2016
    }, {
      "title" : "A unified learning framework of skip-grams and global vectors",
      "author" : [ "Jun Suzuki", "Masaaki Nagata." ],
      "venue" : "Proceedings of ACL 2015, Volume 2: Short Papers. pages 186–191.",
      "citeRegEx" : "Suzuki and Nagata.,? 2015",
      "shortCiteRegEx" : "Suzuki and Nagata.",
      "year" : 2015
    }, {
      "title" : "Cluster-driven model for improved word and text embedding",
      "author" : [ "Zhe Zhao", "Tao Liu", "Bofang Li", "Xiaoyong Du." ],
      "venue" : "Proceedings of ECAI 2016. pages 99–106.",
      "citeRegEx" : "Zhao et al\\.,? 2016",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "finding similar words), but also widely used as the input of the downstream tasks, such as text classification (Kim, 2014) and tagging (Collobert et al.",
      "startOffset" : 111,
      "endOffset" : 122
    }, {
      "referenceID" : 2,
      "context" : "finding similar words), but also widely used as the input of the downstream tasks, such as text classification (Kim, 2014) and tagging (Collobert et al., 2011; Pennington et al., 2014).",
      "startOffset" : 135,
      "endOffset" : 184
    }, {
      "referenceID" : 20,
      "context" : "finding similar words), but also widely used as the input of the downstream tasks, such as text classification (Kim, 2014) and tagging (Collobert et al., 2011; Pennington et al., 2014).",
      "startOffset" : 135,
      "endOffset" : 184
    }, {
      "referenceID" : 20,
      "context" : "A challenger of word2vec is GloVe (Pennington et al., 2014).",
      "startOffset" : 34,
      "endOffset" : 59
    }, {
      "referenceID" : 14,
      "context" : "A preliminary conclusion is obtained in (Levy et al., 2015), which states that none of them consistently outperform the other methods.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "In language models, co-occurrence of words and ngrams is used to predict the next word (Kneser and Ney, 1995; Katz, 1987).",
      "startOffset" : 87,
      "endOffset" : 121
    }, {
      "referenceID" : 8,
      "context" : "In language models, co-occurrence of words and ngrams is used to predict the next word (Kneser and Ney, 1995; Katz, 1987).",
      "startOffset" : 87,
      "endOffset" : 121
    }, {
      "referenceID" : 2,
      "context" : "finding similar words), but also widely used as the input of the downstream tasks, such as text classification (Kim, 2014) and tagging (Collobert et al., 2011; Pennington et al., 2014). Word2vec is one of the most popular word embedding models (Mikolov et al., 2013b,a). It is trained upon <word, context> pairs in the local window. Word2vec gains the reputation by its amazing effectiveness and efficiency. It achieves state-of-the-art results on a range of linguistic tasks with only a fraction of time compared with previous techniques. A challenger of word2vec is GloVe (Pennington et al., 2014). Instead of training on <word, context> pairs, GloVe directly uses word co-occurrence matrix. They claim that the change brings the improvement over word2vec on both accuracy and speed. Levy and Goldberg (2014b) further reveal that the attractive properties observed in word embeddings are not restricted to neural models.",
      "startOffset" : 136,
      "endOffset" : 812
    }, {
      "referenceID" : 0,
      "context" : "Its training procedure follows the majority of neural embedding models (Bengio et al., 2003): (1) Scan the corpus and use <word, context> pairs in the local window as training samples.",
      "startOffset" : 71,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : "Dependency parsetree is used for defining context in (Levy and Goldberg, 2014a).",
      "startOffset" : 53,
      "endOffset" : 79
    }, {
      "referenceID" : 24,
      "context" : "Sub-word information is considered in (Sun et al., 2016; Soricut and Och, 2015).",
      "startOffset" : 38,
      "endOffset" : 79
    }, {
      "referenceID" : 23,
      "context" : "Sub-word information is considered in (Sun et al., 2016; Soricut and Och, 2015).",
      "startOffset" : 38,
      "endOffset" : 79
    }, {
      "referenceID" : 0,
      "context" : "Its training procedure follows the majority of neural embedding models (Bengio et al., 2003): (1) Scan the corpus and use <word, context> pairs in the local window as training samples. (2) Train the models to make words useful for predicting contexts (or in reverse). The details of SGNS is discussed in Section 3.1. Compared to previous neural embedding models, SGNS speeds up the training process, reducing the training time from days or weeks to hours. Also, the trained embeddings possess valuable properties. They are able to reflect relations between the two words accurately, which is evaluated by a fancy task called word analogy. Due to the above advantages, many models are proposed on the basis of word2vec. For example, Faruqui et al. (2015) introduce knowledge in lexical resources into the word2vec.",
      "startOffset" : 72,
      "endOffset" : 754
    }, {
      "referenceID" : 0,
      "context" : "Its training procedure follows the majority of neural embedding models (Bengio et al., 2003): (1) Scan the corpus and use <word, context> pairs in the local window as training samples. (2) Train the models to make words useful for predicting contexts (or in reverse). The details of SGNS is discussed in Section 3.1. Compared to previous neural embedding models, SGNS speeds up the training process, reducing the training time from days or weeks to hours. Also, the trained embeddings possess valuable properties. They are able to reflect relations between the two words accurately, which is evaluated by a fancy task called word analogy. Due to the above advantages, many models are proposed on the basis of word2vec. For example, Faruqui et al. (2015) introduce knowledge in lexical resources into the word2vec. Zhao et al. (2016) extend the contexts from the local window to the entire texts.",
      "startOffset" : 72,
      "endOffset" : 833
    }, {
      "referenceID" : 0,
      "context" : "Its training procedure follows the majority of neural embedding models (Bengio et al., 2003): (1) Scan the corpus and use <word, context> pairs in the local window as training samples. (2) Train the models to make words useful for predicting contexts (or in reverse). The details of SGNS is discussed in Section 3.1. Compared to previous neural embedding models, SGNS speeds up the training process, reducing the training time from days or weeks to hours. Also, the trained embeddings possess valuable properties. They are able to reflect relations between the two words accurately, which is evaluated by a fancy task called word analogy. Due to the above advantages, many models are proposed on the basis of word2vec. For example, Faruqui et al. (2015) introduce knowledge in lexical resources into the word2vec. Zhao et al. (2016) extend the contexts from the local window to the entire texts. Li et al. (2015) use supervised information to guide the training.",
      "startOffset" : 72,
      "endOffset" : 913
    }, {
      "referenceID" : 20,
      "context" : "2 GloVe Different from typical neural embedding models which are trained on <word, context> pairs, GloVe learns word representation on the basis of co-occurrence matrix (Pennington et al., 2014).",
      "startOffset" : 169,
      "endOffset" : 194
    }, {
      "referenceID" : 14,
      "context" : "However, There is still dispute about the advantages of GloVe over word2vec (Levy et al., 2015; Schnabel et al., 2015).",
      "startOffset" : 76,
      "endOffset" : 118
    }, {
      "referenceID" : 21,
      "context" : "However, There is still dispute about the advantages of GloVe over word2vec (Levy et al., 2015; Schnabel et al., 2015).",
      "startOffset" : 76,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "However, There is still dispute about the advantages of GloVe over word2vec (Levy et al., 2015; Schnabel et al., 2015). GloVe and other embedding models are essentially based on word cooccurrence statistics of the corpus. The <word, context> pairs and matrix can be converted to each other. Suzuki and Nagata (2015) try to unify GloVe and SGNS in one framework.",
      "startOffset" : 77,
      "endOffset" : 316
    }, {
      "referenceID" : 3,
      "context" : "At last, we factorize PPMI matrix via SVD factorization, a classic method for generating low-dimensional vectors from sparse matrix (Deerwester et al., 1990).",
      "startOffset" : 132,
      "endOffset" : 157
    }, {
      "referenceID" : 10,
      "context" : "However, Levy and Goldberg (2014c) reveal that SGNS is just factoring PMI matrix implicitly.",
      "startOffset" : 9,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "However, Levy and Goldberg (2014c) reveal that SGNS is just factoring PMI matrix implicitly. The experiments also confirm this idea. Levy and Goldberg (2014b) show that positive PMI (PPMI) matrix still rivals the newly proposed embedding models on a range of linguistic tasks.",
      "startOffset" : 9,
      "endOffset" : 159
    }, {
      "referenceID" : 1,
      "context" : "Recently, convolutional neural networks (CNNs) are reported to perform well on a range of NLP tasks (Blunsom et al., 2014; Hu et al., 2014; Severyn and Moschitti, 2015).",
      "startOffset" : 100,
      "endOffset" : 168
    }, {
      "referenceID" : 6,
      "context" : "Recently, convolutional neural networks (CNNs) are reported to perform well on a range of NLP tasks (Blunsom et al., 2014; Hu et al., 2014; Severyn and Moschitti, 2015).",
      "startOffset" : 100,
      "endOffset" : 168
    }, {
      "referenceID" : 22,
      "context" : "Recently, convolutional neural networks (CNNs) are reported to perform well on a range of NLP tasks (Blunsom et al., 2014; Hu et al., 2014; Severyn and Moschitti, 2015).",
      "startOffset" : 100,
      "endOffset" : 168
    }, {
      "referenceID" : 15,
      "context" : "In (Li et al., 2016), ngram embedding is introduced into Paragraph Vector model, where text embedding is trained to be useful to predict ngrams in the text.",
      "startOffset" : 3,
      "endOffset" : 20
    }, {
      "referenceID" : 19,
      "context" : "In the word embedding literature, a work that is related to ngram is from (Mikolov et al., 2013b), where phrases are embedded into vectors.",
      "startOffset" : 74,
      "endOffset" : 97
    }, {
      "referenceID" : 19,
      "context" : "In this paper, negative sampling (Mikolov et al., 2013b) is used to approximate the conditional probability:",
      "startOffset" : 33,
      "endOffset" : 56
    }, {
      "referenceID" : 13,
      "context" : "SGNS is proven to be equivalent with factorizing pointwise mutual information (PMI) matrix (Levy and Goldberg, 2014c).",
      "startOffset" : 91,
      "endOffset" : 117
    }, {
      "referenceID" : 20,
      "context" : "To speed up the process of building ngram co-occurrence matrix, we take advantages of ‘mixture’ strategy (Pennington et al., 2014) and ‘stripes’ strategy (Dyer et al.",
      "startOffset" : 105,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : ", 2014) and ‘stripes’ strategy (Dyer et al., 2008; Lin, 2008).",
      "startOffset" : 31,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : ", 2014) and ‘stripes’ strategy (Dyer et al., 2008; Lin, 2008).",
      "startOffset" : 31,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : "Instead of using computer clusters (Lin, 2008), we can build the matrix of ‘bi bi’ type even in a laptop.",
      "startOffset" : 35,
      "endOffset" : 46
    }, {
      "referenceID" : 20,
      "context" : "It only requires 12GB to store temporary files (win=2, subsampling=0, memory size=4GB), which is much smaller than the implementations in (Pennington et al., 2014; Levy et al., 2015) .",
      "startOffset" : 138,
      "endOffset" : 182
    }, {
      "referenceID" : 14,
      "context" : "It only requires 12GB to store temporary files (win=2, subsampling=0, memory size=4GB), which is much smaller than the implementations in (Pennington et al., 2014; Levy et al., 2015) .",
      "startOffset" : 138,
      "endOffset" : 182
    }, {
      "referenceID" : 14,
      "context" : "1 Datasets The datasets used in this paper is the same with (Levy et al., 2015), including six similarity and two analogy datasets.",
      "startOffset" : 60,
      "endOffset" : 79
    }, {
      "referenceID" : 21,
      "context" : "It is hard to say which pair has closer relations (higher score) (Schnabel et al., 2015).",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "Following (Levy and Goldberg, 2014b), both additive (add) and multiplicative (mul) functions are used for finding word ‘d’.",
      "startOffset" : 10,
      "endOffset" : 36
    }, {
      "referenceID" : 14,
      "context" : "We basically follow the default setting in (Levy et al., 2015): low-frequency words (ngrams) are removed with a threshold of 100.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "Clean strategy (Levy et al., 2015) is used to ensure no information beyond pre-specified window is included.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 11,
      "context" : "Levy and Goldberg (2014b) demonstrate that sparse representation (PPMI matrix) can still achieve competitive results on many linguistic tasks, challenging the dominance of neural embedding models.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "For example, Johnson and Zhang (2015) use one-hot ngram representation as the input of CNN.",
      "startOffset" : 13,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "For example, Johnson and Zhang (2015) use one-hot ngram representation as the input of CNN. Li et al. (2016) use the ngram embeddings to represent texts.",
      "startOffset" : 13,
      "endOffset" : 109
    } ],
    "year" : 2017,
    "abstractText" : "The existing word representation methods mostly limit their information source to word co-occurrence statistics. In this paper, we introduce ngrams into four representation methods: SGNS, GloVe, PPMI matrix and its SVD factorization. Comprehensive experiments are conducted on word analogy and similarity tasks. The results show that improved word representations are learned from ngram cooccurrence statistics. We also demonstrate that the trained ngram representations are useful in many aspects such as finding antonyms and collocations. Besides, a novel approach of building co-occurrence matrix is proposed to alleviate the hardware burden brought by ngrams.",
    "creator" : "LaTeX with hyperref package"
  }
}