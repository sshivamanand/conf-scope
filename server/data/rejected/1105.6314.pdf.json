{
  "name" : "1105.6314.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Activity-Based Search for Black-Box Contraint-Programming Solvers",
    "authors" : [ "L. Michel", "P. Van Hentenryck" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Historically, the constraint-programming (CP) community has focused on developing open, extensible optimization tools, where the modeling and the search procedure can be specialized to the problem at hand. This focus stems partially from the roots of CP in programming languages and partly from the rich modeling language typically found in CP systems. While this flexibility is appealing for experts in the field, it places significant burden on practitioners, reducing its acceptance across the wide spectrum of potential users. In recent years however, the constraint-programming community devoted increasing attention to the development of black-box constraint solvers. This new focus was motivated by the success of Mixed-Integer Programming (MIP) and SAT solvers on a variety of problem classes. MIP and SAT solvers are typically black-box systems with automatic model reformulations and general-purpose search procedures. As such, they allow practitioners to focus on modeling aspects and may reduce the time to solution significantly.\nThis research is concerned with one important aspect of black-box solvers: the implementation of a robust search procedure. In recent years, various proposals have addressed this issue. Impact-based search (Ibs) [10] is motivated by concepts found in MIP solvers such as strong branching and pseudo costs. Subsequent work about solution counting can be seen as an alternative to impacts [8] that exploits the structure of CP constraints. The weighted-degree heuristic (wdeg) [1] is a direct adaptation of the SAT heuristic Vsids[5] to CSPs that relies on information collected from failures to define the variable ordering.\nThis paper proposes Activity-Based Search (Abs), a search heuristic that recognizes the central role of constraint propagation in constraint-programming\nar X\niv :1\n10 5.\n63 14\nv1 [\ncs .A\nI] 3\n1 M\nay 2\nsystems. Its key idea is to associate with each variable a counter which measures the activity of a variable during propagation. This measure is updated systematically during search and initialized by a probing process. Abs has a number of advantages compared to earlier proposals. First, it does not deal explicitly with variable domains which complicates the implementation and runtime requirements of Ibs. Second, it does not instrument constraints which is a significant burden in solution-counting heuristics. Third, it naturally deals with global constraints, which is not the case of wdeg since all variables in a failed constraint receive the same weight contribution although only a subset of them is relevant to the conflict. Abs was compared experimentally to Ibs and wdeg on a variety of benchmarks. The results show that Abs is the most robust heuristic and can produce significant improvements in performance over Ibs and wdeg, especially when the problem complexity increases.\nThe rest of the paper is organized as follows. Sections 2 and 3 review the Ibs and wdeg heuristics. Section 4 presents Abs. Section 5 presents the experimental results and Section 6 concludes the paper."
    }, {
      "heading" : "2 Impact-Based Search",
      "text" : "Impact-based search was motivated by the concept of pseudo-cost in MIP solvers. The idea is to associate with a branching decision x = v a measure of how effectively it shrinks the search space. This measure is called the impact of the branching decision.\nFormalization Let P = 〈X,D,C〉 be a CSP defined over variables X, domains D, and constraints C. Let D(xi) denote the domain of variable xi ∈ X and |D(xi)| denote the size of this domain. A trivial upper-bound on the size of the search space of S(P ) is given by the product of the domain sizes:\nS(P ) = ∏ x∈X |D(x)|\nAt node k, the search procedure receives a CSP Pk−1 = 〈X,Dk−1, Ck−1〉, where Ck−1 = C ∪ {c0, c1, c2, · · · , ck−1} and ci is the constraint posted at node i. Labeling a variable x with value a ∈ Dk−1(x) adds a constraint x = a to Ck−1 to produce, after propagation, the CSP Pk = 〈X,Dk, Ck〉.\nThe contraction of the search space induced by a labeling x = a is defined as\nI(x = a) = 1− S(Pk) S(Pk−1)\nI(x = a) = 1 when the assignment produces a failure since S(Pk) = 0 and I(x = a) ≈ 0 whenever S(Pk) ≈ S(Pk−1), i.e., whenever there is almost no domain reduction. An estimate of the impact of the labeling constraint x = a over a set of search tree nodes K can then be defined as\nĪ(x = a) =\n∑ k∈K 1− S(Pk) S(Pk−1)\n|K|\nActual implementations (e.g., [7]) rely instead on\nĪ1(x = a) = Ī0(x = a) · (α− 1) + I(x = a)\nα\nwhere α is a parameter of the engine and the subscripts in Ī0 and Ī1 denote the impact before and after the update. Clearly, α = 1 yields a forgetful strategy, α = 2 gives a running average that progressively decays past impacts, while a choice α > 2 favors past information over most recent observations.\nThe (approximate) impact of a variable x at node k is defined as I(x) = ∑\na∈Dk(x)\n1− Ī(x = a)\nTo obtain suitable estimates of the assignment and variable impacts at the root node, Ibs simulates all the ∑ x∈X |D(x)| possible assignments. For large domains, domain values are partitioned in blocks. Namely, for a variable x, let D(x) = ∪bi=1Bi with Bi ∩ Bj = ∅ ∀i, j : i 6= j ∈ 1..b. The impact of a value a ∈ Bi (i ∈ 1..b) is then set to I(x = a) = I(x ∈ Bi). With partitioning, the initialization costs drop from |D(x)| propagations to b propagations (one per block). The space requirement for Ibs is Θ( ∑ x∈X |D(x)|), since it stores the impacts of all variable/value pairs.\nThe Search Procedure Ibs defines a variable and a value selection heuristic. Ibs first selects a variable x with the largest impact, i.e., x ∈ argMaxx∈XI(x). It then selects a value a with the least impact, i.e., a ∈ argMinv∈D(x)Ī(x = v). Neither argMaxx∈XI(x) nor argMinv∈D(x)Ī(x = v) are guaranteed to be a singleton and, in case of ties, Ibs breaks the ties uniformly at random.\nAs any randomized search procedure, Ibs can be augmented with a restart strategy. A simple restarting scheme limits the number of failures in round i to li and increases the limit between rounds to li+1 = ρ · li where ρ > 1."
    }, {
      "heading" : "3 The WDEG Heuristic",
      "text" : "wdeg maintains, for each constraint, a counter (weight) representing the number of times a variable appears in a failed constraint, i.e., a constraint whose propagation removes all values in the domain of a variable. The weighted degree of variable x is defined as\nαwdeg(x) = ∑ c∈C weight[c] s.t. x ∈ vars(c)x ∧ |FutV ars(c)| > 1\nwhere FutV ars(c) is the set of uninstantiated variables in c. wdeg only defines a variable selection heuristic: It first selects a variable x with the smallest ratio |D(x)|αwdeg(x) . All the weights are initialized to 1 and, when a constraint fails, its weight is incremented. The space overhead of wdeg is Θ(|C|) for a CSP 〈X,D,C〉."
    }, {
      "heading" : "4 Activity-Based Search",
      "text" : "Abs is motivated by the key role of propagation in constraint-programming solvers. Contrary to SAT solvers, CP uses sophisticated filtering algorithms to prune the search space by removing values that cannot appear in solutions. Abs exploits this filtering information and maintains, for each variable x, a measure of how often the domain of x is reduced during the search. The space requirement for this statistic is Θ(|X|). Abs can optionally maintain a measure of how much activity can be imputed to each assignments x = a in order to drive a valueselection heuristic. If such a measure is maintained, the space requirement is proportional to the number of distinct assignments performed during the search and is bounded by O( ∑ x∈X |D(x)|). Abs relies on a decaying sum to forget the oldest statistics progressively, using an idea from Vsids. It also initializes the activity of the variables by probing the search space.\nAbs is simple to implement and does not require sophisticated constraint instrumentation. It scales to large domains without special treatment and is independent of the domain sizes when the value heuristic is not used. Also, Abs does not favor variables appearing in failed constraints, since a failure in a CP system is typically the consequence of many filtering algorithms.\nFormalization Given a CSP P = 〈X,D,C〉, a CP solver applies a constraintpropagation algorithm F after a labeling decision. F produces a new domain store D′ ⊆ D enforcing the required level of consistency. Applying F to P identifies a subset X ′ ⊆ X of affected variables defined by\n∀x ∈ X ′ : D′(x) ⊂ D(x); ∀x ∈ X \\X ′ : D′(x) = D(x).\nThe activity of x, denoted by A(x), is updated at each node k of the search tree by the following two rules:\n∀x ∈ X s.t. |D(x)| > 1 : A(x) = A(x) · γ ∀x ∈ X ′ : A(x) = A(x) + 1\nwhere X ′ is the subset of affected variables and γ be an age decay parameter satisfying 0 ≤ γ ≤ 1. The aging only affects free variables since otherwise it would quickly erase the activity of variables labeled early in the search.\nThe activity of an assignment x = a at a search node k is defined as the number of affected variables in |X ′| when applying F on C ∪ {x = a}, i.e.,\nAk(x = a) = |X ′|.\nAs for impacts, the activity of x = a over the entire tree can be estimated by an average over all the tree nodes seen so far, i.e., over the set of nodes K. The estimation is thus defined as\nÃ(x = a) =\n∑ k∈KAk(x = a)\n|K|\nOnce again, it is simpler to favor a weighted sum instead\nÃ1(x = a) = Ã0(x = a) · (α− 1) +Ak(x = a)\nα\nwhere the subscripts on Ã capture the estimate before and after the update. When the value heuristic is not used, it is not necessary to maintain Ã(x = a) which reduces the space requirements without affecting variable activities.\nThe Search Procedure Abs defines a variable ordering and possibly a value ordering. It selects the variable x with the largest ratio A(x)|D(x)| , i.e., the most active variable per domain value. Ties are broken uniformly at random. When a value heuristic is used, Abs selects a value a with the least activity, i.e., a ∈ argMinv∈D(x)Ã(x = v). The search procedure can be augmented with restarts. The activities can be used “as-is” to guide the search after a restart. It is also possible to reinitialize activities in various ways, but this option was not explored so far in the experimentations.\nInitializing Activities Abs uses probing to initialize the activities. Consider a path π going from the root to a leaf node k in a search tree for the CSP P = 〈X,D,C〉. This path π corresponds to a sequence of labeling decisions (x0 = v0, x1 = v1, · · · , xk = vk) in which the jth decision labels variable xj with vj ∈ Dj(xj). If Xj ⊆ X is the subset of variables whose domains are filtered as a result of applying F after decision xj = vj , the activity of variable x along path π is defined as Aπ(x) = Aπk (x) where Aπ0 (x) = 0 Aπj (x) = A π j−1(x) + 1⇔ x ∈ Xj (1 ≤ j ≤ k)\nAπj (x) = A π j−1(x) ⇔ x /∈ Xj (1 ≤ j ≤ k)\nAπ(x) = 0 if x was never involved in any propagation along π and Aπ(x) = k if the domain of x was filtered by each labeling decision in π. Also, Aπ(x) = A(x) when γ = 1 (no aging) and path π is followed.\nNow let us now denote Π the set of all paths in some search tree of P . Each such path π ∈ Π defines an activity Aπ(x) for each variable x. Ideally, we would want to initialize the activities of x as the average over all paths in Π, i.e.,\nµA(x) =\n∑ π∈Π A π(x)\n|Π| .\nAbs initializes the variables activities by sampling Π to obtain an estimate of the mean activity µ̃A(x) from a sample Π̃ ⊂ Π. More precisely, Abs repeatedly draws paths from Π. These paths are called probes and the jth assignment xj = vj in a probe p is selected uniformly at random as follows: (1) xj is a free variable and (2) value vj is picked from Dj(xj). During the probe execution, variable activities are updated normally but no aging is applied in order to ensure that all probes contribute equally to µ̃A(x). Observe that some probes may\nterminate prematurely since a failure may be encountered; others may actually find a solution if they reach a leaf node. Moreover, if if a failure is discovered at the root node, singleton arc-consistency [9] has been established and the value is removed from the domain permanently.\nThe number of probes is chosen to provide a good estimate of the mean activity over the paths. The probing process delivers an empirical distribution Ã(x) of the activity of each variable x with mean µ̃A(x) and standard deviation σ̃A(x). Since the probes are iid, the distribution can be approximated by a normal distribution and the probing process is terminated when the 95% confidence interval of the t-distribution, i.e., when\n[µ̃A(x)− t0.05,n−1 · σ̃A(x)√\nn , µ̃A(x) + t0.05,n−1 · σ̃A(x)√ n ]\nis sufficiently small (e.g., within δ% of the empirical mean) for all variables x with n being the number of probes,\nObserve that this process does not require a separate instrumentation. It uses the traditional activity machinery with γ = 1. In addition, the probing process does not add any space requirement: the sample mean µ̃A(x) and the sample standard deviation σ̃A(x) are computed incrementally, including the activity vector Ap for each probe as it is completed. If a value heuristic is used the sampling process also maintains A(x = a) for every labeling decision x = a attempted during the probes."
    }, {
      "heading" : "5 Experimental Results",
      "text" : ""
    }, {
      "heading" : "5.1 The Experimental Setting",
      "text" : "The Configurations All the experiments were done on a Macbook Pro with a core i7 at 2.66Ghz running MacOS 10.6.7. Ibs, wdeg, and Abs were were all implemented in the Comet system [3]. Since the search algorithms are in general randomized, the empirical results are based on 50 runs and the tables report the average (µT ) and the standard deviation σT of the running times in seconds. Unless specified otherwise, a timeout of 5 minutes was used and runs that timeout were assigned a 300s runtime. The following parameter values were used for the experimental results: α = 8 in both Ibs and Abs, γ = 0.999 (slow aging), and δ = 20%. Experimental results on the sensitivities of these parameters will also be reported. For every heuristic, the results are given for three strategies: no restarts (NR), fast restarting (ρ = 1.1) or slow restarting (ρ = 2). depending on which strategy is best across the board. The initial failure limit is set to 3 · |X|.\nSearch Algorithms The search algorithms were run on the exact same models, with a single line changed to select the search procedure. In our experiments, Ibs does not partition the domains when initializing the impacts and always computes the impacts exactly. Both the variable and value heuristics break ties\nrandomly. In wdeg, no value heuristic is used: The values are tried in the sequential order of the domain. Ties in the variable selection are broken randomly. All the instances are solved using the same parameter values as explained earlier. No comparison with model-counting heuristic is provided, since these are not available in publicly available CP solvers.\nBenchmarks The experimental evaluation uses benchmarks that have been widely studied, often by different communities. The multi-knapsack and magic square problems both come from the Ibs paper [10]. The progressive party has been a standard benchmark in the local search, mathematical-programming, and constraint-programming communities, and captures a complex, multi-period allocation problem. The nurse rostering problem [11] originated from a mathematicalprogramming paper and constraint programming was shown to be a highly effective and scalable approach. The radiation problem is taken from the 2008 MiniZinc challenge [6] and has also been heavily studied. These benchmarks typically exploit many features of constraint-programming systems including numerical, logical, reified, element, and global constraints."
    }, {
      "heading" : "5.2 The Core Results",
      "text" : "Multi-Knapsack This benchmark is from [10] and our implementation follows Refalo’s informal description. The satisfaction model uses an arithmetic encoding of the binary knapsacks (not a global constraint) where the objective is replaced by a linear equality with a right-hand-side set to the known optimal value. All\nthe constraints use traditional bound-consistency algorithms for filtering linear constraints. A second set of experiments considers the optimization variant. The COP uses n global binary knapsack constraints (binaryKnapsackAtmost in Comet) based on the filtering algorithm in [13]. These benchmarks contain up to 50 variables.\nFigure 1 is a pictorial depiction of the behavior of the three search algorithms with no restarts. The chart on the left gives the absolute running time (in seconds) with a logarithmic scale. The stacked bar chart on the right reports the same data using a relative scale instead where the height of the bar is the normalized sum of the running time of all three algorithms and the length of each segment is its normalized running time. Note that, since this view is not logarithmic, adjacent bars correspond to different totals. The results clearly show that, as the difficulty of the problem increases, the quality of wdeg sharply decreases and the quality of Abs significantly improves. On the harder instances, Abs is clearly superior to Ibs and vastly outperforms wdeg. Figure 2 conveys the same information for the optimization variant with similar observations.\nTable 1 gives the numerical results for instances 1 − 2 to 1 − 6. The first column specifies the instance, while the remaining columns report the average run times, the standard deviations, and the number of runs that did not time-out. The results are given for no-restart and slow-restart strategies for all heuristics. On the decision instance 1 − 6, wdeg often fails to find a solution within the time limit and, in general, takes considerable time. Abs always finds solutions and is about 5 times faster than Ibs for the no-restart strategy which is most effective on the decision variant. On the optimization variant, wdeg cannot solve instance 1 − 5 in any of the 50 runs and Ibs does not always find a solution. Abs, in contrast, finds a solution in all 50 runs well within the time limit.\nIn summary, on this benchmark, wdeg is vastly outperformed by Ibs and Abs as soon as the instances are not easy. Abs is clearly the most robust heuristic and produces significant improvements in performance on the most difficult instances, both in the decision and optimization variants.\nMagic Square This benchmark is also from [10] and the model is based on a direct algebraic encoding with 2 · n linear equations for the rows and columns (the square side is n), 2 linear equations for the diagonals, one alldifferent constraint (not enforcing domain consistency) for the entire square, 2 · n binary inequalities to order the elements in the diagonals, and two binary inequalities to order the top-left corner against the bottom-left and top-right corners. Table 2 report results for squares of size 7 to size 10. The F column in Table 2 reports the number of successful runs (no timeout).\nOn magic squares, wdeg is completely dominated by Ibs and Abs: It has poor performance and is not robust even on the simpler instances. The best performance for Ibs and Abs is obtained using a fast restart, in which case Abs provides a slight improvement over Ibs. Ibs is more effective than Abs with slow or no restarts.\nProgressive Party The progressive party problem [12] is a constraint satisfaction problem featuring a mix of global constraint and has been used frequently for benchmarking CP, LS, and MIP solvers. The instance considered here is the 2−8 instance with 29 guests, 8 periods and 13 hosts, i.e., 232 variables with domains of size 13. The goal is to find a schedule for a social event taking place over k time periods subject to constraints on the sizes of the venues (the boats), sizes of the group, and social constraints (two groups cannot meet more than once and one group cannot go back to the same boat more than once). The model relies on multiple global alldifferent, multi-knapsacks and arithmetic constraints with reifications. This model breaks the search in k phases (one per period) and uses the black-box heuristic within each period.\nThe results are given in Table 3. Abs clearly dominates Ibs on this benchmark for all restarting strategies. Abs is also clearly superior to wdeg when no restarts are used but is slightly slower than wdeg when slow or fast restarts are used.\nNurse Rostering This benchmark is taken from [11] and is a rostering problem assigning nurses to infants in an hospital ward, while balancing the workload. The multi-zone model can be found in Listing 1.2 in [11]. The custom search procedure is removed and replaced by a call to one of the generic searches (Ibs,Abs,wdeg). Table 4 reports the performance results for the three heuristics and 3 restarting strategies on the one-zone instances (z1-z5,z8). Note that the custom procedure in [11] relies on a dynamic-symmetry breaking on values and sophisticated variable/value ordering. Results for wdeg beyond z5 are not reported as it times out systematically. As before, column F reports the number of runs that finish (out of 50), C reports the number of choice points and the T columns reports the mean and standard deviation of the running time.\nwdeg exhibits extremely poor performance and robustness on this benchmark. Abs is clearly the most robust procedure as it solves all instances in all its runs for all the restarting strategies. It is also significantly faster than Ibs on z4 and z8. Ibs behaves well in general, except on z4 and z8 for which it sometimes fails to find solutions and takes significantly more time than Abs. It is faster than Abs on z3 and z5.\nRadiation This last benchmark is a constrained optimization problem for radiation therapy taken from the 2008 MiniZinc challenge [6]. The objective is to find a setup of a radiation therapy machine to deliver a desired radiation intensity to a tumor. The problem uses algebraic constraint and the formulation can be found in the mini-zinc repository [4]3. The search procedure must deal with all the variables at once. In 2008, several solvers were unable to solve most instances in a reasonable amount of time as seen in [4], which indicates the difficulty of the instances. The instance sizes are specified in Table 5. A row gives a term for each array in the problem with its size and the size of the domains. For instance, instance 9 has one variable with domain size 175, ten variables with domain size 37, and 333 variables with domain sizes 37.\nTable 6 reports the results for 5 instances. Abs clearly dominates Ibs on all instances and Ibs cannot solve the largest instance within the time limit for any restarting strategy. wdeg performs well in general on this benchmark. It is slightly faster than Abs on the largest instance for the slow and fast restarts,\n3 In this model, the time that the beam is on is a variable and must be optimized alongside the number of patterns.\nbut is slower with no restarts. On instance 9, it is faster with no restart and slower when slow or fast restart are used. Both wdeg and Abs are effective on this problem and clearly superior to Ibs.\nSummary On this collection of benchmarks, Abs is clearly the most robust and effective heuristic. It is robust across all benchmarks and restarting strategies and is, in general, the fastest heuristic. wdeg has significant robustness and performance issues on the multi-knapsack, magic square, and nurse rostering benchmarks. Ibs has some robustness issues on radiation, some rostering instances, and the optimization variant of the large knapsack problems. It is in general significantly less efficient than Abs on the knapsack, rostering, and radiation benchmarks."
    }, {
      "heading" : "5.3 Sensitivity Analysis",
      "text" : "Criticality of the Variable Ordering Table 7 reports the performance of activitybased search when no value ordering is used. The value heuristic simply tries the value in increasing order as in wdeg. The results indicate that the value selection heuristic of Abs does not play a critical role and is only marginally faster/more robust on the largest instances.\nSensitivity to the Sample Size Figure 3 illustrates graphically the sensitivy of Abs to the confidence interval parameter δ used to control the number of probes in the initialization process. The statistics are based on 50 runs of the nonrestarting strategy. The boxplots show the four main quartiles for the running time (in seconds) of Abs with δ ranging from 0.8 down to 0.05. The blue line connects the medians whereas the red line connects the means. The circles beyond the extreme quartiles are outliers. The left boxplot shows results on msq-10 while the right one shows results on the optimization version of knap1-4.\nThe results show that, as the number of probes increases (i.e., δ becomes smaller), the robustness of the search heuristic improves and the median and the mean tend to converge. This is especially true on knap1-4, while msq-10 still exhibits some variance when δ = 0.05. Also, the mean decreases with more probes on msq-10, while the mean increases on knap1-4 as the probe time becomes more important. The value δ = 0.2 used in the core experiments seem to be a reasonable compromise.\nSensitivity to γ (Aging) Figure 4 illustrates the sensitivity to the aging parameter. The two boxplots are once again showing the distribution of running times in seconds for 50 runs of msq-10 (left) and knap1-4 (right). What is not immediately visible on the figure is that the number of timeouts for msq-10 increases from 0 for γ = 0.999 to 9 for γ = 0.5. Overall, the results seem to indicate that the slow aging process is desirable."
    }, {
      "heading" : "5.4 Some Behavioral Observations",
      "text" : "Figure 5 depicts the behavior of Abs and Ibs on radiation #9 under all three restarting strategies. The x axis is the running time in a logarithmic scale and the y axis is the objective value each time a new upper bound is found. The three red curves depict the performance of activity-based search, while the three blue curves correspond to impact-based search. What is striking here is the difference in behavior between the two searches. Abs quickly dives to the optimal solution and spends the remaining time proving optimality. Without restarts, activitybased search hits the optimum within 3 seconds. With restarts, it finds the optimal solution within one second and the proof of optimality is faster too. Ibs slowly reaches the optimal solution but then proves optimality quickly. Restarts have a negative effect on Ibs. We conjecture that the reduction of large domains\nmay not be such a good indicator of progress and may bias the search toward certain variables.\nFigure 6 provide interesting data about activities on radiation #9. In particular, Figure 6 gives the frequencies of activity levels at the root, and plots the activity levels for all the variables. (Only the variables not fixed by singleton arc-consistency are shown in the figures). The two figures highlight that the probing process has isolated a small subset of the variables with very high activity levels, indicating that, on this benchmark, there are relatively few very active variables. It is tempting to conjecture that this benchmark has backdoors [14] or good cycle-cutsets [2] and that activity-based search was able to discover them, but more experiments are needed to confirm or disprove this conjecture."
    }, {
      "heading" : "6 Conclusion",
      "text" : "Robust search procedures is a central component in the design of black-box constraint-programming solvers. This paper proposed activity-based search, the\nidea of using the activity of variables during propagation to guide the search. A variable activity is incremented every time the propagation step filters its domain and is aged otherwise. A sampling process is used to initialize the variable activities prior to search. Activity-based search was compared experimentally to the Ibs and wdeg heuristics on a variety of benchmarks. The experimental results have shown that Abs was significantly more robust than both Ibs and wdeg on these classes of benchmarks and often produces significant performance improvements."
    } ],
    "references" : [ {
      "title" : "Boosting systematic search by weighting constraints",
      "author" : [ "F. Boussemart", "F. Hemery", "C. Lecoutre", "L. Sais" ],
      "venue" : "In R. L. de Mántaras and L. Saitta, editors, ECAI, pages 146–150. IOS Press,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "The cycle-cutset method for improving search performance in ai applications",
      "author" : [ "R. Dechter", "J. Pearl" ],
      "venue" : "In Proceedings of 3 IEEE conference on AI Applications, Orlando, FL,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Comet v2.1 user manual",
      "author" : [ "I. Dynadec" ],
      "venue" : "Technical report, Providence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Chaff: engineering an efficient sat solver",
      "author" : [ "M.W. Moskewicz", "C.F. Madigan", "Y. Zhao", "L. Zhang", "S. Malik" ],
      "venue" : "In Proceedings of the 38th annual Design Automation Conference, DAC ’01, pages 530–535, New York, NY, USA,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Minizinc: Towards a standard cp modelling language",
      "author" : [ "N. Nethercote", "P.J. Stuckey", "R. Becket", "S. Brand", "G.J. Duck", "G. Tack" ],
      "venue" : "In In: Proc. of 13th International Conference on Principles and Practice of Constraint Programming, pages 529–543. Springer,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Counting and estimating lattice points: Special polytopes for branching heuristics in constraint programming",
      "author" : [ "G. Pesant" ],
      "venue" : "Optima Newsletter, 81:9–14,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Singleton consistencies",
      "author" : [ "P. Prosser", "K. Stergiou", "T. Walsh" ],
      "venue" : "In CP’02, pages 353–368, London, UK,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Impact-based search strategies for constraint programming",
      "author" : [ "P. Refalo" ],
      "venue" : "In M. Wallace, editor, CP, volume 3258 of Lecture Notes in Computer Science, pages 557–571. Springer,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Scalable load balancing in nurse to patient assignment problems",
      "author" : [ "P. Schaus", "P. Van Hentenryck", "J.-C. Rgin" ],
      "venue" : "In W.-J. van Hoeve and J. Hooker, editors, Integration of AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems, volume 5547 of Lecture Notes in Computer Science, pages 248–262. Springer Berlin / Heidelberg,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The Progressive Party Problem: Integer Linear Programming and Constraint Programming Compared",
      "author" : [ "B. Smith", "S. Brailsford", "P. Hubbard", "H. Williams" ],
      "venue" : "Constraints, 1:119–138,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "A dynamic programming approach for consistency and propagation for knapsack constraints",
      "author" : [ "M.A. Trick" ],
      "venue" : "In Annals of Operations Research, pages 113–124,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Backdoors to typical case complexity",
      "author" : [ "R. Williams", "C.P. Gomes", "B. Selman" ],
      "venue" : "In Proceedings of the 18th international joint conference on Artificial intelligence, 1173–1178, San Francisco, CA, USA,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Impact-based search (Ibs) [10] is motivated by concepts found in MIP solvers such as strong branching and pseudo costs.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 5,
      "context" : "Subsequent work about solution counting can be seen as an alternative to impacts [8] that exploits the structure of CP constraints.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "The weighted-degree heuristic (wdeg) [1] is a direct adaptation of the SAT heuristic Vsids[5] to CSPs that relies on information collected from failures to define the variable ordering.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "The weighted-degree heuristic (wdeg) [1] is a direct adaptation of the SAT heuristic Vsids[5] to CSPs that relies on information collected from failures to define the variable ordering.",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "Moreover, if if a failure is discovered at the root node, singleton arc-consistency [9] has been established and the value is removed from the domain permanently.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "Ibs, wdeg, and Abs were were all implemented in the Comet system [3].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 7,
      "context" : "The multi-knapsack and magic square problems both come from the Ibs paper [10].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : "The nurse rostering problem [11] originated from a mathematicalprogramming paper and constraint programming was shown to be a highly effective and scalable approach.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "The radiation problem is taken from the 2008 MiniZinc challenge [6] and has also been heavily studied.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "Multi-Knapsack This benchmark is from [10] and our implementation follows Refalo’s informal description.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "The COP uses n global binary knapsack constraints (binaryKnapsackAtmost in Comet) based on the filtering algorithm in [13].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 7,
      "context" : "Magic Square This benchmark is also from [10] and the model is based on a direct algebraic encoding with 2 · n linear equations for the rows and columns (the square side is n), 2 linear equations for the diagonals, one alldifferent constraint (not enforcing domain consistency) for the entire square, 2 · n binary inequalities to order the elements in the diagonals, and two binary inequalities to order the top-left corner against the bottom-left and top-right corners.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "Progressive Party The progressive party problem [12] is a constraint satisfaction problem featuring a mix of global constraint and has been used frequently for benchmarking CP, LS, and MIP solvers.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 8,
      "context" : "Nurse Rostering This benchmark is taken from [11] and is a rostering problem assigning nurses to infants in an hospital ward, while balancing the workload.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "2 in [11].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 8,
      "context" : "Note that the custom procedure in [11] relies on a dynamic-symmetry breaking on values and sophisticated variable/value ordering.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "Radiation This last benchmark is a constrained optimization problem for radiation therapy taken from the 2008 MiniZinc challenge [6].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 11,
      "context" : "It is tempting to conjecture that this benchmark has backdoors [14] or good cycle-cutsets [2] and that activity-based search was able to discover them, but more experiments are needed to confirm or disprove this conjecture.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : "It is tempting to conjecture that this benchmark has backdoors [14] or good cycle-cutsets [2] and that activity-based search was able to discover them, but more experiments are needed to confirm or disprove this conjecture.",
      "startOffset" : 90,
      "endOffset" : 93
    } ],
    "year" : 2011,
    "abstractText" : "Robust search procedures are a central component in the design of black-box constraint-programming solvers. This paper proposes activity-based search, the idea of using the activity of variables during propagation to guide the search. Activity-based search was compared experimentally to impact-based search and the wdeg heuristics. Experimental results on a variety of benchmarks show that activity-based search is more robust than other heuristics and may produce significant improvements in performance.",
    "creator" : "TeX"
  }
}