{
  "name" : "1206.0259.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Harnad,  Stevan  (2012)  The  Causal  Topography  of  Cognition.  [commentary  on:   Chalmers,  David:  “A  Computational  Foundation  for  the  Study  of  Cognition”]  Journal   of  Cognitive  Science.  http://consc.net/papers/computation.html    \n   The  Causal  Topography  of  Cognition  \n   Stevan  Harnad  \nChaire  de  recherche  du  Canada   Institut  des  sciences  cognitives  (ISC)     Université  du  Québec  à  Montréal    \nMontréal,  Québec   Canada  H3C  3P8  \nhttp://www.crsc.uqam.ca/fr/index2.html     &    \nSchool  of  Electronics  and  Computer  Science     University  of  Southampton     Highfield,  Southampton    \nSO17  1BJ  UNITED  KINGDOM   http://users.ecs.soton.ac.uk/harnad/    \n   ABSTRACT:  The  causal  structure  of  cognition  can  be  simulated  but  not  implemented   computationally,  just  as  the  causal  structure  of  a  comet  can  be  simulated  but  not  implemented   computationally.  The  only  thing  that  allows  us  even  to  imagine  otherwise  is  that  cognition,   unlike  a  comet,  is  invisible  (to  all  but  the  cognizer).      Keywords:  computation,  cognition,  causation,  consciousness  \n   David  Chalmers’s  (2012)  thesis  on  cognition  and  computation  is  either  true  but   trivial  or  nontrivial  but  false.    \nWhat  is  true  but  trivial  is  that  (just  about)  any  causal  system  –  whether  it’s  a  galaxy,   a  gall-‐bladder  or  a  grain  of  sand  -‐-‐  can  be  modeled  computationally,  thereby  (if   successful)  fully  capturing  (and  hence  explaining)  its  (relevant)  causal  mechanism:   how  it  works.      \nThat  is  certainly  something  that  psychology,  cognitive  science,  neuroscience  and   artificial  intelligence  would  want  for  the  kinds  of  things  they  study  and  hope  to   explain  causally:  organisms,  their  brains,  their  behavior,  and  artificial  devices  we   build  that  are  capable  of  similar  kinds  of  behavior.  But  that’s  something  that  physics,   chemistry,  biology  and  engineering  already  have  for  the  kinds  of  things  they  study,   without  the  need  for  “A  Computational  Foundation  for  the  Study  of    Planetary   Motion”  (or  of  Helium,  or  of  Hemoglobin,  or  of  Convection  Heaters).  It’s  simply  the   ubiquitous  observation  that  –  alongside  language  and  mathematics  –  computers  and   computational  algorithms  are  useful  tools  in  explaining  the  things  there  are  in  the   world  and  how  they  work  (Searle’s  1980  “Weak  AI”).  "
    }, {
      "heading" : null,
      "text" : "But  there  is  another  way  of  construing  David’s  thesis,  and  that  is  that  –  unlike,  say,   flying,  or  digesting,  which  can  likewise  be  modeled  and  explained  computationally   (but  are  not,  as  David  agrees,  themselves  instances  of  computation)  –  cognition  is   (just)  computation  (Harnad  1994).  \nIt’s  not  really  clear  which  of  these  two  theses  David  intends.  I  will  try  to  flesh  out   both  theses  without  getting  bogged  down  in  technical  details  that  are  interesting  but   not  pertinent  to  this  fundamental  distinction.  \nComputation  is  symbol  manipulation:  symbols  are  objects  of  arbitrary  shape  and   they  are  manipulated  on  the  basis  of  rules  (“algorthms”)  that  operate  on  the  shapes   of  the  symbols,  not  their  meanings.  In  other  words,  computation  is  syntactic,  not   semantic.  However,  most  computations  can  be  interpreted  as  meaning  something   (otherwise  we  would  not  bother  designing  and  doing  them):  We  are  interested  in   algorithms  that  can  compute  something  useful,  whether  it’s  planetary  motion  or   payroll  checks.    \nHow  do  computations  do  useful  things?  There  are  many  ways.  Numerical  algorithms   compute  quantitative  results  we  are  interested  in.  Desk  calculators  implement   numerical  algorithms.  Boolean  (and/or/not)  search  in  Google’s  database  retrieves   documents  we  are  interested  in.  NASA’s  simulations  anticipate  problems  that  might   arise  in  space  flights.  If  Copernicus  and  Galileo  had  had  digital  computers,  they   might  (just  might!)  have  reached  their  conclusions  faster,  or  more  convincingly.   Appel  &  Haken  proved  the  four-‐color  theorem  with  the  help  of  computation  in  1976.   And  if  Mozart  had  had  a  computer  to  convert  keyboard  improvisation  into  metered   notation,  ready  to  print  or  edit  and  revise  online,  humankind  might  have  been  left  a   much  larger  legacy  of  immortal  masterpieces  from  his  tragically  short  35  years  of   life.    \nA  word  about  causal  structure  –  a  tricky  notion  that  goes  to  the  heart  of  the  matter.   Consider  gravitation.  As  currently  understood,  gravitation  is  a  fundamental  causal   force  of  attraction  between  bodies,  proportional  to  their  respective  masses.  If  ever   there  was  a  prototypical  instance  of  causal  structure,  causing,  gravitational   attraction  is  such  an  instance.    \nNow  gravitational  attraction  can  be  modeled  exactly,  by  differential  equations,  or   computationally,  with  discrete  approximations.  Our  solar  system’s  planetary  bodies   and  sun,  including  the  causal  structure  of  their  gravitational  interactions,  can  be   “mirrored”  in  a  computer  simulation  to  as  close  an  approximation  as  we  like.  But  no   one  would  imagine  that  the  computer  simulation  actually  embodied  planetary   motion:  It  would  be  evident  that  there  was  nothing  actually  moving,  nor  anything   actually  exerting  gravitational  attraction  in  the  computational  model.  The  computer   implementation  of  the  algorithm  would  indeed  have  causal  structure  –  a  computer,   computing,  is,  after  all,  a  physical,  dynamical  system  too,  hence,  like  the  solar  system   itself,  governed  by  differential  equations.  But  the  causal  structure  of  the   implementation,  as  a  dynamical  system  in  its  own  right,  would  be  the  wrong  causal   structure  (and  would  obey  the  wrong  differential  equations),  insofar  as  planetary  "
    }, {
      "heading" : null,
      "text" : "motion  was  concerned.  It  would  not  be  the  causal  structure  of  planets,  moving:  it   would  be  the  causal  structure  of  computer  hardware,  executing  a  certain  algorithm,   thereby  formally  “mirroring”  the  causal  structure  of  planetary  motion,  as  encoded  in   the  algorithm.  Two  different  dynamical  systems,  with  different  dynamical   properties:  those  of  the  hardware,  implementing  the  algorithm,  and  those  of  the   planets,  orbiting.    \nSo  in  what  sense  does  the  causal  structure  of  the  computational  model  “mirror”  the   causal  structure  of  the  thing  that  it  is  modeling?  The  reply  is  that  it  mirrors  it   formally.  That  means  that  the  symbols  and  symbol  manipulations  in  the  model  can   be  systematically  interpreted  as  having  counterparts  in  the  thing  being  modeled.  We   don’t  even  have  to  resort  to  computational  simulations  of  planetary  motion,  nor   even  to  the  exact  differential  equations  of  physics  in  order  to  see  this:  We  can  see  it   in  geometry,  in  the  way  x2  +  y2  =  r2  “mirrors”  the  shape  of  a  circle:  Yes,  x2  +  y2  =  r2   “captures”  the  invariant  structure  of  the  circle,  but  it  is  not  a  circle,  it  is  not  shaped   like  a  circle.  (Reminder:  a  circle  is  the  kind  of  thing  you  see  on  the  Japanese  flag.)  No   one  would  think  otherwise,  despite  the  accurate  mirroring.  \nBut  this  is  all  obvious.  Everyone  knows  that  the  mathematical  (or  verbal)   description  of  a  thing  is  not  the  same  kind  of  thing  as  the  thing  itself,  despite  the   shared  formal  invariance.  Why  would  one  even  be  tempted  to  think  otherwise?  We   immediately  see  by  observation  that  a  computational  solar  system  lacks  the   essential  feature  of  a  real  solar  system  despite  the  shared  “causal  structure,”  namely,   there  are  no  bodies  there,  moving,  any  more  than  there  is  anything  round  in  the   formal  equation  for  a  circle.  The  model  –  whether  static  or  dynamic  -‐-‐  is  just  an   explanatory  device,  not  a  reincarnation  of  the  thing  it  is  modelling.  \nSo  it  is  evident  in  the  case  of  physics,  chemistry,  biology  (synthetic  hearts  pump   blood,  but  computational  hearts  do  not)  and  even  mathematics  that  the  “causal   structure”  of  the  model  (whether  computational  or  analytic,  symbolic  or  numeric,   discrete  or  continuous,  approximate  or  exact)  may  be  the  right  one  for  a  full  causal   explanation  of  the  thing  being  modeled,  but  not  for  a  causal  instantiation  of  it,  unless   it  embodies  the  relevant  causal  properties  of  the  thing  being  modeled  (the  way  a   synthetic  heart  does)  rather  than  just  formally  “mirroring”  them  by  physically   implementing  their  computation  (the  way  a  computational  heart  would  do).  Why  is   this  not  so  evident  in  the  case  of  cognition?  \nHow  could  David  (and  so  many  others)  have  fallen  into  the  error  of  confusing  these   two  senses  of  causality,  one  formal,  the  other  physical?  Again,  I  think  the  answer  is   obvious.  The  error  is  always  made  in  the  special  case  of  cognition.  But  what  on  earth   is  cognition?  Unlike,  say,  movement,  cognition  is  invisible!  (1)  We  all  know  what   cognizing  organisms  can  do.  (2)  We  all  know  what  brain  activity  is.  (3)  And  we  all   know  what  it  feels  like  to  cognize.  (1)  and  (2)    (behavior  and  brain  activity)  are   perfectly  visible;  but  we  have  no  idea  what  it  is  about  the  brain  activity  that   generates  the  behavioral  capacity,  let  alone  the  cognition.    And  the  only  one  to   whom  cognition  itself  is  “visible”  is  the  cognizer  (3).  So  when  we  speculate  about   what  generates  cognition,  we  are  speculating  about  something  that  is  invisible  to  "
    }, {
      "heading" : null,
      "text" : "everyone  except  the  cognizer,  namely,  cognition  itself;  and  we  are  speculating  about   what  brain  activity  (or  synthetic  device  activity,  if  cognition  is  possible  in  synthetic   devices)  generates  that  invisible  cognition.    \nLet’s  contrast  the  case  of  the  brain  and  its  invisible  cognition  with  the  case  of   planetary  motion,  as  well  as  with  the  case  of  a  bodily  organ  other  than  the  brain,  one   for  which  the  problem  of  invisibility  does  not  arise:  the  heart.  The  reason  we  would   never  dream  of  saying  that  planetary  motion  was  just  computational,  or  of  saying   that  planets  in  a  computational  model  were  actually  moving  because  the  model   mirrors  their  “causal  structure”  is  simply  that  planetary  motion  is  visible  (or   observable  by  instruments).  And  it’s  just  obvious  that  the  computational  model,   even  if  it  shares  the  formal  causal  invariants  of  planetary  motion,  does  not  move.  The   same  is  true  of  the  computational  heart:  Unlike  the  synthetic  heart,  which  really  can   pump  blood  (or  some  other  liquid),  a  (purely)  computational  heart  cannot  pump  a   thing.  (And  note  that  I  am  talking  about  a  physically  implemented  computational   heart  that  mirrors  the  causal  structure  of  a  real  heart.  It  is  merely  symbolically   pumping  symbolic  blood.)  \nNow  imagine  the  same  thing  for  the  brain.  It’s  a  bit  more  complicated,  because,   unlike  the  heart,  the  brain  is  actually  doing  not  one,  nor  two  but  three  different   things.  One  thing  is  generating  (1)  behavioral  capacity:  The  brain  is  generating  just   about  everything  our  bodies  do,  and  are  able  to  do,  in  the  external  world.  The   second  thing  is  (2)  the  internal  activity  of  the  brain  itself  (the  action  potentials  and   secretions  that  are  going  on  inside  it).  And  finally,  the  brain  is  (3)  cognizing   (whatever  that  turns  out  to  be  –  we  will  return  to  this).  \nSo,  unlike  the  planets  and  the  heart,  which  are  doing  just  one  kind  of  thing,  all  of  it   fully  observable  to  us  (moving  and  pumping,  respectively),  the  brain  is  doing  three   kinds  of  things,  two  of  them  visible  (behavior  and  brain  activity),  one  of  them  not   (cognition).  \nNow  we  are  in  a  position  to  pinpoint  exactly  where  the  error  keeps  creeping  in:  No   one  would  call  a  cognitive  model  a  success  if  it  could  not  be  demonstrated  to   generate  our  behavioral  capacity.  So  the  question  becomes:  what  kind  of  model  can   generate  our  behavioral  capacity?  That’s  where  the  Turing  Test  (TT)  comes  in   (Harnad  2008):  A  model  can  generate  our  behavioral  capacity  if  it  can  pass  TT  -‐-‐  the   full  robotic  version  of  TT,  not  just  the  verbal  version  (i.e.,  the  ability  to  do  everything   a  human  can  do  in  the  world,  not  just  to  talk  about  it):  A  sensory-‐motor  system  that   could  pass  the  robotic  TT  would  have  to  be  able  to  perform  indistinguishably  from   any  of  us,  for  a  lifetime  .  \nLet’s  set  aside  the  second  kind  of  thing  that  brains  do  -‐-‐  internal  brain  activity  -‐-‐   because  it  is  controversial  how  many  (and  which)  of  the  specific  features  of  brain   activity  are  necessary  either  to  generate  our  behavioral  capacity  or  to  generate   cognition.  It  could  conceivably  turn  out  to  be  true  that  the  only  way  to  successfully   generate  cognition  is  one  that  preserves  some  of    the  dynamic  features  of  brain   activity  (electrochemical  activity,  secretions,  chemistry  etc.).  In  that  case  the  fact  "
    }, {
      "heading" : null,
      "text" : "that  those  observable  (neural)  features  were  missing  from  the  computational  model   of  cognition  would  be  as  visible  as  the  fact  that  motion  was  missing  from  the   computational  model  of  planetary  motion.  Let’s  call  the  hypothesis  that  that  is  true   “neuralism.”  \nIt’s  important  to  understand  that  my  critique  of  the  thesis  that  cognition  is   computation  does  not  rest  on  the  assumption  that  neuralism  is  true.  We  will  discuss   the  issue  of  the  implementation-‐independence  of  computation  in  a  moment,  but  I   don’t  want  to  suggest  that  generating  cognition  depends  on  a  requirement  of   neurosimilitude  (necessarily  preserving  some  of  the  dynamic  properties  of  the   brain).  Neurosimilitude  would  be  needed  in  order  to  explain  how  the  brain  works,   but  not  necessarily  in  order  to  explain  either  how  to  generate  the  brain’s  behavioral   capacity  or  to  explain  how  to  generate  cognition.  \nConsider  behavioral  capacity  first:  Let  us  agree  at  once  that  whatever  model  we   build  that  succeeds  in  generating  our  actual  behavioral  capacity  –  i.e.,  the  power  to   pass  the  full  robotic  version  of  TT,  for  a  lifetime  –  would  definitely  have  explained   our  behavioral  capacity  (1),  fully  and  causally,  regardless  of  whether  it  did  it  via   secretions  or  computations.  If  it  were  doing  it  computationally,  by  implementing  an   algorithm,  it’s  clear  that  it  would  also  need  a  robotic  body,  with  sensors  and  moving   parts,  and  that  those  could  not  be  just  the  implementations  of  algorithms.    \nSensing,  like  movement  (and  flying,  and  digestion),  is  not  implementation-‐ independent  symbol-‐manipulation.  Consider  the  question  of  whether  there  could  be   a  successful  TT-‐passing  robot  that  consisted  of  nothing  other  than  (i)  sensors  and   movable  peripheral  parts  plus  (ii)  a  functional  “core”  within  which  all  the  work   (other  than  the  I/O  itself)  was  being  done  by  an  independent  computational  module   that  mirrored  the  causal  structure  of  the  brain  (or  of  any  other  system  capable  of   passing  the  TT).  This  really  boils  down  to  the  question  of  whether  the  causal  link-‐up   between  our  sensory  and  motor  systems,  on  the  one  hand,  and  the  rest  of  our   nervous  system,  on  the  other,  can  really  be  split  into  two  autonomous  modules  -‐-‐  a   peripheral  sensorimotor  one  that  was  necessarily  noncomputational,  plus  a  central   one  that  was  computational.  \nI  cannot  answer  the  question  of  whether  such  a  causal  split  is  possible  or  makes   sense.  To  me  it  seems  just  as  unlikely  as  that  we  could  divide  heart  function  into  a   noncomputational  I/O  module  feeding  into  and  out  of  a  computational  core.  I  think   sensorimotor  function  is  what  the  brain  does  through  and  through,  and  that  the   intuition  of  a  brain-‐in-‐a-‐vat  receiving  its  I/O  from  the  world  –  the  intuition  from   which  the  computational-‐core-‐in-‐a-‐vat  intuition  derives  -‐-‐  is  both  unrealistic  and   homuncular.  But  let  us  agree  that  the  possibility  of  this  functional  partition  is  an   empirical  question,  insofar  as  what  it  takes  to  generate  our  behavioral  capacity  is   concerned.  If  TT  could  not  be  passed  by  a  computational  core  plus  I/O  peripherals   then  the  cognitive  computationalism  fails.  But  if  TT  could  be  successfully  passed  by   a  computational  core  plus  I/O  peripherals,  would  that  entail  that  cognitive   computationalism  was  correct  after  all?  "
    }, {
      "heading" : null,
      "text" : "This  is  the  point  to  remind  ourselves  that  we’ve  left  out  the  third  burden  of  cognitive   theory,  in  addition  to  behavioral  capacity  (1)  (and  brain  function  (2),  which  we’ve   agreed  to  ignore):  Even  if  we  define  cognition  as  whatever  it  takes  to  generate  TT   capacity,  there  is  something  the  TT  leaves  out,  something  invisible  to  everyone   except  the  cognizer,  namely,  consciousness:  it  feels  like  something  to  cognize  (3).  But   that  property,  unlike  movement  or  secretions,  cannot  be  perceived  by  anyone  other   than  the  cognizer  himself.  And  I  think  it  is  this  invisibility  of  cognition  that  is  the  real   reason  for  the  error  of  confusing  the  computational  implementation  of  the  causal   structure  of  cognition  with  the  causal  structure  of  cognition  itself:  It  looks  from  the   outside  as  if  there  is  no  difference  between  what  is  going  on  inside  a  computational   model  of  cognition  and  what  is  going  on  inside  the  brain  of  a  cognizer.  And  it  is  for   that  reason  that  computation  alone  looks  like  a  viable  candidate  for  instantiating,   rather  than  merely  explaining  cognition.  \nIt  is  not  that  David  is  not  aware  of  this  distinction.  He  writes:  “Psychological   properties  are  concerned  with  the  sort  of  thing  the  mind  does,  and  phenomenal   properties  are  concerned  with  the  way  it  feels.”  But  he  thinks  his  “dancing  qualia”   argument  shows  that  feeling  must,  like  computation  itself,  be  an  implementation-‐ independent  property,  present  in  every  implementation  of  the  algorithm  that   successfully  captures  the  right  causal  structure,  no  matter  how  radically  the   implementations  differ.  For  if  we  hypothesize  that  there  could  be  two  physically   different  but    causally  invariant  implementations  of  the  same  causal  structure,  one   feeling  one  way  and  the  other  feeling  another  way  (or  not  feeling  at  all),  both   variants  implemented  within  the  same  hardware  so  that  one  could  throw  a  switch  to   flip  from  one  implementation  variant  to  the  other,  the  fact  that  the  causal  structure   was  the  same  for  both  variants  would  prevent  the  hypothetical  difference  in  feeling   from  being  felt.  So  the  causal  invariance  would  guarantee  the  feeling  invariance.  \nBut  the  trouble  with  this  argument  is  that  it  takes  for  granted  the  fact  that  feeling,   unlike  movement,  is  invisible.  The  reason  the  flip/flop  thought  experiment  does  not   guarantee  that  all  implementations  of  the  causal  structure  of  the  solar  system  or  the   heart  move  and  beat,  respectively,  is  that  moving  and  beating  are  not  computational   properties,  and  we  can  see  that  (it  is  empirically  “observable”).  In  the  case  of  feeling,   the  reason  this  very  same  distinct  possibility  is  not  equally  evident  is  that  the  only   one  feeling  the  feeling  (or  not-‐feeling  it,  as  the  case  may  be)  is  the  cognizer.  But   unless  we  are  prepared  to  declare  feeling  to  be  identical  with  observable  behavioral   capacity    by  definition  -‐-‐  which  would  not  only  be  a  rather  ad  hoc  and  retro  move,   but  would  also  be  tantamount  to  declaring  cognitive  computationalism  to  be  true  by   definition  as  well  –  we  have  to  allow  for  the  possibility  that  feeling,  like  moving  or   flying,  is  not  an  implementation-‐independent  computational  property  but  a   dynamical,  implementation-dependent  one.  For  then  flipping  from  one  causally   invariant  implementation  to  another  could  be  like  flipping  between  two  causally   invariant  implementations  of  planetary  motions,  one  that  moves  and  one  that  does   not.  "
    }, {
      "heading" : null,
      "text" : "“Demoting”  feeling  to  a  dynamical  property,  its  presence  or  absence  marked  by   conformity  with  the  right  differential  equations  rather  than  the  right  computer   program,  releases  feeling  from  having  to  be  an  implementation-‐independent   computational  property.  \nBut  does  cognition  have  to  be  felt  cognition?  Here  David  has  a  distinction  that  –   unlike  the  distinction  between  sensorimotor  peripherals  and  computational  core  –   marks  a  solid  empirical  difference:  The  problem  of  explaining  behavioral  capacity   (doing)  is  “easy”  to  solve.  Cognitive  science  is  nowhere  near  solving  it,  but  there   seem  to  be  no  principled  obstacles.  The  problem  of  explaining  why  and  how  we  feel   (the  “mind/body  problem”),  in  contrast,  is  a  hard  to  solve,  perhaps  even  impossible   (Harnad  2000).  \nSo  there’s  something  to  be  said  for  concentrating  on  solving  the  “easy”  problem  of   explaining  how  to  pass  the  TT  first  (Harnad  &  Scherzer  2008).    I  think  it  is  unlikely   that  the  solution  will  be  a  purely  computational  one.  It  is  more  likely  that  the  causal   mechanism  that  succeeds  in  passing  TT  will  be  hybrid  dynamic/computational  –   and  that  the  dynamics  will  not  be  those  of  the  hardware  implementation  of  the   computation  (those  really  are  irrelevant  to  cognition).  The  dynamics  will  be  those  of   the  sensori-‐motor  surface  –  plus  all  the  internal  topographic  (spatial  shape-‐ preserving)  dynamics  in  between  (Silver  &  Kastner  2009).  That  –  and  not   computation  alone  –  will  be  the  “causal  topography”  of  cognition.    \nBut  once  that  (“easy”)  problem  is  solved,  only  the  TT-‐passing  system  itself  will  know   whether  it  really  does  cognize  –  i.e.,  whether  it  really  feels  like  something  to  be  that   TT-‐passer.  (Being  able  to  determine  that  through  empirical  observation  would   require  solving  the  (insoluble)  other-‐minds  problem.)  And  even  if  we  had  a   guarantee  from  the  gods  that  the  TT-‐passer  really  cognizes,  that  still  would  not   explain  the  causal  role  of  the  fact  that  the  TT-‐passer  feels.  The  causal  role  of  feeling   in  the  causal  topography  of  cognition  will  continue  to  defy  explanation  even  if   feeling  is  really  going  on  in  there  -‐-‐  probably  because  there  is  no  more  explanatory   “room”  left,  in  a  causal  explanation,  once  all  the  relevant  dynamics  are  taken  into   account  (Harnad  2011).  \nSo  not  only  is  it  unlikely  that  implementing  the  right  computations  will  generate   cognition,  but  whatever  it  is  that  does  generate  cognition  -‐-‐  whether   computationally,  dynamically,  or  via  a  hybrid  combination  of  both  –  will  not  explain   the  causal  sole  of  consciousness  in  cognition.  And  that  problem  may  not  just  be   “hard,”  but  insoluble.  \nReferences  \nChalmers,  D.  (2012)  A  Computational  Foundation  for  the  Study  of  Cognition.  Journal   of  Cognitive  Science.  http://consc.net/papers/computation.html    \nHarnad,  S.  (1994)  Computation  Is  Just  Interpretable  Symbol  Manipulation:   Cognition  Isn't.  Special  Issue  on  \"What  Is  Computation\"  Minds  and  Machines  4:379-‐ 390  http://cogprints.org/1592/  "
    }, {
      "heading" : null,
      "text" : "Harnad,  S.  (2000)  Correlation  vs.  Causality:  How/Why  the  Mind/Body  Problem  Is   Hard.  [Invited  Commentary  of  Humphrey,  N.  \"How  to  Solve  the  Mind-‐Body   Problem\"]  Journal  of  Consciousness  Studies  7(4):  54-‐61.  http://cogprints.org/1617/  \nHarnad,  S.  (2008)  The  Annotation  Game:  On  Turing  (1950)  on  Computing,   Machinery  and  Intelligence.  In:  Epstein,  Robert  &  Peters,  Grace  (Eds.)  Parsing  the   Turing  Test:  Philosophical  and  Methodological  Issues  in  the  Quest  for  the  Thinking   Computer.  Springer    http://eprints.ecs.soton.ac.uk/7741/        Harnad,  S.  and  Scherzer,  P.  (2008)  First,  Scale  Up  to  the  Robotic  Turing  Test,  Then   Worry  About  Feeling.    Artificial  Intelligence  in  Medicine  44(2):  83-‐89       http://eprints.ecs.soton.ac.uk/14430/      Harnad,  S.  (2011)  Doing,  Feeling,  Meaning  And  Explaining.  In:  On  the  Human.   http://eprints.ecs.soton.ac.uk/22243/  \nSearle,  John  R.  (1980)  Minds,  brains,  and  programs.  Behavioral  and  Brain  Sciences  3   (3):  417-‐57  http://cogprints.org/7150/    \nSilver,  Michael  A.  &    Kastner,  Sabine  (2009)  Topographic  maps  in  human  frontal  and   parietal  cortex.  Trends  in  Cognitive  Sciences  13  (11):  488–495  \nSilver,  Michael  A.  &    Kastner,  Sabine  (2009)  Topographic  maps  in  human  frontal  and   parietal  cortex.  Trends  in  Cognitive  Sciences  13  (11):  488–495   http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2767426/    \n  \n  "
    } ],
    "references" : [ {
      "title" : "A Computational Foundation for the Study of Cognition",
      "author" : [ "D. Chalmers" ],
      "venue" : "Journal of Cognitive Science",
      "citeRegEx" : "Chalmers,? \\Q2012\\E",
      "shortCiteRegEx" : "Chalmers",
      "year" : 2012
    }, {
      "title" : "Correlation vs. Causality: How/Why the Mind/Body Problem Is Hard. [Invited Commentary of Humphrey, N. \"How to Solve the Mind-­‐Body Problem\"] Journal of Consciousness Studies",
      "author" : [ "S. Harnad" ],
      "venue" : null,
      "citeRegEx" : "Harnad,? \\Q2000\\E",
      "shortCiteRegEx" : "Harnad",
      "year" : 2000
    }, {
      "title" : "Topographic maps in human frontal and parietal cortex",
      "author" : [ "Silver", "Michael A", "Kastner", "Sabine" ],
      "venue" : "Trends in Cognitive Sciences",
      "citeRegEx" : "Silver et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "David\t\r  Chalmers’s\t\r  (2012)\t\r  thesis\t\r  on\t\r  cognition\t\r  and\t\r  computation\t\r  is\t\r  either\t\r  true\t\r  but trivial\t\r  or\t\r  nontrivial\t\r  but\t\r  false.",
      "startOffset" : 9,
      "endOffset" : 30
    } ],
    "year" : 2012,
    "abstractText" : "The\t\r  causal\t\r  structure\t\r  of\t\r  cognition\t\r  can\t\r  be\t\r  simulated\t\r  but\t\r  not\t\r  implemented computationally,\t\r  just\t\r  as\t\r  the\t\r  causal\t\r  structure\t\r  of\t\r  a\t\r  comet\t\r  can\t\r  be\t\r  simulated\t\r  but\t\r  not\t\r  implemented computationally.\t\r  The\t\r  only\t\r  thing\t\r  that\t\r  allows\t\r  us\t\r  even\t\r  to\t\r  imagine\t\r  otherwise\t\r  is\t\r  that\t\r  cognition, unlike\t\r  a\t\r  comet,\t\r  is\t\r  invisible\t\r  (to\t\r  all\t\r  but\t\r  the\t\r  cognizer).",
    "creator" : "Microsoft Word"
  }
}