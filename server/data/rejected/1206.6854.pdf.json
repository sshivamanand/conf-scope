{
  "name" : "1206.6854.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Belief Update in CLG Bayesian Networks With Lazy Propagation",
    "authors" : [ "Anders L Madsen" ],
    "emails" : [ "Anders.L.Madsen@hugin.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In recent years Bayesian networks (BNs) with a mixture of continuous and discrete variables have received an increasing level of attention. We present an architecture for exact belief update in Conditional Linear Gaussian BNs (CLG BNs). The architecture is an extension of lazy propagation using operations of Lauritzen & Jensen [6] and Cowell [2]. By decomposing clique and separator potentials into sets of factors, the proposed architecture takes advantage of independence and irrelevance properties induced by the structure of the graph and the evidence. The resulting benefits are illustrated by examples. Results of a preliminary empirical performance evaluation indicate a significant potential of the proposed architecture."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "The framework of BNs is an efficient knowledge representation for reasoning under uncertainty [12, 3, 4]. Traditionally, the variables of a BN are assumed to be either discrete or continuous. In recent years BNs with a mixture of continuous and discrete variables have received an increasing level of attention. Here exact belief update in CLG BNs is considered.\nExtending the class of BNs containing discrete (or continuous) variables only to the class of BNs containing both discrete and continuous variables is not simple. The work by Pearl [12] on BNs containing continuous variables imposed three constraints on the variables in the network. The interaction between variables is linear, the sources of uncertainty are Gaussian distributed and uncorrelated, and the causal network is singly connected. Later, Shachter & Kenley [14] described how to solve Gaussian influence diagrams\nunder similar constraints, but allowing multiple connected causal networks.\nLauritzen [5] presents a scheme for modeling and exact belief update in CLG BNs. This scheme is more general than the scheme proposed by Pearl. The conditional distribution of the continuous variables given the discrete variables is assumed to be multivariate Gaussian. Only continuous variables which are linear additively Gaussian distributed are considered. The asymmetry between continuous and discrete variables induces a number of constraints on the model specification and the inference structure. Using a similar approach Chang & Fung [1] extend the SPI algorithm [13] to solve arbitrary queries against CLG BNs.\nThe Lauritzen [5] architecture is known to suffer from problems causing numerical instability. For this reason the architecture was later revised by Lauritzen & Jensen [6] in order to improve the numerical stability of belief update. Recently, Cowell [2] introduced an alternative architecture for belief update based on message passing in an elimination tree using the arcreversal operation of Shachter & Kenley [14] (referred to as the EXCHANGE operation). By performing message passing in an elimination tree the need for complex matrix operations is eliminated.\nWe introduce a new architecture for belief update in CLG BNs. The architecture is an extension of lazy propagation [10] based on operations introduced by Lauritzen & Jensen [6] and Cowell [2]. Belief update proceeds by message passing in a strong junction tree structure where messages are computed using arc-reversal and EXCHANGE operations. The EXCHANGE operation is extended to eliminate discrete variables by arc-reversal. Variables are eliminated using a sequence of EXCHANGE operations and barren variable removals. Posterior marginal distributions are computed using EXCHANGE and PUSH [6] operations.\nWe investigate the computational efficiency of the proposed architecture by comparing its performance on a\nnumber of randomly generated CLG BNs with the performance of the Lauritzen & Jensen [6] architecture as implemented in the HUGIN Decision Engine, i.e., the inference engine of the HUGIN tools. Furthermore, we analyze the performance of various steps of belief update such as computing posterior distributions."
    }, {
      "heading" : "2 PRELIMINARIES AND NOTATION",
      "text" : ""
    }, {
      "heading" : "2.1 CLG BAYESIAN NETWORK",
      "text" : "A CLG BN N = (X,G,P,F) over variables X consists of an acyclic, directed graph (DAG) G = (V, E), a set of conditional probability distributions P = {P(X| π(X)) : X ∈ ∆}, and a set of CLG density functions F = {f(Y | π(Y)) : Y ∈ Γ } where ∆ is the set of discrete variables and Γ is the set of continuous variables such that X = ∆ ∪ Γ . The vertices V of G correspond one to one with the variables of X. CLG BN N induces a multivariate normal mixture density over X on the form:\nP(∆) · f(Γ |∆) = ∏\nX∈∆\nP(X| π(X)) · ∏\nY∈Γ\nf(Y | π(Y)),\nwhere π(X) is the set of variables corresponding to the parents of the vertex representing X in G.\nLet Y ∈ Γ with I = π(Y) ∩ ∆ and Z = π(Y) ∩ Γ , then Y has a CLG distribution if:\nL(Y |I = i, Z = z) = N(α(i) + β(i)z, σ2(i)), (1)\nwhere the mean value of Y depends linearly on the values of the continuous parent variables Z, while the variance is independent of Z. In (1), α(i) is a table of real numbers, β(i) is a table of |Z|-dimensional vectors, and σ2(i) is a table of non-negative values.\nEvidence on a variable X ∈ ∆ is assumed to be an instantiation, i.e., X = x. Evidence on a variable Y ∈ Γ is an assignment of a value y to Y, i.e., Y = y. We let ǫ∆ and ǫΓ denote the set of evidence on variables of ∆ and Γ , respectively, such that ǫ = ǫ∆ ∪ ǫΓ .\nDefinition 2.1 [Barren Variable] A variable X is a barren variable w.r.t. a set of variables T , evidence ǫ, and DAG G, if X 6∈ T , X 6∈ ǫ and X only has barren descendants in G (if any)."
    }, {
      "heading" : "2.2 THE EXCHANGE OPERATION",
      "text" : "Let Y, Z ∈ Γ with parent sets π(Z) = {Z1, . . . , Zn} ⊆ Γ and π(Y) = {Z,Z1, . . . , Zn} ⊆ Γ such that:\nY |Z,Z1, . . . , Zn ∼ N(αY + βZZ +\nn∑\ni=1\nβiZi, σ 2 Y),\nZ|Z1, . . . , Zn ∼ N(αZ +\nn∑\ni=1\nδiZi, σ 2 Z).\nThe EXCHANGE operation is essentially Bayes’ theorem. It converts the above pair of distributions such that Y becomes a parent of Z in the domain graph spanned by the two distributions maintaining the same joint probability density function of the original pair [2]. Graphically speaking the EXCHANGE operation corresponds to arc-reversal in the domain graph. The distribution of Y after EXCHANGE is:\nY |Z1, . . . , Zn ∼\nN(αY + βZαZ +\nn∑\ni=1\n(βi + βZδi)Zi, σ 2 Y + β 2 Zσ 2 Z),\nwhile the distribution of Z is (Cowell [2] considers three different cases depending on the values of σ2Y and σ2Z that are mathematical limits of this case):\nZ|Y, Z1, . . . , Zn ∼ N\n(\nρ\nσ2Y + β 2 Zσ 2 Z\n, σ2Zσ 2 Y\nσ2Y + β 2 Zσ 2 Z\n)\n,\nwhere\nρ = αZσ 2 Y−αYβZσ 2 Z+\nn∑\ni=1\n(δiσ 2 Y−βiβZσ 2 Z)Zi+βZσ 2 ZY.\nIt is straightforward to extend the EXCHANGE operation to handle discrete variables. Let Xi, Xj ∈ ∆ with parent sets π(Xi) = {X1, . . . , Xn} ⊆ ∆ and π(Xj) = {Xi, X1, . . . , Xn} ⊆ ∆ such that p(Xj |Xi, X1, . . . , Xn) and p(Xi |X1, . . . , Xn) are the corresponding probability potentials of Xi and Xj, respectively. In the discrete case the EXCHANGE operation is also essentially Bayes’ theorem. That is, the EXCHANGE operation converts the above pair of potentials such that Xj becomes a parent of Xi in the domain graph spanned by the two potentials maintaining the same joint probability potential of the original pair:\np(Xj |X1, . . . , Xn) = ∑\nXi\np(Xj |Xi, X1, . . . , Xn)p(Xi |X1, . . . , Xn),\np(Xi |Xj, X1, . . . , Xn) =\np(Xj |Xi, X1, . . . , Xn)p(Xi |X1, . . . , Xn)\np(Xj |X1, . . . , Xn) .\nGraphically speaking the EXCHANGE operation corresponds to arc-reversal in the domain graph.\nBy construction it is never necessary to apply the EXCHANGE operation to a pair of mixed variables (i.e., a continuous and a discrete variable). Also, prior to applying the EXCHANGE operation on a pair of adjacent variables we make sure that the two variables share the same set of parents. This is achieved by straightforward domain extensions."
    }, {
      "heading" : "2.3 THE STRONG JUNCTION TREE",
      "text" : "Belief update is performed by message passing in a strong junction tree T = (C, S) with cliques C, separators S and strong root R ∈ C. T has the property that for all adjacent cliques A and B with A closer to R than B, it holds that S = A∩B ⊆ ∆ or B\\A ⊆ Γ . Let A and B be adjacent cliques with A closer to R than B and such that S = A ∩ B. Then, A is referred to as the parent clique of B (denoted πC(B)) and S is referred to as the parent separator of B (denoted πS(B)).\nA clique C ∈ C is referred to as a boundary clique if C∩ Γ 6= ∅ and either B ⊆ ∆ or B ∩ Γ is instantiated by evidence where B = πC(C). Let bd(C) denote the set of boundary cliques."
    }, {
      "heading" : "3 LAZY PROPAGATION",
      "text" : "A junction tree for a discrete BN is by construction wide enough to support the computation of any posterior marginal given any subset of evidence. The junction tree is, however, often too wide to take advantage of independence properties induced by evidence. Lazy propagation aims at taking advantage of independence and irrelevance properties induced by evidence in a Shenoy-Shafer message passing scheme [10]. In Lazy propagation belief update proceeds by message passing in a junction tree maintaining decompositions of clique and separator potentials."
    }, {
      "heading" : "3.1 POTENTIALS AND OPERATIONS",
      "text" : "Definition 3.1 [Potential] A potential on W ⊆ X is a pair πW = (P,F) where P is a set of (discrete) probability potentials on subsets of W and F is a set of probability density functions on subsets of W ∩ Γ conditional on subsets of W ∩ ∆.\nElements of P are referred to as factors and elements of F as density functions (or densities). Furthermore, we call a potential πW vacuous if πW = (∅, ∅). The vacuous potential is denoted π∅.\nDefinition 3.2 [Combination] The combination of two potentials πW1 = (P1,F1) and πW2 = (P2,F2) denotes the potential onW1∪W2 given by πW1 ⊗ πW2 = (P1 ∪ P2,F1 ∪ F2).\nNotice that potential combination is set union.\nDefinition 3.3 [Contraction] The contraction c(πW) of a potential πW = (P,F) is the non-negative function on W given by:\nc(πW) = ∏\np∈P\np · ∏\nf∈F\nf.\nWe define the contraction of π∅ as c(π∅) = 1.\nDefinition 3.4 [Projection] The projection of a potential πW = (PW ,FW) to a subset U ⊆ W denotes the potential πU = π ↓U W = (PU,FU) on U obtained by performing a sequence of EXCHANGE operations and barren variable removals eliminating variables of W \\U.\nIn projection continuous variables are eliminated before discrete variables. Notice that the head of any factor or density will consists of a single variable or a single piece of evidence. If a variable X is barren, then X and its factor or density may be removed without further computation."
    }, {
      "heading" : "3.2 INITIALIZATION",
      "text" : "The first step in initialization of T = (C, S) is to associate π∅ with each clique C ∈ C. Next, for each X ∈ ∆, we assign P(X| π(X)) ∈ P, to the clique C closest to R such that fa(X) ⊆ C where fa(X) = π(X) ∪ {X}. Similarly, for each Y ∈ Γ . After initialization each clique C holds a potential πC = (P,F). The joint potential πX on T = (C, S) is therefore:\nπX = ⊗\nC∈C\nπC =\n(\n⋃\nX∈∆\n{P(X| π(X))}, ⋃\nY∈Γ\n{f(Y | π(Y))}\n)\n.\nThe contraction of the joint potential πX is:\nc(πX) = c( ⊗\nC∈C\nπC) = ∏\nX∈∆\nP(X| π(X)) · ∏\nY∈Γ\nf(Y | π(Y)).\nEvidence ǫ∆ is inserted as part of initialization while evidence ǫΓ is inserted during message passing.\nExample 3.5 Figure 1 shows a CLG BN over variables Yi ∈ Γ for i =\n1, . . . , 4 and Xj ∈ ∆ for j = 1, 2, 3 and its strong junction tree T. After initialization the clique potentials are:\nπC1 = ({P(X1), P(X2), P(X3)}, {f(Y1 |X1)}), πC2 = (∅, {f(Y2 |X2)}), πC3 = (∅, {f(Y3 |X3)}), πC4 = (∅, {f(Y4 |Y1, Y2, Y3)}).\nThe domain of each factor in any clique potential consists of a single variable. This is contrary to both the Lauritzen & Jensen [6] and Cowell [2] architectures where each clique has a probability potential over all discrete variables in the clique. This representation is storage demanding when Y4 has additional parents each having a single discrete variable as parent and when the discrete variables have many states.\nThe above example illustrates the structure of a set of CLG BNs used in production by a commercial customer. In this application a large part of the discrete variables are observed making the present inference scheme very efficient on this type of network."
    }, {
      "heading" : "3.3 PROPAGATION",
      "text" : "Propagation of information in T proceeds by message passing via the separators S. The separator S = A ∩ B between two adjacent cliques A and B stores the messages passed between A and B, see Figure 2.\nMessages are passed from leaf cliques toward R by recursively letting each clique A pass a message to its parent B whenever A has received a message from each C ∈ adj(A) \\ {B} (COLLECT). Messages are, subsequently, passed in the opposite direction (DISTRIBUTE). DISTRIBUTE is performed from the root to boundary cliques."
    }, {
      "heading" : "3.4 MESSAGES",
      "text" : "The message πA→B is passed from A ∈ C to B ∈ adj(A) by absorption. Absorption fromA to B involves eliminating the variables A \\ B from the combination of the potential associated with A and the messages passed to A from each neighbor except B. The message πA→B is computed as:\nπA→B = ( πA ⊗ ( ⊗C∈adj(A)\\{B}πC→A ))↓B ,\nwhere πC→A is the message passed from C to A and ↓ is the projection operation based on EXCHANGE operations and barren variable removals."
    }, {
      "heading" : "3.5 THE PUSH OPERATION",
      "text" : "A strong junction tree representation T of a CLG BN is not always wide enough to support the insertion of evidence on any continuous variable or the calculation of any posterior marginal density function. If the junction tree is not wide-enough to support a calculation, then the PUSH operation is used [6].\nThe marginal density of a variable Y ∈ Γ is, in general, a mixture of Gaussian distributions. In order to compute the marginal mixture of Y, it may be necessary to (temporarily) rearrange the content of cliques and separators of T. The PUSH operation is applied in order to rearrange T such that Y becomes part of a boundary clique. This is achieved by extending cliques and separators to include Y and collecting Y towards R.\nAssume A is the clique closest to R such that Y ∈ A, A 6∈ bd(C), B = πC(A), and S = πS(A), see Figure 2. The PUSH operation extends S and B to include Y. In the process any continuous variable Z ∈ T(f) such that Z 6∈ S is eliminated from the density f of Y where T(f) is the tail of f, i.e., the set of conditioning variables. The process of eliminating tail variables not in S is repeated recursively until T(f) ⊆ S. The resulting density f is associated with πB and πA→B.\nThe PUSH operation is applied recursively on the parent clique until Y becomes part of a boundary clique."
    }, {
      "heading" : "3.6 INSERTION OF CONTINUOUS EVIDENCE",
      "text" : "Let Y ∈ Γ be instantiated by evidence ǫY = {Y = y}, let f(Y | π(Y)) be the density function for Y given π(Y) and let C be the clique to which f(Y | π(Y)) is associated. Assume Y has only discrete parents, if any, i.e., I = π(Y) ⊆ ∆. Insertion of evidence ǫY produces a factor p(y|I) such that:\np(y|I = i) = exp\n(\n−(y − αY(i)) 2/(2σ2(i))\n)\n√ 2πσ2(i) ,\nwhere we assume σ2Y(i) > 0 for all i [6, 2]. The clique potential πC = (P,F) is updated such that π ∗ C = (P ∪ {p},F \\ {f}). If σ2(i) = 0, insertion of evidence may be undefined, see [2] who cites [6].\nIf π(Y) 6⊆ ∆, then a sequence of PUSH operations are performed in order to compute the marginal density function for Y. The density f of Y is pushed to the boundary clique. Subsequently, evidence ǫY is inserted as described above. This implies that the insertion of evidence on a continuous variable may change\nthe content of clique and separator potentials. This occurs when it is necessary to apply the PUSH operation in order to insert ǫY . Finally, Y is instantiated in all density functions where Y is a tail variable.\nNotice that bd(C) may change when ǫY is inserted.\nExample 3.6 Consider the simple CLG BN and its corresponding junction tree T shown in Figure 3. After initialization the clique potentials are:\nπC1 = ({P(X)}, {f(Y1 |X)}), πC2 = (∅, {f(Y2 |Y1}).\nAssume evidence ǫ = {Y2 = y2}. Since the tail of f(Y2 |Y1) is continuous and a subset of the parent separator, it is necessary to apply the PUSH operation on Y2 in order to insert ǫ into T.\nFirst the density f(Y2 |Y1) is pushed to the parent clique, next an EXCHANGE operation is performed on the arc (Y1, Y2). Next, densities including Y2 in the domain are instantiated to reflect the evidence. Once the PUSH operation completes the revised clique potentials are:\nπ∗C1 = ({P(X), p(y2 |X)}, {f(Y1 |X, y2)}), π∗C2 = (∅, ∅).\nThis completes the insertion of evidence ǫ."
    }, {
      "heading" : "3.7 PROPAGATION OF CONTINUOUS EVIDENCE",
      "text" : "Section 3.3 describes the propagation scheme used when ǫΓ = ∅. When ǫΓ 6= ∅, the recursive message passing scheme is terminated at each boundary clique. Once each boundary clique A ∈ bd(C) has received messages from each C ∈ adj(A) \\ {πC(A)}, continuous evidence is inserted using the PUSH operation.\nLet T = (C, S) be a strong junction tree representation and let ǫ = ǫ∆ ∪ ǫΓ be the evidence to propagate. The evidence ǫ is propagated in T by performing the following sequence of steps:\n1. Initialization including insertion of evidence ǫ∆.\n2. At each A ∈ bd(C) COLLECT from every B ∈ adj(A) \\ {πC(A)}.\n3. Insert evidence ǫΓ using the PUSH operation.\n4. Perform in sequence a COLLECT and a DISTRIBUTE operation on R.\nDuring the COLLECT operation performed in step 4 messages are passed from the boundary cliques to R. Thus, the effect of steps 2 and 4 is that two messages have been passed between each pair of adjacent cliques on any path between the root R and a boundary clique. No messages are passed from boundary cliques to leave cliques.\nThe architectures described in [2], [6], and [9] each does a full propagation over all the nodes of the computation tree prior to inserting ǫ whereas we do only a partial COLLECT prior to inserting ǫ∆."
    }, {
      "heading" : "3.8 POSTERIOR MARGINALS",
      "text" : "The posterior marginal P(X|ǫ) for X ∈ ∆ may be computed from any clique or separator containing X. Since ǫΓ is incorporated using PUSH operations, no Y ∈ Γ needs to be eliminated in the process of computing P(X|ǫ). If X ∈ C, then P(X|ǫ) is computed as:\nP(X|ǫ) ∝ ∑\nC\\{X}\nc(πC) = ∑\nC\\{X}\n∏\np∈PC\np · ∏\nf∈FC\nf\n= ∑\nC\\{X}\n∏\np∈PC\np,\nwhere πC = (PC,FC) is the clique potential for C. On the other hand, if S is a separator containing X with adjacent cliques A and B, then P(X|ǫ) is computed as:\nP(X|ǫ) ∝ ∑\nS\\{X}\nc(πA→B ⊗ πB→A)\n= ∑\nS\\{X}\n∏\np∈PA→B∪PB→A\np · ∏\nf∈FA→B∪FB→A\nf\n= ∑\nS\\{X}\n∏\np∈PA→B∪PB→A\np,\nwhere potential πA→B = (PA→B,FA→B) and potential πB→A = (PB→A,FB→A) are the potentials passed from A to B and from B to A, respectively.\nThe posterior mixture for Y ∈ Γ is computed using PUSH operations followed by a projection of the relevant boundary clique to Y and a contraction.\nExample 3.7 The prior mixture densities of Y1 and Y2 of the CLG BN shown in Figure 4 are:\nf(Y1) = ∑\nx1∈X1\nP(x1) · f(Y1 |x1),\nf(Y2) = ∑\nx1∈X1,x2∈X2\nP(x1)P(x2 |x1) · f(Y2 |x1, x2).\nThe density for Y1 has only ‖X1‖ components. This is a reduction compared to the Lauritzen & Jensenand Cowell architectures where the marginal density will have ‖X1‖ · ‖X2‖ components. The reduction is due to the decomposition of clique and separator potentials.\nExample 3.8 Consider again Figure 1 of Example 3.5. The number of components in the mixture marginal for Y4 is ‖X1‖ · ‖X2‖ · ‖X3‖ whereas the number of components in the mixture marginal for Yi is equal to ‖Xi‖. This is a reduction compared to the Lauritzen & Jensenand Cowell architectures where the number of components is ‖X1‖ ·‖X2‖ ·‖X3‖. Hence, in the case of a larger number of variables (and same structure), the storage and time reduction can be significant."
    }, {
      "heading" : "4 COMPARISON",
      "text" : ""
    }, {
      "heading" : "4.1 COWELL",
      "text" : "Cowell [2] presents an algorithm for belief update where message passing proceeds on an elimination tree rather than a (strong) junction tree. This produces a local propagation scheme in which all calculations involving continuous variables are performed by manipulating univariate regressions (avoiding matrix operations) such that continuous variables are eliminated using EXCHANGE operations.\nThe three main differences between the present propagation scheme and Cowell [2] are: use of a strong junction tree as opposed to an elimination tree, use of EXCHANGE to eliminate both continuous and discrete variables and a single round of message passing."
    }, {
      "heading" : "4.2 LAURITZEN AND JENSEN",
      "text" : "The architecture of Lauritzen & Jensen [6] performs belief update by message passing in a strong junction tree architecture. A CG potential [8] is associated with each clique and separator. A CG potential consists of a probability potential over discrete variables and a probability density function over continuous variables conditional on the discrete variables. Each clique and\nseparator has a CG potential over its variables. This implies that complex matrix operations are required during belief update.\nInitialization plays an important role in the Lauritzen & Jensen [6] architecture. It produces a Lauritzen & Spiegelhalter-like junction tree representation [7] where clique potentials are conditioned on the continuous variables of the parent separator. This ensures that a variable Y ∈ Γ is only propagated when inserting evidence on Y or when computing the mixture marginal for Y. Furthermore, a complex recursive combination operator may be required during initialization in order to combine CG potentials. The need for conditioning, recursive combination, and complex matrix operations is eliminated in both the Cowell [2] and the present architectures.\nExample 4.1 Figure 5 shows a CLG BN and its junction tree T. The initial clique potentials are:\nπC1 = ({P(X1)}, {f(Y1 |X1), f(Y3 |X1, Y1, Y2)}), πC2 = (∅, {f(Y2 |Y1, Y4), f(Y4)}).\nIn the Lauritzen & Jensen [6] architecture initialization of T requires a recursive combination operation.\nIn the proposed architecture initialization amounts to associating probability distributions and densities with cliques of T. The prior distribution of each variable is readily computed using the EXCHANGE operation.\nThe Lauritzen & Jensen [6] architecture calculates weak marginals during DISTRIBUTE. This is not the case for the Cowell [2] nor the present architecture."
    }, {
      "heading" : "4.3 MADSEN",
      "text" : "The present architecture is quite different from the architecture proposed by Madsen [9]. The latter architecture is an extension of Madsen & Jensen [11] to the case of CLG BNs based on the propagation scheme of Lauritzen & Jensen [6]. This implies a number of differences when compared to the present scheme. First, the architecture is based solely on the operations of Lauritzen & Jensen [6] whereas the present scheme is based on operations of both Lauritzen & Jensen [6]\nand Cowell [2]. Second, a Lauritzen & Spiegelhalterlike junction tree representation is achieved as the result of initialization, i.e., during the initial COLLECT operation, the sender clique is conditioned on the continuous variables of the parent separator. Finally, in the present scheme variable eliminations are performed using EXCHANGE operations and barren variable removals."
    }, {
      "heading" : "5 PERFORMANCE ANALYSIS",
      "text" : "A preliminary performance analysis on a set of randomly generated CLG BNs has been made. Networks with 25, 50, 75, 100, 125, and 150 variables with different fractions of continuous variables (0, 0.25, 0.5, 0.75, 1) were randomly generated (ten networks of each size). For each network, evidence sets of 0 to 20 instantiated variables were generated and 40 sets of evidence were generated for each size.\nWe compared the performance of the present architecture with the performance of the commercial implementation of the Lauritzen & Jensen [6] architecture in the HUGIN Decision Engine. Table 1 shows statis-\ntics on one of the networks used in the tests (net50-4) where s(C) = ∏ X∈∆∩C ‖X‖ and s(C) = ∑ C∈C s(C). Figure 6 shows the average time cost of belief update in net50-4 whereas Figure 7 shows the average size of the largest discrete configuration. A discrete configuration is either the domain of a factor or the discrete conditioning set of a density. This is an example where the proposed architecture is most efficient.\nThe present architecture maintains a factorization of clique and separator potentials into sets of factors and densities. This decomposition implies that the largest discrete domain size considered during belief update is often significantly smaller than the discrete domain size of the largest clique in the strong junction tree. This insight is supported by the experimental analysis, which indicates that the Lauritzen & Jensen [6] implementation runs out of memory on most networks with 75 or more variables for a large fraction of the evidence sets whereas the present architecture runs out of memory on a much smaller fraction of the evidence\nsets. Figure 7 illustrates how the average largest discrete domain size decreases as |ǫ| increases. Notice that the average largest size is significantly smaller than the size of the largest clique in the strong junction tree.\nFor networks with only discrete or only continuous variables the Lauritzen & Jensen implementation is faster than the implementation of the proposed architecture. However, for some networks with a fraction of 0.25 or 0.5 continuous variables Lauritzen & Jensen is significantly slower than the proposed architecture.\nThe typical decrease in average time cost as |ǫ| increases for lazy propagation is not as significant on CLG BNs. The reason is that computing marginal densities is a dominant and a non-constant factor in the time cost of belief update. A significant amount of the total time for propagating evidence is spent on computing posterior mixture marginals. In the proposed architecture the number and the computational cost\nof PUSH operations is reduced by a decomposition of clique and separator potentials. The significance of the decrease depends on the ratio of continuous variables. Figure 8 shows the average time cost of computing marginals in net50-4. Notice that a significant amount of the time cost originates from computing marginals.\nOn most of the networks considered in the test — where belief update is feasible — the commercial implementation of Lauritzen & Jensen [6] is most efficient (typically networks with less than 75 variables). The HUGIN Decision Engine has significantly more efficient data structures and operations than the implementation of the proposed architecture though.\nThe experiments were performed using a Java implementation on a desktop computer with a 2.2 GHz AMD AthlonTM CPU and 768 MB RAM running Redhat 8."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "An architecture for belief update in CLG BNs based on lazy propagation where messages are computed using EXCHANGE operations and barren variable eliminations has been presented. The architecture is based on extended versions of operations introduced by Lauritzen & Jensen [6] and Cowell [2].\nDespite a significant difference in the efficiency of table operations the proposed architecture is — in some cases — more efficient than a commercial implementation of the Lauritzen & Jensen [6] architecture. The results of the performance evaluation indicate a significant potential of the proposed architecture."
    } ],
    "references" : [ {
      "title" : "Symbolic probabilistic inference with both discrete and continuous  variables",
      "author" : [ "K.C. Chang", "R. Fung" ],
      "venue" : "IEEE Transactions on Systems, Man. and Cybernetics, 25(6):910–916",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Local Propagation In Conditional Gaussian Bayesian Networks",
      "author" : [ "R.G. Cowell" ],
      "venue" : "Journal of Machine Learning Research, 6:1517–1550",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Probabilistic Networks and Expert Systems",
      "author" : [ "R.G. Cowell", "A.P. Dawid", "S.L. Lauritzen", "D.J. Spiegelhalter" ],
      "venue" : "Springer-Verlag",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Bayesian Networks and Decision Graphs",
      "author" : [ "F.V. Jensen" ],
      "venue" : "Springer-Verlag",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Propagation of probabilities",
      "author" : [ "S.L. Lauritzen" ],
      "venue" : "means and variances in mixed graphical association models. Journal of the American Statistical Association, 87(420):1098–1108",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Stable Local Computation with Mixed Gaussian Distributions",
      "author" : [ "S.L. Lauritzen", "F. Jensen" ],
      "venue" : "Statistics and Computing, 11(2):191–203",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Local computations with probabilities on graphical structures and their application to expert systems",
      "author" : [ "S.L. Lauritzen", "D.J. Spiegelhalter" ],
      "venue" : "Journal of the Royal Statistical Society, B., 50(2):157–224",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Graphical models for associations between variables",
      "author" : [ "S.L. Lauritzen", "N. Wermuth" ],
      "venue" : "some of which are qualitative and some quantitative. The Annals of Statistics, 17:31–57",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "All Good Things Come to Those Who Are Lazy - Efficient Inference in Bayesian Networks and Influence Diagrams Based on Lazy Evaluation",
      "author" : [ "A.L. Madsen" ],
      "venue" : "PhD thesis, Department of Computer Science, Aalborg University",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Variations Over the Message Computation Algorithm of Lazy Propagation",
      "author" : [ "A.L. Madsen" ],
      "venue" : "IEEE Transactions on Systems, Man. and Cybernetics Part B",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Lazy propagation: A junction tree inference algorithm based on lazy evaluation",
      "author" : [ "A.L. Madsen", "F.V. Jensen" ],
      "venue" : "Artificial Intelligence, 113(1- 2):203–245",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Probabilistic Reasoning in Intelligence Systems",
      "author" : [ "J. Pearl" ],
      "venue" : "Morgan Kaufmann Publishers",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "B",
      "author" : [ "R. Shachter" ],
      "venue" : "D’Ambrosio, and B. DelFavero. Symbolic probabilistic inference in belief networks. In Proceedings Eighth National Conference on AI, pages 126–131",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Gaussian influence diagrams",
      "author" : [ "R.D. Shachter", "C.R. Kenley" ],
      "venue" : "Management Science, 35(5):527– 549",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1989
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "The architecture is an extension of lazy propagation using operations of Lauritzen & Jensen [6] and Cowell [2].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 1,
      "context" : "The architecture is an extension of lazy propagation using operations of Lauritzen & Jensen [6] and Cowell [2].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "The framework of BNs is an efficient knowledge representation for reasoning under uncertainty [12, 3, 4].",
      "startOffset" : 94,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : "The framework of BNs is an efficient knowledge representation for reasoning under uncertainty [12, 3, 4].",
      "startOffset" : 94,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "The framework of BNs is an efficient knowledge representation for reasoning under uncertainty [12, 3, 4].",
      "startOffset" : 94,
      "endOffset" : 104
    }, {
      "referenceID" : 11,
      "context" : "The work by Pearl [12] on BNs containing continuous variables imposed three constraints on the variables in the network.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 13,
      "context" : "Later, Shachter & Kenley [14] described how to solve Gaussian influence diagrams under similar constraints, but allowing multiple connected causal networks.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "Lauritzen [5] presents a scheme for modeling and exact belief update in CLG BNs.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "Using a similar approach Chang & Fung [1] extend the SPI algorithm [13] to solve arbitrary queries against CLG BNs.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 12,
      "context" : "Using a similar approach Chang & Fung [1] extend the SPI algorithm [13] to solve arbitrary queries against CLG BNs.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "The Lauritzen [5] architecture is known to suffer from problems causing numerical instability.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 5,
      "context" : "For this reason the architecture was later revised by Lauritzen & Jensen [6] in order to improve the numerical stability of belief update.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 1,
      "context" : "Recently, Cowell [2] introduced an alternative architecture for belief update based on message passing in an elimination tree using the arcreversal operation of Shachter & Kenley [14] (referred to as the EXCHANGE operation).",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 13,
      "context" : "Recently, Cowell [2] introduced an alternative architecture for belief update based on message passing in an elimination tree using the arcreversal operation of Shachter & Kenley [14] (referred to as the EXCHANGE operation).",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 9,
      "context" : "The architecture is an extension of lazy propagation [10] based on operations introduced by Lauritzen & Jensen [6] and Cowell [2].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "The architecture is an extension of lazy propagation [10] based on operations introduced by Lauritzen & Jensen [6] and Cowell [2].",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "The architecture is an extension of lazy propagation [10] based on operations introduced by Lauritzen & Jensen [6] and Cowell [2].",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "Posterior marginal distributions are computed using EXCHANGE and PUSH [6] operations.",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "number of randomly generated CLG BNs with the performance of the Lauritzen & Jensen [6] architecture as implemented in the HUGIN Decision Engine, i.",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "It converts the above pair of distributions such that Y becomes a parent of Z in the domain graph spanned by the two distributions maintaining the same joint probability density function of the original pair [2].",
      "startOffset" : 208,
      "endOffset" : 211
    }, {
      "referenceID" : 1,
      "context" : "while the distribution of Z is (Cowell [2] considers three different cases depending on the values of σ2Y and σ2Z that are mathematical limits of this case):",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 9,
      "context" : "Lazy propagation aims at taking advantage of independence and irrelevance properties induced by evidence in a Shenoy-Shafer message passing scheme [10].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 5,
      "context" : "This is contrary to both the Lauritzen & Jensen [6] and Cowell [2] architectures where each clique has a probability potential over all discrete variables in the clique.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "This is contrary to both the Lauritzen & Jensen [6] and Cowell [2] architectures where each clique has a probability potential over all discrete variables in the clique.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "If the junction tree is not wide-enough to support a calculation, then the PUSH operation is used [6].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "where we assume σ2Y(i) > 0 for all i [6, 2].",
      "startOffset" : 37,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "where we assume σ2Y(i) > 0 for all i [6, 2].",
      "startOffset" : 37,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "If σ(i) = 0, insertion of evidence may be undefined, see [2] who cites [6].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "If σ(i) = 0, insertion of evidence may be undefined, see [2] who cites [6].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "The architectures described in [2], [6], and [9] each does a full propagation over all the nodes of the computation tree prior to inserting ǫ whereas we do only a partial COLLECT prior to inserting ǫ∆.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "The architectures described in [2], [6], and [9] each does a full propagation over all the nodes of the computation tree prior to inserting ǫ whereas we do only a partial COLLECT prior to inserting ǫ∆.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : "The architectures described in [2], [6], and [9] each does a full propagation over all the nodes of the computation tree prior to inserting ǫ whereas we do only a partial COLLECT prior to inserting ǫ∆.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "Cowell [2] presents an algorithm for belief update where message passing proceeds on an elimination tree rather than a (strong) junction tree.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 1,
      "context" : "The three main differences between the present propagation scheme and Cowell [2] are: use of a strong junction tree as opposed to an elimination tree, use of EXCHANGE to eliminate both continuous and discrete variables and a single round of message passing.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "The architecture of Lauritzen & Jensen [6] performs belief update by message passing in a strong junction tree architecture.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "A CG potential [8] is associated with each clique and separator.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "Initialization plays an important role in the Lauritzen & Jensen [6] architecture.",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 6,
      "context" : "It produces a Lauritzen & Spiegelhalter-like junction tree representation [7] where clique potentials are conditioned on the continuous variables of the parent separator.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "The need for conditioning, recursive combination, and complex matrix operations is eliminated in both the Cowell [2] and the present architectures.",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "In the Lauritzen & Jensen [6] architecture initialization of T requires a recursive combination operation.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "The Lauritzen & Jensen [6] architecture calculates weak marginals during DISTRIBUTE.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 1,
      "context" : "This is not the case for the Cowell [2] nor the present architecture.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : "The present architecture is quite different from the architecture proposed by Madsen [9].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 10,
      "context" : "The latter architecture is an extension of Madsen & Jensen [11] to the case of CLG BNs based on the propagation scheme of Lauritzen & Jensen [6].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "The latter architecture is an extension of Madsen & Jensen [11] to the case of CLG BNs based on the propagation scheme of Lauritzen & Jensen [6].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "First, the architecture is based solely on the operations of Lauritzen & Jensen [6] whereas the present scheme is based on operations of both Lauritzen & Jensen [6]",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "First, the architecture is based solely on the operations of Lauritzen & Jensen [6] whereas the present scheme is based on operations of both Lauritzen & Jensen [6]",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "and Cowell [2].",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 5,
      "context" : "We compared the performance of the present architecture with the performance of the commercial implementation of the Lauritzen & Jensen [6] architecture in the HUGIN Decision Engine.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 5,
      "context" : "This insight is supported by the experimental analysis, which indicates that the Lauritzen & Jensen [6] implementation runs out of memory on most networks with 75 or more variables for a large fraction of the evidence sets whereas the present architecture runs out of memory on a much smaller fraction of the evidence 0 1 2 3 4 5 6",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "On most of the networks considered in the test — where belief update is feasible — the commercial implementation of Lauritzen & Jensen [6] is most efficient (typically networks with less than 75 variables).",
      "startOffset" : 135,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : "The architecture is based on extended versions of operations introduced by Lauritzen & Jensen [6] and Cowell [2].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "The architecture is based on extended versions of operations introduced by Lauritzen & Jensen [6] and Cowell [2].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "Despite a significant difference in the efficiency of table operations the proposed architecture is — in some cases — more efficient than a commercial implementation of the Lauritzen & Jensen [6] architecture.",
      "startOffset" : 192,
      "endOffset" : 195
    } ],
    "year" : 2006,
    "abstractText" : "In recent years Bayesian networks (BNs) with a mixture of continuous and discrete variables have received an increasing level of attention. We present an architecture for exact belief update in Conditional Linear Gaussian BNs (CLG BNs). The architecture is an extension of lazy propagation using operations of Lauritzen & Jensen [6] and Cowell [2]. By decomposing clique and separator potentials into sets of factors, the proposed architecture takes advantage of independence and irrelevance properties induced by the structure of the graph and the evidence. The resulting benefits are illustrated by examples. Results of a preliminary empirical performance evaluation indicate a significant potential of the proposed architecture.",
    "creator" : "dvips(k) 5.95a Copyright 2005 Radical Eye Software"
  }
}