{
  "name" : "1006.2743.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Global Optimization for Value Function Approximation Global Optimization for Value Function Approximation",
    "authors" : [ "Marek Petrik" ],
    "emails" : [ "petrik@cs.umass.edu", "shlomo@cs.umass.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: value function approximation, Markov decision processes, reinforcement learning, approximate dynamic programming"
    }, {
      "heading" : "1. Motivation",
      "text" : "Solving large Markov Decision Problems (MDPs) is a very useful, but computationally challenging problem addressed widely in the AI literature, particularly in the area of reinforcement learning. It is widely accepted that large MDPs can only be solved approximately. The commonly used approximation methods can be divided into three broad categories: 1) policy search, which explores a restricted space of all policies, 2) approximate dynamic programming, which searches a restricted space of value functions, and 3) approximate linear programming, which approximates the solution using a linear program. While all of these methods have achieved impressive results in many application domains, they have significant limitations.\nPolicy search methods rely on local search in a restricted policy space. The policy may be represented, for example, as a finite-state controller (Stanley and Miikkulainen, 2004) or as a greedy policy with respect to an approximate value function (Szita and Lorincz, 2006). Policy search methods have achieved impressive results in such domains as Tetris (Szita and\nar X\niv :1\n00 6.\n27 43\nv1 [\ncs .A\nI] 1\n4 Ju\nLorincz, 2006) and helicopter control (Abbeel et al., 2006). However, they are notoriously hard to analyze. We are not aware of any established theoretical guarantees regarding the quality of the solution.\nApproximate dynamic programming (ADP) methods iteratively approximate the value function (Bertsekas and Ioffe, 1997; Powell, 2007; Sutton and Barto, 1998). They have been extensively analyzed and are the most commonly used methods. However, approximate dynamic programming methods typically do not converge and they only provide weak guarantees of approximation quality. The approximation error bounds are usually expressed in terms of the worst-case approximation of the value function over all policies (Bertsekas and Ioffe, 1997). In addition, most available bounds are with respect to the L∞ norm, while the algorithms often minimize the L2 norm. While there exist some L2-based bounds (Munos, 2003), they require values that are difficult to obtain.\nApproximate linear programming (ALP) uses a linear program to compute the approximate value function in a particular vector space (de Farias, 2002). ALP has been previously used in a wide variety of settings (Adelman, 2004; de Farias and van Roy, 2004; Guestrin et al., 2003). Although ALP often does not perform as well as ADP, there have been some recent efforts to close the gap (Petrik and Zilberstein, 2009). ALP has better theoretical properties than ADP and policy search. It is guaranteed to converge and return the closest L1-norm approximation ṽ of the optimal value function v\n∗ up to a multiplicative factor. However, the L1 norm must be properly weighted to guarantee a small policy loss, and there is no reliable method for selecting appropriate weights (de Farias, 2002).\nTo summarize, the existing reinforcement learning techniques often provide good solutions, but typically require significant domain knowledge (Powell, 2007). The domain knowledge is needed partly because useful a priori error bounds are not available, as mentioned above. Our goal is to develop a more reliable method that is guaranteed to minimize bounds on the policy loss in various settings.\nWe present new formulations of value function approximation that provably minimize bounds on the policy loss using global optimization. Most of these bounds do not rely on values that are hard to obtain, unlike, for example, approximate linear programming. The focus of the work is on two broad bound minimization approaches: 1) minimizing L∞ bounds, and 2) minimizing weighted L1 norm bounds on the policy loss. In some sense, the formulations minimize the bounds by unifying policy value-function search methods.\nWe start with a description of the framework and notation in Section 2 and the description of value function approximation in Section 3. Then, in Section 4, we describe the proposed approximate bilinear programming (ABP) formulations. Bilinear programs are typically solved using global optimization methods, which we briefly discuss in Section 5. A drawback of the bilinear formulation is that solving bilinear programs may require exponential time. We also show in Section 5 that this is unavoidable, because minimizing the approximation error bound is in fact NP-hard.\nIn practice, only sampled versions of ABPs are often solved. While a thorough treatment of sampling is beyond the scope of this paper, we examine the impact of sampling and establish some guarantees in Section 6. Unlike classical sampling bounds on approximate linear programming, we describe bounds that apply to the worst-case error. Section 7 shows that ABP is related to other approximate dynamic programming methods, such as approximate linear programming and policy iteration. Section 8 demonstrates the applicability\nof ABP using common reinforcement learning benchmark problems. Technical proofs are provided in the appendix.\nThe general setting considered in this paper is a restriction of reinforcement learning. Reinforcement learning methods can use samples without requiring a model of the environment. The methods we propose can also be based on samples, but they require additional structure. In particular, they require that all or most actions are sampled for every state. Such samples can be easily generated when a model of the environment is available."
    }, {
      "heading" : "2. Framework and Notation",
      "text" : "This section formally defines the framework and the notation we use. We also define Markov decision processes and the approximation errors involved. Markov decision processes come in many flavors based on the objective function that is optimized. This work focuses on the infinite horizon discounted MDPs, which are defined as follows.\nDefinition 1 (e.g. (Puterman, 2005)). A Markov Decision Process is a tuple (S,A, P, r, α). Here, S is a finite set of states, A is a finite set of actions, P : S × A × S 7→ [0, 1] is the transition function (P (s, a, s′) is the probability of transiting to state s′ from state s given action a), and r : S×A 7→ R+ is a reward function. The initial distribution is: α : S 7→ [0, 1], such that ∑ s∈S α(s) = 1.\nThe goal is to find a sequence of actions that maximizes γ-discounted discounted cumulative sum of the rewards, also called the return. A solution of a Markov decision process is a policy, which is defined as follows.\nDefinition 2. A deterministic stationary policy π : S 7→ A assigns an action to each state of the Markov decision process. A stochastic policy policy π : S ×A 7→ [0, 1]. The set of all stochastic stationary policies is denoted as Π and satisfies ∑ a∈A π(s, a) = 1.\nGeneral non-stationary policies may take different actions in states in different timesteps. We limit our treatment to stationary policies, since for infinite-horizon MDPs there exists an optimal stationary and deterministic policy. We also consider stochastic policies because they are more convenient to use in some settings that we consider.\nThe transition and reward functions for a given policy are denoted by Pπ and rπ. The value function update for a policy π is denoted by Lπ, and the Bellman operator is denoted by L. That is:\nLπv = Pπv + rπ Lv = max π∈Π Lπv.\nThe optimal value function, denoted v∗, satisfies v∗ = Lv∗. We assume a vector representation of the policy π ∈ R|S||A|. The variables π are defined for all state-action pairs and represent policies. That is π(s, a) represents the probability of taking action a ∈ A in state s ∈ S. The space of all correct (stochastic) policies can be represented using a set of linear equations:∑\na∈A π(s, a) = 1 ∀s ∈ S\nπ(s, a) ≥ 0 ∀s ∈ S, ∀a ∈ A\nThese inequalities can be represented using matrix notation as follows.\nBπ = 1\nπ ≥ 0,\nwhere the matrix B : |S| × (|S| · |A|) is defined as follows.\nB(s′, (s, a)) =\n{ 1 s = s′\n0 otherwise .\nWe use 0 and 1 to denote vectors of all zeros or ones of the appropriate size respectively. The symbol I denotes an identity matrix of the appropriate dimension.\nIn addition, a policy π induces a state visitation frequency uπ : S → R, defined as follows:\nuπ = ( I− γPTπ )−1 α.\nThe return of a policy depends on the state-action visitation frequencies and αTvπ = r Tuπ. The optimal state-action visitation frequency is uπ∗ . State-action visitation frequency u : S ×A → R is defined for all states and actions. Notice the missing subscript. We use ua to denote the part of u that corresponds to action a ∈ A. State-action visitation frequencies must satisfy: ∑\na∈A (I− γP )Tua = α.\nTo formulate approximate linear and bilinear programs, it is necessary to restrict the value functions so that their Bellman residuals are non-negative (or at least bounded from below). We call such value functions transitive-feasible and define them as follows.\nDefinition 3. A value function is transitive-feasible when v ≥ Lv. The set of transitivefeasible value functions is:\nK = {v ∈ R|S| v ≥ Lv}.\nGiven some ≥ 0, the set of -transitive-feasible value functions is:\nK( ) = {v ∈ R|S| v ≥ Lv − 1}.\nNotice that the optimal value function v∗ is transitive-feasible. The following lemma summarizes the key property of transitive-feasible value functions:\nLemma 4. Transitive feasible value functions form an upper bound on the optimal value function. If v ∈ K( ) is an -transitive-feasible value function, then\nv ≥ v∗ − 1− γ1."
    }, {
      "heading" : "3. Value Function Approximation",
      "text" : "This section describes the basic methods for value function approximation. MDPs used in practical applications are often too large for the optimal policy to be computed precisely. In these cases, we first calculate an approximate value function ṽ and then take the greedy policy π with respect to it. The quality of such a policy can be characterized using its value function vπ in one of the following two main ways.\nDefinition 5 (Policy Loss). Let π be a policy computed from value function approximation. The expected policy loss measures the expected loss of π, defined as follows:\n‖v∗ − vπ‖1,α = αTv∗ − αTvπ (1)\nwhere ‖x‖1,c denotes the weighted L1 norm: ||x‖1,c = ∑\ni |c(i)x(i)|. The robust policy loss measures the worst-case loss of π, defined as follows:\n‖v∗ − vπ‖∞ = max s∈S |v∗(s)− vπ(s)| (2)\nThe expected policy loss captures the total loss of discounted reward when following the policy π instead of the optimal policy assuming the initial distribution. The robust policy loss ignores the initial distribution and, in some sense, measures the difference for the worst-case initial distribution.\nA set of state features is a necessary component of value function approximation. These features must be supplied in advance and must capture the essential structure of the problem. The features are defined by mapping each state s to a vector φ(s) of features. We denote φi : S → R to be a function that maps states to the value of feature i:\nφi(s) = (φ(s))i.\nThe desirable properties of the features depend strongly on the algorithm, samples, and attributes of the problem; the tradeoffs are not yet fully understood. The function φi can also be treated as a vector, similarly to the value function v.\nValue function approximation methods compute value functions that can be represented using the state features. We call such value functions representable and define them below.\nDefinition 6. Given a convex polyhedral setM⊆ R|S|, a value function v is representable (in M) if v ∈M.\nMany methods that compute a value function based on a given set of features have been developed, such as neural networks and genetic algorithms (Bertsekas and Ioffe, 1997). Most of these methods are extremely hard to analyze, computationally complex, and hard to use. Moreover, these complex methods do not satisfy the convexity assumption in Definition 6. A simpler, more common, method is linear value function approximation. In linear value function approximation, the value function of state s is represented as a linear combination of nonlinear features φ(s). Linear value function approximation is easy to apply and analyze.\nLinear value function approximation can be expressed in terms of matrices as follows. Let the matrix Φ : |S|×m represent the features for the state-space, where m is the number of features. The rows of the feature matrix Φ, also known as the basis, correspond to the\nfeatures of the states φ(s). The feature matrix can be defined in one of the following two ways:\nΦ = − φ(s1) T −\n− φ(s2)T − ...  Φ =  | |φ1 φ2 . . . | |  The value function v is then represented as v = Φx and the set of representable functions is M = colspan (Φ).\nThe goal of value function approximation is not just to obtain a good value function ṽ but a policy with a small policy loss. Unfortunately, the policy loss of a greedy policy, as formulated in Definition 5, depends non-trivially on the approximate value function ṽ. Often, the only reliable method of precisely computing the policy loss is to simulate the policy, which can be very costly. Value function approximation methods, therefore, optimize bounds on the policy loss.\nTheorem 7. [Robust Policy Loss, e.g. (Williams and Baird, 1994)] Let π be a greedy policy with respect to a value function ṽ. Then:\n‖v∗ − vπ‖∞ ≤ 2\n1− γ ‖ṽ − Lṽ‖∞.\nIn addition, if ṽ ∈ K, the policy loss is minimized for the greedy policy and:\n‖v∗ − vπ‖∞ ≤ 1\n1− γ ‖ṽ − Lṽ‖∞.\nThe bounds above ignore the initial distribution and may often be overly conservative. We establish new bounds on the expected policy loss that also consider the initial distribution.\nTheorem 8. [Expected Policy Loss] Let π be a greedy policy with respect to a value function ṽ and let the state-action visitation frequencies of π be bounded as u ≤ uπ ≤ ū. Then:\n‖v∗ − vπ‖1,α = αTv∗ − αTṽ + uTπ (ṽ − Lṽ) ≤ αTv∗ − αTṽ + uT [ṽ − Lṽ]− + ūT [ṽ − Lṽ]+ .\nThe state-visitation frequency uπ depends on the initial distribution α, unlike v ∗. In addition, when ṽ ∈ K, the bound is: ‖v∗ − vπ‖1,α ≤ −‖v∗ − ṽ‖1,α + ‖ṽ − Lṽ‖1,ū ‖v∗ − vπ‖1,α ≤ −‖v∗ − ṽ‖1,α + 1\n1− γ ‖ṽ − Lṽ‖∞\nHere we use [x]+ = max{x,0} componentwise.\nProof. The bound is derived as follows:\nαTv∗ − αTvπ = αTv∗ − αTvπ + (uTπ (I− γPπ)− αT)ṽ = αTv∗ − rTπuπ + (uTπ (I− γPπ)− αT)ṽ = αTv∗ − rTπuπ + uTπ (I− γPπ)ṽ − αTṽ = αTv∗ − αTṽ + uTπ ((I− γPπ)ṽ − rπ) = αTv∗ − αTṽ + uTπ (ṽ − Lṽ) .\nWe used the fact that uTπ (I−γPπ)−αT = 0, which follows from the definition of state-action visitation frequencies and that v∗ ≥ vπ. The inequalities for ṽ ∈ K follow from Lemma 38, Lemma 43, and the trivial version of the Holder’s inequality:\nαTv∗ − αTṽ = −‖v∗ − ṽ‖1,α uTπ (ṽ − Lṽ) ≤ ‖uπ‖1 ‖ṽ − Lṽ‖∞ = 1\n1− γ ‖ṽ − Lṽ‖∞\nNotice that the bounds in Theorem 8 can be minimized even without knowing the optimal v∗. The optimal value function v∗ is independent of the approximate value function ṽ and the greedy policy π depends only on ṽ.\nRemark 9. The bounds in Theorem 8 generalize the bounds established by de Farias (2002, Theorem 1.3), which state that whenever v ∈ K:\n‖v∗ − vπ‖1,u ≤ 1 1− γ ‖v ∗ − ṽ‖1,(1−γ)u.\nThis bound is a special case of Theorem 8 because αTv∗ − αTṽ ≤ 0 and:\n‖ṽ − Lṽ‖1,u ≤ ‖v∗ − ṽ‖1,u ≤ 1 1− γ ‖v ∗ − ṽ‖1,(1−γ)u,\nfrom v∗ ≤ Lṽ ≤ ṽ. The proof of Theorem 8 also simplifies the proof of Theorem 1.3 in (de Farias, 2002).\nThe methods that we propose require the following standard assumption.\nAssumption 10. All multiples of the constant vector 1 are representable in M. That is, for all k ∈ R we have that k1 ∈M.\nNotice that the representation set M satisfies Assumption 10 when a first column of Φ is 1. The impact of including the constant feature is typically negligible because adding a constant to the value function does not change the greedy policy.\nValue function approximation algorithms are typically variations of the exact algorithms for solving MDPs. Hence, they can be categorized as approximate value iteration, approximate policy iteration, and approximate linear programming. The ideas of approximate value iteration could be traced to Bellman (1957), which was followed by many additional research efforts (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Powell, 2007). Below, we only discuss approximate policy iteration and approximate linear programming, because they are the most closely related to our approach.\nApproximate policy iteration (API) is summarized in algorithm 1. The function Z(π) denotes the specific method used to approximate the value function for the policy π. The two most commonly used methods — Bellman residual approximation and least-squares approximation (Lagoudakis and Parr, 2003) — minimize the L2 norm of the Bellman residual.\nThe approximations based on minimizing L2 norm of the Bellman residual are common in practice since they are easy to compute and often lead to good results. Most theoretical analyses of API, however, assume minimization of the L∞ norm of the Bellman residual:\nZ(π) ∈ arg min v∈M ‖(I− γPπ)v − rπ‖∞ (3)\nAlgorithm 1: Approximate policy iteration, where Z(π) denotes a custom value function approximation for the policy π. 1 π0, k ← rand, 1 ; 2 while πk 6= πk−1 do 3 ṽk ← Z(πk−1) ; 4 πk(s)← arg maxa∈A r(s, a) + γ ∑ s′∈S P (s, a, s\n′)ṽk(s) ∀s ∈ S ; 5 k ← k + 1 ;\nL∞-API is shown in algorithm 1, where Z(π) is calculated using the following program:\nmin φ,v φ s.t. (I− γPπ)v + 1φ ≥ rπ −(I− γPπ)v + 1φ ≥ −rπ v ∈M\n(4)\nWe are not aware of convergence or divergence proofs of L∞-API, and this analysis is beyond the scope of this paper. Theoretically, it is also possible to minimize the L1 norm of the Bellman residual, but we are not aware of any study of such an approximation.\nIn the above description of API, we assumed that the value function is approximated for all states and actions. This is impossible in practice due to the size of the MDP. Instead, API only relies on a subset of states and actions, provided as samples. API is not guaranteed to converge in general and its analysis is typically in terms of limit behavior. The limit bounds are often very loose. We discuss the performance of API and how it relates to approximate bilinear programming in more detail in Section 7.\nApproximate linear programming — a method for value function approximation — is based on the linear program formulation of exact MDPs:\nmin v ∑ s∈S c(s)v(s)\ns.t. v(s)− γ ∑ s′∈S P (s′, s, a)v(s′) ≥ r(s, a) ∀(s, a) ∈ (S,A) (5)\nWe use A as a shorthand notation for the constraint matrix and b for the right-hand side. The value c represents a distribution over the states, usually a uniform one. That is, ∑\ns∈S c(s) = 1. The linear program (5) is often too large to be solved precisely, so it is approximated to get an approximate linear program by assuming that v ∈ M (de Farias and van Roy, 2003), as follows:\nmin v\ncTv\ns.t. Av ≥ b v ∈M\n(ALP–L1)\nThe constraint v ∈ M denotes the approximation. To actually solve this linear program, the value function is represented as v = Φx. Assumption 10 guarantees the feasibility of\nthe ALP. The optimal solution of the ALP, ṽ, satisfies: ṽ ≥ v∗. Therefore, the objective of (ALP–L1) represents the minimization of ‖ṽ − v∗‖1,c (de Farias, 2002).\nApproximate linear programming is guaranteed to converge to a solution and minimize a weighted L1 norm on the solution quality.\nTheorem 11 (e.g. (de Farias, 2002)). Given Assumption 10, let ṽ be the solution of the approximate linear program (ALP–L1). If c = α then\n‖v∗ − ṽ‖1,α ≤ 2 1− γ minv∈M ‖v ∗ − v‖∞.\nThe difficulty with the solution of ALP is that it is hard to derive guarantees on the policy loss based on the bounds in terms of the L1 norm; it is possible when the objective function c represents ū, as Remark 9 shows. In addition, the constant 1/(1 − γ) may be very large when γ is close to 1.\nApproximate linear programs are often formulated in terms of samples instead of the full formulation above. The performance guarantees are then based on analyzing the probability that a large number of constraints is violated. It is generally hard to translate the constraint violation bounds to bounds on the quality of the value function and the policy."
    }, {
      "heading" : "4. Bilinear Program Formulations",
      "text" : "This section shows how to formulate value function approximation as a separable bilinear program. Bilinear programs are a generalization of linear programs with an additional bilinear term in the objective function. A separable bilinear program consists of two linear programs with independent constraints and are fairly easy to solve and analyze.\nDefinition 12 (Separable Bilinear Program). A separable bilinear program in the normal form is defined as follows:\nmin w,x y,z\nsT1w + r T 1 x+ x TCy + rT2 y + s T 2 z\ns.t. A1x+B1w = b1 A2y +B2z = b2 w, x ≥ 0 y, z ≥ 0 (BP–m)\nThe objective of the bilinear program (BP–m) is denoted as f(w, x, y, z). We separate the variables using a vertical line and the constraints using different columns to emphasize the separable nature of the bilinear program. In this paper, we only use separable bilinear programs and refer to them simply as bilinear programs.\nWe present three different approximate bilinear formulations that minimize the following bounds on the approximate value function.\n1. Robust policy loss: Minimizes ‖v∗ − vπ‖∞ by minimizing the bounds in Theorem 7:\nmin π∈Π ‖v∗ − vπ‖∞ ≤ min v∈M\n1\n1− γ ‖v − Lv‖∞\n2. Expected policy loss: Minimizes ‖v∗−v)π‖1,α by minimizing the bounds in Theorem 8:\nmin π∈Π ‖v∗ − vπ‖1,α ≤ αTv∗ + min v∈M\n( −αTṽ + 1 1− γ ‖v − Lv‖∞ )\nmin π∈Π ‖v∗ − vπ‖1,α ≤ αTv∗ + min v∈M\n( −αTṽ + ‖v − Lv‖ū ) .\n3. The sum of k largest errors: This formulation represents a hybrid between the robust and expected formulations. It is more robust than simply minimizing the expected performance but is not as sensitive to worst-case performance.\nThe appropriateness of each formulation depends on the particular circumstances of the domain. For example, minimizing robust bounds is advantageous when the initial distribution is not known and the performance must be consistent under all circumstances. On the other hand, minimizing expected bounds on the value function is useful when the initial distribution is known.\nIn the formulations described below, we initially assume that samples of all states and actions are used. This means that the precise version of the operator L is available. To solve large problems, the number of samples would be much smaller; either simply subsampled or reduced using the structure of the MDP. Reducing the number of constraints in linear programs corresponds to simply removing constraints. In approximate bilinear programs it also reduces the number of some variables, as Section 6 describes.\nThe formulations below denote the value function approximation generically by v ∈M. That restricts the value functions to be representable using features. Representable value functions v can be replaced by a set of variables x as v = Φx. This reduces the number of variables to the number of features."
    }, {
      "heading" : "4.1 Robust Policy Loss",
      "text" : "The solution of the robust approximate bilinear program minimizes the L∞ norm of the Bellman residual ‖v − Lv‖∞. This minimization can be formulated as follows.\nmin π λ,λ′,v\nπTλ+ λ′\ns.t. Bπ = 1 Av − b ≥ 0 π ≥ 0 λ+ λ′1 ≥ Av − b\nλ, λ′ ≥ 0 v ∈M\n(ABP–L∞)\nAll the variables are vectors except λ′, which is a scalar. The matrix A represents constraints that are identical to the constraints in (ALP–L1). The variables λ correspond to all state-action pairs. These variables represent the Bellman residuals that are being minimized. This formulation offers the following guarantees.\nTheorem 13. Given Assumption 10, any optimal solution (π̃, ṽ, λ̃, λ̃′) of the approximate bilinear program (ABP–L∞) satisfies:\nπ̃Tλ̃+ λ̃′ = ‖Lṽ − ṽ‖∞ ≤ min v∈K∩M ‖Lv − v‖∞ ≤ 2 min\nv∈M ‖Lv − v‖∞\n≤ 2(1 + γ) min v∈M ‖v − v∗‖∞.\nMoreover, there exists an optimal solution π̃ that is greedy with respect to ṽ for which the policy loss is bounded by:\n‖v∗ − vπ̃‖∞ ≤ 2\n1− γ ( min v∈M ‖Lv − v‖∞ ) .\nIt is important to note that the theorem states that solving the approximate bilinear program is equivalent to minimization over all representable value functions, not only the transitive-feasible ones. This follows by subtracting a constant vector 1 from ṽ to balance the lower bounds on the Bellman residual error with the upper ones. This reduces the Bellman residual by 1/2 without affecting the policy. Finally, note that whenever v∗ ∈M, both ABP and ALP will return the optimal value function v∗.\nTo prove the theorem, we first define the following linear program, which solves for the L∞ norm of the Bellman update Lπ for fixed value function v and policy π.\nf1(π, v) = min λ,λ′\nπTλ+ λ′\ns.t. 1λ′ + λ ≥ Av − b λ ≥ 0\n(6)\nThe linear program (6) corresponds to the bilinear program (ABP–L∞) with a fixed policy π and value function v.\nLemma 14. Let v ∈ K be a transitive-feasible value function and let π be a policy. Then:\nf1(π, v) ≥ ‖v − Lπv‖∞,\nwith an equality for a deterministic policy π.\nProof. The dual of the linear program (6) is the following program.\nmax x xT(Av − b) s.t. x ≤ π\n1Tx = 1\nx ≥ 0\n(7)\nNote that replacing 1Tx = 1 by 1Tx ≤ 1 preserves the properties of the linear program and would add an additional constraint in (ABP–L∞): λ\n′ ≥ 0. First, we show that f1(π, v) ≥ ‖Lπv − v‖∞. Because v is feasible in the approximate bilinear program (ABP–L∞), Av − b ≥ 0 and v ≥ Lv from Lemma 38. Let state s be the state in which t = ‖Lπv − v‖∞ is achieved. That is:\nt = v(s)− ∑ a∈A π(s, a)\n( r(s, a) +\n∑ s′∈S γP (s′, s, a)v(s′)\n) .\nNow let x(s, a) = π(s, a) for all a ∈ A. This is a feasible solution with value t, from the stochasticity of the policy and therefore a lower bound on the objective value.\nTo show the equality for a deterministic policy π, we show that f1(π, v) ≤ ‖Lv − v‖∞, using that π ∈ {0, 1}. Then let x∗ be an optimal solution of (7). Define the index of x∗ with the largest objective value as:\ni ∈ arg max {i x∗(i)>0} (Av − b)(i).\nLet solution x′(i) = 1 and x′(j) = 0 for j 6= i, which is feasible since π(i) = 1. In addition:\n(Av − b)(i) = ‖Lπv − v‖∞.\nNow (x∗)T(Av − b) ≤ (x′)T(Av − b) = ‖Lπv − v‖∞, from the fact that i is the index of the largest element of the objective function.\nWhen the policy π is fixed, the approximate bilinear program (ABP–L∞) becomes the following linear program:\nf2(π) = min λ,λ′,v\nπTλ+ λ′\ns.t. Av − b ≥ 0 1λ+ λ′ ≥ Av − b λ ≥ 0 v ∈M\n(8)\nUsing Lemma 38 , this linear program corresponds to:\nf2(π) = min v∈M∩K f1(π, v).\nThen it is easy to show that:\nLemma 15. Given a policy π, let ṽ be an optimal solution of the linear program (8). Then:\nf2(π) = ‖Lπṽ − ṽ‖∞ ≤ min v∈M∩K ‖Lπv − v‖∞.\nWhen v is fixed, the approximate bilinear program (ABP–L∞) becomes the following linear program:\nf3(v) = min π f2(π, v) s.t. Bπ = 1\nπ ≥ 0 (9)\nNote that the program is only meaningful if v is transitive-feasible and that the function f2 corresponds to a minimization problem.\nLemma 16. Let v ∈M∩K be a transitive-feasible value function. There exists an optimal solution π̃ of the linear program (9) such that:\n1. π̃ represents a deterministic policy 2. Lπ̃v = Lv 3. ‖Lπ̃v − v‖∞ = minπ∈Π ‖Lπv − v‖∞ = ‖Lv − v‖∞\nProof. The existence of an optimal π that corresponds to a deterministic policy follows from Lemma 15, the correspondence between policies and values π, and the existence of a deterministic greedy policy.\nSince v ∈ K, we have for some policy π that:\nv ≥ Lv = Lπv ≥ Lπ̃v.\nAssuming that Lπ̃ < Lv leads to a contradiction since π is also a feasible solution in the linear program (9) and:\nv − Lπ̃ > v − Lv ‖v − Lπ̃‖∞ > ‖v − Lv‖∞.\nThis proves the lemma.\nTheorem 13 now easily follows from the lemmas above.\nProof. Let v̄ be a value function with the minimal ‖Lv̄−v̄‖∞ feasible in approximate bilinear program (ABP–L∞), and let π̄ be a greedy policy with respect to v̄. Because v̄ ≥ v∗, as Lemma 38 shows, we get:\nt = ‖Lv̄ − v̄‖∞ = ‖Lv̄v̄ − v̄‖∞.\nLet f∗ be the optimal objective value of (ABP–L∞). Because both v̄ and π̄ are feasible in (ABP–L∞), we have that f\n∗ ≤ t. Now, assume that ṽ is an optimal solution of (ABP–L∞) with an objective value f̃ = ‖Lṽ − ṽ‖∞ > t. Then, from Lemma 16, f̃ > t ≥ f∗, which contradicts the optimality of ṽ.\nTo show that the optimal policy is deterministic and greedy, let π∗ be the optimal policy. Then consider the state s for which π̃ does not define a deterministic greedy action. From the definition of greedy action ā:\n(Lāṽ)(s) ≤ (Lπ̃ṽ)(s).\nFrom the bilinear formulation (ABP–L∞), it is easy to show that there is an optimal solution such that:\n(Laṽ)(s) ≤ λ̃′ + λ̃(s, a) λ̃(s, ā) ≤ λ̃(s, a).\nThen setting π̃(s, ā) = 1 and all other action probabilities to 0, the difference in the objective value function: λ̃(s, ā)− ∑ a∈A λ̃(s, a) ≤ 0.\nTherefore, the objective function for the deterministic greedy policy does not increase. The remainder of the theorem follows directly from Proposition 40, Proposition 41, and Proposition 42. The bounds on the policy loss then follow directly from Theorem 7."
    }, {
      "heading" : "4.2 Expected Policy Loss",
      "text" : "This section describes bilinear programs that minimize expected policy loss for a given initial distribution ‖v − Lv‖1,α. The initial distribution can be used to derive tighter bounds on the policy loss. We describe two formulations. They respectively minimize an L∞ and a weighted L1 norm on the Bellman residual.\nThe expected policy loss can be minimized by solving the following bilinear formulation.\nmin π λ,λ′,v πTλ+ λ′ − (1− γ)αTv s.t. Bπ = 1 Av − b ≥ 0\nπ ≥ 0 λ+ λ′1 ≥ Av − b λ, λ′ ≥ 0 v ∈M\n(ABP–L1)\nNotice that this formulation is identical to the bilinear program (ABP–L∞) with the exception of the term −(1− γ)αTv.\nTheorem 17. Given Assumption 10, any optimal solution (π̃, ṽ, λ̃, λ̃′) of the approximate bilinear program (ABP–L1) satisfies:\n1 1− γ ( π̃Tλ̃+ λ̃′ ) − αTṽ = 1 1− γ ‖Lṽ − ṽ‖∞ − α Tv ≤ min v∈K∩M ( 1 1− γ ‖Lv − v‖∞ − α Tv ) ≤ min\nv∈M\n( 1\n1− γ ‖Lv − v‖∞ − α Tv ) Moreover, there exists an optimal solution π̃ that is greedy with respect to ṽ for which the policy loss is bounded by:\n‖v∗ − vπ̃‖1,α ≤ 2\n1− γ ( min v∈M 1 1− γ ‖Lv − v‖∞ − ‖v ∗ − v‖1,α ) .\nNotice that the bound in this theorem is tighter than the one in Theorem 13. In particular, ‖v∗ − ṽ‖1,α > 0, unless the solution of the ABP is the optimal value function.\nProof. The proof of the theorem is almost identical to the proof of Theorem 13 with two main differences. First, the objective function of (ABP–L1) is insensitive to adding a constant to the value function:\n‖(ṽ + k1)− L(v + k1)‖∞ − αT(v + k1) = ‖ṽ − Lv‖∞ − αTv.\nHence the missing factor 2 when going from minimization over K∩M to minimization over M. The second difference is in the derivation of the bound on the policy loss, which follows directly from Theorem 8.\nThe bilinear program formulation in (ABP–L1) can be further strengthened when an upper bound on the state-visitation frequencies is available.\nmin π λ,v πTUλ− αTv s.t. Bπ = 1 Av − b ≥ 0\nπ ≥ 0 λ ≥ Av − b λ ≥ 0 v ∈M\n(ABP–U)\nHere U : |S| · |A| × |S| · |A| is a matrix that maps a policy to bounds on state-action visitation frequencies. It must satisfy that:\nπ(s, a) = 0⇒ (πTU)(s, a) = 0 ∀s ∈ S ∀a ∈ A.\nRemark 18. One simple option is to have U represent a diagonal matrix of ū, where ū is the bound for all policies π ∈ Π. That is:\nU((s, a), (s′, a′)) =\n{ ū(s) s′ = s\n0 otherwise ∀s, s′ ∈ S a, a′ ∈ A.\nThe formal guarantees for this formulation are as follows. Theorem 19. Given Assumption 10 and that for all π ∈ Π : ∑a∈A(πTU)(s, a) ≥ uTπ (s), any optimal solution (π̃, ṽ, λ̃, λ̃′) of the bilinear program (ABP–U) then satisfies:\nπ̃TUλ̃− αTṽ = ‖ṽ − Lṽ‖ū − αTṽ ≤ min v∈K∩M\n( ‖v − Lv‖ū − αTv ) .\nAssuming that U is defined as in Remark 18, there exists an optimal solution π̃ that is greedy with respect to ṽ and:\n‖v∗ − vπ̃‖1,α ≤ min v∈M∩K\n( ‖v − Lv‖1,ū(v) − ‖v∗ − v‖1,α ) .\nHere, ū(v) represents an upper bound on the state-action visitation frequencies for a policy greedy with respect to value function v.\nUnlike Theorem 13 and Theorem 17, the bounds in this theorem do not guarantee that the solution quality does not degrade by restricting the value function to be transitivefeasible.\nTo prove the theorem we first define the following linear program that solves for the L1 norm of the Bellman update Lπ for a value function v.\nf1(π, v) = min λ,λ′\nπTUλ\ns.t. 1λ′ + λ ≥ Av − b λ ≥ 0\n(10)\nThe linear program (6) corresponds to the bilinear program (ABP–U) with a fixed policy π and value function v. Notice, in particular, that αTv is a constant.\nLemma 20. Let value function v be feasible in the bilinear program (ABP–U), and let π be an arbitrary policy. Then:\nf1(π, v) ≥ ‖Lπv − v‖1,ū,\nwith an equality for a deterministic policy.\nProof. The dual of the linear program (10) program is the following.\nmax x xT(Av − b) s.t. x ≤ UTπ\nx ≥ 0 (11)\nWe have that f1(π, v) ≥ ‖Lπv − v‖1,ū since x = UTπ is a feasible solution. To show the equality for a deterministic policy π, let x∗ be an optimal solution of linear program (11). Since Av ≥ b and U is non-negative, an optimal solution satisfies x = UTπ. The optimal value of the linear program thus corresponds to the definition of the weighted L1 norm.\nThe proof of Theorem 19 is similar to the proof of Theorem 13, but using Theorem 8 instead of Theorem 7 to bound the policy loss. The existence of a deterministic and greedy optimal solution π̃ follows also like Theorem 13, omitting λ′ and weighing λ by ū."
    }, {
      "heading" : "4.3 Hybrid Formulation",
      "text" : "While the robust bilinear formulation (ABP–L∞) guarantees to minimize the robust approximation error it may be overly pessimistic. The bilinear program (ABP–U), on the other hand, optimizes the average performance, but does not provide strong guarantees. It is possible to combine the advantages (and disadvantaged) of these programs using a hybrid formulation. The hybrid formulation minimizes the hybrid norm of the Bellman residual, defined as:\n‖x‖k,c = max {y 1Ty=k,1≥y≥0} n∑ i=1 y(i)c(i)|x(i)|,\nwhere n is the length of vector x and c ≥ 0. It is easy to show that this norm represents the c-weighted L1 norm of the k largest components of the vector. As such, it is more robust than the plain L1 norm, but is not as sensitive to outliers as the L∞ norm. Notice that the solution may be fractional when k /∈ Z — that is, some elements are counted only partially.\nThe bilinear program that minimizes the hybrid norm is defined as follows.\nmin π λ,λ′,v\nπTUλ+ kλ′\ns.t. Bπ = 1 Av − b ≥ 0 π ≥ 0 λ+ λ′U−11 ≥ Av − b\nλ, λ′ ≥ 0 v ∈M\n(ABP–h)\nHere U is a matrix that maps a policy to bounds on state-action visitation frequencies, for example, as defined in Remark 18.\nTheorem 21. Given Assumption 10 and U that is defined as in Remark 18, any optimal solution (π̃, ṽ, λ̃, λ̃′) of (ABP–h) then satisfies:\nπ̃TUλ̃+ kλ̃′ = ‖Lṽ − ṽ‖k,ū(π̃) ≤ min v∈M∩K ‖Lṽ − ṽ‖k,ū(v).\nHere, ū(v) represents the upper bound on the state-action visitation frequencies for policy greedy with respect to value function v.\nThe implication of these bounds on the policy loss is beyond the scope of this paper, but it is likely that some form of policy loss bounds can be developed.\nThe proof of the theorem is almost identical to the proof of Theorem 13 lemma. We first define the following linear program, which solves for the required norm of the Bellman update Lπ for value function v and policy π.\nf1(π, v) = min λ,λ′\nπTUλ+ kλ′\ns.t. λ′U−11 + λ ≥ Av − b λ, λ′ ≥ 0\n(12)\nThe linear program (12) corresponds to the bilinear program (ABP–h) with a fixed policy π and value function v.\nLemma 22. Let v ∈ K be a transitive-feasible value function and let π be a policy and U be defined as in Remark 18. Then:\nf1(π, v) ≥ ‖v − Lπv‖k,ū,\nwith an equality for a deterministic policy π.\nProof. The dual of the linear program (6) program is the following.\nmax x xT(Av − b) s.t. x ≤ UTπ\n1T ( UT )−1\nx ≤ k x ≥ 0\n(13)\nFirst, change the variables in the linear program to x = UTz to get:\nmax z zTU(Av − b) s.t. z ≤ π\n1Tz ≤ k z ≥ 0\n(14)\nusing the fact that U is diagonal and positive.\nThe norm‖Lπv − v‖k,c can be expressed as the following linear program:\nmax y yTXU(Av − b) s.t. y ≤ 1\n1Ty ≤ k y ≥ 0\n(15)\nHere, the matrix X : |S| × |S| · |A| selects the subsets of the Bellman residuals that correspond the the policy as defined:\nX(s, (s′, a′)) =\n{ π(s′, a′) when s = s′\n0 otherwise .\nIt is easy to shows that v − Lπv = X(Av − b). Note that XU = UX from the definition of U .\nClearly, when π ∈ {0, 1} is deterministic the linear programs (14) and (15) are identical. When the policy π is stochastic, assume an optimal solution y of (15) and let z = XTy. Then, z is feasible in (14) with the identical objective value, which shows the inequality."
    }, {
      "heading" : "5. Solving Bilinear Programs",
      "text" : "This section describes methods for solving approximate bilinear programs. Bilinear programs can be easily mapped to other global optimization problems, such as mixed integer linear programs (Horst and Tuy, 1996). We focus on a simple iterative algorithm for solving bilinear programs approximately, which also serves as a basis for many optimal algorithms.\nSolving a bilinear program is an NP-complete problem (Bennett and Mangasarian, 1992). The membership in NP follows from the finite number of basic feasible solutions of the individual linear programs, each of which can be checked in polynomial time. The NP-hardness is shown by a reduction from the SAT problem.\nThere are two main approaches to solving bilinear programs optimally. In the first approach, a relaxation of the bilinear program is solved. The solution of the relaxed problem represents a lower bound on the optimal solution. The relaxation is then iteratively refined, for example by adding cutting plane constraints, until the solution becomes feasible. This is a common method used to solve integer linear programs. The relaxation of the bilinear program is typically either a linear or semi-definite program (Carpara and Monaci, 2009).\nIn the second approach, feasible, but suboptimal, solutions of the bilinear program are calculated approximately. The approximate algorithms are usually some variation of algorithm 2. The bilinear program formulation is then refined — using concavity cuts (Horst and Tuy, 1996) — to eliminate previously computed feasible solutions and solved again. This procedure can be shown to find the optimal solution by eliminating all suboptimal feasible solutions.\nThe most common and simplest approximate algorithm for solving bilinear programs is algorithm 2. This algorithm is shown for the general bilinear program (BP–m), where f(w, x, y, z) represents the objective function. The minimizations in the algorithm are linear\nAlgorithm 2: Iterative algorithm for solving (BP–m) 1 (x0, w0)← random ; 2 (y0, z0)← arg miny,z f(w0, x0, y, z) ; 3 i← 1 ; 4 while yi−1 6= yi or xi−1 6= xi do 5 (yi, zi)← arg min{y,z A2y+B2z=b2 y,z≥0} f(wi−1, xi−1, y, z) ; 6 (xi, wi)← arg min{x,w A1x+B1w=b1 x,w≥0} f(w, x, yi, zi) ; 7 i← i+ 1 8 return f(wi, xi, yi, zi)\nprograms which can be easily solved. Interestingly, as we will show in Section 7, algorithm 2 applied to ABP generalizes a version of API.\nWhile algorithm 2 is not guaranteed to find an optimal solution, its empirical performance is often remarkably good (Mangasarian, 1995). Its basic properties are summarized by the following proposition.\nProposition 23 (e.g. (Bennett and Mangasarian, 1992)). algorithm 2 is guaranteed to converge, assuming that the linear program solutions are in a vertex of the optimality simplex. In addition, the global optimum is a fixed point of the algorithm, and the objective value monotonically improves during execution.\nThe proof is based on the finite count of the basic feasible solutions of the individual linear programs. Because the objective function does not increase in any iteration, the algorithm will eventually converge.\nalgorithm 2 can be further refined in case of approximate bilinear programs. For example, the constraint v ∈ M in the bilinear programs serves just to simplify the bilinear program and a value function that violates it may still be acceptable. The following proposition motivates the construction of a new value function from two transitive-feasible value functions.\nProposition 24. Let ṽ1 and ṽ2 be feasible value functions in (ABP–L∞). Then the value function ṽ(s) = min{ṽ1(s), ṽ2(s)} is also feasible in bilinear program (ABP–L∞). Therefore ṽ ≥ v∗ and\n‖v∗ − ṽ‖∞ ≤ min {‖v∗ − ṽ1‖∞, ‖v∗ − ṽ2‖∞} .\nThe proof of the proposition is based on Jensen’s inequality and is provided in the appendix. Note that ṽ may have a greater Bellman residual than either ṽ1 or ṽ2.\nProposition 24 can be used to extend algorithm 2 when solving ABPs. One option is to take the state-wise minimum of values from multiple random executions of algorithm 2, which preserves the transitive feasibility of the value function. However, the increasing number of value functions used to obtain ṽ also increases the potential sampling error."
    }, {
      "heading" : "6. Sampling Guarantees",
      "text" : "In most practical problems, the number of states is too large to be explicitly enumerated. Even though the value function is restricted to be representable, the problem cannot be solved. The usual approach is to sample a limited number of states, actions, and their transitions to approximately calculate the value function. This section shows basic properties of the samples that can provide guarantees of the solution quality with incomplete samples.\nFirst, we show a formal definition of the samples and then show how to use them. The simplest samples are defined as follows.\nDefinition 25. One-step simple samples are defined as:\nΣ̃ ⊆ {(s, a, (s1 . . . sn), r(s, a)) s, s′ ∈ S, a ∈ A},\nwhere s1 . . . sn are selected i.i.d. from the distribution P (s, a).\nMore informative samples include the full distribution instead of samples from the distribution. While these samples are often unavailable in practice, they are useful in the theoretical analysis of sampling issues.\nDefinition 26. One-step samples with expectation are defined as follows:\nΣ̄ ⊆ {(s, a, P (s, a), r(s, a)) s ∈ S, a ∈ A}.\nMembership a state in the samples is denoted simply as s ∈ Σ or (s, a) ∈ Σ with the remaining variables, such as r(s, a) considered to be available implicitly.\nThe sampling models may vary significantly in different domains. The focus of this work is on problems with either a fixed set of available samples or a domain model. Therefore, we do not analyze methods for gathering samples. We also do not assume that the samples come from previous executions, but rather from a deliberate sample-gathering process.\nThe samples are used to approximate the Bellman operator and the set of transitivefeasible value functions.\nDefinition 27. The sampled Bellman operator and the corresponding set of sampled transitive-feasible functions are defined as:\n(L̄(v))(s̄) = max {a (s̄,a)∈Σ̄} r(s̄, a) + γ ∑ s′∈S P (s̄, a, s′)v(s′) ∀s̄ ∈ Σ̄ (16)\nK̄ = { v (s̄, a, P (s̄, a), r(s̄, a)) ∈ Σ̄, v(s̄) ≥ (L̄v)(s̄) } (17)\nThe less-informative set of samples Σ̃ can be used as follows.\nDefinition 28. The estimated Bellman operator and the corresponding set of estimated transitive-feasible functions are defined as:\n(L̃(v))(s̄) = max {a (s̄,a)∈Σ̃}\nr(s̄, a) + γ 1\nn n∑ i=1 v(si) ∀s̄ ∈ Σ̃ (18)\nK̃ = { v (s̄, a, (s1 . . . sn), r(s̄, a)) ∈ Σ̃, v(s̄) ≥ (L̃v)(s̄) } (19)\nNotice that operators L̃ and L̄ map value functions to a subset of all states — only states that are sampled. The values for other states are assumed to be undefined.\nThe samples can also be used to create an approximation of the initial distribution, or the distribution of visitation-frequencies of a given policy. The estimated initial distribution ᾱ is defined as:\nᾱ(s) = { α(s) (s, ·, ·, ·) ∈ Σ̄ 0 otherwise .\nThe existing sampling bounds for approximate linear programming focus on bounding the probability that a large number of constraints is violated when assuming a distribution over the constraints (de Farias and van Roy, 2004). The difficulty with this approach is that the bounds on the number of violated constraints do not transform easily to the bounds on the quality of the value function, or the policy. In addition, the constraint distribution is often somewhat arbitrary because it is difficult to define and sampling from the appropriate distributions.\nOur approach, on the other hand, is to define properties of the sampled operators that guarantee that the sampling error bounds are small. These bounds do not rely on distributions over constraints and transform directly to bounds on the policy loss. To define bounds on the sampling behavior, we propose the following assumptions.The first assumption limits the error due to missing transitions in the sampled Bellman operator L̄.\nAssumption 29 (Constraint Sampling Behavior). For all representable value functions v ∈M:\nK ⊆ K̄ ⊆ K( p)\nThe second assumption quantifies the error on the estimation of the transitions of the estimated Bellman operator L̃.\nAssumption 30 (Constraint Estimation Behavior). For all representable value functions v ∈M the following holds:\nK̄(− s) ⊆ K̃ ⊆ K̄( s).\nThese assumptions are intentionally made generic so that they apply to a wide range of scenarios. Domain specific assumptions are likely to lead to much tighter bounds, but these are beyond the scope of this paper.\nAlthough we define the sampled Bellman operator directly, in practice only its approximate version is typically estimated. The direct definitions are defined only for the sake of theoretical analysis. The sampled matrices used in bilinear program (ABP–L∞) are defined as follows for all (si, aj) ∈ Σ̃.\nÃΦ = (− φ(si)T − γ 1m∑s′∈s′1...s′m P (si, aj , s′)φ(s′)T − − ... − ) b̃ = ( r(si, aj) ... ) B̃(s′, (si, aj)) = I { s′ = si } ∀s′ ∈ Σ̃\nThe ordering over states in the definitions above is also assumed to be consistent. The sampled version of the bilinear program (ABP–L∞) is then:\nmin π λ,λ′,x\nπTλ+ λ′\ns.t. B̃π = 1 ÃΦx− b ≥ 0 π ≥ 0 λ+ λ′1 ≥ ÃΦx− b̃\nλ, λ′ ≥ 0\n(s–ABP–L∞)\nThe size of the bilinear program (s–ABP–L∞) scales with the number of samples and features, not with the size of the full MDP, because the variables λ and π are defined only for state — action pairs in Σ̃. That is |π| = |λ| = |{(s, a) ∈ Σ}|. The number of constraints in (s–ABP–L∞) is approximately three times the number of variables λ. Finally, the number of variables x corresponds to the number of approximation features.\nTheorem 13 shows that sampled robust ABP minimizes ‖v − L̃v‖∞ or ‖v − L̄v‖∞, depending on the samples used. It is then easy to derive sampling bounds that rely on the sampling assumptions defined above.\nTheorem 31. Let the optimal solutions to the sampled and precise Bellman residual minimization problems be:\nv1 ∈ min v∈M∩K ‖v − Lv‖∞ v2 ∈ min v∈M∩K ‖v − L̄v‖∞ v3 ∈ min v∈M∩K ‖v − L̃v‖∞\nValue functions v1, v2, v3 correspond to solutions of instances of robust approximate bilinear programs for the given samples. Also let v̂i = vπi, where πi is greedy with respect to vi. Then, given Assumptions 10, 29, and 30, the following holds:\n‖v∗ − v̂1‖∞ ≤ 2\n1− γ minv∈M ‖v − Lv‖∞\n‖v∗ − v̂2‖∞ ≤ 2\n1− γ ( min v∈M ‖v − Lv‖∞ + p )\n‖v∗ − v̂3‖∞ ≤ 2\n1− γ ( min v∈M ‖v − Lv‖∞ + p + 2 s )\nThese bounds show that it is possible to bound policy loss due to incomplete samples. As mentioned above, existing bounds on constraint violation in approximate linear programming (de Farias and van Roy, 2004) typically do not easily lead to policy loss bounds.\nSampling guarantees for other bilinear program formulations are very similar. Because they also rely on an approximation of the initial distribution and the policy loss, they require additional assumptions on uniformity of state-samples.\nProof. We show bounds on ‖vi−Lvi‖∞; the remainder of the theorem follows directly from Theorem 13. The second inequality follows from Assumption 29 and Lemma 36, as follows:\nv2 − Lv2 ≤ v2 − L̄v2 ≤ v1 − L̄v1 ≤ v1 − Lv1 + p1\nThe second inequality follows from Assumptions 29, 30 and Lemma 36, as follows:\nv3 − Lv3 ≤ v2 − L̄v2 + p1 ≤ v2 − L̃v2 + s1 + p1 ≤ v1 − L̃v1 + s1 + p1 ≤ v1 − Lv1 + 2 s1 + p1\nHere, we use the fact that vi ≥ Lvi and that vi’s minimize the corresponding Bellman residuals.\nTo summarize, this section identifies basic assumptions on the sampling behavior and shows that approximate bilinear programming scales well in the face of uncertainty caused by incomplete sampling. More detailed analysis will need to focus on identifying problemspecific assumptions and sampling modes that guarantee the basic conditions, namely satisfying Assumption 30 and Assumption 29. Such analysis is beyond the scope of this paper."
    }, {
      "heading" : "7. Discussion and Related ADP Methods",
      "text" : "This section describes connections between approximate bilinear programming and two closely related approximate dynamic programming methods: ALP, and L∞-API, which are commonly used to solve factored MDPs (Guestrin et al., 2003). Our analysis sheds light on some of their observed properties and leads to a new convergent form of approximate policy iteration.\nApproximate bilinear programming addresses some important issues with ALP: 1) ALP provides value function bounds with respect to L1 norm, which does not guarantee small policy loss, 2) ALP’s solution quality depends significantly on the heuristically-chosen objective function c in (ALP–L1) (de Farias, 2002), 3) the performance bounds involve a constant 1/(1 − γ) which can be very large when γ is close to 1 and 4) incomplete constraint samples in ALP easily lead to unbounded linear programs. The drawback of using approximate bilinear programming, however, is the higher computational complexity.\nThe first and the second issue in ALP can be addressed by choosing a problem-specific objective function c (de Farias, 2002). Unfortunately, all existing bounds require that c is chosen based on the optimal ALP solution for c. This is impossible to compute in practice. Heuristic values for c are used instead. Robust approximate bilinear program (ABP–L∞), on the other hand, has no such parameters. On the other hand, the expectedloss bilinear program (ABP–U) can be seen as a method for simultaneously optimizing c and the approximate linear program.\nThe fourth issue in approximate linear programs arises when the constraints need to be sampled. The ALP may become unbounded with incomplete samples because its objective value is defined using the L1 norm on the value function, and the constraints are defined using the L∞ norm of the Bellman residual. In approximate bilinear programs, the Bellman residual is used in both the constraints and objective function. The objective function of ABP is then bounded below by 0 for an arbitrarily small number of samples.\nThe NP-completeness of ABP compares unfavorably with the polynomial complexity of ALP. However, most other approximate dynamic programming algorithms are not guar-\nanteed to converge to a solution in finite time. As we show below, the exponential time complexity of ABP is unavoidable (unless P = NP).\nThe following theorem shows that the computational complexity of the ABP formulation is asymptotically the same as the complexity of tightly approximating the value function.\nTheorem 32. Assume 0 < γ < 1, and a given > 0. Then it is NP-complete to determine:\nmin v∈K∩M ‖Lv − v‖∞ < min v∈M ‖Lv − v‖∞ < .\nThe problem remains NP-complete when Assumption 10 is satisfied. It is also NP-complete to determine:\nmin v∈M ‖Lv − v‖∞ − ‖v∗ − v‖1,α < min v∈M ‖Lv − v‖1,ū − ‖v∗ − v‖1,α < ,\nassuming that ū is defined as in Remark 18.\nAs the theorem states, the value function approximation does not become computationally simpler even when Assumption 10 holds — a universal assumption in the paper. Notice that ALP can determine whether minv∈K∩M ‖Lv − v‖∞ = 0 in polynomial time.\nThe proof of Theorem 32 is based on a reduction from SAT and can be found in Section A.2. The policy in the reduction determines the true literal in each clause, and the approximate value function corresponds to the truth value of the literals. The approximation basis forces literals that share the same variable to have consistent values.\nApproximate bilinear programming can also improve on API with L∞ minimization (L∞-API for short), which is a leading method for solving factored MDPs (Guestrin et al., 2003). Minimizing the L∞ approximation error is theoretically preferable, since it is compatible with the existing bounds on policy loss (Guestrin et al., 2003). The bounds on value function approximation in API are typically (Munos, 2003):\nlim sup k→∞\n‖v∗ − v̂k‖∞ ≤ 2γ\n(1− γ)2 lim supk→∞ ‖ṽk − vk‖∞.\nThese bounds are looser than the bounds on solutions of ABP by at least a factor of 1/(1−γ). Often the difference may be up to 1/(1−γ)2 since the error ‖ṽk−vk‖∞ may be significantly larger than ‖ṽk−Lṽk‖∞. Finally, the bounds cannot be easily used, because they only hold in the limit.\nWe propose Optimistic Approximate Policy Iteration (OAPI), a modification of API. OAPI is shown in algorithm 1, where Z(π) is calculated using the following program:\nmin φ,v φ s.t. Av ≥ b (≡ (I− γPπ)v ≥ rπ ∀π ∈ Π) −(I− γPπ)v + 1φ ≥ −rπ v ∈M\n(20)\nIn fact, OAPI corresponds to algorithm 2 applied to ABP because the linear program (20) corresponds to (ABP–L∞) with a fixed π (see (8)). Then, using Proposition 23, we get the following corollary.\nCorollary 33. Optimistic approximate policy iteration converges in finite time. In addition, the Bellman residual of the generated value functions monotonically decreases.\nOAPI differs from L∞-API in two ways: 1) OAPI constrains the Bellman residuals by 0 from below and by φ from above, and then it minimizes φ. L∞-API constrains the Bellman residuals by φ from both above and below. 2) OAPI, like API, uses only the current policy for the upper bound on the Bellman residual, but uses all the policies for the lower bound on the Bellman residual.\nL∞-API cannot return an approximate value function that has a lower Bellman residual than ABP, given the optimality of ABP described in Theorem 13. However, even OAPI, an approximate ABP algorithm, performs comparably to L∞-API, as the following theorem states.\nTheorem 34. Assume that L∞-API converges to a policy π and a value function v that both satisfy: φ = ‖v − Lπv‖∞ = ‖v − Lv‖∞. Then\nṽ = v + φ\n1− γ1\nis feasible in the bilinear program (ABP–L∞), and it is a fixed point of OAPI. In addition, the greedy policies with respect to ṽ and v are identical.\nNotice that while the optimistic and standard policy iterations can converge to the same solutions, the steps in their computation may not be identical. The actual results will depend on the initialization.\nTo prove the theorem, we first consider L∞-API2 as a modification of L∞-API. L∞API2 is shown in algorithm 1, where Z(π) is calculated using the following program:\nmin φ,v φ s.t. (I− γPa)v + 1φ ≥ ra ∀a ∈ A −(I− γPπ)v + 1φ ≥ −rπ v ∈M\n(21)\nThe difference between linear programs (4) and (27) is that (4) involves only the current policy, while (27) bounds (I− γPa)v + 1φ ≥ ra from below for all policies. Linear program (27) differs from linear program (20) by not bounding the Bellman residual from below by 0.\nProposition 35. L∞-API and L∞-API2 generate the same sequence of policies if the initial policies and tie-breaking is the same.\nProof. The proposition follows simply by induction from Lemma 39. The basic step follows directly from the assumption. For the inductive step, let π1i = π 2 i , where π\n1 and π2 are the policies with (4) and (27). Then from Lemma 39, we have that the corresponding value functions v1i = v 2 i + c1. Because π 1 i+1 and π 2 i+1 are chosen greedily, we have that π1i+1 = π 2 i+1.\nThe proof of Theorem 34 follows.\nProof. The proof is based on two facts. First, ṽ is feasible with respect to the constraints in (ABP–L∞). The Bellman residual changes for all the policies identically, since a constant vector is added. Second, because Lπ is greedy with respect to ṽ, we have that ṽ ≥ Lπṽ ≥ Lṽ. The value function ṽ is therefore transitive-feasible.\nFrom Proposition 45, L∞-API can be replaced by L∞-API2, which will converge to the same policy π. L∞-API2 will converge to the value function\nṽ = v + φ\n1− γ1.\nFrom the constraints in (27) we have that ṽ ≥ Lπṽ. Then, since π is the greedy policy with regard to this value function, we have that ṽ ≥ Lπṽ ≥ Lṽ. Thus ṽ is transitive-feasible and feasible in (BP–m) according to Lemma 38. The theorem then follows from Lemma 39 and from the fact that the greedy policy minimizes the Bellman residual, as in the proof of Lemma 16.\nTo summarize, OAPI guarantees convergence, while matching the performance of L∞API. The convergence of OAPI is achieved because given a non-negative Bellman residual, the greedy policy also minimizes the Bellman residual. Because OAPI ensures that the Bellman residual is always non-negative, it can progressively reduce it. In comparison, the greedy policy in L∞-API does not minimize the Bellman residual, and therefore L∞API does not always reduce it. Theorem 34 also explains why API provides better solutions than ALP, as observed in (Guestrin et al., 2003). From the discussion above, ALP can be seen as an L1-norm approximation of a single iteration of OAPI. L∞-API, on the other hand, performs many such ALP-like iterations."
    }, {
      "heading" : "8. Experimental Results",
      "text" : "In this section, we validate the approach by applying it to simple reinforcement learning benchmark problems. The focus of the paper is on the theoretical properties and the experiments are intentionally designed to avoid interaction between the approximation in the formulation and approximate solution methods. As Theorem 34 shows, even OAPI, the very simple approximate algorithm for ABP, can perform as well as existing methods on factored MDPs.\nABP is an off-policy approximation method, like LSPI (Lagoudakis and Parr, 2003) or ALP. That means that the samples can be gathered independently of the control policy. It is necessary, though, that multiple actions are sampled for each state to enable the selection of different policies.\nFirst, we demonstrate and analyze the properties of ABP on a simple chain problem with 200 states, in which the transitions move to the right or left (2 actions) by one step with a centered Gaussian noise of standard deviation 3. The rewards were set to sin(i/20) for the right action and cos(i/20) for the left action, where i is the index of the state. This problem is small enough to calculate the optimal value function and to control the approximation features. The approximation basis in this problem is represented by piecewise linear features, of the form φ(si) = [i− c]+, for c from 1 to 200. The discount factor in the experiments was γ = 0.95 and the initial distribution was α(130) = 1. We verified that the solutions of the bilinear programs were always close to optimal, albeit suboptimal.\nWe experimented with the full state-action sample and randomly chose the features. All results are averages over 50 runs with 15 features. In the results, we use ABP to denote a close-to-optimal solution of robust ABP, ABPexp for the bilinear program (ABP–L1), and ABPhyb for (ABP–h) with k = 5. API denotes approximate policy iteration that minimizes the L2 norm.\nFigure 1 shows the Bellman residual attained by the methods. It clearly shows that the robust bilinear formulation most reliably minimizes the Bellman residual. The other two bilinear formulations are not much worse. Notice also the higher standard deviation of ALP and API. Figure 2 shows the expected policy loss, as specified in Definition 5, for the calculated value functions. It confirms that the ABP formulation outperforms the robust formulation, since its explicit objective is to minimize the expected loss. Similarly, Figure 3 shows the robust policy loss. As expected, it confirms the better performance of the robust ABP formulation in this case.\nNote that API and ALP may achieve lower policy loss on this particular domain than ABP formulations, even though their Bellman residual is significantly higher. This is possible since ABP simply minimizes bounds on the policy loss. The analysis of tightness of policy loss bounds is beyond the scope of this paper.\nIn the mountain-car benchmark, an underpowered car needs to climb a hill (Sutton and Barto, 1998). To do so, it first needs to back up to an opposite hill to gain sufficient momentum. The car receives a reward of 1 when it climbs the hill. The discount factor in the experiments was γ = 0.99.\nThe experiments are designed to determine whether OAPI reliably minimizes the Bellman residual in comparison with API and ALP. We use a uniformly-spaced linear spline to approximate the value function. The constraints were based on 200 uniformly sampled states with all 3 actions per state. We evaluated the methods with the number of the approximation features 100 and 144, which corresponds to the number of linear segments.\nThe results of robust ABP (in particular OAPI), ALP, API with L2 minimization, and LSPI are depicted in Table 1. The results are shown for both L∞ norm and uniformlyweighted L2 norm. The run-times of all these methods are comparable, with ALP being\nthe fastest. Since API (LSPI) is not guaranteed to converge, we ran it for at most 20 iterations, which was an upper bound on the number of iterations of OAPI. The results demonstrate that ABP minimizes the L∞ Bellman residual much more consistently than the other methods. Note, however, that all the considered algorithms would perform significantly better given a finer approximation."
    }, {
      "heading" : "9. Conclusion and Future Work",
      "text" : "We proposed and analyzed approximate bilinear programming, a new value-function approximation method, which provably minimizes bounds on policy loss. ABP returns the optimal approximate value function with respect to the Bellman residual bounds, despite being formulated with regard to transitive-feasible value functions. We also showed that there is no asymptotically simpler formulation, since finding the closest value function and\nsolving a bilinear program are both NP-complete problems. Finally, the formulation leads to the development of OAPI, a new convergent form of API which monotonically improves the objective value function.\nWhile we only discussed approximate solutions of the ABP, a deeper study of bilinear solvers may render optimal solution methods feasible. ABPs have a small number of essential variables (that determine the value function) and a large number of constraints, which can be leveraged by the solvers (Petrik and Zilberstein, 2007). The L∞ error bound provides good theoretical guarantees, but it may be too conservative in practice. A similar formulation based on L2 norm minimization may be more practical.\nWe believe that the proposed formulation will help to deepen the understanding of value function approximation and the characteristics of existing solution methods, and potentially lead to the development of more robust and widely-applicable reinforcement learning algorithms."
    }, {
      "heading" : "Appendix A. Proofs",
      "text" : "A.1 Properties of Transitive-Feasible Value Functions\nBasic properties of the Bellman operator, which we often use without a reference are the following.\nLemma 36. Let v be any value function and let c be a scalar. Then:\nL(v + c1) = Lv + γc1.\nLemma 37 (Monotonicity). Let P be a stochastic matrix. Then both the linear operators P and (I − γP )−1 are monotonous:\nx ≥ y ⇒ Px ≥ Py x ≥ y ⇒ (I − γP )−1x ≥ (I − γP )−1y\nfor all x and y.\nLemma 4. Transitive feasible value functions form an upper bound on the optimal value function. If v ∈ K( ) is an -transitive-feasible value function, then\nv ≥ v∗ − 1− γ1.\nProof. Let P ∗ and r∗ be the transition matrix and the reward vector of the policy. Then, we have using Lemma 37:\nv ≥ Lv − 1 v ≥ γP ∗v + r∗ − 1\n(I− γP ∗)v ≥ r∗ − 1 v ≥ (I− γP ∗)−1 r∗ −\n1− γ\nLemma 38. A value function v satisfies Av ≥ b if an only if v ≥ Lv. In addition, if v is feasible in (ABP–L∞), then v ≥ v∗.\nProof. The backward implication of the first part of the lemma follows directly from the definition. The forward implication follows by an existence of λ = 0, λ′ = ‖ [Av − r]+ ‖∞, which satisfy the constraints. The constraints on π are independent and therefore can be satisfied independently. The second part of the lemma also holds in ALPs (de Farias, 2002) and is proven identically.\nThe minimization minv∈M ‖Lv− v‖∞ for a policy π can be represented as the following linear program.\nmin φ,v φ s.t. (I− γPπ)v + 1φ ≥ rπ −(I− γPπ)v + 1φ ≥ −rπ v ∈M\n(22)\nConsider also the following linear program.\nmin φ,v φ s.t. (I− γPπ)v ≥ rπ −(I− γPπ)v + 1φ ≥ −rπ v ∈M\n(23)\nNext we show that the optimal solutions of (22) and (23) are closely related.\nLemma 39. Assume Assumption 10 and a given policy π. Let φ1, v1 and φ2, v2 optimal solutions of linear programs (22) and (23) respectively. Define:\nv̄1 = v1 + φ1 1− γ v̄2 = v1 − φ2 2(1− γ)1\nThen: 1. 2φ1 = φ2 2. v̄1 is an optimal solution in (23). 3. v̄2 is an optimal solution in (22). 4. Greedy policies with respect to v1 and v̄1 are identical. 5. Greedy policies with respect to v2 and v̄2 are identical.\nProof. Let φ̄1 = 2φ1 and φ̄2 = φ2 2 . We first show φ̄1, v̄1 is feasible in (23). It is representable since 1 ∈M and it is feasible by the following simple algebraic manipulation:\n(I− γPπ)v̄1 = (I− γPπ)v1 + (I− γPπ) φ1\n1− γ1\n= (I− γPπ)v1 + φ11 ≥ −φ11 + rπ + φ11 = rπ\nand\n−(I− γPπ)v̄1 + φ̄11 = −(I− γPπ)v̄1 + 2φ11\n= −(I− γPπ)v1 − (I− γPπ) φ1\n1− γ1 + 2φ11\n= −(I− γPπ)v1 − φ11 + 2φ11 ≥ −φ11− rπ + 2φ11 = −rπ\nNext we show that φ̄2, v̄2 is feasible in (22). This solution is representable, since 1 ∈M, and it is feasible by the following simple algebraic manipulation:\n(I− γPπ)v̄2 + φ̄21 = (I− γPπ)v2 − (I− γPπ) φ2 2(1− γ)1 + φ2 2 1\n= (I− γPπ)v2 − φ2 2 1 + 1 2 φ̄21 = (I− γPπ)v2 ≥ rπ\nand\n−(I− γPπ)v̄2 + φ̄21 = −(I− γPπ)v̄2 + φ2 2 1\n= −(I− γPπ)v2 − (I− γPπ) φ2 1− γ1 + φ2 2 1 = −(I− γPπ)v2 + φ21 ≥ −rπ\nIt is now easy to shows that φ̄1, v̄1 is optimal by contradiction. Assume that there exists a solution φ2 < φ̄1. But then: 2φ̄2 ≤ φ2 < φ̄1 ≤ 2φ1, which is a contradiction with the optimality of φ1. The optimality of φ̄2, v̄2 can be shown similarly.\nProposition 40. Assumption 10 implies that:\nmin v∈M∩K ‖Lv − v‖∞ ≤ 2 min v∈M ‖Lv − v‖∞.\nProof. Let v̂ be the minimizer of φ̂ = minv∈M ‖Lv − v‖∞, and let π̂ be a policy that is greedy with respect to v̂. Define:\nṽ = v̂ + φ̂\n1− γ .\nThen from Lemma 39: 1. Value function ṽ is an optimal solution of (23): ṽ ≥ Lπṽ 2. Policy π̂ is greedy with regard to ṽ: Lπ̂ṽ ≥ Lṽ 3. ‖Lπṽ − ṽ‖∞ = 2φ̂ Then using a simple algebraic manipulation:\nṽ ≥ Lπ̂ṽ = Lṽ\nand the proposition follows from Lemma 38.\nProposition 41. Let ṽ be a solution of the approximate bilinear program (ABP–L∞) and let:\nv′ = v − 1/2 (1− γ)‖Lv − v‖∞1.\nThen:\n1. ‖Lv′ − v′‖∞ = ‖Lv−v‖∞2 . 2. Greedy policies with respect to v and v′ are identical.\nThe proposition follows directly from Lemma 39.\nProposition 42. Assumption 10 implies that:\nmin v∈M ‖Lv − v‖∞ ≤ (1 + γ) min v∈M ‖v − v∗‖∞.\nProof. Assume that v̂ is the minimizer of minv∈M ‖v − v∗‖∞ ≤ . Then: v∗ − 1 ≤ v ≤ v∗ + 1\nLv∗ − γ 1 ≤ Lv ≤ Lv∗ + γ 1 Lv∗ − γ 1− v ≤ Lv − v ≤ Lv∗ + γ 1− v\nLv∗ − v∗ − (1 + γ) 1 ≤ Lv − v ≤ Lv∗ − v∗ + (1 + γ) 1 −(1 + γ) 1 ≤ Lv − v ≤ (1 + γ) 1.\nProposition 24. Let ṽ1 and ṽ2 be feasible value functions in (ABP–L∞). Then the value function ṽ(s) = min{ṽ1(s), ṽ2(s)} is also feasible in bilinear program (ABP–L∞). Therefore ṽ ≥ v∗ and\n‖v∗ − ṽ‖∞ ≤ min {‖v∗ − ṽ1‖∞, ‖v∗ − ṽ2‖∞} .\nProof. Consider a state s and action a. Then from transitive feasibility of the value functions ṽ1 and ṽ2 we have:\nṽ1(s) ≥ γ ∑ s′∈S P (s′, a, a)ṽ1(s ′) + r(s, a)\nṽ2(s) ≥ γ ∑ s′∈S P (s′, a, a)ṽ2(s ′) + r(s, a).\nFrom the convexity of the min operator we have that:\nmin {∑ s′∈S P (s′, a, a)ṽ1, ∑ s′∈S P (s′, a, a)ṽ2(s ′) } ≥ ∑ s′∈S P (s′, a, a) min{ṽ1(s′), ṽ2)(s′)}.\nThen the proposition follows by the following simple algebraic manipulation: ṽ = min{ṽ1(s), ṽ2(s)} ≥ γmin {∑ s′∈S P (s′, a, a)ṽ1, ∑ s′∈S P (s′, a, a)ṽ2(s ′) } + r(s, a)\n≥ γ ∑ s′∈S P (s′, a, a) min{ṽ1(s′), ṽ2)(s′)}+ r(s, a)\n= γ ∑ s′∈S P (s′, a, a)ṽ(s) + r(s, a).\nLemma 43. Let uπ be the state-action visitation frequency of policy π. Then:\n1Tu = 1\n1− γ .\nProof. Let ua(s) = uπ(s, π(s, a)) for all states s ∈ S and actions a ∈ A. The lemma follows as: ∑\na∈A uTa (I− γPa) = cT∑ a∈A uTa (I− γPa)1 = cT1\n(1− γ) ∑ a∈A uTa1 = 1\nuT1 = 1\n1− γ .\nA.2 NP-Completeness\nProposition 44 (e.g. (Mangasarian, 1995)). A bilinear program can be solved in NP time.\nThere is an optimal solution of the bilinear program such that the solutions of the individual linear programs are basic feasible. The set of all basic feasible solutions is finite, because the feasible regions of w, x and y, z are independent. The value of a basic feasible solution can be calculated in polynomial time. Theorem 32. Assume 0 < γ < 1, and a given > 0. Then it is NP-complete to determine:\nmin v∈K∩M ‖Lv − v‖∞ < min v∈M ‖Lv − v‖∞ < .\nThe problem remains NP-complete when Assumption 10 is satisfied. It is also NP-complete to determine:\nmin v∈M ‖Lv − v‖∞ − ‖v∗ − v‖1,α < min v∈M ‖Lv − v‖1,ū − ‖v∗ − v‖1,α < ,\nassuming that ū is defined as in Remark 18.\nProof. The membership in NP follows from Theorem 13 and Proposition 44. We show NPhardness by a reduction from the 3SAT problem. We first don’t assume Assumption 10. We show the theorem for = 1. The appropriate can be obtained by simply scaling the rewards in the MDP.\nThe main idea is to construct an MDP and an approximation basis, such that the approximation error is small whenever the SAT is satisfiable. The value of the states will correspond to the truth value of the literals and clauses. The approximation features φ will be used to constraint the values of literals that share the same variable. The MDP constructed from the SAT formula is depicted in Figure 4.\nConsider a SAT problem with clauses Ci:∧ i=1,...,n Ci = ∧ i=1,...,n (li1 ∨ li2 ∨ li3) ,\nwhere lij are literals. A literal is a variable or a negation of a variable. The variables in the SAT are x1 . . . xm. The corresponding MDP is constructed as follows. It has one state s(lij) for every literal lij , one state s(Ci) for each clause Ci and an additional state s̄. That is: S = {s(Ci) i = 1, . . . , n} ∪ {s(lij) i = 1, . . . , n, j = 1, . . . , 3} ∪ {s̄}. There are 3 actions available for each state s(Ci), which determine the true literal of the clause. There is only a single action available in states s(lij) and s̄. All transitions in the MDP are deterministic. The transition t(s, a) = (s′, r) is from the state s to s′, when action a is taken, and the reward received is r. The transitions are the following:\nt(s(Ci), aj) = (s(lij), 1− γ) (24) t(s(lij), a) = (s(lij),−(1− γ)) (25)\nt(s̄, a) = (s̄, 2− γ) (26) Notice that the rewards depend on the discount factor γ, for notational convenience.\nThere is one approximation feature for every variable xk such that:\nφk(s(Ci)) = 0\nφk(s̄) = 0\nφk(s(lij)) =\n{ 1 if lij = xk\n−1 if lij = ¬xk An additional feature in the problem φ̄ is defined as:\nφ̄(s(Ci)) = 1\nφ̄(s(lij)) = 0\nφ̄(s̄) = 1.\nThe purpose of state s̄ is to ensure that v(s(ci)) ≥ 2− γ, as we assume in the remainder of the proof.\nFirst, we show that if the SAT problem is satisfiable, then minv∈M∩K ‖Lv − v‖∞ < 1. The value function ṽ ∈ K is constructed as a linear sum of the features as: v = Φy, where y = (y1, . . . , ym, ȳ). Here yk corresponds to φk and ȳ corresponds to φ̄. The coefficients yk are constructed from the truth value of the variables as follows:\nyk =\n{ γ if xk = true\n−γ if xk = false ȳ = 2− γ.\nNow define the deterministic policy π as:\nπ(s(Ci)) = aj where lij = true .\nThe true literals are guaranteed to exist from the satisfiability. This policy is greedy with respect to ṽ and satisfies that ‖Lπṽ − ṽ‖∞ ≤ 1− γ2.\nThe Bellman residuals for all actions and states, given a value function v, are defined as: v(s)− γv(s′)− r, where t(s, a) = (s′, r). Given the value function ṽ, the residual values are:\nt(s(Ci), aj) = (s(lij), 1− γ) : {\n2− γ − γ2 + (1− γ) = 1− γ2 if lij = true 2− γ + γ2 + (1− γ) = 1 + γ2 if lij = false\nt(s(lij), a) = (s(lij), (1− γ)) : { γ − γ2 + 1− γ = 1− γ2 if lij = true −γ + γ2 + 1− γ = (1− γ)2 > 0 if lij = false\nt(s̄, a) = (s̄, 1− γ) : (1− γ) + γ − 1 = 0\nIt is now clear that π is greedy and that:\n‖Lṽ − ṽ‖∞ = 1− γ2 < 1. We now show that if the SAT problem is not satisfiable then minv∈K∩M ‖Lv − v‖∞ ≥ 1 − γ22 . Now, given a value function v, there are two possible cases for each v(s(lij)): 1) a positive value, 2) a non-positive value. Two literals that share the same variable will have the same sign, since there is only one feature per each variable.\nAssume now that there is a value function ṽ. There are two possible cases we analyze: 1) all transitions of a greedy policy are to states with positive value, and 2) there is at least one transition to a state with a non-positive value. In the first case, we have that\n∀i ∃j, ṽ(s(lij)) > 0. That is, there is a function q(i), which returns the positive literal for the clause j. Now, create a satisfiable assignment of the SAT as follows:\nxk =\n{ true if liq(i) = xk\nfalse if liq(i) = ¬xk ,\nwith other variables assigned arbitrary values. Given this assignment, all literals with states that have a positive value will be also positive. Since every clause contains at least one positive literal, the SAT is satisfiable, which is a contradiction with the assumption. Therefore, there is at least one transition to a state with a non-positive value.\nLet C1 represent the clause with a transition to a literal l11 with a non-positive value, without loss of generality. The Bellman residuals at the transitions from these states will be:\nb1 = ṽ(s(l11))− γṽ(s(l11)) + (1− γ) ≥ 0− 0 + (1− γ) = 1− γ b1 = ṽ(s(C1))− γṽ(s(l11))− (1− γ) ≥ 2− γ − 0− 1 + γ = 1\nTherefore, the Bellman residual ṽ is bounded as:\n‖Lṽ − ṽ‖∞ ≥ max{b1, b2} ≥ 1.\nSince we did not make any assumptions on ṽ, the claim holds for all representable and transitive-feasible value functions. Therefore, minv∈M∩K ‖Lv− v‖∞ ≤ 1− γ2 is and only if the 3-SAT problem is feasible.\nIt remains to show that the problem remains NP-complete even when Assumption 10 holds. Define a new state s̄1 with the following transition:\nt(s̄2, a) = (s̄2,− γ\n2 ).\nAll previously introduced features φ are zero on the new state. That is φk(s̄1) = φ̄(s̄1) = 0. The new constant feature is: φ̂(s) = 1 for all states s ∈ S, and the matching coefficient is denoted as ȳ1. When the formula is satisfiable, then clearly minv∈M∩K ‖Lv− v‖∞ ≤ 1− γ2 since the basis is now richer and the Bellman error on the new transition is less than 1− γ2 when ȳ1 = 0.\nNow we show that when the formula is not satisfiable, then:\nmin v∈M∩K\n‖Lv − v‖∞ ≥ 1− γ2\n2 .\nThis can be scaled to an appropriate by scaling the rewards. Notice that\n0 ≤ ȳ1 ≤ γ\n2 .\nWhen ȳ1 < 0, the Bellman residual on transitions s(Ci) → s(lij) may be decreased by increasing ȳ1 while adjusting other coefficients to ensure that v(s(Ci)) = 2 − γ. When ȳ1 > γ 2 then the Bellman residual from the state s̄1 is greater than 1 − γ2\n2 . Given the bounds on ȳ1, the argument for yk = 0 holds and the minimal Bellman residual is achieved when:\nv(s(Ci))− γv(s(lij))− (1− γ) = v(s(s̄1))− γv(s(s̄1)) + γ\n2\n2− γ − γȳ1 − (1− γ) = ȳ1 − γȳ1 + γ\n2\nȳ1 = γ\n2 .\nTherefore, when the SAT is unsatisfiable, the Bellman residual is at least 1− γ22 . The NP-completeness of minv∈M ‖Lv − v‖∞ < follows trivially from Proposition 40. The proof for ‖v − Lv‖∞ − αTv is almost identical. The difference is a new state ŝ, such that φ(ŝ) = 1 and α(ŝ) = 1. In that case αTv = 1 for all v ∈M. The additional term thus has no effect on the optimization.\nThe proof can be similarly extended to the minimization of ‖v−Lv‖1,ū. Define ū(Ci) = 1/n and ū(lij) = 0. Then the SAT problem is satisfiable if an only if ‖v − Lv‖1,ū = 1− γ2. Note that ū, as defined above, is not an upper bound on the visitation frequencies uπ. It is likely that the proof could be extended to cover the case ū ≥ uπ by more carefully designing the transitions from Ci. In particular, there needs to be high probability of returning to Ci and ū(lij > 0.\nA.3 Equivalence of OAPI and API\nWe first consider L∞-API2 as a modification of L∞-API. L∞-API2 is shown in algorithm 1, where f(π) is calculated using the following program:\nmin φ,v φ s.t. (I− γPπ)v + 1φ ≥ rπ −(I− γPπ)v + 1φ ≥ −rπ v ∈M\n(27)\nProposition 45. L∞-API and L∞-API2 generate the same sequence of policies if the initial policies and tie-breaking is the same.\nProof. The proposition follows simply by induction from Lemma 39. The basic step follows directly from the assumption. For the inductive step, let π1i = π 2 i , where π\n1 and π2 are the policies with (4) and (27). Then from Lemma 39, we have that the corresponding value functions v1i = v 2 i + c1. Because π 1 i+1 and π 2 i+1 are chosen greedily, we have that π1i+1 = π 2 i+1.\nWe are ready now to prove the theorem. Theorem 34. Assume that L∞-API converges to a policy π and a value function v that both satisfy: φ = ‖v − Lπv‖∞ = ‖v − Lv‖∞. Then\nṽ = v + φ\n1− γ1\nis feasible in the bilinear program (ABP–L∞), and it is a fixed point of OAPI. In addition, the greedy policies with respect to ṽ and v are identical.\nProof. From Proposition 45, L∞-API can be replaced by L∞-API2, which will converge to the same policy π. L∞-API2 will converge to the value function\nṽ = v + φ\n1− γ1.\nFrom the constraints in (27) we have that:\nṽ ≥ Lπṽ.\nThen, since π is the greedy policy with regard to this value function, we have that:\nṽ ≥ Lπṽ ≥ Lṽ.\nThus ṽ is transitive-feasible and feasible in (BP–m) according to Lemma 38. The theorem then follows from Lemma 39 and from the fact that the greedy policy minimizes the Bellman residual, as in the proof of Lemma 16.\nNotice that while the optimistic and standard policy iterations can converge to the same solutions, the steps in their computation may not be identical. The actual results will depend on the initialization."
    } ],
    "references" : [ {
      "title" : "Learning vehicular dynamics, with application to modeling helicopters",
      "author" : [ "Pieter Abbeel", "Varun Ganapathi", "Andrew Y. Ng" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Abbeel et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Abbeel et al\\.",
      "year" : 2006
    }, {
      "title" : "A price-directed approach to stochastic inventory/routing",
      "author" : [ "Daniel Adelman" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Adelman.,? \\Q2004\\E",
      "shortCiteRegEx" : "Adelman.",
      "year" : 2004
    }, {
      "title" : "Dynamic Programming",
      "author" : [ "Richard Bellman" ],
      "venue" : null,
      "citeRegEx" : "Bellman.,? \\Q1957\\E",
      "shortCiteRegEx" : "Bellman.",
      "year" : 1957
    }, {
      "title" : "Bilinear separation of two sets in n-space",
      "author" : [ "Kristin P. Bennett", "Olvi L. Mangasarian" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Bennett and Mangasarian.,? \\Q1992\\E",
      "shortCiteRegEx" : "Bennett and Mangasarian.",
      "year" : 1992
    }, {
      "title" : "Temporal differences-based policy iteration and applications in neuro-dynamic programming",
      "author" : [ "Dimitri P. Bertsekas", "Sergey Ioffe" ],
      "venue" : "Technical Report LIDS-P-2349,",
      "citeRegEx" : "Bertsekas and Ioffe.,? \\Q1997\\E",
      "shortCiteRegEx" : "Bertsekas and Ioffe.",
      "year" : 1997
    }, {
      "title" : "Bidimensional packing by bilinear programming",
      "author" : [ "Alberto Carpara", "Michele Monaci" ],
      "venue" : "Mathematical Programming Series A,",
      "citeRegEx" : "Carpara and Monaci.,? \\Q2009\\E",
      "shortCiteRegEx" : "Carpara and Monaci.",
      "year" : 2009
    }, {
      "title" : "The Linear Programming Approach to Approximate Dynamic Programming: Theory and Application",
      "author" : [ "Daniela P. de Farias" ],
      "venue" : "PhD thesis, Stanford University,",
      "citeRegEx" : "Farias.,? \\Q2002\\E",
      "shortCiteRegEx" : "Farias.",
      "year" : 2002
    }, {
      "title" : "The linear programming approach to approximate dynamic programming",
      "author" : [ "Daniela P. de Farias", "Ben van Roy" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Farias and Roy.,? \\Q2003\\E",
      "shortCiteRegEx" : "Farias and Roy.",
      "year" : 2003
    }, {
      "title" : "On constraint sampling in the linear programming approach to approximate dynamic programming",
      "author" : [ "Daniela Pucci de Farias", "Benjamin van Roy" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Farias and Roy.,? \\Q2004\\E",
      "shortCiteRegEx" : "Farias and Roy.",
      "year" : 2004
    }, {
      "title" : "Efficient solution algorithms for factored MDPs",
      "author" : [ "Carlos Guestrin", "Daphne Koller", "Ronald Parr", "Shobha Venkataraman" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Guestrin et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Guestrin et al\\.",
      "year" : 2003
    }, {
      "title" : "Global optimization: Deterministic approaches",
      "author" : [ "Reiner Horst", "Hoang Tuy" ],
      "venue" : null,
      "citeRegEx" : "Horst and Tuy.,? \\Q1996\\E",
      "shortCiteRegEx" : "Horst and Tuy.",
      "year" : 1996
    }, {
      "title" : "Least-squares policy iteration",
      "author" : [ "Michail G. Lagoudakis", "Ronald Parr" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Lagoudakis and Parr.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lagoudakis and Parr.",
      "year" : 2003
    }, {
      "title" : "The linear complementarity problem as a separable bilinear program",
      "author" : [ "Olvi L. Mangasarian" ],
      "venue" : "Journal of Global Optimization,",
      "citeRegEx" : "Mangasarian.,? \\Q1995\\E",
      "shortCiteRegEx" : "Mangasarian.",
      "year" : 1995
    }, {
      "title" : "Error bounds for approximate policy iteration",
      "author" : [ "Remi Munos" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Munos.,? \\Q2003\\E",
      "shortCiteRegEx" : "Munos.",
      "year" : 2003
    }, {
      "title" : "Anytime coordination using separable bilinear programs",
      "author" : [ "Marek Petrik", "Shlomo Zilberstein" ],
      "venue" : "In Conference on Artificial Intelligence,",
      "citeRegEx" : "Petrik and Zilberstein.,? \\Q2007\\E",
      "shortCiteRegEx" : "Petrik and Zilberstein.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Lorincz, 2006) and helicopter control (Abbeel et al., 2006).",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 4,
      "context" : "Approximate dynamic programming (ADP) methods iteratively approximate the value function (Bertsekas and Ioffe, 1997; Powell, 2007; Sutton and Barto, 1998).",
      "startOffset" : 89,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "The approximation error bounds are usually expressed in terms of the worst-case approximation of the value function over all policies (Bertsekas and Ioffe, 1997).",
      "startOffset" : 134,
      "endOffset" : 161
    }, {
      "referenceID" : 13,
      "context" : "While there exist some L2-based bounds (Munos, 2003), they require values that are difficult to obtain.",
      "startOffset" : 39,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "ALP has been previously used in a wide variety of settings (Adelman, 2004; de Farias and van Roy, 2004; Guestrin et al., 2003).",
      "startOffset" : 59,
      "endOffset" : 126
    }, {
      "referenceID" : 9,
      "context" : "ALP has been previously used in a wide variety of settings (Adelman, 2004; de Farias and van Roy, 2004; Guestrin et al., 2003).",
      "startOffset" : 59,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "Many methods that compute a value function based on a given set of features have been developed, such as neural networks and genetic algorithms (Bertsekas and Ioffe, 1997).",
      "startOffset" : 144,
      "endOffset" : 171
    }, {
      "referenceID" : 11,
      "context" : "The two most commonly used methods — Bellman residual approximation and least-squares approximation (Lagoudakis and Parr, 2003) — minimize the L2 norm of the Bellman residual.",
      "startOffset" : 100,
      "endOffset" : 127
    }, {
      "referenceID" : 2,
      "context" : "The ideas of approximate value iteration could be traced to Bellman (1957), which was followed by many additional research efforts (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998; Powell, 2007).",
      "startOffset" : 60,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "Bilinear programs can be easily mapped to other global optimization problems, such as mixed integer linear programs (Horst and Tuy, 1996).",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "Solving a bilinear program is an NP-complete problem (Bennett and Mangasarian, 1992).",
      "startOffset" : 53,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "The relaxation of the bilinear program is typically either a linear or semi-definite program (Carpara and Monaci, 2009).",
      "startOffset" : 93,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : "The bilinear program formulation is then refined — using concavity cuts (Horst and Tuy, 1996) — to eliminate previously computed feasible solutions and solved again.",
      "startOffset" : 72,
      "endOffset" : 93
    }, {
      "referenceID" : 12,
      "context" : "While algorithm 2 is not guaranteed to find an optimal solution, its empirical performance is often remarkably good (Mangasarian, 1995).",
      "startOffset" : 116,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "(Bennett and Mangasarian, 1992)).",
      "startOffset" : 0,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "Discussion and Related ADP Methods This section describes connections between approximate bilinear programming and two closely related approximate dynamic programming methods: ALP, and L∞-API, which are commonly used to solve factored MDPs (Guestrin et al., 2003).",
      "startOffset" : 240,
      "endOffset" : 263
    }, {
      "referenceID" : 9,
      "context" : "Approximate bilinear programming can also improve on API with L∞ minimization (L∞-API for short), which is a leading method for solving factored MDPs (Guestrin et al., 2003).",
      "startOffset" : 150,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : "Minimizing the L∞ approximation error is theoretically preferable, since it is compatible with the existing bounds on policy loss (Guestrin et al., 2003).",
      "startOffset" : 130,
      "endOffset" : 153
    }, {
      "referenceID" : 13,
      "context" : "The bounds on value function approximation in API are typically (Munos, 2003):",
      "startOffset" : 64,
      "endOffset" : 77
    }, {
      "referenceID" : 9,
      "context" : "Theorem 34 also explains why API provides better solutions than ALP, as observed in (Guestrin et al., 2003).",
      "startOffset" : 84,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "ABP is an off-policy approximation method, like LSPI (Lagoudakis and Parr, 2003) or ALP.",
      "startOffset" : 53,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "ABPs have a small number of essential variables (that determine the value function) and a large number of constraints, which can be leveraged by the solvers (Petrik and Zilberstein, 2007).",
      "startOffset" : 157,
      "endOffset" : 187
    } ],
    "year" : 2010,
    "abstractText" : "Existing value function approximation methods have been successfully used in many applications, but they often lack useful a priori error bounds. We propose a new approximate bilinear programming formulation of value function approximation, which employs global optimization. The formulation provides strong a priori guarantees on both robust and expected policy loss by minimizing specific norms of the Bellman residual. Solving a bilinear program optimally is NP-hard, but this is unavoidable because the Bellman-residual minimization itself is NP-hard. We describe and analyze both optimal and approximate algorithms for solving bilinear programs. The analysis shows that this algorithm offers a convergent generalization of approximate policy iteration. We also briefly analyze the behavior of bilinear programming algorithms under incomplete samples. Finally, we demonstrate that the proposed approach can consistently minimize the Bellman residual on simple benchmark problems.",
    "creator" : "LaTeX with hyperref package"
  }
}