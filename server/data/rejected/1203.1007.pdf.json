{
  "name" : "1203.1007.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Agnostic System Identification for Model-Based Reinforcement Learning",
    "authors" : [ "Stéphane Ross", "Andrew Bagnell" ],
    "emails" : [ "STEPHANEROSS@CMU.EDU", "DBAGNELL@RI.CMU.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Model-based reinforcement learning (MBRL) and much of control rely on system identification: building a model of a system from observations that is useful for controller synthesis. While often treated as a typical statistical learning problem, system identification presents different fundamental challenges as the executed controller and data generating process are inextricably intertwined. Naively attempting to estimate a controlled system can lead to a model that makes small error on a training set, but exhibits poor controller performance. This problem arises as the policy resulting from controller synthesis is often very different from the “exploration” policy used to collect data. While we might expect the model to make good predictions at states frequented by the exploration policy, the learned\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nFigure 1. Example train-test mismatch in a helicopter domain. Train: model is fit based on samples near the desired trajectory, e.g. from watching an expert. Test: learned policy ends up in new regions where model is bad, leading to poor control performance.\npolicy usually induces a different state distribution, where the model may poorly capture system behavior (Fig. 1).\nThis problem is fully appreciated in the system identification literature and has been attacked by considering “open loop” identification procedures and “persistent excitation” (Ljung, 1999; Abbeel & Ng, 2005) that attempt to sufficiently “cover” the state-action space. Unfortunately, such methods rely on the strong assumption that the true system lies in the class of models considered: e.g., for continuous systems, they may require the true system to be modeled in a class of linear models. With this assumption, they ensure that eventually the correct model is learned– e.g., by learning about every discrete state-action pair or all modes of the linear system– to provide guarantees.\nIn this work, we provide algorithms for system identification and controller synthesis (i.e. MBRL) that have strong performance guarantees with a weaker agnostic assumption that the system identification achieves statistically good prediction. We adopt a reduction-based analysis (Beygelzimer et al., 2005) that relates the learned policy’s performance to prediction error during training. We begin by providing agnostic bounds for a simple generic “batch” algorithm that can represent many learning methods used in practice (e.g., building a model from open loop controls,\nar X\niv :1\n20 3.\n10 07\nv2 [\ncs .L\nG ]\n3 J\nul 2\n01 2\nwatching an expert, or running a base policy we want to improve upon). Due to the mismatch in train/test distributions, uniform exploration is often the best option with this approach. Unfortunately, this makes the sample complexity and performance bounds scale with the size of the Markov Decision Process (MDP) (i.e. state/action space). Next, we propose a simple iterative approach, closely related to online learning, with stronger guarantees that do not scale with the size of the MDP when given a good exploration distribution. The approach is very simple to implement and iterates between 1) collecting new data about the system by executing a good policy under the current model, as well as by sampling from a given exploration distribution, and 2) updating the model with that new data.\nThis approach is inspired by a recent reduction of imitation learning to no-regret online learning (Ross et al., 2011) that addresses mismatch between train/test distributions. Our results can be interpreted as a reduction of MBRL to noregret online learning and optimal control, and show that any no-regret algorithm can be used in such a way to learn a policy with strong agnostic guarantees. This enables MBRL methods to match the strongest existing agnostic guarantees of model-free RL methods (Kakade & Langford, 2002; Bagnell et al., 2003).\nWe first introduce notation and related work. Then we present the batch method and our online learning approach with their agnostic guarantees (proofs are deferred to the supplementary material). Finally we demonstrate the efficacy of our approach on a challenging domain from the literature: learning to perform aerobatic maneuvers with a simulated helicopter (Abbeel & Ng, 2005)."
    }, {
      "heading" : "2. Background and Notation",
      "text" : "We assume the real system behaves according to some unknown MDP, represented by a set of states S and actions A (both potentially infinite and continuous), a transition function T , where Tsa denotes the next state distribution if we do action a in state s, and the initial state distribution µ at time 1. We assume the cost function C : S × A → R is known and seek to minimize the expected sum of discounted costs over an infinite horizon with discount γ.\nFor any policy π, let πs be the action distribution performed by π in state s; Dtω,π the state-action distribution at time t if we started in state distribution ω at time 1 and followed π; Dω,π = (1 − γ) ∑∞ t=1 γ\nt−1Dtω,π the state-action distribution over the infinite horizon if we follow π, starting in ω at time 1; Vπ(s) = Ea∼πs,s′∼Tsa [C(s, a) + γVπ(s′)] the value function of π (the expected sum of discounted costs of following π starting in state s); Qπ(s, a) = C(s, a) + γEs′∼Tsa [Vπ(s′)] the action-value function of π (the expected sum of discounted costs of following π af-\nter starting in s and performing action a); and Jω(π) = Es∼ω[Vπ(s)] = 11−γE(s,a)∼Dω,π [C(s, a)] the expected sum of discounted costs of following π starting in ω.\nOur goal is to obtain a policy π with small regret, i.e. for any policy π′, Jµ(π) − Jµ(π′) is small. This is achieved indirectly by learning a model T̂ of the system and solving for a (near-)optimal policy (under T̂ ); e.g., using dynamic programming (Puterman, 1994) or approximate methods (Szepesvári, 2005; Williams, 1992). For continuous systems, an important special case is linear models with quadratic cost functions, and potentially additive Gaussian noise, known as Linear Quadratic Regulators (LQR)1 which can be solved exactly and efficiently. Non-linear systems with non-quadratic cost functions can also be solved approximately (local optima) using efficient iterative linearization techniques such as iLQR(Li & Todorov, 2004).\nRelated Work: In contrast with “textbook” system identification methods, in practice control engineers often proceed iteratively to build good models for controller synthesis. A first batch of data is collected to fit a model and obtain a controller, which is then tested in the real system. If performance is unsatisfactory, data collection is repeated with different sampling distributions to improve the model where needed, until control performance is satisfactory. By doing so, engineers can use feedback of the policies found during training to decide how to collect data and improve performance. Such methods are commonly used in practice and have demonstrated good performance in the work of Atkeson & Schaal (1997); Abbeel & Ng (2005). In both works, the authors proceed by fitting a first model from state transitions observed during expert demonstrations of the task, and at following iterations, using the optimal policy under the current model to collect more data and fit a new model with all data seen so far. Abbeel & Ng (2005) show this approach has good guarantees in non-agnostic settings (for finite MDPs or LQRs), in that it must find a policy that performs as well as the expert providing the initial demonstrations. Our method can be seen as making algorithmic this engineering practice, extending and generalizing the previous methods of Atkeson & Schaal (1997); Abbeel & Ng (2005), and suggesting slight modifications that provide good guarantees even in agnostic settings.\nSimilarly, the Dataset Aggregation (DAgger) algorithm of Ross et al. (2011) uses a similar data aggregation procedure over iterations to obtain policies that mimic an expert well in imitation learning. The authors show that such\n1LQR is defined by 4 matrices A,B,Q,R s.t. xt+1 = Axt + But + ξt, for xt and ut the state and action at time t, and ξt ∼ N(0,Σ) is (optional) Gaussian white noise, and the cost C(x, u) = x>Qx+ u>Ru (Q 0, R 0). The optimal policy is linear (u = Kx) and the value function is quadratic (x>V x). LQR can be solved by dynamic programming on V and K.\na procedure can be interpreted as an online learning algorithm (Hazan et al., 2006; Kakade & Shalev-Shwartz, 2008), more specifically, Follow-the-(Regularized)-Leader (Hazan et al., 2006), and that using any no-regret online algorithm ensures good performance. Our approach can be seen as an extension of DAgger to MBRL settings.\nOur approach leverages the way agnostic model-free RL algorithms perform exploration. Methods such as Conservative Policy Iteration (CPI) (Kakade & Langford, 2002) and Policy-Search by Dynamic Programming (PSDP) (Bagnell et al., 2003) learn a policy directly by updating policy parameters iteratively. For exploration, they assume access to a state exploration distribution ν that they can restart the system from and can guarantee finding a policy performing nearly as well as any policies inducing a state distribution (over a whole trajectory) close to ν. Similarly, our approach uses a state-action exploration distribution to sample transitions and allows us to guarantee small regret against any policy with a state-action distribution close to this exploration distribution. If the exploration distribution is close to that of a near-optimal policy, then our approach guarantees near-optimal performance, provided a good model of data exists. This allows our model-based method to match the strongest agnostic guarantees of existing model-free methods. Good exploration distributions can often be obtained in practice; e.g., from human expert demonstrations, domain knowledge, or from a desired trajectory we would like the system to follow. Additionally, if we have a base policy we want to improve, it can be used to generate the exploration distribution – with potentially additional random exploration in the actions."
    }, {
      "heading" : "3. A Simple Batch Algorithm",
      "text" : "We now describe a simple algorithm, refered to as Batch, that can be used to analyze many common approaches from the literature, e.g., learning from a generative model2, open loop excitation or by watching an expert (Ljung, 1999).\nLet T denote the class of transition models considered, and ν a state-action exploration distribution we can sample the system from. Batch first executes in the real system m state-action pairs sampled i.i.d. from ν to obtain m sampled transitions. Then it finds the best model T̂ ∈ T of observed transitions, and solves (potentially approximately) the optimal control (OC) problem with T̂ and known cost function C to return a policy π̂ for test execution."
    }, {
      "heading" : "3.1. Analysis",
      "text" : "Our reduction analysis seeks to answer the following question: if Batch learns a model T̂ with small error on train-\n2With a generative model, we can set the system to any state, perform any action to obtain a sample transition.\ning data, and solves the OC problem well, what guarantees does it provide on control performance of π̂? Our results illustrate the drawbacks of a purely batch method due to the mismatch in train-test distribution.\nWe measure the quality of the OC problem’s solution as follows. For any policy π′, let π ′ oc = Es∼µ[V̂ π̂(s) − V̂ π ′ (s)] denote how much better π′ is compared to π̂ on model T̂ (V̂ π̂ and V̂ π ′ are the value functions of π̂ and π′ under learned model T̂ respectively). If π̂ is an -optimal policy on T̂ within some class of policies Π, then π ′\noc ≤ for all π′ ∈ Π. A natural measure of model error that arises from our analysis is in terms of L1 distance between the predicted and true next state’s distributions. That is, we define L1prd = E(s,a)∼ν [||Tsa − T̂sa||1] the predictive error of T̂ , measured in L1 distance, under the training distribution ν. However, the L1 distance cannot be evaluated or optimized from sampled transitions during training (we observe samples from Tsa but not the distribution). Therefore we also provide our bounds in terms of other losses we can minimize from samples. This directly relates control performance to the model’s training loss. A convenient loss is the KL divergence between Tsa and T̂sa: KLprd = E(s,a)∼ν,s′∼Tsa [log(Tsa(s′))− log(T̂sa(s′))]. Minimizing KL corresponds to maximizing the log likelihood of the sampled transitions. This is convenient for common model classes, such as linear models (as in LQR), where it amounts to linear regression. For particular cases where T is a set of deterministic models and the real system has finitely many states, the predictive error can be measured via a classification loss at predicting the next state: clsprd = E(s,a)∼ν,s′∼Tsa [`(T̂ , s, a, s′)], for ` the 0-1 loss of whether T̂ predicts s′ for (s, a), or any upper bound on the 0-1 loss, e.g., the multi-class hinge loss if T is a set of SVMs. In this case, model fitting is a supervised classification problem and the guarantee is directly related to the training classification loss. These are related as follows:\nLemma 3.1. L1prd ≤ √ 2 KLprd and L1 prd ≤ 2 clsprd. The latter holds with equality if ` is the 0-1 loss.\nIn general, we can use any loss minimizable from samples that upper bounds L1prd for models in the class. Our bounds are also related to the mismatch between the exploration distribution ν and distribution induced by executing another policy π starting in µ, denoted cπν = sups,a Dµ,π(s,a) ν(s,a) . We assume the costs C(s, a) ∈ [Cmin, Cmax] ∀(s, a). Let Crng = Cmax − Cmin and H = γCrng(1−γ)2 . H is a scaling factor that relates model error to error in total cost predictions.\nTheorem 3.1. The policy π̂ is s.t. for any policy π′:\nJµ(π̂) ≤ Jµ(π′) + π ′ oc + cπ̂ν + c\nπ′ ν\n2 H L1prd\nThis also holds as a function of KLprd or cls prd using Lem. 3.1.\nThis bound indicates that if Batch solves the OC problem well and T̂ has small enough error under the training distribution ν, then it must find a good policy. Importantly, this bound is tight: i.e. we can construct examples where it holds with equality (see supplementary material). More interestingly is what happens as we collect more data. If the fitting procedure is consistent (i.e. picks a model with minimal loss in the class asymptotically), then we can relate this guarantee to the capacity of the model class to achieve low error under the training distribution ν. We denote the modeling error, measured in L1 distance, as L1mdl = infT ′∈T E(s,a)∼ν [||Tsa − T ′sa||1]. Similarly, define KLmdl = infT ′∈T E(s,a)∼ν,s′∼Tsa [log(Tsa(s′)) − log(T ′sa(s′))] and clsmdl = infT ′∈T E(s,a)∼ν,s′∼Tsa [`(T ′, s, a, s′)]. These are all 0 in realizable settings, but generally non-zero in agnostic settings. After sampling m transitions, the generalization error L1gen(m, δ) bounds with high probability 1− δ the quantity L1prd − L1mdl. Similarly, KLgen(m, δ) and clsgen(m, δ) denote the generalization error for the KL and classification loss respectively. clsgen(m, δ) can be related to the VC dimension (or multi-class equivalent) in finite MDPs.\nCorollary 3.1. After observing m transitions, with probability at least 1− δ, for any policy π′:\nJµ(π̂) ≤ Jµ(π′) + π ′ oc + cπ̂ν + c\nπ′ ν\n2 H[ L1mdl + L1 gen(m, δ)].\nThis also holds as a function of KLmdl + KL gen(m, δ) (or\nclsmdl + cls gen(m, δ)) using Lem. 3.1. In addition, if the fitting procedure is consistent in terms ofL1 distance (or KL, classification loss), then L1gen(m, δ) → 0 (or KLgen(m, δ) → 0, clsgen(m, δ)→ 0) as m→∞ for any δ > 0.\nThe generalization error typically scales with the complexity of the class T and goes to 0 at a rate of O( 1√\nm )\n(Õ( 1m ) in ideal conditions). Given enough samples, the dominating factor limiting performance becomes the modeling error: i.e. the term c π̂ ν+c π′ ν\n2 H L1 mdl (or equivalently\ncπ̂ν+c π′ ν 2 H √ 2 KLmdl and (c π̂ ν + c π′ ν )H cls mdl) quantifies how performance degrades for agnostic settings.\nDrawback of Batch: The two factors cπ̂ν and cπ ′\nν are qualitatively different. cπ ′\nν measures how well ν explores stateactions visited by the policy π′ we compare to. This factor is inevitable: we cannot hope to compete against policies that spend most of their time where we rarely explore. cπ̂ν measures the mismatch in train-test distribution. Its presence is the major drawback of Batch. As π̂ cannot be known in advance, we can only bound cπ̂ν by considering all policies we could learn: supπ∈Π c π ν . This worst case is likely to be realized in practice: if ν rarely explores some state-action regions, the model could be bad for these and significantly underestimate their cost. The learned policy is thus encouraged to visit these low-cost regions where\nfew data were collected. To minimize supπ∈Π c π ν , the best ν for Batch is often a uniform distribution, when possible. This introduces a dependency on the number of states and actions (or state-action space volume) (i.e. cπ̂ν + c π′\nν is O(|S||A|)) multiplying the modeling error. Sampling from a uniform distribution often requires access to a generative model. If we only have access to a reset model3 and a base policy π0 inducing ν when executed in the system, then cπ̂ν could be arbitrarily large (e.g., if π̂ goes to 0 probability states under π0), and π̂ arbitrarily worse than π0.\nIn the next section, we show that iterative learning methods can leverage feedback of the learned policies to obtain bounds that do not depend on cπ̂ν . This leads to better guarantees when we have a good exploration distribution ν (e.g., that of a near-optimal policy), or when we can only collect data via a reset model. This also leads to better performance in practice as shown in the experiments."
    }, {
      "heading" : "4. No-Regret Methods for Agnostic MBRL",
      "text" : "Our extension of DAgger to the MBRL setting proceeds as follows. Starting from an initial model T̂ 1 ∈ T , solve (approximately) the OC problem with T̂ 1 to obtain policy π1. At each iteration n, collect data about the system by sampling state-action pairs from distribution ρn = 1 2ν + 1 2Dµ,πn : i.e. w.p. 1 2 , sample a transition occurring from an exploratory state-action pair drawn from ν and add it to dataset D, otherwise, sample a state transition occurring from running the current policy πn starting in µ, stopping the trajectory w.p. 1 − γ at each step and adding the last transition to D. The dataset D contains all transitions observed so far over all iterations. Once data is collected, find the best model T̂n+1 ∈ T that minimizes an appropriate loss (e.g. regularized negative log likelihood) onD, and solve (approximately) the OC problem with T̂n+1 to obtain the next policy πn+1. This is iterated for N iterations. At test time, we could either find and use the policy with lowest expected total cost in the sequence π1:N , or use the uniform “mixture” policy4 over π1:N . We guarantee good performance for both. The last policy πN often performs equally well, it has been trained with most data. Our experimental results confirm this intuition. In theory, πN has good guarantees when the distributions Dµ,πi converge to a small region in the space of distributions as i → ∞, but we do not guarantee this always occurs.\nImplementation with Off-the-Shelf Online Learner: DAgger as described can be interpreted as using a FollowThe-(Regularized)-Leader (FTRL) online algorithm to pick the sequence of models: at each iteration n we pick the\n3To sample transitions with a reset model, we can only simulate the system forward in time, or reset to a random initial state.\n4At start of any trajectory, the mixture policy picks uniformly randomly a policy in π1:N , and uses it for the whole trajectory.\nbest (regularized) model T̂n in hindsight under all samples seen so far. In general, DAgger can also be implemented using any no-regret online algorithm (see Algorithm 1) to provide good guarantees. This is done as follows. When minimizing the negative log likelihood, the loss function of the online learning problem at iteration i is: LKLi (T̂ ) = E(s,a)∼ρi,s′∼Tsa [− log(T̂sa(s′))]. This can be estimated from sampled state transitions at iteration i, and evaluated for any model T̂ . The online algorithm is applied on the sequence of loss LKL1:N to obtain a sequence of models T̂ 1:N over the iterations. As before, each model T̂ i is solved to obtain the next policy πi. By doing so, the online algorithm effectively runs over mini-batches of data collected at each iteration to update the model, and each mini-batch comes from a different distribution that changes as we update the policy. Similarly, in a finite MDP with a deterministic model class T , we can minimize the 0-1 loss instead (or any upper bound such as hinge loss) where the loss at iteration i is: Lclsi (T̂ ) = E(s,a)∼ρi,s′∼Tsa [`(T̂ , s, a, s′)], for ` the particular classification loss. This corresponds to an online classification problem. For many model classes, the negative log likelihood and convex upper bounds on the 0- 1 loss (such as hinge loss) lead to convex online learning problems, for which no-regret algorithms exist (e.g., gradient descent, FTRL). As shown below, if the sequence of models is no-regret, then performance can be related to the minimum KL divergence (or classification loss) achievable with model class T under the overall training distribution ρ = 1N ∑N i=1 ρi (i.e. a quantity akin to KL mdl or cls mdl for Batch).\nAlgorithm 1 DAgger algorithm for Agnostic MBRL. Input: exploration distribution ν, number of iterations N , number of samples per iteration m, cost function C, online learning procedure ONLINELEARNER, optimal control procedure OCSOLVER.\nGet initial guess of model: T̂ 1 ← ONLINELEARNER(). π1 ← OCSOLVER(T̂ 1, C). for n = 2 to N do\nfor k = 1 to m do With prob. 12 sample (s, a) ∼ Dµ,πn−1 using πn−1, otherwise sample (s, a) ∼ ν. Obtain s′ ∼ Tsa Add (s, a, s′) to Dn−1. end for Update model: T̂n ← ONLINELEARNER(Dn−1). πn ← OCSOLVER(T̂n, C).\nend for Return the sequence of policies π1:N ."
    }, {
      "heading" : "4.1. Analysis",
      "text" : "Similar to our analysis of Batch, we seek to answer the following: if there exists a low error model of training data,\nand we solve each OC problem well, what guarantees does DAgger provide on control performance? Our results show that by sampling data from the learned policies, DAgger provides guarantees that have no train-test mismatch factor, leading to improved performance.\nFor any policy π′, define π ′\noc = 1 N ∑N i=1 Es∼µ[V̂i(s) −\nV̂ π ′ i (s)], where V̂i and V̂ π′\ni are respectively the value function of πi and π′ under model T̂ i. This measures how well we solved each OC problem on average over the iterations. For instance, if at each iteration i we found an i-optimal policy within some class of policies Π on learned model T̂ i, then π ′ oc ≤ 1N ∑N i=1 i for all π\n′ ∈ Π. As in Batch, the average predictive error of the models T̂ 1:N can be measured in terms of the L1 distance between the predicted and true next state distribution: L1prd = 1 N ∑N i=1 E(s,a)∼ρi [||T̂ isa − Tsa||1]. However, as was discussed, the L1 distance is not observed from samples which makes it hard to minimize. Instead we can define other measures which upper bounds this L1 distance and can be minimized from samples, such as the KL divergence or classification loss: i.e. KLprd = 1 N ∑N i=1 E(s,a)∼ρi,s′∼Tsa [log(Tsa(s))− log(T̂ isa(s′))] and\nclsprd = 1 N ∑N i=1 E(s,a)∼ρi,s′∼Tsa [`(T̂ i, s, a, s′)]. Now, given the sequence of policies π1:N , let π̂ = argminπ∈π1:N Jµ(π) be the best policy in the sequence and π the uniform mixture policy on the sequence.\nLemma 4.1. The policies π1:N are s.t. for any policy π′:\nJµ(π̂) ≤ Jµ(π) ≤ Jµ(π′) + π ′ oc + c π′ ν H L1 prd\nThis also holds as a function of KLprd or cls prd using Lem. 3.1.\nWe note that KLprd = 1 N ∑N i=1 L KL i (T̂\ni) − LKLi (T ) and clsprd = 1 N ∑N i=1 L cls i (T̂\ni). Using a no-regret algorithm on the sequence of losses LKL1:N implies 1 N ∑N i=1 L KL i (T̂\ni) ≤ infT ′∈T 1 N ∑N i=1 L KL i (T ′) + KLrgt , for KL rgt the average regret of the algorithm after N iterations, s.t. KLrgt → 0 as N → ∞. This relates KLprd to the modeling error of the class T : KLmdl = infT ′∈T E(s,a)∼ρ,s′∼Tsa [log(Tsa(s)) − log(T ′sa(s\n′))], i.e. KLprd ≤ KLmdl + KLrgt , for KLrgt → 0. Similarly define clsmdl = infT ′∈T E(s,a)∼ρ,s′∼Tsa [`(T ′, s, a, s′)] and by using a no-regret algorithm on Lcls1:N , cls prd ≤ clsmdl + clsrgt for clsrgt → 0. In some cases, even if the L1 distance cannot be estimated from samples, statistical estimators can still be no-regret with high probability on the sequence of loss LL1i (T\n′) = E(s,a)∼ρi [||Tsa − T ′sa||1]. This is the case in finite MDPs if we use the empirical estimator of T based on data seen so far (see supplementary material). If we define L1mdl = infT ′∈T E(s,a)∼ρ[||Tsa − T ′sa||1], this implies that L1prd ≤ L1mdl + L1rgt, for L1rgt → 0. Our main result follows: Theorem 4.1. The policies π1:N are s.t. for any policy π′:\nJµ(π̂) ≤ Jµ(π) ≤ Jµ(π′) + π ′ oc + c π′ ν H[ L1 mdl + L1 rgt]\nThis also holds as a function of KLmdl + KL rgt (or cls mdl + cls rgt) using Lem. 3.1. If the fitting procedure is no-regret w.r.t the sequence of losses LL11:N (or L KL 1:N , L cls 1:N ), then L1 rgt → 0 (or\nKLrgt → 0, clsrgt → 0) as N →∞.\nAdditionally, the performance of πN can be related to π if the distributions Dµ,πi converge to a small region:\nLemma 4.2. If there exists a distribution D∗ and some ∗cnv ≥ 0 s.t. ∀i, ||Dµ,πi − D∗||1 ≤ ∗cnv + icnv for some sequence { icnv}∞i=1 that is o(1), then πN is s.t.:\nJµ(πN ) ≤ Jµ(π) + Crng\n2(1− γ) [2 ∗cnv + N cnv +\n1\nN N∑ i=1 icnv]\nThus: lim supN→∞ Jµ(πN )− Jµ(π) ≤ Crng 1−γ ∗ cnv\nThm. 4.1 illustrates how we can reduce the original MBRL problem to a no-regret online learning problem on a particular sequence of loss functions. In general, no-regret algorithms have average regret of O( 1√\nN ) (Õ( 1N ) in ideal\ncases) such that the regret term goes to 0 at a similar rate to the generalization error term for Batch in Cor. 3.1. Here, given enough iterations, the term cπ ′\nν H L1 mdl deter-\nmines how performance degrades in the agnostic setting (or cπ ′ ν H √ 2 KLmdl or 2c π′ ν H cls mdl if we use a no-regret algorithm on the sequence of KL or classification loss respectively). Unlike for Batch, there is no dependence on cπ̂ν , only on cπ ′\nν . Thus, if a low error model exists under training distribution ρ, no-regret methods are guaranteed to learn policies that performs well compared to any policy π′ for which cπ ′\nν\nis small. Hence, ν is ideally Dµ,π of a near-optimal policy π (i.e. explore where good policies go).\nFinite Sample Analysis: A remaining issue is that the current guarantees apply if we can evaluate the expected loss (LL1i , L KL i or L cls i ) exactly. This requires infinite samples at each iteration. If we run the no-regret algorithm on estimates of these loss functions, i.e. loss on m sampled transitions, we can still obtain good guarantees using martingale inequalities as in online-to-batch (Cesa-Bianchi et al., 2004) techniques. The extra generalization error term is\ntypically O( √\nlog(1/δ) Nm ) with high probability 1− δ. While\nour focus is not on providing such finite sample bounds, we illustrate how these can be derived for two scenarios in the supplementary material. For instance, in finite MDPs with |S| states and |A| actions, if T̂ i is the empirical estimator of T based on samples collected in the first i− 1 iterations, then choosing m = 1 and N in Õ( C2rng|S| 2|A| log(1/δ)\n2(1−γ)4 ) guarantees that w.p. 1− δ, for any policy π′:\nJµ(π̂) ≤ Jµ(π) ≤ Jµ(π′) + π ′ oc +O(c π′ ν )\nHere, mdl does not appear as it is 0 (realizable case). Given a good state-action distribution ν, the sample complexity to\nget a near-optimal policy is Õ( C2rng|S| 2|A| log(1/δ) 2(1−γ)4 ). This improves upon other state-of-the-art MBRL algorithms, such as Rmax, Õ( C3rng|S| 2|A| log(1/δ) 3(1−γ)6 ) (Strehl et al., 2009) and a recent modification of Rmax, Õ( C2rng|S||A| log(1/δ)\n2(1−γ)6 ) (Szita & Szepesvári, 2010) (when |S| < 1(1−γ)2 ). Here, the dependency on |S|2|A| is due to the complexity of the class (|S|2|A| parameters). With simpler classes, it can have no dependency on the size of the MDP. In the supplementary material, we analyze a scenario where T is a set of kernel SVM (deterministic models) with RKHS norm bounded by K. Choosing m = 1 and N in O( C2rng(K 2+log(1/δ))\n2(1−γ)4 ) guarantees that w.p. 1− δ, for any policy π′:\nJµ(π̂) ≤ Jµ(π) ≤ Jµ(π′) + π ′ oc + 2c π′ ν Ĥ cls mdl +O(c π′ ν ),\nfor ̂clsmdl the multi-class hinge loss on the training set after N iterations of the best SVM in hindsight. Thus, if we have a good exploration distribution and there exists a good model in T for predicting observed data, we obtain a nearoptimal policy with sample complexity that depends only on the complexity of T , not the size of the MDP."
    }, {
      "heading" : "5. Discussion",
      "text" : "We emphasize that we provide reduction-style guarantees. DAgger may sometimes fail to find good policies, e.g., when no model in the class achieves low error on the training data. However, DAgger guarantees that one of the following occur: either (1) we find good policies or (2) no models with low error on the aggregate dataset exist. If the latter occurs, we need a better model class. In contrast, Batch can find models with low training error, but still fail at obtaining a policy with good control performance, due to train/test mismatch. This occurs even in scenarios where DAgger finds good policies, as shown in the experiments.\nDAgger needs to solve many OC problems. This can be computationally expensive, e.g., with non-linear or highdimensional models. Many approximate methods can be used, e.g., policy gradient (Williams, 1992), fitted value iteration (Szepesvári, 2005) or iLQR (Li & Todorov, 2004). As the models often change only slightly from one iteration to the next, we can often run only a few iterations of dynamic programming/policy gradient from the last value function/policy to obtain a good policy for the current model. As long as we get good solutions on average, π ′ oc remains small and does not hinder performance.\nDAgger generalizes the approach of Atkeson & Schaal (1997) and Abbeel & Ng (2005) so that we can use any noregret algorithm to update the model, as well as any exploration distribution. A key difference is that DAgger keeps an even balance between exploration data and data from running the learned policies. This is crucial to avoid set-\ntling on suboptimal performance in agnostic settings as the exploration data could be ignored if it occupies only a small fraction of the dataset, in favor of models with lower error on the data from the learned policies. With this modification, our main contribution is showing that such methods have good guarantees even in agnostic settings."
    }, {
      "heading" : "6. Experiments on Helicopter Domain",
      "text" : "We demonstrate the efficacy of DAgger on a challenging problem: learning to perform aerobatic maneuvers with a simulated helicopter, using the simulator of Abbeel & Ng (2005), which has a continuous 21-dimensional state and 4-dimensional control space. We consider learning to 1) hover and 2) perform a “nose-in funnel” maneuver. We compare DAgger to Batch with several choices for ν: 1) νt: adding small white Gaussian noise5 to each state and action along the desired trajectory, 2) νe: run an expert controller, and 3) νen: run the expert controller with additional white Gaussian noise6 in the controls of the expert. The expert controller is obtained by linearizing the true model about the desired trajectory and solving the LQR (iLQR for the nose-in funnel). We also compare against Abbeel’s algorithm, where the expert is only used at the first iteration.\nHover: All approaches begin with an initial model ∆xt+1 = A∆xt + B∆ut, for ∆xt the difference between the current and hover state at time t, ∆ut the delta controls at time t, A is identity and B adds the delta controls to the actual controls in ∆xt. We seek to learn offset matrices A′, B′ that minimizes ||∆xt+1 − [(A + A′)∆xt + (B + B′)∆ut]||2 on observed data7. We attempt to learn to hover in the presence of noise8 and delay of 0 and 1. A delay of 1 introduces high-order dynamics that cannot be modeled with the current state. All methods sample 100 transitions per iteration and run for: 50 iterations when delay is 0; 100 iterations when delay is 1. Figure 2 shows the test performance of each method after each iteration. In both cases, for any choice of ν, DAgger outperforms Batch significantly and converges to a good policy faster. DAgger is more robust to the choice of ν, as it always obtains good performance given enough iterations, whereas Batch obtains good performance with only one choice of\n5Covariance of 0.0025I for states and 0.0001I for actions. 6Covariance of 0.0001I . 7We also use a Frobenius norm regularizer on A′ and B′:\nminA′,B′ 1 n ∑n i=1 ||∆x ′ i− [(A+A′)∆xi + (B+B′)∆ui]||2 + λ√ n\n(||A′||2F + ||B′||2F ), for λ = 10−3, n the number of samples and (∆xi,∆ui,∆x′i) the i\nth transition in the dataset. During training we stop a trajectory if it becomes too far from the hover state, i.e. if ||[∆x; ∆u]||2 > 5 as this represents an event that would have to be recovered from. During testing, we run the trajectory until completion (400 timesteps of 0.05s, 20s total).\n8White Gaussian noise with covariance I on the forces and torques applied to the helicopter at each step.\nν in each case. Also, DAgger eventually learns a policy that outperforms the expert policy (L). As the expert policy is inevitably visiting states far from the hover state due to the large noise and delay (unknown to the expert), the linearized model is not as good at those states, leading to slightly suboptimal performance. Thus DAgger is learning a better linear model for the states visited by the learned policy which leads to better performance. Abbeel’s algorithm improves the initial policy but reaches a plateau. This is due to lack of exploration (expert demonstrations) after the first iteration. While our objective is to show that DAgger outperforms other model-based approaches, we also compared against a model-free policy gradient method similar to CPI9. However, 100 samples per iteration were insufficient to get good gradient estimates and lead to only small improvement. Even with 500 samples per iteration, it could only reach an avg. total cost ∼15000 after 100 iterations.\nNose-In Funnel: This maneuver consists in rotating at a fixed speed and distance around an axis normal to the ground with the helicopter’s nose pointing towards the axis of rotation (desired trajectory in Fig. 1). We attempt to learn to perform 4 complete rotations of radius 5 in the presence of noise10 but no delay. We start each approach with a linearized model about the hover state and learn a time-varying linear model11. All methods collect 500 samples per iteration over 100 iterations. Figure 2 (bottom) shows the test performance after each iteration. With the initial model (0 data), the controller fails to produce the maneuver and performance is quite poor. Again, with any choice of ν, DAgger outperforms Batch, and unlike Batch, it performs well with all choices of ν. A video comparing qualitatively the learned maneuver with DAgger and Batch is available on YouTube (Ross, 2012). Abbeel’s method improves performance slightly but again suffers from lack of expert demonstrations after the first iteration."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We presented a no-regret online learning approach to MBRL that has strong performance, both in theory and practice, even in agnostic settings. It is simple to implement, formalizes and makes algorithmic the engineering practice of iterating between controller synthesis and system identification, and can be applied to any control problem where approximately solving the OC problem is feasible. Additionally, its sample complexity scales with model\n9Same as CPI, except gradient descent is done directly on deterministic linear controller. We solve a linear system to estimate the gradient from sample cost with perturbed parameters.\n10Zero-mean spherical Gaussian with standard deviation 0.1 on the forces and torques applied to the helicopter at each step.\n11For each time step t, we learn offset matrices A′t, B′t such that ∆xt+1 = (A+A′t)∆xt + (B +B′t)∆ut + x∗t+1 − x∗t , for x∗t the desired state at time t and A,B the given hover model.\nclass complexity, not the size of the MDP. To our knowledge, this is the first practical MBRL algorithm with agnostic guarantees. The only other agnostic MBRL approach we are aware of is a recent agnostic extension of Rmax (Szita & Szepesvári, 2011) that is largely theoretical: it requires unknown quantities to run the algorithm (e.g., distance between the real system and the model class) and its sample complexity is exponential in the class complexity."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work is supported by the ONR MURI grant N0001409-1-1052, Reasoning in Reduced Information Spaces."
    } ],
    "references" : [ {
      "title" : "Exploration and apprenticeship learning in reinforcement learning",
      "author" : [ "P. Abbeel", "A.Y. Ng" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Abbeel and Ng,? \\Q2005\\E",
      "shortCiteRegEx" : "Abbeel and Ng",
      "year" : 2005
    }, {
      "title" : "Robot learning from demonstration",
      "author" : [ "C.G. Atkeson", "S. Schaal" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Atkeson and Schaal,? \\Q1997\\E",
      "shortCiteRegEx" : "Atkeson and Schaal",
      "year" : 1997
    }, {
      "title" : "Policy search by dynamic programming",
      "author" : [ "J.A. Bagnell", "A.Y. Ng", "S. Kakade", "J. Schneider" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Bagnell et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Bagnell et al\\.",
      "year" : 2003
    }, {
      "title" : "Error limiting reductions between classification tasks",
      "author" : [ "A. Beygelzimer", "V. Dani", "T. Hayes", "J. Langford", "B. Zadrozny" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Beygelzimer et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Beygelzimer et al\\.",
      "year" : 2005
    }, {
      "title" : "On the generalization ability of on-line learning algorithms",
      "author" : [ "N. Cesa-Bianchi", "A. Conconi", "C. Gentile" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2004
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "E. Hazan", "A. Kalai", "S. Kale", "A. Agarwal" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2006
    }, {
      "title" : "Approximately optimal approximate reinforcement learning",
      "author" : [ "S. Kakade", "J. Langford" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Kakade and Langford,? \\Q2002\\E",
      "shortCiteRegEx" : "Kakade and Langford",
      "year" : 2002
    }, {
      "title" : "Mind the duality gap: Logarithmic regret algorithms for online optimization",
      "author" : [ "S. Kakade", "S. Shalev-Shwartz" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Kakade and Shalev.Shwartz,? \\Q2008\\E",
      "shortCiteRegEx" : "Kakade and Shalev.Shwartz",
      "year" : 2008
    }, {
      "title" : "Iterative linear quadratic regulator design for nonlinear biological movement systems",
      "author" : [ "W. Li", "E. Todorov" ],
      "venue" : "In ICINCO,",
      "citeRegEx" : "Li and Todorov,? \\Q2004\\E",
      "shortCiteRegEx" : "Li and Todorov",
      "year" : 2004
    }, {
      "title" : "System Identification: Theory for the User",
      "author" : [ "L. Ljung" ],
      "venue" : null,
      "citeRegEx" : "Ljung,? \\Q1999\\E",
      "shortCiteRegEx" : "Ljung",
      "year" : 1999
    }, {
      "title" : "Helicopter learning nose-in funnel",
      "author" : [ "S. Ross" ],
      "venue" : "URL http: //www.youtube.com/user/icml12rl",
      "citeRegEx" : "Ross,? \\Q2012\\E",
      "shortCiteRegEx" : "Ross",
      "year" : 2012
    }, {
      "title" : "A reduction of imitation learning and structured prediction to no-regret online learning",
      "author" : [ "S. Ross", "G. Gordon", "J.A. Bagnell" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Ross et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2011
    }, {
      "title" : "Reinforcement learning in finite MDPs: PAC analysis",
      "author" : [ "A.L. Strehl", "L. Li", "M.L. Littman" ],
      "venue" : null,
      "citeRegEx" : "Strehl et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Strehl et al\\.",
      "year" : 2009
    }, {
      "title" : "Finite time bounds for sampling based fitted value iteration",
      "author" : [ "C. Szepesvári" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Szepesvári,? \\Q2005\\E",
      "shortCiteRegEx" : "Szepesvári",
      "year" : 2005
    }, {
      "title" : "Model-based reinforcement learning with nearly tight exploration complexity bounds",
      "author" : [ "I. Szita", "C. Szepesvári" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Szita and Szepesvári,? \\Q2010\\E",
      "shortCiteRegEx" : "Szita and Szepesvári",
      "year" : 2010
    }, {
      "title" : "Agnostic kwik learning and efficient approximate reinforcement learning",
      "author" : [ "I. Szita", "C. Szepesvári" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Szita and Szepesvári,? \\Q2011\\E",
      "shortCiteRegEx" : "Szita and Szepesvári",
      "year" : 2011
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "R.J. Williams" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Williams,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams",
      "year" : 1992
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "This problem is fully appreciated in the system identification literature and has been attacked by considering “open loop” identification procedures and “persistent excitation” (Ljung, 1999; Abbeel & Ng, 2005) that attempt to sufficiently “cover” the state-action space.",
      "startOffset" : 177,
      "endOffset" : 209
    }, {
      "referenceID" : 3,
      "context" : "We adopt a reduction-based analysis (Beygelzimer et al., 2005) that relates the learned policy’s performance to prediction error during training.",
      "startOffset" : 36,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : "This approach is inspired by a recent reduction of imitation learning to no-regret online learning (Ross et al., 2011) that addresses mismatch between train/test distributions.",
      "startOffset" : 99,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : "This enables MBRL methods to match the strongest existing agnostic guarantees of model-free RL methods (Kakade & Langford, 2002; Bagnell et al., 2003).",
      "startOffset" : 103,
      "endOffset" : 150
    }, {
      "referenceID" : 13,
      "context" : ", using dynamic programming (Puterman, 1994) or approximate methods (Szepesvári, 2005; Williams, 1992).",
      "startOffset" : 68,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : ", using dynamic programming (Puterman, 1994) or approximate methods (Szepesvári, 2005; Williams, 1992).",
      "startOffset" : 68,
      "endOffset" : 102
    }, {
      "referenceID" : 10,
      "context" : "Similarly, the Dataset Aggregation (DAgger) algorithm of Ross et al. (2011) uses a similar data aggregation procedure over iterations to obtain policies that mimic an expert well in imitation learning.",
      "startOffset" : 57,
      "endOffset" : 76
    }, {
      "referenceID" : 5,
      "context" : "gorithm (Hazan et al., 2006; Kakade & Shalev-Shwartz, 2008), more specifically, Follow-the-(Regularized)-Leader (Hazan et al.",
      "startOffset" : 8,
      "endOffset" : 59
    }, {
      "referenceID" : 5,
      "context" : ", 2006; Kakade & Shalev-Shwartz, 2008), more specifically, Follow-the-(Regularized)-Leader (Hazan et al., 2006), and that using any no-regret online algorithm ensures good performance.",
      "startOffset" : 91,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "Methods such as Conservative Policy Iteration (CPI) (Kakade & Langford, 2002) and Policy-Search by Dynamic Programming (PSDP) (Bagnell et al., 2003) learn a policy directly by updating policy parameters iteratively.",
      "startOffset" : 126,
      "endOffset" : 148
    }, {
      "referenceID" : 9,
      "context" : "loop excitation or by watching an expert (Ljung, 1999).",
      "startOffset" : 41,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "loss on m sampled transitions, we can still obtain good guarantees using martingale inequalities as in online-to-batch (Cesa-Bianchi et al., 2004) techniques.",
      "startOffset" : 119,
      "endOffset" : 146
    }, {
      "referenceID" : 12,
      "context" : "as Rmax, Õ( C rng|S| |A| log(1/δ) 3(1−γ)6 ) (Strehl et al., 2009) and a",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 16,
      "context" : ", policy gradient (Williams, 1992), fitted value iteration (Szepesvári, 2005) or iLQR (Li & Todorov, 2004).",
      "startOffset" : 18,
      "endOffset" : 34
    }, {
      "referenceID" : 13,
      "context" : ", policy gradient (Williams, 1992), fitted value iteration (Szepesvári, 2005) or iLQR (Li & Todorov, 2004).",
      "startOffset" : 59,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "A video comparing qualitatively the learned maneuver with DAgger and Batch is available on YouTube (Ross, 2012).",
      "startOffset" : 99,
      "endOffset" : 111
    } ],
    "year" : 2012,
    "abstractText" : "A fundamental problem in control is to learn a model of a system from observations that is useful for controller synthesis. To provide good performance guarantees, existing methods must assume that the real system is in the class of models considered during learning. We present an iterative method with strong guarantees even in the agnostic case where the system is not in the class. In particular, we show that any no-regret online learning algorithm can be used to obtain a nearoptimal policy, provided some model achieves low training error and access to a good exploration distribution. Our approach applies to both discrete and continuous domains. We demonstrate its efficacy and scalability on a challenging helicopter domain from the literature.",
    "creator" : "LaTeX with hyperref package"
  }
}