{
  "name" : "1206.6817.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Variational Approach for Approximating Bayesian Networks by Edge Deletion",
    "authors" : [ "Arthur Choi" ],
    "emails" : [ "aychoi@cs.ucla.edu", "darwiche@cs.ucla.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We consider in this paper the formulation of approximate inference in Bayesian networks as a problem of exact inference on an approximate network that results from deleting edges (to reduce treewidth). We have shown in earlier work that deleting edges calls for introducing auxiliary network parameters to compensate for lost dependencies, and proposed intuitive conditions for determining these parameters. We have also shown that our earlier method corresponds to Iterative Belief Propagation (IBP) when enough edges are deleted to yield a polytree, and corresponds to some generalizations of IBP when fewer edges are deleted. In this paper, we propose a different criteria for determining auxiliary parameters based on optimizing the KL– divergence between the original and approximate networks. We discuss the relationship between the two methods for selecting parameters, shedding new light on IBP and its generalizations. We also discuss the application of our new method to approximating inference problems which are exponential in constrained treewidth, including MAP and nonmyopic value of information."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "The complexity of algorithms for exact inference on Bayesian networks is generally exponential in the network treewidth (Jensen, Lauritzen, & Olesen, 1990; Lauritzen & Spiegelhalter, 1988; Zhang & Poole, 1996; Dechter, 1996; Darwiche, 2001). Therefore, networks with high treewidth (and no local structure, Chavira & Darwiche, 2005) can be inaccessible to these methods, necessitating the use of approximate algorithms. Iterative Belief Propagation (IBP), also known as Loopy\nBelief Propagation (Pearl, 1988; Murphy, Weiss, & Jordan, 1999), is one such algorithm that has been critical for enabling certain classes of applications, which have been intractable for exact algorithms (e.g., Frey & MacKay, 1997). We have proposed in previous work a new perspective on this influential algorithm, viewing it as an exact inference algorithm on a polytree approximation of the original network (Choi & Darwiche, 2006; Choi, Chan, & Darwiche, 2005). The approximate polytree results from deleting edges from the original network, where the loss of each edge is offset by introducing new parameters into the approximate network. We have shown that the iterations of IBP can be understood as searching for specific values of these parameters that satisfy intuitive conditions that we characterized formally (Choi & Darwiche, 2006). This has led to a number of implications. On the theoretical side, it provided a new, network–specific, characterization of the fixed points of IBP. On the practical side, it has led to a concrete framework for improving approximations returned by IBP by deleting fewer edges than those necessary to yield a polytree; that is, we delete enough edges to obtain a multiply connected network which is still tractable for exact inference.\nIn this paper, we consider another criterion for determining the auxiliary parameters introduced by deleting edges, which is based on minimizing the KL– divergence between the original and approximate network. This proposal leads to a number of interesting results. First, we provide intuitive, yet necessary and sufficient, conditions that characterize the stationary points of this optimization problem. These conditions suggest an iterative procedure for finding parameters that satisfy these conditions, leading to a new approximate inference method that parallels IBP and its generalizations. Second, the sufficiency of these conditions lead to new results on IBP and its generalizations, characterizing situations under which these algorithms will indeed be optimizing the KL–divergence.\nWe seek to optimize the form of the KL–divergence\nthat uses weights from the original distribution, and as it turns out, the update equations for our new method are more expensive than those for IBP and its generalizations, requiring the availability of true node marginals in the original network. This means that the method as described is, in general, applicable only to networks whose treewidth is manageable, but whose constrained treewidth is not.1 That is, this approximation will typically be useful for problems which remain hard even if treewidth is manageable. This includes MAP (Park & Darwiche, 2004), inference in credal networks (Cozman, de Campos, Ide, & da Rocha, 2004), and the computation of nonmyopic value of information (Krause & Guestrin, 2005). In complexity theoretic terms, computing node marginals is PP– complete, while computing MAP is NPPP–complete. Hence, our proposed method can be used to approximate NPPP–complete problems, while IBP and its generalizations approximate PP–complete problems.\nThis paper is structured as follows. Section 2 reviews the framework of approximating networks by edge deletion. Section 3 treats the characterization of auxiliary parameters introduced by deleting edges, discussing the new characterization proposed in this paper, and comparing it to the one corresponding to IBP and its generalizations. Section 4 considers the problem of selecting which edges to delete in order to optimize the quality of approximations. Section 5 presents empirical results, Section 6 discusses related work, and Section 7 closes with some concluding remarks. Proofs of theorems are sketched in Appendix A."
    }, {
      "heading" : "2 DELETING AN EDGE",
      "text" : "Let U → X be an edge in a Bayesian network, and suppose that we wish to delete this edge to make the network more amenable to exact inference algorithms. This deletion will introduce two problems. First, variable X will lose its direct dependence on parent U .\n1Networks may admit elimination orders with manageable treewidths, but certain queries may constrain these orders, leading to constrained treewidths.\nSecond, variable U may lose evidential information received through its child X. To address these problems, we propose to add two auxiliary variables for each deleted edge U → X as given in Figure 1. The first is a variable U ′ which is made a parent of X, therefore acting as a clone of the lost parent U . The second is an instantiated variable S′ which is made a child of U , meant to provide evidence on U in lieu of the lost evidence.2 Note that the states u′ of auxiliary variable U ′ are the same as the states u of variable U , since U ′ is a clone of U . Moreover, auxiliary variable S′ is binary as it represents evidence.\nThe deletion of an edge U → X will then lead to introducing new parameters into the network, as we must now provide conditional probability tables (CPTs) for the new variables U ′ and S′. Variable U ′, a root node in the network, needs parameters θu′ representing the prior marginal on variable U ′. We will use PM (U ′) to denote these parameters, where PM (u′) = θu′ . Variable S′, a leaf node in the network, needs parameters θs′|u representing the conditional probability of s′ given U . We will use SE (U) to denote these parameters, where SE (u) = θs′|u. Moreover, we will collectively refer to PM (U ′) and SE (U) as edge parameters. Figure 2 depicts a simple network with a deleted edge, together with one possible assignment of the corresponding edge parameters.\nWe have a number of observations about our proposal for deleting edges:\n• The extent to which this proposal is successful will depend on the specific values used for the parameters introduced by deleting edges. This is a topic which we address in the following section.\n• If the deleted edge U → X splits the network into two disconnected networks, one can always find edge parameters which are guaranteed to lead to exact computation of variable marginals in both subnetworks (Choi & Darwiche, 2006).\n• The auxiliary variable S′ can be viewed as injecting a soft evidence on variable U , whose strength is defined by the parameters SE (U). Note that for queries that are conditioned on evidence s′, only the relative ratios of parameters SE (U) matter, not their absolute values (Pearl, 1988; Chan & Darwiche, 2005).\nOur goal now is to answer the following two questions. First, how do we parametrize deleted edges? Second, which edges do we delete?\n2Our proposal for deleting an edge is an extension of the proposal given by (Choi et al., 2005), who proposed the addition of a clone variable U ′ but missed the addition of evidence variable S′."
    }, {
      "heading" : "3 PARAMETRIZING EDGES",
      "text" : "Given a network N and evidence e, our proposal is then to approximate this network with anotherN ′ that results from deleting some edges U → X as given earlier. Moreover, when performing inference on network N ′, we will condition on the augmented evidence e′, composed of the original evidence e and all auxiliary evidence s′ introduced when deleting edges. More formally, if Pr and Pr ′ are the distributions induced by networks N and N ′, respectively, we will use Pr ′(X|e′) to approximate Pr(X|e) where X is a set of variables in the original network N .\nTo completely specify our approximate network N ′, we need to specify parameters PM (u′) and SE (u) for each edge that we delete. We have proposed in (Choi & Darwiche, 2006) an iterative procedure that uses the following update equations to parametrize edges:\nPM (u′) = α ∂Pr ′(e′)\n∂θs′|u\nSE (u) = α ∂Pr ′(e′)\n∂θu′ , (1)\nwhere α is a normalizing constant.3 This procedure, which we call ed-bp, starts with some arbitrary values for PM (U ′) and SE (U), leading to an initial approximate network N ′. This network can then be used to compute new values for these parameters according to the update equations in (1). The process is then repeated until convergence to a fixed point (if at all). We have also shown that when deleting enough edges to yield a polytree, the parametrizations PM (U ′) and SE (U) computed in each iteration correspond precisely to the messages passed by IBP. Moreover, if the edges deleted do not yield a polytree, edbp corresponds to a generalization of IBP (simulated by a particular choice of a joingraph; see also Aji & McEliece, 2001; Dechter, Kask, & Mateescu, 2002).\n3This is an alternative, but equivalent, formulation of the update equations given by (Choi & Darwiche, 2006).\nFinally, these update equations lead to fixed points characterized by the following conditions:\nPr ′(u|e′) = Pr ′(u′|e′), Pr ′(u|e′ \\ s′) = Pr ′(u′). (2)\nThe first condition says that variables U ′ and U should have the same posterior marginals. The second condition, in light of the first, says that the impact of evidence s′ on variable U is equivalent to the impact of all evidence on its clone U ′. Indeed, these conditions correspond to the intuitions that motivated ed-bp."
    }, {
      "heading" : "3.1 A VARIATIONAL APPROACH",
      "text" : "We propose now a variational approach to parametrizing deleted edges, based on the KL–divergence:\nKL(Pr(.|e),Pr ′(.|e′)) def =\n∑\nw\nPr(w|e) log Pr(w|e)\nPr ′(w|e′) ,\nwhere w is a world, denoting an instantiation over all variables. Note that the KL–divergence is not symmetric: the divergence KL(Pr(.|e),Pr ′(.|e′)) is weighted by the true distribution while the divergence KL(Pr ′(.|e′),Pr(.|e)) is weighted by the approximate one. Common practice weighs the KL–divergence using the approximate distribution, which is typically more accessible computationally (e.g., Yedidia, Freeman, & Weiss, 2005). In contrast, we will weigh by the true distribution in what follows.\nBefore we proceed to optimize the KL–divergence, we must ensure that the domains of the distributions Pr(.|e) and Pr ′(.|e′) coincide. One way to ensure this is to use the following construction, demonstrated in Figure 3. Given a Bayesian network N?, we can replace each edge U → X to be deleted with a chain U → U ′ → X, where the equivalence edge U → U ′ denotes an equivalence constraint: θu′|u = 1 iff u ′ = u. The resulting augmented network N will then satisfy three important properties. First, it is equivalent to the original network N? over common variables. Second, it has the same treewidth as N?. Finally, when\nwe delete equivalence edges U → U ′ from N , we get an approximate network N ′ that does not require the introduction of clone variables U ′ as they are already present in N ′.4 We can therefore compute the KL– divergence between the augmented network N and its approximation N ′.\nWe can now state the following key result:\nLemma 1 Let N be an (augmented) Bayesian network and N ′ be the network that results from deleting equivalence edges U → U ′. Then:\nKL(Pr(.|e),Pr ′(.|e′))\n= ∑\nU→U ′\n∑\nu=u′\nPr(uu′|e) log 1\nθu′θs′|u + log\nPr ′(e′)\nPr(e) .\nSince Pr ′(e′) is a function of our edge parameters, the KL–divergence is thus also a function of our edge parameters. This result will be used later to derive update equations for our variational method, and to develop a heuristic for choosing edges to delete.\nBefore we proceed though, we observe the following. Let X be the variables of the original network N? and U′ be the clone variables introduced via the equivalence edges U → U ′ in N . The KL– divergence of Lemma 1 is then over the variables XU′. One would normally prefer to optimize the KL–divergence KL(Pr(X|e),Pr ′(X|e′)) over the original variables X, but our method will seek to optimize KL(Pr(XU′|e),Pr ′(XU′|e′)) instead. In fact, the properties of the KL–divergence tell us that\nKL(Pr(X|e),Pr ′(X|e′))\n≤ KL(Pr(XU′|e),Pr ′(XU′|e′)). (3)\nIn our experimental results in Section 5, we will report results on both versions of the KL–divergence, referring to KL(Pr(X|e),Pr ′(X|e′)) as the exact KL, and to KL(Pr(XU′|e),Pr ′(XU′|e′)) as the KL bound.\n4We still need to add a new child S′ for each U however. Since variables S′ are observed, they do not prohibit us from computing the KL–divergence between N and N ′ even though they are not present in network N .\nFor the remainder of this paper, we will only be dealing with augmented networks N , leaving the original network N? implicit."
    }, {
      "heading" : "3.2 THE APPROXIMATE NETWORK",
      "text" : "We have the KL–divergence KL(Pr(.|e),Pr ′(.|e′)) as a function of our edge parameters PM (u′) = θu′ and SE (u) = θs′|u. If we set to zero the partial derivatives of the KL–divergence with respect to each edge parameter, we get the following.\nTheorem 1 Let N be a Bayesian network and N ′ be the network that results from deleting equivalence edges U → U ′. The edge parameters of N ′ are a stationary point of KL(Pr(.|e),Pr ′(.|e′)) if and only if\nPr ′(u|e′) = Pr ′(u′|e′) = Pr(u|e), (4)\nfor all deleted edges U → U ′.\nThat is, if we delete the edge U → U ′, then the marginals on both U and U ′ must be exact in the approximate network N ′. Note, however, that this does not imply that other node marginals must be exact in N ′: only those corresponding to deleted edges need be.\nTheorem 1 has a number of implications. First, the necessity of Condition (4) will be exploited in the following section to provide an iterative method that searches for parameters that are a stationary point for the KL–divergence. Second, the sufficiency of Condition (4) implies that any method that searches for edge parameters, regardless of the criteria chosen, will yield parameters that are a stationary point for the KL–divergence, if the parameters give rise to exact marginals for variables corresponding to deleted edges. For example, if we search for parameters using ed-bp (Choi & Darwiche, 2006), and the parameters found lead to exact marginals, then these parameters will indeed be a stationary point for the KL–divergence.\nBefore we show how to identify parameters satisfying Condition (4), we note that parameters satisfying Condition (2) do not necessarily satisfy Condition (4) and, hence, are not necessarily a stationary point for KL(Pr(.|e),Pr ′(.|e′)). We provide a simple network with four nodes in Appendix B demonstrating this point. Recall that Condition (2) characterizes IBP and some of its generalizations (Choi & Darwiche, 2006)."
    }, {
      "heading" : "3.3 SEARCHING FOR PARAMETERS",
      "text" : "Having characterized stationary points of the KL– divergence, we now proceed to develop an iterative procedure for finding a stationary point. Our method is based on the following result.\nTheorem 2 Let N be a Bayesian network and N ′ be the network that results from deleting equivalence edges U → U ′. The edge parameters of N ′ are a stationary point of KL(Pr(.|e),Pr ′(.|e′)) if and only if:\nPM (u′) = Pr(u|e)\n(\nPr ′(e′)/ ∂Pr ′(e′)\n∂θu′\n)\n, (5)\nSE (u) = Pr(u|e)\n(\nPr ′(e′)/ ∂Pr ′(e′)\n∂θs′|u\n)\n. (6)\nWe have a number of observations about this theorem. First, if we have access to the true marginals Pr(u|e), then this theorem suggests an iterative method that starts with some arbitrary values of parameters PM (u′) and SE (u), leading to some initial approximate network N ′. Using this network, we can compute the quantities Pr ′(e′), ∂Pr ′(e′)\n∂θu′ and ∂Pr ′(e′) ∂θs′|u ,\nwhich can then be used to compute new values for the parameters PM (u′) and SE (u), one set at a time. The process can then be repeated until convergence (if at all). We will refer to this method as ed-kl, to be contrasted with ed-bp given earlier (Choi & Darwiche, 2006). Note that since the KL–divergence is non-negative, there exists a set of edge parameters that are globally minimal. However, a stationary point of the KL–divergence is not necessarily a global minima.\nSecond, the availability of the true marginals Pr(u|e) typically implies that the network treewidth is small enough to permit the computation of these marginals. Hence, ed-kl is in general applicable to situations where the treewidth is manageable, but where the constrained treewidth is not. In these situations, the goal of deleting edges is to reduce the network constrained treewidth, making it amenable to algorithms that are exponential in constrained treewidth, such as MAP (Park & Darwiche, 2004), inference in credal networks (Cozman et al., 2004), and the computation of nonmyopic value of information (Krause & Guestrin, 2005).\nThird, we have the following result which is critical for the practical application of ed-kl:\nTheorem 3 Let N be a Bayesian network and N ′ be the network that results from deleting a single equivalence edge U → U ′. We then have\nPr ′(e′) = ∑\nuu′\nθs′|uθu′ ∂Pr(e)\n∂θu′|u ,\nwhich implies:\n∂Pr ′(e′)\n∂θu′ =\n∑\nu\nθs′|u ∂Pr(e)\n∂θu′|u ,\n∂Pr ′(e′)\n∂θs′|u =\n∑\nu′\nθu′ ∂Pr(e)\n∂θu′|u .\nThe main observation here is that ∂Pr(e)/∂θu′|u is a function of the original network and, therefore, is independent of the parameters θu′ and θs′|u—this is why the second and third equations above follow immediately from the first. Given the above equations, we can apply ed-kl to a single deleted edge, without the need for inference. That is, assuming that we have computed ∂Pr(e)/∂θu′|u, we can use the above equations to compute updated values for edge parameters from the old values in constant time. This result will have implications in the following section, as we present a heuristic for deciding which edges to delete."
    }, {
      "heading" : "4 CHOOSING EDGES TO DELETE",
      "text" : "Our method for deciding which edges to delete is based on scoring each network edge in isolation, leading to a total order on network edges, and then deleting edges according to the resulting order. For example, if we want to delete k edges, we simply delete the first k edges in the order.\nThe score for edge U → U ′ is based on the KL– divergence between the original network N and an approximate network N ′ which results from deleting the single edge U → U ′. The KL–divergence is computed using Lemma 1. This lemma requires some quantities from the original network N , which can be computed since the network is assumed to have a manageable treewidth. The lemma also requires that we have the parameters θu′ and θs′|u for the deleted edge U → U\n′, and the corresponding probability Pr ′(e′). These can be computed relatively easily using Theorem 3, assuming that we have computed ∂Pr(e)/∂θu′|u as explained in the previous section. Given these observations, all edge parameters, together with the corresponding KL scores, can be computed simultaneously for all edges using a single evaluation of the original network. Moreover, the computed parameters have another use beyond scoring edges: when used as initial values for ed-kl, they tend to lead to better convergence rates. We indeed employ this observation in our experiments."
    }, {
      "heading" : "5 EMPIRICAL ANALYSIS",
      "text" : "We present experimental results in this section on a number of Bayesian networks, to illustrate a number of points on the relative performance of ed-kl and edbp. We start with Figure 4 which depicts the quality of computed approximations according to the exact KL–divergence5; see Equation 3. For each approximation scheme, we consider two methods for deleting edges. For ed-kl, we delete edges randomly (ed-kl-\n5To compute the exact KL–divergence, see, for example, (Choi et al., 2005).\nrand) and according to the heuristic of Section 4 (edkl-guided). For ed-bp, we delete edges randomly (ed-bp-rand) and according to a heuristic based on mutual information (ed-bp-mi) given in (Choi & Darwiche, 2006). As is clear from the figures, ed-kl is overwhelmingly superior as far as minimizing the KL– divergence, sometimes even using random deletion of edges.\nFigure 5 depicts sample results for the quality of approximations according to the KL–divergence bound; see Equation 3. As mentioned earlier, ed-kl searches for stationary points of this bound instead of stationary points of the exact KL–divergence, yet empirically one does not see much difference between the two on these networks.\nFigures 4 and 5 depict another approximation scheme, ed-bp-guided, which deletes edges based on the heuristic of Section 4, but then uses ed-bp to search for parameters instead of ed-kl. This is a hypothetical method since the mentioned heuristic assumes that the network treewidth is manageable, a situation under which one would want to apply ed-kl instead of ed-bp. Yet, our results show that ed-bp is consistently very close to ed-kl in this case as far as minimizing the KL–divergence. This observation is critical\nas it highlights the great importance of heuristics for deleting edges. In particular, these results show that ed-bp can do quite well in terms of minimizing the KL–divergence if the right edges are deleted!\nFigure 6 depicts sample results on the speed of convergence for both ed-kl and ed-bp, again using the different methods for edge deletion. In two of these networks, ed-kl consistently converges faster than edbp. In the three omitted figures, due to space limitations, ed-kl is also superior to ed-bp.\nWe consider also sample results from using approximations identified by ed-kl to approximate MAP. Figure 7 depicts the relative difference p/q, where p is the value of the MAP solution found in the approximate network N ′ and q is the value of a MAP solution in the original network N . It is clear from the figure that ed-kl-guided produces the superior approximations, and can provide accurate solutions even when many edges are deleted. Again, based on the hypothetical method ed-bp-guided, we see that it is possible for ed-bp to produce good MAP approximations as well if the right edges are deleted.\nFigure 8 highlights how effective deleting edges is in reducing the constrained treewidth (approximated using a min-fill heuristic), and thus how effective deleting\nedges is in reducing the complexity of computing MAP. We see that good approximations can be maintained even when the constrained treewidth is reduced to the network treewidth. When we further delete every network edge, we have a fully factorized approximation of MAP, where the constrained (and network) treewidth corresponds to the size of the largest network CPT.\nThe plots given in this section correspond to averages of at least 50 instances per data point, where each instance correspond to evidence over all leaf nodes drawn from the network joint. We have also experimented with evidence drawn randomly (not from the joint), leading to similar results. Networks tcc and emdec are courtesy of HRL Labs, LLC. The grid and chain networks are synthetic and available from the authors. Networks alarm and win95pts are available at http://www.cs.huji.ac.il/labs/compbio/Repository."
    }, {
      "heading" : "6 RELATED WORK",
      "text" : "Many variational methods pose the problem of approximate inference as exact inference in some approximate model, often seeking to minimize the KL– divergence, but weighing it by the approximate distribution (e.g., Jordan, Ghahramani, Jaakkola, & Saul, 1999; Jaakkola, 2000; Wiegerinck, 2000; Geiger & Meek, 2005). One example is the mean–field method, where we seek to approximate a network N by a fully disconnected N ′ (Haft, Hofmann, & Tresp, 1999). If we delete all edges from the network and try to parametrize edges using ed-kl, we would be solving the same problem solved by mean–field, except that our KL–divergence is weighted by the true distribution, leading to more expensive update equations. Other variational approaches typically assume particular structures in their approximate models, such as chains (Ghahramani & Jordan, 1997), trees (Frey, Patrascu, Jaakkola, & Moran, 2000; Minka & Qi, 2003), or disconnected subnetworks (Saul & Jordan, 1995; Xing, Jordan, & Russell, 2003). In contrast, ed-kl works for any network structure which is a subset of the original. In fact, the efficient edge deletion heuristic of Section 4 tries to select the most promising subnetworks and is quite effective as illustrated earlier. Again, most of these approaches weigh the KL– divergence by the approximate distribution for computational reasons, with the notable exceptions of (Frey et al., 2000; Minka & Qi, 2003).\nOther methods of edge deletion have been proposed for Bayesian networks (Suermondt, 1992; Kjærulff, 1994; van Engelen, 1997), some of which can be rephrased using a variational perspective. All of these approaches, however, approximate a network independent of the given evidence, which is a dramatic depar-\nture from ed-kl and can lead to much worse behavior for less likely evidence. That is, these approaches approximate a network once for all queries, while ed-kl can approximate a network for each specific query."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "We proposed a method, ed-kl, for approximating Bayesian networks by deleting edges from the original network and then finding stationary points for the KL–divergence between the original and approximate networks (while weighing the divergence by the true distribution). We also proposed an efficient heuristic for deciding which edges to delete from a network, with the aim of choosing network substructures that lead to high quality approximations.\nThe update equations of ed-kl require exact posteriors from the original network. This means that edkl is, in general, applicable to problems that remain hard even when treewidth is manageable, including MAP, nonmyopic value of information, and inference in credal networks. This is to be contrasted with our earlier method ed-bp, which updates parameters differently, coinciding with IBP and some of its generalizations.\nOur empirical results provide good evidence to the quality of approximations returned by ed-kl, especially when compared to the approximations returned by ed-bp. Moreover, our results, both theoretical and empirical, shed new and interesting light on edbp (and, hence, IBP and some of its generalizations), showing that it can also produce high quality approximations (from a KL–divergence viewpoint), when deleting the right set of network edges."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work has been partially supported by Air Force grant #FA9550-05-1-0075-P00002 and by JPL/NASA grant #1272258."
    }, {
      "heading" : "A Proof Sketches",
      "text" : "Note that u ∼ w signifies that u and w are compatible instantiations.\nProof of Lemma 1 Deleting edges U → U ′, we have:\nKL(Pr(.|e),Pr ′(.|e′)) = ∑\nw\nPr(w|e) log Pr(w|e)\nPr ′(w|e′)\n= ∑\nw\nPr(w|e) log Pr(w, e)\nPr ′(w, e′) + log\nPr ′(e′)\nPr(e)\n= ∑\nw\nPr(w|e) log ∏\nuu′∼w\nθu′|u\nθu′θs′|u + log\nPr ′(e′)\nPr(e)\n= ∑\nw\n∑\nuu′∼w\nPr(w|e) log θu′|u\nθu′θs′|u + log\nPr ′(e′)\nPr(e)\n= ∑\nU→U ′\n∑\nuu′\n∑\nw|=uu′\nPr(w|e) log θu′|u\nθu′θs′|u + log\nPr ′(e′)\nPr(e)\n= ∑\nU→U ′\n∑\nuu′\nPr(uu′|e) log θu′|u\nθu′θs′|u + log\nPr ′(e′)\nPr(e)\n= ∑\nU→U ′\n∑\nu=u′\nPr(uu′|e) log 1\nθu′θs′|u + log\nPr ′(e′)\nPr(e) .\nThe last equation follows, since when u does not agree with u′, we have that Pr(uu′|e) log θu′|u = 0 log 0, which we assume is equal to zero, by convention.\nProof of Theorem 1 Note that when u = u′, we have Pr(uu′|e) = Pr(u|e) = Pr(u′|e).\nFirst direction of theorem. Let f be the KL–divergence as given in Lemma 1. Setting ∂f/∂θu′ to zero, we get:\n∂f\n∂θu′ = −\nPr(u|e)\nθu′ +\n1\nPr ′(e′)\n∂Pr ′(e′)\n∂θu′ = 0, (7)\nwhere u agrees with u′. We then have\nPr(u|e) = θu′\nPr ′(e′)\n∂Pr ′(e′)\n∂θu′ =\nPr ′(u′, e′)\nPr ′(e′) = Pr ′(u′|e′)\nSimilarly, to show Pr(u|e) = Pr ′(u|e′). Note that constraints such as normalization are inactive here.\nSecond direction of theorem. Given a network N ′ where marginals on U and U ′ are exact, we want to verify that the edge parameters are stationary points. If we take the partial derivative with respect to θu′ :\n∂f\n∂θu′ =\n1\nθu′\n(\n−Pr(u|e) + θu′\nPr ′(e′)\n∂Pr ′(e′)\n∂θu′\n)\n= 1\nθu′\n(\n−Pr(u|e) + Pr ′(u′, e′)\nPr ′(e′)\n)\n= 1\nθu′ (−Pr(u|e) + Pr ′(u′|e′)) .\nWe are given Pr ′(u′|e′) = Pr(u|e), thus ∂f/∂θu′ = 0. Similarly, to show ∂f/∂θs′|u = 0.\nProof of Theorem 2 Equation 5 follows easily from Equation 7. Equation 6 follows from ∂f/∂θs′|u.\nProof of Theorem 3 First, we have:\nPr ′(e′) = ∑\nuu′\nPr ′(uu′, e′)\n= ∑\nuu′\nθs′|uθu′ ∂2Pr ′(e′)\n∂θs′|u∂θu′ =\n∑\nuu′\nθs′|uθu′ ∂Pr ′(u, e)\n∂θu′ .\nNote that the distribution induced by a network where a single edge U → U ′ has been deleted is equivalent to\nthe distribution induced by another network N ′ that is identical in structure to N , except that θu′|u = θu′ for all u. We then have:\nPr ′(e′) = ∑\nuu′\nθs′|uθu′ ∂Pr ′(e)\n∂θu′|u =\n∑\nuu′\nθs′|uθu′ ∂Pr(e)\n∂θu′|u .\nThe other relations follow easily."
    }, {
      "heading" : "B Example",
      "text" : "We demonstrate here an example where ed-bp fixed points are not necessarily stationary points of the KL–divergence. This example also shows that even if we delete a single edge, ed-bp can have infinitely many parametrizations satisfying Condition (2), even though there exists an ed-bp (and ed-kl) fixed point minimizing the KL bound, KL(Pr(.|e),Pr ′(.|e′)) (as well as minimizing the exact KL). This example also corresponds to an instance of IBP (with a particular message passing schedule), since edge deletion renders the network a polytree (Choi & Darwiche, 2006).\nOur example is depicted in Figure 9. Variables Ui have parameters θui = θūi = 0.5. Variables Xj are fixed to states xj , and assert the equivalence of U1 and U2: θxj |u1u2 = 1 iff u1 = u2. Conditioning on evidence e = x1x2, we have Pr(u1|e) = Pr(u2|e) = 0.5. If we delete the edge U1 → X1 (implicitly, we delete an edge U1 → U ′ 1), then any non-zero parameterization of our edge parameters satisfies the ed-bp fixed point conditions given by Condition (2). For example, when θs′|u1 = θs′|ū1 = 0.5, and θu′1 = θū′1 = 0.5, the KL–divergence is zero and thus minimized, yielding parent and clone marginals that are exact. By Theorem 1, edges parameters are then a stationary point of the KL–divergence. However, when θu′\n1 6= 0.5, the\nparent and clone marginals are not exact, and thus edges parameters are not a stationary point of the KL– divergence, again by Theorem 1."
    } ],
    "references" : [ {
      "title" : "The generalized distributive law and free energy minimization",
      "author" : [ "S.M. Aji", "R.J. McEliece" ],
      "venue" : "In Proceedings of the 39th Allerton Conference on Communication, Control and Computing,",
      "citeRegEx" : "Aji and McEliece,? \\Q2001\\E",
      "shortCiteRegEx" : "Aji and McEliece",
      "year" : 2001
    }, {
      "title" : "On the revision of probabilistic beliefs using uncertain evidence",
      "author" : [ "H. Chan", "A. Darwiche" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Chan and Darwiche,? \\Q2005\\E",
      "shortCiteRegEx" : "Chan and Darwiche",
      "year" : 2005
    }, {
      "title" : "Compiling Bayesian networks with local structure",
      "author" : [ "M. Chavira", "A. Darwiche" ],
      "venue" : "In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Chavira and Darwiche,? \\Q2005\\E",
      "shortCiteRegEx" : "Chavira and Darwiche",
      "year" : 2005
    }, {
      "title" : "On Bayesian network approximation by edge deletion",
      "author" : [ "A. Choi", "H. Chan", "A. Darwiche" ],
      "venue" : "In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Choi et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2005
    }, {
      "title" : "An edge deletion semantics for belief propagation and its practical impact on approximation quality",
      "author" : [ "A. Choi", "A. Darwiche" ],
      "venue" : "In Proc. AAAI National Conference. To appear",
      "citeRegEx" : "Choi and Darwiche,? \\Q2006\\E",
      "shortCiteRegEx" : "Choi and Darwiche",
      "year" : 2006
    }, {
      "title" : "Propositional and relational Bayesian networks associated with imprecise and qualitative probabilistic assessments",
      "author" : [ "F.G. Cozman", "C.P. de Campos", "J.S. Ide", "J.C.F. da Rocha" ],
      "venue" : "In Proceedings of the Conference on Uncertainty",
      "citeRegEx" : "Cozman et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Cozman et al\\.",
      "year" : 2004
    }, {
      "title" : "Recursive conditioning",
      "author" : [ "A. Darwiche" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Darwiche,? \\Q2001\\E",
      "shortCiteRegEx" : "Darwiche",
      "year" : 2001
    }, {
      "title" : "Bucket elimination: A unifying framework for probabilistic inference",
      "author" : [ "R. Dechter" ],
      "venue" : "In Proceedings of the 12th Conference on Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Dechter,? \\Q1996\\E",
      "shortCiteRegEx" : "Dechter",
      "year" : 1996
    }, {
      "title" : "Iterative join-graph propagation",
      "author" : [ "R. Dechter", "K. Kask", "R. Mateescu" ],
      "venue" : "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Dechter et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Dechter et al\\.",
      "year" : 2002
    }, {
      "title" : "A revolution: Belief propagation in graphs with cycles",
      "author" : [ "B.J. Frey", "D.J.C. MacKay" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Frey and MacKay,? \\Q1997\\E",
      "shortCiteRegEx" : "Frey and MacKay",
      "year" : 1997
    }, {
      "title" : "Sequentially fitting “inclusive” trees for inference in noisy-or networks",
      "author" : [ "B.J. Frey", "R. Patrascu", "T. Jaakkola", "J. Moran" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Frey et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Frey et al\\.",
      "year" : 2000
    }, {
      "title" : "Structured variational inference procedures and their realizations",
      "author" : [ "D. Geiger", "C. Meek" ],
      "venue" : "In Proceedings of Tenth International Workshop on Artificial Intelligence and Statistics. The Society for Artificial Intelligence and Statistics",
      "citeRegEx" : "Geiger and Meek,? \\Q2005\\E",
      "shortCiteRegEx" : "Geiger and Meek",
      "year" : 2005
    }, {
      "title" : "Factorial hidden markov models",
      "author" : [ "Z. Ghahramani", "M.I. Jordan" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Ghahramani and Jordan,? \\Q1997\\E",
      "shortCiteRegEx" : "Ghahramani and Jordan",
      "year" : 1997
    }, {
      "title" : "Modelindependent mean-field theory as a local method for approximate propagation of information",
      "author" : [ "M. Haft", "R. Hofmann", "V. Tresp" ],
      "venue" : "Network: Computation in Neural Systems,",
      "citeRegEx" : "Haft et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Haft et al\\.",
      "year" : 1999
    }, {
      "title" : "Advanced Mean Field methods - Theory and Practice, chap. Tutorial on Variational Approximation Methods",
      "author" : [ "T. Jaakkola" ],
      "venue" : null,
      "citeRegEx" : "Jaakkola,? \\Q2000\\E",
      "shortCiteRegEx" : "Jaakkola",
      "year" : 2000
    }, {
      "title" : "Bayesian updating in recursive graphical models by local computation",
      "author" : [ "F.V. Jensen", "S. Lauritzen", "K. Olesen" ],
      "venue" : "Computational Statistics Quarterly,",
      "citeRegEx" : "Jensen et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Jensen et al\\.",
      "year" : 1990
    }, {
      "title" : "An introduction to variational methods for graphical models",
      "author" : [ "M.I. Jordan", "Z. Ghahramani", "T. Jaakkola", "L.K. Saul" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Jordan et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Jordan et al\\.",
      "year" : 1999
    }, {
      "title" : "Reduction of computational complexity in Bayesian networks through removal of weak dependences",
      "author" : [ "U. Kjærulff" ],
      "venue" : "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Kjærulff,? \\Q1994\\E",
      "shortCiteRegEx" : "Kjærulff",
      "year" : 1994
    }, {
      "title" : "Optimal nonmyopic value of information in graphical models - efficient algorithms and theoretical limits",
      "author" : [ "A. Krause", "C. Guestrin" ],
      "venue" : "In Proc. International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Krause and Guestrin,? \\Q2005\\E",
      "shortCiteRegEx" : "Krause and Guestrin",
      "year" : 2005
    }, {
      "title" : "Local computations with probabilities on graphical structures and their application to expert systems",
      "author" : [ "S.L. Lauritzen", "D.J. Spiegelhalter" ],
      "venue" : "Journal of Royal Statistics Society, Series B,",
      "citeRegEx" : "Lauritzen and Spiegelhalter,? \\Q1988\\E",
      "shortCiteRegEx" : "Lauritzen and Spiegelhalter",
      "year" : 1988
    }, {
      "title" : "Tree-structured approximations by expectation propagation",
      "author" : [ "T.P. Minka", "Y.A. Qi" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Minka and Qi,? \\Q2003\\E",
      "shortCiteRegEx" : "Minka and Qi",
      "year" : 2003
    }, {
      "title" : "Loopy belief propagation for approximate inference: An empirical study",
      "author" : [ "K.P. Murphy", "Y. Weiss", "M.I. Jordan" ],
      "venue" : "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Murphy et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Murphy et al\\.",
      "year" : 1999
    }, {
      "title" : "Complexity results and approximation strategies for map explanations",
      "author" : [ "J. Park", "A. Darwiche" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Park and Darwiche,? \\Q2004\\E",
      "shortCiteRegEx" : "Park and Darwiche",
      "year" : 2004
    }, {
      "title" : "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference",
      "author" : [ "J. Pearl" ],
      "venue" : null,
      "citeRegEx" : "Pearl,? \\Q1988\\E",
      "shortCiteRegEx" : "Pearl",
      "year" : 1988
    }, {
      "title" : "Exploiting tractable substructures in intractable networks",
      "author" : [ "L.K. Saul", "M.I. Jordan" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Saul and Jordan,? \\Q1995\\E",
      "shortCiteRegEx" : "Saul and Jordan",
      "year" : 1995
    }, {
      "title" : "Explanation in Bayesian Belief Networks",
      "author" : [ "H.J. Suermondt" ],
      "venue" : null,
      "citeRegEx" : "Suermondt,? \\Q1992\\E",
      "shortCiteRegEx" : "Suermondt",
      "year" : 1992
    }, {
      "title" : "Approximating Bayesian belief networks by arc removal",
      "author" : [ "R.A. van Engelen" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Engelen,? \\Q1997\\E",
      "shortCiteRegEx" : "Engelen",
      "year" : 1997
    }, {
      "title" : "Variational approximations between mean field theory and the junction tree algorithm",
      "author" : [ "W. Wiegerinck" ],
      "venue" : "In Proceedings of the Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Wiegerinck,? \\Q2000\\E",
      "shortCiteRegEx" : "Wiegerinck",
      "year" : 2000
    }, {
      "title" : "A generalized mean field algorithm for variational inference in exponential families",
      "author" : [ "E.P. Xing", "M.I. Jordan", "S.J. Russell" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Xing et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Xing et al\\.",
      "year" : 2003
    }, {
      "title" : "Constructing free-energy approximations and generalized belief propagation algorithms",
      "author" : [ "J. Yedidia", "W. Freeman", "Y. Weiss" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Yedidia et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Yedidia et al\\.",
      "year" : 2005
    }, {
      "title" : "Exploiting causal independence in bayesian network inference",
      "author" : [ "N.L. Zhang", "D. Poole" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Zhang and Poole,? \\Q1996\\E",
      "shortCiteRegEx" : "Zhang and Poole",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "The complexity of algorithms for exact inference on Bayesian networks is generally exponential in the network treewidth (Jensen, Lauritzen, & Olesen, 1990; Lauritzen & Spiegelhalter, 1988; Zhang & Poole, 1996; Dechter, 1996; Darwiche, 2001).",
      "startOffset" : 120,
      "endOffset" : 240
    }, {
      "referenceID" : 6,
      "context" : "The complexity of algorithms for exact inference on Bayesian networks is generally exponential in the network treewidth (Jensen, Lauritzen, & Olesen, 1990; Lauritzen & Spiegelhalter, 1988; Zhang & Poole, 1996; Dechter, 1996; Darwiche, 2001).",
      "startOffset" : 120,
      "endOffset" : 240
    }, {
      "referenceID" : 23,
      "context" : "Iterative Belief Propagation (IBP), also known as Loopy Belief Propagation (Pearl, 1988; Murphy, Weiss, & Jordan, 1999), is one such algorithm that has been critical for enabling certain classes of applications, which have been intractable for exact algorithms (e.",
      "startOffset" : 75,
      "endOffset" : 119
    }, {
      "referenceID" : 23,
      "context" : "Note that for queries that are conditioned on evidence s, only the relative ratios of parameters SE (U) matter, not their absolute values (Pearl, 1988; Chan & Darwiche, 2005).",
      "startOffset" : 138,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : "Our proposal for deleting an edge is an extension of the proposal given by (Choi et al., 2005), who proposed the addition of a clone variable U ′ but missed the addition of evidence variable S.",
      "startOffset" : 75,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "In these situations, the goal of deleting edges is to reduce the network constrained treewidth, making it amenable to algorithms that are exponential in constrained treewidth, such as MAP (Park & Darwiche, 2004), inference in credal networks (Cozman et al., 2004), and the computation of nonmyopic value of information (Krause & Guestrin, 2005).",
      "startOffset" : 242,
      "endOffset" : 263
    }, {
      "referenceID" : 3,
      "context" : "To compute the exact KL–divergence, see, for example, (Choi et al., 2005).",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 14,
      "context" : "Many variational methods pose the problem of approximate inference as exact inference in some approximate model, often seeking to minimize the KL– divergence, but weighing it by the approximate distribution (e.g., Jordan, Ghahramani, Jaakkola, & Saul, 1999; Jaakkola, 2000; Wiegerinck, 2000; Geiger & Meek, 2005).",
      "startOffset" : 207,
      "endOffset" : 312
    }, {
      "referenceID" : 27,
      "context" : "Many variational methods pose the problem of approximate inference as exact inference in some approximate model, often seeking to minimize the KL– divergence, but weighing it by the approximate distribution (e.g., Jordan, Ghahramani, Jaakkola, & Saul, 1999; Jaakkola, 2000; Wiegerinck, 2000; Geiger & Meek, 2005).",
      "startOffset" : 207,
      "endOffset" : 312
    }, {
      "referenceID" : 10,
      "context" : "Again, most of these approaches weigh the KL– divergence by the approximate distribution for computational reasons, with the notable exceptions of (Frey et al., 2000; Minka & Qi, 2003).",
      "startOffset" : 147,
      "endOffset" : 184
    }, {
      "referenceID" : 25,
      "context" : "Other methods of edge deletion have been proposed for Bayesian networks (Suermondt, 1992; Kjærulff, 1994; van Engelen, 1997), some of which can be rephrased using a variational perspective.",
      "startOffset" : 72,
      "endOffset" : 124
    }, {
      "referenceID" : 17,
      "context" : "Other methods of edge deletion have been proposed for Bayesian networks (Suermondt, 1992; Kjærulff, 1994; van Engelen, 1997), some of which can be rephrased using a variational perspective.",
      "startOffset" : 72,
      "endOffset" : 124
    } ],
    "year" : 2006,
    "abstractText" : "We consider in this paper the formulation of approximate inference in Bayesian networks as a problem of exact inference on an approximate network that results from deleting edges (to reduce treewidth). We have shown in earlier work that deleting edges calls for introducing auxiliary network parameters to compensate for lost dependencies, and proposed intuitive conditions for determining these parameters. We have also shown that our earlier method corresponds to Iterative Belief Propagation (IBP) when enough edges are deleted to yield a polytree, and corresponds to some generalizations of IBP when fewer edges are deleted. In this paper, we propose a different criteria for determining auxiliary parameters based on optimizing the KL– divergence between the original and approximate networks. We discuss the relationship between the two methods for selecting parameters, shedding new light on IBP and its generalizations. We also discuss the application of our new method to approximating inference problems which are exponential in constrained treewidth, including MAP and nonmyopic value of information.",
    "creator" : "dvips(k) 5.94a Copyright 2003 Radical Eye Software"
  }
}