{
  "name" : "1203.3474.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Distribution over Beliefs for Memory Bounded Dec-POMDP Planning",
    "authors" : [ "Gabriel Corona", "François Charpillet" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose a new point-based method for approximate planning in Dec-POMDP which outperforms the state-of-the-art approaches in terms of solution quality. It uses a heuristic estimation of the prior probability of beliefs to choose a bounded number of policy trees: this choice is formulated as a combinatorial optimisation problem minimising the error induced by pruning."
    }, {
      "heading" : "1 Introduction",
      "text" : "The problem of the control of a team of cooperative agents in interaction with a stochastic environment has a large field of applications: packet routing in a network, robot coordination, control of sensor networks, zone surveillance etc. The Dec-POMDP (Decentralised Partially Observable Markov Decision Process) formal framework may be used to tackle this problem rigorously. We focus here on the particular case of planning: given the rules of the environment and a goal modelled as a reward function, we are searching for a policy to assign to each agent in order to maximise the rewards. The computation of an optimal joint policy is NEXP-complete in a finite horizon [Bernstein et al., 2000] when the horizon is smaller than the number of states. The exact computation is thus not usable except for very small problems (especially, with a small number of observations) and for small horizons.\nSeveral approaches have been proposed to solve Dec-POMDPs: bottom-up dynamic programming [Hansen et al., 2004], [Szer and Charpillet, 2006], topdown search [Szer et al., 2005] and mathematical programming [Aras et al., 2007]. The number of partial policies to consider in an exact dynamic programming approach is generally exponential in the planning horizon and the number of agents and doubly exponential\nin the number of observations [Hansen et al., 2004]. Some algorithms have been proposed to overcome this problem: MBDP (Memory Bounded Dynamic Programming) [Seuken and Zilberstein, 2007b], IMBDP (Improved MBDP) [Seuken and Zilberstein, 2007a], MBDP-OC (MBDP with Observation Compression) [Carlin and Zilberstein, 2008], PBIP (Point Based Incremental Pruning) [Dibangoye et al., 2009]. They use only a bounded number of partial plans at each planning step. By bounding the memory, they are able to solve problems with significantly longer horizons than previous algorithms: the complexity is linear with the horizon. By avoiding the exhaustive backup, some of them can solve problems with a higher number of observations.\nIn this article, we propose a new point based approach, PSMBDP (Policy Search Memory Bounded Dynamic Programming), which uses a heuristic estimation of the prior probability distribution of reachable beliefs and formulates the choice of a bounded number of partial plans as a combinatorial optimisation problem. Using this heuristics information, our algorithm finds solutions of better quality than state-of-the-art approaches such as MBDP and PBIP."
    }, {
      "heading" : "2 Models",
      "text" : ""
    }, {
      "heading" : "2.1 MDP",
      "text" : "The MDP framework (Markov Decision Process) models the decision problem of a single agent which seeks to maximise rewards by interacting with a fully observable environment. The time is discrete and at each time step t, the agent knows exactly the state St of the environment and chooses an action At+1. A MDP is a tupleM = 〈S,A, T ,R〉 where: S et A are respectively the sets of states s of the environment and of actions a of the agent; the transition function T represents the dynamic of the system by defining the transition probabilities, T (s′|s, a) = Pr(St+1 = s′|St = s,At+1 = a), from state s to state s′ when performing action a; the\nreward function R represents the goal of the agent by defining the expectation R(s, a) = E[Rt+1|St = s,At+1 = a] of the reward Rt+1 obtained after doing action a in state s. In finite horizon H, the goal of the agent is to maximise the expected sum of the rewards, E[ ∑H t=1R t]."
    }, {
      "heading" : "2.2 POMDP",
      "text" : "The POMDP framework (Partially Observable MDP) generalises the MDP framework for partially observable environments: instead of observing the state St, the agent receives an observations Ot which can be used to infer St. At each time t, the agent chooses an action At given the past observations O1 . . . Ot−1 and actions A1 . . . At−1. A POMDP is a tuple M = 〈S,A, T ,R,Ω,O〉 where: Ω is the set of observations o that the agent can receive; the observation function O defines the observation probabilities O(o|s, a, s′) = Pr(Ot+1 = o|St = s,At+1 = a, St+1 = s′) of observing o in the transition from s to s′ with action a.\nA probability distribution Bt ∈ ∆S represents the belief of the agent, i.e. the Bayesian knowledge it has inferred on the current state St given the past observations and actions:\nBt(s) = Pr(St = s|B0, A1, O1 . . . At, Ot) = Pr(St = s|Bt−1, At, Ot)\nWe note τ the belief update function : Bt = τ(Bt−1, At, Ot). The belief is a sufficient statistic for the agent so a POMDP policy is usually a mapping from belief to action.\nFor convenience, we use the notation:\nO(o|b, a) = Pr(Ot+1 = o|Bt = b, At = a) = ∑\n(s,s′)∈S2 T (s′|s, a)O(o|s, a, s′)b(s)"
    }, {
      "heading" : "2.3 Dec-POMDP",
      "text" : "The Dec-POMDP framework generalises the POMDP framework for a team of agents. A Dec-POMDP is a tupleM = 〈I,S, (Ai)i∈I , T ,R, (Ωi)i∈I ,O〉 where I is the finite set of agents i. Each agent has its own sets Ai and Ωi of local actions ai and observations oi: a joint action a = (a1, . . . an) ∈ A is a tuple of local actions where A = ∏ i∈I Ai is the set of joint actions; a joint observation o = (o1, . . . on) ∈ Ω is a tuple of local observations where Ω = ∏ i∈I Ωi is the set of joint observations. At each time t, each agent i chooses a local action Ati given the past local observations O1i . . . O t−1 i and actions A1i . . . A t−1 i . The initial belief B\n0 is a prior on the initial state S0 shared by the agents. The joint\npolicy π is searched as a tuple of local policies πi, one for each agent i.\nThe underlying POMDP of a Dec-POMDP M is the POMDP obtained by replacing the |I| agents by a single agent receiving the joint observation and choosing the joint action. The underlying MDP is defined as the MDP obtained by observing the state directly."
    }, {
      "heading" : "2.4 Policy Trees",
      "text" : "In a finite horizon problem, local deterministic policies of a Dec-POMDP can be modelled as policy trees (figure 1) whose nodes are actions and whose branches are observations. We note aqi the root action of policy tree qi and qi(oi) its subtree for local observation oi. At each time step, the internal state of the agent Qti is defined by a local policy tree qi.\nJoint policies trees are tuples of local policy trees (one for each agent), q = (q1, . . . , qn) ∈ Q = ∏ iQi. We note aq = (aq1 , . . . , aqn) the root joint action of q made of the local root actions aqi and q(o) = (q1(o1), . . . , qn(on)) the joint sub-policy tree made of subtrees qi(oi) of the local policy trees qi for the local observations oi. A joint policy tree q can be evaluated using its value function Vq defined for every distribution b on the states:\nVq(b) = ∑ s∈S b(s)Vq(s)\nVq(s) = R(s, aq) + ∑\n(s′,o)∈S×Ω\nT (s′|s, aq)O(o|s, aq, s′)Vq(o)(s′)\nThe value function can be represented by its αvector, a |S|-vector containing the Vq(s): αq = (Vq(s1), . . . , Vq(s|S|)) T and Vq(b) = b · αq."
    }, {
      "heading" : "3 State of the Art",
      "text" : ""
    }, {
      "heading" : "3.1 Dynamic Programming",
      "text" : "Dynamic Programming (DP) Dec-POMDP planning builds policy trees in a bottom-up way\n[Hansen et al., 2004] (see algorithm 1). The DP operator (line 2) builds the sets Qti of policy trees of horizon H − t (executed from time t to H) from the sets Qt+1i of policy trees of horizon H− t−1 (executed from time t+ 1 to H). This operator is usually made of two phases: backup and pruning (algorithm 2).\nAlgorithm 1: Dynamic Programming planning Result: ∀i,Q0i and their α-vectors foreach i ∈ I do QHi ← {ψ} (empty tree)1 for t=H-1 to 0 do (Qt)i∈I ← DP((Qt+1i )i∈I)2\nAlgorithm 2: General Two Phases DP operator (Q̄ti)i∈I ← Backup((Q t+1 i )i∈I)1 (Qti)i∈I ← Prune((Q̄ti)i∈I)2\nBackup The exhaustive backup step builds the sets Q̄ti of local policy trees of horizon H − t by building every tree whose root is an action of Ai and whose branches associate a local observation to a tree of Qt+1i from the previous iteration: Q̄ti = Ai×(Q t+1 i )\nΩi . However the number of policy trees of agent i is exponential with the number of local observations Ωi and doubly exponential with the horizon.\nPruning In order to limit the growth of the number of policy trees, a pruning phase (algorithm 3) is used which consists of eliminating trees of Q̄ti to produce the sets Qti. In exact pruning, the dominated policy trees are pruned. This is checked by solving a linear program. In general, the number of policy trees is still in the same order of magnitude.\nAlgorithm 3: Exact Prune ∀i ∈ I,Qti ← Q̄ti1 flag← true2 while flag do3 flag← false4 foreach i ∈ I do5 foreach qi ∈ Qti do6 if qi dominated then7 Qti ← Qti \\ {qi}8 flag← true9"
    }, {
      "heading" : "3.2 MBDP",
      "text" : "Memory Bounded Dynamic Programming (MBDP) [Seuken and Zilberstein, 2007b] has been proposed to solve Dec-POMDPs with a high horizon (algorithm 4).\nIt replaces exact pruning by a memory bounded pruning: for each agent i, at most W policy trees from Q̄ti are kept in the set Qti. W points (probability distribution on the states) are generated using some heuristic policies. For each point b, the best joint policy, q = (q1, . . . , qn), is selected (line 4) and the corresponding local trees qi are moved from Q̄ti to Qti (lines 5 and 6).\nAlgorithm 4: DP Operator for MBDP Data: Bt, W points Data: ∀i,Qt+1i and their α-vectors Result: ∀i,Qti and their α-vectors with Qti| ≤W foreach i ∈ I do Q̄ti ← Ai × (Q t+1 i ) Ωi1 foreach i ∈ I do Qti ← ∅2 foreach b ∈ Bt do3 (qi)i∈I ← arg maxq∈∏i Q̄ti Vq(b)4 foreach i ∈ I do Qti ← Qti ∪ {qi}5 foreach i ∈ I do Q̄ti ← Q̄ti \\ {qi}6\nPartial backup The complexity of the exhaustive backup is exponential with the number of observations. Improved MBDP [Seuken and Zilberstein, 2007a] and MBDP with Observation Compression [Carlin and Zilberstein, 2008], replace the exhaustive backup step by a partial backup in order to scale with the number of observations."
    }, {
      "heading" : "3.3 PBIP",
      "text" : "Point Based Incremental Pruning (PBIP) [Dibangoye et al., 2009] uses another approach to solve the scalability problem of exhaustive backup. It avoids the exhaustive generation and enumeration of joint policies by using one phase of branch-and-bound search instead of the two phases of backup and pruning (algorithm 5). For each point b, a depth-first branch-and-bound search of the optimal joint tree is done (line 5).\nBranch A search tree node q̃ represents a set of joint policy trees q defined by a set of constraints. It can be seen as a partially defined policy tree. Each child node of the search tree is a subset of its parent defined by an additional constraint on q, either by fixing the joint root action aq ∈ A or by fixing a joint subtree q(o) ∈ Qt+1 for a given joint observation o (see figure 2). The policy tree must not already have been selected: this constraint (q /∈ Sel) is checked on the leaf nodes.\nDecentralisability A joint policy tree q is decentralisable i.e. it can be divided in local policy trees qi such that:\n∀o = (o1, . . . , on) ∈ Ω, q(o) = (q1(o1), . . . , qn(on)) (1)\nThis constraint restricts the valid assignments of subtrees q(o).\nBound The heuristic value f(q̃) of a search tree node q̃ is a upper bound of the value Vq(b) of the best policy tree q ∈ q̃ matching the constraints. The set Ω of joint observations is partitioned in two subsets, Ω = Ω1∪Ω2:\n• Ω1 is the set of joint observations o whose joint subtree q(o) is already fixed, either directly by a q(o) = q′ constraint or indirectly through the decentralisability constraint;\n• Ω2 is the set of joint observations o whose joint subtree q(o) is not completely fixed yet; they may be partially constrained by the decentralisability constraint.\nThe problem is relaxed by ignoring the decentralisability constraints for the subtrees q(o) of observation o ∈ Ω2. The contribution of the best joint subtree is used:\nf(q̃) = R(b, aq) + ∑ o∈Ω1 O(o|b, aq) [ αq(o) · τ(b, aq, o) ] + ∑ o∈Ω2 O(o|b, aq) max q′∈Qt+1 [αq′ · τ(b, aq, o)]\nThe theoretical complexity is still exponential with maxi |Ωi| but in practice a large part of the search tree is pruned. In the benchmarks, PBIP handles problems with a higher number of observations than MBDP and finds better solutions than the approaches using partial backup for a lower computation time."
    }, {
      "heading" : "4 Proposition",
      "text" : ""
    }, {
      "heading" : "4.1 Intuitive Idea",
      "text" : "For simplicity of the presentation, our approach is first described in the single agent case. A first approach for\nAlgorithm 5: DP Operator for PBIP Data: Bt, W points Data: ∀i,Qt+1i and their α-vectors Result: ∀i,Qti and their α-vectors, with |Qti| ≤W Sel← ∅1 foreach i ∈ I do Qti ← ∅2 foreach b ∈ Bt do3\nLet the search space Qt+1 = ∏ iAi × (Q t+1 i ) Ωi4 Search (branch and bound) q = (qi)i∈I as5 arg maxq∈Qt+1\\Sel Vq(b) Sel← Sel ∪ {q}6 foreach i ∈ I do ∀i ∈ I,Qti ← Qti ∪ {qi}7\nthe selection of trees consists of maximising the mean of V t(b) (figure 3):\nmaximise ∫ V t(b) db\nThe choice of the set Qt ofW trees among the |A|W |Ω| (generated by exhaustive backup) becomes a combinatorial optimisation problem of variable Qt:\nmax ∫ V t(b) db with V t(b) = max\nq∈Qt Vq(b)\ns.t. Qt ⊆ Q̄t = A× (Qt+1)Ω\n|Qt| ≤W\nb0 1 b(s1)\n1 0b(s2)\nV\nRetained α vectors\nIgnored α vectors\nMean regret (minimised)\nMean value (maximised)\nFigure 3: Intuitive Selection Criterion\nHeuristic distribution Only a subset of the beliefs may be reachable and this information can be used for planning [Szer and Charpillet, 2006]. More generally some beliefs may more probable than others. We introduce a heuristic probability distribution1 µt of the belief Bt at time step t in the criterion:\nmaximise ∫ V t(b)µt(b) db = E\nb∼µt\n[ V t(b) ] 1Bt ∈ ∆S is a posterior probability distribution on states s ∈ S so µt ∈ ∆∆S is a meta-distribution on states.\nThe prior probability distribution of Bt cannot be determined before planning because it depends on the policy. In practice, a heuristic policy π̃ is used to generate the heuristic distributions µt. µt is the prior probability distribution2 of Bt for the given heuristic policy π̃: µt(b) = Pr(Bt = b|B0, π̃).\nMonte-Carlo Sampling The µt distribution is a finite distribution (in finite horizon, for a finite number of observations and for a given initial belief B0 = b0) and the integration is a finite sum:\nE b∼µt\n[ V t(b) ] = ∑ b Pr(Bt = b|B0, π)V t(b)\nHowever the size of the set of reachable beliefs is generally exponential with the horizon which hinders exact evaluation. Monte-Carlo sampling on the heuristic probability distribution of Bt can be used to approximate this criterion,\nE b∼µt\n[ V t(b) ] ≈ 1 N N∑ k=1 V t(btk) (2)\nwhere the btk are independent samples (the points of the method) of µt and N is the number of samples. Each sample is generated by simulating the heuristic policy until time t."
    }, {
      "heading" : "4.2 Criterion",
      "text" : "A Dec-POMDP can be seen as a POMDP with a constraint on the form of its policy: the policy must be decentralisable. This constraint is added to criterion (2). Given the points Bt, we are searching, for each agent i, for a Qti set of at most W policy trees among the |Ai|W |Ωi| generated by exhaustive backup:\nmaximise ∑ b∈Bt max q∈ ∏ i∈I Qti Vq(b) (3)\ns.t. ∀i ∈ I,Qti ⊆ Q̄ti = Ai × (Qt+1i ) Ωi\n∀i ∈ I, |Qti| ≤W\nThe samples are taken from the prior distribution of the POMDP beliefs using some heuristic policies. In order to better take into account the decentralised nature of the problem, we might want to work with DecPOMDP beliefs: it is however not clear how those beliefs should be handled as each agent has its own beliefs; attempts have been made to use them but they are not presented in this paper as they do not provide significant improvements over the first approach\n2The Bt random variable is a function of the history of actions an observations, (A1,Ω1 . . .At,Ωt). We are interested in the problem of prediction of Bt from time 0.\non the benchmarks. The heuristics from MBDP can be used as well.\nOne key difference with MBDP based approaches is that in those approaches each joint policy tree is selected based on only one point b. In PSMBDP, the policy trees are selected as a whole based on the distribution µt. This distribution is approximated by MonteCarlo sampling so the number N of points used can be arbitrarily higher than the W parameter."
    }, {
      "heading" : "4.3 Algorithm",
      "text" : "PSMBDP builds a policy with a memory bounded DP operator: the DP operator uses an exhaustive backup phase and a pruning phase solving (3). Algorithm 6 is a general PSMBDP algorithm using a two step DP operator. Contrary to the approaches based on MBDP, the number N of samples can be higher than the number W of trees which allows to better cover the belief space. In practice, a heuristic solution of (3) is searched.\nIn the current implementation, criterion (3) is solved using a heuristic greedy approach (algorithm 7). The joint policy tree q maximising this criterion is first selected (line 2): this can be done using a PBIP search for the mean belief, b̄ = 1|Bt| ∑ n∈Bt b. Its local trees are placed into the sets Qti (line 3). Then, the algorithm chooses an agent i and adds the best local tree (searched at line 11) in order to optimise:\nmax f(qi) = ∑ b∈Bt max q′∈X Vq′(b) (4)\nwhere X = { (q′j)j∈I ∣∣q′i ∈ Qti ∪ {qi},∀j 6= i, q′j ∈ Qtj}\ns.t. qi ∈ Q̄ti = Ai × (Qt+1i ) Ωi\nThis step is repeated until the maximum number of trees is reached for each agent or the criterion cannot be improved further by adding other local trees.\nAlgorithm 6: Two Phases DP Operator for PSMBDP Data: N points Bt Data: ∀i,Qt+1i and their α-vectors Result: ∀i,Qti and their α-vectors foreach i ∈ I do Q̄ti ← Ai × (Q̄ t+1 i ) Ωi1 Qt ← Prune((Q̄ti)i∈I ,Bt), solving (3)2"
    }, {
      "heading" : "4.4 Branch-and-Bound Search",
      "text" : "Our original algorithm does not scale with the number of observations because the exhaustive backup builds a number of policy trees exponential with the number of observations. In order to scale better with the number\nAlgorithm 7: Prune with Incremental Greedy Optimisation Data: N points Bt Data: ∀i, Q̄ti and their α-vectors Result: ∀i,Qti and their α-vectors Let the criterion ∀Q, I(Q) = ∑ b∈Bt maxq∈Q Vq(b)1\n(qi)i∈I ← arg maxq∈∏i Q̄ti I({q}) (PBIP search)2 foreach i ∈ I do Qti ← {qi}3 foreach i ∈ I do Q̄it ← Q̄it \\ {qi}4 flag← true5 while ∃i, |Qti| 6= W ∧ flag do6 flag← false7 foreach i ∈ I do8 if |Qti| 6= W then9 Let ∀qi,K(qi) = (Qti ∪ {qi})× ∏ j 6=iQtj10 qi ← arg maxq̄i∈Q̄ti I(K(q̄i))11 if enhancement of I(Qt) then12 Qti ← Qti ∪ {qi}13 Q̄ti ← Q̄ti \\ {qi}14 flag← true15\nof observations, the combinatorial optimisation problem (4) (line 11 of algorithm 7) can be solved without explicitly building and exhaustively enumerating the set Q̄ti = Ai × (Q t+1 i )\nΩi of local policy trees: it can be enumerated implicitly by branch-and-bound search similarly to what is done in PBIP. The backup phase is not needed anymore.\nBranch A search tree node q̃i represents a set of local policy trees qi defined by a set of constraints. It can be seen as a partially defined local policy tree. Each child node q̃i of the search tree is a subset of its parent defined by adding a constraint on qi, either by fixing the local action aqi ∈ Ai or by fixing a joint subtree qi(oi) ∈ Qt+1i for a given local observation oi (see figure 4). A difference with PBIP, is that the search is done in the space of local policy trees instead of the space of joint policy trees: the search space is smaller, |Ai|W |Ωi| instead of |A|(W |I|)maxi∈I |Ωi| for PBIP and every search tree node is valid.\nBound The heuristic value f(q̃i) of a search tree node q̃i is a upper bound of the value f(qi) for every local policy tree qi ∈ q̃i matching the constraints (see criterion 4):\nf(q̃i) = ∑ b∈Bt ( max q∈Qt Vq(b), max q−i∈Qt−i V +〈q̃i,q−i〉(b) )\nwhere q̃ = 〈q̃i, q−i〉 = {〈qi, q−i〉|qi ∈ q̃i} is the set of joint policy trees q = 〈qi, q−i〉 made of the policy trees\nq−i and one local policy tree qi ∈ q̃−i. It can be seen as a partially defined joint policy tree made of q−i and the partially defined local policy tree q̃i. V +q̃ is a upper bound of the value of the best policy tree q ∈ q̃. For a given search tree node q̃i, the set of local observations is partitioned in two subsets, Ω = Ω1∪Ω2: Ω1i is the set of local observations oi of agent i whose subtree qi(oi) is already fixed; Ω2i is the set of local observations oi of agent i whose subtree qi(oi) is not fixed yet. Let Ω1 = Ω1i × Ω−i and Ω2 = Ω2i × Ω−i be the corresponding sets of joint observations. The problem is relaxed by ignoring the decentralisability constraints for o ∈ Ω2 and by choosing a different policy tree q = 〈qi, q−i〉 for each belief b and each q−i. The contribution of the best possible local subtree q′i ∈ Q t+1 i is used:\nV +q̃ (b) = R(b, aq)+ ∑ o∈Ω1 O(o|b, aq) [ αq(o) · τ(b, aq, o) ] + ∑ o∈Ω2 O(o|b, aq) max q′i∈Q t+1 i α〈q′i,q−i(o−i)〉 · τ(b, aq, o)\nThe algorithm is the same as 7 but line 11 is solved with branch-and-bound search and Q̄ti is not computed explicitly by the DP operator.\nWe have tested both depth-first and best-first search. The latter uses more memory but the memory requirement is still reasonable and the computation is much faster on the tested benchmarks.\nAs with PBIP, the size of the search space is exponential with the number of observations. If a huge part of the search space is pruned in practice, we have no guarantee that, in general, the explored search space is not exponential with the number of observations."
    }, {
      "heading" : "5 Experiments",
      "text" : "This section contains results of different algorithms for some benchmark problems, the average expected reward (AEV), its standard deviation (σ) and the mean computation time (T ) in seconds.\nAlgorithms PBIP/BeFS is a variation of PBIP with best-first search. PSMBDP and PSMBDP/BeFS are respectively exhaustive PSMBDP and PSMBDP with best-first search. In some cases, the value of the optimal policy of the underlying MDP is shown as well: it is an upper bound of the Dec-POMDP policy values.\nHeuristics PSMBDP considers the prior of the beliefs of the underlying POMDP when using a heuristic policy and sample belief points on those distributions: a half of the points uses the POMDP policy greedy with respect to the optimal MDP value function; the other half uses the random policy. MBDP and PBIP use MDP heuristic based on the prior distribution on states: a half of the points is taken from the MDP heuristic of MBDP; the other half is taken from the random heuristic of MBDP."
    }, {
      "heading" : "5.1 Dec-Tiger",
      "text" : "Table 1 shows results for the Dec-Tiger problem [Nair et al., 2003] for H = 100, with 50 executions and N = 100 for PSMBDP. The (b) variant use the same heuristic as MBDP. Using the POMDP belief prior as heuristic does not give good results. It is not necessary to give a high value of W to PSMBDP to find a good solution because a smaller (usually 5) number of policy trees per time step are kept by PSMBDP.\nComputation times are lower with PSMBDP for this problem, because only a small number of trees per agent are kept: the average width of the local policy trees is 5 for all heuristics. The other trees are not kept as they do not enhance the given criterion. The number of trees and of α-vectors generated by exhaustive backup is thus lower."
    }, {
      "heading" : "5.2 Firefighters",
      "text" : "Results for the firefighting problem [Oliehoek et al., 2008] with 2 agents, 4 houses and 3 fire levels are shown in table 2 for horizon 50\nusing W = 7. For PSMBDP, N = 100. PSMBDP finds solutions of better quality than MBDP."
    }, {
      "heading" : "5.3 Modified Firefighters",
      "text" : "In order to test problems with a higher number of observations, we use a modified firefighting problem where each agent observes the resulting fire intensity in the house it goes to. Table 3 show results for this modified problem with |I| = 2, 4 houses, fire from 0 to 3, H = 100, averaged on 25 executions with W = 3 and N = 100 for PSMBDP. PSMBDP finds solutions of better quality than PBIP."
    }, {
      "heading" : "5.4 Cooperative Box Pushing",
      "text" : "The Cooperative Box Pushing problem [Seuken and Zilberstein, 2007a] is a problem with a higher number of observations (5 observations per agent for two agents) and has been proposed to benchmark algorithms which can handle a larger number of observations. The results shown in table 4 are average over 25 executions. PBIP times out for H = 50 (T > 20, 000s). PSMBDP finds slightly better solutions than PBIP for a comparable computation time."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have introduced a new optimisation criterion to choose the policy trees in a memory-bounded bottomup dynamic programming approach, i.e. the maximisation of the expected reward given a heuristic policy over beliefs. We have presented PSMBDP, a planning algorithm for Dec-POMDPs based on this principle. A simple implementation using exhaustive enumeration of policies may be used for small problems. In order to handle problems with a higher number of observations, a version with implicit enumeration of joint policy trees by branch-and-bound search may be used.\nPSMBDP is able to find solutions of better quality than MBDP and PBIP. However, the results depend strongly on the problems and heuristics used. Our optimisation scheme use a simple greedy heuristic so we may be able to find solutions of better quality by using better optimisation methods."
    } ],
    "references" : [ {
      "title" : "Mixed integer linear programming for exact finite-horizon planning in decentralized POMDPs",
      "author" : [ "Aras et al", "R. 2007] Aras", "A. Dutech", "F. Charpillet" ],
      "venue" : "In Proceedings of the Thirteenth International Conference on Automated Planning and",
      "citeRegEx" : "al. et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2007
    }, {
      "title" : "The complexity of decentralized control of markov decision processes",
      "author" : [ "Bernstein et al", "D.S. 2000] Bernstein", "S. Zilberstein", "N. Immerman" ],
      "venue" : "In Mathematics of Operations Research,",
      "citeRegEx" : "al. et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2000
    }, {
      "title" : "Value-based observation compression for DEC-POMDPs",
      "author" : [ "Carlin", "Zilberstein", "A. 2008] Carlin", "S. Zilberstein" ],
      "venue" : "Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems,",
      "citeRegEx" : "Carlin et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Carlin et al\\.",
      "year" : 2008
    }, {
      "title" : "Point-based incremental pruning heuristic for solving finitehorizon dec-pomdps",
      "author" : [ "Dibangoye et al", "J.S. 2009] Dibangoye", "Mouaddib", "A.-I", "B. Chaib-draa" ],
      "venue" : "In AAMAS ’09: Proceedings of The 8th International Conference on Au-",
      "citeRegEx" : "al. et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2009
    }, {
      "title" : "Dynamic programming for partially observable stochastic games",
      "author" : [ "Hansen et al", "E. 2004] Hansen", "D. Bernstein", "S. Zilberstein" ],
      "venue" : "In Proceedings of the Nineteenth National Conference on Artificial Intelligence,",
      "citeRegEx" : "al. et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2004
    }, {
      "title" : "Taming Decentralized POMDPs: Towards Efficient Policy Computation for Multiagent Settings",
      "author" : [ "Nair et al", "R. 2003] Nair", "M. Tambe", "M. Yokoo", "D. Pynadath", "S. Marsella" ],
      "venue" : "Proceedings of the Twentieth International Joint Con-",
      "citeRegEx" : "al. et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2003
    }, {
      "title" : "Optimal and approximate Q-value functions for decentralized POMDPs",
      "author" : [ "Oliehoek et al", "F.A. 2008] Oliehoek", "M.T.J. Spaan", "N. Vlassis" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "al. et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2008
    }, {
      "title" : "Improved Memory-Bounded Dynamic Programming for Decentralized POMDPs",
      "author" : [ "Seuken", "Zilberstein", "S. 2007a] Seuken", "S. Zilberstein" ],
      "venue" : "In Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligences (UAI-07),",
      "citeRegEx" : "Seuken et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Seuken et al\\.",
      "year" : 2007
    }, {
      "title" : "Memory-Bounded Dynamic Programming for DEC-POMDPs",
      "author" : [ "Seuken", "Zilberstein", "S. 2007b] Seuken", "S. Zilberstein" ],
      "venue" : "Proceedings of the Twentieth International Joint Conference on Artificial Intelligences (IJCAI-07)",
      "citeRegEx" : "Seuken et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Seuken et al\\.",
      "year" : 2007
    }, {
      "title" : "Point-based dynamic programming for DEC-POMDPs",
      "author" : [ "Szer", "Charpillet", "D. 2006] Szer", "F. Charpillet" ],
      "venue" : "In Proceedings of the TwentyFirst AAAI National Conference on Artificial Intelligence",
      "citeRegEx" : "Szer et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Szer et al\\.",
      "year" : 2006
    }, {
      "title" : "MAA*: A Heuristic Search Algorithm for Solving Decentralized POMDPs",
      "author" : [ "Szer et al", "D. 2005] Szer", "F. Charpillet", "S. Zilberstein" ],
      "venue" : "In Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "al. et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ ],
    "year" : 2010,
    "abstractText" : "We propose a new point-based method for approximate planning in Dec-POMDP which outperforms the state-of-the-art approaches in terms of solution quality. It uses a heuristic estimation of the prior probability of beliefs to choose a bounded number of policy trees: this choice is formulated as a combinatorial optimisation problem minimising the error induced by pruning.",
    "creator" : "TeX"
  }
}