{
  "name" : "1204.3255.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Lower complexity bounds for lifted inference",
    "authors" : [ "Manfred Jaeger", "M. Jaeger" ],
    "emails" : [ "jaeger@cs.aau.dk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 4.\n32 55\nv2 [\ncs .A\nI] 2\nM ay\nKEYWORDS: Probabilistic-logic models, lifted inference"
    }, {
      "heading" : "1 Introduction",
      "text" : "Probabilistic logic models (a.k.a. probabilistic or statistic relational models) provide high-level representation languages for probabilistic models of structured data (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006; Taskar et al. 2002; Richardson and Domingos 2006). While supporting model specifications at an abstract, first-order logic level, inference is typically performed at the level of concrete ground instances of the models, i.e., at the propositional level. This mismatch between model specification and inference methods has been noted early on (Jaeger 1997), and has given rise to numerous proposals for inference techniques that operate at the high level of the underlying model specifications (Poole 2003; de Salvo Braz et al. 2005; Milch et al. 2008; Kisyński and Poole 2009; Jha et al. 2010; Gogate and Domingos 2011; Van den Broeck et al. 2011; Van den Broeck 2011; Fierens et al. 2011). Inference methods of this nature have collectively become known as “lifted” inference techniques.\nThe concept of lifted inference is mostly introduced on an informal level: “...lifted,\nthat is, deals with groups of random variables at a first-order level” (de Salvo Braz et al. 2005); “The act of exploiting the high level structure in relational models is called lifted inference” (Apsel and Brafman 2011); “The idea behind lifted inference is to carry out as much inference as possible without propositionalizing (Kisyński and Poole 2009); “lifted inference, which deals with groups of indistinguishable variables, rather than individual ground atoms (Singla et al. 2010). While, thus, the term lifted inference emerges as a quite coherent algorithmic metaphor, it is not immediately obvious what its exact technical meaning should be. Since quite a variety of different algorithmic approaches are collected under the label “lifted”, and since most of them can degenerate for certain models to ground, or propositional, inference, it is difficult to precisely define the class of lifted inference techniques in terms of specific algorithmic techniques employed.\nA more fruitful approach is to make more precise the concept of lifted inference in terms of its objectives. Here one observes that lifted inference techniques very consistently are evaluated on, and compared against each other, by how well inference complexity scales as a function of the domain (or population) for which the general model is instantiated. Thus, empirical evaluations of lifted inference techniques are usually presented in the form of domainsize vs. inference time plots as shown in Figure 1.\nVan den Broeck (2011), therefore, has proposed a formal definition of domain lifted inference in terms of polynomial time complexity in the domainsize parameter. Experimental and theoretical analyses of existing lifted inference techniques then show that they provide domain lifted inference in some cases where basic propositional inference techniques would exhibit exponential complexity (as illustrated in Figure 1). However, until recently, these positive results were mostly limited to examples of individual models, and little was known about the feasibility of lifted inference for certain well-defined classes of models. First results that show the feasibility of lifted inference for whole classes of models are given by Van den Broeck (2011), and Domingos and Webb (2012).\nOn the other hand, (Jaeger 2000) has shown that under certain assumptions on the expressivity of the modeling language, probabilistic inference is not polyno-\nmial in the domainsize, thereby demonstrating some inherent limitations in terms of worst-case complexity for the goals of lifted inference. However, the results of (Jaeger 2000) are based on types of probabilistic logic models that are somewhat different from the models that presently receive the most attention: first, they essentially assume a directed modeling framework, in which the model represents a generative stochastic process for sampling relational structures. The model is defined by specifying marginal and conditional probability distributions for random variables corresponding to ground atoms. Ground instances of the model, then, can be represented by directed graphical models, i.e., Bayesian networks. While the majority of existing model classes fall into the category of directed models (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006), there is currently a lot of interest in undirected models that are given by a set of soft constraints on relational structures, specified in the form of potential functions, and in the ground case giving rise to undirected graphical models, i.e., Markov networks. Secondly, the results of (Jaeger 2000) require quite strong assumptions on the expressivity of the probabilistic-logic modeling language, which is required to allow that conditional distributions of atoms can be specified dependent on unrestricted first-order properties. Much current work, in contrast, is concerned with languages that only incorporate certain weak fragments of first-order logic.\nIn this paper the general approach of (Jaeger 2000) is extended to obtain lower complexity bounds for inference in probabilistic-logic model classes that have emerged as the focus of interest for lifted inference techniques, i.e., undirected models based on quantifier- and function-free fragments of first-order logic.\nIn a sharp contrast with (Jaeger 2000), where a “trivial” constant-time approximate inference method was described, we show that our lower complexity bounds also hold for approximate inference. Further sharpening earlier results, we finally establish that the lower complexity bounds also hold for models not using the equality predicate, which in (Jaeger 2000) was conjectured to be the key source of inherent complexity.\nA preliminary version of this paper has been published as (Jaeger 2012). Its main\nresults were also already included in the survey paper (Jaeger and Van den Broeck 2012), which contains a systematic overview of known results and open problems related to the complexity of lifted inference.\nIn the following section we introduce a general framework in which classes of undirected probabilistic-logic models, and classes of associated inference problems can be defined. Section 3 reviews classic results relating first-order logic models to the complexity class NETIME. Section 4 contains our main results, and Section 5 discusses some notable differences that emerge between the results for directed and for undirected models."
    }, {
      "heading" : "2 Weighted Feature Models",
      "text" : "Similarly as (Richardson and Domingos 2006), (Van den Broeck et al. 2011) and (Gogate and Domingos 2011) we assume the following framework: a model, or knowl-\nedge base, is given by a set of weighted formulas:\nKB :\nφ1(v1) : w1 φ2(v2) : w2 . . . . . . φn(vN ) : wN\n(1)\nwhere the φi are formulas in first-order predicate logic, wi ∈ R are non-negative weights, and vi = (vi,1, . . . , vi,ki) are the free variables of φi. The case ki = 0, i.e., φi is a sentence without free variables, is also permitted. The φi use a given signature S of relation-, function-, and constant symbols.\nAn interpretation (or possible world) (D, I) for S consists of a domain D, and an interpretation function I that maps the symbols in S to functions, relations and elements on D. For a tuple d ∈ Dki then the truth value of φi(vi/d) is defined, and we write (D, I) |= φi(d), or simpler I |= φi(d), if φi(/ vi/d) is true in (D, I). We use I(D,S) to denote the set of all interpretations for the signature S over the domain D.\nIn this paper we are only concerned with finite domains, and assume without loss\nof generality that D = Dn := {1, . . . , n} for some n ∈ N.\nFor I ∈ I(Dn, S) let #(i, I) denote the number of elements d in Dki for which\nI |= φi(d). The weight of I then is\nWKB,n(I) :=\nN∏\ni=1\nw #(i,I) i , (2)\nwhere 00 = 1. The probability of I is\nPKB,n(I) = WKB,n(I)/Z\nwhere Z is the normalizing constant (partition function)\nZ = ∑\nI∈I(Dn,S)\nWKB,n(I). (3)\nFor a first-order sentence φ and n ∈ N then\nPKB,n(φ) := PKB,n({I ∈ I(Dn, S) | I |= φ}) (4)\nis the probability of φ in I(Dn, S).\nWe call a knowledge base (1) together with the semantics given by (2) and (4) a weighted feature model, since it associates weights wi with model features φi. Weighted feature models in our sense can be seen as a slight generalization as weighted model counting (wmc) frameworks (Fierens et al. 2011; Gogate and Domingos 2011) in which non-zero weights are only associated with literals. Knowledge bases of the form (1) can be translated into wmc frameworks via an introduction of new relation symbols R1, . . . , Rn, hard constraints φi(vi) ↔ Ri(vi), and weighted formulas Ri(vi) : wi (Van den Broeck et al. 2011; Gogate and Domingos 2011). Up to an expansion of the signature, thus, weighted feature models and wmc are equally expressive. Markov Logic Networks (Richardson and Domingos 2006) also are based on knowledge bases of the form (1) allowing arbitrary formulas φi. However, the\nsemantics of the model there depends on a transformation of the formulas into conjunctive normal form, and therefore does not exactly correspond to (2) and (4), unless the φi are clauses.\nAll types of models here discussed, thus, are very similar in nature, and only differ with respect to certain restrictions on what types of logically defined features can be associated with a weight. The general definition of weighted feature models gives us the flexibility of considering a variety of classes of such restrictions.\nA probabilistic inference problem PI(KB, n, χ, η) for a weighted feature model is given by a knowledge base KB, a domainsize n ∈ N, and two first-order sentences χ, η. The solution to the inference problem is the conditional probability PKB,n(χ | η).\nA class of inference problems is defined by allowing arguments KB, χ, and η only from some restricted classes KB, Q (the query class), and E (the evidence class), respectively. We use the notation\nPI(KB,Q, E) := {PI(KB, n, φ, ψ) | KB ∈ KB, n ∈ N, χ ∈ Q, η ∈ E}\nfor classes of inference problems.\nThe results of this paper will be given for the case where Q consists of all ground atoms, denoted AT , and E is empty. Thus, as far as Q and E are concerned, we are considering the most restrictive class of inference problems. Since we are deriving lower complexity bounds, this leads to the strongest possible results, which directly apply also to more general classes Q and E .\nClasses KB are defined by various syntactic restrictions on the formulas φi in the knowledge base. In this paper, we consider the following fragments of firstorder logic (FOL): relational FOL (RFOL), i.e. FOL without function and constant symbols; 0-RFOL, which is the quantifier-free fragment of RFOL, and 0-RFOL 6=, which is 0-RFOL without the equality relation.\nAn algorithm solves a class PI(KB,Q, E), if it solves all instances PI(KB, n, χ, η) in the class. An algorithm ǫ-approximately solves PI(KB,Q, E), if for any PI(KB, n, χ, η) in the class it returns a number p ∈ [PKB,n(χ | η) − ǫ, PKB,n(χ | η) + ǫ]. An algorithm that solves PI(KB,Q, E) is polynomial in the domainsize, if for fixed KB, χ, η the computation of PI(KB, n, χ, η) is polynomial in n."
    }, {
      "heading" : "3 Spectra and Complexity",
      "text" : "The following definition introduces the central concept for our analysis.\nDefinition 3.1 Let φ be a sentence in first-order logic. The spectrum of φ is the set of integers n ∈ N for which φ is satisfiable by an interpretation of size n.\nExample 3.2 Let φ = ψ1 ∧ ψ2 ∧ ψ3, where\nψ1 ≡ ∀x, y u(x, y) ⇔ u(y, x) ψ2 ≡ ∀x ∃y y 6= x ∧ u(x, y) ψ3 ≡ ∀x, y, y′ (u(x, y) ∧ u(x, y′) ⇒ y = y′)\nφ expresses that the binary relation u defines an undirected graph (ψ1) in which every node is connected to exactly one other node (ψ2, ψ3). Thus, φ describes a pairing relation that is satisfiable exactly over domains of even size: spec(φ) = {n | n even}.\nThe complexity class ETIME consists of problems solvable in time O(2cn), for some constant c. The corresponding nondeterministic class is NETIME. Note that these classes are distinct from the more commonly studied classes (N)EXPTIME, which are characterized by complexity bounds O(2n c ) (Johnson 1990). For n ∈ N let bin(n) ∈ {0, 1}∗ denote the binary coding of n, and un(n) ∈ {1}∗ the unary coding (i.e., n is represented as a sequence of n 1s). A set S ⊆ N is in (N)ETIME, iff {bin(n) | n ∈ S} is in (N)ETIME, which also is equivalent to {un(n) | n ∈ S} being in (N)PTIME.\nLike (Jaeger 2000), we use the following connection between spectra and NETIME\nas the key tool for our complexity analysis.\nTheorem 3.3 (Jones and Selman 1972) A set A ⊆ N is in NETIME, iff A is the spectrum of a sentence φ ∈ RFOL.\nCorollary 3.4 If NETIME 6= ETIME, then there exists a first-order sentence φ, such that {un(n) | n ∈ spec(φ)} is not recognized in deterministic polynomial time.\nThus, by reducing instances n ∈ spec(φ)? of the spectrum recognition problem to probabilistic inference problems PI(KB, n, χ, η), where KB ∈ KB, χ ∈ Q, η ∈ E are fixed for the given φ, one establishes that the PI(KB,Q, E) is not polynomial in the domainsize (under the assumption ETIME 6= NETIME)."
    }, {
      "heading" : "4 Complexity Results",
      "text" : "This section contains our complexity results. We begin with a result for knowledge bases using full RFOL. This is rather straightforward, and (for exact inference) already implied by the results of (Jaeger 2000). We then proceed to extend this base result to 0-RFOL and 0-RFOL 6=."
    }, {
      "heading" : "4.1 Base Result: the RFOL Case",
      "text" : "Theorem 4.1 If NETIME 6= ETIME, then there does not exist an algorithm that 0.25-approximately solves PI(RFOL,AT , ∅) in time polynomial in the domainsize.\nThe proof of this theorem provides the general pattern also for subsequent proofs.\nIt is therefore here given in full. Proof: Let φ be a sentence with a non-polynomial spectrum as given by Corollary 3.4. Let S be the relational signature of φ. Let a() be a new relation symbol of\narity zero (i.e., a() represents a propositional variable). The first weighted formula in our knowledge base then is\n¬(φ ↔ a()) : 0 (5)\nWe now already have that PKB,n(a()) > 0 iff there exists I ∈ I(Dn, S) with I |= φ, i.e., iff n ∈ spec(φ). This already reduces the decision problem for spec(φ) to solving PI(KB, n, a(), ∅) exactly. However, from the 0-1 laws of first-order logic (Fagin 1976), it follows that for our current KB : PKB,n(a()) →n→∞ 0. Thus, for every ǫ > 0 we could define an ǫ-approximate constant-time inference algorithm by returning 0 for all sufficiently large n.\nIn order to obtain our result for approximate inference, we will now ensure that for all n ∈ spec(φ) the probability PKB,n(a()) is greater than 0.5, while it remains zero for n 6∈ spec(φ). We do this essentially by calibrating the normalization constant Z in (3). For this we introduce another new relation b(), and add to KB :\n¬(( ∧\nR∈S\n∀x¬R(x)) ↔ b()) : 0 (6)\nThus, for every n there is exactly one interpretation I ∈ I(Dn, S) with nonzero weight in which b() is true (the one in which all relations have empty interpretations). Finally, we give zero weight to all interpretations except those in which a() or b() is true:\n¬(a() ∨ b()) : 0 (7)\nLet KB consist of (5),(6),(7). Every I ∈ I(Dn, S) then has weight 0 if it satisfies one of the three formulas, and weight 1 otherwise. Consider the case n 6∈ spec(φ). Then, by (5) WKB,n(a()) = 0. By (7) this then means that in all interpretations of nonzero weight b() must be true. By (6) there is exactly one such interpretation. Thus, Z in (3) is 1, and PKB,n(a()) = 0/1 = 0.\nIf n ∈ spec(φ), then WKB,n(a()) ≥ 1, and Z = WKB,n(a()) (if the interpretation in which all R are empty also is a model of φ), or Z = WKB,n(a()) + 1 (otherwise). Thus, PKB,n(a()) ≥ 1/2. A 0.25-approximate inference algorithm for PI(KB, n, a(), ∅), thus, would decide spec(φ)."
    }, {
      "heading" : "4.2 The 0-RFOL Case",
      "text" : "We now proceed towards our main result, which is going from RFOL to 0-RFOL. If we wanted to allow function and constant symbols in our knowledge base, then one could go to a quantifier-free fragment in a quite straightforward manner using Skolemization. Since satisfiability over a given domain is the same for a formula φ and its quantifier-free Skolemized version φSkol, the arguments of the proof of Theorem 4.1 would go through with little change. In order to accomplish the same using only the relational fragment 0-RFOL, we define the relational Skolemization of a formula. The idea is to replace function and constant symbols in the Skolemized version of a formula with relational representations. For example, the Skolemized\nversion of ψ2 from Example 3.2 is\nψSkol2 ≡ ∀x f(x) 6= x ∧ u(x, f(x))\nwith a new function symbol f(). Introducing a relational encoding of f() leads to\nψR-Skol2 ≡ ∀x, y R f (x, y) → (y 6= x ∧ u(x, y))\nwith Rf a new binary relation symbol encoding f(). This translation must be accompanied by axioms that confine the possible interpretations of Rf to relations that encode functions.\nSuch relational encodings of functions are well established. However, there does not seem to be a standard account of this technique that serves our purpose. The following proposition, therefore, provides the relevant result in a form tailored for our needs.\nProposition 4.2 Let φ(x) ∈ 0-FOL(S ∪ SF ), where S is a set of relation symbols, and SF a set of function and constant symbols. Let S+ be a set of new relation symbols that for every k-ary f ∈ SF contains a k+1-ary Rf (constant symbols are treated as 0-ary function symbols). Let Func be the set of sentences that for every f ∈ SF contains\n∀x y y′ (Rf (x, y) ∧Rf(x, y′) → y = y′) (8)\n∀x∃y Rf (x, y). (9)\nThen there exists a formula φ+(x, z) ∈0-RFOL(S ∪ S+), such that the following are equivalent for all n:\ni there exists I ∈ I(Dn, S ∪ SF ) with I |= ∀xφ(x) ii there exists I+ ∈ I(Dn, S ∪ S+) with I+ |= Func ∧ ∀xz φ+(x, z)\nIf φSkol is the Skolemization of a formula φ ∈RFOL, we then call φSkol + the\nrelational Skolemization of φ, written φR-Skol.\nOur plan, now, is to prove the analogon of Theorem 4.1 for 0-RFOL by replacing φ in (5) with φR-Skol. However, this is not enough, since we also need to constrain the models of our knowledge base (more precisely: those models in which a() is true) to satisfy the axioms (8) and (9). This poses a problem, because (9) contains an existential quantifier, and so we cannot add this axiom directly as a constraint to a knowledge base restricted to 0-RFOL. Indeed, we almost seem to have gone full circle, since we are back at knowledge bases in a relational vocabulary with existential quantification! However, we now have reduced arbitrary occurrences of existential quantifiers to occurrences only within in the special formulas (9).\nOur strategy, now, is to approximate formulas (9) with weighted formulas of the\nform\na() ∧Rf(x, y) : w (10)\nthat reward models of a() in which the existential quantifier of (9) is satisfied for many (all) x. We will no longer be able to ensure that WKB,n(a()) = 0 when n 6∈ spec(φ). However, by a suitable choice of w, and by a careful calibration of the weight of models of the alternative proposition b(), we still can ensure that\nWKB,n(b()) ≫ WKB,n(a()) when n 6∈ spec(φ), and WKB,n(b()) ≈ WKB,n(a()) when n ∈ spec(φ). However, the right calibration of the weights of models of a() and b() within I(Dn, S) will now require that one sets w to a value w(n) depending on n.\nThis means that we no longer can reduce the decision problem n ∈ spec(φ) to the probabilistic inference problem PI(KB, n, a(), ∅) for a fixed knowledge base KB. We only achieve a reduction to the inference problem PI(KB(w(n)), n, a(), ∅), where the logical structure of KB is fixed, but a weight parameter w(n) depends on n. Generally, for a knowledge base KB containing N weighted formulas, we denote with KB(w1, . . . , wN ) the knowledge base that contains the same formulas as KB, but with the weights set to values w1, . . . , wN .\nTo translate the lower complexity bounds of the original spectrum recognition problem into lower complexity bounds for the resulting inference problem, one now has to be precise about the representation of the inference problem. To this end, we assume that weights w are rational numbers, and represented by pairs (u, v) of integers, so that w = u/v. We then define the representation size l(w) as log(| u | +1) + log(| v | +1). The total representation size of the weight parameters w = (w1, . . . , wN ) in a knowledge base is l(w) := ∑N i=1 l(wi). An inference algorithm for probabilistic inference problems in PI(KB,Q, E) is polynomial in the domainsize and the representation size of the weight parameters, if for any KB ∈ KB, χ ∈ Q, η ∈ E the class of inference problems PI(KB(w), n, χ, η) can be solved in time that is bounded by a polynomial ∑d\ni,j=0 αi,j l(w) inj (αi,j ∈ R, d ∈ N ). We can now\nstate the following theorem:\nTheorem 4.3 If NETIME 6= ETIME, then there does not exist an algorithm that 0.2-approximately solves PI(0-RFOL,AT , ∅) in time polynomial in the domainsize and the representation size of the weight parameters.\nThe full proof of the theorem is given in the appendix. It consists of a polynomialtime reduction of the n ∈ spec(φ) decision problem to a probabilistic inference problem PI(KB(w(n)), n, a(), ∅), where l(w(n)) is polynomial in n. An inference algorithm that can solve PI(KB(w(n)), n, a(), ∅) in time polynomial in the domainsize and l(w(n)), thus, would yield a polynomial decision procedure for spec(φ).\n4.3 Polynomiality in l(w)\nOne may wonder how strong or surprising Theorem 4.3 really is in light of its extra runtime polynomial in l(w) condition. It has previously been emphasized that lifted inference procedures should only be expected to be polynomial in the domain size, but not in other parameters that characterize the complexity of KB (Jaeger 2000; Van den Broeck 2011). These remarks, however, have mostly been motivated by considerations of the logical complexity of KB, e.g. in terms of the number and complexity of its weighted formulas, or the size of the signature. The complexity in terms of numerical parameters, on the other hand, has not received much attention.\nTo better understand the nature of the condition of being polynomial in the domainsize and l(w), we have to look a little closer at how the parameters affect\nthe complexity of the computation. We consider algorithms that can be described as follows: to compute PI(KB(w), n, χ, η) the algorithm performs a number of steps i = 1, . . . , L, where step i consists either of executing a constant time operation that does not depend on the numerical model parameters (e.g., a logical operation on formulas), or of a basic operation on numerical parameters.\nWe consider the executions the algorithm performs on inputs with fixed logical structure KB, and fixed χ, η, but varying weight parameters w and domainsizes n. Let Vw,n(i) denote the set of all numerical variables stored by the algorithm before performing step i, when it is run on inputs (w, n). Thus, Vw,n(i) comprises the original weight parameters of the model, as well as computed intermediate results, etc. We now make two basic assumptions on the algorithm:\n(A1) The weight parameters w only influence the numerical values of the variables\nstored in Vw,n(i), but not the sequence of execution steps performed by the algorithm. In particular, the number of execution steps performed by the algorithm only depends on n: L = L(n).\n(A2) The basic operations performed on numerical variables are polynomial time\nin the size of their arguments, and they produce an output whose size is linear in the size of the inputs. This is the case for the basic arithmetic operations addition and multiplication, for example.\nThe total representation size of Vw,n(i) then is bounded by cn(i)l(w), where cn(i) is a coefficient not depending on w. Also, let q() be a polynomial that provides a common complexity bound for the basic numerical operations that can be performed at one step. The total execution time of the algorithm on input (w, n) then is bounded by\nL(n)∑\ni=1\nq(cn(i)l(w)). (11)\nIf, now, for fixed weight vectors w the algorithm is polynomial in n (equivalently: the algorithm is polynomial in n under a computation model where basic numeric operations are constant time), then L(n) and maxi=1,...,L(n) cn(i) must be polynomially bounded in n. The combined complexity (11) then, in fact, is polynomial both in n and l(w).\nIn summary, this shows: an algorithm that for fixedw is polynomial in n, and that satisfies assumptions (A1) and (A2), actually is polynomial in n and l(w). Thus, for this type of algorithm, the additional restriction of Theorem 4.3 compared to Theorem 4.1 is insignificant.\nThe remaining question, then, is how restrictive or realistic assumptions (A1) and (A2) actually are. For exact inference algorithms it appears that (A1) and (A2) are satisfied by all existing approaches, with a small qualification: algorithms might give special treatment to special weight parameters, such as w = 0 or w = ∞, which then can lead to a violation of (A1) in the strict sense. However, our analysis could also be performed based on a weakened form of (A1) that allows certain special weights to influence the computation differently from proper numerical weights\n0 < w < ∞. A slightly more elaborate argument would then arrive at essentially the same conclusions.\nThe situation is less clear for approximate inference algorithms. Here the numerical values stored in Vw,n(i) may influence the algorithm in multiple ways: for example, they can be used to test a termination condition, or to decided which computations to perform next in order to improve approximation bounds derived so far. In all such cases, the model weights w can have an impact on the sequence and the total number of execution steps, and (A1) is not satisfied. Thus, even though the theorem also applies to approximate inference, its implications for the construction of approximate inference algorithms may be less severe, since there might be reasonable ways to build approximate inference algorithms that are polynomial in n, without also being polynomial in l(w)."
    }, {
      "heading" : "4.4 The 0-RFOL 6= Case",
      "text" : "In a final strengthening of our results, we now move on to the fragment 0-RFOL6=. The availability of the equality predicate for the formulas of KB, so far, has been an important prerequisite for our arguments, because Theorem 3.3 crucially depends on equality: spectra for formulas φ ∈ RFOL6= are always of the form N \\ {1, . . . , k} for some k, and, thus, decidable in constant time. For this reason it was suggested in (Jaeger 2000) that one should focus on logical fragments without equality when looking for model classes for which lifted inference scales polynomially in the domainsize. As our final result shows, however, elimination of equality may not have such a large impact on complexity, after all.\nTheorem 4.4\nIf NETIME 6= ETIME, then there does not exist an algorithm that 0.2-approximately solves PI(0-RFOL6=,AT , ∅) in time polynomial both in the domainsize, and the representation size of the weight parameters.\nThis theorem is a generalization of Theorem 4.3, and, strictly speaking, makes 4.3 redundant. It is only for expository purposes, and greater transparency in the proof arguments, that we here develop these results in two steps.\nThe proof of Theorem 4.4 is a refinement of the proof of Theorem 4.3. In addition to approximating Skolem functions f with relations Rf , we now also approximate the equality predicate = with a binary relation E(·, ·). Similarly as we could not impose in 0-RFOL hard constraints that ensure that Rf encodes a function, we also cannot constrain models to always interpret E as the equality relation. However, just as with (8) and (10) we rewarded interpretations with functional Rf , we can penalize interpretations in which E is not true equality by means of the two weighted formulas\na() ∧ ¬E(x, x) : 0 (12)\na() ∧ E(x, y) : 1/w (13)\nwhere w is a large weight."
    }, {
      "heading" : "5 Approximate Inference , Convergence, and Evidence",
      "text" : "There are some notable differences with respect to approximate inference between the results we here obtained for weighted model counting, and the results of (Jaeger 2000). In (Jaeger 2000) it was shown that due to convergence of query probabilities Pn(a()) as n → ∞, in theory a trivial constant time approximation algorithm exists: perform exact inference for all input domains up to a size n∗, and output the limit probability for all domains of size > n∗. This “algorithm”, however, has no practical use, since for a desired accuracy value ǫ one first would have to determine a sufficiently high threshold value n∗ ∈ N to make the output indeed be an ǫ-approximation.\nNevertheless, the difference between the existence of an impractical approximation algorithm on the one hand, and the non-existence of any approximation algorithm on the other hand, is just one consequence of a more fundamental difference: while in the models considered in (Jaeger 2000) query probabilities Pn(a()) converge to a limit, this is not necessarily the case for knowledge bases of weighted formulas – at least when full RFOL is allowed: in the proof of Theorem 4.1 we have constructed knowledge bases KB, such that PKB,n(a()) oscillates between zero and values > 1/2 as n oscillates between spec(φ) and its complement. The construction of knowledge bases with this behavior does not require formulas φ with a non-polynomial spectrum as in Corollary 3.4, and is not contingent on NETIME 6= ETIME. Already a knowledge base as constructed in the proof of Theorem 4.1 with φ replaced by ψ of Example 3.2 will show this behavior.\nThe reason behind these different convergence properties lies in a somewhat different role that conditioning on evidence plays in directed and undirected models: in the former, a conditional probability PM,n(a() | b()) defined by a model M can, in general, not be defined as an unconditional probability PM ′,n(a()) in a modified model M ′. As a result, the convergence guarantees and – theoretical – approximability for certain classes of unconditional queries PM,n(a()), do not carry over to conditional queries PM,n(a() | b()).\nFor weighted feature knowledge bases KB, on the other hand, there is no fundamental difference between unconditional and conditional queries PKB′,n(a()) and PKB,n(a() | b()), respectively. To reduce the conditional to unconditional queries, one can just add to KB the hard constraint ¬b() : 0 to obtain KB′ with PPKB′,n = PKB,n | b(). This means that as long as E is not more expressive than KB, the problem classes PI(KB,Q, E) and PI(KB,Q, ∅) have the same characteristics in terms of complexity as a function of the domainsize. Note, though, that this is only true when we consider complexity of PI(KB, n, χ, η) strictly as a function of n for fixed KB, χ, η. If the evidence is allowed to change with the domainsize, i.e., η = η(n), then even in cases where restrictions on KB make PI(KB,Q, E) polynomial in n, one can define sequences of inference problems PI(KB, n, χ, η(n)) with KB ∈ KB, η(n) ∈ E that are no longer polynomial in n (Van den Broeck and Davis 2012)."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have shown that for currently quite popular probabilistic-logic models consisting of collections of weighted, quantifier- and function-free formulas there is likely to be no general polynomial lifted inference method (contingent on NETIME 6= ETIME). Somewhat surprisingly, this even holds for approximate inference. Between this negative result, and the positive result of (Van den Broeck 2011), there still could be a lot of room for identifying tractable fragments by restricting 0-RFOL further via limits on the number of variables, or the richness of the signature S."
    }, {
      "heading" : "Appendix A Proofs",
      "text" : "Proof of Proposition 4.2:\nWe begin by defining the term-depth of a term t in the signature SF as the maximal nesting depth of function symbols in t. Precisely, we define inductively: if t ≡ x, then t has term depth 0. If t ≡ f() (a constant), or t = f(x1, . . . , xk) (a function term with only variables as arguments), then t has term depth 1. If t = f(t1, . . . , tk), then the term depth of t is one plus the maximal term depth of the ti.\nThe term depth of a formula φ(x) is the maximal term depth of the terms it\ncontains.\nWe now show that every formula φ(x) of term depth l can be transformed into a formula φl−1(x, z) of term depth l − 1 in 0-FOL(S ∪ SF ∪ S+), such that the statement for φ+ of the proposition holds for φl−1 (but with S∪SF ∪S+ instead of S∪S+ in ii). The proposition then follows by defining φ+ as the result of iteratively applying l such transformations to φ. Since the term depth of the resulting φ+ is zero, then actually φ+(x, z) ∈0-RFOL(S ∪ S+).\nLet {fi(xi) | i = 1, . . . , r} be the set of all distinct terms (including sub-terms) of depth 1 appearing in φ(x). Let z1, . . . , zr be new variables. Define φ l−1(x, z) as\nr∧\ni=1\nRfi(xi, zi) → φ(x)[z1/f1(x1), . . . , zr/fr(xr)]\nTo now show i⇒ii let I ∈ I(n, S∪SF ) with I |= ∀xφ(x). Define I+ ∈ I(n, S∪SF∪ S+) as the expansion of I in which each Rf ∈ S+ is interpreted as the relational representation of f , i.e., I+ |= Rf (d, e) iff I |= f(d) = e. Clearly, I+ |= Func. Furthermore, the following are equivalent:\nI |= ∀xφ(x) I |= ∀xz ∧r\ni=1 fi(xi) = zi → φ(x)[z1/f1(x1), . . . , zr/fr(xr)]\nI+ |= ∀xz ∧r\ni=1 R fi(xi, zi)\n→ φ(x)[z1/f1(x1), . . . , zr/fr(xr)]\nFor ii⇒i let I+ as in ii be given. Since I+ |= Func, we can turn I+ into an interpretation for S∪SF by defining f(d) as the unique e for which Rf(d, e) holds in I+. Then, by the same equivalences as above, I+ |= ∀xz φ+(x, z) implies I |= ∀xφ(x).\nProof of Theorem 4.3: Let φ ∈RFOL as given by Corollary 3.4, and ∀x φR-Skol(x) its relational Skolemization. Let S be the original signature of φ, and S+ the relation symbols introduced in the relational Skolemization. Furthermore, for each k-ary R+ ∈ S+ we introduce a new (k− 1)-ary relation R++. These new symbols will be used to calibrate the weight of models for the reference proposition b(). Note that the arity of symbols in S+ is at least 1, and R++, thus, is well-defined, but may contain relations of arity 0. We denote with S++ the collection of all the introduced R++ symbols. We now reduce the spectrum recognition problem for φ to probabilistic inference from a knowledge base in the signature S ∪ S+ ∪ S++ ∪ {a(), b()}.\nThe first formula in our knowledge base is\na() ∧ ¬φR-Skol(x) : 0 (A1)\nWe now approximately axiomatize the functional nature of the symbols R+ ∈ S+.\nThe sentence (8) can be directly encoded as a weighted formula:\nR+(x, y) ∧R+(x, y′) ∧ y 6= y′ : 0 (A2)\nNext, we would like to enforce (9) by means of a weighted formula. However, (9) encodes the essence of the existential quantifiers we are about to eliminate, and, thus, it is not surprising that this is not possible to enforce strictly. However, we can reward models in which the existential quantification of (9) is satisfied via the weighted formulas\na() ∧R+(x, y) : w (R+ ∈ S+) (A3)\nwhere w > 1 is a weight whose exact value is to be defined later.\nWe now proceed with constraining models of the reference proposition b(). First,\nall symbols in S ∪ S+ shall have an empty interpretations in models of b():\nb() ∧R(x) : 0 (R ∈ S) (A4)\nb() ∧R+(x, y) : 0 (R+ ∈ S+) (A5)\nIn order to allow b()-models to gain some weight, we use the extra symbols in\nS++:\nb() ∧R++(x) : w (R++ ∈ S++) (A6)\nwhere w is the same weight as in (A3). To further limit the possible interpretations of b()-models, we also stipulate:\nb() ∧ ¬R++(x) : 0 (R++ ∈ S++) (A7)\nThe extra symbols R++ must have empty interpretations in a()-models:\na() ∧R++(x) : 0 (R++ ∈ S++) (A8)\nFinally, we add:\n¬(a() ∨ b()) : 0 (A9)\nWe now determine (approximately) WKB,n(a()) and WKB,n(b()) for the cases\nn ∈ spec(φ) and n 6∈ spec(φ).\nFirst, consider b(): for any n, there exists exactly one interpretation Ib() ∈ I(Dn, S ∪ S+ ∪ S++ ∪ {a(), b()}) with nonzero weight in which b() is true. This is the interpretation in which all relations in S ∪ S+ are empty ((A4),(A5)), all relations in S++ are maximal (A7), and, in consequence of the latter, because of (A8), a() is false.\nAssume that S+ = {R+1 , . . . , R + m}, where R + i has arity ki +1. Then R ++ i ∈ S ++\ncontributes via (A6) a factor of wn ki\nto WKB,n(Ib()), and the total weight is:\nWKB,n(Ib()) = WKB,n(b()) = w nk1+···+nkm = wK(n), (A10)\nusing for abbreviation K(n) := nk1 + · · ·+ nkm .\nWe next turn to WKB,n(a()) in the case n ∈ spec(φ). Then there exists at least one interpretation I ∈ I(Dn, S ∪ S+), in which ∀xφR-Skol(x) is true, and in which the relations from S+ have a functional interpretation. We can expand this interpretation to an interpretation in I(n, S ∪S+ ∪S++ ∪{a(), b()}) by giving all relations in S++ an empty interpretation, and setting a() to true and b() to false. Then I does not violate any hard constraint in KB, and collects from (A3) a total weight of wK(n). Thus\nWKB,n(a()) ≥ w K(n),\nand therefore, when n ∈ spec(φ)\nPKB,n(a()) ≥ WKB,n(a())/(WKB,n(a()) +WKB,n(b())) ≥ 1/2. (A11)\nFinally, we have to consider WKB,n(a()) in the case n 6∈ spec(φ). For any I with nonzero weight in which a() is true, because of (A1), also ∀xφR-Skol(x) must be true. This, now, only is possible when some R+ ∈ S+ is not a functional relation, which, because of (A2) can only mean that for some x there exists no y with R+(x, y). The total weight of I accrued from (A3) then is at most wK(n)−1. Because of (A8), I cannot obtain any additional weight from (A6), so that\nWKB,n(I) ≤ w K(n)−1. (A12)\nThe total number of interpretations in I(Dn, S ∪S+ ∪S++ ∪{a(), b()}) is 2L(n) for a polynomial L(n). Thus\nWKB,n(a()) ≤ 2 L(n)wK(n)−1. (A13)\nWe now obtain for the case n 6∈ spec(φ)\nPKB,n(a()) ≤ WKB,n(a())/WKB,n(b()) ≤ 2 L(n)wK(n)−1/wK(n) = 2L(n)/w. (A14)\nSetting w = 10 ·2L(n), we thus have PKB,n(a()) ≤ 1/10 if n 6∈ spec(φ). The representation size of w is polynomial in n. Thus, an algorithm that computes PKB,n(a()) up to an accuracy of 0.2 = (0.5− 0.1)/2 in time polynomial in n and the represen-\ntation size of w would give a polynomial time decision procedure for spec(φ).\nProof of Theorem 4.4: The proof is an extension of the proof of Theorem 4.3, and we here just give the necessary modifications.\nLet E be a new binary relation symbol. We replace equalities x = y in (A1) and (A2) with E(x, y). To (approximately) axiomatize E as the identity relation in models of a(), we add to the knowledge base consisting of (A1)-(A9) the weighted formulas\na() ∧ ¬E(x, x) 0 (A15)\na() ∧ E(x, y) 1/w (A16)\nwhere w > 1 is the same weight as in (A3) and (A6), and whose exact value is to be determined later. To calibrate the weight of b()-models, we introduce in analogy to the R++ relations a unary relation E++, and in analogy to (A6) - (A8) add to the knowledge base\nb() ∧ E++(x) 1/w (A17)\nb() ∧ ¬E++(x) 0 (A18)\na() ∧ E++(x) 0 (A19)\nWe now obtain for all n\nWKB,n(b()) = w K(n)(1/w)n = wK(n)−n. (A20)\nIf n ∈ spec(φ), then there exists an interpretation in which a() is true, the R+ have a functional interpretation, and the interpretation of E is the identity relation. We can thus lower-bound the weight of a() by the weight of that interpretation:\nWKB,n(a()) ≥ w K(n)(1/w)n = wK(n)−n. (A21)\nAs in (A11), one then obtains PKB,n(a()) ≥ 1/2.\nWe now turn to the case n 6∈ spec(φ). Consider any I in which a() is true, and that has nonzero weight. This now, only is possible when in I there is an R+ ∈ S+ which is not a functional relation, or when E is not the identity relation in I (or both). In all cases, the weight of I coming from (A3) and (A16) is at most wK(n)−n−1. The total number of interpretations in I(Dn, S ∪ S+ ∪ S++ ∪ {a(), b(), E}) is 2M(n) for a polynomial M(n). Thus\nWKB,n(a()) ≤ 2 M(n)wK(n)−n−1, (A22)\nfrom which, as in (A14), then PKB,n(a()) ≤ 2M(n)/w. Now setting w = 10 · 2M(n) again yields the bound PKB,n(a()) ≤ 1/10."
    } ],
    "references" : [ {
      "title" : "Extended lifted inference with joint formulas",
      "author" : [ "U. Apsel", "R.I. Brafman" ],
      "venue" : "In",
      "citeRegEx" : "Apsel and Brafman,? 2011",
      "shortCiteRegEx" : "Apsel and Brafman",
      "year" : 2011
    }, {
      "title" : "Construction of belief and decision networks",
      "author" : [ "J.S. Breese" ],
      "venue" : "Computational Intelligence 8, 4, 624–647.",
      "citeRegEx" : "Breese,? 1992",
      "shortCiteRegEx" : "Breese",
      "year" : 1992
    }, {
      "title" : "Lifted first-order probabilistic inference",
      "author" : [ "R. de Salvo Braz", "E. Amir", "D. Roth" ],
      "venue" : "In Proceedings of the 19th International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "Braz et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Braz et al\\.",
      "year" : 2005
    }, {
      "title" : "A tractable first-order probabilistic logic",
      "author" : [ "P. Domingos", "W.A. Webb" ],
      "venue" : "Proc. of AAAI-12. To appear.",
      "citeRegEx" : "Domingos and Webb,? 2012",
      "shortCiteRegEx" : "Domingos and Webb",
      "year" : 2012
    }, {
      "title" : "Probabilities on finite models",
      "author" : [ "R. Fagin" ],
      "venue" : "Journal of Symbolic Logic 41, 1, 50–58.",
      "citeRegEx" : "Fagin,? 1976",
      "shortCiteRegEx" : "Fagin",
      "year" : 1976
    }, {
      "title" : "Inference in probabilistic logic programs using weighted cnf’s",
      "author" : [ "D. Fierens", "G.V. den Broeck", "I. Thon", "B. Gutmann", "L.D. Raedt" ],
      "venue" : "In Proc. of UAI",
      "citeRegEx" : "Fierens et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Fierens et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning probabilistic relational models",
      "author" : [ "N. Friedman", "L. Getoor", "D. Koller", "A. Pfeffer" ],
      "venue" : "Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI-99).",
      "citeRegEx" : "Friedman et al\\.,? 1999",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 1999
    }, {
      "title" : "Probabilistic theorem proving",
      "author" : [ "V. Gogate", "P. Domingos" ],
      "venue" : "Proceedings of the 27th Conference of Uncertainty in Artificial Intelligence (UAI-11).",
      "citeRegEx" : "Gogate and Domingos,? 2011",
      "shortCiteRegEx" : "Gogate and Domingos",
      "year" : 2011
    }, {
      "title" : "Relational bayesian networks",
      "author" : [ "M. Jaeger" ],
      "venue" : "Proceedings of the 13th Conference of Uncertainty in Artificial Intelligence (UAI-13), D. Geiger and P. P. Shenoy, Eds. Morgan Kaufmann, Providence, USA, 266–273.",
      "citeRegEx" : "Jaeger,? 1997",
      "shortCiteRegEx" : "Jaeger",
      "year" : 1997
    }, {
      "title" : "On the complexity of inference about probabilistic relational models",
      "author" : [ "M. Jaeger" ],
      "venue" : "Artificial Intelligence 117, 297–308.",
      "citeRegEx" : "Jaeger,? 2000",
      "shortCiteRegEx" : "Jaeger",
      "year" : 2000
    }, {
      "title" : "Liftability of probabilistic inference: Upper and lower bounds",
      "author" : [ "M. Jaeger", "G. Van den Broeck" ],
      "venue" : "Proceedings of the 2nd International Workshop on Statistical Relational AI.",
      "citeRegEx" : "Jaeger and Broeck,? 2012",
      "shortCiteRegEx" : "Jaeger and Broeck",
      "year" : 2012
    }, {
      "title" : "Lifted inference seen from the other side: The tractable features",
      "author" : [ "A. Jha", "V. Gogate", "A. Meliou", "D. Suciu" ],
      "venue" : "Proc. of NIPS.",
      "citeRegEx" : "Jha et al\\.,? 2010",
      "shortCiteRegEx" : "Jha et al\\.",
      "year" : 2010
    }, {
      "title" : "A catalog of complexity classes",
      "author" : [ "D.S. Johnson" ],
      "venue" : "Handbook of Theoretical Computer Science, J. van Leeuwen, Ed. Vol. 1. Elsevier, Amsterdam, 67–161.",
      "citeRegEx" : "Johnson,? 1990",
      "shortCiteRegEx" : "Johnson",
      "year" : 1990
    }, {
      "title" : "Turing machines and the spectra of first-order formulas with equality",
      "author" : [ "N.D. Jones", "A.L. Selman" ],
      "venue" : "Proceedings of the Fourth ACM Symposium on Theory of Computing. 157–167.",
      "citeRegEx" : "Jones and Selman,? 1972",
      "shortCiteRegEx" : "Jones and Selman",
      "year" : 1972
    }, {
      "title" : "Towards combining inductive logic programming with bayesian networks",
      "author" : [ "K. Kersting", "L.D. Raedt" ],
      "venue" : "Proceedings of the 11th International Conference on Inductive Logic Programming (ILP-01). LNAI, vol. 2157. 118–131.",
      "citeRegEx" : "Kersting and Raedt,? 2001",
      "shortCiteRegEx" : "Kersting and Raedt",
      "year" : 2001
    }, {
      "title" : "Lifted aggregation in directed first-order probabilistic models",
      "author" : [ "J. Kisyński", "D. Poole" ],
      "venue" : "Proc. of IJCAI 2009.",
      "citeRegEx" : "Kisyński and Poole,? 2009",
      "shortCiteRegEx" : "Kisyński and Poole",
      "year" : 2009
    }, {
      "title" : "Blog: Probabilistic logic with unknown objects",
      "author" : [ "B. Milch", "B. Marthi", "S. Russell", "D. Sontag", "D. Ong", "A. Kolobov" ],
      "venue" : "Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI-05). 1352–1359.",
      "citeRegEx" : "Milch et al\\.,? 2005",
      "shortCiteRegEx" : "Milch et al\\.",
      "year" : 2005
    }, {
      "title" : "Lifted probabilistic inference with counting formulas",
      "author" : [ "B. Milch", "L.S. Zettlemoyer", "K. Kersting", "M. Haimes", "L.P. Kaelbling" ],
      "venue" : "Proceedings of the 23rd AAAI Conference on Artificial Intelligence (AAAI-08).",
      "citeRegEx" : "Milch et al\\.,? 2008",
      "shortCiteRegEx" : "Milch et al\\.",
      "year" : 2008
    }, {
      "title" : "A theoretical framework for contextsensitive temporal probability model construction with application to plan projection",
      "author" : [ "L. Ngo", "P. Haddawy", "J. Helwig" ],
      "venue" : "Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence. 419– 426.",
      "citeRegEx" : "Ngo et al\\.,? 1995",
      "shortCiteRegEx" : "Ngo et al\\.",
      "year" : 1995
    }, {
      "title" : "Probabilistic horn abduction and Bayesian networks",
      "author" : [ "D. Poole" ],
      "venue" : "Artificial Intelligence 64, 81–129.",
      "citeRegEx" : "Poole,? 1993",
      "shortCiteRegEx" : "Poole",
      "year" : 1993
    }, {
      "title" : "First-order probabilistic inference",
      "author" : [ "D. Poole" ],
      "venue" : "Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI-03).",
      "citeRegEx" : "Poole,? 2003",
      "shortCiteRegEx" : "Poole",
      "year" : 2003
    }, {
      "title" : "Markov logic networks",
      "author" : [ "M. Richardson", "P. Domingos" ],
      "venue" : "Machine Learning 62, 1-2, 107 – 136.",
      "citeRegEx" : "Richardson and Domingos,? 2006",
      "shortCiteRegEx" : "Richardson and Domingos",
      "year" : 2006
    }, {
      "title" : "A statistical learning method for logic programs with distribution semantics",
      "author" : [ "T. Sato" ],
      "venue" : "Proceedings of the 12th International Conference on Logic Programming (ICLP’95). 715–729.",
      "citeRegEx" : "Sato,? 1995",
      "shortCiteRegEx" : "Sato",
      "year" : 1995
    }, {
      "title" : "Approximate lifted belief propagation",
      "author" : [ "P. Singla", "A. Nath", "P. Domingos" ],
      "venue" : "Proc. of AAAI-10 Workshop on Statistical Relational AI.",
      "citeRegEx" : "Singla et al\\.,? 2010",
      "shortCiteRegEx" : "Singla et al\\.",
      "year" : 2010
    }, {
      "title" : "Discriminative probabilistic models for relational data",
      "author" : [ "B. Taskar", "P. Abbeel", "D. Koller" ],
      "venue" : "Proc. of UAI 2002.",
      "citeRegEx" : "Taskar et al\\.,? 2002",
      "shortCiteRegEx" : "Taskar et al\\.",
      "year" : 2002
    }, {
      "title" : "On the completeness of first-order knowledge compilation for lifted probabilistic inference",
      "author" : [ "G. Van den Broeck" ],
      "venue" : "Proc. of the 25th Annual Conf. on Neural Information Processing Systems (NIPS).",
      "citeRegEx" : "Broeck,? 2011",
      "shortCiteRegEx" : "Broeck",
      "year" : 2011
    }, {
      "title" : "Conditioning in first-order knowledge compilation and lifted probabilistic inference",
      "author" : [ "G. Van den Broeck", "J. Davis" ],
      "venue" : "Proceedings of the Twenty-Sixth AAAI Conference on Articial Intelligence. 1961–1967.",
      "citeRegEx" : "Broeck and Davis,? 2012",
      "shortCiteRegEx" : "Broeck and Davis",
      "year" : 2012
    }, {
      "title" : "Lifted probabilistic inference by first-order knowledge compilation",
      "author" : [ "G. Van den Broeck", "N. Taghipour", "W. Meert", "J. Davis", "L.D. Raedt" ],
      "venue" : "Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI-11).",
      "citeRegEx" : "Broeck et al\\.,? 2011",
      "shortCiteRegEx" : "Broeck et al\\.",
      "year" : 2011
    }, {
      "title" : "Representing causal information about a probabilistic process",
      "author" : [ "J. Vennekens", "M. Denecker", "M. Bruynooghe" ],
      "venue" : "Logics in Artificial Intelligence, 10th European Conference, JELIA 2006, Proceedings. Lecture Notes in Computer Science, vol. 4160. Springer, 452–464.",
      "citeRegEx" : "Vennekens et al\\.,? 2006",
      "shortCiteRegEx" : "Vennekens et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Lower complexity bounds that imply some limitations for the feasibility of lifted inference on more expressive model classes were established earlier in (Jaeger 2000).",
      "startOffset" : 153,
      "endOffset" : 166
    }, {
      "referenceID" : 1,
      "context" : "probabilistic or statistic relational models) provide high-level representation languages for probabilistic models of structured data (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006; Taskar et al. 2002; Richardson and Domingos 2006).",
      "startOffset" : 134,
      "endOffset" : 339
    }, {
      "referenceID" : 19,
      "context" : "probabilistic or statistic relational models) provide high-level representation languages for probabilistic models of structured data (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006; Taskar et al. 2002; Richardson and Domingos 2006).",
      "startOffset" : 134,
      "endOffset" : 339
    }, {
      "referenceID" : 22,
      "context" : "probabilistic or statistic relational models) provide high-level representation languages for probabilistic models of structured data (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006; Taskar et al. 2002; Richardson and Domingos 2006).",
      "startOffset" : 134,
      "endOffset" : 339
    }, {
      "referenceID" : 18,
      "context" : "probabilistic or statistic relational models) provide high-level representation languages for probabilistic models of structured data (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006; Taskar et al. 2002; Richardson and Domingos 2006).",
      "startOffset" : 134,
      "endOffset" : 339
    }, {
      "referenceID" : 8,
      "context" : "probabilistic or statistic relational models) provide high-level representation languages for probabilistic models of structured data (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006; Taskar et al. 2002; Richardson and Domingos 2006).",
      "startOffset" : 134,
      "endOffset" : 339
    }, {
      "referenceID" : 6,
      "context" : "probabilistic or statistic relational models) provide high-level representation languages for probabilistic models of structured data (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006; Taskar et al. 2002; Richardson and Domingos 2006).",
      "startOffset" : 134,
      "endOffset" : 339
    }, {
      "referenceID" : 14,
      "context" : "probabilistic or statistic relational models) provide high-level representation languages for probabilistic models of structured data (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006; Taskar et al. 2002; Richardson and Domingos 2006).",
      "startOffset" : 134,
      "endOffset" : 339
    }, {
      "referenceID" : 16,
      "context" : "probabilistic or statistic relational models) provide high-level representation languages for probabilistic models of structured data (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006; Taskar et al. 2002; Richardson and Domingos 2006).",
      "startOffset" : 134,
      "endOffset" : 339
    }, {
      "referenceID" : 28,
      "context" : "probabilistic or statistic relational models) provide high-level representation languages for probabilistic models of structured data (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006; Taskar et al. 2002; Richardson and Domingos 2006).",
      "startOffset" : 134,
      "endOffset" : 339
    }, {
      "referenceID" : 24,
      "context" : "probabilistic or statistic relational models) provide high-level representation languages for probabilistic models of structured data (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006; Taskar et al. 2002; Richardson and Domingos 2006).",
      "startOffset" : 134,
      "endOffset" : 339
    }, {
      "referenceID" : 21,
      "context" : "probabilistic or statistic relational models) provide high-level representation languages for probabilistic models of structured data (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006; Taskar et al. 2002; Richardson and Domingos 2006).",
      "startOffset" : 134,
      "endOffset" : 339
    }, {
      "referenceID" : 8,
      "context" : "This mismatch between model specification and inference methods has been noted early on (Jaeger 1997), and has given rise to numerous proposals for inference techniques that operate at the high level of the underlying model specifications (Poole 2003; de Salvo Braz et al.",
      "startOffset" : 88,
      "endOffset" : 101
    }, {
      "referenceID" : 20,
      "context" : "This mismatch between model specification and inference methods has been noted early on (Jaeger 1997), and has given rise to numerous proposals for inference techniques that operate at the high level of the underlying model specifications (Poole 2003; de Salvo Braz et al. 2005; Milch et al. 2008; Kisyński and Poole 2009; Jha et al. 2010; Gogate and Domingos 2011; Van den Broeck et al. 2011; Van den Broeck 2011; Fierens et al. 2011).",
      "startOffset" : 239,
      "endOffset" : 435
    }, {
      "referenceID" : 17,
      "context" : "This mismatch between model specification and inference methods has been noted early on (Jaeger 1997), and has given rise to numerous proposals for inference techniques that operate at the high level of the underlying model specifications (Poole 2003; de Salvo Braz et al. 2005; Milch et al. 2008; Kisyński and Poole 2009; Jha et al. 2010; Gogate and Domingos 2011; Van den Broeck et al. 2011; Van den Broeck 2011; Fierens et al. 2011).",
      "startOffset" : 239,
      "endOffset" : 435
    }, {
      "referenceID" : 15,
      "context" : "This mismatch between model specification and inference methods has been noted early on (Jaeger 1997), and has given rise to numerous proposals for inference techniques that operate at the high level of the underlying model specifications (Poole 2003; de Salvo Braz et al. 2005; Milch et al. 2008; Kisyński and Poole 2009; Jha et al. 2010; Gogate and Domingos 2011; Van den Broeck et al. 2011; Van den Broeck 2011; Fierens et al. 2011).",
      "startOffset" : 239,
      "endOffset" : 435
    }, {
      "referenceID" : 11,
      "context" : "This mismatch between model specification and inference methods has been noted early on (Jaeger 1997), and has given rise to numerous proposals for inference techniques that operate at the high level of the underlying model specifications (Poole 2003; de Salvo Braz et al. 2005; Milch et al. 2008; Kisyński and Poole 2009; Jha et al. 2010; Gogate and Domingos 2011; Van den Broeck et al. 2011; Van den Broeck 2011; Fierens et al. 2011).",
      "startOffset" : 239,
      "endOffset" : 435
    }, {
      "referenceID" : 7,
      "context" : "This mismatch between model specification and inference methods has been noted early on (Jaeger 1997), and has given rise to numerous proposals for inference techniques that operate at the high level of the underlying model specifications (Poole 2003; de Salvo Braz et al. 2005; Milch et al. 2008; Kisyński and Poole 2009; Jha et al. 2010; Gogate and Domingos 2011; Van den Broeck et al. 2011; Van den Broeck 2011; Fierens et al. 2011).",
      "startOffset" : 239,
      "endOffset" : 435
    }, {
      "referenceID" : 5,
      "context" : "This mismatch between model specification and inference methods has been noted early on (Jaeger 1997), and has given rise to numerous proposals for inference techniques that operate at the high level of the underlying model specifications (Poole 2003; de Salvo Braz et al. 2005; Milch et al. 2008; Kisyński and Poole 2009; Jha et al. 2010; Gogate and Domingos 2011; Van den Broeck et al. 2011; Van den Broeck 2011; Fierens et al. 2011).",
      "startOffset" : 239,
      "endOffset" : 435
    }, {
      "referenceID" : 0,
      "context" : "2005); “The act of exploiting the high level structure in relational models is called lifted inference” (Apsel and Brafman 2011); “The idea behind lifted inference is to carry out as much inference as possible without propositionalizing (Kisyński and Poole 2009); “lifted inference, which deals with groups of indistinguishable variables, rather than individual ground atoms (Singla et al.",
      "startOffset" : 104,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "2005); “The act of exploiting the high level structure in relational models is called lifted inference” (Apsel and Brafman 2011); “The idea behind lifted inference is to carry out as much inference as possible without propositionalizing (Kisyński and Poole 2009); “lifted inference, which deals with groups of indistinguishable variables, rather than individual ground atoms (Singla et al.",
      "startOffset" : 237,
      "endOffset" : 262
    }, {
      "referenceID" : 23,
      "context" : "2005); “The act of exploiting the high level structure in relational models is called lifted inference” (Apsel and Brafman 2011); “The idea behind lifted inference is to carry out as much inference as possible without propositionalizing (Kisyński and Poole 2009); “lifted inference, which deals with groups of indistinguishable variables, rather than individual ground atoms (Singla et al. 2010).",
      "startOffset" : 375,
      "endOffset" : 395
    }, {
      "referenceID" : 9,
      "context" : "On the other hand, (Jaeger 2000) has shown that under certain assumptions on the expressivity of the modeling language, probabilistic inference is not polyno-",
      "startOffset" : 19,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "2005); “The act of exploiting the high level structure in relational models is called lifted inference” (Apsel and Brafman 2011); “The idea behind lifted inference is to carry out as much inference as possible without propositionalizing (Kisyński and Poole 2009); “lifted inference, which deals with groups of indistinguishable variables, rather than individual ground atoms (Singla et al. 2010). While, thus, the term lifted inference emerges as a quite coherent algorithmic metaphor, it is not immediately obvious what its exact technical meaning should be. Since quite a variety of different algorithmic approaches are collected under the label “lifted”, and since most of them can degenerate for certain models to ground, or propositional, inference, it is difficult to precisely define the class of lifted inference techniques in terms of specific algorithmic techniques employed. A more fruitful approach is to make more precise the concept of lifted inference in terms of its objectives. Here one observes that lifted inference techniques very consistently are evaluated on, and compared against each other, by how well inference complexity scales as a function of the domain (or population) for which the general model is instantiated. Thus, empirical evaluations of lifted inference techniques are usually presented in the form of domainsize vs. inference time plots as shown in Figure 1. Van den Broeck (2011), therefore, has proposed a formal definition of domain lifted inference in terms of polynomial time complexity in the domainsize parameter.",
      "startOffset" : 105,
      "endOffset" : 1420
    }, {
      "referenceID" : 0,
      "context" : "2005); “The act of exploiting the high level structure in relational models is called lifted inference” (Apsel and Brafman 2011); “The idea behind lifted inference is to carry out as much inference as possible without propositionalizing (Kisyński and Poole 2009); “lifted inference, which deals with groups of indistinguishable variables, rather than individual ground atoms (Singla et al. 2010). While, thus, the term lifted inference emerges as a quite coherent algorithmic metaphor, it is not immediately obvious what its exact technical meaning should be. Since quite a variety of different algorithmic approaches are collected under the label “lifted”, and since most of them can degenerate for certain models to ground, or propositional, inference, it is difficult to precisely define the class of lifted inference techniques in terms of specific algorithmic techniques employed. A more fruitful approach is to make more precise the concept of lifted inference in terms of its objectives. Here one observes that lifted inference techniques very consistently are evaluated on, and compared against each other, by how well inference complexity scales as a function of the domain (or population) for which the general model is instantiated. Thus, empirical evaluations of lifted inference techniques are usually presented in the form of domainsize vs. inference time plots as shown in Figure 1. Van den Broeck (2011), therefore, has proposed a formal definition of domain lifted inference in terms of polynomial time complexity in the domainsize parameter. Experimental and theoretical analyses of existing lifted inference techniques then show that they provide domain lifted inference in some cases where basic propositional inference techniques would exhibit exponential complexity (as illustrated in Figure 1). However, until recently, these positive results were mostly limited to examples of individual models, and little was known about the feasibility of lifted inference for certain well-defined classes of models. First results that show the feasibility of lifted inference for whole classes of models are given by Van den Broeck (2011), and Domingos and Webb (2012).",
      "startOffset" : 105,
      "endOffset" : 2150
    }, {
      "referenceID" : 0,
      "context" : "2005); “The act of exploiting the high level structure in relational models is called lifted inference” (Apsel and Brafman 2011); “The idea behind lifted inference is to carry out as much inference as possible without propositionalizing (Kisyński and Poole 2009); “lifted inference, which deals with groups of indistinguishable variables, rather than individual ground atoms (Singla et al. 2010). While, thus, the term lifted inference emerges as a quite coherent algorithmic metaphor, it is not immediately obvious what its exact technical meaning should be. Since quite a variety of different algorithmic approaches are collected under the label “lifted”, and since most of them can degenerate for certain models to ground, or propositional, inference, it is difficult to precisely define the class of lifted inference techniques in terms of specific algorithmic techniques employed. A more fruitful approach is to make more precise the concept of lifted inference in terms of its objectives. Here one observes that lifted inference techniques very consistently are evaluated on, and compared against each other, by how well inference complexity scales as a function of the domain (or population) for which the general model is instantiated. Thus, empirical evaluations of lifted inference techniques are usually presented in the form of domainsize vs. inference time plots as shown in Figure 1. Van den Broeck (2011), therefore, has proposed a formal definition of domain lifted inference in terms of polynomial time complexity in the domainsize parameter. Experimental and theoretical analyses of existing lifted inference techniques then show that they provide domain lifted inference in some cases where basic propositional inference techniques would exhibit exponential complexity (as illustrated in Figure 1). However, until recently, these positive results were mostly limited to examples of individual models, and little was known about the feasibility of lifted inference for certain well-defined classes of models. First results that show the feasibility of lifted inference for whole classes of models are given by Van den Broeck (2011), and Domingos and Webb (2012). On the other hand, (Jaeger 2000) has shown that under certain assumptions on the expressivity of the modeling language, probabilistic inference is not polyno-",
      "startOffset" : 105,
      "endOffset" : 2180
    }, {
      "referenceID" : 9,
      "context" : "However, the results of (Jaeger 2000) are based on types of probabilistic logic models that are somewhat different from the models that presently receive the most attention: first, they essentially assume a directed modeling framework, in which the model represents a generative stochastic process for sampling relational structures.",
      "startOffset" : 24,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "While the majority of existing model classes fall into the category of directed models (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006), there is currently a lot of interest in undirected models that are given by a set of soft constraints on relational structures, specified in the form of potential functions, and in the ground case giving rise to undirected graphical models, i.",
      "startOffset" : 87,
      "endOffset" : 242
    }, {
      "referenceID" : 19,
      "context" : "While the majority of existing model classes fall into the category of directed models (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006), there is currently a lot of interest in undirected models that are given by a set of soft constraints on relational structures, specified in the form of potential functions, and in the ground case giving rise to undirected graphical models, i.",
      "startOffset" : 87,
      "endOffset" : 242
    }, {
      "referenceID" : 22,
      "context" : "While the majority of existing model classes fall into the category of directed models (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006), there is currently a lot of interest in undirected models that are given by a set of soft constraints on relational structures, specified in the form of potential functions, and in the ground case giving rise to undirected graphical models, i.",
      "startOffset" : 87,
      "endOffset" : 242
    }, {
      "referenceID" : 18,
      "context" : "While the majority of existing model classes fall into the category of directed models (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006), there is currently a lot of interest in undirected models that are given by a set of soft constraints on relational structures, specified in the form of potential functions, and in the ground case giving rise to undirected graphical models, i.",
      "startOffset" : 87,
      "endOffset" : 242
    }, {
      "referenceID" : 8,
      "context" : "While the majority of existing model classes fall into the category of directed models (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006), there is currently a lot of interest in undirected models that are given by a set of soft constraints on relational structures, specified in the form of potential functions, and in the ground case giving rise to undirected graphical models, i.",
      "startOffset" : 87,
      "endOffset" : 242
    }, {
      "referenceID" : 6,
      "context" : "While the majority of existing model classes fall into the category of directed models (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006), there is currently a lot of interest in undirected models that are given by a set of soft constraints on relational structures, specified in the form of potential functions, and in the ground case giving rise to undirected graphical models, i.",
      "startOffset" : 87,
      "endOffset" : 242
    }, {
      "referenceID" : 14,
      "context" : "While the majority of existing model classes fall into the category of directed models (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006), there is currently a lot of interest in undirected models that are given by a set of soft constraints on relational structures, specified in the form of potential functions, and in the ground case giving rise to undirected graphical models, i.",
      "startOffset" : 87,
      "endOffset" : 242
    }, {
      "referenceID" : 16,
      "context" : "While the majority of existing model classes fall into the category of directed models (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006), there is currently a lot of interest in undirected models that are given by a set of soft constraints on relational structures, specified in the form of potential functions, and in the ground case giving rise to undirected graphical models, i.",
      "startOffset" : 87,
      "endOffset" : 242
    }, {
      "referenceID" : 28,
      "context" : "While the majority of existing model classes fall into the category of directed models (Breese 1992; Poole 1993; Sato 1995; Ngo et al. 1995; Jaeger 1997; Friedman et al. 1999; Kersting and Raedt 2001; Milch et al. 2005; Vennekens et al. 2006), there is currently a lot of interest in undirected models that are given by a set of soft constraints on relational structures, specified in the form of potential functions, and in the ground case giving rise to undirected graphical models, i.",
      "startOffset" : 87,
      "endOffset" : 242
    }, {
      "referenceID" : 9,
      "context" : "Secondly, the results of (Jaeger 2000) require quite strong assumptions on the expressivity of the probabilistic-logic modeling language, which is required to allow that conditional distributions of atoms can be specified dependent on unrestricted first-order properties.",
      "startOffset" : 25,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "In this paper the general approach of (Jaeger 2000) is extended to obtain lower complexity bounds for inference in probabilistic-logic model classes that have emerged as the focus of interest for lifted inference techniques, i.",
      "startOffset" : 38,
      "endOffset" : 51
    }, {
      "referenceID" : 9,
      "context" : "In a sharp contrast with (Jaeger 2000), where a “trivial” constant-time approximate inference method was described, we show that our lower complexity bounds also hold for approximate inference.",
      "startOffset" : 25,
      "endOffset" : 38
    }, {
      "referenceID" : 9,
      "context" : "Further sharpening earlier results, we finally establish that the lower complexity bounds also hold for models not using the equality predicate, which in (Jaeger 2000) was conjectured to be the key source of inherent complexity.",
      "startOffset" : 154,
      "endOffset" : 167
    }, {
      "referenceID" : 21,
      "context" : "Similarly as (Richardson and Domingos 2006), (Van den Broeck et al.",
      "startOffset" : 13,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "2011) and (Gogate and Domingos 2011) we assume the following framework: a model, or knowl-",
      "startOffset" : 10,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "Weighted feature models in our sense can be seen as a slight generalization as weighted model counting (wmc) frameworks (Fierens et al. 2011; Gogate and Domingos 2011) in which non-zero weights are only associated with literals.",
      "startOffset" : 120,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : "Weighted feature models in our sense can be seen as a slight generalization as weighted model counting (wmc) frameworks (Fierens et al. 2011; Gogate and Domingos 2011) in which non-zero weights are only associated with literals.",
      "startOffset" : 120,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : ", Rn, hard constraints φi(vi) ↔ Ri(vi), and weighted formulas Ri(vi) : wi (Van den Broeck et al. 2011; Gogate and Domingos 2011).",
      "startOffset" : 74,
      "endOffset" : 128
    }, {
      "referenceID" : 21,
      "context" : "Markov Logic Networks (Richardson and Domingos 2006) also are based on knowledge bases of the form (1) allowing arbitrary formulas φi.",
      "startOffset" : 22,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "Note that these classes are distinct from the more commonly studied classes (N)EXPTIME, which are characterized by complexity bounds O(2 c ) (Johnson 1990).",
      "startOffset" : 141,
      "endOffset" : 155
    }, {
      "referenceID" : 9,
      "context" : "Like (Jaeger 2000), we use the following connection between spectra and NETIME as the key tool for our complexity analysis.",
      "startOffset" : 5,
      "endOffset" : 18
    }, {
      "referenceID" : 13,
      "context" : "3 (Jones and Selman 1972) A set A ⊆ N is in NETIME, iff A is the spectrum of a sentence φ ∈ RFOL.",
      "startOffset" : 2,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "This is rather straightforward, and (for exact inference) already implied by the results of (Jaeger 2000).",
      "startOffset" : 92,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "However, from the 0-1 laws of first-order logic (Fagin 1976), it follows that for our current KB : PKB,n(a()) →n→∞ 0.",
      "startOffset" : 48,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "It has previously been emphasized that lifted inference procedures should only be expected to be polynomial in the domain size, but not in other parameters that characterize the complexity of KB (Jaeger 2000; Van den Broeck 2011).",
      "startOffset" : 195,
      "endOffset" : 229
    }, {
      "referenceID" : 9,
      "context" : "For this reason it was suggested in (Jaeger 2000) that one should focus on logical fragments without equality when looking for model classes for which lifted inference scales polynomially in the domainsize.",
      "startOffset" : 36,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : "There are some notable differences with respect to approximate inference between the results we here obtained for weighted model counting, and the results of (Jaeger 2000).",
      "startOffset" : 158,
      "endOffset" : 171
    }, {
      "referenceID" : 9,
      "context" : "In (Jaeger 2000) it was shown that due to convergence of query probabilities Pn(a()) as n → ∞, in theory a trivial constant time approximation algorithm exists: perform exact inference for all input domains up to a size n, and output the limit probability for all domains of size > n.",
      "startOffset" : 3,
      "endOffset" : 16
    }, {
      "referenceID" : 9,
      "context" : "Nevertheless, the difference between the existence of an impractical approximation algorithm on the one hand, and the non-existence of any approximation algorithm on the other hand, is just one consequence of a more fundamental difference: while in the models considered in (Jaeger 2000) query probabilities Pn(a()) converge to a limit, this is not necessarily the case for knowledge bases of weighted formulas – at least when full RFOL is allowed: in the proof of Theorem 4.",
      "startOffset" : 274,
      "endOffset" : 287
    } ],
    "year" : 2013,
    "abstractText" : "One of the big challenges in the development of probabilistic relational (or probabilistic logical) modeling and learning frameworks is the design of inference techniques that operate on the level of the abstract model representation language, rather than on the level of ground, propositional instances of the model. Numerous approaches for such “lifted inference” techniques have been proposed. While it has been demonstrated that these techniques will lead to significantly more efficient inference on some specific models, there are only very recent and still quite restricted results that show the feasibility of lifted inference on certain syntactically defined classes of models. Lower complexity bounds that imply some limitations for the feasibility of lifted inference on more expressive model classes were established earlier in (Jaeger 2000). However, it is not immediate that these results also apply to the type of modeling languages that currently receive the most attention, i.e., weighted, quantifier-free formulas. In this paper we extend these earlier results, and show that under the assumption that NETIME6 =ETIME, there is no polynomial lifted inference algorithm for knowledge bases of weighted, quantifierand function-free formulas. Further strengthening earlier results, this is also shown to hold for approximate inference, and for knowledge bases not containing the equality predicate.",
    "creator" : "LaTeX with hyperref package"
  }
}