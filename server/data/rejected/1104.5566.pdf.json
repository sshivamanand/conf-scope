{
  "name" : "1104.5566.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Limits of Preprocessing∗",
    "authors" : [ "Stefan Szeider" ],
    "emails" : [ "stefan@szeider.net" ],
    "sections" : [ {
      "heading" : null,
      "text" : "combinatorial problems from various areas in AI. We consider problems from Constraint Satisfaction, Global Constraints, Satisfiability, Nonmonotonic and Bayesian Reasoning. We show that, subject to a complexity theoretic assumption, none of the considered problems can be reduced by polynomial-time preprocessing to a problem kernel whose size is polynomial in a structural problem parameter of the input, such as induced width or backdoor size. Our results provide a firm theoretical boundary for the performance of polynomial-time preprocessing algorithms for the considered problems.\nKeywords: Fixed-Parameter Tractability, Constraint Satisfaction, Global Constraints, Satisfiability, Bayesian Networks, Normal Logic Programs, Computational Complexity."
    }, {
      "heading" : "1 Introduction",
      "text" : "Many important computational problems that arise in various areas of AI are intractable. Nevertheless, AI research was very successful in developing and implementing heuristic solvers that work well on realworld instances. An important component of virtually every solver is a powerful polynomial-time preprocessing procedure that reduces the problem input. For instance, preprocessing techniques for the propositional satisfiability problem are based on Boolean Constraint Propagation (see, e.g., Eén and Biere, 2005), CSP solvers make use of various local consistency algorithms that filter the domains of variables (see, e.g., Bessière, 2006); similar preprocessing methods are used by solvers for Nonmonotonic and Bayesian reasoning problems (see, e.g., Gebser et al., 2008, Bolt and van der Gaag, 2006, respectively).\nUntil recently, no provable performance guarantees for polynomial-time preprocessing methods have been obtained, and so preprocessing was only subject of empirical studies. A possible reason for the lack of theoretical results is a certain inadequacy of the P vs NP framework for such an analysis: if we could reduce in polynomial time an instance of an NP-hard problem just by one bit, then we can solve the entire problem in polynomial time by repeating the reduction step a polynomial number of times, and P = NP follows.\nWith the advent of parameterized complexity (Downey, Fellows, and Stege, 1999), a new theoretical framework became available that provides suitable tools to analyze the power of preprocessing. Parameterized complexity considers a problem in a two-dimensional setting, where in addition to the input size n, a problem parameter k is taken into consideration. This parameter can encode a structural aspect of the problem instance. A problem is called fixed-parameter tractable (FPT) if it can be solved in time f(k)p(n) where f is a function of the parameter k and p is a polynomial of the input size n. Thus, for FPT problems, the combinatorial explosion can be confined to the parameter and is independent of the input size. It is known that a problem is fixed-parameter tractable if and only if every problem input can be reduced by polynomial-time preprocessing to an equivalent input whose size is bounded by a function of the parameter (Downey, Fellows, and Stege, 1999). The reduced instance is called the problem kernel, the preprocessing is called kernelization. The power of polynomial-time preprocessing can now be benchmarked in terms of the size of the kernel. Once a small kernel is obtained, we can apply any method\n∗Research funded by the ERC (COMPLEX REASON, Grand Reference 239962).\nar X\niv :1\n10 4.\n55 66\nv2 [\ncs .A\nI] 1\n1 A\nug 2\n01 1\nof choice to solve the kernel: brute-force search, heuristics, approximation, etc. (Guo and Niedermeier, 2007). Because of this flexibility a small kernel is generally preferable to a less flexible branching-based fixed-parameter algorithm. Thus, small kernels provide an additional value that goes beyond bare fixedparameter tractability.\nIn general the size of the kernel is exponential in the parameter, but many important NP-hard optimization problems such as Minimum Vertex Cover, parameterized by solution size, admit polynomial kernels, see, e.g., (Bodlaender et al., 2009) for references.\nIn previous research several NP-hard AI problems have been shown to be fixed-parameter tractable. We list some important examples from various areas:\n• Constraint satisfaction problems (CSP) over a fixed universe of values, parameterized by the induced width (Gottlob, Scarcello, and Sideri, 2002).\n• Consistency and generalized arc consistency for intractable global constraints, parameterized by the cardinalities of certain sets of values (Bessière et al., 2008).\n• Propositional satisfiability (SAT), parameterized by the size of backdoors (Nishimura, Ragde, and Szeider, 2004).\n• Positive inference in Bayesian networks with variables of bounded domain size, parameterized by size of loop cutsets (Pearl, 1988; Bidyuk and Dechter, 2007).\n• Nonmonotonic reasoning with normal logic programs, parameterized by feedback width (Gottlob, Scarcello, and Sideri, 2002).\nHowever, only exponential kernels are known for these fundamental AI problems. Can we hope for polynomial kernels?\nResults Our results are throughout negative. We provide strong theoretical evidence that none of the above fixed-parameter tractable AI problems admits a polynomial kernel. More specifically, we show that a polynomial kernel for any of these problems causes a collapse of the Polynomial Hierarchy to its third level, which is considered highly unlikely by complexity theorists.\nOur results are general: The kernel lower bounds are not limited to a particular preprocessing technique but apply to any clever technique that could be conceived in future research. Hence the results contribute to the foundations of AI.\nOur results suggest the investigation of alternative approaches to polynomial-time preprocessing; for instance, preprocessing that produces in polynomial time a Boolean combination of polynomially sized kernels instead of one single kernel."
    }, {
      "heading" : "2 Formal Background",
      "text" : "A parameterized problem P is a subset of Σ∗ × N for some finite alphabet Σ. For a problem instance (x, k) ∈ Σ∗ × N we call x the main part and k the parameter. We assume the parameter is represented in unary. For the parameterized problems considered in this paper, the parameter is a function of the main part, i.e., k = π(x) for a function π. We then denote the problem as P(π), e.g., U -CSP(width) denotes the problem U -CSP parameterized by the width of the given tree decomposition.\nA parameterized problem P is fixed-parameter tractable if there exists an algorithm that solves any input (x, k) ∈ Σ∗ × N in time O(f(k) · p(|x|) where f is an arbitrary computable function of k and p is a polynomial in n.\nA kernelization for a parameterized problem P ⊆ Σ∗×N is an algorithm that, given (x, k) ∈ Σ∗×N, outputs in time polynomial in |x| + k a pair (x′, k′) ∈ Σ∗ × N such that (i) (x, k) ∈ P if and only if (x′, k′) ∈ P and (ii) |x′| + k′ ≤ g(k), where g is an arbitrary computable function, called the size of the kernel. In particular, for constant k the kernel has constant size g(k). If g is a polynomial then we say that P admits a polynomial kernel.\nEvery fixed-parameter tractable problem admits a kernel. This can be seen by the following argument due to Downey, Fellows, and Stege (1999). Assume we can decide instances (x, k) of problem P in time f(k)|n|O(1). We kernelize an instance (x, k) as follows. If |x| ≤ f(k) then we already have a kernel of size f(k). Otherwise, if |x| > f(k), then f(k)|x|O(1) ≤ |x|O(1) is a polynomial; hence we can decide the instance in polynomial time and replace it with a small decision-equivalent instance (x′, k′). Thus we always have a kernel of size at most f(k). However, f(k) is super-polynomial for NP-hard problems (unless P = NP), hence this generic construction is not providing polynomial kernels.\nWe understand preprocessing for an NP-hard problem as a polynomial-time procedure that transforms an instance of the problem to a (possible smaller) solution-equivalent instance of the same problem. Kernelization is such a preprocessing with a performance guarantee, i.e., we are guaranteed that the preprocessing yields a kernel whose size is bounded in terms of the parameter of the given problem instance. In the literature also different forms of preprocessing have been considered. An important one is knowledge compilation, a two-phases approach to reasoning problems where in a first phase a given knowledge base is (possibly in exponential time) preprocessed (“compiled”), such that in a second phase various queries can be answered in polynomial time (Cadoli et al., 2002)."
    }, {
      "heading" : "3 Tools for Kernel Lower Bounds",
      "text" : "In the sequel we will use recently developed tools to obtain kernel lower bounds. Our kernel lower bounds are subject to the widely believed complexity theoretic assumption NP 6⊆ co-NP/poly (or equivalently, PH 6= Σ3p). In other words, the tools allow us to show that a parameterized problem does not admit a polynomial kernel unless the Polynomial Hierarchy collapses to its third level (see, e.g., Papadimitriou, 1994).\nA composition algorithm for a parameterized problem P ⊆ Σ∗ × N is an algorithm that receives as input a sequence (x1, k), . . . , (xt, k) ∈ Σ∗ × N, uses time polynomial in ∑t i=1 |xi| + k, and outputs (y, k′) ∈ Σ∗ × N with (i) (y, k′) ∈ P if and only if (xi, k) ∈ P for some 1 ≤ i ≤ t, and (ii) k′ is polynomial in k. A parameterized problem is compositional if it has a composition algorithm. With each parameterized problem P ⊆ Σ∗ × N we associate a classical problem\nUP[P] = {x#1k : (x, k) ∈ P }\nwhere 1 denotes an arbitrary symbol from Σ and # is a new symbol not in Σ. We call UP[P] the unparameterized version of P.\nThe following result is the basis for our kernel lower bounds.\nTheorem 1 (Bodlaender et al., 2009, Fortnow and Santhanam, 2008). Let P be a parameterized problem whose unparameterized version is NP-complete. If P is compositional, then it does not admit a polynomial kernel unless NP ⊆ co-NP/poly, i.e., the Polynomial Hierarchy collapses.\nLet P,Q ⊆ Σ∗ × N be parameterized problems. We say that P is polynomial parameter reducible to Q if there exists a polynomial time computable function K : Σ∗ × N → Σ∗ × N and a polynomial p, such that for all (x, k) ∈ Σ∗ × N we have (i) (x, k) ∈ P if and only if K(x, k) = (x′, k′) ∈ Q, and (ii) k′ ≤ p(k). The function K is called a polynomial parameter transformation.\nThe following theorem allows us to transform kernel lower bounds from one problem to another.\nTheorem 2 (Bodlaender, Thomassé, and Yeo, 2009). Let P and Q be parameterized problems such that UP[P] is NP-complete, UP[Q] is in NP, and there is a polynomial parameter transformation from P to Q. If Q has a polynomial kernel, then P has a polynomial kernel."
    }, {
      "heading" : "4 Constraint Networks",
      "text" : "Constraint networks have proven successful in modeling everyday cognitive tasks such as vision, language comprehension, default reasoning, and abduction, as well as in applications such as scheduling, design, diagnosis, and temporal and spatial reasoning (Dechter, 2010). A constraint network is a triple I =\n(V,U, C) where V is a finite set of variables, U is a finite universe of values, and C = {C1, . . . , Cm} is set of constraints. Each constraint Ci is a pair (Si, Ri) where Si is a list of variables of length ri called the constraint scope, and Ri is an ri-ary relation over U , called the constraint relation. The tuples of Ri indicate the allowed combinations of simultaneous values for the variables Si. A solution is a mapping τ : V → U such that for each 1 ≤ i ≤ m and Si = (x1, . . . , xri), we have (τ(x1), . . . , τ(xri)) ∈ Ri. A constraint network is satisfiable if it has a solution.\nWith a constraint network I = (V,U, C) we associate its constraint graph G = (V,E) where E contains an edge between two variables if and only if they occur together in the scope of a constraint. A width w tree decomposition of a graph G is a pair (T, λ) where T is a tree and λ is a labeling of the nodes of T with sets of vertices of G such that the following properties are satisfied: (i) every vertex of G belongs to λ(p) for some node p of T ; (ii) every edge of G is is contained in λ(p) for some node p of T ; (iii) For each vertex v of G the set of all tree nodes p with v ∈ λ(p) induces a connected subtree of T ; (iv) |λ(p)| − 1 ≤ w holds for all tree nodes p. The treewidth of G is the smallest w such that G has a width w tree decomposition. The induced width of a constraint network is the treewidth of its constraint graph (Dechter and Pearl, 1989). We note in passing that the problem of finding a tree decomposition of width w is NP-hard but fixed-parameter tractable in w.\nLet U be a fixed universe containing at least two elements. We consider the following parameterized version of the constraint satisfaction problem (CSP).\nU -CSP(width) Instance: A constraint network I = (V,U, C) and a widthw tree decomposition of the constraint graph of I . Parameter: The integer w. Question: Is I satisfiable?\nIt is well known that U -CSP(width) is fixed-parameter tractable over any fixed universe U (Dechter and Pearl, 1989; Gottlob, Scarcello, and Sideri, 2002) (for generalizations see Samer and Szeider, 2010). We contrast this classical result and show that it is unlikely that U -CSP(width) admits a polynomial kernel, even in the simplest case where U = {0, 1}. Theorem 3. {0, 1}-CSP(width) does not admit a polynomial kernel unless the Polynomial Hierarchy collapses.\nProof. We show that {0, 1}-CSP(width) is compositional. Let (Ii, Ti), 1 ≤ i ≤ t, be a given sequence of instances of {0, 1}-CSP(width) where Ii = (Vi, Ui, Ci) is a constraint network and Ti is a width w tree decomposition of the constraint graph of Ii. We may assume, w.l.o.g., that Vi ∩ Vj = ∅ for 1 ≤ i < j ≤ t (otherwise we can simply change the names of variables). We form a new constraint network I = (V, {0, 1}, C) as follows. We put V = ⋃t i=1 Vi ∪ {a1, . . . , at, b0, . . . , bt} where ai, bi are new variables. We define the set C of constraints in three groups. (1) For each 1 ≤ i ≤ t and each constraint C = ((x1, . . . , xr), R) ∈ Ci we add to C a new constraint C ′ = ((x1, . . . , xr, ai), R ′)) where R′ = { (u1, . . . , ur, 0) : (u1, . . . , ur) ∈ R } ∪ {(1, . . . , 1)}.\n(2) We add t ternary constraints C∗1 , . . . , C ∗ t where C ∗ i = ((bi−1, bi, ai), R ∗) and R∗ = {(0, 0, 1), (0, 1, 0), (1, 1, 1)}.\n(3) Finally, we add two unary constraints C0 = ((b0), (0)) and C1 = ((bt), (1)) which force the values of b0 and bt to 0 and 1, respectively.\nLetG,Gi be the constraint graphs of I and Ii, respectively. Fig. 1 shows an illustration ofG for t = 4. We observe that a1, . . . , at are cut vertices of G. Removing these vertices separates G into independent parts P,G′1, . . . , G ′ t where P is the path b0, b1, . . . , bt, andG ′ i is isomorphic toGi. By standard techniques (see, e.g., Kloks, 1994), we can put the given width w tree decompositions T1, . . . , Tt of G′1, . . . , G ′ t and the trivial width 1 tree decomposition of P together to a width w + 1 tree decomposition T of G. Clearly (I, T ) can be obtained from (Ii, Ti), 1 ≤ i ≤ t, in polynomial time.\nWe claim that I is satisfiable if and only if at least one of the Ii is satisfiable. This claim can be verified by means of the following observations: The constraints in groups (2) and (3) provide that for any satisfying assignment there will be some 0 ≤ i ≤ t − 1 such that b0, . . . , bi are all set to 0 and\nbi+1, . . . , bt are all set to 1; consequently ai is set to 0 and all aj for j 6= i are set to 1. The constraints in group (1) provide that if we set ai to 0, then we obtain from C ′ the original constraint C; if we set ai to 1 then we obtain a constraint that can be satisfied by setting all remaining variables to 1. We conclude that {0, 1}-CSP(width) is compositional.\nIn order to apply Theorem 1, it remains to establish that the unparameterized version of {0, 1}-CSP(width) is NP-complete. Deciding whether a constraint network I over the universe {0, 1} is satisfiable is well-known to be NP-complete (say by reducing 3-SAT). To a constraint network I on n variables we can always add a trivial width w = n−1 tree decomposition of its constraint graph (taking a single tree node t where λ(t) contains all variables of I). Hence UP[{0, 1}-CSP(width)] is NP-complete."
    }, {
      "heading" : "5 Satisfiability",
      "text" : "The propositional satisfiability problem (SAT) was the first problem shown to be NP-hard (Cook, 1971). Despite its hardness, SAT solvers are increasingly leaving their mark as a general-purpose tool in areas as diverse as software and hardware verification, automatic test pattern generation, planning, scheduling, and even challenging problems from algebra (Gomes et al., 2008). SAT solvers are capable of exploiting the hidden structure present in real-world problem instances. The concept of backdoors, introduced by Williams, Gomes, and Selman (2003) provides a means for making the vague notion of a hidden structure explicit. Backdoors are defined with respect to a “sub-solver” which is a polynomial-time algorithm that correctly decides the satisfiability for a class C of CNF formulas. More specifically, Gomes et al. (2008) define a sub-solver to be an algorithm A that takes as input a CNF formula F and has the following properties: (i) Trichotomy: A either rejects the input F , or determines F correctly as unsatisfiable or satisfiable; (ii) Efficiency: A runs in polynomial time; (iii) Trivial Solvability: A can determine if F is trivially satisfiable (has no clauses) or trivially unsatisfiable (contains only the empty clause); (iv.) SelfReducibility: if A determines F , then for any variable x and value ε ∈ {0, 1}, A determines F [x = ε]. F [τ ] denotes the formula obtained from F by applying the partial assignment τ , i.e., satisfied clauses are removed and false literals are removed from the remaining clauses.\nWe identify a sub-solver A with the class CA of CNF formulas whose satisfiability can be determined by A. A strong A-backdoor set (or A-backdoor, for short) of a CNF formula F is a set B of variables such that for each possible truth assignment τ to the variables in B, the satisfiability of F [τ ] can be determined by sub-solver A in time O(nc). Hence, if we know an A-backdoor of size k, we can decide the satisfiability of F by running A on 2k instances F [τ ], yielding a time bound of O(2knc). Hence SAT decision is fixed-parameter tractable in the backdoor size k for any sub-solver A. Hence the following problem is clearly fixed-parameter tractable for any sub-solver A.\nSAT(A-backdoor) Instance: A CNF formula F , and an A-backdoor B of F of size k. Parameter: The integer k. Question: Is F satisfiable?\nWe are concerned with the question of whether instead of trying all 2k possible partial assignments we can reduce the instance to a polynomial kernel. We will establish a very general result that applies to all possible sub-solvers.\nTheorem 4. SAT(A-backdoor) does not admit a polynomial kernel for any sub-solver A unless the Polynomial Hierarchy collapses.\nProof. We will devise polynomial parameter transformations from the following parameterized problem which is known to be compositional (Fortnow and Santhanam, 2008) and therefore unlikely to admit a polynomial kernel.\nSAT(vars) Instance: A propositional formula F in CNF on n variables. Parameter: The number n of variables. Question: Is F satisfiable?\nLet F be a CNF formula and V the set of all variables of F . Due to property (ii) of a sub-solver, V is an A-backdoor set for any A. Hence, by mapping (F, n) (as an instance of SAT(vars)) to (F, V ) (as an instance of SAT(A-backdoor)) provides a (trivial) polynomial parameter transformation from SAT(vars) to SAT(A-backdoor). Since the unparameterized versions of both problems are clearly NP-complete, the result follows by Theorem 2.\nLet 3SAT(π) (where π is an arbitrary parameterization) denote the problem SAT(π) restricted to 3CNF formula, i.e., to CNF formulas where each clauses contains at most three literals. In contrast to SAT(vars), the parameterized problem 3SAT(vars) has a trivial polynomial kernel: if we remove duplicate clauses, then any 3CNF formula on n variables contains at most O(n3) clauses, and so is a polynomial kernel. Hence the easy proof of Theorem 4 does not carry over to 3SAT(A-backdoor). We therefore consider the cases 3SAT(HORN-backdoor) and 3SAT(2CNF-backdoor) separately, these cases are important since the detection of HORN and 2CNF-backdoors is fixed-parameter tractable (Nishimura, Ragde, and Szeider, 2004).\nTheorem 5. Neither 3SAT(HORN-backdoor) nor 3SAT(2CNF-backdoor) admit a polynomial kernel unless the Polynomial Hierarchy collapses.\nProof. Let C ∈ {HORN, 2CNF}. We show that 3SAT(C-backdoor) is compositional. Let (Fi, Bi), 1 ≤ i ≤ t, be a given sequence of instances of 3SAT(C-backdoor) where Fi is a 3CNF formula and Bi is a C-backdoor set of Fi of size k. We distinguish two cases.\nCase 1: t > 2k. Let ‖Fi‖ := ∑\nC∈Fi |C| and n := max t i=1 ‖Fi‖. Whether Fi is satisfiable or not can\nbe decided in timeO(2kn) since the satisfiability of a Horn or 2CNF formula can be decided in linear time. We can check whether at least one of the formulas F1, . . . , Ft is satisfiable in time O(t2kn) ≤ O(t2n) which is polynomial in t+ n. If some Fi is satisfiable, we output (Fi, Bi); otherwise we output (F1, B1) (F1 is unsatisfiable). Hence we have a composition algorithm.\nCase 2: t ≤ 2k. This case is more involved. We construct a new instance (F,B) of 3SAT(C-backdoor) as follows.\nLet s = dlog2 te. Since t ≤ 2k, s ≤ k follows. Let Vi denote the set of variables of Fi. We may assume, w.l.o.g., that B1 = · · · = Bt and that Vi ∩ Vj = B1 for all 1 ≤ i < j ≤ t since otherwise we can change names of variable accordingly. In a first step we obtain from every Fi a CNF formula F ′i as follows. For each variable x ∈ Vi \\ B1 we take two new variables x0 and x1. We replace each positive occurrence of a variable x ∈ Vi \\ B1 in Fi with the literal x0 and each negative occurrence of x with the literal ¬xs. We add all clauses of the form (¬xj−1 ∨ xj) for 1 ≤ j ≤ s; we call these clauses “connection clauses.” Let F ′i be the formula obtained from Fi in this way. We observe that F ′i and Fi are SAT-equivalent, since the connection clauses form an implication chain. Since the connection clauses are both Horn and 2CNF, B1 is also a C-backdoor of F ′i .\nWe take a set Y = {y1, . . . , ys} of new variables. Let C1, . . . , C2s be the sequence of all 2s possible clauses (modulo permutation of literals within a clause) containing exactly s literals over the variables in Y . Consequently we can write Ci as (`i1 ∨ · · · ∨ `is) where ` j i ∈ {yi,¬yi}.\nFor 1 ≤ i ≤ t we add to each connection clause (¬xj−1∨xj) of F ′i the literal `ij ∈ Ci. Let F ′′i denote the 3CNF formula obtained from F ′i this way.\nFor t < i ≤ 2s we define 3CNF formulas F ′′i as follows. If s ≤ 3 then F ′′i consists just of the clause Ci. If s > 3 then we take new variables zi2, . . . , z i s−2 and let F ′′ i consist of the clauses (` i 1 ∨ `i2 ∨ ¬zi2), (`i3 ∨ zi2 ∨ ¬zi3), . . . , (`is−2 ∨ zis−3 ∨ ¬zis−2), (`is−1 ∨ `is ∨ zis−2). Finally, we let F be the 3CNF formula containing all the clauses from F ′′1 , . . . , F ′′ 2s . Any assignment τ to Y ∪B1 that satisfies Ci can be extended to an assignment that satisfies F ′′i since such assignment satisfies at least one connection clause (xj−1 ∨ xj ∨ `ij) and so the chain of implications from from xo to xs is broken.\nIt is not difficult to verify the following two claims. (i) F is satisfiable if and only if at least one of the formulas Fi is satisfiable. (ii) B = Y ∪ B1 is a C-backdoor of F . Hence we have also a composition algorithm in Case 2, and thus 3SAT(C-backdoor) is compositional. Clearly UP[3SAT(C-backdoor)] is NP-complete, hence the result follows from Theorem 1."
    }, {
      "heading" : "6 Global Constraints",
      "text" : "The success of today’s constraint solvers relies heavily on efficient algorithms for special purpose global constraints (van Hoeve and Katriel, 2006). A global constraint specifies a pattern that frequently occurs in real-world problems, for instance, it is often required that variables must all take different values (e.g., activities requiring the same resource must all be assigned different times). The ALLDIFFERENT global constraint efficiently encodes this requirement.\nMore formally, a global constraint is defined for a set S of variables, each variable x ∈ S ranges over a finite domain dom(x) of values. An instantiation is an assignment α such that α(x) ∈ dom(x) for each x ∈ S. A global constraint defines which instantiations are legal and which are not. A global constraint is consistent if it has at least one legal instantiation, and it is domain consistent (or hyper arc consistent) if for each variable x ∈ S and each value d ∈ dom(x) there is a legal instantiation α with α(x) = d. For all global constraints considered in this paper, domain consistency can be reduced to a quadratic number of consistency checks, hence we will focus on consistency. We assume that the size of a representation of a global constraint is polynomial in ∑ x∈S |dom(x)|.\nFor several important types T of global constraints, the problem of deciding whether a constraint of type T is consistent (in symbols T -Cons) is NP-hard. Examples for such intractable types of constraints are NVALUE, DISJOINT, and USES (Bessière et al., 2004). An NVALUE constraint over a set X of variables requires from a legal instantiation α that |{α(x) : x ∈ X }| = N ; ALLDIFFERENT is the special case where N = |X|. The global constraints DISJOINT and USES are specified by two sets of variables X,Y ; DISJOINT requires that α(x) 6= α(y) for each pair x ∈ X and y ∈ Y ; USES requires that for each x ∈ X there is some y ∈ Y such that α(x) = α(y). For a set X of variables we write dom(X) = ⋃ x∈X dom(x).\nBessière et al. (2008) considered dx = |dom(X)| as parameter for NVALUE, dxy = |dom(X) ∩ dom(Y )| as parameter for DISJOINT, and dy = |dom(Y )| as parameter for USES. They showed that consistency checking is fixed-parameter tractable for the constraints under the respective parameterizations, i.e., the problems NVALUE-CONS(dx), DISJOINT-CONS(dxy), and USES-CONS(dy) are fixedparameter tractable. We show that it is unlikely that their results can be improved in terms of polynomial kernels. Theorem 6. The problems NVALUE-CONS(dx), DISJOINT-CONS(dxy), USES-CONS(dy) do not admit polynomial kernels unless the Polynomial Hierarchy collapses.\nProof. We devise a polynomial parameter reduction from SAT(vars). We use a construction of Bessière et al. (2004). Let F = {C1, . . . , Cm} be a CNF formula over variables x1, . . . , xn. We consider the clauses and variables of F as the variables of a global constraint with domains dom(xi) = {−i, i}, and dom(Cj) = { i : xi ∈ Cj } ∪ {−i : ¬xi ∈ Cj }. Now F can be encoded as an NVALUE constraint with X = {x1, . . . , xn, C1, . . . , Cm} and N = n (clearly F is satisfiable if and only if the constraint is consistent). Since dx = 2n we have a polynomial parameter reduction from SAT(vars) to NVALUE-CONS(dx). Similarly, as observed by Bessière et al. (2009), F can be encoded as a DISJOINT constraint with X = {x1, . . . , xn} and Y = {C1, . . . , Cm} (dxy ≤ 2n), or as a USES constraint with X = {C1, . . . , Cm} and Y = {x1, . . . , xn} (dy = 2n). Since the unparameterized problems are clearly NP-complete, the result follows by Theorem 2.\nFurther results on kernels for global constraints have been obtained by Gaspers and Szeider (2011).7"
    }, {
      "heading" : "7 Bayesian Reasoning",
      "text" : "Bayesian networks (BNs) have emerged as a general representation scheme for uncertain knowledge (Pearl, 2010). A BN models a set of stochastic variables, the independencies among these variables, and a joint probability distribution over these variables. For simplicity we consider the important special case where the stochastic variables are Boolean. The variables and independencies are modelled in the BN by a directed acyclic graph G = (V,A), the joint probability distribution is given by a table Tv for each node v ∈ V which defines a probability Tv|U for each possible instantiation U = (d1, . . . , ds) ∈ {true, false}s of the parents v1, . . . , vs of v in G. The probability Pr(U) of a complete instantiation U of the variables ofG is given by the product of Tv|U over all variables v. We consider the problem Positive-BN-Inference which takes as input a Boolean BN (G,T ) and a variable v, and asks whether Pr(v = true) > 0. The problem is NP-complete (Cooper, 1990) and moves from NP to #P if we ask to compute Pr(v = true) (Roth, 1996). The problem can be solved in polynomial time if the BN is singly connected, i.e, if there is at most one undirected path between any two variables (Pearl, 1988). It is natural to parametrize the problem by the number of variables one must delete in order to make the BN singly connected (the deleted variables form a loop cutset). In fact, POSITIVE-BN-INFERENCE(loop cutset size) is easily seen to be fixed-parameter tractable as we can determine whether Pr(v = true) > 0 by taking the maximum of Pr(v = true | U) over all 2k possible instantiations of the k cutset variables, each of which requires processing of a singly connected network. However, although fixed-parameter tractable, it is unlikely that the problem admits a polynomial kernel. Theorem 7. POSITIVE-BN-INFERENCE(loop cutset size) does not admit a polynomial kernel unless the Polynomial Hierarchy collapses.\nProof. (Sketch.) We give a polynomial parameter transformation from SAT(vars) and apply Theorem 2. The reduction is based on the reduction from 3SAT given by Cooper (1990). However, we need to allow clauses with an arbitrary number of literals since, as observed above, 3SAT(vars) has a polynomial kernel. Let F be a CNF formula on n variables. We construct a BN (G,T ) such that for a variable v we have Pr(v = true) > 0 if and only if F is satisfiable. Cooper uses input nodes ui for representing variables of F , clause nodes ci for representing the clauses of F , and conjunction nodes di for representing the conjunction of the clauses. We proceed similarly, however, we cannot represent a clause of large size with a single clause node ci, as the required table Tci would be of exponential size. Therefore we split clauses containing more than 3 literals into several clause nodes, as indicated in Figure 2. It remains to observe that\nthe set of input nodes E = {u1, . . . , un} is a loop cutset of the constructed BN, hence we have indeed a polynomial parameter transformation from SAT(vars) to POSITIVE-BN-INFERENCE(loop cutset size). The result follows by Theorem 2."
    }, {
      "heading" : "8 Nonmonotonic Reasoning",
      "text" : "Logic programming with negation under the stable model semantics is a well-studied form of nonmonotonic reasoning (Gelfond and Lifschitz, 1988; Marek and Truszczyński, 1999). A (normal) logic program P is a finite set of rules r of the form\nh←− a1 ∧ · · · ∧ am ∧ ¬b1 ∧ · · · ∧ ¬bn\nwhere h, ai, bi are atoms, where h forms the head and the ai, bi from the body of r. We write H(r) = h, B+(r) = {a1, . . . , am}, and B−(r) = {b1, . . . , bn}. Let I be a finite set of atoms. The GF reduct P I of\na logic program P under I is the program obtained from P by removing all rules r with B−(r) ∩ I 6= ∅, and removing from the body of each remaining rule r′ all literals ¬b with b ∈ I . I is a stable model of P if I is a minimal model of P I , i.e., if (i) for each rule r ∈ P I with B+(r) ⊆ I we have H(r) ∈ I , and (ii) there is no proper subset of I with this property. The undirected dependency graph U(P ) of P is formed as follows. We take the atoms of P as vertices and add an edge x − y between two atoms x, y if there is a rule r ∈ P with H(r) = x and y ∈ B+(r), and we add a path x− u− y if H(r) = x and y ∈ B−(r) (u is a new vertex of degree 2). The feedback width of P is the size of a smallest set V of atoms such that every cycle of U(P ) runs through an atom in V .\nA fundamental computational problems is Stable Model Existence (SME), which asks whether a given normal logic program has a stable model. The problem is well-known to be NP-complete (Marek and Truszczyński, 1991). Gottlob, Scarcello, and Sideri (2002) showed that SME(feedback width) is fixed-parameter tractable (see Fichte and Szeider (2011) for generalizations). We show that this result cannot be strengthened with respect to a polynomial kernel.\nTheorem 8. SME(feedback width) does not admit a polynomial kernel unless the Polynomial Hierarchy collapses.\nProof. (Sketch.) We give a polynomial parameter transformation from SAT(vars) to SME(feedback width) using a construction of Niemelä (1999). Given a CNF formula F on n variables, we construct a logic program P as follows. For each variable x of F we take two atoms x and x̂ and include the rules (x̂← ¬x) and (x← ¬x̂); for each clause C of F we take an atom c and include for each positive literal a of C the rule (c ← a), and for each negative literal ¬a of C the rule (c ← â); finally, we take two atoms s and f and include the rule (f ← ¬f ∧ ¬s) and for each clause C of F the rule (s ← c). F is satisfiable if and only if P has a stable model (Niemelä, 1999). It remains to observe that each cycle of U(P ) runs through a vertex in V = {x, x̂ : x ∈ vars(F ) }, hence the feedback width of P is at most 2n. Hence we have a polynomial parameter transformation from SAT(vars) to SME(feedback width). The result follows by Theorem 2."
    }, {
      "heading" : "9 Conclusion",
      "text" : "We have established super-polynomial kernel lower bounds for a wide range of important AI problems, providing firm limitations for the power of polynomial-time preprocessing for these problems. We conclude from these results that in contrast to many optimization problems (see Section 1), typical AI problems do not admit polynomial kernels. Our results suggest the consideration of alternative approaches. For example, it might still be possible that some of the considered problems admit polynomially sized Turing kernels, i.e., a polynomial-time preprocessing to a Boolean combination of a polynomial number of polynomial kernels. In the area of optimization, parameterized problems are known that do not admit polynomial kernels but admit polynomial Turing kernels (Fernau et al., 2009). This suggests a theoretical and empirical study of Turing kernels for the AI problems considered."
    } ],
    "references" : [ {
      "title" : "The complexity of global constraints",
      "author" : [ "Bessière et al", "2004] Bessière", "E. Hebrard", "B. Hnich", "T. Walsh" ],
      "venue" : "Proceedings of the Nineteenth National Conference on Artificial Intelligence, July 25-29,",
      "citeRegEx" : "C. et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "C. et al\\.",
      "year" : 2004
    }, {
      "title" : "The parameterized complexity of global constraints",
      "author" : [ "Bessière et al", "2008] Bessière", "E. Hebrard", "B. Hnich", "Z. Kiziltan", "C.-G. Quimper", "T. Walsh" ],
      "venue" : "In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "C. et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "C. et al\\.",
      "year" : 2008
    }, {
      "title" : "Range and roots: Two common patterns for specifying and propagating counting and occurrence constraints. Artificial Intelligence 173(11):1054–1078",
      "author" : [ "Bessière et al", "2009] Bessière", "E. Hebrard", "B. Hnich", "Z. Kiziltan", "T. Walsh" ],
      "venue" : null,
      "citeRegEx" : "C. et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "C. et al\\.",
      "year" : 2009
    }, {
      "title" : "and Dechter",
      "author" : [ "B. Bidyuk" ],
      "venue" : "R.",
      "citeRegEx" : "Bidyuk and Dechter. 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "M",
      "author" : [ "H.L. Bodlaender", "R.G. Downey", "Fellows" ],
      "venue" : "R.; and Hermelin, D.",
      "citeRegEx" : "Bodlaender et al.. 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A",
      "author" : [ "H.L. Bodlaender", "S. Thomassé", "Yeo" ],
      "venue" : "2009. Kernel bounds for disjoint cycles and disjoint paths. In Fiat, A., and Sanders, P., eds., Algorithms - ESA 2009, 17th Annual European Symposium, Copenhagen, Denmark, September 7-9,",
      "citeRegEx" : "Bodlaender. Thomassé. and Yeo. 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and van der Gaag",
      "author" : [ "J. Bolt" ],
      "venue" : "L.",
      "citeRegEx" : "Bolt and van der Gaag. 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "F",
      "author" : [ "Cadoli, M.", "Donini" ],
      "venue" : "M.; Liberatore, P.; and Schaerf, M.",
      "citeRegEx" : "Cadoli et al.. 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "S",
      "author" : [ "Cook" ],
      "venue" : "A.",
      "citeRegEx" : "Cook. 1971",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "G",
      "author" : [ "Cooper" ],
      "venue" : "F.",
      "citeRegEx" : "Cooper. 1990",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "and Pearl",
      "author" : [ "R. Dechter" ],
      "venue" : "J.",
      "citeRegEx" : "Dechter and Pearl. 1989",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "M",
      "author" : [ "Downey, R.", "Fellows" ],
      "venue" : "R.; and Stege, U.",
      "citeRegEx" : "Downey. Fellows. and Stege. 1999",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "and Biere",
      "author" : [ "N. Eén" ],
      "venue" : "A.",
      "citeRegEx" : "Eén and Biere. 2005",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "F",
      "author" : [ "Fernau, H.", "Fomin" ],
      "venue" : "V.; Lokshtanov, D.; Raible, D.; Saurabh, S.; and Villanger, Y.",
      "citeRegEx" : "Fernau et al.. 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "and Szeider",
      "author" : [ "J.K. Fichte" ],
      "venue" : "S.",
      "citeRegEx" : "Fichte and Szeider. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "and Santhanam",
      "author" : [ "L. Fortnow" ],
      "venue" : "R.",
      "citeRegEx" : "Fortnow and Santhanam. 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "and Szeider",
      "author" : [ "S. Gaspers" ],
      "venue" : "S.",
      "citeRegEx" : "Gaspers and Szeider. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Advanced preprocessing for answer set solving",
      "author" : [ "Gebser et al", "2008] Gebser", "B. Kaufmann", "A. Neumann", "T. Schaub" ],
      "venue" : "ECAI 2008 - 18th European Conference on Artificial Intelligence,",
      "citeRegEx" : "M. et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "M. et al\\.",
      "year" : 2008
    }, {
      "title" : "and Lifschitz",
      "author" : [ "M. Gelfond" ],
      "venue" : "V.",
      "citeRegEx" : "Gelfond and Lifschitz. 1988",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "C",
      "author" : [ "Gomes" ],
      "venue" : "P.; Kautz, H.; Sabharwal, A.; and Selman, B.",
      "citeRegEx" : "Gomes et al.. 2008",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Fixed-parameter complexity in AI and nonmonotonic reasoning",
      "author" : [ "Scarcello Gottlob", "Sideri", "2002] Gottlob", "F. Scarcello", "M. Sideri" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "Gottlob et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Gottlob et al\\.",
      "year" : 2002
    }, {
      "title" : "and Niedermeier",
      "author" : [ "J. Guo" ],
      "venue" : "R.",
      "citeRegEx" : "Guo and Niedermeier. 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "and Truszczyński",
      "author" : [ "W. Marek" ],
      "venue" : "M.",
      "citeRegEx" : "Marek and Truszczyński. 1991",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "and Truszczyński",
      "author" : [ "V.W. Marek" ],
      "venue" : "M.",
      "citeRegEx" : "Marek and Truszczyński. 1999",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Detecting backdoor sets with respect to Horn and binary clauses",
      "author" : [ "Ragde Nishimura", "Szeider", "2004] Nishimura", "P. Ragde", "S. Szeider" ],
      "venue" : "In Proceedings of SAT 2004 (Seventh International Conference on Theory and Applications of Satisfiability Testing,",
      "citeRegEx" : "Nishimura et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Nishimura et al\\.",
      "year" : 2004
    }, {
      "title" : "C",
      "author" : [ "Papadimitriou" ],
      "venue" : "H.",
      "citeRegEx" : "Papadimitriou. 1994",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "and Szeider",
      "author" : [ "M. Samer" ],
      "venue" : "S.",
      "citeRegEx" : "Samer and Szeider. 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "On the connections between backdoors, restarts, and heavy-tailedness in combinatorial search",
      "author" : [ "Gomes Williams", "Selman", "2003] Williams", "C. Gomes", "B. Selman" ],
      "venue" : "In Informal Proc. of the Sixth International Conference on Theory and Applications of Satisfiability Testing, S. Margherita Ligure - Portofino,",
      "citeRegEx" : "Williams et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2003
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "We present a first theoretical analysis of the power of polynomial-time preprocessing for important combinatorial problems from various areas in AI. We consider problems from Constraint Satisfaction, Global Constraints, Satisfiability, Nonmonotonic and Bayesian Reasoning. We show that, subject to a complexity theoretic assumption, none of the considered problems can be reduced by polynomial-time preprocessing to a problem kernel whose size is polynomial in a structural problem parameter of the input, such as induced width or backdoor size. Our results provide a firm theoretical boundary for the performance of polynomial-time preprocessing algorithms for the considered problems.",
    "creator" : "TeX"
  }
}