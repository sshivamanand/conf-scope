{
  "name" : "1206.6827.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Linear Algebra Approach to Separable Bayesian Networks",
    "authors" : [ "Chalee Asavathiratham" ],
    "emails" : [ "chalee@alum.mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Separable Bayesian Networks, or the Influence Model, are dynamic Bayesian Networks in which the conditional probability distribution can be separated into a function of only the marginal distribution of a node’s parents, instead of the joint distributions. We describe the connection between an arbitrary Conditional Probability Table (CPT) and separable systems using linear algebra. We give an alternate proof to [Pfeffer00] on the equivalence of sufficiency and separability. We present a computational method for testing whether a given CPT is separable."
    }, {
      "heading" : "1 Introduction",
      "text" : "Separable Bayesian Networks, or the Influence Model, are dynamic Bayesian Networks in which the conditional probability distribution can be separated into a function of only the marginal distribution of a node’s neighbors, instead of the joint distributions. In terms of modeling, separable networks has rendered possible siginificant reduction in complexity, as the state space is only linear in the number of variables on the network, in contrast to a typical state space which is exponential.\nWe describe the connection between an arbitrary Conditional Probability Table (CPT) and separable systems using linear algebra. We give an alternate proof to [Pfeffer00] on the equivalence of sufficiency and separability. We present a computational method for testing whether a given CPT is separable."
    }, {
      "heading" : "2 Results",
      "text" : ""
    }, {
      "heading" : "2.1 Probability Mass Functions",
      "text" : "Suppose X , Y and Z are three random variables and let the size of their sample spaces be some finite integers mx, my and my respectively. Similar to [Pfeffer00], we denote the space of Probability Mass Functions (PMF) over the joint random variables (X,Y ) as ∆XY , i.e.,\n∆XY = {q ∈ Rmxmy | q ≥ 0, q′1mxmy = 1},\nwhere 1mxmy is the all-ones column vector of length (mxmy), and q ≥ 0 means each entry of the column vector q is non-negative. When there is no ambiguity, we will drop the subscript from 1."
    }, {
      "heading" : "2.2 Event Matrix",
      "text" : "We now introduce a particular class of matrices which would be useful for our discussion later. These matrices have been introduced in [Asavathiratham00] (Sec. 5.2.1) and is instrumental in some of the key theoretical results in the thesis.\nLet ei = [ 0 · · · 0 1 0 · · · 0 ]′ be the vector in which the single 1-entry appears in the ith position. For random variable X , we can represent its ith outcome as ei ∈ Rmx . Stacking these vectors into a matrix, we get the event matrix BX – in this case, the mx-by-mx identity matrix – whose rows form a sample space for X .\nFor joint variables (X,Y ), let us define the event matrix as\nBXY = [Imx ⊗ 1my |1mx ⊗ Imy ],\nwhere J⊗K denotes the Kronecker product of matrices\nJ and K.1 For example, if mx = 2 and my = 3, then\nBXY = ⎡ ⎢⎢⎢⎢⎢⎢⎣\n1 0 1 0 1 0 0 1 0 1 0 1 ∣∣∣∣∣∣∣∣∣∣∣∣ 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 ⎤ ⎥⎥⎥⎥⎥⎥⎦ .\nFor convenience, let us define B = BXY . It can be seen that each row of B is an outcome of the joint random variable of (X,Y ). Moreover, the ordering of the row B also provides for us an index for elements in the sample space of (X,Y ). Note that\nB [ 1mx 0 ] = B [ 0 1my ] = 1mxmy . (1)\nWe shall also list some properties of B that will be used later on in the paper.\nTheorem 1\nrank(B) = mx +my − 1\nProof: See Theorem 5.8 in [Asavathiratham00].\nLet N (K) and R(K) be the null space and the subspace of some matrix K.\nCorollary 2\nN (B) = R ( [ 1mx\n−1my\n])\nProof: See Corollary 5.10 in [Asavathiratham00].\nNote that for a given q ∈ ∆XY , the product q′B = [ q′X q ′ Y ],\nwhere qX and qY are marginal of q over X and Y respectively. That is, multiplication by B gives us the marginal distribution on X and Y from q. As such, we define the marginalization matrices as follows\nBXY,X = B [ B′X 0 ] (2)\nBXY,Y = B [ 0 B′Y ] , (3)\nso that q′X = q ′BXY,X and q′Y = q ′BXY,Y . These marginilization matrices shall be used below.\n1For example, if J is a 2-by-2 matrix with entries given by j11, . . . , j22 and if K is some arbitrary matrix, then\nJ ⊗ K = j11K j12K j21K j22K ."
    }, {
      "heading" : "2.3 Sufficiency",
      "text" : "For a given conditional PMF P (Z|XY ), we let CP (Z|XY ), or simply C, denote its Conditional Probability Table (CPT). That is, C is a (mxmy)-by-mz matrix such that each row of C corresponds to an outcome of (X,Y ), sorted in the order defined by B. Specifically, let the (i, j)th entry of C be defined as\n[C]ij = P (Z = j | (X,Y )is ith row of B )\nIt then follows that C has nonnegative entries and\nC1mz = 1mxmy .\nIn [Pfeffer00], the function ΦP : ∆XY → ∆Z was defined as ΦP (q) = ∑ xy q(xy)P (Z|xy). Expressing this definition in a matrix form, we get\nΦP (q) = q′C.\nBecause ΦP , P (Z|XY ) and C completely describe one another, we will focus mainly on C.\nIn general, obtaining a marginal on Z requires that we know the full joint distribution on (X,Y ). Below we define a special condition such that the marginals on X and Y alone can uniquely determine the marginal on Z.\nDefinition 1 C is sufficient if for any q1,q2 ∈ ∆XY such that q′1B = q ′ 2B, then q ′ 1C = q ′ 2C.\nTheorem 3 C is sufficient if and only if N (B′) ⊂ N (C′)\nProof: First suppose C is sufficient. Take any q̃ ∈ N (B′) so that q̃′B = 0. If q̃ ≥ 0, then it must follow that q̃ = 0. Otherwise, if q̃ has any positive entry, q̃′B would have at least some positive entries, because every row of B has some positive entries. This would contradict the fact q̃′B = 0.\nThen q̃ must have at least one negative entry. This implies that it must also have at least one positive entry, because\nq̃′1 = q̃′B [\n1mx 0\n] = 0. (4)\nwhere the second equality follows from (1). We can, therefore, separate the positive entries from the negative ones to obtain\nq̃ = q+ − q−, where q+ and q− are the absolute values of the pos-\nitive and negative entries of q̃ respectively. Let d =\nq′+1 = q ′ −1, where the second equality follows from (4). Because q+ has some positive entry, d > 0. Define q̃+ = 1dq+ and q̃− = 1 dq−. It can then be shown that q̃+ and q̃− are both valid PMFs. Therefore,\nq̃′B = 0 ↔ d(q̃+ − q̃−)′B = 0 ↔ q̃′+B = q̃′−B ↔ q̃′+C = q̃′−C (5) ↔ q̃′C = 0\nwhere (5) follows from sufficiency of C. This completes the forward part of the proof. The converse of the proof is straightforward.\nLet R(A) denote the subspace spanned by the matrix A.\nCorollary 4 C is sufficient if and only if R(C) ⊂ R(B).\nProof: This follows from the fact that N (B′) ⊂ N (C′) is equivalent to R(C) ⊂ R(B)."
    }, {
      "heading" : "2.4 Separability",
      "text" : "First we state the definition of separability as given in [Pfeffer00].\nDefinition 2 P (Z|XY ) is separable if there exists condition distributions PX(Z|X), PY (Z|Y ) and a constant γ ∈ [0, 1] such that P (Z|XY ) = γPX(Z|X)+(1− γ)PY (Z|Y ).\nSeparability is the key to the reduction in complexity. To see this, consider, for example, the task of listing P (Z|XY ) for all possible values of X ,Y and Z. In general, we would require a table whose number of entries is on the order of O(mxmymz). Through separability, we only need to list the conditional probabilities P (Z|X) and P (Z|Y ) separately, and compute P (Z|XY ) upon needed. This would result in a table on the order of O(mzmx + mzmy). The reduction would be even greater as we extend this to multiple variables such as P (Z|X1X2 · · ·Xn) later on.\nProposition 1 P (Z|XY ) is separable if and only if C can be expressed as\nC = γBXY,XCX + (1− γ)BXY,Y CY (6)\nwhere CX and CY are mx-by-mz and my-by-mz nonnegative matrices with every row summing to 1.\nTo see why this is the case, one only has to recognize that that the rows of CX and CY are respectively the conditional PMFs PX(Z|X) and PY (Z|Y ) expressed in a matrix form, with one row for each different outcome\nof X and Y . If we multiply (6) by some q ∈ ∆XY , the left hand-side q′C would be a marginal distribution P (Z). On the right-hand side, the first-term would be\nq′BXY,XCX = q′XCX ,\nwhich is PX(Z|X) in vector form. Similarly, the second-term on the right-hand side would be PY (Z|Y ) as desired.\nExample 1 Let (mx,my,mz) = (2, 3, 2) and let\nC = ⎡ ⎢⎢⎢⎢⎢⎢⎣ .65 .35 .35 .65 .20 .80 .60 .40\n.30 .70 .15 .85\n⎤ ⎥⎥⎥⎥⎥⎥⎦ .\nOne way to factorize C into the form in (6) is to as follows.\nγ = 0.5, CX = [ .3 .7\n.2 .8\n] , CY = ⎡ ⎣ 1 0.4 .6\n.1 .9\n⎤ ⎦ .\nSince the factorization is not unique, another choice would be\nγ = 0.5, CX = [ .4 .6\n.3 .7\n] , CY = ⎡ ⎣ .9 .1.3 .7\n0 1\n⎤ ⎦ .\nThe following result has been first shown in [Pfeffer00]. The proof we are presenting below is different in that it is based on linear algebra. By establishing this connection, we hope to apply some of the results from matrix analysis to simplify some theoretical and computational problems in the Bayesian Network area.\nTheorem 5 C is sufficient if and only if P (Z|XY ) is separable.\nProof: First suppose that C is sufficient. By Corollary 4, C = BF for some (mx +my)-by-mz matrix F . Because each row of C is a valid PMF, C1mz = 1mxmy , which means\nBF1mz = 1mxmy . (7)\nWe would like factorize F in (7) to show its separability. From (1), we know that\nB · 1 2 [ 1mx 1my ] = 1 2 (1mxmy + 1mxmy) = 1mxmy . (8)\nCombining (8) and Corollary 2, we can express the term F1 as a sum of particular and homogeneous solution, i.e.,\nF1mz = 1 2 [ 1mx 1my ] + δ [ 1mx −1my ] (9)\nwhere δ is some constant.\nCase A: If F ≥ 0, then (9) implies that − 12 ≤ δ ≤ 12 . For the special cases when − 12 < δ < 12 , we let γ = 1 2 + δ and\nCX = 1\n1 2 + δ\n[Imx 0]F\nCY = 1\n1 2 − δ\n[ 0 Imy ] F\nIf, however, δ = 12 , then we let γ = 1 and CX = [Imx 0]F , and CY can be any CPT. Similarly, when δ = − 12 , we let CY = [ 0 Imy ] F , and γ = 0.\nIt can easily verified that CX and CY as defined are valid CPTs, and that γ would lie in [0, 1]. Substituting these values into (6), we thus have proved that P (Z|XY ) is separable. Case B: If F has some negative entries, we can convert it to a non-negative matrix one column at a time as follows. Let f be a column that contains at least one negative entry. Let us partition it into blocks of mx and my entries so that f ′ = [f ′x f ′ y]. Each row of Bf can be written as\n(fx)i + (fy)j (10)\nwhere (fx)i and (fy)j represents some ith and jth entries of fx and fy respectively. Because BF = C ≥ 0, at most one of the terms in (10) can be negative. Indeed, if fx has a negative entry, there can be no negative value in fy at all, and vice versa. Assume without loss of generality that fy ≥ 0, and fx contains some negative entry. Let i∗ = argmini(fx)i, and j∗ = argminj(fy)j . Because Bf ≥ 0, (fy)j∗ ≥ −(fx)i∗ > 0. Then replace f with\nf̃ = f + α\n[ 1mx\n−1my\n] ,\nwhere −(fx)i∗ ≤ α ≤ (fy)j∗ . By construction, f̃ would have no negative entry. Furthermore, because of Corollary 2, Bf = Bf̃ . We can apply this to all other columns of F that has a negative entry until we arrive at F̃ ≥ 0. Then we can apply Case A to obtain CX and CY . This thus proves that P (Z|XY ) is separable. On the other hand, if P (Z|XY ) is separable, then we can factorize C according to Proposition 1. By (2)- (3), BXY,X , BXY,Y ∈ R(B). Thus, C ∈ R(B). By Corollary 4, C is sufficient."
    }, {
      "heading" : "2.5 Extension to Multiple Variables",
      "text" : "The results above can be extended to the multiplevariable case such as P (Z|X1 · · ·Xn).\nLet m1, . . . ,mn be the size of the sample spaces of X1, . . . , Xn respectively. We construct a sequence of matrices {Bi} from i = 1 to i = n through a recursive procedure as follows:\nB1 = Im1\nBi = [ Bi−1 ⊗ 1mi | 1µi−1 ⊗ Imi ] (11)\nwhere µi = ∏i j=1 mj . The event matrix and, will be\ndenoted by B = Bn. Some properties of B are presented again for the general case.\nTheorem 6 For 1 ≤ i ≤ n,\nrank ( Bi ) = ( i∑ k=1 mk ) − i+ 1\nProof: See Theorem 5.8 in [Asavathiratham00].\nTheorem 7\nN (B) = { v ∣∣∣ v = ⎡ ⎢⎣ a11m1 ...\nan1mn\n⎤ ⎥⎦ with ∑\ni\nai = 0 }\nProof: See Corollary 5.10 in [Asavathiratham00].\nDefinition 3 (General Sufficient Condition) C is sufficient if for any q1,q2 ∈ ∆X1···Xn such that q′1B = q ′ 2B, then q ′ 1C = q ′ 2C.\nThereom 3 and Corollary 4 and their proofs still hold in the general case.\nDefinition 4 (General Sepability Condition) P (Z|X1 · · ·Xn) is separable if there exist a set of distributions {Pi(Z|X1 · · ·Xn)} and non-negative constants {γi} such that ∑ i γi = 1 and\nP (Z|X1 · · ·Xn) = n∑\ni=1\nγiPi(Z|Xi).\nLet B̂i = B · (ei ⊗ Imi), i.e., B̂i extracts the columns corresponding to the ith variable from B.\nProposition 2 P (Z|X1 · · ·Xn) is separable if and only if C can be written as\nC = n∑\ni=1\nγiB̂iCi (12)\nwhere each Ci is an mi-by-mz nonnegative matrix in which every row sums to 1, and the coefficients {γi} are non-negative and sum to 1.\nTheorem 8 C is sufficient if and only if P (Z|X1 · · ·Xn) is separable.\nSketch of Proof: The proof would proceed similarly as the one in Theorem 5, except that (9) should be written as\nF1 = 1 n\n⎡ ⎢⎣ 1m1 ...\n1mn\n⎤ ⎥⎦+ ⎡ ⎢⎣ a11m1 ...\nan1mn\n⎤ ⎥⎦ ,\nin which ∑\ni ai = 0 from Theorem 7.\nCase A: If F ≥ 0, we apply the same arguments and let Ci = 11\nn +ai\n[ 0 · · · Imi · · · 0 ] F .\nCase B: If F has some negative entry, we would again add some columns to its to make it non-negative. Here we partition f into n blocks according to {mi}. Then there must be some row of Bf that is equal to\nn∑ i=1 min(fi), (13)\ni.e., some row of B would happen to select the minimal entry of every block {fi} into the sum. Let us group all the indices that contribute positive (or zero) value in (13) into A+, and let the rest be A−. Now, for all i ∈ A−, let αi = |min(fi)|. For i ∈ A+, let αi be some negative coefficients chosen in any way such that αi +min(fi) ≥ 0 and ∑ i∈A+ αi = − ∑ i∈A− αi. These {αi} must exist2 because∑ i∈A+ min(fi) ≥ − ∑ i∈A− min(f−),\nwhich follows from the fact that Bf ≥ 0. Thus, by replacing f with\nf̃ = f + ⎡ ⎢⎣ α11m1 ...\nαn1mn\n⎤ ⎥⎦ ,\nwe would have a non-negative column as desired. The rest of the proof is the same."
    }, {
      "heading" : "2.6 Test for Separability",
      "text" : "Given a CPT matrix C we can test if it is separable by simply verifying that it lies in the subspace of B. One way to do this is to test whether the orthogonal projection C onto R(B) is equal to itself. If C ∈ R(B), then the projection would only be the least-square approximation.\nTo achieve this, we would need a basis for R(B). Unfortunately, B itself does not have full column rank\n2For example, a greedy algorithm would work.\naccording to Theorem 6 and thus cannot be directly used to create an orthogonal projection. We propose the following slightly modified matrix A, which is defined in the following recursive way:\nA1 = Im1\nAi = [ Ai−1 ⊗ 1mi | 1µi−1 ⊗ Ĩmi ]\nwhere µi = ∏i j=1 mj , and Ĩm is the matrix consisting of the first m− 1 columns of the m-by-m identity matrix. The resulting A = An would be a matrix with µn\nrows and ( ∑\ni mi)−n+1 columns. By inspection, one can see A can be obtained from B by dropping certain n− 1 columns. This means that R(A) ⊂ R(B). Theorem 9 The columns of A are linearly independent and spans R(B). Proof: For i = 1 the claim is trivially true. Assume that the assertion is true up to some index i− 1. Because B(i−1) ∈ R(A(i−1)), there exists some matrix K such that B(i−1) = A(i−1)K. Thus, by the Kronecker product property, Bi−1⊗1mi = (A(i−1) ⊗1mi)K. Define\nB̃i = [ B(i−1) ⊗ 1mi | 1µi−1 ⊗ Ĩmi ] (14)\nFrom Lemma B.1 in Appendix B of [Asavathiratham00], we have that\nrank ( B̃i ) = rank ( B(i−1) ) +mi − 1\n= rank ( A(i−1) ) +mi − 1 (15)\n= rank(Ai). (16)\nEq. (15) comes from assertion of the proof up to i−1, and (16) is due to the fact that mi − 1 new columns are added in going from A(i−1) to Ai. 3\nDefine the orthogonal projection of C as\nP(C) = A(A′A)−1A′C. (17) Note that the inverse (A′A)−1 must exist because A has full column rank.\nTheorem 10 (Separability Test) C is separable if and only if P(C) = C. Proof: This follows from standard linear algebra theory and from Corollary 4 and Theorem 8."
    }, {
      "heading" : "2.7 Connection to the Influence Model",
      "text" : "The Influence Model was first introduced in [Asavathiratham00] and succinctly described in [Asavathiratham01]. Originally intended to explain cascading\n3Can we find another basis that is both orthonormal and recursive?\nphenomena in power systems, the model has been applied in applications such as social interaction, viral marketing networks, information diffusion, and distributed control.\nRecall that an Influence Model is defined by an n-by-n stochastic matrix D called the influence matrix and a set of matrices {Aij} for all i, j = 1, . . . , n such that every Aij ≥ 0 and Aij1 = 1. Each site i (node i) at time k has a status that is represented by\nsi[k] = [ 0 · · · 1 · · · 0]′\nwhere the position of the 1-entry represents the its current status. For each time step, given the states {si[k]} the next-state PMF, or the posterior belief, at time k + 1 state i is given by\np′i[k + 1] = di1s ′ 1[k]Ai1 + · · ·+ dins′n[k]Ain.\nThe state at time k + 1 would be realized randomly according to pi[k + 1].\nIt turns out that a DBN in which all CPT’s are separable is an influence model. To make the relation explicit, one can see that the entries in each row of D is equivalent to a set of {γi} in (12), and {Aij} are the Ci’s.\nApproximating a given DBN with an Influence Model should allow us computational savings in various ways. For example, the Influence Model allows us to compute the marginal distribution of a node at any given time efficiently. It also provides for a simple way for analyzing the recurrent classes based on reduced-order graph structure. Absorption probabilities of each recurrent classes can also be efficiently computed."
    }, {
      "heading" : "3 Open Issues",
      "text" : ""
    }, {
      "heading" : "3.1 Approximating General DBN with a Separable System",
      "text" : "So far, we have only focused on the CPT of a given variable Z, but the idea can be extended to a network of random variables. That is, if we are given a Dynamic Bayesian Network in which every node has an arbitrary CPT based on the joint distribution of its neighbors (including possibly itself), then we can appoximate it with a separable version. What would then be the optimal way to achieve this?\nOne possible procedure is to work with the CPT of each node separately. That is, one approximates the CPT table at each node with a separable version that is optimal in some sense. Once we have a separable CPT, we can factorize it into smaller CPT tables according to (12).\nWhat should be the optimality criterion in the approximation? Although P(C) is optimal in the least-square sense, i.e.,\nP(C) = arg min X∈R(B) ||X − C||2, (18)\nthere might be other objective function that serves us better such as the Kullback-Leibler distance between the original and the approximated distributions."
    }, {
      "heading" : "3.2 Learning and Inference on Separable Systems",
      "text" : "Separable systems may offer a significant advantage over general DBNs in terms of speed in parameter learning and state inference. Because the number of parameters for such systems is usually much smaller than that for a general DBN, the parameter learning should require much less data and behave much more stably. This benefit has been the key to the application demonstrated in [Basu01].\nFor a general DBN, the task of updating the posterior distribution still requires a computation that is exponential in number of variables in the system, for instance the forward-backward algorithm. Given that separable BNs are defined so that the marginal distributions are sufficient to predict themselves in the future, it seems plausible that there exists some method for the inference tasks with reduced computation."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The author wishes to thank Prof. Benjamin Van Roy (Stanford), Prof. George C. Verghese (MIT), and Prof. Sandip Roy (Washington State) for many helpful discussions."
    } ],
    "references" : [ {
      "title" : "Influence Model: A Tractable Representations for the Dynamics of Networked Markov Chains, Ph.D",
      "author" : [ "C. Asavathiratham" ],
      "venue" : null,
      "citeRegEx" : "Asavathiratham.,? \\Q2000\\E",
      "shortCiteRegEx" : "Asavathiratham.",
      "year" : 2000
    }, {
      "title" : "The Influence Model, IEEE Control Systems",
      "author" : [ "C. Asavathiratham", "S. Roy", "B.C. Lesieutre", "G.C. Verghese" ],
      "venue" : null,
      "citeRegEx" : "Asavathiratham et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Asavathiratham et al\\.",
      "year" : 2001
    }, {
      "title" : "Towards Measuring Human Interactions in Conversational Settings",
      "author" : [ "S. Basu", "T. Choudhury", "B. Clarkson", "A. Pentland" ],
      "venue" : "Proceedings of the IEEE Int’l Workshop on Cues in Communication (CUES",
      "citeRegEx" : "Basu et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Basu et al\\.",
      "year" : 2001
    }, {
      "title" : "Sufficiency, Separability, and Temporal Probabilistic Models. Uncertainty in Artificial Intelligence",
      "author" : [ "A. Pfeffer" ],
      "venue" : null,
      "citeRegEx" : "Pfeffer,? \\Q2001\\E",
      "shortCiteRegEx" : "Pfeffer",
      "year" : 2001
    } ],
    "referenceMentions" : [ ],
    "year" : 2006,
    "abstractText" : "Separable Bayesian Networks, or the Influence Model, are dynamic Bayesian Networks in which the conditional probability distribution can be separated into a function of only the marginal distribution of a node’s parents, instead of the joint distributions. We describe the connection between an arbitrary Conditional Probability Table (CPT) and separable systems using linear algebra. We give an alternate proof to [Pfeffer00] on the equivalence of sufficiency and separability. We present a computational method for testing whether a given CPT is separable.",
    "creator" : "dvips(k) 5.94a Copyright 2003 Radical Eye Software"
  }
}