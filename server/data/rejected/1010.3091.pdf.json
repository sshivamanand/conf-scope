{
  "name" : "1010.3091.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Near–Optimal Bayesian Active Learning with Noisy Observations",
    "authors" : [ "Daniel Golovin", "Andreas Krause" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "How should we perform experiments to determine the most accurate scientific theory among competing candidates, or choose among expensive medical procedures to accurately determine a patient’s condition, or select which labels to obtain in order to determine the hypothesis that minimizes generalization error? In all these applications, we have to sequentially select among a set of noisy, expensive observations (outcomes of experiments, medical tests, expert labels) in order to determine which hypothesis (theory, diagnosis, classifier) is most accurate. This fundamental problem has been studied in a number of areas, including statistics [16], decision theory [12], machine learning [18, 7] and others. One way to formalize such active learning problems is Bayesian experimental design [6], where one assumes a prior on the hypotheses, as well as probabilistic assumptions on the outcomes of tests. The goal then is to determine the correct hypothesis while minimizing the cost of the experimentation. Unfortunately, finding this optimal policy is not just NP-hard, but also hard to approximate [5]. Several heuristic approaches have been proposed that perform well in some applications, but do not carry theoretical guarantees (e.g., [17]). In the case where observations are noise-free1, a simple algorithm, generalized binary search2(GBS) run on a modified prior, is guaranteed to be competitive with the optimal policy; the expected number of queries is a factor of O(log n) (where n is the number of hypotheses) more than that of the optimal policy [14], which matches lower bounds up to constant factors [5].\nThe important case of noisy observations, however, as present in most applications, is much less well understood. While there are some recent positive results in understanding the label complexity of noisy active learning [18, 1], to our knowledge, so far there are no algorithms that are provably\n1This case is known as the Optimal Decision Tree (ODT) problem. 2GBS greedily selects tests to maximize, in expectation over the test outcomes, the prior probability mass of eliminated hypotheses (i.e., those with zero posterior probability, computed w.r.t. the observed test outcomes).\nar X\niv :1\n01 0.\n30 91\nv1 [\ncs .L\nG ]\n1 5\nO ct\n2 01\ncompetitive with the optimal sequential policy, except in very restricted settings [15]. In this paper, we introduce a general formulation of Bayesian active learning with noisy observations that we call the Equivalence Class Determination problem. We show that, perhaps surprisingly, generalized binary search performs poorly in this setting, as do greedily (myopically) maximizing the information gain (measured w.r.t. the distribution on equivalence classes) or the decision-theoretic value of information. This motivates us to introduce a novel active learning criterion, and use it to develop a greedy active learning algorithm called the Equivalence Class Edge Cutting algorithm (EC2), whose expected cost is competitive to that of the optimal policy. Our key insight is that our new objective function satisfies adaptive submodularity [9], a natural diminishing returns property that generalizes the classical notion of submodularity to adaptive policies. Our results also allow us to relax the common assumption that the outcomes of the tests are conditionally independent given the true hypothesis. We also develop the Efficient Edge Cutting approXimate objective algorithm (EFFECXTIVE), an efficient approximation to EC2, and evaluate it on a Bayesian experimental design problem intended to tease apart competing theories on how people make decisions under uncertainty, including Expected Value [21], Prospect Theory [13], Mean-Variance-Skewness [11] and Constant Relative Risk Aversion [19]. In our experiments, EFFECXTIVE typically outperforms existing experimental design criteria such as information gain, uncertainty sampling, GBS, and decision-theoretic value of information. Our results from human subject experiments further reveal that EFFECXTIVE can be used as a real-time tool to classify people according to the economic theory that best describes their behaviour in financial decision-making, and reveal some interesting heterogeneity in the population."
    }, {
      "heading" : "2 Bayesian Active Learning in the Noiseless Case",
      "text" : "In the Bayesian active learning problem, we would like to distinguish among a given set of hypotheses H = {h1, . . . , hn} by performing tests from a set T = {1, . . . , N} of possible tests. Running test t incurs a cost of c(t) and produces an outcome from a finite set of outcomes X = {1, 2, . . . , `}. We let H denote the random variable which equals the true hypothesis, and model the outcome of each test t by a random variable Xt taking values in X . We denote the observed outcome of test t by xt. We further suppose we have a prior distribution P modeling our assumptions on the joint probability P (H,X1, . . . , XN ) over the hypotheses and test outcomes. In the noiseless case, we assume that the outcome of each test is deterministic given the true hypothesis, i.e., for each h ∈ H, P (X1, . . . , XN | H = h) is a deterministic distribution. Thus, each hypothesis h is associated with a particular vector of test outcomes. We assume, w.l.o.g., that no two hypotheses lead to the same outcomes for all tests. Thus, if we perform all tests, we can uniquely determine the true hypothesis. However in most applications we will wish to avoid performing every possible test, as this is prohibitively expensive. Our goal is to find an adaptive policy for running tests that allows us to determine the value of H while minimizing the cost of the tests performed. Formally, a policy π (also called a conditional plan) is a partial mapping π from partial observation vectors xA to tests, specifying which test to run next (or whether we should stop testing) for any observation vector xA. Hereby, xA ∈ XA is a vector of outcomes indexed by a set of tests A ⊆ T that we have performed so far 3 (e.g., the set of labeled examples in active learning, or outcomes of a set of medical tests that we ran). After having made observations xA, we can rule out inconsistent hypotheses. We denote the set of hypotheses consistent with event Λ (often called the version space associated with Λ) by V(Λ) := {h ∈ H : P (h | Λ) > 0}. We call a policy feasible if it is guaranteed to uniquely determine the correct hypothesis. That is, upon termination with observation xA, it must hold that |V(xA)| = 1. We can define the expected cost of a policy π by\nc(π) := ∑ h P (h)c(T (π, h))\nwhere T (π, h) ⊆ T is the set of tests run by policy π in case H = h. Our goal is to find a feasible policy π∗ of minimum expected cost, i.e., π∗ = arg min {c(π) : π is feasible} (2.1) A policy π can be naturally represented as a decision tree Tπ , and thus problem (2.1) is often called the Optimal Decision Tree (ODT) problem.\nUnfortunately, obtaining an approximate policy π for which c(π) ≤ c(π∗) · o(log(n)) is NP-hard [5]. Hence, various heuristics are employed to solve the Optimal Decision Tree problem and its variants.\n3Formally we also require that (xt)t∈B ∈ dom(π) and A ⊆ B, implies (xt)t∈A ∈ dom(π) (c.f., [9]).\nTwo of the most popular heuristics are to select tests greedily to maximize the information gain (IG) conditioned on previous test outcomes, and generalized binary search (GBS). Both heuristics are greedy, and after having made observations xA will select\nt∗ = arg max t∈T ∆Alg (t |xA) /c(t),\nwhere Alg ∈ {IG,GBS}. Here, ∆IG (t |xA) := H (XT | xA) − Ext∼Xt|xA [H (XT |xA, xt)] is the marginal information gain measured with respect to the Shannon entropy H (X) := Ex[− log2 P (x)], and ∆GBS (t |xA) := P (V(xA)) − ∑ x∈X P (Xt = x | xA)P (V(xA, Xt = x)) is the expected reduction in version space probability mass. Thus, both heuristics greedily chooses the test that maximizes the benefit-cost ratio, measured with respect to their particular benefit functions. They stop after running a set of tests A such that |V(xA)| = 1, i.e., once the true hypothesis has been uniquely determined.\nIt turns out that for the (noiseless) Optimal Decision Tree problem, these two heuristics are equivalent [22], as can be proved using the chain rule of entropy. Interestingly, despite its myopic nature GBS has been shown [14, 7, 10, 9] to obtain near-optimal expected cost: the strongest known bound is c(πGBS) ≤ c(π∗) (ln(1/pmin) + 1) where pmin := minh∈H P (h). Let xS(h) be the unique vector xS ∈ XS such that P (xS | h) = 1. The result above is proved by exploiting the fact that fGBS(S, h) := 1 − P (V(xS(h))) + P (h) is adaptive submodular and strongly adaptively monotone [9]. Call xA a subvector of xB if A ⊆ B and P (xB | xA) > 0. In this case we write xA ≺ xB. A function f : 2T ×H is called adaptive submodular w.r.t. a distribution P , if for any xA ≺ xB and any test t it holds that ∆(t |xA) ≥ ∆(t |xB), where\n∆(t |xA) := EH [f(A ∪ {t} , H)− f(A, H) | xA] .\nThus, f is adaptive submodular if the expected marginal benefits ∆(t |xA) of adding a new test t can only decrease as we gather more observations. f is called strongly adaptively monotone w.r.t. P if, informally, “observations never hurt” with respect to the expected reward. Formally, for all A, all t /∈ A, and all x ∈ X we require EH [f(A, H) | xA] ≤ EH [f(A ∪ {t} , H) | xA, Xt = x] .\nThe performance guarantee for GBS follows from the following general result about the greedy algorithm for adaptive submodular functions (applied with Q = 1 and η = pmin): Theorem 1 (Theorem 10 of [9] with α = 1). Suppose f : 2T ×H → R≥0 is adaptive submodular and strongly adaptively monotone with respect to P and there exists Q such that f(T , h) = Q for all h. Let η be any value such that f(S, h) > Q−η implies f(S, h) = Q for all sets S and hypotheses h. Then for self–certifying instances the adaptive greedy policy π satisfies c(π) ≤ c(π∗) ( ln ( Q η ) + 1 ) .\nThe technical requirement that instances be self–certifying means that the policy will have proof that it has obtained the maximum possible objective value, Q, immediately upon doing so. It is not difficult to show that this is the case with the instances we consider in this paper. We refer the interested reader to [9] for more detail.\nIn the following sections, we will use the concept of adaptive submodularity to provide the first approximation guarantees for Bayesian active learning with noisy observations.\n3 The Equivalence Class Determination Problem and the EC2 Algorithm We now wish to consider the Bayesian active learning problem where tests can have noisy outcomes. Our general strategy is to reduce the problem of noisy observations to the noiseless setting. To gain intuition, consider a simple model where tests have binary outcomes, and we know that the outcome of exactly one test, chosen uniformly at random unbeknown to us, is flipped. If any pair of hypotheses h 6= h′ differs by the outcome of at least three tests, we can still uniquely determine the correct hypothesis after running all tests. In this case we can reduce the noisy active learning problem to the noiseless setting by, for each hypothesis, creating N “noisy” copies, each obtained by flipping the outcome of one of the N tests. The modified prior P ′ would then assign mass P ′(h′) = P (h)/N to each noisy copy h′ of h. The conditional distribution P ′(XT | h′) is still deterministic (obtained by flipping the outcome of one of the tests). Thus, each hypothesis hi in the original problem is now associated with a setHi of hypotheses in the modified problem instance. However, instead of selecting tests to determine which noisy copy has been realized, we only care which setHi is realized.\nThe Equivalence Class Determination problem (ECD). More generally, we introduce the Equivalence Class Determination problem4, where our set of hypothesesH is partitioned into a set of m equivalence classes {H1, . . . ,Hm} so thatH = ⊎m i=1Hi, and the goal is to determine which class Hi the true hypothesis lies in. Formally, upon termination with observations xA we require that V(xA) ⊆ Hi for some i. As with the ODT problem, the goal is to minimize the expected cost of the tests, where the expectation is taken over the true hypothesis sampled from P . In §4, we will show how the Equivalence Class Determination problem arises naturally from Bayesian experimental design problems in probabilistic models.\nGiven the fact that GBS performs near-optimally on the Optimal Decision Tree problem, a natural approach to solving ECD would be to run GBS until the termination condition is met. Unfortunately, and perhaps surprisingly, GBS can perform very poorly on the ECD problem. Consider an instance with a uniform prior over n hypotheses, h1, . . . , hn, and two equivalence classesH1 := {hi : 1 ≤ i < n} andH2 := {hn}. There are tests T = {1, . . . , n} such that hi(t) = 1[i = t], all of unit cost. Hereby, 1[Λ] is the indicator variable for event Λ. In this case, the optimal policy only needs to select test n, however GBS may select tests 1, 2, . . . , n in order until running test t, where H = ht is the true hypothesis. Given our uniform prior, it takes n/2 tests in expectation until this happens, so that GBS pays, in expectation, n/2 times the optimal expected cost in this instance.\nThe poor performance of GBS in this instance may be attributed to its lack of consideration for the equivalence classes. Another natural heuristic would be to run the greedy information gain policy, only with the entropy measured with respect to the probability distribution on equivalence classes rather than hypotheses. Call this policy πIG. It is clearly aware of the equivalence classes, as it adaptively and myopically selects tests to reduce the uncertainty of the realized class, measured w.r.t. the Shannon entropy. However, we can show there are instances in which it pays Ω(n/ log(n)) times the optimal cost, even under a uniform prior. Refer to Appendix B for details.\nThe EC2 algorithm. The reason why GBS fails is because reducing the version space mass does not necessarily facilitate differentiation among the classesHi. The reason why πIG fails is that there are complementarities among tests; a set of tests can be far better than the sum of its parts. Thus, we would like to optimize an objective function that encourages differentiation among classes, but lacks complementarities. We adopt a very elegant idea from Dasgupta [8], and define weighted edges between hypotheses that we aim to distinguish between. However, instead of introducing edges between arbitrary pairs of hypotheses (as done in [8]), we only introduce edges between hypotheses in different classes. Tests will allow us to cut edges inconsistent with their outcomes, and we aim to eliminate all inconsistent edges while minimizing the expected cost incurred. We now formalize this intuition.\nSpecifically, we define a set of edges E = ∪1≤i<j≤m {{h, h′} : h ∈ Hi, h′ ∈ Hj}, consisting of all (unordered) pairs of hypotheses belonging to distinct classes. These are the edges that must be cut, by which we mean for any edge {h, h′} ∈ E , at least one hypothesis in {h, h′} must be ruled out (i.e., eliminated from the version space). Hence, a test t run under true hypothesis h is said to cut edges Et (h) := {{h′, h′′} : h′(t) 6= h(t) or h′′(t) 6= h(t)}. See Fig. 1(a) for an illustration. We define a weight function w : E → R≥0 by w({h, h′}) := P (h) · P (h′). We extend the weight function to an additive (modular) function on sets of edges in the natural manner, i.e., w(E ′) := ∑ e∈E′ w(e). The objective fEC that we will greedily maximize is then defined as the weight of the edges cut (EC):\nfEC(A, h) := w (⋃ t∈A Et (h) ) (3.1)\nThe key insight that allows us to prove approximation guarantees for fEC is that fEC shares the same beneficial properties that make fGBS amenable to efficient greedy optimization. We prove this fact, as stated in Proposition 2, in Appendix A.\nProposition 2. The objective fEC is strongly adaptively monotone and adaptively submodular.\nBased on the objective fEC , we can calculate the marginal benefits for test t upon observations xA as\n∆EC (t |xA) := EH [fEC(A ∪ {t} , H)− fEC(A, H) | xA] .\n4Bellala et al. simultaneously studied ECD [2], and, like us, used it to model active learning with noise [3]. They developed an extension of GBS for ECD. We defer a detailed comparison of our approaches to future work.\nWe call the adaptive policy πEC that, after observing xA, greedily selects test t∗ ∈ arg maxt ∆EC (t |xA) /c(t), the EC2 algorithm (for equivalence class edge cutting).\nNote that these instances are self–certifying, because we obtain maximum objective value if and only if the version space lies within an equivalence class, and the policy can certify this condition when it holds. So we can apply Theorem 1 to show EC2 obtains a ln(Q/η) + 1 approximation to Equivalence Class Determination. Hereby, Q = w(E) = 1 − ∑ i(P (h ∈ Hi))2 ≤ 1 is the total weight of all edges that need to be cut, and η = mine∈E w(e) ≥ p2min is a bound on the minimum weight among all edges. We have the following result:\nTheorem 3. Suppose P (h) is rational for all h ∈ H. For the adaptive greedy policy πEC implemented by EC2 it holds that\nc(πEC) ≤ (2 ln(1/pmin) + 1)c(π∗),\nwhere pmin := minh∈H P (h) is the minimum prior probability of any hypothesis, and π∗ is the optimal policy for the Equivalence Class Determination problem.\nIn the case of unit cost tests, we can apply a technique of Kosaraju et al. [14], originally developed for the GBS algorithm, to improve the approximation guarantee to O(log n) by applying EC2 with a modified prior distribution. We defer details to the full version of this paper.\nA Fast Implementation of EC2. The time running time of EC2 is dominated by the evaluations of ∆EC (t |xA). The naive way to compute ∆EC (t |xA) is to construct a graph on the n hypotheses with weighted edges as prescribed by EC2, and then see which edges are cut by t for each potential test outcome. Assuming there are ` possible outcomes of a test, and that we can evaluate h(t) in unit time for all h and t, this will takeO(n2`) time. With N tests, the total time per round of EC2 isO(Nn2`). However, there is a much faster way to compute ∆EC (t |xA). Note that ∆EC (t |xA) equals\nExt∼Xt|xA 1 2 ∑ i 6=j ( P (Hi ∩ V(xA))P (Hj ∩ V(xA))− P (Hi ∩ V(xA, xt))P (Hj ∩ V(xA, xt)) ) . Now, compute α(i, xt) := P (Hi ∩ V(xA, xt)) for each i and x, then compute β(i) := P (Hi) =∑ x α(i, x). Next, compute γ(xt) := P (xt | xA) = ∑ i α(i, xt)/ ∑ i β(i). All of these terms can be computed in total time O(n) by iterating over the hypotheses and for each h adding P (h) to the appropriate terms (i.e., β(i), α(i, x), and γ(x) if h ∈ Hi and h(t) = x). Using these variables, we can rewrite ∆EC (t |xA) as Ext∼Xt|xA [ 1 2 ∑ i 6=j ( β(i)β(j)− α(i, xt)α(j, xt) )] . Note that for any\nη1, η2, . . . , ηm ∈ R, we have ∑ i 6=j ηiηj = ( ∑ i ηi) 2 − ∑ i η 2 i . Using this equality, we can evaluate\nsums like ∑ i 6=j ( β(i)β(j)−α(i, xt)α(j, xt) ) inO(m) time, where there are m equivalence classes. Hence the total time to evaluate ∆EC (t |xA) is O(n+m`) using this method. In a similar manner, we can reduce the running time still further to O(n), by incrementally computing terms such as ( ∑ i α(i, x)) 2 and ∑ i α(i, x)\n2 as we iterate through the hypotheses. The total time per round of EC2 is then O (Nn). Additionally, the number of evaluations the algorithm needs to make can often be significantly reduced in practice using the accelerated adaptive greedy algorithm, as discussed in [9].\n4 Bayesian Active Learning with Noise and the EFFECXTIVE Algorithm We now address the case of noisy observations, using ideas from §3. With noisy observations, the conditional distribution P (X1, . . . , XN | h) is no longer deterministic. We model the noise using an additional random variable Θ. Fig. 1(b) depicts the underlying graphical model. The vector of test outcomes xT is assumed to be an arbitrary, deterministic function xT : H× supp(Θ)→ XN ; hence XT | h is distributed as xT (h,Θh) where Θh is distributed as P (θ | h). For example, there might be up to s = | supp(Θ)| ways any particular disease could manifest itself, with different patients with the same disease suffering from different symptoms.\nIn cases where it is always possible to identify the true hypothesis, i.e., xT (h, θ) 6= xT (h′, θ′) for all h 6= h′ and all θ, θ′ ∈ supp(Θ), we can reduce the problem to Equivalence Class Determination with hypotheses {xT (h, θ) : h ∈ H, θ ∈ supp(Θ)} and equivalence classes Hi := {xT (hi, θ) : θ ∈ supp(Θ)} for all i. Then Theorem 3 immediately yields that the approximation factor of EC2 is at most 2 ln (1/minh,θ P (h, θ)) + 1, where the minimum is taken over all (h, θ) in the support of P . In the unit cost case, running EC2 with a modified prior à la Kosaraju et al. [14] allows us to obtain an O(log |H|+ log | supp(Θ)|) approximation factor. Note this model allows us to incorporate noise with complex correlations.\nHowever, a major challenge when dealing with noisy observations is that it is not always possible to distinguish distinct hypotheses. Even after we have run all tests, there will generally still be uncertainty about the true hypothesis, i.e., the posterior distribution P (H | xT ) obtained using Bayes’ rule may still assign non-zero probability to more than one hypothesis. If so, uniquely determining the true hypothesis is not possible. Instead, we imagine that there is a set D of possible decisions we may make after (adaptively) selecting a set of tests to perform and we must choose one (e.g., we must decide how to treat the medical patient, which scientific theory to adopt, or which classifier to use, given our observations). Thus our goal is to gather data to make effective decisions [12]. Formally, for any decision d ∈ D we take, and each realized hypothesis h, we incur some loss `(d, h). Decision theory recommends, after observing xA, to choose the decision d∗ that minimizes the risk, i.e., the expected loss, namely d∗ ∈ arg mind EH [`(d,H) | xA].\nA natural goal in Bayesian active learning is thus to adaptively pick observations, until we are guaranteed to make the same decision (and thus incur the same expected loss) that we would have made had we run all tests. Thus, we can reduce the noisy Bayesian active learning problem to the ECD problem by defining the equivalence classes over all test outcomes that lead to the same minimum risk decision. Hence, for each decision d ∈ D, we define\nHd := {xT : d = arg min d′ EH [`(d′, H) | xT ]}. (4.1)\nIf multiple decisions minimize the risk for a particular xT , we break ties arbitrarily. Identifying the best decision d ∈ D then amounts to identifying which equivalence classHd contains the realized vector of outcomes, which is an instance of ECD.\nOne common approach to this problem is to myopically pick tests maximizing the decision-theoretic value of information (VoI): ∆VoI (t |xA) := mind EH [`(d,H) | xA] − Ext∼Xt|xA [mind EH [`(d,H) | xA, xt]]. The VoI of a test t is the expected reduction in the expected loss of the best decision due to the observation of xt. However, we can show there are instances in which such a policy pays Ω(n/ log(n)) times the optimal cost, even under a uniform prior on (h, θ) and with | supp(Θ)| = 2. Refer to Appendix B for details. In contrast, on such instances EC2 obtains an O(log n) approximation. More generally, we have the following result for EC2 as an immediate consequence of Theorem 3.\nTheorem 4. Fix hypotheses H, tests T with costs c(t) and outcomes in X , decision set D, and loss function `. Fix a prior P (H,Θ) and a function xT : H × supp(Θ) → XN which define the probabilistic noise model. Let c(π) denote the expected cost of π incurs to find the best decision, i.e., to identify which equivalence classHd the outcome vector xT belongs to. Let π∗ denote the policy minimizing c(·), and let πEC denote the adaptive policy implemented by EC2. Then it holds that\nc(πEC) ≤ ( 2 ln ( 1\np′min\n) + 1 ) c(π∗),\nwhere p′min := minh∈H {P (h, θ) : P (h, θ) > 0}.\nIf all tests have unit cost, by using a modified prior [14] the approximation factor can be improved to O (log |H|+ log | supp(Θ)|) as in the case of Theorem 3.\nThe EFFECXTIVE algorithm. For some noise models, Θ may have exponentially–large support. In this case reducing Bayesian active learning with noise to Equivalence Class Determination results in instances with exponentially-large equivalence classes. This makes running EC2 on them challenging, since explicitly keeping track of the equivalence classes is impractical. To overcome this challenge, we develop EFFECXTIVE, a particularly efficient algorithm which approximates EC2.\nFor clarity, we only consider the 0−1 loss, i.e., our goal is to find the most likely hypothesis (MAP estimate) given all the data xT , namely h∗(xT ) := arg maxh P (h | xT ). Recall definition (4.1), and consider the weight of edges between distinct equivalence classesHi andHj : w(Hi×Hj) =\n∑ xT ∈Hi,x′T ∈Hj P (xT )P (x ′ T )= ( ∑ xT ∈Hi P (xT ) )( ∑ x′T ∈Hj P (x′T ) ) = P (XT ∈ Hi)P (XT ∈ Hj).\nIn general, P (XT ∈ Hi) can be estimated to arbitrary accuracy using a rejection sampling approach with bounded sample complexity. We defer details to a longer version of the paper. Here, we focus on the case where, upon observing all tests, the hypothesis is uniquely determined, i.e., P (H | xT ) is deterministic for all xT in the support of P . In this case, it holds that P (XT ∈ Hi) = P (H = hi). Thus, the total weight is∑\ni 6=j\nw(Hi ×Hj) = (∑\ni P (hi) )2 − ∑ i P (hi) 2 = 1− ∑ i P (hi) 2.\nThis insight motivates us to use the objective function ∆Eff (t |xA) := [∑ x P (Xt = x | xA) (∑ i P (hi | xA, Xt = x)2 )] − ∑ i P (hi | xA)2,\nwhich is the expected reduction in weight from the prior to the posterior distribution. Note that the weight of a distribution 1 − ∑ i P (hi)\n2 is a monotonically increasing function of the Rényi entropy (of order 2), which is − 12 log ∑ i P (hi)\n2. Thus the objective ∆Eff can be interpreted as a (non-standard) information gain in terms of the (exponentiated) Rényi entropy. In our experiments, we show that this criterion performs well in comparison to existing experimental design criteria, including the classical Shannon information gain. Computing ∆Eff (t |xA) requires us to perform one inference task for each outcome x of Xt, and O(n) computations to calculate the weight for each outcome. We call the algorithm that greedily optimizes ∆Eff the EFFECXTIVE algorithm (since it uses an Efficient Edge Cutting approXimate objective), and present pseudocode in Algorithm 1.\nInput: Set of hypothesesH; Set of tests T ; prior distribution P ; function f . begin A ← ∅; while ∃h 6= h′ : P (h | xA) > 0 and P (h′ | xA) > 0 do\nforeach t ∈ T \\ A do ∆Eff (t |xA) := [∑ xP (Xt = x | xA) (∑ iP (hi | xA, Xt = x)2 )] − ∑ i P (hi | xA)2;\nSelect t∗ ∈ arg maxt ∆Eff (t |xA) /c(t); Set A ← A∪ {t∗} and observe outcome xt∗ ; end Algorithm 1: The EFFECXTIVE algorithm using the Efficient Edge Cutting approXimate objective."
    }, {
      "heading" : "5 Experiments",
      "text" : "Several economic theories make claims to explain how people make decisions when the payoffs are uncertain. Here we use human subject experiments to compare four key theories proposed in literature. The uncertainty of the payoff in a given situation is represented by a lottery L, which is simply a random variable with a range of payoffs L := {`1, . . . , `k}. For our purposes, a payoff is an integer denoting how many dollars you receive (or lose, if the payoff is negative). Fix lottery L, and let pi := P [L = `i]. The four theories posit distinct utility functions, with agents preferring larger utility lotteries. Three of the theories have associated parameters. The Expected Value\ntheory [21] posits simply UEV (L) = E [L], and has no parameters. Prospect theory [13] posits UPT (L) = ∑ i f(`i)w(pi) for nonlinear functions f(`i) = ` ρ i , if `i ≥ 0 and f(`i) = −λ(−`i)ρ, if `i < 0, and w(pi) = e−(log(1/pi)) α\n[20]. The parameters ΘPT = {ρ, λ, α} represent risk aversion, loss aversion and probability weighing factor respectively. For Portfolio optimization problems, financial economists have used value functions that give weights to different moments of the lottery [11]: UMV S(L) = wµµ−wσσ+wνν, where ΘMV S = {wµ, wσ, wν} are the weights for the mean, standard deviation and standardized skewness of the lottery respectively. In Constant Relative Risk Aversion theory [19], there is a parameter ΘCRRA = a representing the level of risk aversion, and the utility posited is UCRRA(L) = ∑ i pi` 1−a i /(1− a) if a 6= 1, and UCRRA(L) = ∑ i pi log(`i), if a = 1.\nThe goal is to adaptively select a sequence of tests to present to a human subject in order to distinguish which of the four theories best explains the subject’s responses. Here a test t is a pair of lotteries, (Lt1, L t 2). Based on the theory that represents behaviour, one of the lotteries would be preferred to the other, denoted by a binary response xt ∈ {1, 2}. The possible payoffs were fixed to L = {−10, 0, 10} (in dollars), and the distribution (p1, p2, p3) over the payoffs was varied, where pi ∈ {0.01, 0.99} ∪ {0.1, 0.2, . . . , 0.9}. By considering all non-identical pairs of such lotteries, we obtained the set of possible tests.\nWe compare five different methods: EFFECXTIVE, greedily maximizing Information Gain (IG), Uncertainty Sampling5 (US), minimizing Version Space (VS), and tests selected at Random. We first evaluated the ability of the algorithms to recover the true model based on simulated responses. We chose parameter values for the theories, such that they made distinct predictions, and were consistent with the values proposed in literature [13]. We drew 1000 samples of the true model and fixed the parameters of the model to some canonical values, ΘPT = {0.9, 2.2, 0.9},ΘMV S = {0.8, 0.25, 0.25},ΘCRRA = 1. Responses were generated using a softmax function, with the probability of response xt = 1 given by P (xt = 1) = 1/(1 + eU(L t 2)−U(L t 1)). Fig. 2(a) shows the performance of the 5 methods, in terms of the accuracy of recovering the true model with the number of tests. We find that US and VS both perform significantly worse than Random in the presence of noise. EFFECXTIVE outperforms InfoGain significantly, which outperforms Random.\nWe also considered uncertainty in the values of the parameters, by setting ρ from 0.85-0.95, λ from 2.1-2.3, α from 0.9-1; wµ from 0.8-1.0, wσ from 0.2-0.3, wν from 0.2-0.3; and a from 0.9-1.0, all with 3 values per parameter. We generated 500 random samples by first randomly sampling a model and then randomly sampling parameter values. EFFECXTIVE and InfoGain outperformed Random significantly, Fig. 2(b), although InfoGain did marginally better among the two. The increased parameter range potentially poses model identifiability issues, and violates some of the assumptions behind EFFECXTIVE, decreasing its performance to the level of InfoGain.\nAfter obtaining informed consent, we tested 11 human subjects to determine which model fit their behaviour best. Laboratory experiments have been used previously to distinguish economic theories, [4], and here we used a real-time, dynamically optimized experiment that required fewer tests. Subjects were presented 30 tests using EFFECXTIVE. To incentivise the subjects, one of these tests\n5Uncertainty sampling greedily selects the test whose outcome distribution has maximum Shannon entropy.\nwas picked at random, and subjects received payment based the outcome of their chosen lottery. The behavior of most subjects (7 out of 10) was best described by EV. This is not unexpected given the unusually high quantitative abilities of the subjects. We also found heterogeneity in classification: One subject got classified as MVS, as identified by her violations of stochastic dominance in the last few tests. 2 subjects were best described by prospect theory since they exhibited a high degree of loss aversion and risk aversion. One subject was also classified as a CRRA-type (log-utility maximizer). Figure 2(c) shows the probability of the classified model with number of tests. Although we need a bigger and more diverse sample to make significant claims of the validity of different economic theories, our preliminary results indicate that subject types can be identified and there is heterogeneity in the population. It also serves as a practical application of real-time dynamic experimental design optimization that is necessary to collect data on human economic behavior."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we considered the problem of adaptively selecting which noisy tests to perform in order to identify an unknown hypothesis sampled from a known prior distribution. We studied the Equivalence Class Determination problem as a means to reduce the case of noisy observations to the classic, noiseless case. We introduced EC2, an adaptive greedy algorithm that is guaranteed to choose the same hypothesis as if it had observed the outcome of all tests, and incurs near-minimal expected cost among all policies with this guarantee. This is in contrast to popular heuristics that are greedy w.r.t. version space mass reduction, information gain or value of information, all of which we show can be very far from optimal. EC2 works by greedily optimizing an objective tailored to differentiate between sets of observations that lead to different decisions. Our bounds rely on the fact that this objective function is adaptive submodular. We also develop EFFECXTIVE, a practical algorithm based on EC2, that can be applied to arbitrary probabilistic models in which efficient exact inference is possible. We apply EFFECXTIVE to a Bayesian experimental design problem, and our results indicate its effectiveness in comparison to existing algorithms. We believe that our results provide an interesting direction towards providing a theoretical foundation for practical active learning and experimental design problems.\nAcknowledgments. This research was partially supported by ONR grant N00014-09-1-1044, NSF grant CNS-0932392, NSF grant IIS-0953413, a gift by Microsoft Corporation, an Okawa Foundation Research Grant, and by the Caltech Center for the Mathematics of Information."
    }, {
      "heading" : "A Additional Proofs",
      "text" : "Lemma 5. The objective function f of Eq. (3.1) is strongly adaptive monotone.\nProof. We must show that for all xA, t /∈ A and possible answer x for test t that EH [f(A, H) | xA] ≤ EH [f(A ∪ {t} , H) | xA, Xt = x] (A.1)\nTowards this end, it is useful to notice that for all t ∈ T the function h 7→ Et (h) depends only on Xt. Hence for any xA, the function h 7→ f(A, h) is constant over realizations xT xA, so we can define a function g(xA) such that g(xA) = EH [f(A, H) | xA] by g(xA) := w (⋃ t∈A Et (xt) ) where xA = (xt)t∈A and Et (x) is the set of edges cut by t if Xt = x. Note that for all xA ≺ xB we have g(xA) ≤ g(xB), since the edge weights are nonnegative. Setting B = A ∪ {t} yields Eq. (A.1) and hence implies strong adaptive monotonicity.\nLemma 6. The objective function f of Eq. (3.1) is adaptive submodular for any prior with rational values.\nProof. We first prove the result assuming a uniform prior P (·), and then show how to reduce the general prior case to the uniform prior case. Hence all edges have weight 1/n2, where there are n hypotheses. For convenience, we also rescale our units of reward so that all edges have unit weight. (Note that f is adaptive submodular iff cf is for any constant c > 0.) To prove adaptive submodularity, we must show that for all xA ≺ xB and t ∈ T , we have ∆(t |xB) ≤ ∆(t |xA). Fix t and xA, and let V(xA) := {h : P (h | xA) > 0} denote the version space, if xA encodes the observed outcomes. Let nV := |V(xA)| be the number of hypotheses in the version space. Likewise, let ni,a(xA) := | {h : h ∈ V(xA, Xt = a) ∩Hi} |, and let na(xA) := ∑m i=1 ni,a(xA). Also, define ea(xA) := 1 2 ∑ i 6=j ∑ b 6=a ni,b(xA) · nj,b(xA) to be the number of edges cut such that at t both hypotheses agree with each other but disagree with the realized hypothesis h∗, conditioning on Xt = a. We define a function θ of these quantities such that ∆(t |xA) = θ(n(xA), e(xA)), where n(xA) is the vector consisting of ni,a(xA) for all i and a and e(xA) is the vector consisting of ea(xA) for all a. For brevity, we suppress the dependence of xA where it is unambiguous. Then, as we will explain below, θ is defined as\nθ(n, e) := 1\n2 ∑ i6=j ∑ a 6=b ni,a · nj,b + ∑ a ea ( 1− na nV ) (A.2)\nHere, i and j range over all class indices, and a and b range over all possible outcomes of test t. The first term on the right-hand side counts the number of edges that will be cut by selecting test t no matter what the outcome of t is. Such edges consist of hypotheses that disagree with each other at t and, as with all edges, lie in different classes. The second term counts the expected number of edges cut by t consisting of hypotheses that agree with each other at t. Such edges will be cut by t iff they disagree with h∗ at t. The edges {h, h′} with h, h′ ∈ V(xA) and P (Xt = a | h) = P (Xt = a | h′) = 1 (of which there are ea) will be cut by t iff Xt 6= a. Since we assume a uniform prior, P [Xt 6= a | xA] = 1− na/nV for any partial realization xA with t /∈ A, hence the expected contribution of these edges to ∆(t |xA) is ea (1− na/nV), from whence we get the second term.\nNow fix xB xA. Our strategy for proving ∆(t |xB) ≤ ∆(t |xA) is as follows. As more observations are made, the version space can only shrink, i.e. V(xB) ⊆ V(xA). This means that for all i and a, ni,a is nonincreasing, i.e., ni,a(xB) ≤ ni,a(xA). Note we may interpret ea as a function of the variables in {ni,a : 1 ≤ i ≤ m, a ∈ X}, and that it is nondecreasing in each ni,a, so we may also deduce that ea(xB) ≤ ea(xA) for all a. Hence we consider a parameterized path p(τ) in R(m+1)·` from p(0) := (n(xB), e(xB)) to p(1) := (n(xA), e(xA)). Then by integrating along the path we obtain\n∆(t |xA)−∆(t |xB) = ∫ 1 τ=0 ( d(θ◦p) dτ ) dτ . (A.3)\nWe require that at each point in p it holds that ea = 12 ∑ i6=j ∑ b6=a ni,b ·nj,b for all a, and also ensure that p is nondecreasing in each coordinate. There exists a path meeting these requirements, since (n(xB), e(xB)) ≤ (n(xA), e(xA)) and each ea is nondecreasing in each ni,b variable. This implies ∂ni,a/∂τ ≥ 0 and ∂ea/∂τ ≥ 0 for all i and a. Hence we can prove the integral is nonnegative by applying the chain rule for the derivative to obtain\nd(θ◦p) dτ = ∑ i,a ∂θ ∂ni,a ∂ni,a ∂τ + ∑ a ∂θ ∂ea ∂ea ∂τ\nand then proving that ∂θ/∂ni,a ≥ 0 and ∂θ/∂ea ≥ 0 for all i and a. Next, observe that ∂θ/∂ea = (1− na/nV) ≥ 0. So fix a class index k and an outcome c and consider ∂θ/∂nk,c. Elementary calculus tells us that\n∂θ\n∂nk,c = ∑ j 6=k, b 6=c nj,b + ∑ b ebnb n2V − ec nV\n(A.4)\nThis quantity is nonnegative iff nVec ≤ n2V · ∑\nj 6=k, b 6=c nj,b + ∑ b ebnb (A.5)\nNow substitute 12 ∑ i 6=j ∑ b 6=c ni,b · nj,b for ec to obtain\nnVec = nV 2 ∑ i6=j ∑ b6=c ni,b ·nj,b ≤ nV ·  ∑ j 6=k, b 6=c nj,b ∑ i,a ni,a  ≤ n2V ·  ∑ j 6=k, b 6=c nj,b  (A.6) Since ∑ b ebnb ≥ 0, we obtain Eq. (A.5) from Eq. (A.6) by inspection, and hence ∂θ/∂nk,c ≥ 0 for all k and c. This completes the proof of the adaptive submodularity of f under a uniform prior.\nWe now show how to reduce the general prior case to the uniform prior case. Fix any prior P with rational probabilities, i.e. P (h) ∈ Q for all h. Then there exists d ∈ N and function k : H → N such that such that P (h) = k(h)/d. Create a new instance containing d hypotheses, where for each h ∈ H there are k(h) copies of h, denoted by h1, . . . , hk(h). Each copy of h induces the same conditional distribution of test outcomes P (X1, . . . , XN | h). All copies of h belong to the same class, and copies of h and h′ belong to the same class iff h and h′ do. Finally, assign a uniform prior to this new instance. Then the adaptive submodularity of f on this new instance implies the adaptive submodularity on the original instance, if the weight of edge {h, h′} in the original instance is proportional to the number of edges between the copies of h and the copies of h′ in the new instance. That is, it suffices to set w({h, h′}) ∝ k(h) · k(h′), and our choice of weight function, w({h, h′}) := P (h) · P (h′), satisfies this condition."
    }, {
      "heading" : "B A Bad Example for the Info-Gain and Value of Information Criteria",
      "text" : "A popular heuristic for the Optimal Decision Tree problem are to adaptively greedily select the test that maximizes the information gain in the distribution over hypotheses, conditioned on all previous test outcomes. The same heuristic can be applied to the Equivalence Class Determination problem, in which we compute the information gain with respect to the entropy of the distribution over classes rather than hypotheses. Let πIG denote the resulting policy for Equivalence Class Determination.\nAnother common heuristic for Optimal Decision Tree is to adaptively greedily select the test maximizing the Bayesian decision-theoretic value of information (VoI) criterion. Recall the value of information of a test t is the expected reduction in the expected risk of the minimum risk decision, where the risk is the expected loss. Formally, consider the Bayesian decision-theoretic setup described in §4. The VoI criterion myopically selects test to maximize\n∆VoI (t |xA) := min d EH [`(d,H) | xA]− Ext∼Xt|xA [ min d EH [`(d,H) | xA, xt] ] .\nThis heuristic can be also be applied to the Equivalence Class Determination problem, by taking the decision set D to be the set of equivalence classes, and the loss function to be the 0–1 classification loss function, i.e., `(d,H) = 1[H /∈ d]. Let πVoI denote the resulting policy.\nIn this section we present a family of Equivalence Class Determination instances for which both πIG and πVoI perform significantly worse than the optimal policy.\nTheorem 7. There exists a family of Equivalence Class Determination instances with uniform priors such that c(πIG) = Ω (n/ log(n)) c(π∗) and c(πVoI) = Ω (n/ log(n)) c(π∗), where n is the number of hypotheses and π∗ is an optimal policy.\nIn fact, we will prove a lower bound for each policy within a large family of adaptive greedy policies which contains πIG and πVoI, which we call posterior–based. Informally, this family consists of all greedily policies that use only information about the posterior equivalence class distribution to select the next test. More precisely, these policies define a potential function Φ which maps distributions of distributions over equivalence classes to real numbers, and at each time step select the test t which maximizes the Φ of the posterior distribution (over test outcomes xt) of the posterior distribution over equivalence classes generated by adding xt to the previously seen test outcomes. In the event of a tie, we select any test maximizing this quantity at random. The information gain policy is posterior–based; Φ is simply −1 times the expected entropy of the posterior equivalence-class distribution. Likewise, the value of information policy is also posterior–based; Φ is simply −1 times the expected loss of the best action for the posterior equivalence-class distribution. Hence to prove Theorem 7 it suffices to prove the following more general theorem.\nTheorem 8. There exists a family of Equivalence Class Determination instances with uniform priors such that c(π) = Ω (n/ log(n)) c(π∗) for any posterior–based policy π, where n is the number of hypotheses and π∗ is an optimal policy.\nProof. Fix integer parameter q ≥ 1. There are m = 2q classes Ha for each 1 ≤ a ≤ 2q. Each Ha consists of two hypotheses, ha,0 and ha,1. We call a the index of Ha. The prior is uniform over the hypotheses H = {ha,v : 1 ≤ a ≤ m, 0 ≤ v ≤ 1}. There are four types of tests, all with binary outcomes and all of unit cost. There is only one test of the first type, t0, which tells us the value of v in the realized hypothesis h∗a,v. Hence for all a, H = ha,v ⇒ Xt0 = v. Tests of the second type are designed to help us quickly discover the index of the realized class via binary search if we have already run t0, but to offer no information gain whatsoever if t0 has not yet been run. There is one such test tk for all t with 1 ≤ k ≤ q. For z ∈ N, let φk (z) denote the kth least-significant bit of the binary encoding of z, so that z = ∑∞ k=1 2\nk−1φk (z). Then for each ha,v we have H = ha,v ⇒ Xtk = 1[φk (a) = v]. Tests of the third type are designed to allow us to do a (comparatively slow) sequential search on the index of the realized class. Specifically, we have tests tseqk for all 1 ≤ k ≤ m, such that H = ha,v ⇒ Xtseqk = 1[a = k]. Finally, tests of the fourth type,{ tdumbk : k ∈ N } , are dummy tests that reveal no information at all. Formally, Xtdumbk always equals zero.\nGiven this input, suppose H = ha,v. One solution is to run t0 to find v, then run tests t1, . . . , tq to determine φk (a) for all 1 ≤ k ≤ q and hence to determine a. This reveals the value of H , and hence the class H belongs to. Since the tests have unit cost, this policy π′ has cost c(π′) = q + 1.\nNext, fix a posterior–based policy π and consider what it will do. Call a class possible if not all of its hypotheses have been ruled out by tests performed so far. Note that all possible classes contain the same number of hypotheses, because they initially have two, and each test tk that can reduce the size of a possible class to one, will reduce the size of every possible class to one. This, and the fact that the prior is uniform, implies that the posterior equivalence-class distribution is uniform over the remaining possible classes. If no tests in {tk : 0 ≤ k ≤ q} have been run, as is initially the case, any single test in this set will not change the posterior equivalence-class distribution. Hence, as measured with respect to Φ, such tests are precisely as good as the dummy tests. If these tests are each better than any test in { tseqk : 1 ≤ k ≤ m } , then π selects among {tk : 0 ≤ k ≤ q} ∪ { tdumbk : k ∈ N } at random. Since there are infinitely many dummy tests, with probability one a dummy test is selected. Since the posterior remains the same, π will repeatedly select a test at random from this set, resulting in an infinite loop as dummy tests are selected repeatedly ad infinitum. Otherwise, some test tseqk is preferable to the other tests, measured with respect to Φ. In the likely event that t is not the index of H , we are left with a residual problem in which tests in {tk : 0 ≤ k ≤ q} still have no effect on the posterior, there is one less class, and the prior is again uniform. Hence our previous argument still applies, and π will either enter an infinite loop or will repeatedly select tests in { tseqk : 1 ≤ k ≤ m\n} until a test has an outcome of 1. Thus in expectation π costs at least c(π) ≥ 1m ∑m z=1 z = (m+ 1)/2. Since m = 2q , n = 2m, and c(π∗) ≤ c(π′) = q + 1 = log2(n) we infer\nc(π) ≥ m 2 =\n( n\n4 log2(n)\n) c(π∗)\nwhich completes the proof."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We tackle the fundamental problem of Bayesian active learning with noise, where<lb>we need to adaptively select from a number of expensive tests in order to identify<lb>an unknown hypothesis sampled from a known prior distribution. In the case of<lb>noise–free observations, a greedy algorithm called generalized binary search (GBS)<lb>is known to perform near–optimally. We show that if the observations are noisy,<lb>perhaps surprisingly, GBS can perform very poorly. We develop EC, a novel,<lb>greedy active learning algorithm and prove that it is competitive with the optimal<lb>policy, thus obtaining the first competitiveness guarantees for Bayesian active learn-<lb>ing with noisy observations. Our bounds rely on a recently discovered diminishing<lb>returns property called adaptive submodularity, generalizing the classical notion<lb>of submodular set functions to adaptive policies. Our results hold even if the tests<lb>have non–uniform cost and their noise is correlated. We also propose EFFECX-<lb>TIVE, a particularly fast approximation of EC, and evaluate it on a Bayesian<lb>experimental design problem involving human subjects, intended to tease apart<lb>competing economic theories of how people make decisions under uncertainty.",
    "creator" : "TeX"
  }
}