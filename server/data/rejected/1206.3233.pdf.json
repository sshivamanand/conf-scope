{
  "name" : "1206.3233.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Speeding Up Planning in Markov Decision Processes via Automatically Constructed Abstractions",
    "authors" : [ "Alejandro Isaza" ],
    "emails" : [ "isaza@cs.ualberta.ca", "szepesva@cs.ualberta.ca", "bulitko@cs.ualberta.ca", "greiner@cs.ualberta.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, we consider planning in stochastic shortest path (SSP) problems, a subclass of Markov Decision Problems (MDP). We focus on medium-size problems whose state space can be fully enumerated. This problem has numerous important applications, such as navigation and planning under uncertainty. We propose a new approach for constructing a multi-level hierarchy of progressively simpler abstractions of the original problem. Once computed, the hierarchy can be used to speed up planning by first finding a policy for the most abstract level and then recursively refining it into a solution to the original problem. This approach is fully automated and delivers a speed-up of two orders of magnitude over a state-of-the-art MDP solver on sample problems while returning near-optimal solutions. We also prove theoretical bounds on the loss of solution optimality resulting from the use of abstractions."
    }, {
      "heading" : "1 Introduction and Motivation",
      "text" : "We focus on planning in stochastic shortest path problems (the problem of reaching some goal state under uncertainty) when planning time is critical — a situation that arises, for instance, in path planning for agents in commercial video games, where map congestions are modeled as uncertainty of transitions. Another example is path planning for multi-link robotic manipulators, where the uncertainty comes from unmodeled dynamics as well as sensor and actuator noise. More specifically, we consider the problem of finding optimal policies in a sequence of stochastic shortest-path problems (Bertsekas & Tsitsiklis, 1996), where the problems share the same dynamics and transition costs, and differ only in the location of the goal-state.\nWhen the state space underlying the problems is sufficiently large, exact planning methods are unable to deliver a solution within the required time, forcing the user to resort to approximate methods in order to scale to large domains. Exploiting the fact that multiple planning\nproblems share the same dynamics and transition costs, we build an abstracted representation of the shared structure where planning is faster, then map the individual planning problem into the abstract space and derive a solution there. The solution is then refined back into the original space.\nIn a related problem of path planning under real-time constraints in deterministic environments (e.g., Sturtevant, 2007), a particularly successful approach is implemented in the PR LRTS algorithm (Bulitko, Sturtevant, Lu, & Yau, 2007), which builds an abstract state space by partitioning the set of states into cliques (i.e., each state within each cluster is connected to each other state in that cluster with a single action). Each such cluster becomes a single abstract state. Two abstract states are connected by an abstract transition if there is a pair of non-abstract states (one from each abstract state) connected by a single action. The resulting abstract space is smaller and simpler, yet captures some of the structure of the original search problem. Thus, an abstract solution can be used to guide and constrain the search in the original problem, yielding a significant speed-up. Further speed-ups can be obtained by building abstractions on top of abstractions, which creates a hierarchy of progressively smaller abstract search spaces. PR LRTS can then be tuned to meet strict real-time constraints while minimizing solution suboptimality.\nNote that state cliques produced by PR LRTS make good abstract states because landing anywhere in such a cluster puts the agent a single action away from any other state in the clique. This also means that the costs of the resulting actions are similar, and that the cost of a single action is negligible compared with the cost of a typical path. Finally, any (optimal) path in the original problem can be closely approximated at the abstract level, as an agent following an (optimal) path has to traverse from cluster to cluster. Since all neighboring clusters are connected in the abstract problem, it is always possible to find a path in the abstract problem that is “close” to the original path.\nGiven the attractive properties or PR LRTS, it is natural to ask whether the ideas underlying it can be extended to stochastic shortest path problems, with arbitrary cost structures.\nIn a stochastic problem, the result of planning is a closed loop policy that assigns actions to states. A successful ab-\nstraction must be suitable for approximating the execution trace of an optimal policy. Imagine that clustering has been done in some way. The idea is again to have abstract actions that connect neighboring clusters cheaply — that is, the system should not produce expensive connections.\nIntuitively, we want to connect one cluster to another if, from any state of the first cluster, we can reliably get to some state of the second cluster at roughly a fixed cost (the same for any state in the first cluster). This way, “simulating” a policy of the original problem becomes possible at a small additional cost (the meaning of simulation will become clear later). This means that a connection between clusters is implemented by a policy with a specific set of initial states that brings the agent from any state of the source cluster to some state of the target cluster. We will use options (Sutton, Precup, & Singh, 1999) for such policies, and choose clusters to allow such policies for any two neighboring clusters. Thus, it is natural to look for clusters of states that allow one to reliably simulate any trajectory from any of the states to any other state.\nFinally, we need an extra mechanism, the “goal approach”, that deals with the challenge of reaching the base-level goal itself from states that are close to the goal. Thus, our planner first plans in the abstract space to reach the “goal cluster”. After arriving at some state of the “goal approach region”, the planner then uses the “goal-approach policy” that, with high probability, moves the agent to the goal state itself. These ideas form the core of our algorithm.\nThe three major contributions of the paper are: (i) a novel theoretical analysis of option-based abstractions, (ii) an effective algorithm for constructing high-quality option-based abstractions, and (iii) experimental results demonstrating that our algorithm performs effectively over a range of problems of varying size and difficulty.\nSection 2 formally describes our problem, and provides the theoretical underpinning of our approach. Section 3 then presents our algorithm for automatically building options-based abstractions, and Section 4, our planning algorithm that uses these abstractions. Section 5 empirically evaluates this approach, in terms of both efficiency and effectiveness (suboptimality). Finally, Section 6 summarizes related work."
    }, {
      "heading" : "2 Problem Formulation and Theory",
      "text" : "This section formally defines stochastic shortest path problems and the abstractions that we will consider. It also presents a theoretical result that characterizes the relationship between the performance of abstract policies and policies of the original problem.\nDefinition 1 A Markov Decision Process (MDP) is defined by a finite state space X = {1, . . . , n}; a finite set of actions A(x) for each state x ∈ X; transition probabilities p(y|x, a) ∈ [0, 1] that correspond to the probability that the next state is y when action a is applied in state x; immediate cost c(x, a, y) ∈ ℜ for all x, y ∈ X and all a ∈ A(x) and a discount factor γ ∈ (0, 1].\nThe MDP is undiscounted if γ = 1. An action a ∈ ∪x∈XA(x) is called admissible in state x if a ∈ A(x).\nDefinition 2 A (generic) policy is a mapping that assigns to each history (x0, a0, c0, . . . , xt−1, at−1, ct−1, xt) an action admissible in the most recent state xt. In general, a mapping that maps possible histories to some set is called a history dependent mapping.\nUnder mild conditions, it suffices to consider only stationary, deterministic policies (Bertsekas & Tsitsiklis, 1996), on which we will focus:\nDefinition 3 A stationary and deterministic policy π is a mapping of states to actions such that π(x) ∈ A(x) holds for any state x ∈ X .\nIn what follows, we will use “policy” to mean stationary and deterministic policies, unless otherwise mentioned.\nThe expected cost of policy π when the system starts in state x0 is vπ(x0) = E [ ∑∞ t=0 γ\ntc(Xt, π(Xt),Xt+1)] whereXt is a Markov chain with P (Xt+1 = y|Xt = x) = p(y|x, π(x)). The function vπ is called the value-function underlying policy π.\nOne “solves” an MDP by finding a policy that minimizes the cost from every state, simultaneously. In this paper we deal only with stochastic shortest path problems, a subclass of MDPs. In these MDPs the problem is to get to a goal state with the least cost:\nDefinition 4 A finite stochastic shortest path (SSP) problem is a finite undiscounted MDP that has a special state, called the goal state g, such that ∀a ∈ A(g), we have p(g|g, a) = 1 and c(g, a, g) = 0 and the immediate costs for all the other transitions are positive.\nConsider a finite SSP (X,A, p, c). Let π be a stationary policy. We say that this policy is proper if it reaches the goal state g with probability one, regardless of the initial state. Let Tπ : R\nX → RX be the policy’s evaluation operator:\n(Tπv)(x) = ∑\ny∈X\np(y|x, π(x)) [c(x, π(x), y) + v(y)] .\nBertsekas and Tsitsiklis (1996) prove that Tπ is a contraction with respect to a weighted maximum norm, ‖·‖w,∞, with some positive weights, w ∈ RX+ , where ‖v‖w,∞ = maxx |v(x)|/w(x). In particular, w(x) can be chosen to be the expected number of steps until π reaches the goal state when started from x. The contraction coefficient of Tπ , γπ , satisfies 1/(1 − γπ) = maxx∈X w(x). Thus, 1/(1− γπ) is the maximum of the expected number of steps to reach the goal state, or in other words, the maximum expected time policy π spends in the MDP (cf. Prop 2.2 in Bertsekas & Tsitsiklis, 1996).\nWe adopt the notion of options from Sutton et al. (1999):\nDefinition 5 An option is a triple (π, I, ψ), where I ⊂ X is the set of initial states, π is a (generic) policy that is\ndefined for histories that start with a state in I and ψ is a history dependent mapping with range {0, 1}, called the terminating condition. We say that the terminating condition fires when ψ(ht) = 1. Let T > 0 denote the random time when the terminating condition fires for the first time while following π. (Note that T = 0 is not allowed.) We assume that P (T < +∞) = 1, independent of the initial state when the policy π is started (i.e., the option terminates in finite time with probability one).\nAs suggested in the introduction, an abstraction is a way to group states and the abstract actions correspond to options:\nDefinition 6 We say that the MDP (X̃ , Ã, p̃, c̃) is an option-based abstraction of (X,A, p, c), if there exists a mapping, S : X̃ → 2X specifying the states S(x̃) ⊂ X that correspond to an abstract state x̃ ∈ X̃ , a set Π of options abstracting the actions of the MDP and a mapping Ψ : ∪x̃∈X̃ Ã(x̃) → Π such that for any ã ∈ Ã(x̃), if Ψ(ã) = (I, π, ψ) then S(x̃) ⊂ I .1\nHenceforth we will use “abstraction” instead of “optionbased abstraction” and will call (X̃ , Ã, p̃, c̃) the “abstract MDP”, X̃ the set of abstract states, Ã the set of abstract actions, etc. Notationally, we call (X,A, p, c) the ground level MDP, and we will identify quantities related to the\nabstract MDP by using a tilde (˜ ). For simplicity, we will identify the abstract actions with their corresponding options. In particular, we will call ã both an abstract action and an option, depending on the context.\nIn the following, we will assume that {S(x̃) | x̃ ∈ X̃} is a partition of X; we can then let x̃ : X → X̃ denote the (unique) abstract state that includes x: x̃(x) ∈ X̃ such that x ∈ S(x̃(x)), and say that (X̃ , S) is an aggregation of the states in X . We also define S(x) = S(x̃(x)) as the set of states in X that are in the same partition with x.\nThe restriction on Ψ in the above definition ensures that the execution of any policy π̃ in the abstract MDP is well-defined and proceeds as follows. Initially, there is no active option. In general, whenever there is no active option, we look up the abstract state x̃ = x̃(x) based on the current state x and activate the option Ψ(π̃(x̃)). When there is an active option, the option remains active until the corresponding terminating condition fires. When an option is active, the option’s policy selects the actions in the ground level MDP. This way a policy π̃ in the abstract MDP induces a policy in the ground level MDP.\nOur goal now is to characterize what makes an abstraction accurate. The following theoretical analysis is novel as it considers abstractions where the action set is changed. In particular, the action set can potentially be reduced and the abstract actions can be options. To our knowledge, such options-based abstractions have not been analyzed previously; the closest results are probably Theorem 2 of Kim and Dean (2003) and Theorem 4 of Dean, Givan, and Leach (1997). The proof is rather technical and is given\n12X denotes the power set of X: the set of all subsets of X .\nin the extended version of our paper (Isaza, Szepesvári, Bulitko, & Greiner, 2008).\nConsider a proper policy π of the ground level MDP. We want abstractions such that one can always find a policy in the abstract MDP (X̃ , Ã, p̃, c̃) that approximates π well, no matter how π was chosen. Clearly, this depends on how the action set Ã and the corresponding transitions and costs are defined in the abstract MDP. Quantifying this requires a few definitions: Let p̃π(x̃, ỹ) be the probability of landing in some state of S(ỹ) when following policy π until it leaves the states of S(x̃), when the initial state is selected at random from the states of S(x̃) based on the distribution µS(x̃). Let c̃π(x̃) denote the corresponding expected “immediate” cost. Now pick a proper policy π̃ of the abstract MDP. Let w̃ be the weight vector that makes Tπ̃ a contraction in the abstract MDP. Further, define p̃π̃(x̃, ỹ) = p̃(ỹ|x̃, π̃(x̃)) and c̃π̃(x̃) = ∑ ỹ∈X̃ p̃π̃(x̃, ỹ)c̃π̃(x̃, ỹ) and the mixed ℓ1/ℓ∞ norm ‖·‖w̃,1/∞:\n‖p̃1 − p̃2‖w̃,1/∞ = max x̃∈X̃\n∑\nỹ∈X̃\n|p̃1(x̃, ỹ)− p̃2(x̃, ỹ)| w̃(ỹ)\nw̃(x̃) .\nLet\nεπ,π̃ = ‖c̃π − c̃π̃‖w̃,∞ + cmax ‖p̃π − p̃π̃‖w̃,1/∞ , (1)\nwhere cmax is the maximum of the immediate costs in the ground level MDP. Hence, επ,π̃ measures how well the costs and the transition probabilities induced by π “after state aggregation” match those of π̃. Introduce c(x, π) as the expected total cost incurred, conditioned on that policy π starting in state x and stopping when it exits S(x). Further, introduce p(ỹ|x, π) as the probability that, given that policy π is started in state x, when it exits S(x) it enters S(ỹ) (ỹ 6= x̃(x)). Now fix an abstract state x̃ ∈ X̃ . If the costs {c(x, π)}x∈S(x̃) and probabilities {p(ỹ|x, π)}x∈S(x̃), ỹ 6= x̃, have a small range then we can model closely the behavior of π locally at S(x̃) by introducing an option with initial states in S(x̃) which mimics the “expected” behavior of π as it leaves S(x̃), assuming, say, that the initial state in S(x̃) is selected at random according to the distribution µS(x̃). If we do so for all abstract states x̃ ∈ X̃ then we can make sure that minπ επ,π̃ is small. If the above range conditions hold for all policies π of the ground level MDP and all abstract states x̃ ∈ X̃ then by introducing a sufficiently large number of abstract actions it is possible to keep maxπ minπ̃ επ,π̃ small. Further, notice that maxπ p(ỹ|x, π) is zero unless there exists a transition from some state of S(x) to some state of S(ỹ), in which case we say that S(x) is connected to S(ỹ). Hence, no abstract action is needed “between” x̃ and ỹ, unless S(x̃) is connected in the ground level MDP to S(ỹ).\nDefine T̃π : B(X̃) → B(X̃), (T̃π ṽ)(x̃) = c̃π(x̃) + ∑ ỹ p̃π̃(x̃, ỹ)ṽ(ỹ). Since π is proper in the ground level MDP, it is not difficult to show that T̃π is a contraction with respect to an appropriately defined weighted supremum norm.\nThe next result gives a bound on the difference of value functions of π and π̃ in terms of επ,π̃:\nTheorem 1 Let π be a proper policy in the ground level MDP and let π̃ be a proper policy in the abstract MDP. Let wπ (resp., w̃π) be the weight vector that makes Tπ (resp., T̃π) a contraction and let the corresponding contraction factor be γπ (resp., γ̃π). Let vπ be the value function of π and ṽπ̃ be the value function of π̃. Then\n‖vπ − Eṽπ̃‖w,∞ ≤ ‖Avπ − vπ‖w,∞\n1− γπ + λπ επ,π̃ 1− γ̃π ,\nwhere the operator E extends functions defined over X̃ to functions defined over X in a piecewise constant manner: E : B(X̃ ) → B(X), (Eṽ)(x) = ṽ(x̃(x)), and A : B(X) → B(X) is the aggregation operator defined by\n(AV )(x) = ∑\nz∈S(x)\nµS(x)(z)V (z),\nand λπ = maxx∈X w̃π(x̃(x))/wπ(x).\nThe factor λπ measures how many more steps are needed to reach the goal if the execution of policy π is modified such that, whenever the policy enters a new cluster x̃, the state gets perturbed, by choosing a random state according to µS(x̃).\nThe theorem provides a bound on the difference between the value function of a ground-level policy π and the value function of an abstract policy when its value function is extended to the ground-level states. The bound has two terms: The first bounds the loss due to state abstraction, while the second bounds the loss due to action abstraction. When a similar range condition holds for the abstract actions, too, then it is possible to bound the difference between the value function of the policy induced in the ground level MDP by π̃ and Eṽπ̃ , yielding a difference on the value functions of π and the policy induced by π̃. Isaza et al. (2008) provides further details.\nIf we apply this result to an optimal policy π∗ of the ground level MDP, we immediately get a bound on the quality of the abstraction. We may conclude then that the quality of abstraction is determined by the following factors: (i) whether states with different optimal values are aggregated; (ii) whether the random perturbation described in the previous paragraph can increase the number of steps to the goal substantially; and (iii) whether the immediate costs c̃π∗ and transition probabilities p̃π∗ can be matched in the abstract MDP.\nSince we want to build abstractions that work independently of where the goal is placed, the knowledge of the optimal policy with respect to a particular goal cannot be exploited when constructing the abstractions. In order to prevent large errors due to (i) and (ii), we restrict aggregation such that only a few states are grouped together. This makes the job of creating an aggregation easier. Fortunately, we can achieve higher compression by adding additional layers of abstractions. We can address (iii) by creating a sufficiently large number of abstract actions. Here, we use the simplifying assumption that we only create abstract actions that bring the agent from some cluster of states to some neighboring cluster. These can\nserve as a “basis” for matching any complex next-state distribution over the clusters by choosing an appropriate stochastic policy in the abstract MDP. We also want to ensure that the initial state within a cluster has a small influence on the probability of transitioning to some neighboring cluster and the associated costs. We use two constants, ε and µ, to bound the amount of variation with respect to initial states; note this allows us to control the difference between the value function of a policy induced in the ground level MDP by some abstract policy π̃ and the extension of the value function of π̃ defined in the abstract MDP to the ground level states, Eṽπ̃ . This is necessary to ensure that a good policy in the abstract MDP produces a good policy in the ground-level MDP, ultimately assuring that the optimal policy of the abstract MDP will give rise to a close to optimal policy in the ground-level MDP. The resulting procedure is described in the next section."
    }, {
      "heading" : "3 Abstracting an SSP",
      "text" : "This section describes our algorithm BuildAbstraction for automatically building options-based abstractions. These abstractions are goal-independent and thus apply to a series of SSPs that share the state space and transition dynamics. The process consists of four main steps (Figure 1): (1) Cluster proposes candidates for abstract states; (2) GenerateLinkCandidates proposes candidates for abstract actions (or “links”); (3) Repair validates and, if necessary, repairs the links in order to satisfy the so-called (ε, µ)-connectivity property (the formal definition is given later) and Prune discards excessive links.\nOnce an abstraction is built, we use a special-purpose planning procedure (described in Section 4) to solve specific SSPs. The rest of this section describes the four steps of our BuildAbstraction algorithm in detail.\nStep 1: Cluster. A straightforward cluster-er will cluster a state with some of its immediate neighbors. Unfortunately, this approach may group states with diverging trajectories (the trajectories from one state can differ from those of the other state). By looking for the peers of a state (predecessors of its successors, line 2, Figure 1) we hope to find a peer whose trajectories are similar to the trajectories of the first state. Note that the clustering routine creates minimal clusters. This is advantageous as it means the subsequent steps, which connect clusters, is more likely to succeed. Unfortunately, it also means relatively low reduction in the number of states. Several layers of abstractions can help increase this reduction.\nStep 2: Generate Link Candidates. After forming the initial clusters (i.e., the initial abstract states), BuildAbstraction generates candidates for abstract actions. One approach is simply to propose abstract actions for all pairs of abstract states, in the hope that only important ones will remain after pruning. We use a less expensive strategy and propose abstract action candidates only for “nearby” clusters (line 8). For each such pair we add two candidate links: one in the forward and another in the backward direction— this heuristic quickly generates reasonable link candidates. We typically use k = 1. Our experiments confirm this is\nsufficient; increasing k results in slightly better quality, but slower running times when solving the planning problems.\nStep 3: Repair. For each candidate abstract action connecting abstract states x̃ and ỹ, we first need to derive an option that, starting in any state in cluster x̃ leads the agent to some state in cluster ỹ with a minimum total expected cost. We derive this option by setting up a shortest path problem S, whose domain includes S(x̃) and S(ỹ). We set the domain of S to be sufficiently large that a policy within this domain can reliably take the agent from any state of S(x̃) to some state of S(ỹ). BuildAbstraction builds this domain by performing a breadth-first search from S(ỹ), proceeding backwards along the transitions, stopping at depth D +m, whereD is the search depth from S(ỹ) andm is the margin to leave after all states of S(x̃) were added to the domain. If there is any state of S(x̃) that was not included at depth D, the Repair routine reports ‘no solution’. The transitions, actions and costs of S are inherited from the MDPM . We also add a new terminating state, which is the destination\n2Here IS is the characteristic function of S: IS(x) = 1 iff x ∈ S and IS(x) = 0 otherwise.\nof transitions leaving the region — i.e., those transitions are redirected to this new terminal, with a transition cost that exceeds the maximum of the total expected costs of the ground level MDP. The high cost discourages the solutions to enter the extra terminating state. The optimal solution to S is obtained by using the Improved Prioritized Sweeping (IPS) algorithm of McMahan and Gordon (2005), (line 14). We selected this algorithm based on its excellent performance and known optimality properties (IPS reduces to Dijkstra’s method in deterministic problems). The resulting policy π is checked against (ε, µ)-connectivity, defined as follows: we first compute the expected total cost of reaching some states in S(ỹ) for all states of S(x̃); let the resulting costs be c(x, π). Similarly, we compute the probabilities p(S(ỹ)|x, π) for every x ∈ S(x̃). Then we check if maxx,x′∈S(x̃) |c(x, π) − c(x′, π)| ≤ ε and maxx,x′∈S(x̃) |p(S(ỹ)|x, π) − p(S(ỹ)|x′, π)| ≤ µ both hold. If these constraints are met, a new abstract action is created and is added to the set of admissible actions at x̃ and the policy is stored as the option corresponding to this new abstract action (lines 20–22). Otherwise, the cluster is split (since every cluster has two states, this is trivial) and the appropriate link candidates are added to the repair queue so that no link between potentially connected clusters is missed.\nStep 4: Prune. After step 3, we have an abstract SSP whose abstract states are (ε, µ)-connected. However, our abstract action generation mechanism may produce too many actions, which may slow down the planning algorithm (see Section 4). We address this problem using a pruning step that leaves only the “critical” and cheapest abstract actions. An action is “critical” if it connects clusters that are connected at the ground level with a single transition; these actions are important to keep the structure of the ground level MDP. We also keep the cheapest abstract actions as they are likely to help achieve high quality solutions. The “pruning parameter”, p, specifies the total number of actions to keep. (If p is smaller or equal than the number of ground actions, then only the “critical” actions are kept.)\nBuildAbstraction runs in time linear in the size of the input MDP, as every step is restricted to some fixed size neighborhood of some state (i.e., every step is local). Further, employing a suitable data structure, the memory requirements can also be kept linear in the size of the input. These properties are important when scaling up to realistic, real-world problem sizes."
    }, {
      "heading" : "4 Planning with an Abstraction",
      "text" : "After building an abstraction, we can use it to solve particular SSP problems. When we specify a new goal, our abstraction planner, AbsPlanner, then creates a goal-approach region in the abstract MDP that includes the goal and is large enough to include all states of the cluster containing the goal. AbsPlanner builds this region by starting with the ground goal and adding states and transitions in a breadth-first fashion to a certain depth, proceeding backwards along the transitions, stopping only after adding all states of the goal-cluster. After building the region, AbsPlanner produces an SSP. The domain of this\nSSP includes the states found in the breadth-first search, and also a new terminal state that becomes the destination of transitions leaving the region — i.e., those transitions are redirected to this new terminal, with a high transition cost. All other costs and transitions of this SSP are inherited from the ground level MDP. AbsPlanner uses IPS to solve the local MDP, and saves the resulting goal-approach policy. It then solves the abstract MDP, where the goal cluster is set as the goal. When executing the resulting policy π̃, AbsPlanner proceeds normally until reaching a state of the goal-approach region; it then switches to the goal-approach policy, which it follows until reaching the goal or leaving the region. When this latter event happens and the state is x, execution switches to the option π̃(x̃(x)).\nWhen using multiple levels of abstraction, AbsPlanner’s execution follows a recursive, hierarchical strategy. Note that the size of the goal-approach region is independent of the size of the MDP. Thus, the planning time will depend on the size of the top-level abstract MDP. For an MDP of size n, by using log n levels of hierarchy, in theory it is then possible to achieve planning times that scale withO(log n). However, depending on the problem, it might be hard to guarantee high quality solutions when using many levels of abstraction. Furthermore, in practice (over the problems used in our tests), the computation time is dominated by the time needed to set up and solve the goal-approach SSPs, which is required for even one layer of abstraction. This is partly because our abstractions result in deterministic shortest path problems, whose solutions can be found significantly faster than those of stochastic problems."
    }, {
      "heading" : "5 Empirical Evaluation",
      "text" : "This section summarizes our empirical evaluation of this approach, in terms of the quality (suboptimality) of the solutions and the solution times. Here we report the tradeoffs of using different levels of abstraction as well as the dependence on the “stochasticity” of the transitions. (Note that stochasticity makes it difficult to build abstractions.) We also tested the performance of the algorithm on more practical problems. In addition to the results presented here, we conducted extensive experiments, studying the trade off between solution quality and solution time as a function of the various parameters of our algorithm (e.g., the values of p, k, or the number of abstraction levels), the scaling behavior of our algorithm in terms of its resource usage, the quality of solutions and the solution time. These results, appearing in (Isaza et al., 2008), confirm that the algorithm is robust to the choices of its parameters and scales as expected by increasing problem sizes.\nWe run experiments over three domains: noisy gridworlds, a “river” and congested game maps. The gridworlds are empty and have four actions: up, down, left, and right, each with cost 1. The probability that an action leaded to the expected position (e.g., the action up moves the agent up one cell) is 0.7, while the probability of reaching any of the other three adjacent cells is 0.1.\nThe river is similar to the gridworld: its dimensions are w × h, but there is a current flowing from left to right\nand a fork corresponding to a line connecting the points (w/2, h/2) and (w, h/2).3 The flow is represented by modifying both the cost structure and the transition probabilities of the actions: action forward costs 1, backward costs 5, diagonally-up-and-forward and diagonally-downand-forward each cost √ 2. These actions are also stochastic: For the backward action, the probabilities are 0.7 for going back and 0.1 for each of the other actions. For the other three actions, the anticipated move occurs with probability 0.6 and the other moves except backwards occur each with probability 0.2, and backwards has probability 0. We include the river domain to determine whether our system can deal with non-uniform structures and because the fork complicates the task of creating abstractions. We empirically found the time to build abstractions for the n-state gridworld was close to n/100 seconds, and around n/50 for an n-state river domain. The build time for the maps, using k = 1, was between 75 and 100 seconds.4\nThe congested game maps are again similar to gridworlds, but with obstacles and with transitions probabilities that depend on the congestion. The obstacle layout comes from commercial game maps, and the stochastic dynamics simulate what happens if multiple units traverse the same map: in narrow passages, the units to become congested, which means an agent trying to traverse such a passage is likely to be blocked. We model this by modifying each action by including a probability that the action will “fail” and cause the agent to stay at the same position. This “failure probability” depends on the position on the game map, calculated by simulating many units randomly traversing the game maps and measuring the average occupation of the individual cells, then turning the occupation numbers into probabilities. The optimal policy of an agent in a congested game map will then try to avoid narrow passages, since the higher probability of traffic congestion in such regions means an agent takes much longer to get through those regions.\nThe baseline performance measures are obtained by running the state-of-the-art SSP solver algorithm IPS. For each study, we generate the abstraction and then use it to solve 1,000 problems, whose start and goal locations are selected uniformly at random. For each problem we measure the solution time in seconds and the total solution cost for both IPS and our method, then compute the geometric average of the individual suboptimalities and the individual solution time ratios."
    }, {
      "heading" : "5.1 Abstraction level trade-offs",
      "text" : "We used a 100× 100 gridworld to analyze the trade-offs of different abstraction levels, with several different parameter configurations. We say a configuration is “dominant” if it was a Pareto optimal — i.e., if no other configuration is better in both time and suboptimality.\nFigure 2 presents properties of the dominant configurations\n3See (Isaza et al., 2008) for more details, including relevant pictures.\n4We ran all experiments on a 2GHz AMD Opteron(tm) Processor with 4GB of RAM running Linux with kernel 2.6.18.\nfor various abstraction levels. We see that using a smaller number of abstractions required more time but produced better solutions (i.e., lower suboptimality), and higher levels of abstractions required less solution time but produced inferior solutions (i.e., increased suboptimality). Note that there are dominant configurations for every level of abstraction, from 0 to 5.\nWe obtain a “level 0” abstraction by converting the given ground-level SSP to deterministic shortest path problem with the same states. (Recall that our abstraction process abstracts the state space and produces a deterministic SSP; here we just used the original state space.) Figure 2 shows that this transformation provides solutions whose quality is slightly inferior to the original problem, but it finds this solution significantly faster (e.g., in 0.005 to 0.0073 of the time). We also see that these “level 0” solutions are superior to those based on higher abstraction levels, but one can obtain these level-i solutions in yet less time."
    }, {
      "heading" : "5.2 Sensitivity to Stochasticity of the Dynamics",
      "text" : "As the environment becomes noisier, it becomes more difficult to construct a high quality abstraction. This section quantifies how the solution quality and construction time relate to noise in the dynamics. In general, we consider an action “successful” if the agent moves to the appropriate direction; our gridworld model set the success probability to P = 0.7, leaving a probability of (1 − P )/3 to moving in each of the other three directions. Here, we vary the value of P . All of these experiments use a 50 × 50 gridworld with k = 2 and p = 4 (which means we keep only the “critical” actions; see Section 3).\nFigure 3 plots the suboptimality and the speed-up of finding a solution using our method, as compared to IPS, for different values of P . We see that our method loses optimality as the dynamics becomes noisier (i.e., when P gets smaller). This is because our abstract actions, trying to move the agent from one abstract state to the next will fail with higher probability for noisier dynamics. Note\nthat the advantage of our method, in terms of planning time, becomes larger with increased stochasticity. This is because our abstractions are deterministic and planning in a deterministic system is much faster than planning with a stochastic system.\nFigure 4, plotting the absolute values of cost and time for both our method and IPS, provides another insight: It shows that for increasing stochasticity both methods are slowed down, but our method can cope better with this situation. This figure also confirms that this leads to a loss in solution quality.\nFor our method the typical parameters produce a suboptimality of around 1.4 for the river, and around 1.25 for the gridworld domain. The speed-up for the gridworld is around 30, while for the river it is around 800."
    }, {
      "heading" : "5.3 Congested Game Maps",
      "text" : "To test the performance of our approach in a more practical application, we used maps modeled after game environments from commercial video games. We first created simplified gridworlds that resemble some levels from a\npopular role-playing and real-time-strategy game. We then converted the gridworlds into congested maps as described earlier. This produced maps with state space sizes of 6176 (BG1), 5672 (BG2), 5852 (BG3), 20249 (WC1) and 9848 (WC2). Figure 5 provides one such map, where each state’s color indicates the associated congestion: warmer/redder colors indicates high congestion (i.e., low probability of success P ) while colder/bluer colors indicates low congestion (i.e., high value of P ). Very dark blue indicates impassable obstacles. We see that many of the states in cluttered regions are highly congested and should therefore be avoided.\nFigure 6 shows the solution time and the solution suboptimalities for both our method and IPS, for two maps from WarCraft (Blizzard Entertainment, 2002) and three maps from Baldur’s Gate (BioWare Corp., 1998), including Figure 5, using only a single layer of abstraction. We see that our approach is indeed successful in speeding up the planning process, while keeping the quality of the resulting solutions high."
    }, {
      "heading" : "6 Related Work",
      "text" : "Due to space constraints we review only the most relevant work; references to other related works can be found in the extensive bibliography lists of the cited works. Dean et al. (1997) introduced the notion of ε-homogeneous partitions and analyzed its properties, but without giving explicit loss bounds. Kim and Dean (2003) developed some loss bounds. Their Theorem 2 can be strengthened with our proof method to ‖v∗ − v∗P ‖∞ ≤ ‖Tv∗P − v∗P ‖∞ /(1 − γ) (using our notation), basically dropping the first term in their bound. Here v∗ is the optimal value function in the original MDP, v∗P is the optimal value function of the aggregated MDP extended back to the original state space in a piecewise constant manner and T is the Bellman-optimality operator in the original MDP. This bound is problematic as it does not show how the quality of\npartitions influences the loss. Our bound improves on this bound in this respect, and also by extending it to the case when the abstract actions correspond to options. While Asadi and Huber (2004) also considered such optionsbased abstractions, they assume that the abstract actions (options) are given externally (possibly by specifying goal states for each of them) and they do not develop bounds. In a number of subsequent papers, the authors refined their methods. In particular, they became increasingly focussed on learning problems. For example, in the recent follow-up work, Asadi and Huber (2007) provide a method to learn an abstract hierarchical representation that uses state aggregation and options. Since they are interested in skill transfer through a series of related problems that can differ in their cost structure, they introduce a heuristic to discover subgoals based on bottleneck states. They learn options for achieving the discovered subgoals and introduce a partitioning that respects the learned options (in the clusters typically there are many states). The success of the approach relies critically on the existence of meaningful bottleneck states. This leads to a granularity issue: identifying the bottleneck states requires computing a statistic for each state visited, meaning bottlenecks will not be pronounced if resolution is increased in narrow pathways. Nevertheless, the approach has been successfully tested in a non-trivial domain of 20,000 states.\nHauskrecht, Meuleau, Kaelbling, Dean, and Boutilier (1998) introduce a method that also uses options, but the abstract states correspond to boundary states of regions. The regions are assumed to be given a priori. The idea is similar to using bottleneck states. In contrast to that work, we do not assume any prior knowledge, but construct the abstractions completely autonomously. Further, we deal with undiscounted SSPs, while Hauskrecht et al. (1998) dealt with discounted MDPs (but this difference is probably not crucial)."
    }, {
      "heading" : "7 Discussion and Future Directions",
      "text" : "In the approach presented, options serve as closed-loop abstract actions. Another way to use an abstract solution would be to use the abstract value function to guide local search initiated from the current state. These ideas has proven successful in pattern-database research where the cost of an optimal solution of an abstract problem is used as a powerful heuristic for the original problem. Such a procedure has the potential to improve solution quality, while keeping low the cost of the planning steps interleaved with execution. Another idea is to use the abstraction to select the amount of such local search (i.e., the depth of the rollouts); these ideas has proven successful in deterministic environments (Bulitko, Björnsson, Luštrek, Schaeffer, & Sigmundarson, 2007; Bulitko, Luštrek, Schaeffer, Björnsson, & Sigmundarson, 2008).\nPresently, our abstractions are deterministic. This suggests two avenues for future work. First, applying advanced heuristic search methods to such abstractions may lead to performance gains. Second, in highly stochastic domains, the abstraction’s determinism may lead to a poor quality of solution, as the cost of ensuring arrival at an abstract\nstate with certainty (or very high probability) can lead to very conservative and costly paths. Thus, it would be of interest to investigate stochastic abstractions. One idea is to modify the way abstract actions are defined: When planning to connect to abstract states after a solution of the local SSP is found, with a little extra work we can compute the probabilities of reaching various neighboring abstract states under the policy found when the policy leaves the region of interest.\nYet another avenue for future work would be to move from a state-based problem formulation to a feature-based one, assuming that the features describe the states. The challenge is to design an algorithm that can construct an abstraction without enumerating all the states, as ours currently does. Although this paper has not attempted to address this problem, we believe that the approach proposed here (i.e., incremental clustering and defining options by solving local planning problems) is applicable.\nFinally, although the present paper dealt only with undiscounted, stochastic shortest path problems, the approach can be extended to work for discounted problems. This holds because a discounted problem can always be viewed as an undiscounted stochastic shortest path problem where every time step a transition is made to some terminal state with probability 1 − γ, where 0 < γ < 1 is the discount factor."
    }, {
      "heading" : "8 Conclusions",
      "text" : "This paper has explored ways to speed up planning in SSP problems via goal-independent state and action abstraction. We strengthen existing theoretical results, then provide an algorithm for building abstraction hierarchies automatically. Finally, we empirically demonstrate the advantages of this approach by showing that it works effectively on SSPs of varying size and difficulty."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We gratefully acknowledge the insightful comments by the reviewers. This research was funded in part by the National Science and Engineering Research Council (NSERC), iCore and the Alberta Ingenuity Fund."
    } ],
    "references" : [ {
      "title" : "State space reduction for hierarchical reinforcement learning",
      "author" : [ "M. Asadi", "M. Huber" ],
      "venue" : "In FLAIRS, pp",
      "citeRegEx" : "Asadi and Huber,? \\Q2004\\E",
      "shortCiteRegEx" : "Asadi and Huber",
      "year" : 2004
    }, {
      "title" : "Effective control knowledge transfer through learning skill and representation hierarchies",
      "author" : [ "M. Asadi", "M. Huber" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Asadi and Huber,? \\Q2007\\E",
      "shortCiteRegEx" : "Asadi and Huber",
      "year" : 2007
    }, {
      "title" : "Baldur’s Gate",
      "author" : [ "BioWare Corp." ],
      "venue" : "November 30, 1998.",
      "citeRegEx" : "Corp.,? 1998",
      "shortCiteRegEx" : "Corp.",
      "year" : 1998
    }, {
      "title" : "Warcraft III: Reign of Chaos",
      "author" : [ "Blizzard Entertainment" ],
      "venue" : "July 3, 2002.",
      "citeRegEx" : "Entertainment,? 2002",
      "shortCiteRegEx" : "Entertainment",
      "year" : 2002
    }, {
      "title" : "Dynamic Control in Path-Planning with Real-Time Heuristic Search",
      "author" : [ "V. Bulitko", "Y. Björnsson", "M. Luštrek", "J. Schaeffer", "S. Sigmundarson" ],
      "venue" : "In ICAPS,",
      "citeRegEx" : "Bulitko et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bulitko et al\\.",
      "year" : 2007
    }, {
      "title" : "Dynamic Control in Real-Time Heuristic Search. JAIR",
      "author" : [ "V. Bulitko", "M. Luštrek", "J. Schaeffer", "Y. Björnsson", "S. Sigmundarson" ],
      "venue" : null,
      "citeRegEx" : "Bulitko et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bulitko et al\\.",
      "year" : 2008
    }, {
      "title" : "Model reduction techniques for computing approximately optimal solutions for Markov decision processes",
      "author" : [ "T. Dean", "R. Givan", "S. Leach" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Dean et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Dean et al\\.",
      "year" : 1997
    }, {
      "title" : "Hierarchical solution of Markov decision processes using macro-actions",
      "author" : [ "M. Hauskrecht", "N. Meuleau", "L.P. Kaelbling", "T. Dean", "C. Boutilier" ],
      "venue" : "In UAI, pp",
      "citeRegEx" : "Hauskrecht et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Hauskrecht et al\\.",
      "year" : 1998
    }, {
      "title" : "Speeding up planning in Markov decision processes via automatically constructed abstraction",
      "author" : [ "A. Isaza", "C. Szepesvári", "V. Bulitko", "R. Greiner" ],
      "venue" : "Tech. rep., Computing Science, U. Alberta. http://www.cs. ualberta.ca/ ̃szepesva/RESEARCH/PRMDP",
      "citeRegEx" : "Isaza et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Isaza et al\\.",
      "year" : 2008
    }, {
      "title" : "Solving factored MDPs using non-homogeneous partitions",
      "author" : [ "K. Kim", "T. Dean" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Kim and Dean,? \\Q2003\\E",
      "shortCiteRegEx" : "Kim and Dean",
      "year" : 2003
    }, {
      "title" : "Fast exact planning in Markov decision processes",
      "author" : [ "H.B. McMahan", "G.J. Gordon" ],
      "venue" : "In ICAPS,",
      "citeRegEx" : "McMahan and Gordon,? \\Q2005\\E",
      "shortCiteRegEx" : "McMahan and Gordon",
      "year" : 2005
    }, {
      "title" : "Memory-efficient abstractions for pathfinding",
      "author" : [ "N. Sturtevant" ],
      "venue" : "AIIDE, pp. 31–36.",
      "citeRegEx" : "Sturtevant,? 2007",
      "shortCiteRegEx" : "Sturtevant",
      "year" : 2007
    }, {
      "title" : "Between MDPs and semi-MDPs: a framework for temporal abstraction in reinforcement learning",
      "author" : [ "R.S. Sutton", "D. Precup", "S. Singh" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "We adopt the notion of options from Sutton et al. (1999):",
      "startOffset" : 36,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : "To our knowledge, such options-based abstractions have not been analyzed previously; the closest results are probably Theorem 2 of Kim and Dean (2003) and Theorem 4 of Dean, Givan, and Leach (1997).",
      "startOffset" : 131,
      "endOffset" : 151
    }, {
      "referenceID" : 9,
      "context" : "To our knowledge, such options-based abstractions have not been analyzed previously; the closest results are probably Theorem 2 of Kim and Dean (2003) and Theorem 4 of Dean, Givan, and Leach (1997). The proof is rather technical and is given",
      "startOffset" : 131,
      "endOffset" : 198
    }, {
      "referenceID" : 8,
      "context" : "Isaza et al. (2008) provides further details.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 10,
      "context" : "The optimal solution to S is obtained by using the Improved Prioritized Sweeping (IPS) algorithm of McMahan and Gordon (2005), (line 14).",
      "startOffset" : 100,
      "endOffset" : 126
    }, {
      "referenceID" : 8,
      "context" : "These results, appearing in (Isaza et al., 2008), confirm that the algorithm is robust to the choices of its parameters and scales as expected by increasing problem sizes.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "See (Isaza et al., 2008) for more details, including relevant pictures.",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 4,
      "context" : "Dean et al. (1997) introduced the notion of ε-homogeneous partitions and analyzed its properties, but without giving explicit loss bounds.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : "Dean et al. (1997) introduced the notion of ε-homogeneous partitions and analyzed its properties, but without giving explicit loss bounds. Kim and Dean (2003) developed some loss bounds.",
      "startOffset" : 0,
      "endOffset" : 159
    }, {
      "referenceID" : 0,
      "context" : "While Asadi and Huber (2004) also considered such optionsbased abstractions, they assume that the abstract actions (options) are given externally (possibly by specifying goal states for each of them) and they do not develop bounds.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "While Asadi and Huber (2004) also considered such optionsbased abstractions, they assume that the abstract actions (options) are given externally (possibly by specifying goal states for each of them) and they do not develop bounds. In a number of subsequent papers, the authors refined their methods. In particular, they became increasingly focussed on learning problems. For example, in the recent follow-up work, Asadi and Huber (2007) provide a method to learn an abstract hierarchical representation that uses state aggregation and options.",
      "startOffset" : 6,
      "endOffset" : 438
    }, {
      "referenceID" : 7,
      "context" : "Further, we deal with undiscounted SSPs, while Hauskrecht et al. (1998) dealt with discounted MDPs (but this difference is probably not crucial).",
      "startOffset" : 47,
      "endOffset" : 72
    } ],
    "year" : 2008,
    "abstractText" : "In this paper, we consider planning in stochastic shortest path (SSP) problems, a subclass of Markov Decision Problems (MDP). We focus on medium-size problems whose state space can be fully enumerated. This problem has numerous important applications, such as navigation and planning under uncertainty. We propose a new approach for constructing a multi-level hierarchy of progressively simpler abstractions of the original problem. Once computed, the hierarchy can be used to speed up planning by first finding a policy for the most abstract level and then recursively refining it into a solution to the original problem. This approach is fully automated and delivers a speed-up of two orders of magnitude over a state-of-the-art MDP solver on sample problems while returning near-optimal solutions. We also prove theoretical bounds on the loss of solution optimality resulting from the use of abstractions.",
    "creator" : "gnuplot 4.2 patchlevel 0"
  }
}