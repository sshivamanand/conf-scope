{
  "name" : "1202.3720.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Inference in Markov Control Problems",
    "authors" : [ "Thomas Furmston", "David Barber" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Markov control algorithms that perform smooth, non-greedy updates of the policy have been shown to be very general and versatile, with policy gradient and Expectation Maximisation algorithms being particularly popular. For these algorithms, marginal inference of the reward weighted trajectory distribution is required to perform policy updates. We discuss a new exact inference algorithm for these marginals in the finite horizon case that is more efficient than the standard approach based on classical forwardbackward recursions. We also provide a principled extension to infinite horizon Markov Decision Problems that explicitly accounts for an infinite horizon. This extension provides a novel algorithm for both policy gradients and Expectation Maximisation in infinite horizon problems."
    }, {
      "heading" : "1 MARKOV DECISION PROBLEMS",
      "text" : "A Markov Decision Problem (MDP) is described by an initial state distribution p1(s1), transition distributions p(st+1|st, at) and reward function Rt(st, at), where the state and action at time t are denoted by st and at respectively1 (Sutton and Barto, 1998). The state and action spaces can be either discrete or continuous. For a discount factor γ the reward is defined as Rt(st, at) = γt−1R(st, at) for a stationary reward R(st, at), where γ ∈ [0, 1). We assume a stationary policy, π, defined as a set of conditional distributions over the action space, πa,s = p(at = a|st = s, π). The total expected reward of the MDP (the policy utility)\n1To avoid cumbersome notation we also use the notation zt = {st, at} to denote a state-action pair. We use the bold typeface, zt, to denote a vector.\nis given by\nU(π) = H∑ t=1 ∑ st,at Rt(st, at)p(st, at|π) (1)\nwhere H is the horizon, which can be either finite or infinite, and p(st, at|π) is the marginal of the joint stateaction trajectory distribution\np(s1:H , a1:H |π) = p(aH |sH , π)p1(s1)\n× H−1∏ t=1 p(st+1|st, at)p(at|st, π). (2)\nGiven a transition model p(st+1|st, at), the MDP learning problem is to find a policy π that maximises (1). For all but the most select cases, such as small discrete environments or linear-quadratic control, this is a notoriously difficult optimisation problem which has given rise to a multitude of competing approaches.\nClassical planning algorithms, such as Policy Iteration or Value Iteration (Sutton and Barto, 1998), generally focus on greedy updates of the policy. While these algorithms work well in small discrete environments it has been difficult to extend them to more complex problems, such as structured or continuous domains. Additionally, these greedy updates of the policy become increasingly unstable as the problem domain becomes more complex. As a consequence a large amount of research has been done in designing algorithms which perform smooth policy updates. Gradient ascent algorithms, e.g. (Sutton et al., 2000), and the Expectation Maximisation algorithm, e.g. (Toussaint et al., 2006), have been particularly popular and have been successfully applied to a large range of complex domains including optimal control (Toussaint et al., 2006), robotics (Kober and Peters, 2009; Peters and Schaal, 2006) and Bayesian reinforcement learning (Furmston and Barber, 2010)."
    }, {
      "heading" : "1.1 EXPECTATION MAXIMISATION",
      "text" : "By expressing the objective function (1) as the likelihood function of an appropriately constructed mixture model the MDP can be solved using techniques from probabilistic inference, such as EM (Dayan and Hinton, 1997; Toussaint et al., 2006; Kober and Peters, 2009; Toussaint et al., 2011), EP (Toussaint, 2009; Furmston and Barber, 2010) or MCMC (Hoffman et al., 2008). Without loss of generality, we assume the reward is non-negative and define the reward weighted path distribution\np̂(s1:t, a1:t, t|π) = Rt(st, at)p(s1:t, a1:t|π)\nU(π) . (3)\nThis distribution is properly normalised, as can be seen from (1) and (2). The graphical structure of this distribution is given by a set of chains, each corresponding to a different time-point at which a reward is received, see figure 1 for an example.\nWe now define a variational distribution q(s1:t, a1:t, t), and take the Kullback-Leibler divergence between the q-distribution and (3). Since\nKL(q(s1:t, a1:t, t)||p̂(s1:t, a1:t, t|π)) ≥ 0 (4)\nwe obtain a lower bound on the log utility\nlogU(π) ≥ H̃(q(s1:t, a1:t, t)) + 〈log p̃(s1:t, a1:t, t|π)〉q (5)\nwhere 〈·〉q denotes the average w.r.t. q(s1:t, a1:t, t), H̃(·) is the entropy function and p̃ is the unnormalised reward weighted trajectory distribution. An EM algorithm can be obtained from the bound in (5) by iterative coordinate-wise maximisation:\nE-step For fixed πold find the best q that maximises the r.h.s. of (5). For no constraint on q, this gives q = p̂(s1:t, a1:t, t|πold). Then compute p̂(sτ = s, aτ = a, t|πold), the marginals of the reward weighted path distribution equation (3) evaluated at the previous policy.\nM-step For fixed q find the best π that maximises the r.h.s. of (5). This is equivalent to maximising w.r.t. π the ‘energy’ contribution\nH∑ t=1 t∑ τ=1 〈log π(aτ |sτ )〉p̂(sτ ,aτ ,t|πold) (6)\nNote that for an unconstrained tabular policy, the Mstep gives the policy update\nπ(s, a) ∝ H∑ t=1 t∑ τ=1 p̂(sτ = s, aτ = a, t|πold). (7)"
    }, {
      "heading" : "1.2 POLICY GRADIENTS",
      "text" : "The policy gradients algorithm iteratively updates the policy parameters in the direction of the gradient of ∇πU(π), in order to increase U(π) and thereby improve the policy. These gradients can be computed using the identity, see e.g. (Salakhutdinov et al., 2003),\n∂π logU(π) = H∑ t=1 t∑ τ=1 〈∂π log π(aτ |sτ )〉p̂(sτ ,aτ ,t|π) . (8)\nTo perform a policy update both the EM and policy gradient algorithms therefore require sufficient statistics of the reward weighted path distribution in equation (3); either the state-action marginals in discrete problems or the moments in exponential family continuous problems.\nWe note that in terms of inference the only difference between these two algorithms is that in the policy gradients algorithm the reward weighted distribution depends on the current policy, while in the EM algorithm it is dependent on the policy of the previous EM step. To ease notation we denote the distribution (3) of both algorithms by q(z1:t, t), see footnote 1."
    }, {
      "heading" : "1.3 FORWARD-BACKWARD INFERENCE",
      "text" : "To perform a policy update for finite horizon problems we need to calculate the marginals q(zτ , t), ∀t ∈ {1, . . . ,H} and τ ∈ {1, . . . , t}. For each component, t, the distribution q(z1:t|t) is chain structured and so all the marginals q(zτ |t) can be calculated in linear time using message-passing, see e.g. (Wainwright and Jordan, 2008). In particular these marginals can be calculated using forward-backward recursions, otherwise known as α-β recursions. The initial messages,\nα1, β1, are given by\nα1(z) = p0(s)p(a|s;π), β1(z) = R(s, a),\nand the forward-backward recursions are given by\nατ+1(z′) = P (z′|z)ατ (z), βτ+1(z) = P (z′|z)Tβτ (z′)\nwhere P (z′|z) is the state-action transition distribution, which is given by P (z′|z) = p(a′|s′;π)p(s′|s, a). Given the sets of forward-backward messages each individual state-action marginal is given\nq(zτ = z|t) ∝ ατ (z)βt+1−τ (z).\nThe marginals can be calculated concurrently through reuse of the forward-backward messages; see figure 1 for an illustration of the reuse of the forward-backward recursions. As there are H separate time components, where each component, t, has t state-action marginals the total computational complexity of calculating the policy update equation is O(H2)."
    }, {
      "heading" : "2 Q-INFERENCE",
      "text" : "We proceed to show that the previously described O(H2) forward-backward algorithm doesn’t fully exploit the conditional independence structure of the distribution (3) and that a more efficient O(H) procedure exists. We focus first on the finite horizon case, for which an exact algorithm exists, before extending the approach to the infinite horizon in §2.1.\nWe first prove the following lemma which shows that, conditioned on the succeeding state-action pair zτ+1 and the time-component t, the reward weighted trajectory distribution over zτ is independent of t and is equal to the system reversal dynamics.\nLemma 1. Given t, τ ∈ {1, . . . ,H−1} s.t. τ < t, then the variational distribution q(zτ |zτ+1, t) is independent of t and takes the form\nq(zτ |zτ+1, t) = p(zτ |zτ+1) (9)\nwhere p(zτ |zτ+1) is the marginal of the trajectory distribution (2).\nProof. For any given τ ∈ {1, . . . , t} the marginal of the variational distribution q(zτ :t|t) takes the form\nq(zτ :t|t) = 1 Vt p(zτ ) { t−1∏ τ ′=τ p(zτ ′+1|zτ ′) } R(zt),\nwhere Vt = E[R(zt)] is the normalisation constant. As τ < t we have a similar expression for the marginal q(zτ+1:t|t). Using the Markovian structure of the variational distribution means that the conditional distribution takes the form\nq(zτ |zτ+1, t) = p(zτ )p(zτ+1|zτ )\np(zτ+1) = p(zτ |zτ+1).\nWe now introduce a new set of ‘Q-functions’ that will play a prominent role in the rest of the paper. For each τ ∈ {1, . . . ,H} we define the function\nQτ (z) = H∑ t=τ q(zτ = z, t). (10)\nNote that the sum of the state-action marginals in both the policy gradient equation and the EM policy update function can be written in terms of the Q-functions as follows\nH∑ t=1 t∑ τ=1 q(zτ , t) = H∑ τ=1 H∑ t=τ q(zτ , t) = H∑ τ=1 Qτ (zτ ). (11)\nAn illustration of how the marginals of the reward weighted trajectory distribution can be written in terms of the Q-functions is given in figure 2.\nThese Q-functions have the intuitive interpretation of being proportional to the probability of reaching stateaction pair z at time τ times the total expected future reward. Therefore the part of the trajectory before time-point τ plays a prominent role in these functions, which isn’t the case in classical planning algorithms.\nWe now use lemma 1 to obtain a recursive relationship over the Q-functions. Lemma 2. Given τ ∈ {1, . . . ,H − 1}, the function Qτ (z) satisfies\nQτ (zτ ) = q(zτ , τ)+ ∑ zτ+1 p(zτ |zτ+1)Qτ+1(zτ+1). (12)\nProof. We start by rewriting the function Qτ (z) as\nQτ (zτ ) = q(zτ , τ)+ H∑\nt=τ+1 ∑ zτ+1 q(zτ , zτ+1|t)q(t), (13)\nwhere we have exploited that fact that q(·|t) is a distribution and introduced the state-action variable of the next time-step, zτ+1. Now, by lemma 1, we have that for each t ∈ {τ + 1, . . . ,H}\nq(zτ , zτ+1|t) = q(zτ |zτ+1, t)q(zτ+1|t) = p(zτ |zτ+1)q(zτ+1|t). (14)\nSubstituting this into (13) we obtain\nQτ (zτ ) = q(zτ , τ)+ ∑ zτ+1 p(zτ |zτ+1) H∑ t=τ+1 q(zτ+1|t)q(t),\nwhere we have used the fact that p(zτ |zτ+1) depends only upon τ and not upon t. The result now follows from the definition of Qτ+1(z).\nWe now briefly describe how the recursive relation in lemma 2 can be used to calculate the Q-functions in linear time for finite planning horizons. Firstly, the trajectory distribution (2) is chain structured so all the state-action marginals, {p(zτ )}τ , can be calculated in linear time. Additionally, the normalisation constant of the distribution (3) is by definition U(π) which may also be calculated in linear time by a standard forward recursion. For each τ ∈ {1, . . . ,H} the term q(zτ = z, τ) that occurs in the function Qτ (z) takes the form\nq(zτ , τ) = 1\nU(π) p(zτ )R(zτ ). (15)\nUsing the previous remarks it is clear that each of these terms can be calculated in linear time (Wainwright and Jordan, 2008). We now make the observation that QH(zH) = q(zH , H), so that this function can be calculated in linear time. Once the function QH(zH) has been calculated all of the remaining functions {Qτ (zτ )}H−1τ=1 can be computed by repeated use of the recursion (12). There are H − 1 applications of the recursion, each of which takes a constant amount of time to compute. Once all the Q-functions have been calculated a policy update can now be performed through (11).\nBefore extending our inference algorithm to infinite horizons it is of interest to note the difference between our inference algorithm and the standard forwardbackward algorithm. The standard forward-backward algorithm focuses on performing the forward and backward messages concurrently, exploiting the similarity in the components of the reward weighted trajectory\nAlgorithm 1 Finite Horizon Inference of Reward Trajectory Distribution\nCalculate Forward Messages: Iterate the forward message recursion until the final forward message, αH(z), has been calculated. Calculate the Final Q-function: Use the forward message, αH(z), and the total expected reward, U(π), to calculate the Q-function for the final time-point, QH(z), using equation (15). Calculate Backward Messages: Use the recursive equation (12) to propagate the Q-functions backwards in time Qt(z), for t = H − 1, . . . , 1.\ndistribution to reuse the messages. On the other hand the Q-inference algorithm first computes the forward messages and then, using them, performs the backward iterations of the Q-functions (12). This enables Q-recursions to use the time-independent Markovian structure of the system reversal dynamics in the reward weighted trajectory distribution. This property enables the Q-recursions to propagate the effect the current state-action pair has on all future rewards in a single calculation. This is equivalent to performing inference over all future time components of the reward weighted trajectory distribution concurrently."
    }, {
      "heading" : "2.1 INFINITE PLANNING HORIZON",
      "text" : "While we have only considered finite horizon problems the recursive relation of the Q-functions can also be used in infinite horizon problems. To do so we rely on the fact that, given the system is controllable, it will reach its stationary state-action distribution in a finite amount of time. Given that the system reaches its stationary state-action distribution by the time-point τ̂ , then it is straightforward to show that for any τ ≥ τ̂ we have the relation\nQτ+1(z) = γQτ (z). (16)\nThis relation can now be used to obtain a formulation for calculating the infinite number of state-action marginals of the reward weighted trajectory distribution. Firstly we split the infinite summation in the policy update function into the terms before and after the stationary state-action distribution has been reached,\n∞∑ t=1 Qt(z) = τ̂−1∑ t=1 Qt(z) + ∞∑ t=τ̂ Qt(z). (17)\nWe now introduce the function Q(z), which is defined by Q(z) = γ1−τ̂Qτ̂ (z). Note that by (16) we have Q(z) = γ1−tQt(z) for all t ≥ τ̂ . The infinite summa-\nAlgorithm 2 Infinite Horizon Inference of Reward Trajectory Distribution\nCalculate Forward Messages: Iterate the forward message recursion until the forward messages converge to the stationary distribution. Calculate Stationary Q-function: Use the stationary state-action distribution and the stationary system reversal dynamics to calculate the stationary Q function, Q(z), using either (20) or the fixed-point equation (19). Calculate Backward Messages: Use the recursive equation (12) to propagate the Q-functions backwards in time Qt(z), for t = τ̂ − 1, . . . , 1.\ntion that occurs (17) can now be performed analytically as follows\n∞∑ t=τ̂ Qt(z) = Q(z) ∞∑ t=τ̂ γt−1 = γ τ̂−1 1− γ Q(z). (18)\nTo perform the summation in (17) it now remains to obtain an analytic solution to Q(z). This is obtained from the following recursion, which is easy to prove using the relations (12) and (16) and the definition of Q(z),\nQ(z) = α(z)R(z) U(π) + γ ∑ z′ ←−p (z|z′)Q(z′), (19)\nwhere α(z) is the stationary state-action distribution and ←−p (z|z′) is the stationary system reversal dynamics. An algebraic solution for Q(z) is obtained from (19) by observing that\nQ = (I − γ ←− P )−1µ, (20)\nwhere µ is the point-wise product of the stationary state-action distribution with the reward function scaled by the inverse of the total expected reward. An alternative solution to Q(z) can be obtained by iterating the fixed point equation (19) until convergence, which may be preferable in systems where the matrix inversion is expensive. The complete algorithm for calculating the infinite number of state-action marginals of the reward weighted trajectory distribution is summarized in algorithm (2).\nWe also note that this gives a new formulation of the infinite horizon EM algorithm. While the current infinite horizon EM algorithm (Toussaint et al., 2006, 2011) uses the idea of the ‘time-marginal’ to select a finite horizon to approximate the infinite horizon problem, our formulation relies on the convergence of the state-action distribution p(z) to its stationary distribution. Once converged we can use (20) or (19) to\ncalculate the infinite number of marginals needed to perform a policy update.\nBefore proceeding we make a brief note about the ‘time-marginal’ criterion used in (Toussaint et al., 2006). It is not possible to implement the infinite horizon policy update function of (Toussaint et al., 2006) exactly and a finite horizon is therefore selected which will give a good approximation2. To select an appropriate finite horizon (Toussaint et al., 2006) propose to use the ‘time-marginal’ q(t), which can be calculated up to proportionality using the equation\nq(t) ∝ ∑ z ατ (z)βt−τ (z), for some τ ∈ {1, . . . , t}.\nIt is proposed that by concurrently iterating forward and backward messages it is a reasonable heuristic to cut-off calculations when q(t + 1) ∑t τ=1 q(τ). We note that this ‘time-marginal’ is actually the proportion of the objective function that is obtained at the tth time-step, i.e.\nq(t) = E[R(zt)] U(π) .\nTaking this into account one can expect the ‘timemarginal’ criterion to perform poorly in situations where the reward function has a sparse multi-modal structure."
    }, {
      "heading" : "3 DYNAMIC PROGRAMMING",
      "text" : "Equations (12) and (19) bear a strong resemblance to policy evaluation from classical infinite horizon planning algorithms, see e.g. (Sutton and Barto, 1998),\nQπ(s, a) = R(s, a)+γ ∑ s′,a′ π(a′|s′)p(s′|s, a)Qπ(s′, a′).\nHowever, while there is a strong resemblance there are also some significant differences. Firstly note that the Q-functions (12) and (19) are weighted inversely by the total expected reward. This occurs because EM and policy gradients work in probability space. Additionally, the standard Q-functions of policy evaluation represent the total expected future reward given the current state-action pair, and so do not depend on\n2We have recently become of aware of a new formulation of the EM algorithm, called incremental EM (Toussaint et al., 2011), that also converges in the limit. This incremental EM has a convergence rate that is exponential in the discount factor γ, while our algorithm has a convergence rate that is exponential in the magnitude of the largest eigenvalue (excluding eigenvalues equal to unity) of the state-action transition matrix. Additionally, our methods generalise to other algorithms that are based on the reward weighted trajectory distribution.\nany previous time-steps. This is in contrast to the Qfunctions of (12) and (19) which explicitly depend on previous time-steps through the state-action marginal. The reason for this difference can be explained through the different nature of the policy updates of these algorithms. The policy update equation of policy improvement takes the greedy form\nπnew(a|s) = δa,a∗(s), a∗(s) = argmax a Qπ(s, a).\nAs an MDP is chain structured the maximisation over a given s is independent of any of the previous timesteps, meaning that these Q-functions need only depend on future time-points. Meanwhile, the EM and policy gradients algorithms don’t condition on the state variable in this way during the policy update and so this splitting of the chain structure doesn’t occur.\nFinally, in the finite horizon case it can be seen through (11) that there corresponds H different Q-functions to the policy update (7) or the policy gradient (8), whereas in dynamic programming there is a one to one correspondence. This is because the algorithms highlighted in §1.1 and §1.2 are for stationary policies. Either of these algorithms can easily be re-derived for non-stationary policies and this will result in a one to one correspondence between the Q-functions and the non-stationary policies, i.e. Qt will correspond to πt."
    }, {
      "heading" : "4 CONTINUOUS MDPs",
      "text" : "The proofs of lemma 1 and lemma 2 follow over to the continuous case easily and the continuous version of equation (12) takes the form\nQτ (zτ ) = q(zτ , τ)+ ∫ dzτ+1p(zτ |zτ+1)Qτ+1(zτ+1).\n(21)\nDue to the summation in (21) we can see that the Qfunctions take the form of a two component mixture model, with one component corresponding to the immediate reward while the second corresponds to future rewards. Although one could model this mixture explicitly it is generally only necessary to calculate the moments\nµi = ∫ ziQτ (z)dz, i ∈ {1, . . . , N}, N ∈ N\nto perform a policy update. The recursive equation (21) can then be used to calculate these moments recursively in linear time, as we now demonstrate.\nFor illustrative purposes we consider the specific example of linear continuous MDPs with arbitrary rewards (Hoffman et al., 2009). In this problem class the initial\nstate distribution, policy and transition distributions are linear, with Gaussian noise, and take the parametric form\np(s1) = N (s1|µ0,Σ0), p(st+1|st, at) = N (st+1|Ast +Bat,Σ), p(at|st;K,m, πσ) = N (at|Kst +m;πσ),\nwhere N (x|µ,Σ) is a Gaussian distribution with mean and covariance µ and Σ respectively. Additionally, the reward function is a mixture of unnormalised Gaussian distributions3\nR(zt) = J∑ j=1 wjN̄ (yj |Mzt, Lj),\nwhere N̄ denotes an unnormalised Gaussian. In our Q-inference algorithm the first step is to calculate the forward messages, which is achieved using standard forward message recursions, see e.g. (Hoffman et al., 2009),\nµt+1 = Fµt+m̄, Σt+1 = FΣtFT +Σ̄, m̄ = [\n0 m\n] ,\nwhere µt and Σt denote the mean and covariance of αt(z) respectively, and\nΣ̄ = [ Σ ΣKT\nKΣ KΣKT + πσInu\n] , F = [ A B KA KB ] .\nAs the system is linear the reversal dynamics can be calculated using standard conditional Gaussian formulae, see e.g. (Barber, 2011). Given the forward messages the system reversal dynamics are given by\np(zt|zt+1) = N (zt|Gtzt+1 +←−mt, ←− Σ t),\nwhere Gt, ←−mt and ←− Σ t are given by\nGt = ΣtFT (FΣtFT + Σ̄)−1, ←−mt = µt −Gt(Fµt + m̄), ←− Σ t = Σt − ΣtFT (FΣtFT + Σ̄)FΣt.\nWe denote the first two moments of Qt(z) by µ Q t and ΣQt respectively. Similarly we denote the first two moments of q(zt|t) by µRt and ΣRt respectively. As zt depends on zt+1 linearly in the reversal dynamics it means that the first moment ∫ zt ztQt(zt) takes the form∫ zt ztq(zt, t)\n+ ∫\nzt,zt+1\n(zt+Gtzt+1+←−mt)N (zt|0, ←− Σ t)Qt+1(zt+1),\n3For simplicity of exposition we restrict our attention to the case J = 1, as the extension to J > 1 is trivial.\nwith a similar formula for the second moment. Defining, Zt+1 = ∑H τ=t+1 q(τ), and using∫\nzt+1 Qt+1(zt+1) = Zt+1, we obtain recursions for the\nfirst two moments of the Q-functions,\nµQt = q(t)µ R t + Zt+1 ←−mt +GtµQt+1 (22)\nΣQt = q(t)Σ R t + Zt+1( ←− Σ t +←−mt←−mTt )\n+Gt(Σ Q t+1 + µ Q t+1 ←−mTt + ←−mt(µQt+1)T )GTt . (23)\nGiven these moments the policy is updated by first solving a set of linear equations in K and m, and then solving for πσ, see (Hoffman et al., 2009).\nTo summarize, instead of calculating the forward and backward messages concurrently and then calculating the marginals q(zτ , t) separately, as in (Hoffman et al., 2009), we have first calculated the forward messages and then used (22) and (23) to calculate the moments of the Q-functions recursively. These recursive equations allow the moments necessary for a policy update to be calculated in linear time, which compares favorably with the forward-backward recursions of (Hoffman et al., 2009) that have quadratic runtime."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "5.1 INFINITE HORIZON MDPs",
      "text" : "The first experiment we performed was on the double reward chain problem, which was designed to highlight the susceptibility of the infinite horizon EM algorithm to get caught in local optima when the ‘time-marginal’ criterion (Toussaint et al., 2006) is used as a convergence criterion and the ‘time-marginal’ is multi-modal.\nThe N length double reward chain problem has N states, where we label the states from left to right in the chain, see figure 3. In each state there are three possible actions; to move left in the chain, to move right in the chain or to stay in the current state. When the agent is in the left end-point of the chain and moves left it remains in the same state, with a similar situation in the right end-point. All the transition dynamics of the system are deterministic. The agent receives a\nreward for staying in either of the two end-points of the chain. This means there are two optimal types of behaviour: to move towards the left end-point or to move to the right end-point, which we denote by πleft and πright respectively. The global/local optimality of these two policies depends on the initial state distribution and the reward function. We defined the initial state as the state adjacent to the left end-point of the chain, while the reward function was defined by\nR(s = 1, a = stay) = γ−1,\nR(s = N, a = stay) = 20γ2−N .\nThe reward function was designed so that the total expected reward of the two optimal policies remains the same regardless of the length of the chain, i.e.\nU(πleft) = 20, U(πright) = 400.\nIt is therefore always optimal to move towards the right end-point of the chain.\nWhile πright is always optimal the initial state of the agent is adjacent to the left end-point. This means that for sufficiently largeN the ‘time marginal’ is likely to have a sparse multi-modal structure, with one mode around low values of t (corresponding to the reward from the left end-point) and the second mode around larger values of t (corresponding to the reward from the right end-point). We ran the experiment for increasing values of N , with N = 3, . . . , 50, and repeated the experiment 100 times. In our infinite horizon EM algorithm we propagated forward messages until the change in magnitude of the forward messages didn’t exceed 0.01. For the ‘time-marginal’ stopping criterion we stopped message-passing when q(t) ≤ η ∑t−1 τ=1 q(τ), where we set η = 0.01 and η = 0.0001. The results of\nthe experiment are shown in figure 4, with our infinite horizon algorithm §2.1 (blue) and the ‘time-marginal’ infinite horizon algorithm with η = 0.01 (green) and with η = 0.0001 (red). As can be seen from figure 4 even for large N our infinite horizon EM algorithm is still able to pick up the global optimum at the right end-point of the chain. This compares favorably with the ‘time-marginal’ criterion, which for N ≥ 10 is generally unable to pick up the global optimum and instead only finds the local optimum.\nIt can also be seen that as N increases the performance of Q-inference infinite horizon EM algorithm decreases. This is because the performance of the EM algorithm depends on the initialization of the policy, and as N increases the chance of a favourable initial policy decreases. Parameter initialization is a general problem of the EM algorithm and the standard solution is to make multiple runs with different initializations, selecting the best final result."
    }, {
      "heading" : "5.2 ROBOT JOINT MANIPULATOR",
      "text" : "The N -link rigid robot arm manipulator is a standard continuous model, consisting of an end effector connected to N linked rigid bodies. A graphical depiction of a 3-link rigid manipulator is given in figure 5. A typical continuous control problem for such systems is to apply appropriate torque forces to the joints of the manipulator so as to move the end effector into a desired position. The state of the system is given by q, q̇, q̈ ∈ RN , where q, q̇ and q̈ denote the angles, velocities and accelerations of the joints respectively, while the control variables are the torques applied to the joints τ ∈ RN . The nonlinear state equations of the system are given by, see e.g. (Spong et al., 2005),\nM(q)q̈ + C(q̇, q)q̇ + g(q) = τ (24)\nwhere M(q) is the inertia matrix, C(q̇, q) denotes the Coriolis and centripetal forces and g(q) is the gravitational force.\nWhile this system is highly nonlinear it is possible to define an appropriate control function τ̂ (q, q̇) that results in linear dynamics in a different state-action space. This process is called feedback linearisation, see e.g. (Khalil, 2001), and in the case of an N -link rigid manipulator recasts the torque action space into the acceleration action space. This means that the state of the system is now given by q and q̇, while the control is u = q̈.\nOrdinarily in such problems the reward would be a function of the generalised co-ordinates of the end effector, which results in a non-trivial reward function in terms of q, q̇ and q̈. While this reward function can be modelled as a mixture of Gaussians, see (Hoffman et al., 2009), for simplicity we consider the simpler problem where the reward is a function of q, q̇ and q̈ directly.\nIn the experiments we considered a 2-link rigid manipulator, which results in a 6-dimensional state-action space and a 11-dimensional policy. In the experiment we discretised the continuous time dynamics into timesteps of ∆t = 0.1 seconds and considered trajectories of 10 seconds in length, which resulted in a planning horizon of H = 100. The mean of the initial state distribution was set zero. The elements of the policy parameters K, and m were initialised randomly from the interval [−1, 1], while πσ was initialised randomly from the interval [1, 2]. In the reward function the desired angles of the joints were randomly sampled from the interval [π/4, 3π/4]. All covariance matrices were set to diagonals and the diagonal elements were initialised randomly from the interval [0, 0.05]. In all runs of the experiment we gave both algorithms 300 seconds of training time.\nThe results of the experiment are shown in figure 6 where the normalised total expected reward is plotted against the training time (in seconds). The experiment was repeated 100 times and the plot shows the mean and standard deviation of the results. The plot shows the results for the Q-inference algorithm §4 (blue) and the forward-backward inference algorithm of (Hoffman et al., 2009) (red). The dashed line shows that the Q-inference algorithm needs only around 35 seconds to obtain the same level of performance as the forward-backward algorithm with 300 seconds of training. Additionally, we can see that after both algorithms have been given the full 300 seconds of training time the forward-backward algorithm only obtains around 70% of the performance compared to that of the Q-inference algorithm. Even in this comparatively small experiment the Q-inference algorithm therefore significantly outperforms the forward-backward algorithm. The difference in performance of the algorithms can be expected to be even more marked in larger scale\nproblems and longer planning horizons."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "We have presented a new efficient algorithm for performing inference in reward weighted trajectory distributions, which play a prominent role in current state of the art control algorithms like policy gradients and Expectation Maximisation. Our new inference algorithm scales linearly with the planning horizon, whereas the standard forward-backward recursions scales quadratically with the horizon. While we have restricted our attention to Markov decision processes the methods in this paper are readily applicable to other Markovian control problems, such as partially observable Markov decision processes (Kaelbling et al., 1998).\nAdditionally, we have presented a novel algorithm for calculating the sufficient statistics of these distributions in infinite horizon problems, where it is necessary to calculate an infinite number of marginals over a distribution with an infinite number of variables. This has provided an alternative procedure for implementing the EM algorithm in infinite horizon problems."
    } ],
    "references" : [ {
      "title" : "Variational Methods for Reinforcement Learning",
      "author" : [ "T. Furmston", "D. Barber" ],
      "venue" : "AISTATS, 9(13):241–248,",
      "citeRegEx" : "Furmston and Barber.,? \\Q2010\\E",
      "shortCiteRegEx" : "Furmston and Barber.",
      "year" : 2010
    }, {
      "title" : "Trans-dimensional MCMC for Bayesian Policy Learning",
      "author" : [ "M. Hoffman", "A. Doucet", "N. de Freitas", "A. Jasra" ],
      "venue" : null,
      "citeRegEx" : "Hoffman et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2008
    }, {
      "title" : "An Expectation Maximization Algorithm for Continuous Markov Decision Processes with Arbitrary Rewards",
      "author" : [ "M. Hoffman", "N. de Freitas", "A. Doucet", "J. Peters" ],
      "venue" : null,
      "citeRegEx" : "Hoffman et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2009
    }, {
      "title" : "Planning and Acting in Partially Observable Stochastic Domains",
      "author" : [ "L. Kaelbling", "M. Littman", "A. Cassandra" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Kaelbling et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Kaelbling et al\\.",
      "year" : 1998
    }, {
      "title" : "Nonlinear Systems",
      "author" : [ "H. Khalil" ],
      "venue" : null,
      "citeRegEx" : "Khalil.,? \\Q2001\\E",
      "shortCiteRegEx" : "Khalil.",
      "year" : 2001
    }, {
      "title" : "Policy search for motor primitives in robotics",
      "author" : [ "J. Kober", "J. Peters" ],
      "venue" : "NIPS, 21:849–856,",
      "citeRegEx" : "Kober and Peters.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kober and Peters.",
      "year" : 2009
    }, {
      "title" : "Policy Gradient Methods for Robotics",
      "author" : [ "J. Peters", "S. Schaal" ],
      "venue" : "IROS, 21:2219–2225,",
      "citeRegEx" : "Peters and Schaal.,? \\Q2006\\E",
      "shortCiteRegEx" : "Peters and Schaal.",
      "year" : 2006
    }, {
      "title" : "Optimization with EM and Expectation-ConjugateGradient",
      "author" : [ "R. Salakhutdinov", "S. Roweis", "Z. Ghahramani" ],
      "venue" : "ICML, (20):672–679,",
      "citeRegEx" : "Salakhutdinov et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Salakhutdinov et al\\.",
      "year" : 2003
    }, {
      "title" : "Robot Modelling and Control",
      "author" : [ "M. Spong", "S. Hutchinson", "M. Vidyasagar" ],
      "venue" : null,
      "citeRegEx" : "Spong et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Spong et al\\.",
      "year" : 2005
    }, {
      "title" : "Policy Gradient Methods for Reinforcement Learning with Function Approximation",
      "author" : [ "R. Sutton", "D. McAllester", "S. Singh", "Y. Mansour" ],
      "venue" : null,
      "citeRegEx" : "Sutton et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2000
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Pros and Cons of truncated Gaussian EP in the context of Approximate Inference Control",
      "author" : [ "M. Toussaint" ],
      "venue" : "NIPS - Workshop on Probabilistic Approaches for Robotics and Control.,",
      "citeRegEx" : "Toussaint.,? \\Q2009\\E",
      "shortCiteRegEx" : "Toussaint.",
      "year" : 2009
    }, {
      "title" : "Probabilistic inference for solving (PO)MDPs",
      "author" : [ "M. Toussaint", "S. Harmeling", "A. Storkey" ],
      "venue" : "Research Report EDI-INF-RR-0934,",
      "citeRegEx" : "Toussaint et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Toussaint et al\\.",
      "year" : 2006
    }, {
      "title" : "Bayesian Time Series Models, chapter Expectation-Maximization methods for solving (PO)MDPs and optimal control problems",
      "author" : [ "M. Toussaint", "A. Storkey", "S. Harmeling" ],
      "venue" : null,
      "citeRegEx" : "Toussaint et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Toussaint et al\\.",
      "year" : 2011
    }, {
      "title" : "Graphical Models, Exponential Families, and Variational Inference",
      "author" : [ "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Wainwright and Jordan.,? \\Q2008\\E",
      "shortCiteRegEx" : "Wainwright and Jordan.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "A Markov Decision Problem (MDP) is described by an initial state distribution p1(s1), transition distributions p(st+1|st, at) and reward function Rt(st, at), where the state and action at time t are denoted by st and at respectively (Sutton and Barto, 1998).",
      "startOffset" : 233,
      "endOffset" : 257
    }, {
      "referenceID" : 10,
      "context" : "Classical planning algorithms, such as Policy Iteration or Value Iteration (Sutton and Barto, 1998), generally focus on greedy updates of the policy.",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "(Sutton et al., 2000), and the Expectation Maximisation algorithm, e.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "(Toussaint et al., 2006), have been particularly popular and have been successfully applied to a large range of complex domains including optimal control (Toussaint et al.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 12,
      "context" : ", 2006), have been particularly popular and have been successfully applied to a large range of complex domains including optimal control (Toussaint et al., 2006), robotics (Kober and Peters, 2009; Peters and Schaal, 2006) and Bayesian reinforcement learning (Furmston and Barber, 2010).",
      "startOffset" : 137,
      "endOffset" : 161
    }, {
      "referenceID" : 5,
      "context" : ", 2006), robotics (Kober and Peters, 2009; Peters and Schaal, 2006) and Bayesian reinforcement learning (Furmston and Barber, 2010).",
      "startOffset" : 18,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : ", 2006), robotics (Kober and Peters, 2009; Peters and Schaal, 2006) and Bayesian reinforcement learning (Furmston and Barber, 2010).",
      "startOffset" : 18,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : ", 2006), robotics (Kober and Peters, 2009; Peters and Schaal, 2006) and Bayesian reinforcement learning (Furmston and Barber, 2010).",
      "startOffset" : 104,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : "By expressing the objective function (1) as the likelihood function of an appropriately constructed mixture model the MDP can be solved using techniques from probabilistic inference, such as EM (Dayan and Hinton, 1997; Toussaint et al., 2006; Kober and Peters, 2009; Toussaint et al., 2011), EP (Toussaint, 2009; Furmston and Barber, 2010) or MCMC (Hoffman et al.",
      "startOffset" : 194,
      "endOffset" : 290
    }, {
      "referenceID" : 5,
      "context" : "By expressing the objective function (1) as the likelihood function of an appropriately constructed mixture model the MDP can be solved using techniques from probabilistic inference, such as EM (Dayan and Hinton, 1997; Toussaint et al., 2006; Kober and Peters, 2009; Toussaint et al., 2011), EP (Toussaint, 2009; Furmston and Barber, 2010) or MCMC (Hoffman et al.",
      "startOffset" : 194,
      "endOffset" : 290
    }, {
      "referenceID" : 13,
      "context" : "By expressing the objective function (1) as the likelihood function of an appropriately constructed mixture model the MDP can be solved using techniques from probabilistic inference, such as EM (Dayan and Hinton, 1997; Toussaint et al., 2006; Kober and Peters, 2009; Toussaint et al., 2011), EP (Toussaint, 2009; Furmston and Barber, 2010) or MCMC (Hoffman et al.",
      "startOffset" : 194,
      "endOffset" : 290
    }, {
      "referenceID" : 11,
      "context" : ", 2011), EP (Toussaint, 2009; Furmston and Barber, 2010) or MCMC (Hoffman et al.",
      "startOffset" : 12,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : ", 2011), EP (Toussaint, 2009; Furmston and Barber, 2010) or MCMC (Hoffman et al.",
      "startOffset" : 12,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : ", 2011), EP (Toussaint, 2009; Furmston and Barber, 2010) or MCMC (Hoffman et al., 2008).",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : "(Salakhutdinov et al., 2003),",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "(Wainwright and Jordan, 2008).",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : "Using the previous remarks it is clear that each of these terms can be calculated in linear time (Wainwright and Jordan, 2008).",
      "startOffset" : 97,
      "endOffset" : 126
    }, {
      "referenceID" : 12,
      "context" : "Before proceeding we make a brief note about the ‘time-marginal’ criterion used in (Toussaint et al., 2006).",
      "startOffset" : 83,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "It is not possible to implement the infinite horizon policy update function of (Toussaint et al., 2006) exactly and a finite horizon is therefore selected which will give a good approximation.",
      "startOffset" : 79,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "To select an appropriate finite horizon (Toussaint et al., 2006) propose to use the ‘time-marginal’ q(t), which can be calculated up to proportionality using the equation",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 10,
      "context" : "(Sutton and Barto, 1998),",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 13,
      "context" : "We have recently become of aware of a new formulation of the EM algorithm, called incremental EM (Toussaint et al., 2011), that also converges in the limit.",
      "startOffset" : 97,
      "endOffset" : 121
    }, {
      "referenceID" : 2,
      "context" : "For illustrative purposes we consider the specific example of linear continuous MDPs with arbitrary rewards (Hoffman et al., 2009).",
      "startOffset" : 108,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "(Hoffman et al., 2009),",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 2,
      "context" : "Given these moments the policy is updated by first solving a set of linear equations in K and m, and then solving for πσ, see (Hoffman et al., 2009).",
      "startOffset" : 126,
      "endOffset" : 148
    }, {
      "referenceID" : 2,
      "context" : "To summarize, instead of calculating the forward and backward messages concurrently and then calculating the marginals q(zτ , t) separately, as in (Hoffman et al., 2009), we have first calculated the forward messages and then used (22) and (23) to calculate the moments of the Q-functions recursively.",
      "startOffset" : 147,
      "endOffset" : 169
    }, {
      "referenceID" : 2,
      "context" : "These recursive equations allow the moments necessary for a policy update to be calculated in linear time, which compares favorably with the forward-backward recursions of (Hoffman et al., 2009) that have quadratic runtime.",
      "startOffset" : 172,
      "endOffset" : 194
    }, {
      "referenceID" : 12,
      "context" : "The first experiment we performed was on the double reward chain problem, which was designed to highlight the susceptibility of the infinite horizon EM algorithm to get caught in local optima when the ‘time-marginal’ criterion (Toussaint et al., 2006) is used as a convergence criterion and the ‘time-marginal’ is multi-modal.",
      "startOffset" : 227,
      "endOffset" : 251
    }, {
      "referenceID" : 8,
      "context" : "(Spong et al., 2005),",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : "(Khalil, 2001), and in the case of an N -link rigid manipulator recasts the torque action space into the acceleration action space.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 2,
      "context" : "While this reward function can be modelled as a mixture of Gaussians, see (Hoffman et al., 2009), for simplicity we consider the simpler problem where the reward is a function of q, q̇ and q̈ directly.",
      "startOffset" : 74,
      "endOffset" : 96
    }, {
      "referenceID" : 2,
      "context" : "The plot shows the results for the Q-inference algorithm §4 (blue) and the forward-backward inference algorithm of (Hoffman et al., 2009) (red).",
      "startOffset" : 115,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "The plot shows the results for our continuous Q-inference algorithm §4 (blue) and the forwardbackward inference algorithm of (Hoffman et al., 2009) (red).",
      "startOffset" : 125,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "While we have restricted our attention to Markov decision processes the methods in this paper are readily applicable to other Markovian control problems, such as partially observable Markov decision processes (Kaelbling et al., 1998).",
      "startOffset" : 209,
      "endOffset" : 233
    } ],
    "year" : 2011,
    "abstractText" : "Markov control algorithms that perform smooth, non-greedy updates of the policy have been shown to be very general and versatile, with policy gradient and Expectation Maximisation algorithms being particularly popular. For these algorithms, marginal inference of the reward weighted trajectory distribution is required to perform policy updates. We discuss a new exact inference algorithm for these marginals in the finite horizon case that is more efficient than the standard approach based on classical forwardbackward recursions. We also provide a principled extension to infinite horizon Markov Decision Problems that explicitly accounts for an infinite horizon. This extension provides a novel algorithm for both policy gradients and Expectation Maximisation in infinite horizon problems. 1 MARKOV DECISION PROBLEMS A Markov Decision Problem (MDP) is described by an initial state distribution p1(s1), transition distributions p(st+1|st, at) and reward function Rt(st, at), where the state and action at time t are denoted by st and at respectively (Sutton and Barto, 1998). The state and action spaces can be either discrete or continuous. For a discount factor γ the reward is defined as Rt(st, at) = γR(st, at) for a stationary reward R(st, at), where γ ∈ [0, 1). We assume a stationary policy, π, defined as a set of conditional distributions over the action space, πa,s = p(at = a|st = s, π). The total expected reward of the MDP (the policy utility) To avoid cumbersome notation we also use the notation zt = {st, at} to denote a state-action pair. We use the bold typeface, zt, to denote a vector. is given by",
    "creator" : "TeX"
  }
}