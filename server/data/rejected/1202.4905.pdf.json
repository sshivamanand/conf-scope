{
  "name" : "1202.4905.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A BI-DIRECTIONAL REFINEMENT ALGORITHM FOR THE CALCULUS OF (CO)INDUCTIVE CONSTRUCTIONS",
    "authors" : [ "ANDREA ASPERTI", "WILMER RICCIOTTI", "CLAUDIO SACERDOTI COEN" ],
    "emails" : [ "asperti@cs.unibo.it", "ricciott@cs.unibo.it", "sacerdot@cs.unibo.it", "enrico.tassi@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "The refinement algorithm is in charge of giving a meaning to the terms, types and proof terms directly written by the user or generated by using tactics, decision procedures or general automation. The terms are written in an “external syntax” meant to be user friendly that allows omission of information, untyped binders and a certain liberal use of user defined sub-typing. The refiner modifies the terms to obtain related well typed terms in the internal syntax understood by the kernel of the ITP. In particular, it acts as a type inference algorithm when all the binders are untyped.\nThe proposed algorithm is bi-directional: given a term in external syntax and a type expected for the term, it propagates as much typing information as possible towards the leaves of the term. Traditional mono-directional algorithms, instead, proceed in a bottomup way by inferring the type of a sub-term and comparing (unifying) it with the type expected by its context only at the end. We propose some novel bi-directional rules for CIC that are particularly effective. Among the benefits of bi-directionality we have better error message reporting and better inference of dependent types. Moreover, thanks to bi-directionality, the coercion system for sub-typing is more effective and type inference generates simpler unification problems that are more likely to be solved by the inherently incomplete higher order unification algorithms implemented.\nFinally we introduce in the external syntax the notion of vector of placeholders that enables to omit at once an arbitrary number of arguments. Vectors of placeholders allow a trivial implementation of implicit arguments and greatly simplify the implementation of primitive and simple tactics.\n1998 ACM Subject Classification: D.3.1, F.3.0. Key words and phrases: refiner, type inference, interactive theorem prover, calculus of inductive construc-\ntions, Matita.\nLOGICAL METHODSl IN COMPUTER SCIENCE DOI:10.2168/LMCS-8 (1:18) 2012 c© A. Asperti, W. Ricciotti, C. Sacerdoti Coen, and E. TassiCC© Creative Commons"
    }, {
      "heading" : "1. Introduction",
      "text" : "In this paper we are interested in describing one of the key ingredients in the implementation of Interactive Theorem Provers (ITP) based on type theory.\nThe architecture of these tools is usually organized in layers and follows the so called de Bruijn principle: the correctness of the whole system solely depends on the innermost component called kernel. Nevertheless, from a user perspective, the most interesting layers are the external ones, the ones he directly interacts with. Among these, the refiner is the one in charge of giving a meaning to the terms and types he writes. The smarter the refiner is, the more freedom the user has in omitting pieces of information that can be reconstructed. The refiner is also the component generating the majority of error messages the user has to understand and react to in order to finish his proof or definition.\nThis paper is devoted to the description of a refinement algorithm for the Calculus of (Co)Inductive Constructions, the type theory on which the Matita [6], Coq [12] and Lego [19] ITPs are based on.\n1.1. Refinement. In this and in the previous paper [4] we are interested in the implementation of interactive theorem provers (ITP) for dependently typed languages that are heavily based on the Curry-Howard isomorphism. Proofs are represented using lambdaterms. Proofs in progress are represented using lambda-terms containing metavariables that are implicitly existentially quantified. Progression in the proof is represented by instantiation of metavariables with terms. Metavariables are also useful to represent missing or partial information, like untyped lambda-abstractions or instantiation of polymorphic functions to omitted type arguments.\nAgda [8] and Matita [6] are examples of systems implemented in this way. Arnaud Spiwack in his Ph.D. thesis [31] partially describes a forthcoming release of Coq 8.4 that will be implemented on the same principles.\nThe software architecture of these systems is usually built in layers. The innermost layer is the kernel of the ITP. The main algorithm implemented by the kernel is the type checker, which is based in turn on conversion and reduction. The type checker takes as input a (proof) term possibly containing metavariables and it verifies if the partial term is correct so far. To allow for type-checking, metavariables are associated to sequents, grouping their types together with the context (hypotheses) available to inhabit the type. The kernel does not alter metavariables since no instantiation takes place during reduction, conversion or type checking.\nThe kernel has the important role of reducing the trusted code base of the ITP. Indeed, the kernel eventually verifies all proofs produced by the outer layers, detecting incorrect proofs generated by bugs in those layers. Nevertheless, the user never interacts directly with the kernel and the output of the kernel is just a boolean that is never supposed to be false when the rest of the system is bug free. The most interesting layers from the user point of view are thus the outer layers. The implementation of a kernel for a variant of the Calculus of (Co)Inductive Constructions (CIC) has been described in [4] down to the gory details that make the implementation efficient.\nThe next layer is the refiner and is the topic of this paper. The main algorithm implemented by the refiner is the refinement algorithm that tries to infer as much information as it is needed to make its input meaningful. In other words it takes as input a partial term, written in an “external syntax”, and tries to obtain a “corresponding” well typed term.\nThe input term can either be user provided or it can be a partial proof term generated by some proof command (called tactic) or automation procedure. The gap between the external and internal syntax is rather arbitrary and system dependent. Typical examples of external syntaxes allow for: • Untyped abstractions. Hence the refiner must perform type inference to recover the explicit types given to bound variables. The polymorphism of CIC is such that binders are required to be typed to make type checking decidable. • Omission of arguments, in particular omission of types used to instantiate polymorphic\nfunctions. Hence the refiner must recover the missing information during type inference to turn implicit into explicit polymorphism. • Linear placeholders for missing terms that are not supposed to be discovered during type\ninference. For instance, a placeholder may be inserted by a tactic to represent a new proof obligation. Hence the refiner must turn the placeholder into a metavariable by constraining the set of free variables that may occur in it and the expected type. • Implicit ad-hoc sub-typing determined by user provided cast functions (called coercions)\nbetween types or type families. Hence the refiner must modify the user provided term by explicitly inserting the casts in order to let the kernel completely ignore sub-typing.\nCoercions are user provided functions and are thus free to completely ignore their input. Thus a refiner that handles coercions is actually able to arbitrarily patch wrong user provided terms turning them into arbitrarily different but well typed terms. Moreover, the insertion of a coercion between type families can also introduce new metavariables (the family indexes) that play the role of proof obligations for pre-conditions of the coercion. For instance, a coercion from lists to ordered lists can open a proof obligation that requires the list to be sorted.\nThe refiner is the most critical system component from the user point of view since it is responsible for the “intelligence” of the ITP: the more powerful the refiner is, the less information is required from the user and the simpler the outer layers become. For instance, a series of recent techniques that really improve the user experience have all been focused in the direction of making the refiner component more powerful and extensible by the user. Canonical structures [16], unification hints [5] and type classes [30] are devices that let the user drive some form of proof search that is seamlessly integrated in the refinement process. While the latter device is directly integrated into the refinement algorithm, the first two are found in the unification algorithm used by the refiner.\nThey all make it possible to achieve similar objectives, the second being more general than the first and the last two being incomparable from the point of view of efficiency (where the second is best) and expressiveness (where the third is more flexible). The implementation of type classes done in Coq is actually provided by an additional layer outside the refiner for historical reasons.\nIn this paper we will describe only the refinement algorithm implemented in a refiner for a variant of the Calculus of (Co)Inductive Constructions. The algorithm is used in the forthcoming major release of the Matita1 ITP (1.0.x). The algorithm calls a unification algorithm that will be specified in this paper and described elsewhere. We do not consider type classes in our refinement algorithm since we prefer to assume the unification algorithm\n1Matita is free software available at http://matita.cs.unibo.it\nto implement unification hints. Nevertheless, type classes can be easily added to our algorithm with minor modifications and indeed the relevant bits that go into the refiner are implemented in Matita.\nBefore addressing bi-directionality, which is a peculiarity of the algorithm that has not been fully exploited yet2 for the CIC, we just conclude our overview of an ITP architecture by talking about the next layer. The next layer after the refiner is that of tactics. This layer is responsible for implementing commands that help the user in producing valid proof terms by completely hiding to him the proof terms themselves. Tactics range from simple ones that capture the introduction and elimination rules of the connectives (called primitive tactics) to complicated proof automation procedures. The complexity of proof automation is inherent in the problem. On the other hand, primitive tactics should be as simple as building small partial proof terms. For instance, to reduce a proof of A⇒ B to a proof of B given A it is sufficient to instantiate the metavariable associated to the sequent ` A⇒ B with the term λx.? in external syntax where ? is a placeholder for a new proof obligation. This is possible when the refinement algorithm is powerful enough to refine λx.? to λx : A.?1 where ?1 is a new metavariable associated to the sequent x : A ` B. When this is not the case or when the refiner component is totally missing, the tactic is forced to first perform an analysis of the current goal, then explicitly create a new metavariable and its sequent, and then emit the new proof term λx : A.?1 directly in the internal syntax.\n1.2. Bi-directionality. When the external syntax of our ITP allows to omit types in binders, the refinement algorithm must perform type inference. Type inference was originally studied in the context of lambda-calculi typed a la Curry, where no type information can be attached at all to the binders. The traditional algorithm for type inference, now called uni-directional, performs type inference by first traversing the term in a top-down way. When a binder is met, a new metavariable (usually called type or unification variable in this context) is introduced for the type of the bound variable. Then type constraints are solved traversing the term in a bottom-up way. When the variable or, more generally, a term is used in a given context, its type (called inferred type) is constrained to be compatible with the one expected by the context (called expected type). This triggers a unification problem.\nType inference, especially for the Hindley-Milner type system, gives the possibility to write extremely concise programs by omitting all types. Moreover, it often detects a higher degree of polymorphism than the one expected by the user. Unluckily, it has some drawbacks. A minor one is that types are useful for program documentation and thus the user desires to add types at least to top level functions. In practice, this is always allowed by concrete implementations. Another problem is error reporting: a typing error always manifests itself as a mismatch between an inferred and an expected type. Nevertheless, an error can be propagated to a very distant point in the code before being detected and the position where it is produced. The mismatch itself can be non informative about where the error actually is. Finally, unification quickly becomes undecidable when the expressive power of the type system increases. In particular, it is undecidable for higher order logic and for dependent types.\n2The refinement algorithm of Coq 8.3, the most widespread implementation of CIC, is almost monodirectional with only the lambda-abstraction case handled in a bi-directional way. Many other interesting cases of bi-directionality are obtained in this paper for inductive types and constructors.\nTo avoid or mitigate the drawbacks of type inference, bi-directional type-checking algorithms have been introduced in the literature [24]. These algorithms take as input a λ-term typed a la Curry and an expected top-level type and they proceed in a top-down manner by propagating the expected type towards the leaves of the term. Additional expected types are given in local definitions, so that all functions are explicitly documented. Error detection is improved by making it more local. The need for unification is reduced and, for simple type systems, unification is totally avoided. Some terms, in particular β-redexes, are no longer accepted, but equivalent terms are (e.g. by using a local definition for the head). An alternative consists of accepting all terms by re-introducing a dependency over some form of unification.\nBi-directionality also makes sense for languages typed à la Church, like the one we consider here. In this case the motivations are slightly different. First of all, typing information is provided both in the binders and at the top-level, in the form of an expected type. Hence information can flow in both direction and, sooner or later, the need to compare the expected and inferred types arises. In the presence of implicit polymorphism, unification is thus unavoidable. Because of dependent types and metavariables for proof obligations, we need the full power of higher order unification. Moreover, again because of unification, the problem remains undecidable also via using a bi-directional algorithm. Hence avoiding unification is no longer a motivation for bi-directionality. The remaining motivations for designing a bi-directional refinement algorithm for CIC are the following:\nImproved error messages. A typing error is issued every time a mismatch is found between the inferred and expected type. With a mono-directional algorithm, the mismatch is always found at the end, when the typing information reaches the expected type. In a bi-directional setting the expected type is propagated towards the leaves and the inferred type towards the root, the mismatch is localized in smaller sub-terms and the error message is simpler. For instance, instead of the message “the provided function has type A ⇒ List B but it is supposed to have type A⇒ List C” related to a whole function definition one could get the simpler message “the list element has type B but it is supposed to have type C” related to one particular position in the function body.\nImprovement of the unification algorithm. To make the system responsive, the semi-decidable unification algorithm is restricted to always give an answer in a finite amount of time. Hence the algorithm could fail to find a solution even when a solution exists. For instance, the algorithms implemented in Coq and Matita are essentially backtracking free and they systematically favor projections over mimics: when unifying an applied metavariable ?1 a b c with a b (for some a, b, c closed in a context Γ), the system instantiates ?1 with λx, y, z.y rather than λx, y, z.b (where x, y, z /∈ dom(Γ)). Moreover, unification for CIC does not admit a most general unifier and it should recursively enumerate the set of solutions. However, it is usual in system implementations to let unification return just one solution and to avoid back-tracking in the refinement algorithm3. Thus, if the solution found by unification is correct locally, but not globally, refinement will fail. Thanks to bi-directionality, unification problems often become more instantiated and thus simpler, and they also admit fewer solutions. In particular, in the presence of dependent types, it is easy to find practical\n3To the authors knowledge, Isabelle [18] is the only interactive prover implementing Huet’s algorithm [17] capable of generating all second order unifiers\nexamples where the unification algorithm finds a solution only on the problems triggered by the bi-directional algorithm.\nAn interesting and practical example that motivated our investigation of bi-directionality is the following. Consider a dependently typed data-type (Term S) that represents the syntax of a programming language with binders. Type dependency is exploited to make explicit the set S of variables bound in the term and every variable occurrence must come with a proof that the variable occurs in the bound variables list: (Var S x I) has type (Term S) where x is a variable name, I is a proof of True and Var has type ∀S.∀a : String.x ∈ S → Term S where x ∈ S is a computable function that reduces to True when x belongs to S and to False otherwise. Consider now the term (Lambda ? x (Var ? x I)) in concrete syntax that represents λx.x in our programming language. Note that no information about the set of bound variables has been provided by the user. Thus it is possible to simply define notational macros so that the user actually writes λx.x and this is expanded4 to Lambda ? x (Var ? x I). A uni-directional refiner is unlikely to accept the given term since it should guess the right value for the second placeholder ? such that x ∈ ? reduces to True and ? is the set of variables actually bound in the term. The latter information is not local and it is still unknown in the bottom-up, uni-directional approach. On the other hand, a bi-directional refiner that tries to assign type Term ∅ to the term would simply propagate ∅ to the first placeholder and then propagate ∅ ∪ {x} to the second one, since Lambda, which is a binder, has type ∀S.∀x.Term (S ∪ {x})→ Term S. Finally, True is the inferred type for I, whose expected type is x ∈ ∅ ∪ {x}. The two types are convertible and the input is now accepted without any guessing.\nImprovement of the coercion mechanism. Coercions are triggered when unification fails. They are explicit cast functions, declared by the user, used to fix the type of sub-terms. Simplifying the unification problem allows to retrieve more coercions. For instance, consider a list [1; 2; 3] of natural numbers used as a list of integer numbers and assume the existence of a coercion function k from natural to integers. In the mono-directional problem, the failing unification problem is (List N) vs (List Z). The coercion required is the one obtained lifting k over lists. The lifting has to be performed manually by the user or by the system. In the latter case, the system needs to recognize that lists are containers and has to have code to lift coercions over containers, like in [10]. In the bi-directional case, however, the expected type (List Z) would propagate to assign to each list element the expected type Z and the coercion k would be applied to all integers in the list without need of additional machinery. The bi-directional algorithm presented in this paper does not allow to remove the need for the coercion over lists in all situations, but it is sufficient in many practical ones, like the one just considered.\nIntroduction of vectors of placeholders (“. . . ”) in the external syntax. A very common use of dependently typed functions consists in explicitly passing to them an argument which is not the first one and have the system infer the previous arguments using type dependencies. For instance, if Cons : ∀A.A → List A → List A and l is a list of integers, the user can simply write (Cons ? 2 l) and have the system infer that ? must be instantiated with the type of 2, which is N.\n4User provided notational macros are used to extend the external syntax of an ITP and they are expanded before refinement, yielding a term in external syntax to be refined.\nThis scenario is so common that many ITPs allow to mark some function arguments as implicit arguments and let the user systematically avoid passing them. This requires additional machinery implemented in the ITP and it has the unfortunate drawback that sometimes the user needs to explicitly pass the implicit arguments anyway, in particular in case of partial function applications. This special situation requires further ad-hoc syntax to turn the implicit argument into an explicit one. For instance, if we declare the first argument of Cons implicit, then the user can simply write (Cons 2 l) for the term presented above, but has to write something like (@Cons N), in Coq syntax, to pass the partial function application to some higher order function expecting an argument of type N → List N → List N.\nAn alternative to implicit arguments is to let the user explicitly insert the correct number of placeholders “?” to be inferred by the system. Series of placeholders are neither aesthetic nor robust to changes in the type of the function.\nA similar case occurs during the implementation of tactics. Given a lemma L : H1 → . . . → Hn → C, to apply it the tactic opens n new proof obligations by refining the term (L ? . . . ?) where the number of inserted placeholders must be exactly n.\nIn this paper we propose a new construct to be added to the external syntax of ITPs: a vector of placeholders to be denoted by _ ? and to be used in argument position only. In the actual external syntax of Matita we use the evocative symbol “. . . ” in place of _ ? . The semantics associated to _ ? is lazy: an _ ? will be expanded to the sequence of placeholders of minimal length that makes the application refineable, so that its inferred type matches its expected type. In a uni-directional setting no expected type is known in advance and the implementation of the lazy semantics would require computationally expensive non-local backtracking, which is not necessary in the bi-directional case.\nThanks to vectors of placeholders the analysis phase of many primitive tactics implementation that was aimed at producing terms with the correct number of placeholders can now be totally omitted. Moreover, according to our experience, vectors of placeholders enable to avoid the implementation of implicit arguments: it is sufficient for the user to insert manually or by means of a notation a _ ? before the arguments explicitly passed, with the benefit that the _ ? automatically adapts to the case of partial function application. For example, using the infix notation :: for (Cons _ ? ), the user can both write 2 :: l, which is expanded to (Cons _ ? 2 l) and refined to (Cons N 2 l), and pass :: to an higher order function expecting an argument of type N → List N → List N. In the latter case, :: is expanded to (Cons _ ? ) that is refined to (Cons N) because of the expected type. If :: is passed instead to a function expecting an argument of type ∀A.A→ List A→ List A, then (Cons _ ? ) will be expanded simply to Cons whose inferred type is already the expected one.\nThe rest of the paper explains the bi-directional refinement algorithm implemented in Matita [6]. The algorithm is presented in a declarative programming style by means of deduction rules. Many of the rules are syntax directed and thus mutually exclusive. The implementation given for Matita in the functional OCaml language takes advantage of the latter observation to speed up the algorithm. We will clarify in the text what rules are mutually exclusive and what rules are to be tried in sequence in case of failure.\nThe refinement algorithm is presented progressively and in a modular way. In Section 3 we introduce the mono-directional type inference algorithm for CIC implemented following the kernel type checker code (that coincides with type inference if the term is ground) of Matita. The presentation is already adapted to be extended in Section 4 to bi-directional refinement. In these two sections the external and internal syntaxes coincides. In Section 5 we augment the external syntax with placeholders and vectors of placeholders. Finally, in Section 6 we add support for coercions. In all sections we will prove the correctness of the refinement algorithms by showing that a term in external syntax accepted by the refiner is turned into a new term that is accepted by the kernel and that has the expected type. Moreover, a precise correspondence is established between the input and output term to grant that the refined term corresponds to the input one.\nFor the sake of the reader, Appendix 8 is taken from [4] with minor modifications and it shows the type checking algorithm implemented by the kernel. The syntax of the calculus and some preliminary notions are also introduced in Section 2 before starting the description of the refinement algorithm."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "2.1. Syntax. We begin introducing the syntax for CIC terms and objects in Table 1 and some naming conventions.\nTo denote constants we shall use c, c1, c2 . . . ; the special case of (co)recursively defined constants will be also denoted using f, f1, f2 . . . ; we reserve x, y, x1, x2 . . . for variables; t, u, v, t′, t′′, t1, t2 . . . for terms; T,U, V,E, L,R, T ′, T ′′, T1, T2 . . . for types and we use s, s ′, s1 . . . for sorts. We denote by Γ a context made of variables declarations (x : T ) or typed definitions (x := t : T ). We denote the capture avoiding substitution of a variable x for a term t by [x/t]. The notation [x1/t1; . . . ;xn/tn] is for simultaneous parallel substitution.\nTo refer to (possibly empty) sequences of entities of the same nature, we use an arrow notation (e.g. −→ t ). For the sake of conciseness, it is sometimes convenient to make the length of a sequence explicit, while still referring to it with a single name: we write −→ tn to mean that −→ tn is a sequence of exactly n elements and, in particular, that it is a shorthand for t1 t2 . . . tn; the index n must be a natural number (therefore the notation −−→ tn+1 refers to a non-empty sequence). The arrow notation is extended to telescopes as in −−−→ (x : t) or −−−−−→ (xn : tn) and used in binders, (co)recursive definitions and pattern matching branches. As usual, Πx : T1.T2 is abbreviated to T1 → T2 when x is not a free variable in T2. Applications are n-ary, consisting of a term applied to a non-empty sequence of terms. Inductive types Il are annotated with the number l of arguments that are homogeneous in the types of all constructors. For example consider the inductive type of vectors Vect of arity ΠA : Type.N → Type. It takes two arguments, a type and a natural number representing the length of the vector. In the types of the two constructors, Vnil : Vect A 0 and Vcons : Πm,Vect A m → A → Vect A (m + 1), every occurrence of Vect is applied to the same argument A, that is also implicitly abstracted in the types of the constructors. Thus Vect has one homogeneous argument, and will be represented by the object\nΠA : Type. inductive Vect : N→ Type := Vnil : Vect A 0 | Vcons : Πm,A→ Vect A m→ Vect A (m+ 1)\nand referred to with Vect1. This is relevant for the pattern matching construction, since the homogeneous arguments are not bound in the patterns because they are inferred from the type of the matched term. For example, to pattern match over a vector v of type (Vect N 3) the user writes\nmatch v in Vect1 return T [Vnil⇒ t1 | Vcons (m : N) (x : N) (v′ : Vect N m)⇒ t2] The inductive type Il in the pattern matching constructor is (almost) redundant, since distinct inductive types have distinct constructors; it is given for the sake of readability and to distinguish the inductive types with no constructors. In a concrete implementation it also allows to totally drop the names of the constructors by fixing an order over them: the i-th pattern will be performed on the i-th constructor of the Il inductive type.\nSince inductive types may have non homogeneous arguments, not every branch is required to have exactly the same type. The term introduced with the return keyword is a function that computes the type expected by a particular branch and also the type of the entire pattern matching. Variables −−−→ (x : t) are abstracted in the right hand side terms of ⇒.\nThe definitions of constants c (including (co)recursive constants f), inductive types Il and constructors k are collected in the syntactic category of CIC objects o.\nMetavariable occurrences, represented with ?j [t1 ; . . . ; tn], are missing typed terms equipped with an explicit local substitution. The index j enables metavariables to occur non-linearly in the term. To give an intuition of the role played by the local substitution, the reader can think of ?j [t1 ; . . . ; tn] as a call to the, still unknown, function ?j with actual arguments t1 . . . tn. The terms t1 . . . tn will be substituted for the formal arguments of the ?j function inside its body only when it will be known.\nWe omit to write the local substitution when it is the identity substitution that sends all variables in the current context with themselves. Thus ?j will be a shorthand for ?j [x1 ; . . . ; xn] when x1, . . . , xn are the variables bound in the right order in the context of the metavariable occurrence.\nThe CIC calculus extended with metavariables has been studied in [21] and the flavor of metavariables implemented in Matita is described in [26].\n2.2. Typing rules. The kernel of Matita is able to handle the whole syntax presented in the previous section, metavariables included. While we report in the Appendix 8 the full set of typing rules implemented by the kernel, here we summarise only the ones that will be reused by the refinement algorithm. We will give a less formal but more intuitive presentation of these rules, defining them with a more concise syntax. Moreover, we will put our definition in linear order, while most of them are actually mutually recursive.\nDefinition 2.1 (Proof problem (Σ)). A proof problem Σ is a finite list of typing declarations of the form Γ?j `?j : T?j .\nA proof problem, as well as a CIC term, can refer to constants, that usually live in an environment that decorates every typing rule (as in the Appendix 8). In the following presentation we consider a global well formed environment Env, basically a collections of CIC objects defining all constants and inductive types and associating them to their respective types. No refinement rule will modify this environment that plays no role in this presentation. In fact it is the task of the kernel to enable well typed definitions, inductive types and (co-)recursive functions to enter the environment.\nWe thus omit the environment Env from the input of every judgment. We will fetch from it the type T of a constant, inductive type or constructor r writing (r : T ) ∈ Env.\nWe regard CIC as a Pure Type System [7], and we denote by PTS the set of axioms. We denote by s ∈ PTS any sort of the PTS, with (s1 : s2) ∈ PTS the fact that s2 types s1, and with (s1, s2, s3) ∈ PTS the fact that a product over s1 to s2 has sort s3. CIC is a full but not functional PTS: all products are well formed but in (s1, s2, s3) ∈ PTS it may be s2 6= s3. This is because the calculus is parameterized over a predicative hierarchy Typeu for u in a given set of universe indexes. In a predicative setting, given s1 = Typeu1 and s2 = Typeu2 , s3 is defined as Typemax {u1,u2} according to some bounded partial order on the universe indexes. The details for the actual PTS used in Matita are given in [4]. We will often write simply Type when we are not interested in the universe index (e.g. in examples). We also write Type> for the biggest sort in the hierarchy, if any, or a variable universe to be later fixed to be big enough to satisfy all the required constraints.\nWe also write (s1, s2) ∈ elim(PTS) to check if an element of an inductive type of sort s1 can be eliminated to inhabit a type whose sort is s2. This is relevant for CIC since the sort of propositions, Prop, is non informative and cannot be eliminated to inhabit a data type of sort Typeu for any u (but for few exceptions described in [4] Section 6).\nProof problems do not only declare missing proofs (i.e. not all T?j have sort Prop) but also missing terms and, of particular interest for this paper, missing types.\nDefinition 2.2 (Metavariable substitution environment (Φ)). A metavariable substitution environment Φ (called simply substitution when not ambiguous) is a list of judgments of the form\nΓ?j `?j := t?j : T?j\nstating that the term t?j of type T?j in Γ?j has been assigned to ?j .\nWe now anticipate the typing judgment of the kernel. A formal definition of well formedness for Σ and Φ will follow.\nDefinition 2.3 (Typing judgment). Given a term t, a proof problem Σ and a substitution Φ, all assumed to be well formed, we write\nΣ, Φ, Γ ` t : T to state that t is well typed of type T .\nWhen Σ, Φ, Γ ` t : T the type T is well typed and its type is either a metavariable or a sort s ∈ PTS.\nThe typing judgment implemented in our kernel is an extension of the regular typing judgment for CIC [35, 23, 13]. It is described in [4] and reported in the Appendix 8. Here we recall the main differences: • Substitution of a regular variable x for a term t is extended with the following rule for metavariables:\n?j [t1 ; . . . ; tn][x/t] =?j [t1[x/t] ; . . . ; tn[x/t]]\n• The conversion relation (denoted by ↓) is enlarged allowing reduction to be performed inside explicit substitution for metavariables:\nΓ ` ti ↓ t′i i ∈ {1 . . . n} Γ `?j [t1 ; . . . ; tn] ↓?j [t′1 ; . . . ; t′n]\n• The following typing rules for metavariables are added: y1 : T1 ; . . . ; yn : Tn `?j : T?j ∈ Σ Γ ` ti : Ti[y1/t1 ; . . . ; yi−1/ti−1] i ∈ {1 . . . n}\nΓ ` WF(?j [t1 ; . . . ; tn]) (y1 : T1 ; . . . ; yn : Tn `?j : T?j ) ∈ Σ Γ ` WF(?j [t1 ; . . . ; tn])\nΓ `?j [t1 ; . . . ; tn] : T?j [y1/t1 ; . . . ; yn/tn] Moreover, in many situations a metavariable occurrence is also accepted as a valid sort, marking it so that it cannot be instantiated with anything different from a sort. This additional labelling will be omitted, being marginal for the refinement algorithm.\nThe technical judgment Γ ` WF(?j [t1 ; . . . ; tn]) states that a metavariable occurrence ?j [t1 ; . . . ; tn] is well formed in Γ.\nIn all the previous rules we assumed access to a global well formed proof problem Σ and substitution Φ. Both Σ and Φ are never modified by the judgments implemented in the kernel.\nWe now present the well formedness conditions, corresponding to the judgments `WF presented in the Appendix 8.\nDefinition 2.4 (Metavariables of term/context (M)). Given a term t, M(t) is the set of metavariables occurring in t. Given a context Γ,M(Γ) is the set of metavariables occurring in Γ.\nThe function M is at the base of the order relation defined between metavariables. Definition 2.5 (Metavariables order relation ( Σ )). Let Σ be a proof problem. Let <Σ be the relation defined as: ?n1 <Σ ?n2 iff ?n1 ∈ M(Γ?n2) ∪ M(T?n2). Let Σ be the transitive closure of <Σ .\nDefinition 2.6 (Valid proof problem). A proof problem Σ is a valid proof problem if and only if Σ is a strict partial order (or, equivalently, if and only if Σ is an irreflexive relation).\nThe intuition behind Σ is that the smallest ?j (or one of them since there may be more than one) does not depend on any other metavariable (e.g. M(Γ?j ) = ∅ and M(T?j ) = ∅ where Γ?j `?j : T?j ∈ Σ). Thus instantiating every minimal ?j with a metavariable free term will give a new Σ in which there is at least one ?j not depending on any other metavariable (or Σ is empty). This definition is the key to avoid circularity in the following definitions.\nIn the rules given in Appendix 8 the partial order is left implicit by presenting Σ as an ordered list. However, as proved by Strecker in his Ph.D. thesis [32], the order is not preserved by unification and thus in any realistic implementation Σ is to be implemented as a set and the fact that Σ remains a partial order must be preserved as an invariant.\nDefinition 2.7 (Well formed context (WF(Γ))). Given a well formed proof problem Σ, a context Γ = y1 : T1, . . . , yn : Tn is well formed (denoted by WF(Γ)) if M(Γ) ⊆ Σ and for every i y1 : T1, . . . yi−1 : Ti−1 ` yi : Ti Definition 2.8 (Well formed proof problem (WF(Σ))). A valid proof problem Σ is a wellformed proof problem (denoted by WF(Σ)) if an only if for all (Γ?j `?j : T?j ) ∈ Σ we have Σ,Γ?j ` T?j : s and s ∈ PTS.\nDefinition 2.9 (Well formed substitution (WF(Φ))). Given a well formed proof problem Σ, a substitution Φ is well formed (denoted byWF(Φ)) if for every (Γ?j `?j := t?j : T?j ) ∈ Φ we have Σ, ∅,Γ?j ` t?j : T?j .\nThe well formedness definitions given so far are actually implemented by the kernel in a more precise but less intuitive way. We thus refer to the kernel judgments in the following definition, that will be used in the specification of all refinement rules.\nDefinition 2.10 (Well formed status (WF(Σ, Φ, Γ))). Given a proof problem Σ, a substitution Φ and a context Γ, the triple Σ, Φ, Γ is well formed (denoted by WF(Σ, Φ, Γ)) when WF(Σ) and WF(Φ) and WF(Γ).\nWe shall sometimes omit Γ, considering it equal to a default, well formed context, like the empty one. The recursive operation of applying a substitution Φ to a term t is denoted by Φ(t) and acts as the identity for any term but metavariables contained in Φ, on which it behaves as follows:\nΦ(?j [t1 ; . . . ; tn]) = t?j [y1/t1 ; . . . ; yn/tn] when (y1 : T1; . . . ; yn : Tn `?j := t?j : T?j ) ∈ Φ Note that, thanks to the extensions to the type checking rules made in Definition 2.3, substitution application is type preserving. Substitutions do apply also to well formed proof problems in the following way:\nΦ(Γ?j `?j : T?j ) = Φ(Γ?j ) `?j : Φ(T?j ) (for each ?j ∈ Σ) The substitution application operation is seldom used explicitly, since all judgments\ntake as input and give back a substitution. Nevertheless it will be used in the examples.\nDefinition 2.11 (Weak-head normalization (.whd)). Given a context Γ, substitution Φ and proof problem Σ, all assumed to be well formed, it computes the weak head normal form\nof a well typed term t according to the reduction rules of CIC. It is denoted by:\nΣ, Φ, Γ ` t .whd t′\nNote that ?j is in weak head normal form iff ?j 6∈ Φ. By abuse of notation we will write Σ, Φ, Γ ` t1 .whd Πx1 : T1 . . .Πxn : Tn.tn+1 to mean that for all i ∈ {1 . . . n} Σ, Φ, Γ;x1 : T1; . . . ;xi−1 : Ti−1 ` ti .whd Πxi : Ti.ti+1 and Σ, Φ, Γ ` tn+1 .whd tn+1. Such repeated use of weak head computation to produce spines of dependent products occur frequently in the kernel and in the refinement rules, especially when dealing with inductive types.\nDefinition 2.12 (Conversion (↓)). Given a proof problem Σ, substitution Φ and context Γ, all assumed to be well formed, and two terms t1 and t2, it verifies if t1 and t2 have a common normal form according to the rules of CIC given in Appendix 8. It is denoted by:\nΣ, Φ, Γ ` t1 ↓ t2"
    }, {
      "heading" : "3. Mono-directional refinement",
      "text" : "We now present the mono-directional refinement algorithm for CIC implemented in the old versions of Matita (0.5.x) and directly inspired by the rules for type checking implemented in the kernel. In this section we assume the external syntax to coincide with the syntax of terms. Hence the algorithm actually performs just type inference. Nevertheless, we already organize the judgments in such a way that the latter extension to bi-directionality will be achieved just by adding new typing rules.\n3.1. Specification. To specify what is a refinement algorithm we must first introduce the notion of proof problem refinement. Intuitively, a pair (proof problem, substitution) is refined by another pair when the second is obtained by reducing some proof obligations to new ones. It thus represents an advancement in the proof discovery process.\nDefinition 3.1 (Proof problem refinement (≤)). We say that Σ′,Φ′ refines Σ,Φ (denoted by Σ′,Φ′ ≤ Σ,Φ) when Φ ⊂ Φ′ and for every (Γ?j `?j : T?j ) ∈ Σ either (Γ′?j `?j : T ′ ?j ) ∈ Σ′ or (Γ′?j `?j := t?j : T ′ ?j ) ∈ Φ′ where Γ′?j = Φ ′(Γ?j ) and T ′ ?j = Φ′(T?j ). Specification 3.2 (Refiner in type inference mode (R⇑)). A refiner algorithm R in type inference mode ⇑ takes as input a proof problem, substitution and context, all assumed to be well formed, and a term t. It fails or gives in output a new proof problem, a new substitution, a term t′ and a type T ′. It is denoted by:\n(Σ, Φ) Γ ` t R ⇑ t′ : T ′ (Σ′, Φ′)\nPrecondition: WF(Σ, Φ, Γ)\nPostcondition (parametric in 4):\nWF(Σ′, Φ′) ∧ Σ′,Φ′ ≤ Σ,Φ ∧ Σ′, Φ′, Γ ` t′ : T ′ ∧ t′ 4 t\nThe specification is parametric in the 4 relation that establishes a correspondence between the term t to be refined and the refiner output t′. In order to prove correctness, we are only interested in admissible 4 relations defined as follows.\nDefinition 3.3 (Admissible relations (4)). A partial order relation 4 is admissible when for every term t1 in external syntax and t2 and T in internal syntax and for every variable x occurring free only linearly in T we have that t1 4 t2 implies T [x/t1] 4 T [x/t2].\nAdmissibility for equivalence relations correspond to asking the equivalence relation to be a congruence.\nWhen the external syntax corresponds to the term syntax and coercions are not considered, we can provide an implementation that satisfies the specification by picking the identity for the 4 relation. Combined with Σ′,Φ′ ≤ Σ,Φ, the two postconditions imply that Φ(t′) must be obtained from t simply by instantiating some metavariables. In Sections 5 and 6, we shall use weaker definitions of 4 than the identity, allowing replacement of (vectors of) placeholders with (vectors of) terms and the insertion of coercions as results of the refinement process. All the 4 relations considered in the paper will be large partial orders over terms of the external syntax (that always include the internal syntax).\nWe will now proceed in presenting an implementation of a refinement algorithm in type inference mode R⇑ . The implementation is directly inspired by the type checking rules used in the kernel. However, since refinement deals with terms containing flexible parts, conversion tests need to be replaced with unification tests. In a higher order and dependently typed calculus like CIC, unification is in the general case undecidable. What is usually implemented in interactive theorem provers is an essentially fist order unification algorithm, handling only some simple higher order cases. The unification algorithm implemented in Matita goes beyond the scope of this paper, the interested reader can find more details in [26, 5]. Here we just specify the expected behavior of the unification algorithm.\nSpecification 3.4 (Unification ( ?≡ −U)). An unification algorithm takes as input a proof problem, a substitution and a context, all assumed to be well formed, and two well typed terms t1 and t2. It fails or gives in output a new proof problem and substitution. It is denoted using the following notation where • can either be = or be omitted. In the former case universe cumulativity (a form of sub-typing) is not taken in account by unification.\n(Σ, Φ) Γ ` t1 ?≡ t2 U• (Σ′, Φ′)\nPrecondition:\nWF(Σ, Φ, Γ) ∧ Σ, Φ, Γ ` t1 : T1 ∧ Σ, Φ, Γ ` t2 : T2 Postcondition:\nWF(Σ′, Φ′) ∧ Σ′,Φ′ ≤ Σ,Φ ∧ Σ′, Φ′, Γ ` t′1 ↓• t′2\n3.2. Implementation.\n3.2.1. Additional judgments. For the sake of clarity we prefer to keep the same structure for the mono and bi-directional refiners. We thus give the definition of some functions that are trivial in the mono-directional case, but will be replaced by more complex ones in the following sections.\nEven if we presented the syntax of CIC using the same category terms, types and sorts, some primitive constructors (like the λ and Π abstractions) expect some arguments to be types or sorts, and not terms. A type level enforcing algorithm forces a term in external syntax to be refined to a valid type.\nSpecification 3.5 (Type level enforcing (F)). A type level enforcing algorithm takes as input a proof problem Σ, a substitution Φ and a context Γ, all assumed to be well formed, and a term T . It fails or it returns a new term T ′, a sort s, a new substitution Φ′ and proof problem Σ′. It is denoted by:\n(Σ, Φ) Γ ` T F T ′ : s (Σ′, Φ′) Precondition: WF(Σ, Φ, Γ) Postcondition (parametric in 4):\nWF(Σ′, Φ′) ∧ Σ′,Φ′ ≤ Σ,Φ ∧ Σ′, Φ′, Γ ` T ′ : s ∧ s ∈ PTS ∧ T ′ 4 T\nNote that one may want to accept a metavariable as the sort s, eventually labelling it in such a way that the unification algorithm will refuse to instantiate it with a different term. The choice must be consistent with the one taken in the implementation of the kernel.\nThe task of checking if a term has the right type is called refinement in type forcing mode\nand it will be denoted by R⇓ . In the mono-directional case, R⇓ will be simply implemented calling the C algorithm that will handle coercions in Section 6 but which, at the moment, only verifies that no coercion is needed by calling the unification procedure.\nSpecification 3.6 (Explicit cast (C)). A cast algorithm takes as input a proof problem Σ, a substitution Φ and a context Γ, all assumed to be well formed, and a term t with its inferred type T and expected type T ′. It fails or it returns a new term t′ of type T ′, a new proof problem Σ′ and substitution Φ′. It is denoted by:\n(Σ, Φ) Γ ` t : T ?≡ T ′ C t′ (Σ′, Φ′) Precondition: WF(Σ, Φ, Γ) ∧ Σ, Φ, Γ ` t : T ∧ Σ, Φ, Γ ` T ′ : s Postcondition (parametric in 4):\nWF(Σ′, Φ′) ∧ Σ′,Φ′ ≤ Σ,Φ ∧ Σ′, Φ′, Γ ` t′ : T ′ ∧ t′ 4 t\nSpecification 3.7 (Refiner in type forcing mode (R⇓)). A refiner algorithm R in type forcing mode ⇓ takes as input a proof problem Σ, a substitution Φ and a context Γ, all assumed to be well formed, and a term t together with its expected well formed type T . It fails or returns a term t′ of type T , a new proof problem Σ′ and substitution Φ′. It is denoted by:\n(Σ, Φ) Γ ` t : T R ⇓ t′ (Σ′, Φ′)\nPrecondition: WF(Σ, Φ, Γ) ∧ Σ, Φ, Γ ` T : s\nPostcondition (parametric in 4):\nWF(Σ′, Φ′) ∧ Σ′,Φ′ ≤ Σ,Φ ∧ Σ′, Φ′, Γ ` t′ : T ∧ t′ 4 t\n3.2.2. Notational conventions. The arguments Σ and Φ will be taken as input and returned as output in all rules that define the refiner algorithm. To increase legibility we adopt the following notation, letting Σ and Φ be implicit. Each rule of the form\n(rule)\nΓ ` t t′ Γ ` t′ t′′ Γ ` t t′′\nhas to be interpreted as:\n(rule)\n(Σ, Φ) Γ ` t t′ (Σ′,Φ′) (Σ′, Φ′) Γ ` t′ t′′ (Σ′′,Φ′′) (Σ, Φ) Γ ` t t′′ (Σ′′,Φ′′)\nMoreover we will apply this convention also to rules not returning Σ or Φ as if they were returning the Σ or Φ taken as input.\nNote that the Σ′ and Φ′ returned by all rules considered in this paper are well formed and are also a proof problem refinement of the Σ and Φ provided as input. Being a proof problem refinement is clearly a transitive relation. Thus we have for free that all the omitted pairs (proof problem, substitution) are refinements of the initial ones.\n3.2.3. Role of the relations and their interaction. In this paragraph we shortly present the role played by the relations R⇑ , R⇓ , C and F introduced so far and the auxiliary ones\nET and Et that will be specified when needed.\nThe relation R⇑ links a term with its inferred type, while R⇓ links a term with the type expected by its context. R⇓ will thus exploit the extra piece of information not only checking that the inferred type unifies with the expected one, but also propagating this information to its recursive calls on subterms (when possible). R⇑ and R⇓ will be defined in a mutually recursive way.\nThe relation F links a term with its refinement asserting that the refinement is a type. This is relevant when typing binders like (λx : t.t′), where t is required to be a type. In its simplest formulation the relation is a simple assertion, linking a type with itself. In Section 6 the refinement relation 4 will admit to link a term t that is not a type with a function applied to t that turns its input into a type. For example t may be a record containing a type and F may link it with (πn t), where πn is the projection extracting the type from the record. F is recursively defined in terms of R⇑\nThe relation C links a term t, its inferred type T1 and the type expected by its context T2 with a refinement of the term t ′ asserting that the refined term has type T2. In its simple formulation the relation is a simple assertion that T1 and T2 are the same and thus links t with itself. In Section 6 the refinement relation 4 will admit to explicitly cast t. For example a natural number n of type N may be casted into the rationals Q refining it to (λx : N.x/1) n. The C relation is non recursive.\nThe relations ET and Et are auxiliary relations only used to ease the presentation\nof the R⇑ and R⇓ relations in the case of applications. Both auxiliary relations are thus recursively defined with R⇑ and R⇓ .\n3.2.4. Rules for terms. We now give an implementation for the refiner in both modes and for the auxiliary judgments. The implementation is parametric on the unification algorithm, that is not described in this paper.\n( C −ok)\nΓ ` T1 ?≡ T2 U\nΓ ` t : T1 ?≡ T2 C t\n( R⇓ −default)\nΓ ` t R ⇑ t′ : T ′ Γ ` t′ : T ′ ?≡ T C t′′\nΓ ` t : T R ⇓ t′′\n( F −ok)\nΓ ` T R ⇑ T ′ : s s1 ∈ PTS Γ ` T ′ : s ?≡ s1 C T ′′\nΓ ` T F T ′′ : s\nNote that s1 is arbitrary, and the actual code prefers the predicative sorts Typeu over Prop. This is the only rule defined in this section to be non syntax oriented: in case of an incorrect choice of s1, backtracking is required. The actual algorithm implemented in Matita performs the choice of s1 lazily to remain backtracking free 5.\n( R⇑ −variable)\n(x : T ) ∈ Γ or (x := t : T ) ∈ Γ\nΓ ` x R ⇑ x : T\n( R⇑ −constant)\n(r : T ) ∈ Env r ∈ {k, I, c}\nΓ ` r R ⇑ r : T\n( R⇑ −sort)\n(s1 : s2) ∈ PTS\nΓ ` s1 R⇑ s1 : s2\n( R⇑ −meta)\n(Γ?j `?j : T?j ) ∈ Σ or (Γ?j `?j := t?j : T?j ) ∈ Φ Γ?j = −−−−→ xn : Tn Γ ` ti : Ti[ −−−−−−→ xi−1/t ′ i−1] R⇓ t′i i ∈ {1 . . . n}\nΓ `?j [ −→ tn ] R⇑ ?j [ −→ t′n ] : T?j [ −−−→ xn/t ′ n]\nNote that the operation of firing a β-redex must commute with the operation of applying a substitution Φ. Consider for example the term v = (λx.?j [x]) u and the substitution Φ = {x : T `?j := t(x) : T (x)}. If one applies the substitution first, and then reduces the redex obtains t(u), whose type is T (u). If one fires the redex fist, the fact that x is substituted by u in ?j is recorded in the local substitution attached to the metavariable\n5Laziness will be no longer sufficient to avoid backtracking when we will add additional rules to handle coercions in Section 6.\ninstance. Indeed ∅, ∅, ∅ ` v .whd?j [u] and Φ(?j [u]) = t(u) : T (u). Therefore ?j [u] is given the type T (u) by the rule ( R⇑ −meta).\n( R⇑ −letin)\nΓ ` T F T ′ : s Γ ` t : T ′ R ⇓ t′ Γ;x := t′ : T ′ ` u R ⇑ u′ : T2\nΓ ` let (x : T ) := t in u R ⇑ let (x : T ′) := t′ in u′ : T2[x/t′1]\n( R⇑ −lambda)\nΓ ` T1 F T ′1 : s1 Γ;x : T ′1 ` t R⇑ t′ : T\nΓ ` λx : T1.t R⇑ λx : T ′1.t ′ : Πx : T ′1.T\n( R⇑ −product)\nΓ ` T1 F T ′1 : s1 Γ;x : T ′1 ` T2 F T ′2 : s2 (s1, s2, s3) ∈ PTS\nΓ ` Πx : T1.T2 R⇑ Πx : T ′1.T ′ 2 : s3\nWe now state the correctness theorem holding for all the rules presented so far and for the few ones that will follow. The proof is partitioned in the following way: here we state the theorem, introduce the proof method we adopted and prove the theorem for the simple rules presented so far. Then we will introduce more complex rules, like the rule for application, and we will prove for each of them the correctness theorem.\nTheorem 3.8 (Correctness). The C , F , R⇑ , R⇓ , ET , and Et algorithms defined by the set of rules presented in this section obey their specification for all admissible 4 relations that include the identity for terms in the internal syntax. In particular, the algorithms are correct when the identity relation is picked for 4.\nProof. We assume the unification algorithm to be correct w.r.t. its own specification. For every judgment, the proof is by induction on the proof tree. For each rule, we assume that the precondition of the judgment holds for the rule conclusion and that the appropriate postcondition holds by induction hypothesis for every hypothesis. We need to prove that the precondition of every hypothesis holds and that the postcondition of the conclusion holds too. The proofs are mostly trivial for the rules presented so far. In particular, the proof for each rule R⇑ −name or R ⇓ −name follows from the corresponding rule K−name reported in the Appendix 8. We will shortly introduce the rules dealing with applicatios together with their correctness proofs since applications are handled slightly differently from the way they are processed by the kernel.\nThe next rule deals with applications which are n-ary in our implementation of CIC. In a calculus without dependent types, n-ary applications could be handled simply by putting the head function type in the form of a spine of n products and then by verifying that the type of each argument matches the corresponding expected type. In the presence of dependent types, however, it is possible to write functions whose arity depends on the arguments passed to the function. For instance, a function f could be given type ∀n : N.(repeat N n) where (repeat N n) reduces to N→ . . .→ N where the number of products is exactly n. For this reason, the only possibility is to process applications one argument at a time, checking at every step if the function still accepts more arguments. We implement this with an additional judgment\nΓ ` t −−−−−−−−−→ (xi := vi : Ti) : T |N −→un ET v : V\ncalled “eat products” to be specified and implemented immediately after the ( R⇑ −appl) rule.\n( R⇑ −appl)\nΓ ` t R ⇑ t′ : T Γ ` t′ : T |N −−→un+1 ET v : V\nΓ ` t −−→un+1 R⇑ v : V\nSpecification 3.9 (Eat products (ET )). The E T\nalgorithm refines an n-ary application by consuming an argument at a time. It takes as input a proof problem Σ, a substitution Φ and a context Γ, all assumed to be well formed, the part of the already processed application t (x1 := v1 : T1) . . . (xr := vr : Tr) together with its type T , and the list of arguments yet to be checked. The notation (xi := vi : Ti) means that the i-th already processed argument has type Ti and is consumed by a product that binds the variable xi. The algorithm fails or returns the refined application v together with its type V, a new substitution Φ′ and proof problem Σ′. It is denoted by:\n(Σ, Φ) Γ ` t −−−−−−−−−−→ (xr := vr : Tr) : T |N −→uk ET v : V (Σ′, Φ′)\nPrecondition:\nWF(Σ, Φ, Γ) ∧ Σ, Φ, Γ ` vi : Ti i ∈ {1 . . . r} ∧ Σ, Φ, Γ ` t v1 . . . vr : T Postcondition (parametric in 4):\nWF(Σ′, Φ′) ∧ Σ′,Φ′ ≤ Σ,Φ ∧ Σ′, Φ′, Γ ` v : V ∧ v 4 t v1 . . . vr u1 . . . uk The applicative case is one of the two most complicated rules. Moreover, the refinement algorithm for the application does not mimic the one used in the kernel. Therefore we show the correctness of the ( R⇑ −appl) rule and of the implementation of the E T algorithm.\nCorrectness of ( R⇑ −appl). The only rule precondition is WF(Σ, Φ, Γ) that is also the precondition for the first premise. By induction hypothesis on the first premise we know Σ′, Φ′, Γ ` t′ : T where Σ′, Φ′ are implicitly returned by the first call and passed to the second one. Moreover Σ′, Φ′ ≤ Σ, Φ and t′ 4 t. Therefore the preconditions for the second premise are satisfied. By induction hypothesis on the second premise we know Σ′′, Φ′′, Γ ` v : V where Σ′′, Φ′′ are implicitly returned by the second call and by the rule as a whole. Moreover Σ′′, Φ′′ ≤ Σ′, Φ′ and v 4 t′ −−→un+1. By transitivity of proof problem refinement, we also have Σ′′, Φ′′ ≤ Σ, Φ. Moreover, since 4 is admissible, we also\nhave v 4 t′ −−→un+1 4 t −−→un+1. All post-conditions have been proved and therefore the rule is correct.\nThe ET algorithm is implemented as follows.\n(ET−empty) Γ ` t −−−−−−−−−−→ (xr := vr : Tr) : T |N ET t −→vr : T\nCorrectness of the (ET−empty) rule is trivial.\n(ET−prod)\nΓ ` T .whd Πx : U1.T1 Γ ` u1 : U1 R⇓ u′1 Γ ` t −−−−−−−−−−→ (xr := vr : Tr) (x := u ′ 1 : U1) : T1[x/u ′ 1] |N −→un ET v : V\nΓ ` t −−−−−−−−−−→ (xr := vr : Tr) : T |N u1 −→un ET v : V\nCorrectness of (ET−prod). Let Σ, Φ be the well formed pair taken as input by the rule and passed to the second premise, that returns the well formed pair Σ′, Φ′. Similarly, let Σ′′, Φ′′ be the well formed pair given in output by the second premise and by the whole rule. By induction hypotheses Σ′, Φ′ ≤ Σ, Φ and Σ′′, Φ′′ ≤ Σ′, Φ′ and thus Σ′′, Φ′′ ≤ Σ, Φ as required. By the rule pre-condition, T is well typed in Σ, Φ, Γ and so are U1 and T1 obtained by reduction. Thus the premises of the second rule are all satisfied and, by induction hypothesis, Σ′, Φ′, Γ ` u′1 : U1 ∧ u′1 4 u1. By rules K−appl− rec and K−appl− base applied to the rule pre-condition t v1 . . . vr : T we get t v1 . . . vr u ′ 1 : T1[x/u ′ 1]. Since all preconditions for the third premise are satisfied, by induction hypothesis we know Σ′′, Φ′′, Γ ` v : V and v 4 t v1 . . . vr u′1 u2 . . . un. By admissibility of 4 we conclude also v 4 t v1 . . . vr u1 . . . un. Since all post-conditions have been proved, the rule is correct.\n(ET−flexible)\nΓ ` T .whd ?j or Γ ` T .whd ?j w1 . . . wl Γ ` u1 R⇑ u′1 : U1 Σ Σ ∪ {Γ; −−−−→ xr : Tr;x : U1 `?k : Type>} Γ ` T ?≡ Πx : U1.?k[ −−−→ xr/vr;x/x] U Γ ` t −−−−−−−−−−→ (xr := vr : Tr) (x := u ′ 1 : U1) : ?k[ −−−→ xr/vr;x/u ′ 1] |N −→un ET v : V\nΓ ` t −−−−−−−−−−→ (xr := vr : Tr) : T |N u1 −→un ET v : V\nCorrectness of (ET−flexible). The proof is similar to the one for the ET−prod rule. We only list the major differences here. The fact Σ′, Φ′, Γ ` u′1 : U1 is now obtained by induction hypothesis on the second premise. The role of T1 is now played by ?k[ −−−→ xr/vr;x/x]. The induction hypothesis on the third premise yields Σ′′′, Φ′′, Γ ` T ↓ Πx : U1.?k[ −−−→ xr/vr;x/x] that was previously given directly by the rule pre-conditions (up to reduction of T ). The rest of the proof follows without any changes. The only remaining check to be performed\nis the well-formedness of ?k[ −−−→ xr/vr;x/x] that follows from rule K−meta using the rule precondition Σ, Φ, Γ ` vi : Ti i ∈ {1 . . . r}.\nAnother reason for the complexity of the ET algorithm is the need to infer a dependent type for the function f when its type is flexible (a metavariable). We now show an example of this scenario and an execution trace for the algorithm.\nExample 3.10 (Inference of a maximally dependent type). Consider the following input, where c1, c2, c3, P1, P2 are such that ` c1 : N and ` c2 : P1(c1) and c3 : P2(c1, c2):\n{`?F : Type}, ∅, ∅ ` λf :?F .f c1 c2 c3 The rule (ET−flexible) matches the input and since the argument c1 has type N, Σ is extended as follows:\nΣ = { `?F : Type; x : N `?S :?T ; x : N `?T : Type}\nThen ?F gets unified with Πx : N.?S obtaining the following substitution: Φ = {`?F := Πx : N.?S : Type}\nThe new type for the head of the application, morally (f c1), represented as f (x := c1), is ?S [c1/x]. In the following call to (ET−flexible), the argument c2 has type P1(c1). Σ is thus extended as follows:\nΣ = { x : N; y : P1(c1) `?U :?V ; x : N; y : P1(c1) `?V : Type;\nx : N `?S :?T ; x : N `?T : Type}\nThen ?S [x/c1] is unified with Πy : P1(c1).?U [x/c1] obtaining\nΦ = { `?F := Πx : N.?S : Type; x : N `?S := Πy : P1(x).?U :?T ; x : N `?T := Type : Type}\nThe new type for the head of the application (f c1 c2) is ?U [c1/x; c2/y]. In the following call to (ET−flexible), the argument c3 has type P2(c1, c2). Σ is thus extended as follows:\nΣ = { x : N; y : P1(c1); z : P2(c1, c2) `?Z :?W ; x : N; y : P1(c1); z : P2(c1, c2) `?W : Type;\nx : N; y : P1(c1) `?U :?V ; x : N; y : P1(c1) `?V : Type}\nThen ?U [x/c1; y/c2] is unified with Πz : P2(c1, c2).?Z [x/c1; y/c2] obtaining\nΦ = { `?F := Πx : N.?S : Type; x : N `?S := Πy : P1(x).?U :?T ; x : N `?T := Type : Type;\nx : N; y : P1(c1) `?U := Πz : P2(x, y).?Z :?V ; x : N; y : P1(c1) `?V := Type : Type}\nThe final instantiation for ?F is thus the maximally dependent type\nΦ(?F ) = Πx : N.Πy : P1(x).Πz : P2(x, y).?Z : Type\nwhere Σ = { x : N; y : P1(c1); z : P2(c1, c2) `?Z :?W ;\nx : N; y : P1(c1); z : P2(c1, c2) `?W : Type}\nWe conclude now the description of the refinement algorithm in type inference mode. The final missing rule is the most complicated one and deals with pattern matching. It is reported in Figure 1.\nThe rule has been slightly simplified: in the actual implementation of Matita the test (s,Φ(?1)) ∈ elim(PTS) is relaxed to accept elimination of inhabitants of non informative data types in all cases under the restriction that the data type must be small. Intuitively, smallness corresponds to the idea that the inhabitant of the data type would be non informative even if declared in Type. Typical examples are empty types and the Leibniz equality type. A precise definition of smallness together with the corresponding rules for pattern matching can be found in [4].\nNote that the return type T ′ is usually an anonymous function, beginning with lambda abstractions. Thus the type inferred for the pattern match construct is a β-redex. In fact the actual code of Matita post-processes that type firing (r + 1) β-redexes.\nTheorem 3.11 (Termination). The R⇑ algorithm defined by the set of rules presented in this section including R⇓ , C , F and ET is terminating.\nProof. The proof is by structural induction of the syntax of terms. The rules ( C −ok), ( R⇑ −variable), (R ⇑ −sort) and (R ⇑ −constant) are base cases. The first one clearly terminates if the unification algorithm ?≡ terminates, while the others terminate since Γ and Env are finite and the test (s1, s2) ∈ PTS is also terminating. Now that we proved that all the rules for C , amounting to only one for the mono directional refiner, terminate, we can consider the ( R⇓ −default) and ( F −ok) as aliases for R⇑ , as if we were inlining their code.\nBy induction hypothesis R⇑ (and R⇓ being now an alias) terminates when called on smaller terms. The rule ( R⇑ −meta) terminates because Φ and Σ are finite, thus lookups are terminating, and calls to R⇓ are done on smaller terms, so the induction hypothesis applies. The rule ( R⇑ −letin) calls F , R ⇓ and R⇑ on smaller terms, thus terminates by induction hypothesis. The same holds for ( R⇑ −lambda) and (R ⇑ −product). To prove that ( R⇑ −appl) terminates we use the induction hypothesis on the first premise and we are left to prove that ET terminates as well. Note that ET calls R⇓ and R⇑ on proper subterms of the n-ary application, thus the induction hypothesis applies and will be used in the next paragraph.\nWe show ET terminates by induction on the list of arguments (i.e. the list of terms after |N) assuming that the input term T is a well typed type. Thanks to the correctness property of R⇑ , ( R⇑ −appl) always passes to ET a well typed type. The rule (ET−empty) clearly terminates. The recursive call in the rule (ET−prod) is on a shorter list of arguments, thus is terminating, and the call to R⇓ terminates by induction hypothesis. The term T [x/u′1] is a well typed type thanks to the subject reduction property of CIC, and the fact that the variable x and the term u′1 have the same type (postcondition of R⇓ , called with expected type U1). The call to .whd is terminating because T is well typed and CIC reduction rules, on well typed terms, form a terminating rewriting system.\nThe rule (ET−flexible) terminates because of the same arguments. The only non obvious step is that Πx : U1.?k[ −−−→ xr/vr;x/x] is a well typed type. The metavariable ?k is declared in Σ of type Type>, thus cannot be instantiated with a term. Moreover, since CIC is a full PTS, the product Πx : U1.?k is well typed of sort Type>.\nThe recursive calls in the last rule ( R⇑ −match) are always on smaller terms. We are\nleft to prove that the expected type (T ′ −−→ M ′jr (kj −→ ?ul −→ yjnj )) passed to the recursive call made on the last line is indeed a well typed type. The term T ′ is obtained using R⇓ , and we thus know it is a function of type Π −−−−→ yr : G ′ r.Πx : Il −→ ?ul −→yr .?1. The arguments −−→ M ′jr are as many as expected and have the right types according to the environment Env (first two lines) and thanks to the fact that the substitutions M ′ji = M j i [ −−−−→ xl/?ul] preserves their types. Finally, the term (kj −→ ?ul −→ yjnj ) has type Il −→ ?ul −→ M jr that is the expected one. Thus (T ′ −−→ M ′jr (kj −→ ?ul −→ yjnj )) has type ?1 that is a well typed type according to Σ.\n3.3. Implementation remarks. The choice of keeping Σ and Φ separate is important and motivated by the fact that their size is usually very different. While the number of proof problems in Σ is usually small, the substitution Φ may record, step by step, the whole proof input by the user and can grow to an arbitrary size. Each metavariable must be either declared in Σ or assigned in Φ, thus to know if a metavariable belongs to Φ it is enough to test if it does not belong to Σ. Knowing if a metavariable is instantiated is a very common operation, needed for example by weak head normalization, and it must thus be possible to implement it efficiently.\nAnother important design choice is to design the kernel of the system so that it handles metavariables [4]. This enables to reuse a number of functionalities implemented in the kernel also during the refinement process, like an efficient reduction machinery. Also note that the extensions made to the type checker described in Definition 2.3 become dead code when the type checker is called on ground terms, and thus do not increase the size of the trusted code of the system.\nLast, it is worth pointing out that the algorithm is mostly independent from the representation chosen for bound variables. Matita is entirely based on De Bruijn indexes, but the tedious lift function is mostly hidden inside reduction and only directly called twice in the actual implementation of this algorithm. In particular it is necessary only to deal with the types of variables that must be pulled from the context. This potentially moves the type under all the context entries following the variable declaration or definition, thus the type must be lifted accordingly.\n3.3.1. Rules for objects. Objects are declarations and definitions of constants, inductive types and recursive and co-recursive functions that inhabit the environment Env. Exactly like terms, the user writes objects down using the external syntax and the objects need to be refined before passing them to the kernel for the final check before the insertion in the environment.\nDefinition 3.12 (Type checking for objects (Env ` WF)). The type checking algorithm for CIC objects takes as input a proof problem Σ and a substitution Φ, all assumed to be well formed, and an object o. It is denoted by:\nEnv ∪ (Σ,Φ, o) `WF and states that o is well typed.\nThis algorithm is part of the kernel, and described in Appendix 8. It is the basis for the construction of the corresponding refinement algorithm for objects, that is specified as follows.\nSpecification 3.13 (Refiner for objects (R)). A refiner algorithm R for CIC objects takes as input a proof problem Σ and a substitution Φ, all assumed to be well formed, and an object o. It fails or returns an object o′, a proof problem Σ′ and substitution Φ′. It is denoted by:\n(Σ, Φ) ` o R o′ (Σ′, Φ′) Precondition:\nWF(Σ, Φ)\nPostcondition (parametric in 4):\nWF(Σ′, Φ′) ∧ Σ′,Φ′ ≤ Σ,Φ ∧ Env ∪ (Σ′,Φ′, o′) `WF ∧ o′ 4 o\nNote that an object can be a block of mutually recursive definitions or declarations, each one characterized by a different type. Thus the R rule does not return a single type, but a new object together with a new metavariable environment Σ’ and substitution Φ’. When Σ′ is not empty, all the metavariables in Σ′ correspond to proof obligations to be proved to complete the definition of the object, necessary to convince the system to accept the object definition. This is especially useful for instance in the formalization of category theory where definitions of concrete categories are made from definitions of terms (objects and morphisms) together with proofs that the categorical axioms hold. In such definitions, objects and functors are immediately fully specified, while the proof parts are turned into proof obligations. Spiwack’s Ph.D. thesis [31] discusses this issue at length as a motivation for a complete re-design of the data type for proofs in Coq. The new data type is essentially the one used in this paper and it will be adopted in some forthcoming version of Coq. The old data type, instead, did not take the Curry-Howard isomorphism seriously in the sense that partial proofs were not represented by partial proof terms and the refinement of an object could not open proof obligations. This problem was already partially addressed by Sozeau [29] where he added a new system layer around the refiner to achieve the behavior that our refiner already provides.\n( R −axiom)\n` T F T ′ : s\n` axiom c : T R axiom c : T ′\n( R −definition)\n` T F T ′ : s ` t : T ′ R ⇓ t′\n` definition c : T := t R definition c : T ′ := t′\n( R −inductive)\n` Π −−−→ xl : Ll.Ai F V ′i ` V ′i .whd Π −−−→ xl : L ′ l.A ′ i ` A′i .whd Π −−−−−→ yri : R ′ ri .si\n} i ∈ {1 . . . n}\n−−−→ xl : L ′ l; −−−−→ In : V ′ n ` Ti,k\nF T ′i,k : si,k−−−→\nxl : L ′ l; −−−−→ In : V ′ n ` T ′i,k .whd Π −−−−−−−→ zpi,k : Vpi,k .Vi,k Σ Σ ∪ { −−−→ xl : L ′ l; −−−−−−−→ yj−1 : R\n′ j−1 `?j : R′j} j ∈ {1 . . . ri}−−−→\nxl : L ′ l; −−−−→ In : A ′ n; −−−−−−−→ zpi,k : Vpi,k ` Vi,k ?≡ Ii −→xl −→ ?ri U=\n i ∈ {1 . . . n} k ∈ {1 . . .mn}\n` Π−−−→xl : Ll. inductive I1 : A1 := k1,1 : T1,2 | . . . | k1,m1 : T1,m1with . . . with In : An := kn,1 : Tn,1 | . . . | kn,mn : Tn,mn  R Π−−−→xl : L′l. inductive I1 : A′1 := k1,1 : T ′1,2 | . . . | k1,m1 : T ′1,m1with . . .\nwith In : A ′ n := kn,1 : T ′ n,1 | . . . | kn,mn : T ′n,mn  The loop from 1 to n, ranges over all mutually inductive types of the block using the index i (for inductive). The other loop, from 1 to mn, ranges over the mn constructors of the n-th inductive type using index k (for constructor). Ti,k is the type of the k-th constructor of\nthe i-th inductive. Every inductive type in the block has the same number of homogeneous parameters l of type Lα for some α in 1 . . . l, and ri extra arguments of type Rβ for some β in 1 . . . ri. As explained in Section 2, homogeneous arguments are not abstracted explicitly in the types of the constructors, thus their context includes not only the inductive types−→ In but also the homogeneous arguments\n−→xl . Note that in this rule we used In to mean the n-th inductive, in contrast with the rest of the paper where the index n means the number of homogeneous arguments, l here. The complete arity of the inductive types −→ V ′ is a closed term. In fact that type is generated in an empty context in the first premise. This makes the context in which Ti,k is processed valid: there is no variable capture when\n−→xl is put before −→ In. Moreover the successful refinement of Π −−−→ xl : Ll.Ai in the empty context grants that the types of the homogeneous arguments do not depend on the inductive types −→ In. The last three premises just check that the type of each constructor is actually a product targeting the inductive type.\nWhilst being already quite involved, this rule is only partial. It lacks the checks for positivity conditions, that are only implemented by the kernel. Since the kernel of Matita is able to deal with metavariables we can test for these conditions using directly the kernel after the refinement process. Nevertheless, when the inductive type fed to the kernel is partial, the checks cannot be precise: all non positive occurrences will be detected, but nothing will prevent the user from instantiating a missing part with a term containing a non positive occurrence. One could label metavariables in a such a way that the unification algorithm refuses to instantiate them with a term containing non positive occurrences of the inductive type, but our current implementation does not. Anyway, once the definition is completed by the user, another call to the kernel is made, and all non positive occurrences are detected.\nMoreover, the kernel also checks that the sort si,k of the type of every constructor Ti,k is properly contained in the sort of the corresponding inductive si. Finally, one should also check that any occurrence of Ii in the types Ti,k of the constructors is applied to\n−→xl . This test is also omitted since it is performed by the kernel during the test for positivity.\n( R −letrec)\n` Π −−−−−→ xipi : T i pi .T i pi+1 F T ′i : si ` T ′i .whd Π −−−−−−→ xipi : T ′i pi .T\n′i pi+1−−−−→\nfn : Tn; −−−−−−→ xipi : T ′i pi ` ti : T ′i pi+1 R⇓ t′i  i ∈ {1 . . . n} ` ( let rec f1( −−−−−→ x1p1 : T 1 p1) : T 1 p1+1 := t1 and . . .\nand fn( −−−−−→ xnpn :T n pn) : T n pn+1 := tn ) R (\nlet rec f1( −−−−−→ x1p1 :T ′1 p1) : T ′1 p1+1 := t ′ 1 and . . .\nand fn( −−−−−−→ xnpn :T ′n pn) : T ′n pn+1 := t ′ n\n)\nAs for inductive types, this rule is only partial: it lacks the checks for guardedness conditions (termination or productivity tests), that are delegated to the kernel. We omit the rule for co-recursive functions, since it is identical to the one presented above."
    }, {
      "heading" : "4. Bi-directional refinement",
      "text" : "To obtain a bi-directional implementation of the refiner, we add new rules to the R⇓ algorithm. These ad-hoc rules for particular cases must take precedence over the generic ( R⇓ −default) rule. The ad-hoc rules are responsible for propagating information from the expected type towards the leaves of the term. The new rule for lambda-abstraction is well known in the literature [24] and it is also the only one implemented in Coq. The rule for let-in statements is given to allow the system infer more concise types. The one for application of constructors is completely novel and it takes advantage of additional knowledge on the constant parameters of an inductive type. It is thus peculiar of the Calculus of (Co)Inductive Constructions. This is also the rule that, according to our experience, mostly affects the behavior of the refiner. It makes it possible to refine many more terms to be refined in frequently occurring situations where, using a mono directional algorithm, more typing information had to be given by hand.\nTheorem 4.1 (Correctness). The new rules given in this section do not alter the correctness of the R⇓ algorithm w.r.t. its specification for all admissible 4 relations that include the identity for terms in the internal syntax. In particular, the algorithm is correct when the identity relation is picked for 4.\n( R⇓ −lambda)\nΓ ` E .whd Πx : E1.E2 Γ ` T F T ′ : s Γ ` T ′ ?≡ E1 U= Γ;x : T ′ ` t : E2 R⇓ t′\nΓ ` λx : T.t : E R ⇓ λx : T ′.t′\nNote that to type t we push into the context the declared type for x and not its expected type E1. This is to avoid displaying a confusing error message in case t is ill-typed, since the user declared x to have type T , and not E1 (that in principle can be arbitrarily different from T ).\n( R⇓ −letin)\nΓ ` T F T ′ : s Γ ` t : T ′ R ⇓ t′ Γ;x := t′ : T ′ ` u : E[t′/x] R ⇓ u′\nΓ ` let (x : T ) := t in u : E R ⇓ let (x : T ′) := t′ in u′\nWhere we denote by [t′/x] the operation of substituting all occurrences of t′ with x. Note that this operation behaves as an identity up to conversion (since x holds the value t′). Nevertheless, it enables the bi-directional type inference algorithm to propagate smaller types towards the leaves and, according to our observation, it leads to more readable inferred typed for sub-terms of u′.\nTheorem 4.2 (Termination). The R⇑ algorithm defined by the set of rules presented above with the addition of ( R⇓ −letin) and (R ⇓ −lambda) terminates.\nProof. The two functions terminate because all recursive calls are on smaller terms and because .whd, F and ?≡ terminate.\nThe next rule deals with applications of constructors to arguments and it is only triggered when the expected type is an inductive type. In that case the application must be total. In CIC, the types of constructors of inductive types are constrained to have a particular shape. Up to reduction, their type must be of the form Πx1 : F1 . . .Πxn : Fn.I x1 . . . xl tl+1 . . . tm where l is the number of uniform parameters of the inductive type. Therefore the application of a constructor to a list u1 . . . un of arguments has type I u1 . . . ul vl+1 . . . vm for some vs. Reversing the reasoning, once we know that the expected type for the application of a constructor is I u1 . . . ul vl+1 . . . vm we already know that the first l arguments of the application must be equal to u1 . . . ul up to conversion. It is thus possible to propagate them following the bi-directional spirit. This is achieved by the following ( R⇓ −appl−k) that calls a new function denoted by Et that consumes the first l arguments unifying them with the expected values. The remaining arguments are consumed as in the generic case of applications.\n( R⇓ −appl−k)\nΓ ` E .whd Il −→vl −→wn Γ ` −→tm ?≡ −→vl Et −→ t′l |N −→uo (k : T ) ∈ Env Γ ` T .whd −−−−−→ Πxl : Sl.T ′ Γ ` k −→ t′l : T ′[ −−→ xl/t ′ l] |N −→uo ET r : R Γ ` R ?≡ Il −→vl −→wn U=\nΓ ` k −→tm : E R⇓ r\nNote that if E does not reduce to an applied inductive type, the implemented algorithm\nfalls back to the standard rule for application. The rule presented only propagates information related to uniform parameters. Uniform parameters must be used consistently in every occurrence of the inductive type in the type of its constructors and not only in the occurrence at the end of the product spine (i.e. in the return type of the constructors). The variant of CIC implemented in Coq also considers non uniform parameters. Non uniform parameters must be used consistently only in the return type of the constructors and not in the premises. We do not consider non uniform parameters in this paper, but we remark that the ( R⇓ −appl−k) rule is also valid when the first l parameters are non uniform.\nSpecification 4.3 (Eat arguments (Et)). The Et algorithm takes a list of arguments for an application and a list of terms, and it verifies that an initial prefix of the arguments is equal to the given terms, up to unification. It takes as input a proof problem Σ, a substitution Φ and a context Γ, all assumed to be well formed, the list of arguments and the list of terms. It fails or it returns the list of arguments split into the consumed ones and the ones yet to be considered. It is denoted by:\n(Σ, Φ) Γ ` t1 . . . tm ?≡ v1 . . . vn Et t′1 . . . t ′ n |N u1 . . . ul (Σ ′, Φ′)\nPrecondition: WF(Σ, Φ, Γ) ∧ Σ, Φ, Γ ` vi : Ti i ∈ {1 . . . n}\nPostcondition (parametric in 4):\nWF(Σ′, Φ′) ∧ Σ′,Φ′ ≤ Σ,Φ ∧ Σ′, Φ′, Γ ` t′i ↓ vi i ∈ {1 . . . n} ∧ t′1 . . . t ′ n u1 . . . ul 4 t1 . . . tm\n(Et−empty) Γ ` −→ul ?≡ Et |N −→ul\n(Et−base)\nΓ ` t1 R⇑ t′1 : T1 Γ ` t′1 ?≡ v1 U= Γ ` −→tm ?≡ −→vn Et −→ t′n |N −→ul\nΓ ` t1 −→ tm ?≡ v1 −→vn Et t′1 −→ t′n |N −→ul\nTheorem 4.4 (Termination). The R⇑ algorithm defined by the set of rules presented above with the addition of ( R⇓ −appl− k), (Et−empty) and (Et−base) terminates.\nProof. The rule ( R⇓ −appl−k) terminates because ?≡ and .whd terminate and ET is called on smaller terms. Moreover the term T ′[ −−→ xl/t\n′ l] is a well typed type because Et grants that−→ t′l are convertible with −→vl and thus have the same types. Also notes that all the calls to R⇑ made by Et are on sub-terms of the input of ( R⇓ −appl− k).\nWe thus show that Et terminates by induction on the second list of arguments (the one between\n?≡ and Et ). Rule (Et−empty) is the base case and clearly terminates. Rule (Et−base) terminates because R⇑ and\n?≡ terminate and because the recursive call terminates by induction hypothesis.\n4.1. Remarks. We present here a simple but frequently occurring case that explains why the bi-directional rule for application of constructors enables to refine many more terms w.r.t. the mono-directional algorithm. A more complicated example was already discussed in the introduction and deals with dependent data types to represent the syntax of languages with binders.\nConsider the inductive type used to define the existential quantification.\nΠT : Type.ΠP : T → Prop. inductive Ex : Prop := Ex intro : Πx : T.P x→ Ex T P\nNote that T and P are homogeneous arguments.\nExample 4.5 (Use of ( R⇓ −appl−k)). Consider the conjecture ∃x : N.x > 0, encoded in CIC as Ex N (λx : N.x > 0)\nGiven a context Γ containing the assumption p stating that 2 > 0, one may want to use the following proof term to prove the conjecture\nt = Ex intro ?T ?P ?x p\nA mono directional refiner encounters a hard unification problem involving the type of p, 2 > 0, and its expected type.\n{` ?T : Type; ` ?P : ?T → Prop; ` ?x : ?T }, ∅,Γ ` 2 > 0 ?≡ ?P ?x\nClearly, the desired solution is to instantiate ?x with 2 and ?P with (λx : N.x > 0) obtaining a proof term of type ∃x : N.x > 0. Unfortunately, this is not the only possible solution. An undesired solution, but as reasonable as the correct one, is\nΦ = {?T := N; ?x := 0; ?P := λx : N.2 > x} under which the resulting proof term Φ(t) has type ∃x : N.2 > x, that is not the expected one. Why one should prefer the former to the latter is also unclear from a computer perspective. Thanks to the polymorphism of CIC, another undesired and less expected solution is also possible: Φ′ = {?T := Prop; ?x := 2 > 0; ?P := λx.x} The proof term Φ′(t) would then be of type ∃x : Prop.x, again different from the desired one.\nUsing the expected type, ?T and ?P are easily inferred looking at the homogeneous argument of the expected type. Φ′′ = {?T := N; ?P := λx : N.x > 0} Then inferring ?x is easy. Applying Φ\n′′ to the right hand side of the unification problem we obtain:\n2 > 0 ?≡ (λx : N.x > 0) ?x\nThen it is sufficient to reduce the right hand side and then perform a simple, first order, unification algorithm to obtain the desired instantiation for ?x.\nThe rule ( R⇓ −appl−k) is only fired when the term to be refined is syntactically the application of a constructor. Because of conversion, the term under analysis could be reducible to an application of a constructor. However, we cannot reduce the term first to try to match the rule. The first motivation is that terms in the external syntax may contain placeholders (see Section 5) and may not be well typed. Duplication of placeholders and substitution into them is not admitted. Moreover, reducing an ill typed term may lead to divergence. Secondly, reduction of proof terms correspond to cut elimination that is known to yield proofs terms of arbitrary size."
    }, {
      "heading" : "5. Extension to placeholders",
      "text" : "We consider here the first extension of our external syntax, obtained introducing linear placeholders for missing terms and for vectors of missing terms of unknown length. The latter are only accepted in argument position, even if we will enforce this only in the refinement algorithm and not in the syntax. The new syntax is obtained extending the one given in Table 1 with the new productions of Table 2. Placeholders are also called implicit arguments in the literature, but that terminology is ambiguous since it is also used for arguments that can be entirely omitted in the concrete syntax.\nIn a concrete implementation, user defined notations are used to further enlarge the external syntax. User defined notations behave as macros; macro expansion gives back a term in the external syntax we consider here. In particular, thanks to user defined notations,\nit is possible to entirely omit the typing information in binders, like in calculi typed à la Curry. Omitted types are turned into placeholders during the macro expansion phase. Implicit arguments can also be simulated by defining notations that insert into applications a fixed number of placeholders or vectors of placeholders in appropriate positions.\nA placeholder ? differs from a metavariable ?i in the fact that it has no sequent associated to it. The intended associated sequent allows in ? occurrences of all variables in the context of ?. Moreover, the type of ? is meant to be the one determined by the context. This information is made explicit by the refinement algorithm that turns each placeholder into a corresponding metavariable.\nPlaceholders occur only linearly in the term (i.e. every occurrence of a placeholder is free to be instantiated with a different term). Non linear placeholders are not allowed since two occurrences could be in contexts that bind different set of variables and instantiation with terms that live in one context would make no sense in the other one.\nSimilarly, substitution is not allowed on placeholders since a placeholder occurrence does not have a corresponding explicit substitution.\nFor both previous reasons, reduction is not allowed on terms in the external syntax that contain placeholders: the reduction, conversion and unification judgements only make sense on refined terms.\nIntuitively, a vector of placeholders can be instantiated by the refiner with zero or more metavariables. In our algorithm we adopt a lazy semantics: vectors of placeholders can only be used in argument position and a vector is expanded to the minimal number of metavariables that make the application well typed. Bi-directionality, i.e. the knowledge about the expected type for the application, is required for the lazy semantics. Indeed, without the expected type, an expansion could produce a locally well typed application whose inferred type will not match later on with the expected one.\nWe extend the R⇑ , R⇓ , ET and Et algorithms with new rules for single placeholders and for vectors of placeholders.\nTheorem 5.1 (Correctness). The C , F , R⇑ , R⇓ , ET , and Et algorithms extended with the set of rules presented in this section obey their specification for all admissible 4 that include the 4′ relation defined as follows: t′ 4′ t when t′ is obtained from t by replacing single placeholders with terms and vectors of placeholders with vectors of terms. In particular, the algorithms are correct w.r.t. 4′.\nProof. Every admissible 4 that includes 4′ also includes the identity. Thus we do not need to re-establish the result on the rules given in the previous sections. Correctness of the new rules given in this section is established by rule inspection.\n( R⇑ −placeholder)\nΣ Σ ∪ {Γ `?l : Type> , Γ `?k : ?l , Γ `?j : ?k}\nΓ `? R ⇑ ?j : ?k\n( R⇓ −placeholder)\nΣ Σ ∪ {Γ `?k : T}\nΓ `? : T R ⇓ ?k\n(ET− _ placeholder −0) Γ ` t\n−−−−−−−−−−→ (xr := vr : Tr) : T |N −→un ET v : V\nΓ ` t −−−−−−−−−−→ (xr := vr : Tr) : T |N _ ? −→un ET v : V\n(ET− _ placeholder +1)\nΓ ` T .whd Πx1 : U1.T1 Γ ` t −−−−−−−−−−→ (xr := vr : Tr) : T |N ? _ ? −→un ET v : V\nΓ ` t −−−−−−−−−−→ (xr := vr : Tr) : T |N _ ? −→un ET v : V\nThe rule (ET− _ placeholder −0) is meant to take precedence over (ET− _ placeholder +1).\nThe second is applied when the first one fails (local backtracking).\nTheorem 5.2 (Termination). The R⇑ algorithm defined by the set of rules presented above with the addition of ( R⇑ −placeholder), (R ⇓ −placeholder), (ET− _ placeholder −0) and (ET− _ placeholder +1) terminates.\nProof. Rules ( R⇑ −placeholder) and (R ⇓ −placeholder) terminate. The proof that ET terminates is, as before, by induction on the list of arguments that follow |N. The rule (ET− _ placeholder −0) terminates by induction hypothesis. The rule (ET− _\nplaceholder +1) deserves an accurate treatment. The check over T , asking it to be a product, is to avoid divergence. Since the input T is a well typed type, also T1 is, and thus it admits a normal form T ′1 in which x may occur. The recursive call does necessarily trigger the rule (ET−product) that will substitute a metavariable ?j for x in T1. Thanks to the reduction rules of CIC, reported in the appendix, substituting a variable for a metavariable declared in Σ (and not in Φ) does not change the normal form, meaning that T ′1[x/?j] .whd T ′ 1[x/?j]. Thus the rule (ET− _\nplaceholder +1) can be applied only a finite number of times, and the number of products in T is an upper bound.\nThe next two rules for the Et judgment follow the same schema of the ones for ET .\n(Et− _ placeholder −0) Γ ` −→tm ?≡ −→vn Et −→ t′n |N −→ul\nΓ ` _ ? −→ tm ?≡ −→vn Et −→ t′n |N −→ul\n(Et− _ placeholder +1) Γ `?\n_ ? −→ tm ?≡ −→vn Et −→ t′n |N −→ul\nΓ ` _ ? −→ tm ?≡ −→vn Et −→ t′n |N −→ul\nTheorem 5.3 (Termination). The R⇑ algorithm defined by the set of rules presented above with the addition of (Et− _ placeholder −0) and (Et− _ placeholder +1) terminates.\nProof. The rule (Et− _\nplaceholder −0) makes a recursive call on the same list of arguments −→vn but consumes a _ ? , and no other rule of the refiner adds one, so it can be repeated only a finite number of times. The recursive call in the rule (Et− _\nplaceholder +1) can trigger only rules (Et−empty) and (Et−base). The former terminates immediately, the latter will do a recursive call consuming one argument in −→vn, and thus terminates.\nNote that inlining the latter would lead to a rule whose termination is trivial to see, but we preferred to present the algorithm in a more modular way.\nExample 5.4 (Vector of placeholders). Assume a theorem τ ∈ Env shows that ∀x : N.P x→ Q x. The proof context may contain a natural number y and optionally a proof H that y validates P . Different proofs or proof styles may use the same theorem τ in different ways. For example, one may want to perform forward reasoning, and tell the system to assume (Q y) providing the following proof for it\ny : N;H : P y ` τ H Nevertheless, sometimes H is not known, and the user may want to tell the system he has intention to use the theorem τ on y, and prove (P y) later.\ny : N ` τ y While the latter application is well typed, the first is not, since the first argument of τ must be of type N. Nevertheless, the type of H depends on y, thus the term (τ ? H) would refine to the well typed term (τ y H) of type (Q y).\nThe vector of placeholders enables the system to accept both terms originally written by\nthe user. In the first case (τ _ ? H) would expand to (τ ? H) thanks to (Et− _ placeholder +1), and refine to (τ y H). In the second case (τ _ ? y) would refine to (τ y) thanks to (Et− _ placeholder −0). This suggests defining (τ _ ? ) as a notation for the theorem τ , obtaining a cheap implementation of what other systems call prenex implicit arguments: the first n arguments of an application whose head has a dependent type like Π −−−−→ xn : Tn.Π −−−−−−−→ ym : P (\n−→xn).T can be omitted, and are inferred thanks to the dependencies in the types of the m following arguments. As a bonus, in case the user wants to pass one of the implicit arguments there is no need to temporarily disable the mechanism, since the expansion of _ ? is computed on the fly and automatically adapts to its context."
    }, {
      "heading" : "6. Coercions",
      "text" : "Coercions are explicit type casts. While the literature [20] considers them mostly as a device to mimic sub-typing in a calculus lacking it, they have other interesting applications. The refiner of Matita inserts coercions in three locations: • around the argument of an application • around the head of an application • around the type of an abstraction The first case is the most common one, and is the one that can easily be explained in terms of sub-typing. For example, if one applies an operation defined over integers Z to an argument lying in the type of natural numbers N, the system injects the argument into the\nright type by means of the obvious, user declared, cast function mapping naturals into the non negative fragment of Z.\nThe second case is handy in two situations. First when the head of the application is implicit in the standard notation, like in 3x where the intended head constant is the multiplication but in the input it happens to be 3. The second is when the head constant has a non ambiguous interpretation as a function, but is not. For example a set may act as its characteristic function.\nThe last case is recurrent when algebraic structures are encoded as dependently typed records [25] embedding the type (or carrier) for the elements together with the operations and properties defining the structure. In that case, one may want to state a theorem quantifying over a structure G, say a group, and some elements in that group. However the statement ∀G : Group.∀x, y : G.P (x, y) is ill-typed since G is a term (of type Group) but is used as a type for x and y. The intended meaning is clear: x and y lie in the carrier type of G. The system can thus insert around G the projection for the carrier component of the Group record.\nDefinition 6.1 (Coercion set (∆)). A coercion set ∆ is a set of pairs (c, k) where c is a constant in Env and k is a natural number smaller than the arity of c (i.e. k points to a possible argument of c)\nIn the literature the coercion set is usually represented as a graph. Given a coercion (c, k) such that (c : Πx1 : T1 . . .Πxk : Tk . . .Πxn : Tn.T ) ∈ Env, Tk and T are nodes in the graph, and c is an edge from Tk to T . Most coercion implementation, like the one of Coq, Lego and Plastic, assume ∆ to be a graph validating a property called coherence. This property states that ∆ is an acyclic graph with at most one path linking every pair of nodes. This property enables to employ a straightforward algorithm to look for a sequence of coercions linking two non adjacent nodes in the graph.\nIn Matita, for various reasons detailed in [34], ∆ is not a graph, but a set of arcs for the transitive closure of the graph. Every time a coercion c is declared by the user, and thus added to ∆, the following set of automatically generated composite coercions is also added to ∆. {ci ◦ c ◦ cj |ci ∈ ∆ ∧ cj ∈ ∆} ∪ {c ◦ cj |cj ∈ ∆} ∪ {ci ◦ c|ci ∈ ∆} Of course the ◦ operator here is partial, and only well typed composite coercions are actually considered. This design choice enables the coercion lookup operation to be single step, since the set is already transitively closed. Moreover, since composite coercions are defined constants in Env, the term resulting after a cast is smaller if compared with the one obtained inserting the corresponding chain of user declared coercions. Last, allowing k to differ from n is a peculiarity of Matita. When k 6= n the application of the coercion creates new uninstantiated metavariables that correspond to proof obligations. This will be detailed later on.\nThe last detail worth mentioning is that, all systems known to the authors with the notable exception of Plastic [9], adopt some approximated representation for the nodes in the coercion graph, usually the name of the head constant of the source and target types. This results in a faster lookup in the coercion graph, but the coherence check is also strengthened. In particular, in a calculus with dependent types, different, but similar, coercions may not be allowed to be declared. Matita drops the coherence check, or better changes it into a warning, and enables the user to attach to coercions a priority: coercions from and to the same approximation of types are all tried according to user defined priorities.\nSpecification 6.2 (Coercion lookup ( − ∆)). Given a context Γ, substitution Φ and proof problem Σ, all assumed to be well formed, two types T1 and T2, this function returns an explicit cast c ?1 . . . ?k . . . ?n for the metavariable of index k and its type T\n′. It is denoted by:\n(Σ, Φ) Γ ` T1 T2 ∆ k, c ?1 . . . ?k . . . ?n : T ′ (Σ′,Φ)\nPrecondition (parametric in ≈): WF(Σ, Φ, Γ) ∧ (c, k) ∈ ∆ ∧ (c : Πx1 : T1 . . .Πxk : Tk . . .Πxn : Tn.T ) ∈ Env ∧\nTk ≈ T1 ∧ T ≈ T2 Postcondition:\nWF(Σ′) ∧ Σ′, Φ, Γ ` c ?1 . . . ?k . . . ?n : T ′\nWe denoted by ≈ the approximated comparison test used to select from ∆ a coercion c from T1 to T2. A proper definition of ≈ is not relevant for the present paper, but we can anyway say that Matita compares the first order skeleton of types obtained by dropping bound variables, metavariables and higher order terms, and that this skeleton can be made less precise on user request. We will give an account of this facility in the example that will follow.\nThe new metavariables ?1, . . . , ?n generated by the lookup operation are all declared in the new proof problem Σ′. The number of metavariables to which c is applied to is defined when the coercion is declared and may be less than the arity of c. In the latter case T is a product and the coercion casts its k-th argument to be a function. The position k of the casted argument is user defined as well. The coerced term has then to be later unified with ?k.\nTheorem 6.3 (Correctness). The C , F , R⇑ , R⇓ , ET , and Et algorithms extended with the set of rules presented in this section obey their specification where 4′′ is the following admissible order relation: t′ 4′′ t when t′ is obtained from t by replacing single placeholders with terms, vectors of placeholders with vectors of terms, and terms uk with terms convertible to (c u1 . . . uk . . . un) where c is a coercion declared in ∆ for its k th argument.\nProof. We do not need to re-establish correctness for the rules given in the previous sections since 4′′ is admissible and includes 4′. Correctness of the new rules given in this section is established by rule inspection as usual.\nIn the following rule the coercion c is applied to its argument t unifying it with ?k. The returned term t′ can still contains metavariables: ?1 . . .?k−1 may appear in the type of ?k, thus unifying ?k with t may instantiate them\n6, but ?k+1 . . .?n do not appear in the type of ?1 . . .?k, and thus cannot be all instantiated. This rule is applied as a fall back in case C −ok fails.\n6In the case of dependent types the unification of the types is a necessary condition for the unification of the two terms, as claimed by Strecker [32].\n( C −coercion)\nΓ ` T1 T2 ∆ k, c −→ ?m ?k −→ ?n : T ′ 2 Γ `?k ?≡ t U= Γ ` T ′2 ?≡ T2 U\nΓ ` t : T1 ?≡ T2 C c −→ ?m ?k −→ ?n\nCorrectness of ( C −coercion). Since ?k is unified with t in the second premise of the rule, by definition of unification we have Σ, Φ, Γ `?k ↓ t, and thus c −→ ?m ?k −→ ?n 4′′ t. Moreover, the postconditions of coercion lookup ∆ grant that c −→ ?m ?k −→ ?n has type T ′ 2 that is later unified with T2. Thus the postconditions of the unification algorithm allow us to prove that c −→ ?m ?k −→ ?n has a type convertible with T2.\nThe C −coercion rule automatically takes care of the insertion of coercions around\narguments of an application and around the types of an abstraction.\nThe following extension to ET take cares of insertion around the head of an application.\n(ET−coercion)\nΣ Σ ∪ {Γ `?1′ : Type> , Γ `?1 : ?1′} Σ Σ ∪ {Γ;x1 :?1 `?2′ : Type> , Γ;x1 :?1 `?2 : ?2′} Γ ` t −→vr : T ?≡ Πx :?1.?2 C c −→ws Γ ` wi : Wi i ∈ {1 . . . s} Γ ` c −−−−−−−−−−−→ (xs := ws : Ws) : Πx :?1.?2 |N u1 −→un ET v : V\nΓ ` t −−−−−−−−−−→ (xr := vr : Tr) : T |N u1 −→un ET v : V\nCorrectness of ET−coercion. Its correctness follows trivially from the correctness of ( C −coercion).\nTheorem 6.4 (Termination). The R⇑ algorithm defined by the set of rules presented above with the addition of ( C −coercion) and (ET−coercion) terminates.\nProof. Rule ( C −coercion) clearly terminates. The rule (ET−coercion) issues a recursive call to ET without consuming u1, but the only rule that can be triggered is (ET−product), that will immediately consume u1.\nInlining (ET−product) would result in a rule that consumes some input and thus clearly terminates, but would be way less readable.\n6.1. Implementation remarks. Since we allow coercion arguments not to be inferred automatically (like proof obligations) their type may depend on the coerced term (e.g. the proof that the coerced integer is greater than zero has an instance of the coerced integer in its type, and the corresponding metavariable will have index greater than k).\nExample 6.5 (Coercion with side conditions). Consider the following coercion set, declaring the coercion v to nel from vectors to non empty lists.\n∆ = {(v to nel, 3)} The environment holds the following type for the coercion:\n(v to nel : ΠA : Type.Πn : N.Πv : Vect A n.n > 0→ ∃l : List A, length l > 0) ∈ Env Now consider the term t = (Vcons N 0 (Vnil N) 2) and the following coercion problem:\nΓ ` Vect N (0 + 1) (∃l : List N, length l > 0) ∆ 3, v to nel ?1 ?2 ?3 ?4 : (∃l : List ?1, length l > 0) Γ `?3 ?≡ t U= Γ ` (∃l : List ?1, length l > 0) ?≡ (∃l : List N, length l > 0) U\nΓ ` t : Vect N (0 + 1) ?≡ (∃l : List N, length l > 0) C v to nel ?1 ?2 ?3 ?4 where the final proof problem and substitutions are:\nΣ = {Γ `?4 : ?2 > 0} Φ = {Γ `?1 := N : Type, Γ `?2 := 0 + 1 : N, Γ `?3 := t : Vect N (0 + 1)}\nNote that ?4 is still in Σ, thus it represent a proof obligation the user will be asked to solve. Also note that the following coercion could be declared as well, with a higher precedence. It is useful since it does not open a side condition when the type of the coerced vector is explicit enough to make the proof that it is not empty constant (not depending on Γ nor on the vector but just on its type) and thus embeddable in the body of the coercion.\n(nev to nel, ΠA : Type.Πn : N.Πv : Vect A (n+ 1).∃l : List A, length l > 0) ∈ Env The system would thus try nev to nel first, and fall back to v to nel whenever needed.\nAlso note that this last coercion can be indexed as a cast from (Vect ( + 1)) to (∃l : List , length l > 0) or in a less precise way. For example the approximation of the source type could be relaxed to (Vect ). This will force the system to try to apply this coercion even if the casted term is a vector whose length is not explicitly mentioning +1, but is something that unifies with ?j + 1. For example the length 1 ∗ 2 would unify, since its normal form is (0 + 1) + 1."
    }, {
      "heading" : "7. Comparison with related work on Type Inference",
      "text" : "Type inference is a very widely studied field of computer science. Nevertheless to the authors’ knowledge there is no precise account of a type inference algorithm for the full CIC calculus in the literature.\nThe extension to the typing algorithm of CIC with explicit casts in [28] follows the same spirit of our refinement algorithm for raw terms. However the work by Saibi does not handle placeholders nor metavariables, and the presentation is in fact quite distant from the actual implementation in the Coq interactive prover.\nAnother work in topic is [22] where Norell describes the bi-directional type inference algorithm implemented in the Agda interactive prover. He presents the rules for a core dependently typed calculus enriched with dependent pairs. Unfortunately he omits the rules for its extension with inductive types. It is thus hard to tell if Agda exploits the type expected by the context to type check inductive constructors as in rule ( R⇓ −appl−k). Agda does not provide an explicit _ ? placeholder but uses the expected type to know when it is necessary to pad an application with meta variables in order to reduce the arity of its type. In our setting this is equivalent to the following transformation: every application (f −→a ) is turned into (f −→a _ ? ) whenever its expected type is known (i.e. not a metavariable).\nOne aspect that allows for a direct comparison with Coq and Agda is the handling of implicit arguments. In both Agda and Coq, abstractions corresponding to arguments the user can freely omit are statically labelled as such. The systems automatically generate fresh metavariables as arguments to these binders and the type inference algorithm eventually instantiates them. Both systems give the user the possibility to locally override the implicit arguments mechanism. In Coq the user can prefix the name of a constant with the @ symbol, while in Agda the user can mark actual arguments as implicit enclosing them in curly braces. This escaping mechanism is required because many lemmas admit multiple and incompatible lists of implicit arguments. As an example, consider a transitivity lemma eqt : ∀x, y, z.x = y → y = z → x = z. When used in a forward proof step the user is likely to pass as arguments a proof p that a = b and a proof q that b = c like in (eqt p q) to put in his context the additional fact a = c. In that case values for x, y and z are determined by the types of p and q. On the contrary if the lemma is used in backward proof step to prove that a = c, no value for y can be inferred, thus the user is likely to use the lemma as in (eqt b) and expect the system to open two new goals: a = b and b = c. The two different uses of eqt make it impossible to statically attach to it a single list of implicit arguments and at the same time to never resort to an escape mechanism to temporarily forget that list. In Matita the user can simply use the _ ? placeholder, thus no escaping mechanism is required. In fact the type inference algorithm described in this paper lets the user write (eqt _ ? p q) in the first case as well as (eqt _ ? c) in the second one.7\nThe lack of a complete and formal study of type inference for raw CIC terms is probably due to the many peculiarities of the CIC type system, in particular inductive and dependent types, explicit polymorphism and the fact that type comparison is not structural, but up to computational equivalence. We thus try to position our work with respect to some of the main approaches adopted by type inference algorithms designed for programming languages.\n7.1. Greedy versus delayed constraint solving. The most notable example of type inference algorithm based on constraint solving is the one adopted for the Agda system [22]. Agda is based on a dependently typed programming language quite similar to CIC, but is designed for programming and not for writing proofs. The type inference algorithm collects constraints and checks for their satisfiability. Nevertheless, their solution is not recorded in the terms. This enables the user to remove an arbitrary part of an already type checked\n7A trailing _ ? is automatically added to any term used in backward proof step.\nterm and have the typing of its context not influenced by the term just removed. While this “compositionality” property is desirable for programming in a language with dependent types, it is not vital for proof systems, where one seldom edits by hand type checked terms.\nA strong characteristic of constraint based type inference is precise error reporting, as described in [33]. Even if it the heuristics adopted in Matita [27] to discard spurious error reports are slightly more complex than the ones proposed by Stuckey, we believe that they provide a similar precision.\nGreedy algorithms [14], like the one presented in this paper, are characterized by a very predictable behavior, at the cost of being forced to take early decisions leading to the rejection of some possibly well typed terms. Also remember that unification has to take computation into account, and user provided functions are known to be total only if they are well typed. Thus the resolution of type constraints cannot be delayed for long. According to our experience, predictability compensates for the extra type annotations the user is sometimes required to produce to drive the greedy algorithm towards a solution.\n7.2. Unification based versus local constraint solving. Many algorithms to infer a polymorphic type for a program prefer to avoid the use of unification [24] since unification variables may represent type constraints coming from distant, loosely related, sub-terms. Moreover a bi-directional approach pushes the type constraints of the context towards subterms, making it effectively possible to drop unification altogether. These approaches also scaled up to types with some sort of dependency over terms, as in [15].\nInteractive provers based on type theory are for (good and) historical reasons based on the two twin approaches. A small kernel based on decision procedures type checks (placeholder free) terms, and a refiner based on heuristics deals with terms with holes performing type inference. Since the kernel is the key component of the system, the one that must be trusted, the language is designed to allow the type checking algorithm to be as simple as possible. Explicit polymorphism makes type checking CIC terms decidable while allowing the same degree of polymorphism as Fω. These explicit type annotations are usually left implicit by the user and represent long-distance constraints. In this context unification seems to be a necessary device. Moreover, the most characterizing feature of CIC is that types are compared taking computation into account, and that types can contain terms, in particular functions applications. Thus the kernel is equipped with a quite elaborate machinery to compute recursive functions and unfold definitions. Type inference has to provide a similar machinery, and possibly extend it to handle types containing metavariables. This extension is commonly named higher order unification, and it is a really critical component of an interactive prover. Recent important developments [16] heavily rely on a user-extensible unification algorithm [5], using it as a predictable form of Prolog-like inference engine. In other words, unification can be employed to infer terms (content) while type inference is employed to infer types and type annotations in the case of explicit polymorphism. For these many reasons, we believe that developing type inference on top of unification is a sound decision probably necessary to scale to a rich type system like CIC."
    }, {
      "heading" : "8. Conclusion",
      "text" : "In this paper we studied the design of an effective refinement algorithm for the Calculus of (Co)Inductive Constructions. Its effectiveness has been validated in all the formalizations carried on using the Matita interactive theorem prover [11, 1, 2, 3], whose refiner is based\non the algorithm described in this paper. Once again we stress that the refiner component, while not being critical for the correctness of the prover, is the user’s main interlocutor, and is thus critical for the overall user’s experience.\nThis algorithm is also the result of the complete rewrite the Matita ITP underwent in the last couple of years. The refiner algorithm described in this paper amounts to approximatively 1600 lines of OCaml code, calling the higher order unification algorithm that amounts to a bit less than 1900 lines. To give a term of comparison to the reader, the kernel of Matita, written by the same authors, amounts to 1500 lines of data structures definitions and basic operations on them, 550 lines of conversion algorithm and 1400 of type checking. More than 300 lines of the type checking algorithm are reused by the refiner for checking inductive types positivity conditions and recursive or co-recursive functions termination or productivity.\nOn top of this refinement algorithm all primitive proof commands have been reimplemented. In the old implementation they were not taking full advantage of the refiner, partially for historical reasons, partially because it was lacking support for placeholder vectors and bi-directionality was not always exploited. The size of the code is now 48.9% of what it used to be in the former implementation. In particular, it became possible to implement many proof commands as simple “notations” for lambda-terms in external syntax.\nA particularity of this work is that the presented algorithm deals with completely raw terms, containing untyped placeholders, whose only precondition is to be syntactically well formed. In addition it also supports a very general form of coercive sub-typing, where inserting the explicit cast may leave uninstantiated metavariables to be later filled by the user. This eased the implementation of subset coercions in the style of [29], but that topic falls outside the scope of the present paper and is thus not discussed.\nThe algorithm could be enhanced adding more rules, capable of propagating more typing information. For instance, a specific type forcing rule for β-redexes (suggested by a referee) could be in the form\n( R⇓ −beta)\nΓ ` U F U ′ : s Γ ` u : U ′ R ⇓ u′ Γ;x : U ′ ` t : E[u′/x] R ⇓ t′\nΓ ` (λx : U.t) u : E R ⇓ (λx : U ′.t′) u′\nenabling the system to propagate the expected type to the abstraction (compare this rule with (R⇓−letin)). In practice the advantage of this rule is limited, since it is quite infrequent for a user to write β-redexes. A type forcing rule for pattern matching based on the same principles, propagating the expected type to the return type of the match, could be of greater value, since this construct is more likely to come from the user input. We will consider adding such rules in a future implementation.\nThe refinement algorithm we presented already validates many desired properties, like correctness and termination. Nevertheless we did not even state the relative completeness theorem. In a simpler framework, admitting most general unifiers, one could have stated that given an oracle for unification, the algorithm outputs a well typed refinement every time it is possible and that any other refinement is less general than the produced one. Unluckily CIC is higher order and does not admit most general unifiers. To state the relative completeness theorem one has to make the oracle aware of the whole refinement\nprocedure and the oracle has to guess a unifier (or all of them) such that the remaining refinement steps succeed. This makes the theorem way less interesting. Alternatively one would have to add backtracking to compensate for errors made by the oracle, and make the algorithm distant from the implemented one, that is essentially greedy and backtracking free.\nThe algorithm presented in the paper is clearly not relatively complete. For example, the rules given in Section 3 do not accept the term f c where c : N and f : match ?1 in N return λx.Type [O ⇒ N | S (x : N) ⇒ N → N]. The term is however refineable, for instance by instantiating ?1 with S O. To obtain a relatively complete algorithm, we could add additional rules based on the invocation of the unifier on difficult problems. For instance, for the example just shown it would be sufficient to unify N →?2 with match ?1 in N return λx.Type [O ⇒ N | S (x : N)⇒ N→ N] for a fresh metavariable ?2. However, we know in advance that the efficient but incomplete algorithm implemented in Matita always fails on such difficult unification problems. The same holds for the similar algorithm implemented in Coq. Therefore a relatively complete version of the algorithm would remain only of theoretical interest.\nThe following weaker theorem, which establishes completeness on well typed terms only, can be easily proved by recursion over the proof tree and by inspection of all cases under the hypothesis that every pair of convertible terms are unified by the identity metavariable instantiation.\nTheorem 8.1 (Completeness for well typed terms). For all well formed Σ, Φ, Γ and for all t and T such that Σ, Φ, Γ ` t : T we have Γ ` t : T R ⇓ t.\nAcknowledgments. We deeply thank Jacques Carette and the anonymous referees for their many observations and corrections."
    }, {
      "heading" : "Appendix A. Syntax-directed type-checking rules",
      "text" : "The following appendix is an extract of the paper [4] in which the reader can find all the details of the type checking algorithm implemented in the Matita interactive prover. A few aesthetic changes have been made to the adopted syntax to increase its consistency with respect to the syntax adopted in this paper. The main differences are summarised in the following list: • We use the membership relation over the PTS set to type sorts and products • The check for the consistency of the metavariable local substitution has been inlined in\nthe rule • A new generic judgement (r : T ) ∈ Env has been introduced to provide a more compact\nsyntax for the lookup of the type of a generic object into the environment • We inlined several auxiliary functions that were used in the presentation of the typecheck-\ning rule for case analysis. This was made possible by the following abuse of notation: Env,Σ,Φ, ∅ ` t1 .whd Π −−−→ xi : Ti.tn+1 is a shortcut to mean that for all i ∈ {1 . . . n} Σ, Φ, Γ;x1 : T1; . . . ;xi−1 : Ti−1 ` ti .whd Πxi : Ti.ti+1 and Σ, Φ, Γ ` tn+1 .whd tn+1. Moreover, the rule presented in [4] is more liberal than the one presented here that just uses the test (s, s′) ∈ elim(PTS) to check that a non informative data is never analyzed to obtain an informative one. The actual rules used in the kernel and the refiner of Matita also allow in every situation the elimination of inhabitants of singleton inductive types, whose definition is given in [4].\nIn this section, I will be short for\nΠ −−−→ xl : Ul.inductive\nI1l : A1 := k 1 1 : K 1 1 . . . k m1 1 : K m1 1\nwith . . . with Inl : An := k 1 n : K 1 n . . . k mn n : K mn n\nA.1. Environment formation rules. Environment formation rules (judgement Env ` WF , function typecheck obj)\n∅ `WF Env `WF d undefined in Env Env,Σ `WF Env,Σ,Φ `WF Env,Σ,Φ, ∅ ` T : S Env,Σ,Φ, ∅ ` S .whd S′ where S′ is a sort Env,Σ,Φ, ∅ ` b : T ′ Env,Σ,Φ, ∅ ` T ↓ T ′\nEnv ∪ (Σ,Φ,definition d : T := b) `WF Env `WF d undefined in Env Env,Σ `WF Env,Σ,Φ `WF Env,Σ,Φ, ∅ ` T : S Env,Σ,Φ, ∅ ` S .whd S′ where S′ is a sort\nEnv ∪ (Σ,Φ, axiom d : T ) `WF\nEnv `WF −→ fn undefined in Env Env,Σ `WF Env,Σ,Φ `WF Env,Σ,Φ, ∅ ` Ti : Si Env,Σ,Φ, ∅ ` Si .whd S′i where S′i is a sort Ti = Π −−−−−→ xipi : T i pi .T i pi+1\nEnv,Σ,Φ, [f1 : T1; . . . ; fn : Tn; −−−−−→ xipi : T i pi ] ` ti : T ′i pi+1 Env,Σ,Φ, [f1 : T1; . . . ; fn : Tn; −−−−−→ xipi : T i pi ] ` T i pi+1 ↓ T ′ipi+1\n} i ∈ {1 . . . n}\n−→ tn guarded by destructors ([4], Sect. 6.3)\nEnv ∪  Σ,Φ,let rec f1(−−−−−→x1p1 : T 1p1) : T 1p1+1 := t1 and . . . and fn( −−−−−→ xnpn :T n pn) : T n pn+1 := tn  `WF Env `WF −→ fn undefined in Env Env,Σ `WF Env,Σ,Φ `WF Env,Σ,Φ, ∅ ` Ti : Si Env,Σ,Φ, ∅ ` Si .whd S′i where S′i is a sort Ti = Π −−−−−→ xipi : T i pi .T i pi+1\nEnv,Σ,Φ, [f1 : T1; . . . ; fn : Tn; −−−−−→ xipi : T i pi ] ` ti : T ′i pi+1 Env,Σ,Φ, [f1 : T1; . . . ; fn : Tn; −−−−−→ xipi : T i pi ] ` T i pi+1 ↓ T ′ipi+1\n} i ∈ {1 . . . n}\n−→ tn guarded by constructors ([4], Sect. 6.3)\nEnv ∪  Σ,Φ,let corec f1(−−−−−→x1p1 : T 1p1) : T 1p1+1 := t1 and . . . and fn( −−−−−→ xnpn :T n pn) : T n pn+1 := tn  `WF Env `WF I1l , . . . , Inl , k11, . . . , kmnn undefined in Env Env,Σ `WF Env,Σ,Φ `WF all the conditions in [4], Sect. 6.1 are satisfied\nEnv ∪ (Φ,Σ, I) `WF\nA.2. Metasenv formation rules. Metasenv formation rules (judgement Env,Σ ` WF , function typecheck metasenv)\nEnv, ∅ `WF Env,Σ `WF ?i undefined in Σ Env,Σ, ∅,Γ `WF Env,Σ, ∅,Γ ` T : S Env,Σ, ∅,Γ ` S .whd S′where S′ is a sort\nEnv,Σ ∪ (Γ `?i : T ) `WF\nA.3. Subst formation rules. Subst formation rules (judgement Env,Σ,Φ `WF , function typecheck subst)\nEnv,Σ, ∅ `WF Env,Σ,Φ `WF ?i undefined in Σ and in Φ Env,Σ,Φ,Γ `WF Env,Σ,Φ,Γ ` T : S Env,Σ,Φ,Γ ` S .whd S′ where S′ is a sort Env,Σ,Φ,Γ ` t : T ′ Env,Σ,Φ,Γ ` T ↓ T ′\nEnv,Σ,Φ ∪ (Γ `?i : T := t) `WF\nA.4. Context formation rules. Context formation rules (judgement Env,Σ,Φ,Γ `WF , function typecheck context)\nEnv,Σ,Φ, ∅ `WF Env,Σ,Φ,Γ `WF x is undefined in Γ Env,Σ,Φ,Γ ` T : S Env,Σ,Φ,Γ ` S .whd S′ where S′ is a sort Env,Σ,Φ,Γ ∪ (x : T ) `WF Env,Σ,Φ,Γ `WF x is undefined in Γ Env,Σ,Φ,Γ ` T : S Env,Σ,Φ,Γ ` S .whd S′ where S′ is a sort Env,Σ,Φ,Γ ` t : T ′ Env,Σ,Φ,Γ ` T ↓ T ′\nEnv,Σ,Φ,Γ ∪ (x : T := t) `WF\nA.5. Term typechecking rules. Term typechecking rules (judgement Env,Σ,Φ,Γ ` t : T , function typeof)\n(K−variable) (x : T ) ∈ Γ or (x : T := t) ∈ Γ\nEnv,Σ,Φ,Γ ` x : T (K−sort) (s1, s2) ∈ PTS Env,Σ,Φ,Γ ` s1 : s2\n(K−meta)\n(x1 : T1; . . . ;xn : Tn `?i : T ) ∈ Σ or (x1 : T1; . . . ;xn : Tn `?i : T := t) ∈ Φ Env,Σ,Φ,Γ ` ti : Ti[ −−−−−−→ xi−1/ti−1] i ∈ {1 . . . n}\nEnv,Σ,Φ,Γ `?i[t1; . . . ; tn] : T [ −−−→ xn/tn]\n(K−constant) (r : T ) ∈ Env\nEnv,Σ,Φ,Γ ` r : T\n(K−definition)\n(Σ′,Φ′,definition d : T := b) ∈ Env or (Σ′,Φ′, axiom d : T ) ∈ Env Σ′ = ∅ Φ′ = ∅\n(d : T ) ∈ Env\n(K−letrec)\n Σ ′,Φ′, let rec f1( −−−−−→ x1p1 : T 1 p1) : T 1 p1+1 := t1 and . . .\nand fn( −−−−−→ xnpn :T n pn) : T n pn+1 := tn  ∈ Env Σ′ = ∅ Φ′ = ∅ 1 ≤ i ≤ n\n(fi : Π −−−−−→ xipi : T i pi .T i pi+1 ) ∈ Env\n(K−letcorec)\n Σ ′,Φ′, let corec f1( −−−−−→ x1p1 : T 1 p1) : T 1 p1+1 := t1 and . . .\nand fn( −−−−−→ xnpn :T n pn) : T n pn+1 := tn  ∈ Env Σ′ = ∅ Φ′ = ∅ 1 ≤ i ≤ n\n(fi : Ti) ∈ Env\n(K−inductive)\n(Σ′,Φ′, I) ∈ Env Σ′ = ∅ Φ′ = ∅ 1 ≤ p ≤ n\n(Ipl : Π −−−→ xl : Ul.Ap) ∈ Env\n(K−constructor)\n(Σ′,Φ′, I) ∈ Env Σ′ = ∅ Φ′ = ∅ 1 ≤ p ≤ n 1 ≤ j ≤ mp\n(kjp : Π −−−→ xl : Ul.K j p) ∈ Env\n(K−lambda)\nEnv,Σ,Φ,Γ ` T : S Env,Σ,Φ,Γ ` S .whd S′ S′ is a sort or a meta Env,Σ,Φ,Γ ∪ (n : T ) ` u : U\nEnv,Σ,Φ,Γ ` λn : T.u : Πn : T.U\n(K−product)\nEnv,Σ,Φ,Γ ` T : s1 Env,Σ,Φ,Γ ∪ (n : T ) ` U : s2 (s1, s2, s3) ∈ PTS Env,Σ,Φ,Γ ` Πn : T.U : s3\n(K−letin)\nEnv,Σ,Φ,Γ ` t : T ′ Env,Σ,Φ,Γ ` T : S Env,Σ,Φ,Γ ` T ↓ T ′ Env,Σ,Φ,Γ ∪ (x : T := t) ` u : U Env,Σ,Φ,Γ ` let (x : T ) := t in u : U [x/t]\n(K−appl− base)\nEnv,Σ,Φ,Γ ` h : Πx : T.U Env,Σ,Φ,Γ ` t : T ′ Env,Σ,Φ,Γ ` T ↓ T ′\nEnv,Σ,Φ,Γ ` h t : U [x/t]\n(K−appl− rec) Env,Σ,Φ,Γ ` (h t1) t2 · · · tn : T Env,Σ,Φ,Γ ` h t1 t2 · · · tn : T\n(K−match)\n(Σ′,Φ′, I) ∈ Env Σ′ = ∅ Φ′ = ∅ Env,Σ,Φ,Γ ` t : T Env,Σ,Φ,Γ ` T .whd Ipl −→ul −→ u′r Ap[ −−−→ xl/ul] = Π −−−−→ yr : Yr.s K j p[ −−−→ xl/ul] = Π −−−−−−→ xjnj : Q j nj .I p l −→xl −→vr j = 1 . . .mp Env,Σ,Φ,Γ ` U : V Env,Σ,Φ,Γ ` V .whd Π −−−−→ zr : Yr.Πzr+1 : I p l −→ul −→zr .s′ (s, s′) ∈ elim(PTS) Env,Σ,Φ,Γ ` λ −−−−−−→ xjnj : P j nj .tj : Tj j = 1, . . . ,mp Env,Σ,Φ,Γ ` Tj ↓ Π −−−−−−→ xjnj : Q j nj .U −→vr (kpj −→ul −→ xjnj ) j = 1, . . . ,mp\nEnv,Σ,Φ,Γ ` match t in Ipl return U [kp1 ( −−−−−−→ x1n1 : P 1 n1)⇒ t1 | . . . |k p mp ( −−−−−−−−→ x mp nmp : P mp nmp )⇒ tmp ] : U −→ u′r t\nA.6. Term conversion rules. Term conversion rules (judgement Env,Σ,Φ,Γ ` T ↓ T ′, function are convertible ; ↓= means test eq only = true; ↓• means that the current rule must be intended as two rules, one with all the ↓• replaced by ↓, the other with all the ↓• replaced by ↓=)\nEnv,Σ,Φ,Γ ` T =α T ′ Env,Σ,Φ,Γ ` T ↓= T ′ Env,Σ,Φ,Γ ` T ↓= T ′\nEnv,Σ,Φ,Γ ` T ↓ T ′\nTypeu ≤ Typev Typev ≤ Typeu are declared constraints ([4], Sect. 4.3) Env,Σ,Φ,Γ ` Typeu ↓= Typev\nTypeu ≤ Typev is a declared constraint ([4], Sect. 4.3) Env,Σ,Φ,Γ ` Typeu ↓ Typev\nEnv,Σ,Φ,Γ ` Prop ↓ Typeu lc = t1, . . . , tn lc ′ = t′1, . . . , t ′ n for all i = 1, . . . , n Env,Σ,Φ,Γ ` ti ↓• t′i Env,Σ,Φ,Γ `?j [lc] ↓•?j [lc′]\nEnv,Σ,Φ,Γ ` T1 ↓= T ′1 Env,Σ,Φ,Γ ∪ (x : T1) ` T2 ↓• T ′2 Env,Σ,Φ,Γ ` Πx : T1.T2 ↓• Πx : T ′1.T ′2\nEnv,Σ,Φ,Γ ∪ (x : T ) ` t ↓• t′\nEnv,Σ,Φ,Γ ` λx : T.t ↓• λx : T ′.t′ In the rule above, no check is performed on the source of the abstractions, since we assume we are comparing well-typed terms whose types are convertible.\nEnv,Σ,Φ,Γ ` h ↓• h′ for all i = 1, . . . , n Env,Σ,Φ,Γ ` ti ↓= t′i\nEnv,Σ,Φ,Γ ` h −→tn ↓• h′ −→ t′n\nEnv,Σ,Φ,Γ ` t ↓• t′ Env,Σ,Φ,Γ ` U ↓• U ′ for all i = 1, . . . ,mp Env,Σ,Φ,Γ ` λ −−−−−→ xini : P i ni .ti ↓• λ −−−−−−→ xini : P ′i ni .t ′ i\nEnv,Σ,Φ,Γ ` match t in Ipl return U [k p 1 ( −−−−−−→ x1n1 : P 1 n1)⇒ t1| . . . |k p mp ( −−−−−−−−→ x mp nmp : P mp nmp )⇒ tmp ] ↓•\nmatch t′ in Ipl return U [k p 1 ( −−−−−−→ x1n1 : P ′1 n1)⇒ t ′ 1| . . . |k p mp ( −−−−−−−−−→ x mp nmp : P ′mp nmp )⇒ t′mp ]\nEnv,Σ,Φ,Γ ` t .whd t′ Env,Σ,Φ,Γ ` u .whd u′ Env,Σ,Φ,Γ ` t′ ↓• u′\nEnv,Σ,Φ,Γ ` t ↓• u In the previous rule, t′ and u′ need not be weak head normal forms: any term obtained from t (respectively, u) by reduction (even non-head reduction) will do. Indeed, the less reduction is performed, the more efficient the conversion test usually is.\nA.7. Term reduction rules. Term reduction rules.\nEnv,Σ,Φ,Γ ` (λx : T.u) t Bβ u[x/t]\nEnv,Σ,Φ,Γ ` let (x : T ) := t in u Bζ u[x/t]\n(∅, ∅,definition d : T := b) ∈ Env Env,Σ,Φ,Γ ` d Bδ b (Γ′ `?i : T := t) ∈ Φ\nEnv,Σ,Φ,Γ `?i[u1 ; . . . ; un] Bδ t[dom(Γ′)/−→un]\nEnv,Σ,Φ,Γ ` match kpi −→ tl −→ t′ni in I p l return U\n[kp1 ( −−−−−−→ x1n1 : P 1 n1)⇒ u1| . . . |k p mp ( −−−−−−−−→ x mp nmp : P mp nmp )⇒ ump ] Bι ui[ −→ xini/ −→ t′ni ] ∅, ∅,let rec f1(−−−−−→x1p1 : T 1p1) : T 1p1+1 := t1 and . . .\nand fn( −−−−−→ xnpn :T n pn) : T n pn+1 := tn  ∈ Env k ∈ {1 . . . n}\nEnv,Σ,Φ,Γ ` fk u1 ...(kij −→vnj )... um Bµ tk[ −→ xkm/u1, . . . , (k i j −→vnj ), . . . , um]\nNotice that (kij −→vnj ) must occur in the position of the recursive argument of fk. This implies that, for this reduction to be performed, fk must be applied at least up to its recursive argument.\n ∅, ∅,let corec f1(−−−−−→x1p1 : T 1p1) : T 1p1+1 := t1 and . . . and fn( −−−−−→ xnpn :T n pn) : T n pn+1 := tn  ∈ Env k ∈ {1 . . . n}\nEnv,Σ,Φ,Γ ` match fk −→uq in Ipl return U [kp1 ( −−−−−→ y1n1 : P 1 n1)⇒ v1| . . . |k p mp ( −−−−−−−−→ y mp nmp : P mp nmp )⇒ vmp ] Bν\nmatch tk[ −→ xkq/ −→uq] in Ipl return U [kp1 ( −−−−−→ y1n1 : P 1 n1)⇒ v1| . . . |k p mp ( −−−−−−−−→ y mp nmp : P mp nmp )⇒ vmp ]\nNotice that here q can be zero."
    } ],
    "references" : [ {
      "title" : "A page in number theory",
      "author" : [ "Andrea Asperti", "Cristian Armentano" ],
      "venue" : "Journal of Formalized Reasoning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "About the formalization of some results by Chebyshev in number theory",
      "author" : [ "Andrea Asperti", "Wilmer Ricciotti" ],
      "venue" : "In Proc. of TYPES’08,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Formal metatheory of programming languages in the Matita interactive theorem prover",
      "author" : [ "Andrea Asperti", "Wilmer Ricciotti", "Claudio Sacerdoti Coen", "Enrico Tassi" ],
      "venue" : "Journal of Automated Reasoning: Special Issue on the Poplmark Challenge. Published online,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "A compact kernel for the Calculus of Inductive Constructions. Sadhana",
      "author" : [ "Andrea Asperti", "Wilmer Ricciotti", "Claudio Sacerdoti Coen", "Enrico Tassi" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Hints in unification",
      "author" : [ "Andrea Asperti", "Wilmer Ricciotti", "Claudio Sacerdoti Coen", "Enrico Tassi" ],
      "venue" : "In TPHOLs 2009,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "User interaction with the Matita proof assistant",
      "author" : [ "Andrea Asperti", "Claudio Sacerdoti Coen", "Enrico Tassi", "Stefano Zacchiroli" ],
      "venue" : "Journal of Automated Reasoning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Lambda Calculi with Types",
      "author" : [ "Henk Barendregt" ],
      "venue" : "In Abramsky, Samson and others, editor, Handbook of Logic in Computer Science,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1992
    }, {
      "title" : "A brief overview of Agda - a functional language with dependent types",
      "author" : [ "Ana Bove", "Peter Dybjer", "Ulf Norell" ],
      "venue" : "In Theorem Proving in Higher Order Logics, 22nd International Conference,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Coherence checking of coercions in Plastic",
      "author" : [ "P. Callaghan" ],
      "venue" : "Proc. Workshop on Subtyping and Dependent Types in Programming,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2000
    }, {
      "title" : "Subtyping, Type Conversion and Transitivity Elimination",
      "author" : [ "Gang Chen" ],
      "venue" : "PhD thesis, University Paris",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1998
    }, {
      "title" : "Formalizing Overlap Algebras in Matita",
      "author" : [ "Claudio Sacerdoti Coen", "Enrico Tassi" ],
      "venue" : "Mathematical Structures in Computer Science,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "The Calculus of Constructions",
      "author" : [ "Thierry Coquand", "Gérard P. Huet" ],
      "venue" : "Inf. Comput.,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1988
    }, {
      "title" : "Greedy bidirectional polymorphism",
      "author" : [ "Joshua Dunfield" ],
      "venue" : "In ML Workshop (ML",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "Tridirectional typechecking",
      "author" : [ "Joshua Dunfield", "Frank Pfenning" ],
      "venue" : "In X. Leroy, editor, Conference Record of the 31st Annual Symposium on Principles of Programming Languages",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2004
    }, {
      "title" : "Packaging mathematical structures",
      "author" : [ "François Garillot", "Georges Gonthier", "Assia Mahboubi", "Laurence Rideau" ],
      "venue" : "In Proceedings of the 22nd International Conference on Theorem Proving in Higher Order Logics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "A unification algorithm for typed lambda-calculus",
      "author" : [ "Gérard P. Huet" ],
      "venue" : "Theor. Comput. Sci.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1975
    }, {
      "title" : "Coercive subtyping",
      "author" : [ "Zhaohui Luo" ],
      "venue" : "J. Logic and Computation,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1999
    }, {
      "title" : "A Calculus of Substitutions for Incomplete-Proof Representation in Type Theory",
      "author" : [ "César Muñoz" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1997
    }, {
      "title" : "Towards a practical programming language based on dependent type theory",
      "author" : [ "Ulf Norell" ],
      "venue" : "PhD thesis, Department of Computer Science and Engineering, Chalmers University of Technology,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "Définitions Inductives en Théorie des Types d’Ordre Supérieur",
      "author" : [ "Christine Paulin-Mohring" ],
      "venue" : "Habilitation à diriger les recherches,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1996
    }, {
      "title" : "Local type inference",
      "author" : [ "Benjamin C. Pierce", "David N. Turner" ],
      "venue" : "ACM Trans. Program. Lang. Syst.,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2000
    }, {
      "title" : "Dependently typed records in type theory",
      "author" : [ "Robert Pollack" ],
      "venue" : "Formal Aspects of Computing,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2002
    }, {
      "title" : "Mathematical Knowledge Management and Interactive Theorem Proving",
      "author" : [ "Claudio Sacerdoti Coen" ],
      "venue" : "PhD thesis, University of Bologna,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2004
    }, {
      "title" : "Spurious disambiguation errors and how to get rid of them",
      "author" : [ "Claudio Sacerdoti Coen", "Stefano Zacchiroli" ],
      "venue" : "Journal of Mathematics in Computer Science, special issue on Management of Mathematical Knowledge,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2008
    }, {
      "title" : "Typing algorithm in type theory with inheritance",
      "author" : [ "Amokrane Säıbi" ],
      "venue" : "In Proceedings of the 24th ACM SIGPLAN-SIGACT symposium on Principles of programming languages,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1997
    }, {
      "title" : "Subset coercions in Coq. In Types for Proofs and Programs, volume 4502/2007 of LNCS, pages 237–252",
      "author" : [ "Matthieu Sozeau" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2006
    }, {
      "title" : "First-class type classes",
      "author" : [ "Matthieu Sozeau", "Nicolas Oury" ],
      "venue" : "In Proceedings of TPHOLs,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2008
    }, {
      "title" : "Verified Computing in Homological Algebra. A Journey Esploring the Power and Limits of Dependent Type Theory",
      "author" : [ "Arnaud Spiwack" ],
      "venue" : "PhD thesis, École Polytechniqe,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2011
    }, {
      "title" : "Construction and Deduction in Type Theories",
      "author" : [ "Martin Strecker" ],
      "venue" : "PhD thesis, Universität Ulm,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1998
    }, {
      "title" : "Type processing by constraint reasoning",
      "author" : [ "Peter J. Stuckey", "Martin Sulzmann", "Jeremy Wazny" ],
      "venue" : "In APLAS, pages",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2006
    }, {
      "title" : "Interactive Theorem Provers: issues faced as a user and tackled as a developer",
      "author" : [ "Enrico Tassi" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2008
    }, {
      "title" : "Une Théorie des Constructions Inductives",
      "author" : [ "Benjamin Werner" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1994
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "This paper is devoted to the description of a refinement algorithm for the Calculus of (Co)Inductive Constructions, the type theory on which the Matita [6], Coq [12] and Lego [19] ITPs are based on.",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "In this and in the previous paper [4] we are interested in the implementation of interactive theorem provers (ITP) for dependently typed languages that are heavily based on the Curry-Howard isomorphism.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 7,
      "context" : "Agda [8] and Matita [6] are examples of systems implemented in this way.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 5,
      "context" : "Agda [8] and Matita [6] are examples of systems implemented in this way.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 27,
      "context" : "thesis [31] partially describes a forthcoming release of Coq 8.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 3,
      "context" : "The implementation of a kernel for a variant of the Calculus of (Co)Inductive Constructions (CIC) has been described in [4] down to the gory details that make the implementation efficient.",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 14,
      "context" : "Canonical structures [16], unification hints [5] and type classes [30] are devices that let the user drive some form of proof search that is seamlessly integrated in the refinement process.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 4,
      "context" : "Canonical structures [16], unification hints [5] and type classes [30] are devices that let the user drive some form of proof search that is seamlessly integrated in the refinement process.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 26,
      "context" : "Canonical structures [16], unification hints [5] and type classes [30] are devices that let the user drive some form of proof search that is seamlessly integrated in the refinement process.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 20,
      "context" : "To avoid or mitigate the drawbacks of type inference, bi-directional type-checking algorithms have been introduced in the literature [24].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 15,
      "context" : "3To the authors knowledge, Isabelle [18] is the only interactive prover implementing Huet’s algorithm [17] capable of generating all second order unifiers",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "In the latter case, the system needs to recognize that lists are containers and has to have code to lift coercions over containers, like in [10].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 5,
      "context" : "The rest of the paper explains the bi-directional refinement algorithm implemented in Matita [6].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "For the sake of the reader, Appendix 8 is taken from [4] with minor modifications and it shows the type checking algorithm implemented by the kernel.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "The CIC calculus extended with metavariables has been studied in [21] and the flavor of metavariables implemented in Matita is described in [26].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 22,
      "context" : "The CIC calculus extended with metavariables has been studied in [21] and the flavor of metavariables implemented in Matita is described in [26].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 6,
      "context" : "We regard CIC as a Pure Type System [7], and we denote by PTS the set of axioms.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 3,
      "context" : "The details for the actual PTS used in Matita are given in [4].",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "This is relevant for CIC since the sort of propositions, Prop, is non informative and cannot be eliminated to inhabit a data type of sort Typeu for any u (but for few exceptions described in [4] Section 6).",
      "startOffset" : 191,
      "endOffset" : 194
    }, {
      "referenceID" : 31,
      "context" : "The typing judgment implemented in our kernel is an extension of the regular typing judgment for CIC [35, 23, 13].",
      "startOffset" : 101,
      "endOffset" : 113
    }, {
      "referenceID" : 19,
      "context" : "The typing judgment implemented in our kernel is an extension of the regular typing judgment for CIC [35, 23, 13].",
      "startOffset" : 101,
      "endOffset" : 113
    }, {
      "referenceID" : 11,
      "context" : "The typing judgment implemented in our kernel is an extension of the regular typing judgment for CIC [35, 23, 13].",
      "startOffset" : 101,
      "endOffset" : 113
    }, {
      "referenceID" : 3,
      "context" : "It is described in [4] and reported in the Appendix 8.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 28,
      "context" : "thesis [32], the order is not preserved by unification and thus in any realistic implementation Σ is to be implemented as a set and the fact that Σ remains a partial order must be preserved as an invariant.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 22,
      "context" : "The unification algorithm implemented in Matita goes beyond the scope of this paper, the interested reader can find more details in [26, 5].",
      "startOffset" : 132,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : "The unification algorithm implemented in Matita goes beyond the scope of this paper, the interested reader can find more details in [26, 5].",
      "startOffset" : 132,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : "A precise definition of smallness together with the corresponding rules for pattern matching can be found in [4].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "Another important design choice is to design the kernel of the system so that it handles metavariables [4].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 27,
      "context" : "thesis [31] discusses this issue at length as a motivation for a complete re-design of the data type for proofs in Coq.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 25,
      "context" : "This problem was already partially addressed by Sozeau [29] where he added a new system layer around the refiner to achieve the behavior that our refiner already provides.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 20,
      "context" : "The new rule for lambda-abstraction is well known in the literature [24] and it is also the only one implemented in Coq.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 16,
      "context" : "While the literature [20] considers them mostly as a device to mimic sub-typing in a calculus lacking it, they have other interesting applications.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 21,
      "context" : "The last case is recurrent when algebraic structures are encoded as dependently typed records [25] embedding the type (or carrier) for the elements together with the operations and properties defining the structure.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 30,
      "context" : "In Matita, for various reasons detailed in [34], ∆ is not a graph, but a set of arcs for the transitive closure of the graph.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "The last detail worth mentioning is that, all systems known to the authors with the notable exception of Plastic [9], adopt some approximated representation for the nodes in the coercion graph, usually the name of the head constant of the source and target types.",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 28,
      "context" : "6In the case of dependent types the unification of the types is a necessary condition for the unification of the two terms, as claimed by Strecker [32].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 24,
      "context" : "The extension to the typing algorithm of CIC with explicit casts in [28] follows the same spirit of our refinement algorithm for raw terms.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 18,
      "context" : "Another work in topic is [22] where Norell describes the bi-directional type inference algorithm implemented in the Agda interactive prover.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 18,
      "context" : "The most notable example of type inference algorithm based on constraint solving is the one adopted for the Agda system [22].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 29,
      "context" : "A strong characteristic of constraint based type inference is precise error reporting, as described in [33].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 23,
      "context" : "Even if it the heuristics adopted in Matita [27] to discard spurious error reports are slightly more complex than the ones proposed by Stuckey, we believe that they provide a similar precision.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 12,
      "context" : "Greedy algorithms [14], like the one presented in this paper, are characterized by a very predictable behavior, at the cost of being forced to take early decisions leading to the rejection of some possibly well typed terms.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 20,
      "context" : "Many algorithms to infer a polymorphic type for a program prefer to avoid the use of unification [24] since unification variables may represent type constraints coming from distant, loosely related, sub-terms.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 13,
      "context" : "These approaches also scaled up to types with some sort of dependency over terms, as in [15].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 14,
      "context" : "Recent important developments [16] heavily rely on a user-extensible unification algorithm [5], using it as a predictable form of Prolog-like inference engine.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "Recent important developments [16] heavily rely on a user-extensible unification algorithm [5], using it as a predictable form of Prolog-like inference engine.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "Its effectiveness has been validated in all the formalizations carried on using the Matita interactive theorem prover [11, 1, 2, 3], whose refiner is based",
      "startOffset" : 118,
      "endOffset" : 131
    }, {
      "referenceID" : 0,
      "context" : "Its effectiveness has been validated in all the formalizations carried on using the Matita interactive theorem prover [11, 1, 2, 3], whose refiner is based",
      "startOffset" : 118,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "Its effectiveness has been validated in all the formalizations carried on using the Matita interactive theorem prover [11, 1, 2, 3], whose refiner is based",
      "startOffset" : 118,
      "endOffset" : 131
    }, {
      "referenceID" : 2,
      "context" : "Its effectiveness has been validated in all the formalizations carried on using the Matita interactive theorem prover [11, 1, 2, 3], whose refiner is based",
      "startOffset" : 118,
      "endOffset" : 131
    }, {
      "referenceID" : 25,
      "context" : "This eased the implementation of subset coercions in the style of [29], but that topic falls outside the scope of the present paper and is thus not discussed.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : "The following appendix is an extract of the paper [4] in which the reader can find all the details of the type checking algorithm implemented in the Matita interactive prover.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 3,
      "context" : "Moreover, the rule presented in [4] is more liberal than the one presented here that just uses the test (s, s′) ∈ elim(PTS) to check that a non informative data is never analyzed to obtain an informative one.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "The actual rules used in the kernel and the refiner of Matita also allow in every situation the elimination of inhabitants of singleton inductive types, whose definition is given in [4].",
      "startOffset" : 182,
      "endOffset" : 185
    }, {
      "referenceID" : 3,
      "context" : "− → tn guarded by destructors ([4], Sect.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "− → tn guarded by constructors ([4], Sect.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : ", kmn n undefined in Env Env,Σ `WF Env,Σ,Φ `WF all the conditions in [4], Sect.",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "Typeu ≤ Typev Typev ≤ Typeu are declared constraints ([4], Sect.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "3) Env,Σ,Φ,Γ ` Typeu ↓= Typev Typeu ≤ Typev is a declared constraint ([4], Sect.",
      "startOffset" : 70,
      "endOffset" : 73
    } ],
    "year" : 2012,
    "abstractText" : "The paper describes the refinement algorithm for the Calculus of (Co)Inductive Constructions (CIC) implemented in the interactive theorem prover Matita. The refinement algorithm is in charge of giving a meaning to the terms, types and proof terms directly written by the user or generated by using tactics, decision procedures or general automation. The terms are written in an “external syntax” meant to be user friendly that allows omission of information, untyped binders and a certain liberal use of user defined sub-typing. The refiner modifies the terms to obtain related well typed terms in the internal syntax understood by the kernel of the ITP. In particular, it acts as a type inference algorithm when all the binders are untyped. The proposed algorithm is bi-directional: given a term in external syntax and a type expected for the term, it propagates as much typing information as possible towards the leaves of the term. Traditional mono-directional algorithms, instead, proceed in a bottomup way by inferring the type of a sub-term and comparing (unifying) it with the type expected by its context only at the end. We propose some novel bi-directional rules for CIC that are particularly effective. Among the benefits of bi-directionality we have better error message reporting and better inference of dependent types. Moreover, thanks to bi-directionality, the coercion system for sub-typing is more effective and type inference generates simpler unification problems that are more likely to be solved by the inherently incomplete higher order unification algorithms implemented. Finally we introduce in the external syntax the notion of vector of placeholders that enables to omit at once an arbitrary number of arguments. Vectors of placeholders allow a trivial implementation of implicit arguments and greatly simplify the implementation of primitive and simple tactics. 1998 ACM Subject Classification: D.3.1, F.3.0.",
    "creator" : "LaTeX with hyperref package"
  }
}