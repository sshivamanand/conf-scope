{
  "name" : "1012.3410.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Descriptive-complexity based distance for fuzzy sets",
    "authors" : [ "Laszlo Kovacs", "Joel Ratsaby" ],
    "emails" : [ "kovacs@iit.uni-miskolc.hu", "ratsaby@ariel.ac.il." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Key words: Fuzzy sets, descriptive complexity, entropy, distance."
    }, {
      "heading" : "1 Introduction",
      "text" : "The notion of distance between two objects is very general. Distance metrics and distances have now become an essential tool in many areas of mathematics and its applications including geometry, probability, statistics, coding/graph theory, data analysis, pattern recognition. For a comprehensive source on this subject see [4]. The notion of a fuzzy set was introduced by [8]. It is a class of objects with continuous values of membership and hence extends the classical definition of a set (to distinguish it from a fuzzy set we refer to it as a crisp set). Formally, a fuzzy set is a pair (E,m) where E is a set of objects and m is a membership function m : E → [0, 1]. Fuzzy set theory can be used in a wide range of domains in which information is incomplete or imprecise, such as pattern recognition, decision theory. The concept of distance and similarity is important in the area of fuzzy logic and sets. We now review some common ways of defining distances on fuzzy sets (see [9] and references therein).\nClassical distances measure how far two points are in Euclidean space. For instance, the Minkowski distance between two points x and y in Rn is defined as\ndr(x, y) := ( n∑ i=1 |xi − yi|r )1/r , r ≥ 1. (1)\n∗Department of Information Technology, University of Miskolc, HUNGARY. H-3515. Miskolc-Egyetemvaros. Email : kovacs@iit.uni-miskolc.hu †Department of Electrical and Electronics Engineering, Ariel University Center of Samaria, Ariel 40700, ISRAEL. Email : ratsaby@ariel.ac.il. (Corresponding author).\n1\nar X\niv :1\n01 2.\n34 10\nv1 [\ncs .A\nI] 1\n5 D\nec 2\n01 0\nLet E be a finite set and let Φ(E) be the set of all fuzzy subsets of E. Consider A,B two fuzzy subsets A, B ∈ Φ(E) with membership functions mA,mB : E → [0, 1]. Then (1) can be extended to the following distance,\ndr(A,B) := (∑ x∈E |mA (x)−mB(x)|r )1/r , r ≥ 1.\nBased on (1) letting r = 2 we have the Hausdorff distance between two nonempty compact crisp sets U , V ⊂ R,\nq (U, V ) := max { sup v∈V inf u∈U d2 (u, v) , sup u∈U inf v∈V d2 (u, v) } . (2)\nThis can be extended to fuzzy sets as follows: let A ∈ Φ(E) be a fuzzy set and denote by Aα the α-level set of the fuzzy set A which is defined as Aα = {x ∈ E | mA(x) ≥ α}. Then for two fuzzy subsets A, B ∈ Φ(E) the distance in (2) can be extended to the following distance between A and B,\nq(A,B) := ∫ 1 0 q(Aα, Bα)dα.\nAnother approach is based on set-theoretic distance functions. For a fuzzy set A ∈ Φ(E) define the cardinality of A as |A| = ∑ x∈EmA(x). Extend the intersection and union operations by defining the membership functions\nmA∩B(x) := min {mA(x),mB(x)}\nand\nmA∪B(x) := max {mA(x),mB(x)} .\nThen for fuzzy sets A, B ∈ Φ(E) we may define the distance function\nS1(A,B) := 1− |A ∩B| |A ∪B| = 1− ∑ x∈EmA∩B(x)∑ x∈EmA∪B(x) .\nAnother distance is based on four features of a fuzzy set. Let the domain of interest be R and consider a fuzzy set A in Φ(R). The power of A (which extends the notion of cardinality) is defined as\npower (A) := ∫ ∞ −∞ mA(x)dx.\nLet S(x) = −x lnx− (1− x) ln(1− x) then define the entropy of A as\nentropy (A) := ∫ ∞ −∞ S(mA(x))dx.\nDefine the centroid as\nc(A) :=\n∫∞ −∞ xmA(x)dx\npower (A)\nand the skewness as skew (A) := ∫ ∞ −∞ (x− c(A))3mA(x)dx.\nLet v(A) = [power (A) , entropy (A) , c(A), skew(A)] then [2] defines the distance between two fuzzy setsA, B ∈ Φ(R) as the Euclidean distance ‖v(A)− v(B)‖.\nWe now proceed to discuss the notion of distances that are based on descriptive complexity of sets."
    }, {
      "heading" : "2 Information based distances",
      "text" : "A good distance is one which picks out only the ‘true’ dissimilarities and ignores factors that arise from irrelevant variables or due to unimportant random fluctuations that enter the measurements. In most applications the design of a good distance requires inside information about the domain, for instance, in the field of information retrieval [1] the distance between two documents is weighted largely by words that appear less frequently since the words which appear more frequently are less informative.\nTypically, different domains require the design of different distance functions which take such specific prior knowledge into account. It can therefore be an expensive process to acquire expertise in order to formulate a good distance. Recently, a new distance for sets was introduced [7] which is based on the concept of descriptional complexity (or discrete entropy). A description of an object in a finite set can be represented as a finite binary string which provides a unique index of the object in the set. The description complexity of the object is the minimal length of a string that describes the object. The distance of [7] is based on the idea that two sets should be considered similar if given the knowledge of one the additional complexity in describing an element of the other set is small (this is also referred to as the conditional combinatorial entropy, see [5, 6] and references therein). The advantage in this formulation of distance is its universality, i.e., it can be applied without any prior knowledge or assumption about the domain of interest, i.e., the elements that the sets contain. Such a distance can be viewed as an information-based distance since the conditional descriptional complexity is essentially the amount of information needed to describe an element in one set given that we know the other set (for more on the notion of combinatorial information and entropy see [5, 6]).\nIn the current paper we introduce a distance function between two general sets, i.e., sets that can be crisp or fuzzy. Following the information-based approach of [7] we resort to entropy as the main operator that gives the measure of dissimilarity between two sets. We use the membership of the symmetric difference of two sets as the probability of a Bernoulli random variable whose entropy is the expected description complexity of an element that belongs to only one of the two sets. Thus the distance function measures how many bits (on average) are needed to describe an element in the symmetric difference of the two sets. In other words, it is the amount of additional information needed to describe any one of the two sets given knowledge of the other.\nBeing a description-complexity based distance gives it certain characteristic properties. For instance, the distance between a crisp set A and its complement A is zero since there is no need for additional information in order to describe\none of these two sets when knowing the other. That is, knowledge of a set A automatically implies knowing the set A and vice versa. They are clearly not equal but our distance function cleverly renders them as the most similar that two sets can be (zero distance apart).\nThe next section formally introduces the distance and in Theorem 2 we prove its metric properties."
    }, {
      "heading" : "3 Distance function",
      "text" : "We write w.p. for “with probability”. Let [N ] = {1, . . . , N} be a domain of interest. Let A ∈ Φ([N ]) be a set with membership function mA : [N ]→ [0, 1]. We use x to denote a value in [N ]. Given two fuzzy subsets A, B ∈ Φ([N ]) with membership functions mA(x), mB(x), as mentioned in section 1 we denote by\nmA∪B(x) := max {mA(x),mB(x)}\nand\nmA∩B(x) := min {mA(x),mB(x)} . Define by A M B = (A ⋃ B) \\ (A ⋂ B) the symmetric difference between crisp sets A,B. For fuzzy sets A, B ∈ Φ([N ]) define by\nmAMB(x) := mA∪B(x)−mA∩B(x).\nDefine a sequence of Bernoulli random variables XA(x) for x ∈ [N ] taking the value 1 w.p. mA(x) and the value 0 w.p. 1−mA(x). Define by H(XA(x)) the entropy of XA(x),\nH(XA(x)) := −mA(x) logmA(x)− (1−mA(x)) log(1−mA(x)).\nDefine the random variable\nXAMB(x) := { 1 w.p. mAMB(x) 0 w.p. 1−mAMB(x).\nWe define a new distance between A, B ∈ Φ([N ]) as\ndist(A,B) := 1\nN N∑ x=1 H(XAMB(x))\nRemark 1. This definition can easily be extended to the case of an infinite domain, for instance, a subset of the real line. In that case the distance can be defined as the expected value of EH(XAMB(ξ)) where ξ is a random variable with some probability distribution P (ξ) with respect to which the expectation is computed.\nThe next theorem shows that the distance satisfies the metric properties.\nTheorem 2. The function dist(A,B) is a semi-metric on Φ([N ]), i.e., it is non-negative, symmetric, it equals zero if A = B and it satisfies the triangle inequality.\nRemark 3. Note that the function dist(A,B) may equal zero even when A 6= B. We now prove Theorem 2.\nProof. Since the entropy function is non-negative (see for instance, [3]) then for any two subsets A, B ∈ Φ([N ]) we have dist(A,B) ≥ 0. It is easy to see that the symmetry property is satisfied since for every x ∈ [N ] we have XAMB(x) = XBMA(x). For every subset A the value dist(A,A) = 0 sincemAMA(x) = 0 hence H(XAMA(x)) = 0 for all x.\nLet us now show that the triangle inequality holds. Let A, B, C be any three elements of Φ([N ]). Fix any point x ∈ [N ] and without loss of generality suppose that mA(x) ≤ mB(x) ≤ mC(x). Denote by p = mB(x) − mA(x) and q = mC(x) − mB(x). Without loss of generality assume that p ≤ q. Then we have mAMC(x) = p + q. Denote by H(p), H(q) and H(p + q) the entropies H(XAMB(x)), H(XBMC(x)) and H(XAMC(x)) respectively. We aim to show that H(p + q) ≤ H(p) + H(q). This will imply that for every x ∈ [N ], H(XAMC(x)) ≤ H(XAMB(x)) +H(XBMC(x)) and hence it holds for the average 1 N ∑ xH(XAMC(x)) ≤ 1 N ∑ xH(XAMB(x)) + 1 N ∑ xH(XBMC(x)).\nWe start by considering the straight line function ` : [0, 1] → [0, 1] defined as:\n`(z) := H(q)−H(p) q − p z +H(p)− H(q)−H(p) q − p p\nwhich cuts through the points (z, `(z)) = (p,H(p)) and (z, `(z)) = (q,H(q)). For a function f let f ′ denote its derivative. We claim the following,\nClaim 1. H ′(z) ≤ `′(z) for all z ∈ [q, 1]. Proof : The derivative of H(z) is H ′(z) = log ( 1−z z ) . This is a decreasing function on [0, 1] hence it suffices to show that H ′(q) ≤ `′(z) for all z ∈ [q, 1]. The derivative of `(z) is H(q)−H(p)q−p . So it suffices to show that\nlog ( 1− q q ) ≤ H(q)−H(p) q − p .\nThis is equivalent to (q − p) log (\n1− q q\n) ≤ H(q)−H(p). (3)\nThe left hand side of (3) can be reduced to,\nq log(1− q)− q log q − p log(1− q) + p log q. (4)\nAdding and subtracting the term (1− q) log(1− q) and using H(q) = −q log q− (1− q) log(1− q) makes (4) be expressed as\nH(q) + p log q + (1− p) log(1− q). Substituting this for the left hand side of (3) and canceling H(q) on both sides gives the following inequality which we need to prove\np log q + (1− p) log(1− q) ≤ p log p+ (1− p) log(1− p).\nIt suffices to show that,\np log\n( p\nq\n) + (1− p) log ( 1− p 1− q ) ≥ 0. (5)\nThat (5) holds follows from the information inequality (see Theorem 2.6.3 of [3]) which lower bounds the divergence D(P ||Q) ≥ 0 where P ,Q are two probability functions and D(P ||Q) = ∑ x P (x) log P (x) Q(x) . Hence the claim is proved.\nNext we claim the following: Claim 2. H(p+ q) ≤ `(p+ q). Proof : Consider the case that p ≤ q ≤ 12 . Since q ≤ 1 2 then H\n′(z) evaluated at z = q is non-negative. Hence both H(z) and `(z) are monotone increasing on q ≤ z ≤ 12 and H(q) = `(q). By Claim 1, ` increases faster than H on [q, 1], in particular on the interval q ≤ z ≤ 12 . Hence, for all z ∈ [q, 1 2 ] we have H(z) ≤ `(z). Now, if p + q ∈ [q, 12 ] then it follows that H(p + q) ≤ `(p + q). Otherwise it must hold that p+q ∈ ( 12 , 1]. ButH is decreasing and ` is increasing over this interval. Hence we have `(z) > `( 12 ) ≥ H( 1 2 ) > H(z) for z ∈ ( 1 2 , 1], in particular for z = p+ q hence `(p+ q) ≥ H(p+ q). This proves the claim. From Claim 2 it follows that\nH(p+ q) ≤ H(q)−H(p) q − p (p+ q) +H(p)− H(q)−H(p) q − p p\n= H(p) + q H(q)−H(p)\nq − p . (6)\nIt suffices to show that q H(q)−H(p)\nq − p ≤ H(q)\nor equivalently, H(q)\nq ≤ H(p) p . (7)\nLetting f(z) = H(z)z and differentiating we obtain\nf ′(z) = log(1− z)\nz2\nwhich is non-positive for z ∈ [0, 1]. Hence f is non-increasing over this interval. Since by assumption q ≥ p then it follows that f(q) ≤ f(p) and 7 holds. This completes the proof of the theorem."
    }, {
      "heading" : "4 Simple Examples",
      "text" : "Let us evaluate this distance for a few examples. Consider two sets A and its complement A. Their membership functions satisfy the relation:\nmA(x) = 1−mA(x)\nhence the membership function for the symmetric difference is\nmAMA(x) = max {mA(x), (1−mA(x))} −min {mA(x), (1−mA(x))} .\nNote that for any x ∈ [N ] with a crisp membership value, i.e., mA(x) = 1, or mA(x) = 0, we have mAMA(x) = 1 and hence in this case H(XAMA(x)) = 0. This means that for a crisp set A (for all x ∈ A, mA(x) ∈ {0, 1}) our distance has the following property (we call this the complement-property):\ndist(A,A) = 0.\nFrom an information theoretic perspective, this property is expected since knowing a set A automatically means that we also know how to describe its complement. Hence there is no additional description necessary to describe A given A. This is what dist(A,A) = 0 means.\nLet us now consider some examples of pairs of fuzzy sets and their distances. Let N = 20 and the domain be [N ] = {1, 2, . . . , 20}. In the following examples we plot the membership functions of several fuzzy sets. Note, we connect the point values of the membership function by lines in order to make the plots clearer (remember that the actual membership functions are defined only on the discrete set [N ]).\nExample 4. Consider the fuzzy sets A,B,C and the complement Ac with membership functions as shown in Figure 1. Note, that A and its complement are crisp sets. The distance matrix D = [di,j ] is shown in (8); the rows and columns correspond to A, B, C and Ac so that for instance the element d2,3 = dist (B,C) = 0.709. As can be seen, C is a translated version of B and they are both the same distance from A. This is due to H(XAMB(x)) = H(XAMC(x+10)). B and C are farther apart than B and A. Since dist (A,Ac) = 0 then each one of B, C is of the same distance to A as to Ac.\nD =  0 0.354 0.354 0 0.354 0 0.709 0.354 0.354 0.709 0 0.354\n0 0.354 0.354 0  (8) Example 5. Continuing with the same domain as in Example 4 let us consider the fuzzy sets A,B,C and the complement Ac with membership functions as shown in Figure 2. The membership function of the set C is now flat and as the distance matrix D = [di,j ] in (9) shows C is now farther from B (which has a triangular membership function). As in the previous example B remains closer to A than to C.\nD =  0 0.354 0.5 0\n0.354 0 0.854 0.354 0.5 0.854 0 0.5 0 0.354 0.5 0  (9) Example 6. Continuing with the same domain as in Example 4 let us consider the fuzzy sets A,B,C and the complement Ac with membership functions as shown in Figure 3. Note that now C is translated from B by an amount that is smaller compared to Example 4. As can be seen from the distance matrix of (10) this results in a smaller distance dist (B,C) = 0.336 compared to 0.709. As in Example 4 , A is as similar to B as to C since the distance dist (A,B) = dist (A,C) = 0.354.\nD =  0 0.354 0.354 0 0.354 0 0.336 0.354 0.354 0.336 0 0.354\n0 0.354 0.354 0\n (10)"
    }, {
      "heading" : "5 Clustering using the distance",
      "text" : "We tested the proposed distance function on real data. The data1 consists of answers from a survey given to the general population of 28 European countries. There are ten questions in the survey where a valid answer is a number in the set {1, . . . , 10}. The value 10 represents the most positive opinion and 1 the most pessimistic opinion (we denote the name of the attribute in parenthesis):\n• trust in local parliament (country_GOV)\n• trust in local politicians (politicians)\n• trust in EU Parliament (EU_GOV)\n• trust in United Nations (UN)\n• trust in country’s parliament (country_GOV)\n• how satisfied with life (Life)\n• how satisfied with the national government (National_GOV)\n• immigration is bad or good (Immigration)\n• the state of health services (Health)\n• how happy are you (happy)\nAfter normalizing each component we represent each country as a fuzzy set on a domain that consists of the ten attributes. Table 1 displays the membership functions for each of the countries. Each row in this table represents a membership function mi(x) of the fuzzy set Ci of country i. Based on this information we compute the distance d(Ci, Cj) between every possible pair of countries Ci, Cj and obtain a distance matrix D = [di,j ], di,j := dist (Ci, Cj). We use D as the newly transformed version of the original data (Table 1) and do data-clustering on it. The ith row of D is a feature vector representation of country i. We use the k-means clustering procedure.\nFigure 4 shows the result of the k-means clustering where the horizontal axis displays the cluster number and the vertical axis shows the distance of each point in a cluster to the mean of the cluster. In order to interpret this result, let us look at the fuzzy sets of some of the clusters. Figure 5 displays the fuzzy sets of cluster #1. As seen, the country Spain is considered similar to the rest of the countries in this cluster although its “happy” value is almost complement to the rest of the countries. This follows from the complement-property of our distance function (see section 4).\nFigure 6 displays the fuzzy sets of cluster #2. Ukraine seems to behave almost the opposite of Denmark (besides on the attributes EU_Gov where both have similar values). Ukraine and Turkey have interesting behaviors: they take very similar values for the attributes country-GOV up to Life and on UN while on the rest of the attributes they are almost mutually complement. Hence\n1The data set is the European Social Survey Round 4 Data (2008). Data file edition 3.0. Norwegian Social Science Data Services, Norway – Data Archive and distributor of ESS data. http://ess.nsd.uib.no/.\naccording to our distance they are considered close (which is why they are placed in the same cluster).\nFigure 7 shows the fuzzy sets of several countries in custer #3. Hungary and the Russian Federation take very similar values and hence are close. Israel versus Hungary or versus Russian Federation has a similar behavior on the attributes country_GOV, EU_GOV, happy, National_GOV, politicians, UN, while on Health, Immigration, Life it has almost the complement values. Hence, overall, our distance function renders Israel as close to Hungary and Russia.\nWe also ran a clustering procedure which is a variant of the Kohonen Self Organizing Map. The results that we obtained are very similar to those obtained by the k-means procedure."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper introduces a new distance function dist(A,B) for fuzzy sets A,B based on their descriptive complexity. The distance is shown to be a semimetric that satisfies the triangle inequality. In comparison to other existing distance-functions for fuzzy sets this new metric is proportional to the additional amount of information needed to describe fuzzy set A when knowing fuzzy set B or vice versa. It thus has a natural information-based interpretation. Doing pattern clustering based on this distance we have shown that fuzzy sets that are clustered together tend to be more mutually informative. This is an interesting new property that can be useful for analyzing other data sets."
    } ],
    "references" : [ {
      "title" : "Modern Information Retrieval",
      "author" : [ "R. Baeza-Yates", "B. Ribeiro-Neto" ],
      "venue" : "Addison-Wesley",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A pattern recognition approach to the problem of linguistic approximation in system analysis",
      "author" : [ "P.P. Bonisson" ],
      "venue" : "Proceeding of the International Conference on Cybernetics and Society, pages 793–798",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Elements of information theory",
      "author" : [ "T.M. Cover", "J.A. Thomas" ],
      "venue" : "Wiley- Interscience, New York, NY, USA",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Encyclopedia of Distances",
      "author" : [ "M. Deza", "E. Deza" ],
      "venue" : "volume 15 of Series in Computer Science. Springer-Verlag",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Information efficiency",
      "author" : [ "J. Ratsaby" ],
      "venue" : "Proc. of 33rd Conference on Current Trends in Theory and Practice of Computer Science (SOFSEM ’07), volume LNCS 4362, pages 475–487",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Information width",
      "author" : [ "J. Ratsaby" ],
      "venue" : "Technical Report # arXiv:0801.4790v2",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Information set distance",
      "author" : [ "J. Ratsaby" ],
      "venue" : "Proc. of Mini-Conference on Applied Theoretical Computer Science (MATCOS-10), Koper, Slovenia, Oct. 13-14. University of Primorska press",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Fuzzy sets",
      "author" : [ "L.A. Zadeh" ],
      "venue" : "Information Control, 8:338–353",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "Measures of similarity among fuzzy concepts: A comparative analysis",
      "author" : [ "R. Zwick", "E. Carlstein", "D.V. Budescu" ],
      "venue" : "International Journal of Approximate Reasoning, 1:221–242",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1987
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "For a comprehensive source on this subject see [4].",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 7,
      "context" : "The notion of a fuzzy set was introduced by [8].",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "Formally, a fuzzy set is a pair (E,m) where E is a set of objects and m is a membership function m : E → [0, 1].",
      "startOffset" : 105,
      "endOffset" : 111
    }, {
      "referenceID" : 8,
      "context" : "We now review some common ways of defining distances on fuzzy sets (see [9] and references therein).",
      "startOffset" : 72,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "Consider A,B two fuzzy subsets A, B ∈ Φ(E) with membership functions mA,mB : E → [0, 1].",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 1,
      "context" : "Let v(A) = [power (A) , entropy (A) , c(A), skew(A)] then [2] defines the distance between two fuzzy setsA, B ∈ Φ(R) as the Euclidean distance ‖v(A)− v(B)‖.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "In most applications the design of a good distance requires inside information about the domain, for instance, in the field of information retrieval [1] the distance between two documents is weighted largely by words that appear less frequently since the words which appear more frequently are less informative.",
      "startOffset" : 149,
      "endOffset" : 152
    }, {
      "referenceID" : 6,
      "context" : "Recently, a new distance for sets was introduced [7] which is based on the concept of descriptional complexity (or discrete entropy).",
      "startOffset" : 49,
      "endOffset" : 52
    }, {
      "referenceID" : 6,
      "context" : "The distance of [7] is based on the idea that two sets should be considered similar if given the knowledge of one the additional complexity in describing an element of the other set is small (this is also referred to as the conditional combinatorial entropy, see [5, 6] and references therein).",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : "The distance of [7] is based on the idea that two sets should be considered similar if given the knowledge of one the additional complexity in describing an element of the other set is small (this is also referred to as the conditional combinatorial entropy, see [5, 6] and references therein).",
      "startOffset" : 263,
      "endOffset" : 269
    }, {
      "referenceID" : 5,
      "context" : "The distance of [7] is based on the idea that two sets should be considered similar if given the knowledge of one the additional complexity in describing an element of the other set is small (this is also referred to as the conditional combinatorial entropy, see [5, 6] and references therein).",
      "startOffset" : 263,
      "endOffset" : 269
    }, {
      "referenceID" : 4,
      "context" : "Such a distance can be viewed as an information-based distance since the conditional descriptional complexity is essentially the amount of information needed to describe an element in one set given that we know the other set (for more on the notion of combinatorial information and entropy see [5, 6]).",
      "startOffset" : 294,
      "endOffset" : 300
    }, {
      "referenceID" : 5,
      "context" : "Such a distance can be viewed as an information-based distance since the conditional descriptional complexity is essentially the amount of information needed to describe an element in one set given that we know the other set (for more on the notion of combinatorial information and entropy see [5, 6]).",
      "startOffset" : 294,
      "endOffset" : 300
    }, {
      "referenceID" : 6,
      "context" : "Following the information-based approach of [7] we resort to entropy as the main operator that gives the measure of dissimilarity between two sets.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "Let A ∈ Φ([N ]) be a set with membership function mA : [N ]→ [0, 1].",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "Since the entropy function is non-negative (see for instance, [3]) then for any two subsets A, B ∈ Φ([N ]) we have dist(A,B) ≥ 0.",
      "startOffset" : 62,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "We start by considering the straight line function ` : [0, 1] → [0, 1] defined as: `(z) := H(q)−H(p) q − p z +H(p)− H(q)−H(p) q − p p",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "We start by considering the straight line function ` : [0, 1] → [0, 1] defined as: `(z) := H(q)−H(p) q − p z +H(p)− H(q)−H(p) q − p p",
      "startOffset" : 64,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "This is a decreasing function on [0, 1] hence it suffices to show that H ′(q) ≤ `′(z) for all z ∈ [q, 1].",
      "startOffset" : 33,
      "endOffset" : 39
    }, {
      "referenceID" : 2,
      "context" : "3 of [3]) which lower bounds the divergence D(P ||Q) ≥ 0 where P ,Q are two probability functions and D(P ||Q) = ∑ x P (x) log P (x) Q(x) .",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "Letting f(z) = H(z) z and differentiating we obtain f ′(z) = log(1− z) z2 which is non-positive for z ∈ [0, 1].",
      "startOffset" : 104,
      "endOffset" : 110
    } ],
    "year" : 2017,
    "abstractText" : "A new distance function dist (A,B) for fuzzy sets A and B is introduced. It is based on the descriptive complexity, i.e., the number of bits (on average) that are needed to describe an element in the symmetric difference of the two sets. The distance gives the amount of additional information needed to describe any one of the two sets given the other. We prove its mathematical properties and perform pattern clustering on data based on this distance.",
    "creator" : "LaTeX with hyperref package"
  }
}