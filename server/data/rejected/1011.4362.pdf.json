{
  "name" : "1011.4362.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "scherrer@loria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n01 1.\n43 62\nv1 [\ncs .A\nI] 1\n9 N\nov 2\nIntroduction\nWe consider linear approximations of the value function of the policy in the framework of Markov Decision Processes (MDP). We focus on two popular methods: the computation of the projected Temporal Difference fixed point (TD(0), TD for short), which Antos et al. (2008); Farahmand et al. (2008); Sutton et al. (2009) have recently presented as the minimization of the mean-square projected Bellman\nAppearing in Proceedings of the 27 th International Conference on Machine Learning, Haifa, Israel, 2010. Copyright 2010 by the author(s)/owner(s).\nEquation, and the minimization of the meansquare Bellman Residual (BR). In this article, we present some new analytical and empirical data, that shed some light on both approaches. The paper is organized as follows. Section 1 describes the MDP linear approximation framework and the two projection methods. Section 2 presents small MDP examples, where each method outperforms the other. Section 3 highlights a simple relation between the quantities TD and BR optimize, and show that while BR enjoys a performance guarantee, TD does not in general. Section 4 contains the main contribution of this paper: we describe a unified view in terms of oblique projections of the Bellman equation, which simplifies and extends the characterization of Schoknecht (2002) and the recent analysis of Yu & Bertsekas (2008). Eventually, Section 5 presents some simulations, that address the following practical questions: which of the method gives the best approximation? and how useful is our analysis for selecting it a priori?"
    }, {
      "heading" : "1. Framework and Notations",
      "text" : "The model We consider an MDP with a fixed policy, that is an uncontrolled discrete-time dynamic system with instantaneous rewards. We assume that there is a state space X of finite size N . When at state i ∈ {1, .., N}, there is a transition probability pij of getting to the next state j. Let ik the state of the system at time k. At each time step, the system is given a reward γkr(ik) where r is the instantaneous reward function, and 0 < γ < 1 is a discount factor. The value at state i is defined as the total expected return: v(i) := limN→∞ E [ ∑N−1 k=0 γ kr(ik) ∣ ∣ ∣ i0 = i ] . We write P the N ×N stochastic matrix whose elements are pij . v can be seen as a vector of R\nN . v is known to be the unique fixed point of the Bellman operator: T v := r+ γPv, that is v solves the Bellman Equation v = T v and is equal to L−1r where L = I − γP .\nTD or BR? The unified oblique projection view\nApproximation Scheme When the size N of the state space is large, one usually comes down to solving the Bellman Equation approximately. One possibility is to look for an approximate solution v̂ in some specific small space. The simplest and best understood choice is a linear parameterization: ∀i, v̂(i) = ∑mj=1 wjφj(i) where m ≪ N , the φj are some feature functions that should capture the general shape of v, and wj are the weights that characterize the approximate value v̂. For all i and j, write φj the N -dimensional vector corresponding to the jth feature function and φ(i) the mdimensional vector giving the features of state i. For any vector of matrix X , denote X ′ its transpose. The following N × m feature matrix Φ = (φ1 . . . φm) = (φ(i1) . . . φ(iN )) ′ leads to write the parameterization of v in a condensed matrix form: v̂ = Φw, where w = (w1, ..., wm) is the m-dimensional weight vector. We will now on denote span (Φ) this subspace of RN and assume that the vectors φ1, ..., φm form a linearly independent set.\nSome approximation v̂ of v can be obtained by minimizing v̂ 7→ ‖v̂−v‖ for some norm ‖ ·‖, that is equivalently by projecting v onto span (Φ) orthogonally with respect to ‖ · ‖. In a very general way, any symmetric positive definite matrix Q of RN induces a quadratic norm ‖ · ‖Q on RN as follows: ‖v‖Q = √ v′Qv. It is well known that the orthogonal projection with respect to such a norm, which we will denote Π‖·‖Q , has the following closed form: Π‖·‖Q = Φπ‖·‖Q where π‖·‖Q = (Φ ′QΦ)−1Φ′Q is the linear application from R N to Rm that returns the coordinates of the projection of a point in the basis (φ1, . . . , φm). With these notations, the following relations π‖·‖QΦ = I and π‖·‖QΠ‖·‖Q = π‖·‖Q hold.\nIn an MDP approximation context, where one is modeling a stochastic system, one usually considers a specific kind of norm/projection. Let ξ = (ξi) be some distribution on X such that ξ > 0 (it assigns a positive probability to all states). Let Ξ be the diagonal matrix with the elements of ξ on the diagonal. Consider the orthogonal projection of RN onto the feature space span (Φ) with respect to the ξ-weighted quadratic norm ‖v‖ξ = √ ∑N j=1 ξivi 2 = √ v′Ξv. For clarity of exposition, we will denote this specific projection Π := Π‖·‖Ξ = Φπ where π := π‖·‖Ξ = (Φ ′ΞΦ)−1Φ′Ξ.\nIdeally, one would like to compute the “best” approximation\nv̂best = Φwbest with wbest = πv = πL −1r.\nThis can be done with algorithms like TD(1) / LSTD(1)(Bertsekas & Tsitsiklis, 1996; Boyan, 2002), but they require simulating infinitely long trajectories\nand usually suffer from a high variance. The projections methods, which we focus on in this paper, are alternatives that only consider one-step samples.\nTD(0) fix point method The principle of the TD(0) method (TD for short) is to look for a fixed point of ΠT , that is, one looks for v̂TD in the space span (Φ) satisfying v̂TD = ΠT v̂TD. Assuming that the matrix inverse below exists1, it can be proved2 that v̂TD = ΦwTD with\nwTD = (Φ ′ΞLΦ)−1Φ′Ξr (1)\nAs pointed out by Antos et al. (2008); Farahmand et al. (2008); Sutton et al. (2009), when the inverse exists, the above computation is equivalent to minimizing for v̂ ∈ span (Φ) the TD error ETD(v̂) := ‖v̂ −ΠT v̂‖ξ down to 03.\nBR minimization method The principle of the Bellman Residual (BR) method is to look for v̂ ∈ span (Φ) so that it minimizes the norm of the Bellman Residual, that is the quantity EBR(v̂) := ‖v̂ − T v̂‖ξ. Since v̂ is of the form Φw, it can be seen that EBR(v̂) = ‖Φw − γPΦw − r‖ξ = ‖Ψw − r‖ξ using the notation Ψ = LΦ. Using standard linear least squares arguments, one can see that the minimum BR is obtained for v̂BR = ΦwBR with\nwBR = (Ψ ′ΞΨ)−1Ψ′Ξr. (2)\nNote that in this case, the above inverse always exists (Schoknecht, 2002)."
    }, {
      "heading" : "2. Two simple examples",
      "text" : "Example 1 Consider the 2 state MDP such that P = ( 0 1\n0 1\n)\n. Denote the rewards r1 and r2. One\nthus have v(1) = r1 + γr2 1−γ and v(2) = r2 1−γ . Consider the one-feature linear approximation with Φ = (1 2)′, with uniform distribution ξ = (.5 .5)′. Φ′ΞΦ = 52 , therefore π = (\n1 5 2 5\n)\n, and the weight of the best approx-\nimation is wbest = πv = 1 5r1 + 2+γ 5(1−γ)r2. This example has been proposed by Bertsekas & Tsitsiklis (1996) in order to show that fitted Value Iteration can diverge if the samples are not generated by the stationary distribution of the policy. In (Bertsekas & Tsitsiklis, 1996), the authors only consider the case r1 = r2 = 0\n1This is not necessary the case, as the forthcoming Example 1 (Section 2) shows.\n2Section 4 will generalize this derivation. 3This remark is also true if we replace ‖ · ‖ξ by any equivalent norm ‖ · ‖. This observation lead Sutton et al. (2009) to propose original off-policy gradient algorithms for computing the TD solution.\nTD or BR? The unified oblique projection view\nso that this diverging result was true even though the exact value function v(0) = v(1) = 0 did belong to the feature space. In the case r1 = r2 = 0, the TD and BR methods do calculate the exact solution (we will see later that this is indeed a general fact when the exact value function belongs to the feature space). We thus extend this model by taking (r1, r2) 6= (0, 0). As a scaling of the reward is translated exactly in the approximation, we consider the general form (r1, r2) = (cos θ, sin θ).\nConsider the TD solution: one has Φ′Ξ = ( 1 2 1 )\n, (I − γP )Φ = (1− 2γ 1− γ), thus (Φ′ΞΨ) = 52 − 3γ and Φ′Ξr = r12 + r2. Eventually the weight of the TD approximation is wTD = r1+2r2 5−6γ . One notices here that the value γ = 5/6 is singular. Now, consider the BR solution. One can see that (Ψ′ΞΨ)−1 = (1−2γ)2+(2−2γ)2\n2 and Ψ ′Ξr = (1−2γ)r1+(2−2γ)r22 . Thus,\nthe weight of the BR approximation is wBR = (1−2γ)r1+(2−2γ)r2 (1−2γ)2+(2−2γ)2 .\nFor all these approximations, one can compute the squared error e with respect to the optimal solution v: For any weight w ∈ {wbest, wTD, wBR}, e(w) = ‖v − Φw‖2ξ = 12 (v(1) − w)2 + 12 (v(2) − 2w)2. In Figure 1, we plot the squared error ratios e(wTD)\ne(wbest) and\ne(wBR) e(wbest) on a log scale (they are by definition greater than 1) with respect to θ and γ. It turns out that these ratios do not depend on θ (instead of showing this through painful arithmetic manipulations, we will come back to this point and prove it later on). This Figure also displays the graph with respect to γ only. We can observe that for any choice of reward function and discount factor, the BR method returns a better value than the TD method. Also, when γ is in the neighborhood of 56 , the TD error ratio tends to ∞ while BR’s stays bounded. This Example shows that there exists MDPs where the BR is consistenly better\nthan the TD method, which can give an unbounded error. One should however not conclude too quickly that BR is always better than TD. The literature contains several arguments in favor of TD, one of which is considered in the following Example.\nExample 2 Sutton et al. (2009) recently described a 3-state MDP example where the TD method computes the best projection while BR does not. The idea behind this 3-state example can be described in a quite general way4: Suppose we have a k + l-state MDP, of which the Bellman Equation has a block triangular structure: v1 = γP1v1 + r1 / v2 = γP21v1 +P22v2 + r2 where v1 ∈ Rk and v2 ∈ Rl (the concatenation of the vectors v1 and v2 form the value function). Suppose also that the approximation subspace span (Φ) is Rk × S2 where S2 is a subspace of Rl. For the first component v1, the approximation space is the entire space Rk. With TD, we obtain the exact value for the k first components of the value, while with Bellman residual minimization, we do not: satisfying the first equation exactly is traded for decreasing the error in satisfying the second one (which also involves v1). In an optimal control context, the example above can have quite dramatic implications, as v1 can be related to the costs at some future states accessible from those states associated with v2, and the future costs are all that matters when making decisions.\nOverall, the two methods generate different types of biases, and distribute error in different manners. In order to gain some more insight, we now turn on to some analytical facts about them."
    }, {
      "heading" : "3. A Relation and Stability Issues",
      "text" : "Though several works have compared and considered both methods (Schoknecht, 2002; Lagoudakis & Parr, 2003; Munos, 2003; Yu & Bertsekas, 2008), the following simple fact has, to our knowledge, never been emphasized per se:\nProposition 1 The BR is an upper bound of the TD error, and more precisely:\n∀v̂ ∈ span (Φ) , EBR(v̂)2 = ETD(v̂)2 + ‖T v̂ −ΠT v̂‖2ξ.\nProof This simply follows from Pythagore, as ΠT v̂− T v̂ is orthogonal to span (Φ) and v̂ −ΠT v̂ belongs to span (Φ).\nThis implies that if one can make the BR small, then the TD Error will also be small. In the limit case where\n4The rest of this section is strongly inspired by a personal communication with Yu.\nTD or BR? The unified oblique projection view\none can make the BR equal to 0, then the TD Error is also 0.\nOne of the motivation for minimizing the BR is historically related to a well-known result of Williams & Baird (1993): ∀v̂, ‖v − v̂‖∞ ≤ 11−γ ‖T v̂ − v̂‖∞. Since one considers the weighted quadratic norm in practice5, the related result6 that really makes sense here is: ∀v̂, ‖v − v̂‖ξ ≤ √ C(ξ)\n1−γ ‖T v̂ − v̂‖ξ where C(ξ) := maxi,j\npij ξi\nis a “concentration coefficient”, that can be seen as some measure of the stochasticity of the MDP7. This result shows that it is sound to minimize the BR, since it controls (through a constant) the approximation error ‖v − v̂BR‖ξ. On the TD side, there does not exist any similar result. Actually, the fact that one can build examples (like Example 1) where the TD projection is numerically unstable implies that one cannot prove such a result. Proposition 1 allows to understand better the TD method: by minimizing the TD Error, one only minimizes one part of the BR, or equivalently this means that one does not care about the term ‖T v −ΠT v‖2ξ, which may be interpreted as a measure of adequacy of the projection Π with the Bellman operator T . In Example 1, the approximation error of the TD projection goes to infinity because this adequacy term diverges. In (Munos & Szepesvári, 2008), the authors use an algorithm based on the TD Error and make an assumption on this adequacy term (there called the inherent Bellman error of the approximation space), so that their algorithm can be proved convergent.\nA complementary view on the potential instability of TD, has been referred to as a norm incompatibility issue (Bertsekas & Tsitsiklis, 1996; Guestrin et al., 2001), and can be revisited through the notion of concentration coefficient. Stochastic matrices P statisfy ‖P‖∞ = 1, which makes the Bellman operator T γcontracting, and thus its fixed point is well-defined. The orthogonal projection with respect to ‖ · ‖ξ is such that ‖Π‖ξ = 1. Thus P and Π are of norm 1, but for different norms. Unfortunately, a general (tight) bound for linear projections is ‖Π‖∞ ≤\n5Mainly because it is computationnally easier than doing a max-norm minimization, see however (Guestrin et al., 2001) for an attempt of doing max-norm projection.\n6The proof is a consequence of Jensen’s inequality and the arguments are very close to the ones in (Munos, 2003).\n7If ξ is the uniform law, then there always exists such a C(ξ) ∈ (1, N) where one recalls that N is the size of the state space; in such a case, C(ξ) is minimal if all next-states are chosen with the uniform law, and maximal as soon as there exists a deterministic transition. See (Munos, 2003) for more discussion on this coefficient.\n1+ √ N\n2 (Thompson, 1996) and it can be shown 8 that\n‖P‖ξ ≤ √ C(ξ) (which can thus also be of the order\nof √ N). Consequently, ‖ΠP‖∞ and ‖ΠP‖ξ may be greater than 1, and thus the fixed point of the projected Bellman equation may not be well-defined. A known exception where the composition ΠP has norm 1, is when one can prove that ‖P‖ξ = 1 (as for instance when ξ is the stationary distribution of P ) and in this case we know from Bertsekas & Tsitsiklis (1996); Tsitsiklis & Van Roy (1997) that\n‖v − v̂TD‖ξ ≤ 1 √\n1− γ2 ‖v − v̂best‖ξ. (3)\nAnother notable such exception is when ‖Π‖max = 1, as in the so-called “averager” approximation (Gordon, 1995). However, in general, the stability of TD is difficult to guarantee."
    }, {
      "heading" : "4. The unified oblique projection view",
      "text" : "In the TD approach, we consider finding the fixed point of the composition of an orthogonal projection Π and the Bellman operator T . Suppose now we consider using a (non necessarily orthogonal) projection Π onto span (φ), that is any linear operator that satisfies Π2 = Π and whose range is span (Φ). In their most general form, such operators are called oblique projections and can be written ΠX = ΦπX with πX = (X\n′Φ)−1X ′. The parameter X specifies the projection direction: precisely, ΠX is the projection onto span (Φ) orthogonally to span (X). As for the orthogonal projections, the following relations πXΦ = I and πXΠX = πX hold. Recall that L = I − γP . We are ready to state the main result of this paper:\nProposition 2 Write XTD = ΞΦ and XBR = ΞLΦ. (1) The TD fix point computation and the BR minimization are solutions (respectively with X = XTD and X = XBR) of the projected equation v̂X = ΠXT v̂X . (2) When it exists, the solution of this projected equation is the projection of v onto span (Φ) orthogonally to span (L′X), i.e. formally v̂X = ΠL′X v.\nProof We begin by showing part (2). Writing v̂X = ΦwX , the fixed point equation is: ΦwX = ΠX(r + γPΦwx). Multiplying on both sides by πX , one obtains: wX = πX(r + γPΦwx) and therefore wX = (I − γπXPΦ)−1πXr. Using the definition of πX , one\n8One can prove that for all x, ‖Px‖2ξ ≤ ‖x‖ 2 ξP ≤ C(ξ)‖x‖2ξ . The argument for the first inequality involves Jensen’s inequality and is again close to what is done in (Munos, 2003).\nTD or BR? The unified oblique projection view\nobtains:\nwX = (I − γ(X ′Φ)−1X ′PΦ)−1(X ′Φ)−1X ′r = [ (X ′Φ)(I − γ(X ′Φ)−1X ′PΦ) ]−1 X ′r\n= (X ′(I − γP )Φ)−1X ′r (4) = (X ′LΦ)−1X ′Lv\n= πL′X v\nwhere we enventually used r = Lv.\nThe proof of part (1) now follows. The fact that TD is a special case with X = ΞΦ is trivial by construction since then ΠX is the orthogonal projection with respect to ‖ · ‖ξ. When X = ΞLΦ, one simply needs to observe from Equations 2 and 4 and the definition of Ψ = LΦ that wX = wBR.\nBeyond its nice and simple geometric flavour, a direct consequence of Proposition 2 is that it allows to derive tight error bounds for TD, BR, and any other method for general X . For any square matrix M , write σ(M) its spectral radius.\nProposition 3 For any choice of X, the approximation error satisfies:\n‖v − v̂X‖ξ ≤ ‖ΠL′X‖ξ‖v − v̂best‖ξ (5) = √ σ(ABCB′)‖v − v̂best‖ξ\nwhere A = Φ′ΞΦ, B = (X ′LΦ)−1 and C = XLΞ−1L′X are matrices of size m×m.\nThus, for any X , the amplification of the smallest error ‖v− v̂best‖ξ depends on the norm of the associated oblique projection, which can be estimated as the spectral radius of the product of small matrices. A simple corollary of this Proposition is the following: if the real value v belongs to the feature space span (Φ) (in such a case v = v̂best) then all oblique projection methods find it (v̂X = v).\nProof of Proposition 3 Proposition 2 implies that v− v̂X = (I −ΠL′X)v = (I −ΠL′X)(I −ΠΞΦ)v. where we used the fact that ΠL′XΠΞΦ = ΠΞΦ since ΠL′X and ΠΞΦ are projections onto span (Φ). Taking the norm, one obtains ‖v − v̂X‖ξ ≤ ‖I − ΠL′X‖ξ‖v − ΠΞΦv‖ξ = ‖ΠL′X‖ξ‖v − v̂best‖ξ where we used the definition of v̂best, and the fact that ‖I − ΠL′X‖ξ = ‖ΠL′X‖ξ since ΠL′X is a (non-trivial) projection (see e.g. (Szyld, 2006)). Thus Equation 5 holds.\nIn order to evaluate the norm in terms of small size matrices, one will use the following Lemma on the projection matrix ΠL′X = ΦπL′X :\nLemma 1 (Yu & Bertsekas (2008)) Let Y be an N ×m matrix, and Z a m×N matrix, then ‖Y Z‖2ξ = σ ( (Y ′ΞY )(ZΞ−1Z ′) ) .\nThus, ‖ΠL′X‖2ξ = ‖ΦπL′X‖2ξ = σ[(Φ′ΞΦ)(πL′XΞ−1(πL′X)′)] = σ[Φ′ΞΦ(X ′LΦ)−1X ′LΞ−1L′X(Φ′L′X)−1] = σ[ABCB′].\nProposition 2 is closely related to the work of (Schoknecht, 2002), in which the author derived the following characterization of the TD and BR solutions:\nProposition 4 (Schoknecht (2002)) The TD fix point computation and the BR minimization are orthogonal projections of the value v respectively induced by the seminorm ‖·‖QTD 9 with QTD = L′ΞΦΦ′ΞL and by the norm ‖ · ‖QBR with QBR = L′ΞL.\nThis “orthogonal projection” characterization and our “oblique projection” characterization are in fact equivalent. On the one hand for BR, it is immediate to notice that Π‖·‖QBR = ΠL′XBR . On the other hand for TD, writing Y = L′XTD, one simply needs to notice that ΠL′XTD = ΠY = Φ(Y\n′Φ)−1Y ′ = Φ(Y ′Φ)−1(Φ′Y )−1(Φ′Y )Y ′ = Φ(Φ′Y Y ′Φ)−1Φ′Y Y ′ = Π‖·‖QTD . The work of Schoknecht (2002) suggests that TD and BR are optimal for different criteria, since both look for some v̂ ∈ span (Φ) that minimizes ‖v̂ − v‖ for some (semi)norm ‖ · ‖. Curiously, our result suggests that neither is optimal, since neither uses the best projection direction X∗ := L′−1ΞΦ for which v̂X∗ = ΠL′X∗v = ΠΞΦv = v̂best and this supports the empirical evidence that there is no clear “winner” between TD and BR.\nOur main results, stated in Propositions 2 and 3, constitutes a revisit of the work of Yu & Bertsekas (2008), where the authors similarly derived error bounds for TD and BR. Our approach mimicks theirs: 1) we derive a linear relation between the projection v̂, the real value v and the best projection v̂best, then 2) analyze the norm of the matrices involved in this relation in terms of spectral radius of small matrices (through Lemma 1, which is taken from (Yu & Bertsekas, 2008)). From a purely quantitative point of view, our bounds are identical to the ones derived there. Two immediate consequences of this quantitative equivalence are that, as in (Yu & Bertsekas,\n9This is a seminorm because the matrix QTD is only semidefinite (since ΦΦ′ has rank smaller than m < N). The corresponding projection can still be well defined (i.e. each point has exactly one projection) provided that span (Φ) ∩ {x; ‖x‖QTD = 0} = {0}.\nTD or BR? The unified oblique projection view\n2008), (1) our bound is tight in the sense that there exists a worst choice for the reward for which it holds with equality, and (2) it is always better than that of Equation 3 from Bertsekas & Tsitsiklis (1996); Tsitsiklis & Van Roy (1997). However, our work is qualitatively different: by highlighting the oblique projection relation between v̂ and v, not only do we provide a clear geometric intuition for both methods, but we also greatly simplify the form of the results and their proofs (see (Yu & Bertsekas, 2008) for details).\nLast but not least, there is globally a significant difference between our work and the two works we have just mentionned. The analysis we propose is unified for TD and BR (and even extends to potential new methods through other choices of the parameter X), while the results in (Schoknecht, 2002) and (Yu & Bertsekas, 2008) are proved independently for each method. We hope that our unified approach will help understanding better the pros and cons of TD, BR, and related alternative approaches."
    }, {
      "heading" : "5. An Empirical Comparison",
      "text" : "In order to further compare the TD and the BR projections, we have made some empirical comparison, which we describe now. We consider spaces of dimensions n = 2, 3, .., 30. For each n, we consider projections of dimensions k = 1, 2, .., n. For each (n, k) couple, we generate 20 random projections (through random matrices10 Φ of size (n, k) and random weight vectors ξ) and 20 random (uncontrolled) chain like MDP: from each state i, there is a probability pi (chosen randomly uniformly on (0, 1)) to get to state i+ 1 and a probability 1 − pi to stay in i (the last state is absorbing);\n10Each entry is a random uniform number between -1 and 1.\nthe reward is a random vector. For the 20 × 20 resulting combinations, we compute the real value v, its exact projection v̂best, the TD fix point v̂TD, and the BR projection v̂BR. We then deduce the best error e = ‖v − v̂best‖ξ, the TD error eTD = ‖v − v̂TD‖ξ and the BR eBR = ‖v − v̂BR‖ξ. We also compute the bounds of Proposition 3 for both methods: bTD and bBR. Each such experiment is done for 4 different values of the discount factor γ: 0.9, 0.95, 0.99, 0.999.\nUsing this raw data on 20× 20 problems, we compute for each (n, k) couple some statistics, which we describe now. All the graphs that we display shows the dimension of the space N and of the projected space m on the x − y axes. The z axis correspond to the different statistics of interest.\nFigure 2 shows the proportion of sampled problems where TD method returns a better approximation than BR (i.e. the expectation of the indicator function of eTD < eBR). It turns out that this ratio is\nTD or BR? The unified oblique projection view\nγ = 0.9 E[TD_err/err]\n0 5 10\n15 20 25 30\nproj. space dim.\n0 5\n10 15\n20 25\n30\nspace dim.\n1\n10\n100\n1000\nγ = 0.9 E[BR_err/err]\n0 5 10\n15 20 25 30\nproj. space dim.\n0 5\n10 15\n20 25\n30\nspace dim.\n1\n10\n100\n1000\nγ = 0.95 E[TD_err/err]\n0 5 10\n15 20 25 30\nproj. space dim.\n0 5\n10 15\n20 25\n30\nspace dim.\n1\n10\n100\n1000\nγ = 0.95 E[BR_err/err]\n0 5 10\n15 20 25 30\nproj. space dim.\n0 5\n10 15\n20 25\n30\nspace dim.\n1\n10\n100\n1000\nγ = 0.99 E[TD_err/err]\n0 5 10\n15 20 25 30\nproj. space dim.\n0 5\n10 15\n20 25\n30\nspace dim.\n1\n10\n100\n1000\nγ = 0.99 E[BR_err/err]\n0 5 10\n15 20 25 30\nproj. space dim.\n0 5\n10 15\n20 25\n30\nspace dim.\n1\n10\n100\n1000\nγ = 0.999 E[TD_err/err]\n0 5 10\n15 20 25 30\nproj. space dim.\n0 5\n10 15\n20 25\n30\nspace dim.\n1\n10\n100\n1000\nγ = 0.999 E[BR_err/err]\n0 5 10\n15 20 25 30\nproj. space dim.\n0 5\n10 15\n20 25\n30\nspace dim.\n1\n10\n100\n1000\nFigure 5. (Left) Expectation of eTD/e and (Right) of eBR/e.\nconsistently greater than 12 , which means that the TD method is usually better than the BR method. Figure 3 presents the ratio of time the bounds we have presented in Propostion 4 correctly guesses which method is the best (i.e. the expectation of the indicator function of [eTD < eBR] = [bTD < bBR]). Unless the feature space dimension is close to the state space dimension, the bounds do not appear very useful for such a decision. Figure 4 displays the expectation of eTD/eBR. One can observe that, on average, this expectation is bigger than 1, that is the BR tends to be better, on average, than the TD error. This may look contradictory with our interpretation of Figure 2, but the explanation is the following: when the BR method is better than the TD method, it is by a larger gap than when it is the other way round. We believe this corresponds to the situation when the TD method in unstable. Figure 5 allows to confirm this point: it shows the expectation of the relative approximation errors with respect to the best possible error, that is the expectation of eTD/e and eBR/e. One observes on\nall charts that this average relative quality of the TD fix point has lots of pikes (corresponding to numerical instabilities), while that of the BR method is smooth."
    }, {
      "heading" : "6. Conclusion and Future Work",
      "text" : "We have presented the TD fix point and the BR minimization methods for approximating the value of some MDP fixed policy. We have described two original examples: in the former, the BR method is consistently better than the TD method, while the latter (which generalizes the spirit of the example of Sutton et al. (2009)) is best treated by TD. Proposition 1 highlights the close relation between the objective criteria that correspond to both methods. It shows that minimizing the BR implies minimizing the TD error and some extra “adequacy” term, which happens to be crucial for numerical stability.\nOur main contribution, stated in Proposition 2, provides a new viewpoint for comparing the two projection methods, and potential ideas for alternatives. Both TD and BR can be characterized as solving a projected fixed point equation and this is to our knowledge new for BR. Also, the solutions to both methods are some oblique projection of the value v and this is to our knowledge new for TD and BR. Eventually, this simple geometric characterization allows to derive some tight error bounds (Proposition 3). We have discussed the close relations of our results with those of Schoknecht (2002) and Yu & Bertsekas (2008), and argued that our work simplifies and extends them. Though apparently new to the Reinforcement Learning community, the very idea of oblique projections of fixed point equations has been studied in the Numerical Analysis community (see e.g. Saad (2003)). In the future, we plan to study more carefully this literature, and particularly investigate whether it may further contribute to the MDP context.\nConcerning the practical question of choosing among the two methods TD and BR, the situation can be summarized as follows: the BR method is sounder than the TD method, since the former has a performance guarantee while the latter will never have one in general. Extensive simulations (on random chainlike problems of size up to 30 states, and for many projection of all the possible space sizes) further suggest the following facts: (a) the TD solution is more often better than the BR solution; (b) however sometimes, TD failed dramatically; (c) overall, this makes BR better on average. Equivalently, one may say that TD is more risky than BR.\nEven if TD is more risky, there remains several reasons\nTD or BR? The unified oblique projection view\nwhy one may want to use it in practice, and which our study did not focus on. In large scale problems, one usually estimates the m × m linear systems through sampling. Sampling based methods for BR are more constraining since they generally require double sampling. Independently, the fact, highlighted by Propostion 1, that the BR is an upper bound of the TD error, suggests two things. First, we believe that the variance of the BR problem is higher than that of the TD problem; thus, given a fixed amount of samples, the TD solution might be less affected by the corresponding stochastic noise than the BR one. More generally, the BR problem may be harder to solve than the TD problem, and from a numerical viewpoint, the latter may provide better solutions. Eventually, we only discussed the TD(0) fix point method, that is the specific variant of TD(λ) (Bertsekas & Tsitsiklis, 1996; Boyan, 2002) where λ = 0. Values of λ > 0 solve some of the weaknesses of TD(0): it can be show that the stability issues disappear for values of λ close to 1, and the optimal projection v̂best is obtained when λ = 1. Further analytical and empirical comparisons of TD(λ) with the algorithms we have considered here (and with some “BR(λ)” algorithm) constitute future research.\nEventually, a somewhat disappointing observation of our study is that the bounds of Proposition 3, which are the tightest possible bounds independent of the reward function, did not prove useful for deciding a priori which of the two methods one should trust better (recall the results showed in Figure 3). Extending them in a way that would take the reward into account, as well as trying to exploit our original unified vision of the bounds (Propositions 2 and 3) are some potential tracks for improvement.\nAcknowlegments\nThe author would like to thank Janey Yu for helpful discussions, and the anonymous reviewers for providing comments that helped to improve the presentation of the paper."
    } ],
    "references" : [ {
      "title" : "Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path",
      "author" : [ "A. Antos", "C. Szepesvári", "R. Munos" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Antos et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Antos et al\\.",
      "year" : 2008
    }, {
      "title" : "Neurodynamic Programming",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "Bertsekas and Tsitsiklis,? \\Q1996\\E",
      "shortCiteRegEx" : "Bertsekas and Tsitsiklis",
      "year" : 1996
    }, {
      "title" : "Regularized policy iteration",
      "author" : [ "A.M. Farahmand", "M. Ghavamzadeh", "C. Szepesvári", "S. Mannor" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Farahmand et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Farahmand et al\\.",
      "year" : 2008
    }, {
      "title" : "Stable function approximation in dynamic programming",
      "author" : [ "G. Gordon" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Gordon,? \\Q1995\\E",
      "shortCiteRegEx" : "Gordon",
      "year" : 1995
    }, {
      "title" : "Max-norm projections for factored mdps",
      "author" : [ "C. Guestrin", "D. Koller", "R. Parr" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Guestrin et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Guestrin et al\\.",
      "year" : 2001
    }, {
      "title" : "Error bounds for approximate policy iteration",
      "author" : [ "R. Munos" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Munos,? \\Q2003\\E",
      "shortCiteRegEx" : "Munos",
      "year" : 2003
    }, {
      "title" : "Finite-time bounds for fitted value iteration",
      "author" : [ "R. Munos", "C. Szepesvári" ],
      "venue" : "JMLR, 9:815–857,",
      "citeRegEx" : "Munos and Szepesvári,? \\Q2008\\E",
      "shortCiteRegEx" : "Munos and Szepesvári",
      "year" : 2008
    }, {
      "title" : "Iterative Methods for Sparse Linear Systems, 2nd edition",
      "author" : [ "Y. Saad" ],
      "venue" : null,
      "citeRegEx" : "Saad,? \\Q2003\\E",
      "shortCiteRegEx" : "Saad",
      "year" : 2003
    }, {
      "title" : "Optimality of reinforcement learning algorithms with linear function approximation",
      "author" : [ "R. Schoknecht" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Schoknecht,? \\Q2002\\E",
      "shortCiteRegEx" : "Schoknecht",
      "year" : 2002
    }, {
      "title" : "Fast gradient-descent methods for temporaldifference learning with linear function approximation",
      "author" : [ "R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesvári", "E. Wiewiora" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Sutton et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2009
    }, {
      "title" : "The many proofs of an identity on the norm of oblique projections",
      "author" : [ "D.B. Szyld" ],
      "venue" : "Numerical Algorithms,",
      "citeRegEx" : "Szyld,? \\Q2006\\E",
      "shortCiteRegEx" : "Szyld",
      "year" : 2006
    }, {
      "title" : "Minkowski Geometry",
      "author" : [ "A.C. Thompson" ],
      "venue" : null,
      "citeRegEx" : "Thompson,? \\Q1996\\E",
      "shortCiteRegEx" : "Thompson",
      "year" : 1996
    }, {
      "title" : "An analysis of temporal-difference learning with function approximation",
      "author" : [ "J.N. Tsitsiklis", "B. Van Roy" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "Tsitsiklis and Roy,? \\Q1997\\E",
      "shortCiteRegEx" : "Tsitsiklis and Roy",
      "year" : 1997
    }, {
      "title" : "Tight performance bounds on greedy policies based on imperfect value functions",
      "author" : [ "R.J. Williams", "L.C. Baird" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Williams and Baird,? \\Q1993\\E",
      "shortCiteRegEx" : "Williams and Baird",
      "year" : 1993
    }, {
      "title" : "New error bounds for approximations from projected linear equations",
      "author" : [ "H. Yu", "D.P. Bertsekas" ],
      "venue" : "Technical Report C-2008-43,",
      "citeRegEx" : "Yu and Bertsekas,? \\Q2008\\E",
      "shortCiteRegEx" : "Yu and Bertsekas",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "We then propose a unified view in terms of oblique projections of the Bellman equation, which substantially simplifies and extends the characterization of Schoknecht (2002) and the recent analysis of Yu & Bertsekas (2008).",
      "startOffset" : 155,
      "endOffset" : 173
    }, {
      "referenceID" : 8,
      "context" : "We then propose a unified view in terms of oblique projections of the Bellman equation, which substantially simplifies and extends the characterization of Schoknecht (2002) and the recent analysis of Yu & Bertsekas (2008). Eventually, we describe some simulations that suggest that if the TD(0) solution is usually slightly better than the BR solution, its inherent numerical instability makes it very bad in some cases, and thus worse on average.",
      "startOffset" : 155,
      "endOffset" : 222
    }, {
      "referenceID" : 0,
      "context" : "We focus on two popular methods: the computation of the projected Temporal Difference fixed point (TD(0), TD for short), which Antos et al. (2008); Farahmand et al.",
      "startOffset" : 127,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "We focus on two popular methods: the computation of the projected Temporal Difference fixed point (TD(0), TD for short), which Antos et al. (2008); Farahmand et al. (2008); Sutton et al.",
      "startOffset" : 127,
      "endOffset" : 172
    }, {
      "referenceID" : 0,
      "context" : "We focus on two popular methods: the computation of the projected Temporal Difference fixed point (TD(0), TD for short), which Antos et al. (2008); Farahmand et al. (2008); Sutton et al. (2009) have recently presented as the minimization of the mean-square projected Bellman",
      "startOffset" : 127,
      "endOffset" : 194
    }, {
      "referenceID" : 8,
      "context" : "Section 4 contains the main contribution of this paper: we describe a unified view in terms of oblique projections of the Bellman equation, which simplifies and extends the characterization of Schoknecht (2002) and the recent analysis of Yu & Bertsekas (2008).",
      "startOffset" : 193,
      "endOffset" : 211
    }, {
      "referenceID" : 8,
      "context" : "Section 4 contains the main contribution of this paper: we describe a unified view in terms of oblique projections of the Bellman equation, which simplifies and extends the characterization of Schoknecht (2002) and the recent analysis of Yu & Bertsekas (2008). Eventually, Section 5 presents some simulations, that address the following practical questions: which of the method gives the best approximation? and how useful is our analysis for selecting it a priori?",
      "startOffset" : 193,
      "endOffset" : 260
    }, {
      "referenceID" : 0,
      "context" : "As pointed out by Antos et al. (2008); Farahmand et al.",
      "startOffset" : 18,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "As pointed out by Antos et al. (2008); Farahmand et al. (2008); Sutton et al.",
      "startOffset" : 18,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "As pointed out by Antos et al. (2008); Farahmand et al. (2008); Sutton et al. (2009), when the inverse exists, the above computation is equivalent to minimizing for v̂ ∈ span (Φ) the TD error ETD(v̂) := ‖v̂ −ΠT v̂‖ξ down to 0.",
      "startOffset" : 18,
      "endOffset" : 85
    }, {
      "referenceID" : 8,
      "context" : "Note that in this case, the above inverse always exists (Schoknecht, 2002).",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "This observation lead Sutton et al. (2009) to propose original off-policy gradient algorithms for computing the TD solution.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 9,
      "context" : "Example 2 Sutton et al. (2009) recently described a 3-state MDP example where the TD method computes the best projection while BR does not.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : "Though several works have compared and considered both methods (Schoknecht, 2002; Lagoudakis & Parr, 2003; Munos, 2003; Yu & Bertsekas, 2008), the following simple fact has, to our knowledge, never been emphasized per se:",
      "startOffset" : 63,
      "endOffset" : 141
    }, {
      "referenceID" : 5,
      "context" : "Though several works have compared and considered both methods (Schoknecht, 2002; Lagoudakis & Parr, 2003; Munos, 2003; Yu & Bertsekas, 2008), the following simple fact has, to our knowledge, never been emphasized per se:",
      "startOffset" : 63,
      "endOffset" : 141
    }, {
      "referenceID" : 4,
      "context" : "A complementary view on the potential instability of TD, has been referred to as a norm incompatibility issue (Bertsekas & Tsitsiklis, 1996; Guestrin et al., 2001), and can be revisited through the notion of concentration coefficient.",
      "startOffset" : 110,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "Mainly because it is computationnally easier than doing a max-norm minimization, see however (Guestrin et al., 2001) for an attempt of doing max-norm projection.",
      "startOffset" : 93,
      "endOffset" : 116
    }, {
      "referenceID" : 5,
      "context" : "The proof is a consequence of Jensen’s inequality and the arguments are very close to the ones in (Munos, 2003).",
      "startOffset" : 98,
      "endOffset" : 111
    }, {
      "referenceID" : 5,
      "context" : "See (Munos, 2003) for more discussion on this coefficient.",
      "startOffset" : 4,
      "endOffset" : 17
    }, {
      "referenceID" : 11,
      "context" : "1+ √ N 2 (Thompson, 1996) and it can be shown 8 that ‖P‖ξ ≤ √",
      "startOffset" : 9,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "Another notable such exception is when ‖Π‖max = 1, as in the so-called “averager” approximation (Gordon, 1995).",
      "startOffset" : 96,
      "endOffset" : 110
    }, {
      "referenceID" : 5,
      "context" : "The argument for the first inequality involves Jensen’s inequality and is again close to what is done in (Munos, 2003).",
      "startOffset" : 105,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : "(Szyld, 2006)).",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "Proposition 2 is closely related to the work of (Schoknecht, 2002), in which the author derived the following characterization of the TD and BR solutions:",
      "startOffset" : 48,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "Proposition 4 (Schoknecht (2002)) The TD fix point computation and the BR minimization are orthogonal projections of the value v respectively induced by the seminorm ‖·‖QTD 9 with QTD = L′ΞΦΦ′ΞL and by the norm ‖ · ‖QBR with QBR = L′ΞL.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : "The work of Schoknecht (2002) suggests that TD and BR are optimal for different criteria, since both look for some v̂ ∈ span (Φ) that minimizes ‖v̂ − v‖ for some (semi)norm ‖ · ‖.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 8,
      "context" : "The analysis we propose is unified for TD and BR (and even extends to potential new methods through other choices of the parameter X), while the results in (Schoknecht, 2002) and (Yu & Bertsekas, 2008) are proved independently for each method.",
      "startOffset" : 156,
      "endOffset" : 174
    }, {
      "referenceID" : 9,
      "context" : "We have described two original examples: in the former, the BR method is consistently better than the TD method, while the latter (which generalizes the spirit of the example of Sutton et al. (2009)) is best treated by TD.",
      "startOffset" : 178,
      "endOffset" : 199
    }, {
      "referenceID" : 7,
      "context" : "We have discussed the close relations of our results with those of Schoknecht (2002) and Yu & Bertsekas (2008), and argued that our work simplifies and extends them.",
      "startOffset" : 67,
      "endOffset" : 85
    }, {
      "referenceID" : 7,
      "context" : "We have discussed the close relations of our results with those of Schoknecht (2002) and Yu & Bertsekas (2008), and argued that our work simplifies and extends them.",
      "startOffset" : 67,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : "Saad (2003)).",
      "startOffset" : 0,
      "endOffset" : 12
    } ],
    "year" : 2009,
    "abstractText" : "We investigate projection methods, for evaluating a linear approximation of the value function of a policy in a Markov Decision Process context. We consider two popular approaches, the one-step Temporal Difference fix-point computation (TD(0)) and the Bellman Residual (BR) minimization. We describe examples, where each method outperforms the other. We highlight a simple relation between the objective function they minimize, and show that while BR enjoys a performance guarantee, TD(0) does not in general. We then propose a unified view in terms of oblique projections of the Bellman equation, which substantially simplifies and extends the characterization of Schoknecht (2002) and the recent analysis of Yu & Bertsekas (2008). Eventually, we describe some simulations that suggest that if the TD(0) solution is usually slightly better than the BR solution, its inherent numerical instability makes it very bad in some cases, and thus worse on average.",
    "creator" : "gnuplot 4.2 patchlevel 5 "
  }
}