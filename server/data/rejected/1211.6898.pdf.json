{
  "name" : "1211.6898.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes",
    "authors" : [ "Bruno Scherrer", "Boris Lesner" ],
    "emails" : [ "bruno.scherrer@inria.fr", "boris.lesner@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n21 1.\n68 98\nv1 [\ncs .L\nG ]\n2 9\nN ov"
    }, {
      "heading" : "1 Introduction",
      "text" : "Given an infinite-horizon stationary γ-discounted Markov Decision Process [24, 4], we consider approximate versions of the standard Dynamic Programming algorithms, Policy and Value Iteration, that build sequences of value functions vk and policies πk as follows\nApproximate Value Iteration (AVI): vk+1 ← Tvk + ǫk+1 (1)\nApproximate Policy Iteration (API):\n{\nvk ← vπk + ǫk πk+1 ← any element of G(vk)\n(2)\nwhere v0 and π0 are arbitrary, T is the Bellman optimality operator, vπk is the value of policy πk and G(vk) is the set of policies that are greedy with respect to vk. At each iteration k, the term ǫk accounts for a possible approximation of the Bellman operator (for AVI) or the evaluation of vπk (for API). Throughout the paper, we will assume that error terms ǫk satisfy for all k, ‖ǫk‖∞ ≤ ǫ for some ǫ ≥ 0. Under this assumption, it is well-known that both algorithms share the following performance bound (see [25, 11, 4] for AVI and [4] for API):\nTheorem 1. For API (resp. AVI), the loss due to running policy πk (resp. any policy πk in G(vk−1)) instead of the optimal policy π∗ satisfies\nlim sup k→∞\n‖v∗ − vπk‖∞ ≤ 2γ\n(1− γ)2 ǫ.\nThe constant 2γ(1−γ)2 can be very big, in particular when γ is close to 1, and consequently the above bound is commonly believed to be conservative for practical applications. Interestingly, this very constant 2γ(1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved. Indeed, the bound (and the 2γ(1−γ)2 constant) are tight for API [4, Example 6.4], and we will show in Section 3 – to our knowledge, this has never been argued in the literature – that it is also tight for AVI.\nEven though the theory of optimal control states that there exists a stationary policy that is optimal, the main contribution of our paper is to show that looking for a non-stationary policy (instead of a stationary one) may lead to a much better performance bound. In Section 4, we will show how to deduce such a non-stationary policy from a run of AVI. In Section 5, we will describe two original policy iteration variations that compute non-stationary policies. For all these algorithms, we will prove that we have a performance bound that can be reduced down to 2γ1−γ ǫ. This is a factor 1 1−γ better than the standard bound of Theorem 1, which is significant when γ is close to 1. Surprisingly, this will show that the problem of “computing near-optimal non-stationary policies” is much simpler than that of “computing near-optimal stationary policies”. Before we present these contributions, the next section begins by precisely describing our setting."
    }, {
      "heading" : "2 Background",
      "text" : "We consider an infinite-horizon discounted Markov Decision Process [24, 4] (S,A, P, r, γ), whereS is a possibly infinite state space, A is a finite action space, P (ds′|s, a), for all (s, a), is a probability kernel on S, r : S × A → R is a reward function bounded in max-norm by Rmax, and γ ∈ (0, 1) is a discount factor. A stationary deterministic policy π : S → A maps states to actions. We write rπ(s) = r(s, π(s)) and Pπ(ds′|s) = P (ds′|s, π(s)) for the immediate reward and the stochastic kernel associated to policy π. The value vπ of a policy π is a function mapping states to the expected discounted sum of rewards received when following π from any state: for all s ∈ S,\nvπ(s) = E\n[\n∞ ∑\nt=0\nγtrπ(st)\n∣ ∣ ∣ ∣ ∣ s0 = s, st+1 ∼ Pπ(·|st) ] .\nThe value vπ is clearly bounded by Vmax = Rmax/(1 − γ). It is well-known that vπ can be characterized as the unique fixed point of the linear Bellman operator associated to a policy π: Tπ : v 7→ rπ + γPπv. Similarly, the Bellman optimality operator T : v 7→ maxπ Tπv has as unique fixed point the optimal value v∗ = maxπ vπ. A policy π is greedy w.r.t. a value function v if Tπv = Tv, the set of such greedy policies is written G(v). Finally, a policy π∗ is optimal, with value vπ∗ = v∗, iff π∗ ∈ G(v∗), or equivalently Tπ∗v∗ = v∗.\nThough it is known [24, 4] that there always exists a deterministic stationary policy that is optimal, we will, in this article, consider non-stationary policies and now introduce related notations. Given a sequence π1, π2, . . . , πk of k stationary policies (this sequence will be clear in the context we describe later), and for any 1 ≤ m ≤ k, we will denote πk,m the periodic non-stationary policy that takes the first action according to πk, the second according to πk−1, . . . , the mth according to πk−m+1 and then starts again. Formally, this can be written as\nπk,m = πk πk−1 · · · πk−m+1 πk πk−1 · · ·πk−m+1 · · ·\nIt is straightforward to show that the value vπk,m of this periodic non-stationary policy πk,m is the unique fixed point of the following operator:\nTk,m = Tπk Tπk−1 · · · Tπk−m+1 .\nFinally, it will be convenient to introduce the following discounted kernel:\nΓk,m = (γPπk)(γPπk−1) · · · (γPπk−m+1).\nIn particular, for any pair of values v and v′, it can easily be seen that Tk,mv−Tk,mv′ = Γk,m(v−v′)."
    }, {
      "heading" : "3 Tightness of the performance bound of Theorem 1",
      "text" : "The bound of Theorem 1 is tight for API in the sense that there exists an MDP [4, Example 6.4] for which the bound is reached. To the best of our knowledge, a similar argument has never been provided for AVI in the literature. It turns out that the MDP that is used for showing the tightness for API also applies to AVI. This is what we show in this section.\nExample 1. Consider the γ-discounted deterministic MDP from [4, Example 6.4] depicted on Figure 1. It involves states 1, 2, . . . . In state 1 there is only one self-loop action with zero reward, for each state i > 1 there are two possible choices: either move to state i − 1 with zero reward or stay\nwith reward ri = −2 γ−γi\n1−γ ǫ with ǫ ≥ 0. Clearly the optimal policy in all states i > 1 is to move to i− 1 and the optimal value function v∗ is 0 in all states.\nStarting with v0 = v∗, we are going to show that for all iterations k ≥ 1 it is possible to have a policy πk+1 ∈ G(vk) which moves in every state but k + 1 and thus is such that vπk+1(k + 1) = rk+1 1−γ = −2 γ−γk+1 (1−γ)2 ǫ, which meets the bound of Theorem 1 when k tends to infinity.\nTo do so, we assume that the following approximation errors are made at each iteration k > 0:\nǫk(i) =\n{\n−ǫ if i = k ǫ if i = k + 1 0 otherwise .\nWith this error, we are now going to prove by induction on k that for all k ≥ 1,\nvk(i) =\n\n \n  −γk−1ǫ if i < k rk/2− ǫ if i = k −(rk/2− ǫ) if i = k + 1 0 otherwise .\nSince v0 = 0 the best action is clearly to move in every state i ≥ 2 which gives v1 = v0 + ǫ1 = ǫ1 which establishes the claim for k = 1.\nAssuming that our induction claim holds for k, we now show that it also holds for k + 1.\nFor the move action, write qmk its action-value function. For all i > 1 we have q m k (i) = 0+ γvk(i− 1), hence\nqmk (i) =\n\n \n  γ(−γk−1ǫ) = −γkǫ if i = 2, . . . , k γ(rk/2− ǫ) = rk+1/2 if i = k + 1 −γ(rk/2− ǫ) = −rk+1/2 if i = k + 2 0 otherwise .\nFor the stay action, write qsk its action-value function. For all i > 0 we have q s k(i) = ri + γvk(i), hence\nqsk(i) =\n\n   \n   \nri + γ(−γ k−1ǫ) = ri − γ kǫ if i = 1, . . . , k − 1 rk + γ(rk/2− ǫ) = rk + rk+1/2 if i = k rk+1 − rk+1/2 = rk+1/2 if i = k + 1 rk+2 + γ0 = rk+2 if i = k + 2 0 otherwise .\nFirst, only the stay action is available in state 1, hence, since r0 = 0 and ǫk+1(1) = 0, we have vk+1(1) = q s k(1) + ǫk+1(1) = −γ\nkǫ, as desired. Second, since ri < 0 for all i > 1 we have qmk (i) > q s k(i) for all these states but k+1 where q m k (k+1) = q s k(k+1) = rk+1/2. Using the fact that vk+1 = max(qmk , q s k) + ǫk+1 gives the result for vk+1.\nThe fact that for i > 1 we have qmk (i) ≥ q s k(i) with equality only at i = k+1 implies that there exists a policy πk+1 greedy for vk which takes the optimal move action in all states but k + 1 where the stay action has the same value, leaving the algorithm the possibility of choosing the suboptimal stay action in this state, yielding a value vπk+1(k + 1), matching the upper bound as k goes to infinity.\nSince Example 1 shows that the bound of Theorem 1 is tight, improving performance bounds imply to modify the algorithms. The following sections of the paper shows that considering non-stationary policies instead of stationary policies is an interesting path to follow."
    }, {
      "heading" : "4 Deducing a non-stationary policy from AVI",
      "text" : "While AVI (Equation (1)) is usually considered as generating a sequence of values v0, v1, . . . , vk−1, it also implicitely produces a sequence1 of policies π1, π2, . . . , πk, where for i = 0, . . . , k − 1, πi+1 ∈ G(vi). Instead of outputing only the last policy πk, we here simply propose to output the periodic non-stationary policy πk,m that loops over the last m generated policies. The following theorem shows that it is indeed a good idea.\nTheorem 2. For all iteration k and m such that 1 ≤ m ≤ k, the loss of running the non-stationary policy πk,m instead of the optimal policy π∗ satisfies:\n‖v∗ − vπk,m‖∞ ≤ 2\n1− γm\n(\nγ − γk\n1− γ ǫ+ γk‖v∗ − v0‖∞\n)\n.\nWhen m = 1 and k tends to infinity, one exactly recovers the result of Theorem 1. For general m, this new bound is a factor 1−γ m\n1−γ better than the standard bound of Theorem 1. The choice that optimizes the bound, m = k, and which consists in looping over all the policies generated from the very start, leads to the following bound:\n‖v∗ − vπk,k‖∞ ≤ 2\n(\nγ\n1− γ −\nγk\n1− γk\n)\nǫ+ 2γk\n1− γk ‖v∗ − v0‖∞,\nthat tends to 2γ1−γ ǫ when k tends to ∞.\nThe rest of the section is devoted to the proof of Theorem 2. An important step of our proof lies in the following lemma, that implies that for sufficiently big m, vk = Tvk−1 + ǫk is a rather good approximation (of the order ǫ1−γ ) of the value vπk,m of the non-stationary policy πk,m (whereas in general, it is a much poorer approximation of the value vπk of the last stationary policy πk).\nLemma 1. For all m and k such that 1 ≤ m ≤ k,\n‖Tvk−1 − vπk,m‖∞ ≤ γ m‖vk−m − vπk,m‖∞ +\nγ − γm\n1− γ ǫ.\nProof of Lemma 1. The value of πk,m satisfies:\nvπk,m = TπkTπk−1 · · ·Tπk−m+1vπk,m . (3)\nBy induction, it can be shown that the sequence of values generated by AVI satisfies:\nTπkvk−1 = TπkTπk−1 · · ·Tπk−m+1vk−m +\nm−1 ∑\ni=1\nΓk,iǫk−i. (4)\nBy substracting Equations (4) and (3), one obtains:\nTvk−1 − vπk,m = Tπkvk−1 − vπk,m = Γk,m(vk−m − vπk,m) +\nm−1 ∑\ni=1\nΓk,iǫk−i\nand the result follows by taking the norm and using the fact that for all i, ‖Γk,i‖∞ = γi.\nWe are now ready to prove the main result of this section.\nProof of Theorem 2. Using the fact that T is a contraction in max-norm, we have:\n‖v∗ − vk‖∞ = ‖v∗ − Tvk−1 + ǫk‖∞ ≤ ‖Tv∗ − Tvk−1‖∞ + ǫ\n≤ γ‖v∗ − vk−1‖∞ + ǫ.\n1A given sequence of value functions may induce many sequences of policies since more than one greedy policy may exist for one particular value function. Our results holds for all such possible choices of greedy policies.\nThen, by induction on k, we have that for all k ≥ 1,\n‖v∗ − vk‖∞ ≤ γ k‖v∗ − v0‖∞ +\n1− γk\n1− γ ǫ. (5)\nUsing Lemma 1 and Equation (5) twice, we can conclude by observing that\n‖v∗ − vπk,m‖∞ ≤ ‖Tv∗ − Tvk−1‖∞ + ‖Tvk−1 − vπk,m‖∞\n≤ γ‖v∗ − vk−1‖∞ + γ m‖vk−m − vπk,m‖∞ +\nγ − γm\n1− γ ǫ\n≤ γ\n(\nγk−1‖v∗ − v0‖∞ + 1− γk−1\n1− γ ǫ\n)\n+ γm ( ‖vk−m − v∗‖∞ + ‖v∗ − vπk,m‖∞ )\n+ γ − γm\n1− γ ǫ\n≤ γk‖v∗ − v0‖∞ + γ − γk\n1− γ ǫ\n+ γm ( γk−m‖v∗ − v0‖∞ + 1− γk−m\n1− γ ǫ + ‖v∗ − vπk,m‖∞\n)\n+ γ − γm\n1− γ ǫ\n= γm‖v∗ − vπk,m‖∞ + 2γ k‖v∗ − v0‖∞ +\n2(γ − γk)\n1− γ ǫ\n≤ 2\n1− γm\n(\nγ − γk\n1− γ ǫ+ γk‖v∗ − v0‖∞\n)\n."
    }, {
      "heading" : "5 API algorithms for computing non-stationary policies",
      "text" : "We now present similar results that have a Policy Iteration flavour. Unlike in the previous section where only the output of AVI needed to be changed, improving the bound for an API-like algorithm is slightly more involved. In this section, we describe and analyze two API algorithms that output non-stationary policies with improved performance bounds.\nAPI with a non-stationary policy of growing period Following our findings on non-stationary policies AVI, we consider the following variation of API, where at each iteration, instead of computing the value of the last stationary policy πk, we compute that of the periodic non-stationary policy πk,k that loops over all the policies π1, . . . , πk generated from the very start:\nvk ← vπk,k + ǫk\nπk+1 ← any element of G(vk)\nwhere the initial (stationary) policy π1,1 is chosen arbitrarily. Thus, iteration after iteration, the nonstationary policy πk,k is made of more and more stationary policies, and this is why we refer to it as having a growing period. We can prove the following performance bound for this algorithm:\nTheorem 3. After k iterations, the loss of running the non-stationary policy πk,k instead of the optimal policy π∗ satisfies:\n‖v∗ − vπk,k‖∞ ≤ 2(γ − γk)\n1− γ ǫ+ γk−1‖v∗ − vπ1,1‖∞ + 2(k − 1)γ kVmax.\nWhen k tends to infinity, this bound tends to 2γ1−γ ǫ, and is thus again a factor 1\n1−γ better than the original API bound.\nProof of Theorem 3. Using the facts that Tk+1,k+1vπk,k = Tπk+1Tk,kvπk,k = Tπk+1vπk,k and Tπk+1vk ≥ Tπ∗vk (since πk+1 ∈ G(vk)), we have:\nv∗ − vπk+1,k+1\n= Tπ∗v∗ − Tk+1,k+1vπk+1,k+1\n= Tπ∗v∗ − Tπ∗vπk,k + Tπ∗vπk,k − Tk+1,k+1vπk,k + Tk+1,k+1vπk,k − Tk+1,k+1vπk+1,k+1\n= γPπ∗(v∗ − vπk,k) + Tπ∗vπk,k − Tπk+1vπk,k + Γk+1,k+1(vπk,k − vπk+1,k+1)\n= γPπ∗(v∗ − vπk,k) + Tπ∗vk − Tπk+1vk + γ(Pπk+1 − Pπ∗)ǫk + Γk+1,k+1(vπk,k − vπk+1,k+1)\n≤ γPπ∗(v∗ − vπk,k) + γ(Pπk+1 − Pπ∗)ǫk + Γk+1,k+1(vπk,k − vπk+1,k+1).\nBy taking the norm, and using the facts that ‖vπk,k‖∞ ≤ Vmax, ‖vπk+1,k+1‖∞ ≤ Vmax, and ‖Γk+1,k+1‖∞ = γ k+1, we get:\n‖v∗ − vπk+1,k+1‖∞ ≤ γ‖v∗ − vπk,k‖∞ + 2γǫ+ 2γ k+1Vmax.\nFinally, by induction on k, we obtain:\n‖v∗ − vπk,k‖∞ ≤ 2(γ − γk)\n1− γ ǫ+ γk−1‖v∗ − vπ1,1‖∞ + 2(k − 1)γ kVmax.\nThough it has an improved asymptotic performance bound, the API algorithm we have just described has two (related) drawbacks: 1) its finite iteration bound has a somewhat unsatisfactory term of the form 2(k − 1)γkVmax, and 2) even when there is no error (when ǫ = 0), we cannot guarantee that, similarly to standard Policy Iteration, it generates a sequence of policies of increasing values (it is easy to see that in general, we do not have vπk+1,k+1 ≥ vπk,k ). These two points motivate the introduction of another API algorithm.\nAPI with a non-stationary policy of fixed period We consider now another variation of API parameterized by m ≥ 1, that iterates as follows for k ≥ m:\nvk ← vπk,m + ǫk\nπk+1 ← any element of G(vk)\nwhere the initial non-stationary policy πm,m is built from a sequence of m arbitrary stationary policies π1, π2, · · · , πm. Unlike the previous API algorithm, the non-stationary policy πk,m here only involves the last m greedy stationary policies instead of all of them, and is thus of fixed period. This is a strict generalization of the standard API algorithm, with which it coincides when m = 1. For this algorithm, we can prove the following performance bound: Theorem 4. For all m, for all k ≥ m, the loss of running the non-stationary policy πk,m instead of the optimal policy π∗ satisfies:\n‖v∗ − vπk,m‖∞ ≤ γ k−m‖v∗ − vπm,m‖∞ +\n2(γ − γk+1−m)\n(1− γ)(1− γm) ǫ.\nWhen m = 1 and k tends to infinity, we recover exactly the bound of Theorem 1. When m > 1 and k tends to infinity, this bound coincides with that of Theorem 2 for our non-stationary version of AVI: it is a factor 1−γ m\n1−γ better than the standard bound of Theorem 1.\nThe rest of this section develops the proof of this performance bound. A central argument of our proof is the following lemma, which shows that similarly to the standard API, our new algorithm has an (approximate) policy improvement property. Lemma 2. At each iteration of the algorithm, the value vπk+1,m of the non-stationary policy\nπk+1,m = πk+1 πk . . . πk+2−m πk+1 πk . . . πk−m+2 . . .\ncannot be much worse than the value vπ′ k,m of the non-stationary policy\nπ′k,m = πk−m+1 πk . . . πk+2−m πk−m+1 πk . . . πk−m+2 . . .\nin the precise following sense:\nvπk+1,m ≥ vπ′k,m − 2γ\n1− γm ǫ.\nThe policy π′k,m differs from πk+1,m in that every m steps, it chooses the oldest policy πk−m+1 instead of the newest one πk+1. Also π′k,m is related to πk,m as follows: π ′\nk,m takes the first action according to πk−m+1 and then runs πk,m; equivalently, since πk,m loops over πkπk−1 . . . πk−m+1, π′k,m = πk−m+1πk,m can be seen as a 1-step right rotation of πk,m. When there is no error (when ǫ = 0), this shows that the new policy πk+1,m is better than a “rotation” of πk,m. When m = 1, πk+1,m = πk+1 and π′k,m = πk and we thus recover the well-known (approximate) policy improvement theorem for standard API (see for instance [4, Lemma 6.1]).\nProof of Lemma 2. Since π′k,m takes the first action with respect to πk−m+1 and then runs πk,m, we have vπ′\nk,m = Tπk−m+1vπk,m . Now, since πk+1 ∈ G(vk), we have Tπk+1vk ≥ Tπk−m+1vk and\nvπ′ k,m − vπk+1,m = Tπk−m+1vπk,m − vπk+1,m\n= Tπk−m+1vk − γPπk−m+1ǫk − vπk+1,m\n≤ Tπk+1vk − γPπk−m+1ǫk − vπk+1,m\n= Tπk+1vπk,m + γ(Pπk+1 − Pπk−m+1)ǫk − vπk+1,m\n= Tπk+1Tk,mvπk,m − Tk+1,mvπk+1,m + γ(Pπk+1 − Pπk−m+1)ǫk\n= Tk+1,mTπk−m+1vπk,m − Tk+1,mvπk+1,m + γ(Pπk+1 − Pπk−m+1)ǫk\n= Γk+1,m(Tπk−m+1vπk,m − vπk+1,m) + γ(Pπk+1 − Pπk−m+1)ǫk\n= Γk+1,m(vπ′ k,m − vπk+1,m) + γ(Pπk+1 − Pπk−m+1)ǫk.\nfrom which we deduce that:\nvπ′ k,m − vπk+1,m ≤ (I − Γk+1,m) −1γ(Pπk+1 − Pπk−m+1)ǫk\nand the result follows by using the facts that ‖ǫk‖∞ ≤ ǫ and ‖(I − Γk+1,m)−1‖∞ = 11−γm .\nWe are now ready to prove the main result of this section.\nProof of Theorem 4. Using the facts that 1) Tk+1,m+1vπk,m = Tπk+1Tk,mvπk,m = Tπk+1vπk,m and 2) Tπk+1vk ≥ Tπ∗vk (since πk+1 ∈ G(vk)), we have for k ≥ m,\nv∗ − vπk+1,m\n= Tπ∗v∗ − Tk+1,mvπk+1,m\n= Tπ∗v∗ − Tπ∗vπk,m + Tπ∗vπk,m − Tk+1,m+1vπk,m + Tk+1,m+1vπk,m − Tk+1,mvπk+1,m\n= γPπ∗(v∗ − vπk,m) + Tπ∗vπk,m − Tπk+1vπk,m + Γk+1,m(Tπk−m+1vπk,m − vπk+1,m)\n≤ γPπ∗(v∗ − vπk,m) + Tπ∗vk − Tπk+1vk + γ(Pπk+1 − Pπ∗)ǫk + Γk+1,m(Tπk−m+1vπk,m − vπk+1,m)\n≤ γPπ∗(v∗ − vπk,m) + γ(Pπk+1 − Pπ∗)ǫk + Γk+1,m(Tπk−m+1vπk,m − vπk+1,m). (6)\nConsider the policy π′k,m defined in Lemma 2. Observing as in the beginning of the proof of Lemma 2 that Tπk−m+1vπk,m = vπ′k,m , Equation (6) can be rewritten as follows:\nv∗ − vπk+1,m ≤ γPπ∗(v∗ − vπk,m) + γ(Pπk+1 − Pπ∗)ǫk + Γk+1,m(vπ′k,m − vπk+1,m).\nBy using the facts that v∗ ≥ vπk,m , v∗ ≥ vπk+1,m and Lemma 2, we get\n‖v∗ − vπk+1,m‖∞ ≤ γ‖v∗ − vπk,m‖∞ + 2γǫ+ γm(2γǫ)\n1− γm\n= γ‖v∗ − vπk,m‖∞ + 2γ\n1− γm ǫ.\nFinally, we obtain by induction that for all k ≥ m,\n‖v∗ − vπk,m‖∞ ≤ γ k−m‖v∗ − vπm,m‖∞ +\n2(γ − γk+1−m)\n(1− γ)(1− γm) ǫ."
    }, {
      "heading" : "6 Discussion, conclusion and future work",
      "text" : "We recalled in Theorem 1 the standard performance bound when computing an approximately optimal stationary policy with the standard AVI and API algorithms. After arguing that this bound is tight – in particular by providing an original argument for AVI – we proposed three new dynamic programming algorithms (one based on AVI and two on API) that output non-stationary policies for which the performance bound can be significantly reduced (by a factor 11−γ ).\nFrom a bibliographical point of view, it is the work of [14] that made us think that non-stationary policies may lead to better performance bounds. In that work, the author considers problems with a finite-horizon T for which one computes non-stationary policies with performance bounds in O(T ǫ), and infinite-horizon problems for which one computes stationary policies with performance bounds in O( ǫ(1−γ)2 ). Using the informal equivalence of the horizons T ≃ 1 1−γ one sees that non-stationary policies look better than stationary policies. In [14], non-stationary policies are only computed in the context of finite-horizon (and thus non-stationary) problems; the fact that nonstationary policies can also be useful in an infinite-horizon stationary context is to our knowledge completely new.\nThe best performance improvements are obtained when our algorithms consider periodic nonstationary policies of which the period grows to infinity, and thus require an infinite memory, which may look like a practical limitation. However, in two of the proposed algorithm, a parameter m allows to make a trade-off between the quality of approximation 2γ(1−γm)(1−γ)ǫ and the amount of memory O(m) required. In practice, it is easy to see that by choosing m = ⌈\n1 1−γ\n⌉\n, that is a\nmemory that scales linearly with the horizon (and thus the difficulty) of the problem, one can get a performance bound of2 2γ(1−e−1)(1−γ)ǫ ≤ 3.164γ 1−γ ǫ.\nWe conjecture that our asymptotic bound of 2γ1−γ ǫ, and the non-asymptotic bounds of Theorems 2 and 4 are tight. The actual proof of this conjecture is left for future work. Important recent works of the literature involve studying performance bounds when the errors are controlled in Lp norms instead of max-norm [19, 20, 21, 1, 8, 18, 17] which is natural when supervised learning algorithms are used to approximate the evaluation steps of AVI and API. Since our proof are based on componentwise bounds like those of the pioneer works in this topic [19, 20], we believe that the extension of our analysis to Lp norm analysis is straightforward. Last but not least, an important research direction that we plan to follow consists in revisiting the many implementations of AVI and API for building stationary policies (see the list in the introduction), turn them into algorithms that look for non-stationary policies and study them precisely analytically as well as empirically."
    } ],
    "references" : [ {
      "title" : "Cs",
      "author" : [ "A. Antos" ],
      "venue" : "Szepesvári, and R. Munos. Learning near-optimal policies with Bellmanresidual minimization based fitted policy iteration and a single sample path. Machine Learning, 71(1):89–129",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Dynamic Policy Programming with Function Approximation",
      "author" : [ "M. Gheshlaghi Azar", "V. Gmez", "H.J. Kappen" ],
      "venue" : "14th International Conference on Artificial Intelligence and Statistics (AISTATS), volume 15, Fort Lauderdale, FL, USA",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Approximate policy iteration: a survey and some new methods",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Journal of Control Theory and Applications, 9:310–335",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Neuro-Dynamic Programming",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Athena Scientific",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Leastsquares methods for Policy Iteration",
      "author" : [ "L. Busoniu", "A. Lazaric", "M. Ghavamzadeh", "R. Munos", "R. Babuska", "B. De Schutter" ],
      "venue" : "M. Wiering and M. van Otterlo, editors, Reinforcement Learning: State of the Art. Springer",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Tree-based batch mode reinforcement learning",
      "author" : [ "D. Ernst", "P. Geurts", "L. Wehenkel" ],
      "venue" : "Journal of Machine Learning Research (JMLR), 6",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Planning in pomdps using multiplicity automata",
      "author" : [ "E. Even-dar" ],
      "venue" : "Uncertainty in Artificial Intelligence (UAI, pages 185–192",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Cs",
      "author" : [ "A.M. Farahmand", "M. Ghavamzadeh" ],
      "venue" : "Szepesvári, and S. Mannor. Regularized policy iteration. Advances in Neural Information Processing Systems, 21:441–448",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Error propagation for approximate policy and value iteration (extended version)",
      "author" : [ "A.M. Farahmand", "R. Munos", "Cs. Szepesvári" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Classification-based Policy Iteration with a Critic",
      "author" : [ "V. Gabillon", "A. Lazaric", "M. Ghavamzadeh", "B. Scherrer" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2011
    }, {
      "title" : "Stable Function Approximation in Dynamic Programming",
      "author" : [ "G.J. Gordon" ],
      "venue" : "ICML, pages 261–268",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Max-norm projections for factored MDPs",
      "author" : [ "C. Guestrin", "D. Koller", "R. Parr" ],
      "venue" : "International Joint Conference on Artificial Intelligence, volume 17-1, pages 673–682",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Efficient Solution Algorithms for Factored MDPs",
      "author" : [ "C. Guestrin", "D. Koller", "R. Parr", "S. Venkataraman" ],
      "venue" : "Journal of Artificial Intelligence Research (JAIR), 19:399–468",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "On the Sample Complexity of Reinforcement Learning",
      "author" : [ "S.M. Kakade" ],
      "venue" : "PhD thesis, University College London",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Approximately Optimal Approximate Reinforcement Learning",
      "author" : [ "S.M. Kakade", "J. Langford" ],
      "venue" : "International Conference on Machine Learning (ICML), pages 267–274",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Least-squares policy iteration",
      "author" : [ "M.G. Lagoudakis", "R. Parr" ],
      "venue" : "Journal of Machine Learning Research (JMLR), 4:1107–1149",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Finite-Sample Analysis of Least-Squares Policy Iteration",
      "author" : [ "A. Lazaric", "M. Ghavamzadeh", "R. Munos" ],
      "venue" : "To appear in Journal of Machine learning Research (JMLR)",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Finite Sample Analysis of Bellman Residual Minimization",
      "author" : [ "O.A. Maillard", "R. Munos", "A. Lazaric", "M. Ghavamzadeh" ],
      "venue" : "Masashi Sugiyama and Qiang Yang, editors, Asian Conference on Machine Learpning. JMLR: Workshop and Conference Proceedings, volume 13, pages 309– 324",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Error Bounds for Approximate Policy Iteration",
      "author" : [ "R. Munos" ],
      "venue" : "International Conference on Machine Learning (ICML), pages 560–567",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Performance Bounds in Lp norm for Approximate Value Iteration",
      "author" : [ "R. Munos" ],
      "venue" : "SIAM J. Control and Optimization",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Finite time bounds for sampling based fitted value iteration",
      "author" : [ "R. Munos", "Cs. Szepesvári" ],
      "venue" : "Journal of Machine Learning Research (JMLR),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Biasing Approximate Dynamic Programming with a Lower Discount Factor",
      "author" : [ "M. Petrik", "B. Scherrer" ],
      "venue" : "Twenty-Second Annual Conference on Neural Information Processing Systems -NIPS 2008, Vancouver, Canada",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Point-based value iteration: An anytime algorithm for POMDPs",
      "author" : [ "J. Pineau", "G.J. Gordon", "S. Thrun" ],
      "venue" : "International Joint Conference on Artificial Intelligence, volume 18, pages 1025–1032",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Markov Decision Processes",
      "author" : [ "M. Puterman" ],
      "venue" : "Wiley, New York",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "An Upper Bound on the Loss from Approximate Optimal-Value Functions",
      "author" : [ "S. Singh", "R. Yee" ],
      "venue" : "Machine Learning, 16-3:227–233",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Least-Squares λ Policy Iteration: Bias-Variance Trade-off in Control Problems",
      "author" : [ "C. Thiery", "B. Scherrer" ],
      "venue" : "International Conference on Machine Learning, Haifa, Israel",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Feature-Based Methods for Large Scale Dynamic Programming",
      "author" : [ "J.N. Tsitsiklis", "B. Van Roy" ],
      "venue" : "Machine Learning, 22(1-3):59–94",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "1 Introduction Given an infinite-horizon stationary γ-discounted Markov Decision Process [24, 4], we consider approximate versions of the standard Dynamic Programming algorithms, Policy and Value Iteration, that build sequences of value functions vk and policies πk as follows Approximate Value Iteration (AVI): vk+1 ← Tvk + ǫk+1 (1) Approximate Policy Iteration (API): {",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 3,
      "context" : "1 Introduction Given an infinite-horizon stationary γ-discounted Markov Decision Process [24, 4], we consider approximate versions of the standard Dynamic Programming algorithms, Policy and Value Iteration, that build sequences of value functions vk and policies πk as follows Approximate Value Iteration (AVI): vk+1 ← Tvk + ǫk+1 (1) Approximate Policy Iteration (API): {",
      "startOffset" : 89,
      "endOffset" : 96
    }, {
      "referenceID" : 24,
      "context" : "Under this assumption, it is well-known that both algorithms share the following performance bound (see [25, 11, 4] for AVI and [4] for API): Theorem 1.",
      "startOffset" : 104,
      "endOffset" : 115
    }, {
      "referenceID" : 10,
      "context" : "Under this assumption, it is well-known that both algorithms share the following performance bound (see [25, 11, 4] for AVI and [4] for API): Theorem 1.",
      "startOffset" : 104,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "Under this assumption, it is well-known that both algorithms share the following performance bound (see [25, 11, 4] for AVI and [4] for API): Theorem 1.",
      "startOffset" : 104,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "Under this assumption, it is well-known that both algorithms share the following performance bound (see [25, 11, 4] for AVI and [4] for API): Theorem 1.",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 91,
      "endOffset" : 136
    }, {
      "referenceID" : 10,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 91,
      "endOffset" : 136
    }, {
      "referenceID" : 26,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 91,
      "endOffset" : 136
    }, {
      "referenceID" : 11,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 91,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 91,
      "endOffset" : 136
    }, {
      "referenceID" : 22,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 91,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 91,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 91,
      "endOffset" : 136
    }, {
      "referenceID" : 19,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 91,
      "endOffset" : 136
    }, {
      "referenceID" : 20,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 91,
      "endOffset" : 136
    }, {
      "referenceID" : 21,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 91,
      "endOffset" : 136
    }, {
      "referenceID" : 8,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 91,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 18,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 15,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 0,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 7,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 17,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 4,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 16,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 9,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 2,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 8,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 1,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 153,
      "endOffset" : 195
    }, {
      "referenceID" : 25,
      "context" : "Interestingly, this very constant 2γ (1−γ)2 appears in many works analyzing AVI algorithms [25, 11, 27, 12, 13, 23, 7, 6, 20, 21, 22, 9], API algorithms [15, 19, 16, 1, 8, 18, 5, 17, 10, 3, 9, 2] and in one of their generalization [26], suggesting that it cannot be improved.",
      "startOffset" : 231,
      "endOffset" : 235
    }, {
      "referenceID" : 23,
      "context" : "2 Background We consider an infinite-horizon discounted Markov Decision Process [24, 4] (S,A, P, r, γ), whereS is a possibly infinite state space, A is a finite action space, P (ds|s, a), for all (s, a), is a probability kernel on S, r : S × A → R is a reward function bounded in max-norm by Rmax, and γ ∈ (0, 1) is a discount factor.",
      "startOffset" : 80,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "2 Background We consider an infinite-horizon discounted Markov Decision Process [24, 4] (S,A, P, r, γ), whereS is a possibly infinite state space, A is a finite action space, P (ds|s, a), for all (s, a), is a probability kernel on S, r : S × A → R is a reward function bounded in max-norm by Rmax, and γ ∈ (0, 1) is a discount factor.",
      "startOffset" : 80,
      "endOffset" : 87
    }, {
      "referenceID" : 23,
      "context" : "Though it is known [24, 4] that there always exists a deterministic stationary policy that is optimal, we will, in this article, consider non-stationary policies and now introduce related notations.",
      "startOffset" : 19,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "Though it is known [24, 4] that there always exists a deterministic stationary policy that is optimal, we will, in this article, consider non-stationary policies and now introduce related notations.",
      "startOffset" : 19,
      "endOffset" : 26
    }, {
      "referenceID" : 13,
      "context" : "From a bibliographical point of view, it is the work of [14] that made us think that non-stationary policies may lead to better performance bounds.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "In [14], non-stationary policies are only computed in the context of finite-horizon (and thus non-stationary) problems; the fact that nonstationary policies can also be useful in an infinite-horizon stationary context is to our knowledge completely new.",
      "startOffset" : 3,
      "endOffset" : 7
    } ],
    "year" : 2012,
    "abstractText" : "We consider infinite-horizon stationary γ-discounted Markov Decision Processes, for which it is known that there exists a stationary optimal policy. Using Value and Policy Iteration with some error ǫ at each iteration, it is well-known that one can compute stationary policies that are 2γ (1−γ)2 ǫ-optimal. After arguing that this guarantee is tight, we develop variations of Value and Policy Iteration for computing non-stationary policies that can be up to 2γ 1−γ ǫ-optimal, which constitutes a significant improvement in the usual situation when γ is close to 1. Surprisingly, this shows that the problem of “computing near-optimal non-stationary policies” is much simpler than that of “computing near-optimal stationary policies”.",
    "creator" : "LaTeX with hyperref package"
  }
}