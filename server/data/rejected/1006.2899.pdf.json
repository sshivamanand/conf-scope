{
  "name" : "1006.2899.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Approximated Structured Prediction for Learning Large Scale Graphical Models",
    "authors" : [ "Tamir Hazan", "Raquel Urtasun" ],
    "emails" : [ "hazan@ttic.edu", "rurtasun@ttic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 6.\n28 99\nv2 [\ncs .L\nG ]\n9 J\nul 2\n01 2"
    }, {
      "heading" : "Approximated Structured Prediction for Learning Large Scale Graphical Models",
      "text" : "Tamir Hazan TTI Chicago\nhazan@ttic.edu\nRaquel Urtasun TTI Chicago\nrurtasun@ttic.edu\nThis manuscript contains the proofs for ”A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction”\nClaim 1 The dual program of the structured prediction program in (3) takes the form\nmax px,y(ŷ)∈∆Y\n∑\n(x,y)∈S\n(\nǫH(px,y) + p ⊤ x,yey\n) − C1−q\nq\n∥ ∥ ∥ ∥ ∥ ∥ ∑\n(x,y)∈S\n∑\nŷ∈Y\npx,y(ŷ)Φ(x, ŷ)− d\n∥ ∥ ∥ ∥ ∥ ∥ q\nq\n,\nwhere ∆Y is the probability simplex over Y and H(px,y) = − ∑ ŷ px,y(ŷ) ln px,y(ŷ) is the entropy.\nProof: We first describe an equivalent program to the one in (3) by adding variables µ(x, ŷ) instead of θ⊤Φ(x, ŷ) to decouple the soft-max from the regularization.\nmin θ, µ(x, ŷ)\nµ(x, ŷ) = θ⊤Φ(x, ŷ)\n\n\n\n∑\n(x,y)∈S\nǫ ln ∑\nŷ\nexp ey(ŷ) + µ(x, ŷ)\nǫ − d⊤θ +\nC p ‖θ‖pp\n\n\n\n,\nTo maintain consistency, we add the constraints µ(x, ŷ) = θ⊤Φ(x, ŷ), for every (x, y) ∈ S and every ŷ ∈ Y . We compute the Lagrangian by adding the Lagrange multipliers px,y(ŷ)\nL() = ∑\n(x,y)∈S\nǫ ln ∑\nŷ∈Y\nexp ey(ŷ) + µ(x, ŷ)\nǫ −d⊤θ+\nC p ‖θ‖pp−\n∑\n(x,y)∈S,ŷ∈Y\npx,y(ŷ) ( µ(x, ŷ)− θ⊤Φ(x, ŷ) ) .\nThe dual function is a function of the Lagrange multipliers, and it is computed by minimizing the Lagrangian, namely q(px,y) = minµ,θ L(µ, θ,px,y). In particular the dual function can be written as ∑\n(x,y)\nmin µ(x,ŷ)\n\n\n\nǫ ln ∑\nŷ\nexp ey(ŷ) + µ(x, ŷ)\nǫ −\n∑\nŷ\nµ(x, ŷ)px,y(ŷ)\n\n\n\n+min θ\n\n\n\nC p ‖θ‖pp − θ ⊤( ∑\n(x,y),ŷ\npx,y(ŷ)Φ(x, ŷ)− d)\n\n\n\nand composed from the conjugate dual of the soft-max and the conjugate dual of the ℓp norm. Recall that the conjugate dual for the soft-max is the entropy barrier ǫH(px,y) over the set of probability distributions ∆Y (cf. [4] Theorem 8.1), and that the linear shift of the soft-max argument by ey(ŷ) result in the linear shift of the conjugate dual, thus we get the first part of the dual function ∑\n(x,y)(ǫH(px,y) + e ⊤ y px,y). Similarly, the conjugate dual of 1 p ‖θ‖pp is 1 q ‖z‖qq for the dual norm\n1/p+ 1/q = 1 (cf. [2]), where in our case z = ∑\n(x,y),ŷ px,y(ŷ)Φ(x, ŷ)− d.\nTheorem 1 The approximation of the structured prediction program in (3) takes the form\nmin λx,y,v→α,θ\n∑\n(x,y)∈S,v\nǫcv ln ∑\nŷv\nexp\n(\ney(ŷv) + ∑\nr:v∈Vr,x θrφr,v(x, ŷv)−\n∑\nα∈N(v) λx,y,v→α(ŷv)\nǫcv\n)\n+ ∑\n(x,y)∈S,α\nǫcα ln ∑\nŷα\nexp\n( ∑\nr:α∈Er θrφr,α(x, ŷα) +\n∑\nv∈N(α) λx,y,v→α(ŷv)\nǫcα\n)\n− d⊤θ − C\np ‖θ‖pp\nProof: We add auxiliary variables z and constrain them such that\nzr = ∑\n(x,y)∈S,v∈Vr,x,ŷv\nbx,y,v(ŷv)φr,v(x, ŷv) + ∑\n(x,y)∈S,α∈Er,x,ŷα\nbx,y,α(ŷα)φr,α(x, ŷα).\nWe derive the Lagrangian by introducing the Lagrange multipliers λx,y,v→α(ŷv) for every marginalization constraint ∑\nŷα\\ŷv bx,y,α(ŷα) = bx,y,v(ŷv), and Lagrange multipliers θr for every equality\nconstraint involving zr. In particular, the Lagrangian has the form:\nL() = ∑\n(x,y)∈S\n\n\n∑\nα∈E\nǫcαH(bx,y,α) + ∑\nv∈V\nǫcvH(bx,y,v) + ∑\nv∈V,ŷv\nbx,y,v(ŷv)ey,v(ŷv)\n\n− C1−q\nq ‖z− d‖qq\n+ ∑\nr\nθr\n\n\n∑\n(x,y)∈S,v∈Vr,ŷv\nbx,y,v(ŷv)φr,v(x, ŷv) + ∑\n(x,y)∈S,α∈Er,ŷα\nbx,y,α(ŷα)φr,α(x, ŷα)− zr\n\n\n+ ∑\nv,α∈N(v),ŷv\nλx,y,v→α(ŷv)\n\n\n∑\nŷα\\ŷv\nbx,y,α(ŷα)− bx,y,v(ŷv)\n\n\nWe obtain the dual function by minimizing the beliefs over their compact domain, i.e.\nq(λx,y,v→α, θ) = max bx,y,v(ŷv)∈∆Yv , bx,y,α(ŷα)∈∆Yα L(bx,y,v,bx,y,α,λx,y,v→α, θ),\nDeriving the dual by minimizing over the compact set of beliefs enables us to obtain an unconstrained dual, which corresponds to the approximated structured prediction program. The dual function is described by the conjugate dual function:\n∑\n(x,y)∈S,v\nmax bx,y,v∈∆Yv\n\n\n\nǫcvH(bx,y,v) + ∑\nŷv\nbx,y,v(ŷv)\n\ney(ŷv) + ∑\nr:v∈Vr\nθrφr,v(x, ŷv)− ∑\nα∈N(v)\nλx,y,v→α(ŷv)\n\n\n\n\n\n+ ∑\n(x,y)∈S,α\nmax bx,y,α∈∆Yα\n\n\n\nǫcαH(bx,y,α) + ∑\nŷα\nbx,y,α(ŷα)\n\n\n∑\nr:α∈Er\nθrφr,α(x, ŷα) + ∑\nv∈N(α)\nλx,y,v→α(ŷv)\n\n\n\n\n\n+max z\n{\n− C1−q\nq ‖z− d‖qq − z ⊤θ\n}\nIts final form is derived similarly to Claim 1, where we show that the conjugate dual of the entropy barrier is the soft-max function and the conjugate dual of the ℓqq is the ℓ p p.\nLemma 1 Given a vertex v in the graphical model, the optimal λx,y,v→α(ŷv) for every α ∈ N(v), ŷv ∈ Yv, (x, y) ∈ S in the approximated program of Theorem 1 satisfies\nµx,y,α→v(ŷv) = ǫcα ln\n\n\n∑\nŷα\\ŷv\nexp\n( ∑\nr:α∈Er,x θrφr,α(x, ŷα) +\n∑\nu∈N(α)\\v λx,y,u→α(ŷu)\nǫcα\n)\n\n\nλx,y,v→α(ŷv) = cα ĉv\n\ney,v(ŷv) + ∑\nr:v∈Vr,x\nθrφr,v(x, ŷr) + ∑\nβ∈N(v)\nµx,y,β→v(ŷv)\n\n − µx,y,α→v(ŷv) + cx,y,v→α\nfor every constant cx,y,v→α1, where ĉv = cv + ∑\nα∈N(v) cα. In particular, if either ǫ and/or cα are zero then µx,y,α→v corresponds to the ℓ∞ norm and can be computed by the max-function. Moreover, if either ǫ and/or cα are zero in the objective, then the optimal λx,y,v→α can be computed for any arbitrary cα > 0, similarly for cv > 0.\n1For numerical stability in our algorithm we set cx,y,v→α such that ∑\nŷv λx,y,v→α(ŷv) = 0\nProof: For a given x, y and v, optimizing λx,y,v→α(ŷv) for every α ∈ N(v) and ŷv ∈ Yv while holding the rest of the variables fixed, reduces the problem to\nmin λx,y,v→α(ŷv)\nǫcv ln ∑\nŷv\nexp\n(\ney(ŷv) + ∑\nr:v∈Vr,x θrφr,v(x, ŷv)−\n∑\nα∈N(v) λx,y,v→α(ŷv)\nǫcv\n)\n+ ∑\nα∈N(v)\nǫcα ln ∑\nŷα\nexp\n( ∑\nr:α∈Er θrφr,α(x, ŷα) +\n∑\nv∈N(α) λx,y,v→α(ŷv)\nǫcα\n)\nLet\nµx,y,α→v(ŷv) = cα ln ∑\nŷα\\ŷv\nexp\n( ∑\nr:α∈Er θrφr,α(x, ŷα) +\n∑\nu∈N(α)\\v λx,y,u→α(ŷu)\nǫcα\n)\n,\nand also φx,y,v(ŷv) = ey(ŷv) + ∑\nr:v∈Vr,x θrφr,v(x, ŷv). We find the optimal λx,y,v→α(ŷv) when-\never the gradient vanishes, i.e.\n0 = ∇\n\n\n\nǫcα ln ∑\nŷv\nexp\n(\nµx,y,α→v(ŷv) + λx,y,v→α(ŷv)\nǫcα\n)\n+ ǫcv ln ∑\nŷv\nexp\n(\nφx,y,v(ŷv)− ∑ α∈N(v) λx,y,v→α(ŷv)\nǫcv\n)\n\n\n\nTaking the vanishing point of the gradient we derive two probabilities over ŷv that need to be the same, namely\nexp (\nµx,y,α→v(ŷv)+λx,y,v→α(ŷv) ǫcα\n)\n∑\nỹv exp\n(\nµx,y,α→v(ỹv)+λx,y,v→α(ỹv) ǫcα\n) = exp\n( φx,y,v(ŷv)− ∑\nβ∈N(v) λx,y,v→β(ŷv)\nǫcv\n)\n∑\nỹv exp\n( φx,y,v(ỹv)− ∑\nβ∈N(v) λx,y,v→β(ỹv)\nǫcv\n) .\nFor simplicity we need to consider only the numerator, while taking one degree of freedom in the normalization. Taking log of the numerator we get that the gradient vanishes if the following holds\nĉx,y,v→α + µx,y,α→v(ŷv) + λx,y,v→α(ŷv)\ncα =\nφx,y,v(ŷv)− ∑ β∈N(v) λx,y,v→β(ŷv)\ncv . (1)\nMultiplying both sides of the equation by cvcα, and summing both sides with respect to β ∈ N(v) gives\nc̃x,y,v→α+cv ∑\nβ∈N(v)\n(µx,y,β→v(ŷv) + λx,y,v→β(ŷv)) =\n\n\n∑\nβ∈N(v)\ncβ\n\n\n\nφx,y,v(ŷv)− ∑\nβ∈N(v)\nλx,y,v→β(ŷv)\n\n .\n(2) We wish to find the optimal value of λx,y,v→α(ŷv), namely the value that satisfies Eq. (1). For that purpose we recover the value of ∑\nb∈N(v) λx,y,v→β(ŷv) from (2):\nc̃x,y,v→α+\n\ncv + ∑\nβ∈N(v)\ncβ\n\n\n\n\n∑\nβ∈N(v)\nλx,y,v→β(ŷv)\n\n =\n\n\n∑\nβ∈N(v)\ncβ\n\nφx,y,v(ŷv)−cv ∑\nβ∈N(v)\nµx,y,β→v(ŷv).\nPlugging this into 1 gives\nµx,y,α→v(ŷv)+λx,y,v→α(ŷv) = cα\ncv + ∑ β∈N(v) cβ\n\nφx,y,v(ŷv) + ∑\nβ∈N(v)\nµx,y,β→v(ŷv)\n\n+cx,y,v→α\nwhich concludes the proof for ǫ, cα, cv > 0. Whenever any of these quantitates is zero, Danskin’s theorem (cf. [1], Theorem 4.5.1) states that its corresponding subgradient is described by a probability distribution over its maximal assignments. Therefore if cα = 0 in the objective function, then equality (1) holds for every cα, and similarly whenever cv = 0 in the objective, equality holds for every cv.\nLemma 2 The gradient of the approximated structured prediction program in Theorem 1 with respect to θr equals to\n∑\n(x,y)∈S,v∈Vr,x,ŷv\nbx,y,v(ŷv)φr,v(x, ŷv) + ∑\n(x,y)∈S,α∈Er,x,ŷα\nbx,y,α(ŷα)φr,α(x, ŷα)− dr +C · |θr| p−1 · sign(θr),\nwhere\nbx,y,v(ŷv) ∝ exp\n(\ney(ŷv) + ∑\nr:v∈Vr,x θrφr,v(x, ŷv)−\n∑\nα∈N(v) λx,y,v→α(ŷv)\nǫcv\n)\nbx,y,α(ŷα) ∝ exp\n( ∑\nr:α∈Er,x θrφr,α(x, ŷα) +\n∑\nv∈N(α) λx,y,v→α(ŷα)\nǫcα\n)\nHowever, if either ǫ and/or cα equal zero, then the beliefs bx,y,α(ŷα) can be taken from the set of probability distributions over support of the max-beliefs, namely bx,y,α(ŷ∗α) > 0 only if ŷ∗α ∈ argmaxŷα { ∑ r:α∈Er,x θrφr,α(x, ŷα) + ∑ v∈N(α) λx,y,v→α(ŷα) } . Similarly for bx,y,v(ŷ∗v) whenever ǫ and/or cv equal zero.\nProof: This is a direct computation of the gradient. In the special case of ǫ, cα = 0 then bx,y,α(ŷα) corresponds to the subgradient and similarly when ǫ, cv = 0, cf. Danskin’s theorem (cf. [1], Theorem 4.5.1).\nClaim 2 The block coordinate descent algorithm in lemmas 1 and 2 monotonically reduces the approximated structured prediction objective in Theorem 1, therefore the value of its objective is guaranteed to converge. Moreover, if ǫ, cα, cv > 0, the objective is guaranteed to converge to the global minimum, and its sequence of beliefs are guaranteed to converge to the unique solution of the approximated structured prediction dual.\nProof: The approximated structured prediction dual is strictly concave in the dual variables bx,y,v(ŷv), bx,y,α(ŷα), z subject to linear constraints. The claim properties are a direct consequence of [3] for this type of programs.\nClaim 3 Whenever the approximated structured prediction is non convex, i.e., ǫ, cα > 0 and cv < 0, the algorithm in lemmas 1 and 2 is not guaranteed to converge, but whenever it converges it reaches a stationary point of the primal and dual approximated structured prediction programs.\nProof: The approximated structured prediction in Theorem 1 is unconstrained. The update rules defined in Lemmas 1 and 2 are directly related to vanishing points of the gradient of this function, even when it is non-convex. Therefore a stationary point of the algorithm corresponds to an assignment λx,y,v→α(ŷv), θr for which the gradient equals zero, or equivalently a stationary point of the approximated structured prediction.\nThe dual approximated structured prediction in (??) is a constrained optimization and its stationary points are saddle points of the Lagrangian, defined in Theorem 1, with respect to the probability simplex bx,y,v(ŷv) ∈ ∆Yv and bx,y,α(ŷα) ∈ ∆Yα . Note that since ǫ, cα, cv 6= 0 the entropy functions act as barrier functions on the nonnegative cone, therefore we need not consider the nonnegative constraints over the beliefs. In the following we show that at stationary points the inferred beliefs of the Lagrangian satisfy the marginalization constraints, therefore are saddle points of the Lagrangian.\nWhen ǫ, cα > 0 the stationary beliefs bx,y,α(ŷα) are achieved by maximizing over ∆Yα , resulting in\nbx,y,α(ŷα) ∝ exp\n( ∑\nr:α∈Er,x θrφr,α(x, ŷα) +\n∑\nv∈N(α) λx,y,v→α(ŷα)\nǫcα\n)\n.\nHowever, since cv < 0 the stationary beliefs bx,y,v(ŷv) are achieved by minimizing over ∆Yv resulting in\nbx,y,v(ŷv) ∝ exp\n(\ney(ŷv) + ∑\nr:v∈Vr,x θrφr,v(x, ŷv)−\n∑\nα∈N(v) λx,y,v→α(ŷv)\nǫcv\n)\n.\nTo prove these beliefs correspond to a stationary point we show that they satisfy the marginalization constraints. This fact is a direct consequence of the update rule in Lemma 1, where by direct computation one can verify that\n∑\nŷα\\ŷv\nbx,y,α(ŷα) ∝ exp\n(\nµx,y,α→v(ŷv) + λx,y,v→α(ŷv)\nǫcα\n)\n.\nFollowing the definition of bx,y,v(ŷv) one can see that the update rule in Lemma 1 enforces the marginalization constraints. This implies that the gradient of the approximated structured prediction program measures the disagreements between ∑\nŷα\\ŷv bx,y,α(ŷα) and bx,y,v(ŷv), and the\ngradient vanishes only when they agree. Therefore these beliefs correspond to a saddle point of the Lagrangian."
    } ],
    "references" : [ {
      "title" : "Convex Analysis and Optimization",
      "author" : [ "D.P. Bertsekas", "A. Nedić", "A.E. Ozdaglar" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2003
    }, {
      "title" : "Convex analysis",
      "author" : [ "R.T. Rockafellar" ],
      "venue" : "Princeton university press,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1970
    }, {
      "title" : "Relaxation methods for problems with strictly convex separable costs and linear constraints",
      "author" : [ "P. Tseng", "D.P. Bertsekas" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1987
    }, {
      "title" : "Graphical models, exponential families, and variational inference",
      "author" : [ "M.J. Wainwright", "M.I. Jordan" ],
      "venue" : "Foundations and Trends R © in Machine Learning,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "[4] Theorem 8.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2]), where in our case z = ∑ (x,y),ŷ px,y(ŷ)Φ(x, ŷ)− d.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1], Theorem 4.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1], Theorem 4.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "The claim properties are a direct consequence of [3] for this type of programs.",
      "startOffset" : 49,
      "endOffset" : 52
    } ],
    "year" : 2012,
    "abstractText" : "and composed from the conjugate dual of the soft-max and the conjugate dual of the lp norm. Recall that the conjugate dual for the soft-max is the entropy barrier ǫH(px,y) over the set of probability distributions ∆Y (cf. [4] Theorem 8.1), and that the linear shift of the soft-max argument by ey(ŷ) result in the linear shift of the conjugate dual, thus we get the first part of the dual function ∑ (x,y)(ǫH(px,y) + e ⊤ y px,y). Similarly, the conjugate dual of 1 p ‖θ‖p is 1 q ‖z‖q for the dual norm 1/p+ 1/q = 1 (cf. [2]), where in our case z = ∑ (x,y),ŷ px,y(ŷ)Φ(x, ŷ)− d. Theorem 1 The approximation of the structured prediction program in (3) takes the form min λx,y,v→α,θ ∑",
    "creator" : "LaTeX with hyperref package"
  }
}