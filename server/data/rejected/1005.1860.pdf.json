{
  "name" : "1005.1860.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Feature Selection Using Regularization in Approximate Linear Programs for Markov Decision Processes",
    "authors" : [ "Marek Petrik", "Gavin Taylor" ],
    "emails" : [ "petrik@cs.umass.edu", "gvtaylor@cs.duke.edu", "parr@cs.duke.edu", "shlomo@cs.umass.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 5.\n18 60\nv2 [\ncs .A\nI] 2\n0 M\nay 2"
    }, {
      "heading" : "1. Introduction",
      "text" : "Solving large Markov Decision Processes (MDPs) is a very useful, but computationally challenging problem addressed widely by reinforcement learning. It is widely accepted that large MDPs can only be solved approximately. This approximation is commonly done by relying on linear value function approximation, in which the value function is chosen from a small-dimensional vector space of features. While this framework offers computational benefits and protection from the overfitting in the training data, selecting\nAppearing in Proceedings of the 27 th International Conference on Machine Learning, Haifa, Israel, 2010. Copyright 2010 by the author(s)/owner(s).\nan effective, small set of features is difficult and requires a deep understanding of the domain. Feature selection, therefore, seeks to automate this process in a way that may preserve the computational simplicity of linear approximation (Parr et al., 2007; Mahadevan, 2008). We show in this paper that L1-regularized approximate linear programs (RALP) can be used with very rich feature spaces.\nRALP relies, like other value function approximation methods, on samples of the state space. The value function error on states that are not sampled is known as the sampling error. This paper shows that regularization in RALP can guarantee small sampling error. The bounds on the sampling error require somewhat limiting assumptions on the structure of the MDPs, as any guarantees must, but this framework can be used to derive tighter bounds for specific problems in the future. The relatively simple bounds can be used to determine automatically the regularization coefficient to balance the expressivity of the features with the sampling error.\nWe derive the approach with the L1 norm, but it could be used with other regularizations with small modifications. The L1 norm is advantageous for two main reasons. First, the L1 norm encourages the sparse solutions, which can reduce the computational requirements. Second, the L1 norm preserves the linearity of RALPs; the L2 norm would require quadratic optimization.\nRegularization using the L1 norm has been widely used in regression problems by methods such as LASSO (Tibshirani, 1996) and Dantzig selector (Candes & Tao, 2007). The value-function approximation setting is, however, quite different and the regression methods are not directly applicable. Regularization has been previously used in value function approximation (Taylor & Parr, 2009; Farahmand et al.,\nFeature Selection Using Regularization\n2008; Kolter & Ng, 2009). In comparison with LARSTD (Kolter & Ng, 2009), an L1 regularized value function approximation method, we explicitly show the influence of regularization on the sampling error, provide a well-founded method for selecting the regularization parameter, and solve the full control problem. In comparison with existing sampling bounds for ALP (de Farias & Van Roy, 2001), we do not assume that the optimal policy is available, make more general assumptions, and derive bounds that are independent of the number of features.\nOur approach is based on approximate linear programming (ALP), which offers stronger theoretical guarantees than some other value function approximation algorithms. We describe ALP in Section 3 and RALP and its basic properties in Section 4. RALP, unlike ordinary ALPs, is guaranteed to compute bounded solutions. We also briefly describe a homotopy algorithm for solving RALP, which exhibits anytime behavior by gradually increasing the norm of feature weights. To develop methods that automatically select features with generalization guarantees, we propose general sampling bounds in Section 5. These sampling bounds, coupled with the homotopy method, can automatically choose the complexity of the features to minimize over-fitting. Our experimental results in Section 6 show that the proposed approach with large feature sets is competitive with LSPI when performed even with small feature spaces hand selected for standard benchmark problems. Section 7 concludes with future work and a more detailed relationship with other methods."
    }, {
      "heading" : "2. Framework and Notation",
      "text" : "In this section, we formally define Markov decision processes and linear value function approximation. A Markov Decision Process is a tuple (S,A, P, r, γ), where S is the possibly infinite set of states, and A is the finite set of actions. P : S × S × A 7→ [0, 1] is the transition function, where P (s′, s, a) represents the probability of transiting to state s′ from state s, given action a. The function r : S × A 7→ R is the reward function, and γ is the discount factor. Pa and ra are used to denote the probabilistic transition matrix and reward vector for action a.\nWe are concerned with finding a value function v that maps each state s ∈ S to the expected total γdiscounted reward for the process. Value functions can be useful in creating or analyzing a policy π : S×A → [0, 1] such that for all s ∈ S, ∑\na∈A π(s, a) = 1. The transition and reward functions for a given policy are denoted by Pπ and rπ . The value function update for\na policy π is denoted by Lπ, and the Bellman operator is denoted by L. That is:\nLπv = γPπv + rπ Lv = max π∈Π Lπv.\nThe optimal value function v∗ satisfies Lv∗ = v∗.\nWe focus on linear value function approximation for discounted infinite-horizon problems, in which the value function is represented as a linear combination of nonlinear basis functions (vectors). For each state s, we define a vector φ(s) of features. The rows of the basis matrix Φ correspond to φ(s), and the approximation space is generated by the columns of the matrix. That is, the basis matrix Φ, and the value function v are represented as:\nΦ =\n(\n— φ(s1) T —\n...\n)\nv = Φw.\nThis form of linear representation allows for the calculation of an approximate value function in a lowerdimensional space, which provides significant computational benefits over using a complete basis; if the number of features is small, this framework can also guard against overfitting noise in the samples.\nDefinition 1. A value function, v, is representable if v ∈ M ⊆ R|S|, where M = colspan (Φ). The set of ǫ-transitive-feasible value functions is defined for ǫ ≥ 0 as follows: K(ǫ) = {v ∈ R|S| v ≥ Lv − ǫ1. Here 1 is a vector of all ones. A value function is transitivefeasible when v ≥ Lv and the set of transitive-feasible functions is defined as K = K(0).\nNotice that the optimal value function v∗ is transitivefeasible, and that M is a linear space."
    }, {
      "heading" : "3. Approximate Linear Programming",
      "text" : "The approximate linear programming (ALP) framework is an approach for calculating a value function approximation for large MDP with a set of features Φ that define a linear space M (Schweitzer & Seidmann, 1985; de Farias & Van Roy, 2003). The ALP takes the following form:\nmin v∈M\n∑\ns∈S\nρ(s)v(s)\ns.t. r(s, a) + γ ∑\ns′∈S\nP (s′, s, a)v(s′) ≤ v(s)\nwhere ρ is a distribution over the initial states and the constraints are for all (s, a) ∈ (S,A); that is ∑\ns∈S ρ(s) = 1. To ensure feasibility, one of the features is assumed to be constant. Therefore, in\nFeature Selection Using Regularization\nthe remainder of the paper, we make the following standard assumption (Schweitzer & Seidmann, 1985), which can be satisfied by setting the first column of M to 1.\nAssumption 2. For all k ∈ R, we have that k·1 ∈ M, where 1 is a vector of all ones.\nFor simplicity and generality of notation, we use L to denote the ALP constraint matrix, so Lv ≤ v is equal to the set of constraints {Lav ≤ v : ∀a ∈ A}. Then, we can rewrite the ALP as follows:\nmin v\nρTv\ns.t. Lv ≤ v v ∈ M (1)\nNotice that the constraints in the ALP correspond to the definition of transitive-feasible functions in Definition 1. A succinct notation of the ALP constraints can then use the set of transitive-feasible functions as v ∈ M∩K.\nThe constraint v ∈ M implies that v = Φw and therefore the number of variables in (1) corresponds to the number of features. Typically, this is a small number. However, the number of required constraints in ALP is |S| × |A|, which is oftentimes impractically large or infinite. The standard solution is to sample a small set of constraints according to a given distribution (de Farias & Van Roy, 2003). It is then possible to bound the probability of violating a randomly chosen constraint. There are, however, a few difficulties with this approach. First, leaving constraints out can lead to an unbounded linear program. Second, in practice the distribution over the constraints can be very different from the distribution assumed by the theory. Finally, the bound provides no guarantees on the solution quality.\nALP has often under-performed ADP methods in practice; this issue has been recently studied and partially remedied (Petrik & Zilberstein, 2009; Desai et al., 2009). Because these methods are independent of the proposed modifications, we only focus on standard approximate linear programs.\nWe show next that RALP with sampled constraints not only guarantees that the solution is bounded and provides worst-case error bounds on the value function, but also is independent of the number of features. As a result, the ALP formulation does not require a small number of features to be selected in advance."
    }, {
      "heading" : "4. Regularized Approximate Linear Programming",
      "text" : "In this section, we introduce L1-regularized ALP (RALP) as an approach to automate feature selection and alleviate the need for all constraints in standard ALP. Adding L1 regularization to ALP permits the user to supply an arbitrarily rich set of features without the risk of overfitting.\nThe RALP for basis Φ and L1 constraint ψ is defined as follows:\nmin w\nρTΦw\ns.t. LΦw ≤ Φw ‖w‖1,e ≤ ψ, (2)\nwhere ‖w‖1,e = ∑\ni e(i)w(i). Note that RALP is a generalization of ALP; when ψ approaches infinity, the RALP solution approaches the ALP solution. The objective value of (2) as a function of ψ is denoted as θ(ψ).\nWe generally use e = 1−1, which is a vector of all ones except the first position, which is 0; because the first feature is the constant feature, we do not include it in the regularization. The main reasons for excluding the constant feature are that the policy is independent of the constant shifts, and the homotopy method we propose requires that the linear program is easy to solve when ψ = 0.\nAlternatively, we can formulate RALP in (2) as a minor modification of ALP in equation (1). This is by modifying M to satisfy the L1 norm as:\nM(ψ) = {Φw ‖w‖1,e ≤ ψ}.\nNotice that RALP introduces an additional parameter ψ over ALP. As with L1 regularization for regression, this raises some concerns about a method for choosing the regularization parameter. Practical methods, such as cross-validation may be used to address this issue. We also propose an automated method for choosing ψ in Section 5 based on the problem and sampling parameters."
    }, {
      "heading" : "5. Sampling Bounds",
      "text" : "The purpose of this section is to show that RALP offers two main benefits over ALP. First, even when the constraints are sampled and incomplete, it is guaranteed to provide a feasible solution. Since feasibility does not imply that the solution is close to optimal, we then show that under specific assumptions — such as smooth reward and transition functions — RALP guarantees that the error due to the missing constraints is small.\nFeature Selection Using Regularization\nTo bound the error from sampling, we must formally define the samples and how they are used to construct ALPs. We consider the following two types of samples Σ̃ and Σ̄ defined as follows.\nDefinition 3. One-step simple samples are defined as follows: Σ̃ ⊆ {(s, a, (s1 . . . sn), r(s, a)) s ∈ S, a ∈ A}, where s1 . . . sn are selected i.i.d. from the distribution P (s, a, ·). One-step samples with expectation are defined as follows: Σ̄ ⊆ {(s, a, P (s, a, ·), r(s, a)) s ∈ S, a ∈ A}.\nOften the samples only include state transitions, as Σ̃ defines. The more informative samples Σ̄ include the probability distribution of the states that follow the given state and action, as follows:\nL̄(v)(s̄) = r(s̄, a) + γ ∑\ns′∈S\nP (s̄, a, s′)v(s′), (3)\nwhere (s̄, a, P (s̄, a, ·), r(s̄, a)) ∈ Σ̄. The lessinformative Σ̃ can be used as follows:\nL̃(v)(s̃) = r(s̃, a) + γ 1\nn\nn ∑\ni=1\nv(si), (4)\nwhere (s̃, a, (s1 . . . sn), r(s̃, a)) ∈ Σ̃. The corresponding transitive-feasible sets K̄ and K̃ are defined similarly. The ALPs can be constructed based on samples as Figure 1 shows. Full ALP corresponds to the RALP formulation in (2), when M is constricted with L1 regularization. In comparison, sampled ALP is missing some of the constraints while estimated ALP is both missing some constraints, and the included constraints may be estimated imprecisely.\nThe following two assumptions quantify the behavior of the ALP with respect to missing and imprecise constraints respectively. The first assumption limits the error due to missing transitions in the sampled Bellman operator L̄.\nAssumption 4 (Constraint Sampling Behavior). The representable value functions satisfy that:\nK ∩M(ψ) ⊆ K̄ ∩M(ψ) ⊆ K(ǫp) ∩M(ψ),\nand that for all representable value functions v ∈ M(ψ) we have that |(ρ− ρ̄)Tv| ≤ ǫc(ψ).\nThe constant ǫp bounds the potential violation of the ALP constraints on states that are not provided as a part of the sample. In addition, all value functions that are transitive-feasible for the full Bellman operator are transitive-feasible in the sampled version; the sampling only removes constraints on the set. The constant ǫc essentially represents the maximal error in estimating\nthe objective value of ALP for any representable value function.\nThe next assumption quantifies the error on the estimation of the transitions of the estimated Bellman operator L̃.\nAssumption 5 (Constraint Estimation Behavior). The representable value functions satisfy that:\nK̄(−ǫs) ∩M(ψ) ⊆ K̃ ∩M(ψ) ⊆ K̄(ǫs) ∩M(ψ),\nwhere Σ̄ and Σ̃ (and therefore K̄ and K̃) are defined for identical sets of states.\nThese assumptions are quite generic in order to apply in a wide range of scenarios. The main idea behind the assumptions is to bound by how much a feasible solution in the sampled or estimated ALP can violate the true ALP constraints. These assumptions may be easily satisfied, for example, by making the following Lipschitz continuity assumptions.\nAssumption 6. Let k : S → Rn be a map of the state-space to a normed vector space. Then for all x, y, z ∈ S and all features (columns) φi ∈ Φ, we define Kr, KP , and Kφ such that\n|r(x) − r(y)| ≤ Kr‖k(x)− k(y)‖\n|p(z|x, a)− p(z|y, a)| ≤ KP ‖k(x)− k(y)‖\n|φi(x) − φi(y)| ≤ Kφ‖k(x)− k(y)‖\nProposition 7. Assume Assumption 6 and that for any s ∈ S there exists a state s̄ ∈ Σ̄ such that ‖s̄−s‖ ≤ c. Then Assumption 4 and Assumption 5 hold with ǫp(ψ) = cKr + cψ(Kφ + γKP )\nBecause of the technical nature of the proof, it is deferred to the appendix.\nThe importance of this bound is that the violation on constraints that were not sampled grows linearly with the increasing coefficient ψ. As we show below, this fact can be used to determine the optimal value of ψ. For the sake of brevity, we do not discuss the estimation error bounds ǫs in more detail, which can be easily derived from existing results (Petrik & Zilberstein, 2009).\nWe are now ready to state the following general bounds on the approximation error of a RALP.\nTheorem 8 (Offline Error Bound). Assume Assumptions 2, 4, and 5. Let v̂, v̄, ṽ be the optimal solutions of (5), (6), and (7), respectively (see Figure 1). Let ǫ = 21−γ minv∈M(ψ) ‖v− v ∗‖∞ Then, the following in-\nequalities hold:\n‖v̂ − v∗‖1,ρ ≤ ǫ\n‖v̄ − v∗‖1,ρ ≤ ǫ + 2ǫc(ψ) + 2 ǫp(ψ)\n1− γ\n‖ṽ − v∗‖1,ρ ≤ ǫ + 2ǫc(ψ) + 3ǫs(ψ) + 2ǫp(ψ)\n1− γ\nBecause of the technical nature of the proof, it is deferred to the appendix.\nNotice that because the weight ρ is often chosen arbitrarily, the bounds may be simply derived for ‖ · ‖1,ρ̄. In that case, ǫc = 0. Unlike most of the existing ALP bounds, we focus on bounding the error of the value function, instead of bounding the number of violated constraints.\nConsider the implications of these bounds combined with the Lipschitz assumptions of Proposition 7. It is clear that reducing ψ tightens Theorem 8, but causes the set M(ψ) to shrink and become more restrictive; this suggests a tradeoff to be considered when setting the regularization parameter. The bound also illustrates the importance of covering the space with samples; as the distance between the samples c approaches zero, the bound tightens. In short, the Lipschitz assumptions limit how quickly the constraints can change between sampled states. As sampled states get closer or the reward, feature, and probability functions become smoother, constraints between samples become more and more restricted; however, smoother basis functions may mean a less expressive space. Similar tradeoffs are likely to appear however Assumption 4 and Assumption 5 are fulfilled.\nThe offline error bounds in Theorem 8 can be used to guarantee the performance of a RALP for a fixed number of samples and the regularization coefficient ψ. It does not, however, prescribe how to choose the regularization coefficient for a given set of samples. To do that, we have to derive bounds for an actual value function v. When the samples are known, these bounds are typically tighter than the offline error bound.\nTheorem 9 (Online Error Bound). Assume Assumption 2 and let v ∈ K̃ ∩ M(ψ) be an arbitrary feasible solution of the estimated ALP (7). Then:\n‖v∗ − v‖1,ρ ≤ ρ̄ Tv − ρTv∗ + ǫc(ψ) + 2\nǫs(ψ) + ǫp(ψ)\n1− γ .\nBecause of the technical nature of the proof, it is deferred to the appendix.\nHere we briefly introduce a homotopy method that not only speeds the computation of the RALP solution, but also can be used to find the optimal value of ψ. Because the homotopy method only relies on standard linear programming analysis and is somewhat technical, the detailed description is provided in Appendix B in the appendix.\nThe main idea of the homotopy method is to first calculate θ(0) and then trace the optimal solution for increasing values of ψ. The optimal solution w(ψ) of the RALP (2) is a piecewise linear function of ψ. At any point in time, the algorithm keeps track of a set of active – or non-zero – variables w and a set of active constraints, which are satisfied with equality. In the linear segments, the number of active constraints and variables are identical, and the non-linearity arises when variables and constraints become active or inac-\nFeature Selection Using Regularization\ntive. Therefore, the linear segments are traced until a variable becomes inactive or a constraint becomes active. Then, the dual solution is traced until a constraint becomes inactive or a variable becomes active.\nSince the homotopy algorithm solves for the optimal value of the RALP (2) for all values of the regularization coefficient ψ, it is possible to increase the coefficient ψ until the error increase between sampled constraints balances out the decrease in the error due to the restricted feature space as defined in Theorem 8. That is, we can calculate the objective value of the linear program (2) for any value of ψ.\nIt is easy to find ψ that minimizes the bounds in this section. As the following corollary shows, to find the global minimum of the bounds, it is sufficient to use the homotopy method to trace θ(ψ) while its derivative is less than the increase in the error due to the sampling (‖v̂ − ṽ‖1,ρ). Let v(ψ) be an optimal solution of (7) as a function of the regularization coefficient ψ.\nCorollary 10. Assume that ǫc(ψ), ǫp(ψ), and ǫs(ψ) are convex functions of ψ. Then, the error bound ‖v(ψ)− v∗‖1,ρ ≤ f(ψ) for any v(ψ) is:\nf(ψ) = θ(ψ) − ρTv∗ + ǫc(ψ) + 2 ǫs(ψ) + ǫp(ψ)\n1− γ .\nThe function f(ψ) is convex and its sub-differential1 ∇ψf is independent of v∗. Therefore, a global minimum ψ∗ of f is attained when 0 ∈ ∇ψf(ψ∗) or when ψ∗ = 0.\nThe corollary follows directly from Theorem 9 and the convexity of the optimal objective value of (2) as a function ψ. Figure 2 illustrates the functions in the corollary. Notice that Proposition 7 is sufficient to satisfy the conditions of this corollary. In particular, the functions ǫs(ψ), ǫp(ψ), ǫc(ψ) are linear in ψ."
    }, {
      "heading" : "6. Experimental Results",
      "text" : "In this section, we present results indicating that RALP effectively selects from rich feature spaces to outperform ALP and other common algorithms, such as LSPI, on several example problems, including the balanced pendulum and the bicycle problems. We also demonstrate the speed and effectiveness of the homotopy method in choosing a value of ψ.\nBenefits of Regularization First, we demonstrate and analyze the properties of RALP on a simple chain problem with 200 states, in which the transitions move to the right by one step with a centered Gaussian noise\n1Function f may be non-differentiable\nwith standard deviation 3. The reward for reaching the right-most state was +1 and the reward in the 20th state was -3. This problem is small to enable calculation of the optimal value function and to control sampling. We uniformly selected every fourth state on the chain and estimated the sampling bound ǫp(ψ) = 0.05ψ. The approximation basis in this problem is represented by piecewise linear features, of the form φ(si) = [i− c]+, for c from 1 to 200; these features were chosen due to their strong guarantees for the sampling bounds. The experimental results were obtained using the proposed homotopy algorithm.\nFigure 3 demonstrates the solution quality of RALP on the chain problem as a function of the regularization coefficient ψ. The figure shows that although the objective of RALP keeps decreasing as ψ increases, the sampling error overtakes that reduction. It is clear that a proper selection of ψ improves the quality of the resulting approximation. To demonstrate the benefits of regularization as it relates to overfitting, we compare the performance of ALP and RALP as a function of the number of available features in Figure 5. While ALP performance improves initially, it degrades severely with more features. The value ψ in RALP is selected automatically using Corollary 10 and the sampling bound of ǫp(ψ) = 0.05ψ. Figure 4 demonstrates that RALP may also overfit, or perform poorly when the regularization coefficient ψ is not selected properly.\nTo find the proper value of ψ, as described in Corollary 10, the problem needs to be solved using the homotopy method. We show that the homotopy method performs significantly faster than a commercially available linear program solver Mosek. Figure 6 compares the computational time of homotopy method and Mosek, when solving the problem for multiple values of ψ in increments of 0.5 on the standard mountain car problem (Barto & Sutton, 1998) with 901 piecewise linear features and 6000 samples. Even for any single value ψ, the homotopy method solves the linear program about 3 times faster than Mosek. The next two experiments, however, do not use the homotopy method. In practice, RALP often works much better than what is suggested by our bounds, which can be loose for sparsely sampled large state spaces. In the following experiments, we determined ψ empirically by solving the RALP for several different values of ψ and selecting the one that produced the best policy. This was practical because we could solve the large RALPs in just a few minutes using constraint generation.\nInverted Pendulum We now offer experimental results demonstrating RALP’s ability to create effective value functions in balancing an inverted pen-\nFeature Selection Using Regularization\n0 10 20 30 40 50 0\n10\n20\n30\nFeatures\nT ru\ne L 1\nE rr\nor\nALP RALP\nFigure 5. Average of 45 runs of ALP and RALP as a function of the number of features. Coefficient ψ was selected using Corollary 10.\n0 500 1000 1500 2000 0\n50\n100\nEpisodes\n% R\nea ch\ned G\noa l\nFigure 8. RALP performance on bicycle as a function on the number of episodes.\ndulum, a standard benchmark problem in reinforcement learning (Wang et al., 1996; Lagoudakis & Parr, 2003). Samples of the form (s, a, r, s′) were drawn from every action on states drawn from random trajectories with the pendulum starting in an upright state, referred to as episodes. Features were kernels on 650 states randomly selected from the training data, consisting of Gaussian kernels of standard deviation 0.5, 1, and 1.5, and a 6th degree polynomial kernel. ψ was 1.4, and an average of 25 features had non-zero weights.\nThe constraints in the RALP were based on a single sample for each state and action pair. The policy was evaluated based on the number of steps it could balance the pendulum, with an upper limit of 3000. This served to evaluate the policy resulting from the approximate value function. We plot the average number of steps the pendulum was balanced as a function of the number of training episodes in Figure 7, as an average of 100 runs. It is clear the controller produced by RALP was effective for small amounts of data, balancing the pendulum for the maximum number of steps nearly all of the time, even with only 50 training episodes. Similarly, it was able to leverage the larger number of available features to construct an effective controller with fewer trajectories than LSPI, which needed 450 training episodes before achieving\nan average of 2500 balanced steps (Lagoudakis & Parr, 2003).\nBicycle Balancing and Riding We also present experimental results for the bicycle problem, in which the goal is to learn to balance and ride a bicycle to a target position (Randløv & Alstrøm, 1998; Lagoudakis & Parr, 2003). This is a challenging benchmark domain in reinforcement learning. Training data consisted of samples for every action on states drawn from trajectories resulting from random actions up to 35 states long, similar to the inverted pendulum domain. The feature set consisted of monomials up to degree 4 on the individual dimensions and products of monomials up to degree 3, for a total of 159 features. ψ was 0.03, and an average of 34 features had nonzero weights. We plot the number of runs out of 100 in which the bicycle reached the goal region as a function of number of training episodes in Figure 8. Again, a high percentage of runs were successful, even with only 500 training episodes. In comparison, LSPI required 1500 training episodes to pass 80% success. It is worth pointing out that due to sampling every action at each state, RALP is using more samples than LSPI, but far fewer trajectories.\nFeature Selection Using Regularization"
    }, {
      "heading" : "7. Conclusion and Related Work",
      "text" : "In this paper, we introduced L1-regularized Approximate Linear Programming and demonstrated its properties for combined feature selection and value function approximation in reinforcement learning. RALP simultaneously addresses the feature selection, value function approximation, and policy determination problems; our experimental results demonstrate that it addresses these issues effectively for several sample problems, while our bounds explain the effects of sampling on the resulting approximation.\nThere are many additional issues that need to be addressed. The first is the construction of better bounds to guide the sampling. Our bounds explain the behavior of RALP approximation as it relates to the trade-off between the richness of the features with the number of available samples, but these bounds may at times be quite loose. Future work must identify conditions that can provide stronger guarantees. Additionally, a data-driven approach which can calculate a tighter bound online would be valuable. Finally, our analysis did not address the conditions that would guarantee sparse RALP solutions and, therefore, allow more computationally efficient solvers."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported in part by DARPA CSSG HR0011-06-1-0027, by NSF IIS-0713435, by the Air Force Office of Scientific Research under Grant No. FA9550-08-1-0171, and by the Duke University Center for Theoretical and Mathematical Sciences. We also thank the anonymous reviewers for their useful comments."
    }, {
      "heading" : "A. Proofs",
      "text" : "A.1. Sampling Bounds\nThe following lemmas summarize the basic properties of the Bellman operator. We include them without proofs, which use standard techniques.\nLemma 11 (Bellman Operator). Let 1 and v be a constant vector of an appropriate size. Then:\nL(v + ǫ1) = γǫ1+ Lv.\nLemma 12 (Transitive Feasible are Upper Bound). Assume an ǫ-transitive-feasible value function v ∈ K(ǫ). That is:\nv ≥ Lv − ǫ1.\nThen: v ≥ v∗ − ǫ\n1− γ 1.\nLemma 13. Assume Assumption 2 and that there exists v ∈ M such that\n‖v − v∗‖∞ ≤ ǫ.\nThen, there exists v′ ∈ M∩K such that\n‖v′ − v∗‖∞ ≤ 2ǫ\n1− γ .\nTheorem 8. [Offline Error Bound] Assume Assumptions 2, 4, and 5. Let v̂, v̄, ṽ be the optimal solutions of (5), (6), and (7), respectively (see Figure 1). Let ǫ = 21−γ minv∈M(ψ) ‖v− v\n∗‖∞ Then, the following inequalities hold:\n‖v̂ − v∗‖1,ρ ≤ ǫ\n‖v̄ − v∗‖1,ρ ≤ ǫ + 2ǫc(ψ) + 2 ǫp(ψ)\n1− γ\n‖ṽ − v∗‖1,ρ ≤ ǫ + 2ǫc(ψ) + 3ǫs(ψ) + 2ǫp(ψ)\n1− γ\nProof. To simplify the notation, we omit ψ in the notation of ǫ in the proof.\nProof of ‖v̂ − v∗‖1,ρ: Let v be the minimizer of minv∈M ‖v − v∗‖∞. Then from Lemma 13, Lemma 12, and ‖x‖1,ρ ≤ ‖x‖∞ there exists v′ ∈ M∩K such that:\n‖v′ − v∗‖∞ ≤ 2\n1− γ ‖v − v∗‖∞\nFrom Lemma 12, we have that:\n‖v̂ − v∗‖1,ρ = ρ T(v̂ − v∗) = ρTv̂ − ρTv∗ ≤ ρTv′ − ρTv∗\n≤ ‖v′ − v∗‖1,ρ.\nFeature Selection Using Regularization\nProof of ‖v̄ − v∗‖1,ρ: Let v̄′ be the solution (6) but with ρTv as the objective function. From Lemma 12 we have:\nv̄ ≥ v∗ − ǫp\n1− γ 1\nv̄′ ≥ v∗ − ǫp\n1− γ 1.\nThe difference between v̄ and v̄′ can be quantified as follows, using their same feasible sets and the fact that v̄ is minimal with respect to ρ̄:\nρTv̄ ≤ ρ̄Tv̄ + ǫc ≤ ρ̄ Tv̄′ + ǫc ≤ ρ Tv̄′ + 2ǫc.\nSince K(ǫp) ⊇ K we also have that ρTv̄′ ≤ ρTv̂. Then, using that v̂ ∈ K̄ and v̂ ≥ v∗:\n‖v̄ − v∗‖1,ρ ≤\n∥ ∥ ∥ ∥ v̄ − v∗ + ǫp\n1− γ 1−\nǫp\n1− γ 1\n∥ ∥ ∥ ∥\n1,ρ\n≤\n∥ ∥ ∥ ∥ v̄ − v∗ + ǫp\n1− γ 1\n∥ ∥ ∥ ∥\n1,ρ\n+ ǫp\n1− γ\n≤ ρT ( v̄ − v∗ + ǫp\n1− γ 1\n)\n+ ǫp\n1− γ\n≤ ρT (v̄ − v∗) + 2 ǫp\n1− γ\n≤ ρT (v̄′ − v∗) + 2ǫc + 2 ǫp\n1− γ\n≤ ρT (v̂ − v∗) + 2ǫc + 2 ǫp\n1− γ\n≤ ‖v̂ − v∗‖1,ρ + 2ǫc + 2 ǫp\n1− γ\nProof of ‖ṽ − v∗‖1,ρ: Let ṽ′ be the solution (7) but with ρ as the objective function. Using Lemma 12 we have:\nṽ ≥ L̃v ≥ L̄v − ǫs1 ≥ Lv − (ǫs + ǫp)1 ṽ ≥ v∗ − ǫs + ǫp 1− γ 1\nAll of these inequalities also hold for ṽ′ since it is also feasible in (7). From Lemma 11:\nv̂ + ǫs\n1− γ 1 ∈ K̃.\nTherefore:\nρTṽ′ ≤ ρTv̂ + ǫs\n1− γ\nThe difference between ṽ and ṽ′ can be bounded as follows, using their same feasible sets and the fact that ṽ is minimal with respect to ρ̄:\nρTṽ ≤ ρ̄Tṽ + ǫc ≤ ρ̄ Tṽ′ + ǫc ≤ ρ Tṽ′ + 2ǫc\nThen:\n‖ṽ − v∗‖1,ρ ≤\n∥ ∥ ∥ ∥ ṽ − v∗ + ǫs + ǫp 1− γ 1− ǫs + ǫp 1− γ 1 ∥ ∥ ∥ ∥\n1,ρ\n≤\n∥ ∥ ∥ ∥ ṽ − v∗ + ǫs + ǫp 1− γ 1 ∥ ∥ ∥ ∥\n1,ρ\n+ ǫs + ǫp 1− γ\n≤ ρT ( ṽ − v∗ + ǫs + ǫp 1− γ 1 ) + ǫs + ǫp 1− γ ≤ ρT (ṽ − v∗) + 2 ǫs + ǫp 1− γ ≤ ρT (ṽ′ − v∗) + 2ǫc + 2 ǫs + ǫp 1− γ ≤ ρT (v̂ − v∗) + 2ǫc + 3ǫs + 2ǫp 1− γ ≤ ‖v̂ − v∗‖1,ρ + 2ǫc + 3ǫs + 2ǫp 1− γ\nTheorem 9. [Online Error Bound] Assume Assumption 2 and let v ∈ K̃ ∩ M(ψ) be an arbitrary feasible solution of the estimated ALP (7). Then:\n‖v∗ − v‖1,ρ ≤ ρ̄ Tv − ρTv∗ + ǫc(ψ) + 2\nǫs(ψ) + ǫp(ψ)\n1− γ .\nProof. To simplify the notation, we omit ψ in the notation of ǫ in the proof. Using Lemma 12 we have:\nṽ ≥ L̃v ≥ L̄v − ǫs1 ≥ Lv − (ǫs + ǫp)1 ṽ ≥ v∗ − ǫs + ǫp 1− γ 1.\nThen, using the above:\n‖v − v∗‖1,ρ = ‖v ∗ − v + ǫs + ǫp 1− γ 1− ǫs + ǫp 1− γ 1‖1,ρ\n≤ ‖v − v∗ + ǫs + ǫp 1− γ 1‖1,ρ + ǫs + ǫp 1− γ = ρTv − ρv∗ + 2 ǫs + ǫp 1− γ = ρ̄Tv − ρv∗ + ǫc + 2 ǫs + ǫp 1− γ\nA.2. Sampling Guarantees\nLet k : S → Rn be a map of the state-space to an Euclidean space. This basically means that the states of the MDP can be mapped to an normed vector space. This assumption trivially generalizes Assumption 6.\nFeature Selection Using Regularization\nAssumption 14. Assume that samples Σ̄ are available. Assume also that features and transitions satisfy Lipschitz-type constraints:\n‖φ(s̄)− φ(s)‖∞ ≤ Kφ‖k(s)− k(s̄)‖\n|r(s̄)− r(s)| ≤ Kr‖k(s)− k(s̄)‖\n‖P (s̄, a)Tφi − P (s, a) Tφi‖∞ ≤ Kp‖k(s)− k(s̄)‖ ∀a ∈ A\nHere φi denotes a vector representation of a feature across all states.\nIt is easy to show the following:\nProposition 15. Assume Assumption 6 and that ‖φi‖1 = 1. Then Assumption 14 holds with the identical constants.\nThe proof is simple and relies on the trivial version of Holder’s inequality to combine ‖P (s̄, a)Tφi − P (s, a)Tφi‖∞ with ‖φi‖1.\nWhile Assumption 14 characterizes general properties of the MDP, the following assumption unifies the assumptions on the MDP and the sampling.\nAssumption 16 (Sufficient Sampling). Assume that for ∀s ∈ S, there exists ∃s̄ ∈ S̄, such that:\n• ‖φ(s̄)− φ(s)‖∞ ≤ δφ • |r(s̄)− r(s)| ≤ δr • ‖P (s̄, a)Tφi − P (s, a)Tφi‖∞ ≤ δp ∀a ∈ A\nwhere P (s, a) represents the vector of transition probabilities from states s given action a.\nTheorem 17. Assume Assumption 2 and Assumption 16. Let the value function v be represented as v ∈ M such that ‖x‖1 ≤ ψ. Then Assumption 4 is satisfied with the following constraints:\nǫp = δr + ψ(δφ + γδp)\nThe theorem also holds if a column of Φ is 1 and the corresponding element of x is not included in the norm.\nProof. From the assumptions in the theorem, we have that v ≥ L̄v and we need to show that v ≥ Lv − ǫp1.\nThen we get, using Holder’s inequality that:\nmin s∈S (v − Lv)(s) ≥\n≥ min s∈S (v − Lv)(s)− (v − L̄v)(s̄)\n≥ − max s∈S,a∈A\n|((φ(s) − γP (s, a)TΦ)\n− (φ(s̄)− γP (s̄, a)TΦ)x)− r(s) + r(s̄)|\n≥ − max s∈S,a∈A\n‖(φ(s)− γP (s, a)TΦ)\n− (φ(s̄)− γP (s̄, a)TΦ)‖∞‖x‖1 + |r(s) + r(s̄)|\n≥ − max s∈S,a∈A\n‖(φ(s)− γP (s, a)TΦ)\n− (φ(s̄)− γP (s̄, a)TΦ)‖∞ψ + δr\n≥ − max s∈S,a∈A ‖(φ(s)− φ(s)‖1 + γ‖P (s, a) TΦ)\n− (φ(s̄)− P (s̄, a)TΦ)‖∞ψ + δr\n≥ −(δφ + δp)ψ − δr\nThe following then summarizes the results.\nTheorem 7. Assume Assumption 6 and that for any s ∈ S there exists a state s̄ ∈ Σ̄ such that ‖s̄−s‖ ≤ c. Then Assumption 4 and Assumption 5 hold with ǫp(ψ) = cKr + cψ(Kφ + γKP )\nProof. The corollary follows simply by setting the following values.\nδφ = cKφ\nδr = cKr\nδp = cKP\nNotice that for simplicity, we did not provide any bounds on ǫc. These bounds require additional assumptions on the sampling procedure. That is the sampling must not only cover the whole space, but must also be uniformly distributed over it. Without such distribution, the objective function ρ must be a weighted sum of the states."
    }, {
      "heading" : "B. Homotopy Continuation Method",
      "text" : "The homotopy algorithm is similar to sensitivity analysis of the standard simplex algorithm. However, the basic feasible solutions in simplex are of the size of the number of variables. Because we are interested in solving very large linear programs, this is often impractical. The homotopy method we propose instead\nFeature Selection Using Regularization\nrelies on basic basic feasible solutions with size that corresponds to the number of variables.\nAs before, we use 1i to denote a zero vector with i-th element set to 1. For a matrix A, we use Aj to denote the j-th row and A·i as i-the column. We also use x(i) to denote the i-the element of the vector. We derive the algorithm for a generic linear program, defined as follows:\nmin x\ncTx\ns.t. Ax ≥ b\neTx ≤ ψ\nx ≥ 0\n(8)\nNote that the variables are constrained to be nonnegative. Any unbounded variable z can be expressed as z = z+ − z−, such that z+, z− ≥ 0. The homotopy algorithm also relies on the dual formulation of the linear program (8), which is as follows.\nmax y,λ\nbTy − ψλ\ns.t. ATy − eλ ≤ c\ny, λ ≥ 0\n(9)\nThe algorithm traces the optimal solution of the linear program (8) as a function of ψ, which is defined as follows.\nDefinition 18. The optimal solution of the linear program (8) as a function of ψ is denoted as x(ψ), assuming that the optimal solution is a singleton. Notice that this is the optimal solution, not the optimal objective value. We also use y(ψ) and λ(ψ) similarly to denote the sets of optimal solutions of the dual program (9) for the given the regularization coefficient.\nThe homotopy algorithm keeps a set of active variables B(x, y) and a set of active constraints C(x, y). A variable is considered to be active if it is non-zero. A constraint is considered to be active when the corresponding dual value is non-negative. Active and inactive variables and constraints are formally defined as follows.\nB(x, y) = {i x(i) ≥ 0} N (x, y) = {i x(i) = 0}\nC(x, y) = {j y(j) ≥ 0} D(x, y) = {j y(j) = 0}\nWe use B in place of B(x, y) when the values of x, y are apparent from the context. Notice that this definition is intentionally ambiguous, that is for a given value of x, y, the active variables and constraints are not uniquely specified. This is intentional, to allow for adding and removing them at the points of discontinuity. Although, active primal and dual variables may\nbe 0, the inactive variables always must be 0. The active variables and constraints can be used to define the following variables.\nc =\n(\ncB cN\n)\nx =\n(\nxB xN\n)\nb =\n(\nbC bD\n)\ny =\n(\nyC yD\n)\nA =\n(\nABC ANC ABD AND\n)\nWe assume the given order of variables.\nThe following assumptions are needed in order to derive the algorithm.\nAssumption 19. The optimal solution of (8) is feasible and bounded for values of ψ ∈ [0,∞). In addition, it is “easy” to solve for ψ = 0.\nAssumption 20. For any ψ ∈ [0,∞) the solution of (8) is not degenerate.\nThese assumptions guarantee that at no point the solutions become degenerate. This is a common assumption in simplex algorithms, and can be remedied for example by assuming a small perturbation of variables (Vanderbei, 2001).\nThe homotopy algorithm traces the optimal solution of the linear program, which can be characterized by a set of linear equations. These optimality conditions for (8) can be derived identically from KKT or from the complementary slackness optimality conditions as follows.\n−eTx ≥ −ψ\nAx ≥ b\nATy ≤ c+ λe\nyT(b −Ax) = 0\nλ(eT − ψ) = 0\nxT(c−ATy + eλ) = 0\nx, y, λ ≥ 0\nWithout loss of generality, we assume that the active constraints and variables are the first in the respective structures. This does not impose any limitations, and could be expressed generally using a permutation matrix P . We implicitly assume that the regularization vector e is partitioned properly for the active variables.\nFor a given set of active variables and constraints, and using the fact that the inactive variables x and y are 0, the optimality conditions can be then rewritten as:\neTxB = ψ\nABCxB = bC ABDxB ≤ bD\nATBCyC = cB + λe A T NCyC ≤ 0\nx, y, λ ≥ 0\nFeature Selection Using Regularization\nWe are assuming here that the regularization constraint is active. If it becomes inactive at ψ̄, the solution is optimal for any value of ψ ≥ ψ̄. The equalities follow from the complementarity conditions, omitted here. Notice that other constraints may also hold with equality.\nThe homotopy algorithm is included in Algorithm 1. The primal update of the algorithm traces the solution in the linear segments and the dual updates determines the update direction the sections with non-linearities. The algorithm implementation is written with the focus on simplicity; an efficient implementation relies on factorization of the matrices. Two homotopy continuation methods — DASSO (James et al., 2009) and primal dual pursuit (Asif, 2009) — have been proposed for the Dantzig selector. These homotopy methods are very efficient when the problems have sparse solutions, as is often the case with Dantzig selectors. RALP cannot be solved directly using the methods for the Dantzig selector, however, because of its different structure. This structure can be used to develop a different homotopy method for solving RALP, which is described in Appendix B. In addition, a method based on parametric linear program solvers has been developed for regularized linear programs. However, the parametric simplex algorithm cannot take advantage of sparse RALP solutions and therefore is not applicable to large RALP. The finite-time convergence of Algorithm 1 can be, however, shown identically to DASSO, or other related algorithms.\nFeature Selection Using Regularization\nψ0 ← 0 ;1 // Find an initial feasible solutions x0 ← x(ψ0) ;2 y0 ← y(ψ0) ;3 // Determine the initial active sets, and set N and D to be their complements B0 = {i x0 > 0} C0 ← {j y(j) > 0} // The regularization constraint is active, or the4\nsolution is optimal\nwhile ψi < ψ̄ and λi > 0 do5 i ← i+ 1 ;6 // Here |C| = |B|+ 2 // Calculate the space (line) of dual solutions -- the update direction (\n∆yi ∆λi\n)\n← null ( ATBC e ) such that yi−1(ψ) = 0 ⇒ ∆yi(ψ) ≥ 0 ; // This is always possible 7\nbecause there is always at most one such constraint, given the assumptions. // Decide based on a potential variable improvement // Calculate the maximum length of the update τ, breaking ties arbitrarily. t1 ← ( maxk∈C −∆yi k\ny i−1\nk\n)−1\n; // Some y becomes 0. 8\nt2 ←\n(\nmaxk∈N −(ANC∆yi) k\n(ANCyi−1)k\n)−1\n; // Some x needs to be added to the active set. 9\nt3 ← λ\n−∆λ ; // Regularization constraint10 τ = min {t1, t2, t3} // Resolve the non-linearity update, where Kl is the set of11 maximizers for tl if τ = t1 then12 Ci ← Ci−1 \\K1, Di ← (Ci)C13\nelse if τ = t2 then14 Bi ← Bi−1 ∪K2, N i ← (Bi)C15\nelse if τ = t3 then16 The regularization constraint is inactive, return the solution.17\n// Update the dual solutions\nyi ← yi−1 + τ∆yi, λi ← λi−1 + τ∆λi ;18 // Here |C| = |B|+ 1 // Calculate the update direction\n∆x ←\n(\nABC eB\n)−1( 0\n∆ψ\n)\n; 19\n// Calculate the maximum length of the update τ, breaking ties arbitrarily. t4 ← ( maxk∈D −aT k ∆xi\naT k xi−1−b\n)−1\n; // A constraint becomes active 20\nt5 ← ( maxk∈B −∆xi k\naT k xi−1−b\n)−1\n; // A variable τ = min {t4, t5}21\n// Update the primal solutions\nxi ← xi−1 + τ∆x i ;22 // Resolve the non-linearity update, where Kl is the set of maximizers for tl if τ = t4 then23 Ci ← Ci−1 ∪K4, Di ← (Ci)C24\nelse if τ = t5 then25 Bi ← Bi−1 \\K5, N i ← (Bi)C26\nAlgorithm 1. Homotopy Continuation Method for Solving ALP\n0 1000 2000 3000 4000 5000 0\n50\n100\nEpisodes\n% R\nea ch\ned G\noa l\n0 2 4 6 0\n0.5\n1\n1.5\nRegularization: ψ\nRALP Objective Value Max Violation % Active Variables\n1000 1500 2000 2500 3000 3500 0\n0.05\n0.1\n0.15\n0.2\n0.25\nFeatures\nT im\ne (s\n)\nMosek(R) Solver Homotopy"
    } ],
    "references" : [ {
      "title" : "Dantzig selector homotopy with dynamic measurements",
      "author" : [ "Asif", "Salman" ],
      "venue" : "In IS&T/SPIE Computational Imaging VII,",
      "citeRegEx" : "Asif and Salman.,? \\Q2009\\E",
      "shortCiteRegEx" : "Asif and Salman.",
      "year" : 2009
    }, {
      "title" : "Reinforcement Learning: an Introduction",
      "author" : [ "Barto", "Andrew G", "Sutton", "Richard S" ],
      "venue" : null,
      "citeRegEx" : "Barto et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Barto et al\\.",
      "year" : 1998
    }, {
      "title" : "The Dantzig selector:statistical estimation when p is much larger than n",
      "author" : [ "Candes", "Emmanuel", "Tao", "Terence" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Candes et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Candes et al\\.",
      "year" : 2007
    }, {
      "title" : "On constraint sampling for the linear programming approach to approximate dynamic programming",
      "author" : [ "de Farias", "Daniela Pucci", "Van Roy", "Benjamin" ],
      "venue" : "Math. of Operations Res,",
      "citeRegEx" : "Farias et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Farias et al\\.",
      "year" : 2001
    }, {
      "title" : "A smoothed approximate linear program",
      "author" : [ "Desai", "Vijay", "Farias", "Vivek", "Moallemi", "Ciamac" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Desai et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Desai et al\\.",
      "year" : 2009
    }, {
      "title" : "Regularized policy iteration",
      "author" : [ "Farahmand", "Amir Massoud", "Ghavamzadeh", "Mohammad", "Szepesvari", "Csaba", "Mannor", "Shie" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Farahmand et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Farahmand et al\\.",
      "year" : 2008
    }, {
      "title" : "DASSO: Connections between the Dantzig selector and lasso",
      "author" : [ "James", "Gareth M", "Radchenko", "Peter", "Lv", "Jinchi" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "James et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "James et al\\.",
      "year" : 2009
    }, {
      "title" : "Regularization and feature selection in least-squares temporal difference learning",
      "author" : [ "Kolter", "J. Zico", "Ng", "Andrew" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Kolter et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kolter et al\\.",
      "year" : 2009
    }, {
      "title" : "LeastSquares Policy Iteration",
      "author" : [ "Lagoudakis", "Michail G", "Parr", "Ronald" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Lagoudakis et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lagoudakis et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning representation and control in markov decision processes: New frontiers",
      "author" : [ "Mahadevan", "Sridhar" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Mahadevan and Sridhar.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mahadevan and Sridhar.",
      "year" : 2008
    }, {
      "title" : "Analyzing feature generation for value-function approximation",
      "author" : [ "Parr", "Ronald", "Painter-Wakefield", "Christopher", "Li", "Lihong", "Littman", "Michael" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Parr et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Parr et al\\.",
      "year" : 2007
    }, {
      "title" : "Constraint Relaxation in Approximate Linear Programs",
      "author" : [ "Petrik", "Marek", "Zilberstein", "Shlomo" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Petrik et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Petrik et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning to drive a bicycle using reinforcement learning and shaping",
      "author" : [ "Randløv", "Jette", "Alstrøm", "Preben" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Randløv et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Randløv et al\\.",
      "year" : 1998
    }, {
      "title" : "Generalized polynomial approximations in Markovian decision processes",
      "author" : [ "Schweitzer", "Paul J", "Seidmann", "Abraham" ],
      "venue" : "Journal of mathematical analysis and applications,",
      "citeRegEx" : "Schweitzer et al\\.,? \\Q1985\\E",
      "shortCiteRegEx" : "Schweitzer et al\\.",
      "year" : 1985
    }, {
      "title" : "Kernelized Value Function Approximation for Reinforcement Learning",
      "author" : [ "Taylor", "Gavin", "Parr", "Ronald" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Taylor et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Taylor et al\\.",
      "year" : 2009
    }, {
      "title" : "Regression shrinkage and selection via the LASSO",
      "author" : [ "Tibshirani", "Robert" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological), pp",
      "citeRegEx" : "Tibshirani and Robert.,? \\Q1996\\E",
      "shortCiteRegEx" : "Tibshirani and Robert.",
      "year" : 1996
    }, {
      "title" : "Linear Programming: Foundations and Extensions",
      "author" : [ "Vanderbei", "Robert J" ],
      "venue" : "Springer, 2nd edition,",
      "citeRegEx" : "Vanderbei and J.,? \\Q2001\\E",
      "shortCiteRegEx" : "Vanderbei and J.",
      "year" : 2001
    }, {
      "title" : "An approach to fuzzy control of nonlinear systems: Stability and design issues",
      "author" : [ "Wang", "Hua O", "Tanaka", "Kazuo", "Griffin", "Michael F" ],
      "venue" : "IEEE Transactions on Fuzzy Systems,",
      "citeRegEx" : "Wang et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Feature selection, therefore, seeks to automate this process in a way that may preserve the computational simplicity of linear approximation (Parr et al., 2007; Mahadevan, 2008).",
      "startOffset" : 141,
      "endOffset" : 177
    }, {
      "referenceID" : 4,
      "context" : "ALP has often under-performed ADP methods in practice; this issue has been recently studied and partially remedied (Petrik & Zilberstein, 2009; Desai et al., 2009).",
      "startOffset" : 115,
      "endOffset" : 163
    }, {
      "referenceID" : 17,
      "context" : "dulum, a standard benchmark problem in reinforcement learning (Wang et al., 1996; Lagoudakis & Parr, 2003).",
      "startOffset" : 62,
      "endOffset" : 106
    } ],
    "year" : 2010,
    "abstractText" : "Approximate dynamic programming has been used successfully in a large variety of domains, but it relies on a small set of provided approximation features to calculate solutions reliably. Large and rich sets of features can cause existing algorithms to overfit because of a limited number of samples. We address this shortcoming using L1 regularization in approximate linear programming. Because the proposed method can automatically select the appropriate richness of features, its performance does not degrade with an increasing number of features. These results rely on new and stronger sampling bounds for regularized approximate linear programs. We also propose a computationally efficient homotopy method. The empirical evaluation of the approach shows that the proposed method performs well on simple MDPs and standard benchmark problems.",
    "creator" : "LaTeX with hyperref package"
  }
}