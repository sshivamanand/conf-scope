{
  "name" : "1202.3762.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Symbolic Dynamic Programming for Discrete and Continuous State MDPs",
    "authors" : [ "Scott Sanner", "Karina Valdivia Delgado", "Leliane Nunes de Barros" ],
    "emails" : [ "ssanner@nicta.com.au", "kvd@ime.usp.br", "leliane@ime.usp.br" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Many real-world decision-theoretic planning problems can be naturally modeled with discrete and continuous state Markov decision processes (DC-MDPs). While previous work has addressed automated decision-theoretic planning for DCMDPs, optimal solutions have only been defined so far for limited settings, e.g., DC-MDPs having hyper-rectangular piecewise linear value functions. In this work, we extend symbolic dynamic programming (SDP) techniques to provide optimal solutions for a vastly expanded class of DCMDPs. To address the inherent combinatorial aspects of SDP, we introduce the XADD — a continuous variable extension of the algebraic decision diagram (ADD) — that maintains compact representations of the exact value function. Empirically, we demonstrate an implementation of SDP with XADDs on various DC-MDPs, showing the first optimal automated solutions to DCMDPs with linear and nonlinear piecewise partitioned value functions and showing the advantages of constraint-based pruning for XADDs."
    }, {
      "heading" : "1 Introduction",
      "text" : "Many real-world stochastic planning problems involving resources, time, or spatial configurations naturally use continuous variables in their state representation. For example, in the MARS ROVER problem [6], a rover must manage bounded continuous resources of battery power and daylight time as it plans scientific discovery tasks for a set of landmarks on a given day.\nWhile problems such as the MARS ROVER are naturally modeled by discrete and continuous state Markov decision processes (DC-MDPs), little progress seems to have been made in recent years in developing exact solutions for DC-MDPs with multiple continuous state variables be-\nyond the subset of DC-MDPs which have an optimal hyperrectangular piecewise linear value function [8, 11].\nYet even simple DC-MDPs may require optimal value functions that are piecewise functions with non-rectangular boundaries; as an illustration, we consider KNAPSACK:\nExample 1.1 (KNAPSACK). We have three continuous state variables: k ∈ [0, 100] indicating knapsack weight, and two sources of knapsack contents: xi ∈ [0, 100] for i ∈ {1, 2}. We have two actions movei for i ∈ {1, 2} that can move all of a resource from xi to the knapsack if the knapsack weight remains below its capacity of 100. We get an immediate reward for any weight added to the knapsack.\nWe can formalize the transition and reward for KNAPSACK action movei (i ∈ {1, 2}) using conditional equations, where (k, x1, x2) and (k′, x′1, x ′ 2) are respectively the preand post-action state and R is immediate reward:\nk′ = ( k + xi ≤ 100 : k + xi k + xi > 100 : k\nR = ( k + xi ≤ 100 : xi k + xi > 100 : 0\nx′i = ( k + xi ≤ 100 : 0 k + xi > 100 : xi\nx′j = xj , (j 6= i)\nIf our objective is to maximize the long-term value V (i.e., the sum of rewards received over an infinite horizon of actions), then we can write the optimal value achievable from a given state in KNAPSACK as a function of state variables:\nV = 8>>>><>>>>: x1 + k > 100 ∧ x2 + k > 100 : 0 x1 + k > 100 ∧ x2 + k ≤ 100 : x2 x1 + k ≤ 100 ∧ x2 + k > 100 : x1 x1 + k ≤ 100 ∧ x2 + k ≤ 100 ∧ x2 > x1 : x2 x1 + k ≤ 100 ∧ x2 + k ≤ 100 ∧ x2 ≤ x1 : x1 x1 + x2 + k ≤ 100 : x1 + x2 (1)\nOne will see that this encodes the following rules (in order): (a) if both resources are too large for the knapsack, 0 reward is obtained, (b) otherwise if only one item can fit,\nthe reward is for the largest item that fits, (c) otherwise if both items can fit then reward x1 + x2 is obtained. Here we note that the value function is piecewise linear, but it contains decision boundaries like x1 + x2 + k ≤ 100 that are clearly non-rectangular; rectangular boundaries are restricted to conjunctions of simple inequalities of a continuous variable and a constant (e.g., x1 ≤ 5∧x2 > 2∧k ≥ 0).\nWhat is interesting to note is that although KNAPSACK is very simple, no previous algorithm in the DC-MDP literature has been proposed to exactly solve it due to the nature of its non-rectangular piecewise optimal value function. Of course our focus in this paper is not just on KNAPSACK — researchers have spent decades finding improved solutions to this particular combinatorial optimization problem— but rather on general stochastic sequential optimization in DC-MDPs that contain structure similar to KNAPSACK, as well as highly nonlinear structure beyond KNAPSACK. Both types of problem structure are exemplified in the MARS ROVER problems we experiment on later.\nIn proposing a solution to these problems, an important question arises: if the solution to KNAPSACK is simple and intuitive, why is it beyond the reach of existing exact DCMDP solutions? In response, it seems that it has not been clear what value function representation supports closedform computation of the Bellman backup (regression and maximization operations) for general DC-MDP transition and reward structures. These questions have been affirmatively addressed for the subset of DC-MDPs with transition functions that are mixtures of delta functions and reward functions that are hyper-rectangular piecewise linear, which provably lead to value functions of the same structure [8, 11]. However, the literature appears to lack a solution to this problem when, for example, the reward instead uses piecewise nonlinear functions with linear or nonlinear boundaries, leading to value functions of similar structure.\nIn this paper, we propose novel ideas to workaround some of the expressiveness limitations of previous approaches and significantly generalize the range of DC-MDPs that can be solved exactly. To achieve this more general solution, this paper contributes a number of important advances:\n• We propose to represent the transition function of a DC-MDP using conditional stochastic equations; in using this formalism, we observe that many aspects of the proposed symbolic DC-MDP solution become readily apparent.\n• The use of conditional stochastic equations facilitates symbolic regression of the value function via substitutions. This is precisely the motivation behind symbolic dynamic programming (SDP) [4] used to solve MDPs with transitions and reward functions defined in first-order logic, except that in prior SDP work, only piecewise constant functions have been used; in this\nwork we introduce techniques for working with arbitrary piecewise symbolic functions.\n• While the case representation for the optimal KNAPSACK solution shown in (1) is sufficient in theory to represent the optimal value functions that our DCMDP solution produces, this representation is unreasonable to maintain in practice since the number of case partitions may grow exponentially on each receding horizon control step. For discrete factored MDPs, algebraic decision diagrams (ADDs) [1] have been successfully used in exact algorithms like SPUDD [9] to maintain compact value representations. Motivated by this work we introduce extended ADDs (XADDs) to compactly represent general piecewise functions and show how to perform efficient operations on them including symbolic maximization. We also borrow techniques from [14] for constraint-based pruning of XADDs that can be applied when XADDs meet certain expressiveness restrictions.\nAided by these algorithmic and data structure advances, we empirically demonstrate that our SDP approach with XADDs can exactly solve a variety of DC-MDPs with general piecewise linear and nonlinear value functions for which no previous analytical solution has been proposed."
    }, {
      "heading" : "2 Discrete and Continuous State MDPs",
      "text" : "We first introduce discrete and continuous state Markov decision processes (DC-MDPs) and then review their finitehorizon solution via dynamic programming following [11]."
    }, {
      "heading" : "2.1 Factored Representation",
      "text" : "In a DC-MDP, states will be represented by vectors of variables (~b, ~x) = (b1, . . . , bn, x1, . . . , xm). We assume that each state variable bi (1 ≤ i ≤ n) is boolean s.t. bi ∈ {0, 1} and each xj (1 ≤ j ≤ m) is continuous s.t. xj ∈ [Lj , Uj ] for Lj , Uj ∈ R;Lj ≤ Uj . We also assume a finite set of actions A = {a1, . . . , ap}.\nA DC-MDP is defined by the following: (1) a state transition model P (~b′, ~x′| · · · , a), which specifies the probability of the next state (~b′, ~x′) conditioned on a subset of the previous and next state (defined below) and action a; (2) a reward function R(~b, ~x, a), which specifies the immediate reward obtained by taking action a in state (~b, ~x); and (3) a discount factor γ, 0 ≤ γ ≤ 1.1 A policy π specifies the action π(~b, ~x) to take in each state (~b, ~x). Our goal is to find an optimal sequence of horizon-dependent policies Π∗ = (π∗,1, . . . , π∗,H) that maximizes the expected sum\n1If time is explicitly included as one of the continuous state variables, γ = 1 is typically used, unless discounting by horizon (different from the state variable time) is still intended.\nof discounted rewards over a horizon h ∈ H;H ≥ 0:2\nV Π ∗ (~x) = Eπ∗ [ H∑ h=0 γh · rh ∣∣∣~b0, ~x0] , (2)\nHere rh is the reward obtained at horizon h following Π∗ where we assume starting state (~b0, ~x0) at h = 0.\nDC-MDPs as defined above are naturally factored [3] in terms of state variables (~b, ~x); as such transition structure can be exploited in the form of a dynamic Bayes net (DBN) [7] where the individual conditional probabilities P (b′i| · · · , a) and P (x′j | · · · , a) condition on a subset of the variables in the current and next state. We disallow synchronic arcs (variables that condition on each other in the same time slice) within the binary ~b and continuous variables ~x, but we allow synchronic arcs from~b to ~x (note that these conditions enforce the directed acyclic graph requirements of DBNs). Thus, the joint transition model can be specified as\nP (~b′,~x′| · · · , a) = (3) n∏ i=1 P (b′i|~b, ~x, a) m∏ j=1 P (x′j |~b,~b′, ~x, a)\nwhere P (b′i|~b, ~x, a) may condition on a subset of ~b and ~x and likewise P (x′j |~b,~b′, ~x, a) may condition on a subset of ~b,~b′, and ~x.\nAs for standard finite discrete factored MDPs, the conditional probabilities P (b′i|~b, ~x, a) for binary variables bi (1 ≤ i ≤ n) can be represented by conditional probability tables (CPTs). For the continuous variables xj (1 ≤ j ≤ m), we represent the continuous probability functions (CPFs) P (x′j |~b, ~b′, ~x, a) with conditional stochastic equations (CSEs). For the solution provided here, we only require two properties of these CSEs: (1) they are Markov, meaning that they can only condition on the previous state, and (2) they are deterministic meaning that the next state must be uniquely determined from the previous state (i.e., x′1 = x1 + x 2 2 is deterministic whereas x ′2 1 = x 2 1 is not because x′1 = ±x1).3 Otherwise, we allow for arbitrary functions in these Markovian, conditional deterministic equations as in the following example:\nP (x′1|~b,~b′, ~x, a) = δ \" x′1 − ( b′1 ∧ x22 ≤ 1 : exp(x21 − x22) ¬b′1 ∨ x22 > 1 : x1 + x2 # (4)\nHere the use of the Dirac δ[·] function ensures that this is a conditional probability function that integrates to 1 over\n2H = ∞ is allowed if an optimal policy has a finitely bounded value (guaranteed if γ < 1); for H = ∞, the optimal policy is independent of horizon, i.e., ∀h ≥ 0, π∗,h = π∗,h+1.\n3While the deterministic requirement may seem to conflict with the label of stochastic, we note that stochasticity enters through the conditional component, to be discussed in a moment.\nx′1 in this case. But in more intuitive terms, one can see that this δ[·] encodes the deterministic transition equation x′1 = . . . where . . . is the conditional portion of (4). In this work, we require all CSEs in the transition function for variable x′i to use the δ[·] as shown in this example.\nIt will be obvious that CSEs in the form of (4) are conditional equations; they are furthermore stochastic because they can condition on boolean random variables in the same time slice that are stochastically sampled, e.g., b′1 in (4). Of course, these CSEs are restricted in that they cannot represent general stochastic noise (e.g., Gaussian noise), but we note that this representation effectively allows modeling of continuous variable transitions as a mixture of δ functions, which has been used heavily in previous exact DC-MDP solutions [8, 11, 13]. Furthermore, we note that our representation is more general than [8, 11, 13] in that we do not restrict the equation to be linear, but rather allow it to specify arbitrary functions (e.g., nonlinear) as demonstrated in (4).\nWe allow the reward function Ra(~b, ~x) to be any arbitrary function of the current state for each action a ∈ A, for example:\nRa(~b, ~x) =\n{ x21 + x 2 2 ≤ 1 : 1− x21 − x22\nx21 + x 2 2 > 1 : 0\n(5)\nor even\nRa(~b, ~x) = 10x3x4 exp(x21 + √ x2) (6)\nWhile our DC-MDP examples throughout the paper will demonstrate the full expressiveness of our symbolic dynamic programming approach, we note that there are computational advantages to be had when the reward and transition case conditions and functions can be restricted, e.g., to polynomials. We will return to this issue later."
    }, {
      "heading" : "2.2 Solution Methods",
      "text" : "Now we provide a continuous state generalization of value iteration [2], which is a dynamic programming algorithm for constructing optimal policies. It proceeds by constructing a series of h-stage-to-go value functions V h(~b, ~x). Initializing V 0(~b, ~x) (e.g., to V 0(~b, ~x) = 0) we define the quality of taking action a in state (~b, ~x) and acting so as to obtain V h(~b, ~x) thereafter as the following:\nQh+1a (~b, ~x) = Ra(~b, ~x) + γ· (7)X ~b′ Z ~x′ nY i=1 P (b′i|~b, ~x, a) mY j=1 P (x′j |~b,~b′, ~x, a) ! V h(~b′, ~x′)d~x′\nGiven Qha(~b, ~x) for each a ∈ A, we can proceed to define the h+ 1-stage-to-go value function as follows:\nV h+1(~b, ~x) = max a∈A\n{ Qh+1a (~b, ~x) } (8)\nIf the horizonH is finite, then the optimal value function is obtained by computing V H(~b, ~x) and the optimal horizondependent policy π∗,h at each stage h can be easily determined via π∗,h(~b, ~x) = argmaxaQha(~b, ~x). If the horizon H = ∞ and the optimal policy has finitely bounded value, then value iteration can terminate at horizon h+1 if V h+1 = V h; then π∗(~b, ~x) = argmaxaQh+1a (~b, ~x).\nOf course this is simply the mathematical definition. In the discrete-only case, we can always compute this in tabular form; however, how to compute this for DC-MDPs with reward and transition function as previously defined is the objective of the symbolic dynamic programming algorithm that we define next."
    }, {
      "heading" : "3 Symbolic Dynamic Programming",
      "text" : "As it’s name suggests, symbolic dynamic programming (SDP) [4] is simply the process of performing dynamic programming (in this case value iteration) via symbolic manipulation. While SDP as defined in [4] was previously only used with piecewise constant functions, we now generalize the representation to work with general piecewise functions needed for DC-MDPs in this paper.\nBefore we define our solution, however, we must formally define our case representation and symbolic case operators."
    }, {
      "heading" : "3.1 Case Representation and Operators",
      "text" : "Throughout this paper, we will assume that all symbolic functions can be represented in case form as follows:\nf =  φ1 f1 ...\n... φk fk\nHere the φi are logical formulae defined over the state (~b, ~x) that can include arbitrary logical (∧,∨,¬) combinations of (a) boolean variables in ~b and (b) inequalities (≥, >,≤, <), equalities (=), or disequalities ( 6=) where the left and right operands can be any function of one or more variables in ~x. Each φi will be disjoint from the other φj (j 6= i); however the φi may not exhaustively cover the state space, hence f may only be a partial function and may be undefined for some state assignments. The fi can be any functions of the state variables in ~x.\nAs concrete examples, consider the transition representation for KNAPSACK in Ex. 1.1, the optimal value function for KNAPSACK from (1), or any of (4), (5), or (6).\nUnary operations such as scalar multiplication c · f (for some constant c ∈ R) or negation −f on case statements f are straightforward; the unary operation is simply applied to each fi (1 ≤ i ≤ k). Intuitively, to perform a binary operation on two case statements, we simply take the cross-\nproduct of the logical partitions of each case statement and perform the corresponding operation on the resulting paired partitions. Letting each φi and ψj denote generic first-order formulae, we can perform the “cross-sum” ⊕ of two (unnamed) cases in the following manner:\n( φ1 : f1 φ2 : f2 ⊕ ( ψ1 : g1 ψ2 : g2 = 8>><>>: φ1 ∧ ψ1 : f1 + g1 φ1 ∧ ψ2 : f1 + g2 φ2 ∧ ψ1 : f2 + g1 φ2 ∧ ψ2 : f2 + g2\nLikewise, we can perform and ⊗ by, respectively, subtracting or multiplying partition values (as opposed to adding them) to obtain the result. Some partitions resulting from the application of the ⊕, , and ⊗ operators may be inconsistent (infeasible); we may simply discard such partitions as they are irrelevant to the function value.\nFor SDP, we’ll also need to perform maximization, restriction, and substitution on case statements. Symbolic maximization is fairly straightforward to define:\nmax ( φ1 : f1 φ2 : f2 , ( ψ1 : g1 ψ2 : g2 ! = 8>>>>>><>>>>>>: φ1 ∧ ψ1 ∧ f1 > g1 : f1 φ1 ∧ ψ1 ∧ f1 ≤ g1 : g1 φ1 ∧ ψ2 ∧ f1 > g2 : f1 φ1 ∧ ψ2 ∧ f1 ≤ g2 : g2 φ2 ∧ ψ1 ∧ f2 > g1 : f2 φ2 ∧ ψ1 ∧ f2 ≤ g1 : g1 φ2 ∧ ψ2 ∧ f2 > g2 : f2 φ2 ∧ ψ2 ∧ f2 ≤ g2 : g2\nOne can verify that the resulting case statement is still within the case language defined previously. At first glance this may seem like a cheat and little is gained by this symbolic sleight of hand. However, simply having a case partition representation that is closed under maximization will facilitate the closed-form regression step that we need for SDP. Furthermore, the XADD that we introduce later will be able to exploit the internal decision structure of this maximization to represent it much more compactly.\nThe next operation of restriction is fairly simple: in this operation, we want to restrict a function f to apply only in cases that satisfy some formula φ, which we write as f |φ. This can be done by simply appending φ to each case partition as follows:\nf = 8><>: φ1 : f1 ... ... φk : fk\nf |φ = 8><>: φ1 ∧ φ : f1 ... ... φk ∧ φ : fk\nClearly f |φ only applies when φ holds and is undefined otherwise, hence f |φ is a partial function unless φ ≡ >.\nThe final operation that we need to define for case statements is substitution. Symbolic substitution simply takes a set σ of variables and their substitutions, e.g., σ = {x′1 = x1 + x2, x′2 = x 2 1 exp(x2)} where the LHS of the = represents the substitution variable and the RHS of the = represents the expression that should be substituted in its place.\nNo variable occurring in any RHS expression of σ can also occur in any LHS expression of σ. We write the substitution of a non-case function fi with σ as fiσ; as an example, for the σ defined previously and fi = x′1 + x ′ 2 then fiσ = x1 + x2 + x21 exp(x2) as would be expected. We can also substitute into case partitions φj by applying σ to its LHS and RHS operands; as an example, if φj ≡ x′1 ≤ exp(x′2) then φjσ ≡ x1 + x2 ≤ exp(x21 exp(x2)). Having now defined substitution of σ for non-case functions fi and case partitions φj we can define it for case statements in general:\nf = 8><>: φ1 : f1 ... ... φk : fk\nfσ = 8><>: φ1σ : f1σ ... ... φkσ : fkσ\nOne property of substitution is that if f has mutually exclusive partitions φi (1 ≤ i ≤ k) then fσ must also have mutually exclusive partitions — this follows from the logical consequence that if φ1∧φ2 |= ⊥ then φ1σ∧φ2σ |= ⊥."
    }, {
      "heading" : "3.2 Symbolic Dynamic Programming (SDP)",
      "text" : "In the SDP solution for DC-MDPs, our objective will be to take a DC-MDP as defined in Section 2, apply value iteration as defined in Section 2.2, and produce the final value optimal function V h at horizon h in the form of a case statement.\nFor the base case of h = 0, we note that setting V 0(~b, ~x) = 0 (or to the reward case statement, if not action dependent) is trivially in the form of a case statement.\nNext, h > 0 requires the application of SDP. Fortunately, given our previously defined operations, SDP is straightforward and can be divided into four steps:\n1. Prime the Value Function: Since V h will become the “next state” in value iteration, we setup a substitution σ = {b1 = b′1, . . . , bn = b′n, x1 = x′1, . . . , xm = x′m} and obtain V ′h = V hσ.\n2. Continuous Integration: Now that we have our primed value function V ′h in case statement format defined over next state variables (~b′, ~x′), we first evaluate the integral marginalization ∫ ~x′\nover the continuous variables in (7). Because the lower and upper integration bounds are respectively −∞ and∞ and we have disallowed synchronic arcs between variables in ~x′ in the transition DBN, we can marginalize out each x′j independently, and in any order. Using variable elimination [17], when marginalizing over x′j we can factor out any functions independent of x′j — that is, for ∫ x′j in (7), one can see that initially, the only functions that can include x′j are V\n′h and P (x′j |~b,~b′, ~x, a) = δ[x′j − g(~x)]; hence, the first marginal over x′j need only be computed over δ[x′j − g(~x)]V ′h.\nWhat follows is one of the key novel insights of SDP in the context of DC-MDPs — the integration∫ x′j δ[x′j − g(~x)]V ′hdx′j simply triggers the substitution σ = {x′j = g(~x)} on V ′h, that is∫ x′j δ[x′j − g(~x)]V ′hdx′j = V ′h{x′j = g(~x)}. (9)\nThus we can perform the operation in (9) repeatedly in sequence for each x′j (1 ≤ j ≤ m) for every action a. The only additional complication is that the form of P (x′j |~b, ~x, a) is a conditional equation, c.f. (4), and represented generically as follows:\nP (x′j |~b, ~x, a) = δ x′j =  φ1 : f1 ...\n... φk : fk  (10) Hence to perform (9) on this more general representation, we obtain that ∫ x′j P (x′j |~b, ~x, a)V ′hdx′j\n=  φ1 : V ′h{x′j = f1} ...\n... φk : V ′h{x′j = fk}\nIn effect, we can read (10) as a conditional substitution, i.e., in each of the different previous state conditions φi (1 ≤ i ≤ k), we obtain a different substitution for x′j appearing in V\n′h (i.e., σ = {x′j = fi}). Here we note that because V ′h is already a case statement, we simply replace the single partition φi with the multiple partitions of V {x′j = fi}|φi . This reduces the nested case statement back down to a non-nested case statement as in the following example: φ1 : { ψ1 : f11 ψ2 : f12 φ2 : { ψ1 : f21 ψ2 : f22 =  φ1 ∧ ψ1 : f11 φ1 ∧ ψ2 : f12 φ2 ∧ ψ1 : f21 φ2 ∧ ψ2 : f22\nTo perform the full continuous integration, if we initialize Q̃h+1a := V\n′h for each action a ∈ A, and repeat the above integrals for all x′j , updating Q̃ h+1 a each time, then after elimination of all x′j (1 ≤ j ≤ m), we will have the partial regression of V ′h for the continuous variables for each action a denoted by Q̃h+1a .\n3. Discrete Marginalization: Now that we have our partial regression Q̃h+1a for each action a, we proceed to derive the full backup Qh+1a from Q̃ h+1 a by evaluat-\ning the discrete marginalization ∑\n~b′ in (7). Because we previously disallowed synchronic arcs between the variables in ~b′ in the transition DBN, we can sum out\neach variable b′i (1 ≤ i ≤ n) independently. Hence, initializing Qh+1a := Q̃ h+1 a we perform the discrete regression by applying the following iterative process for each b′i in any order for each action a:\nQh+1a := [ Qh+1a ⊗ P (b′i|~b, ~x, a) ] |b′i=>\n⊕ [ Qh+1a ⊗ P (b′i|~b, ~x, a) ] |b′i=⊥. (11)\nThis requires a variant of the earlier restriction operator |v that actually sets the variable v to the given value if present. Note that both Qh+1a and P (b ′ i|~b, ~x, a) can be represented as case statements (discrete CPTs are case statements), and each operation produces a case statement. Thus, once this process is complete, we have marginalized over all ~b′ and Qh+1a is the symbolic representation of the intended Q-function.\n4. Maximization: Now that we haveQh+1a in case format for each action a ∈ {a1, . . . , ap}, obtaining V h+1 in case format as defined in (8) requires sequentially applying symbolic maximization as defined previously:\nV h+1 = max(Qh+1a1 ,max(. . . ,max(Q h+1 ap−1 , Q h+1 ap )))\nBy induction, because V 0 is a case statement and applying SDP to V h in case statement form produces V h+1 in case statement form, we have achieved our intended objective with SDP. On the issue of correctness, we note that each operation above simply implements one of the dynamic programming operations in (7) or (8), so correctness simply follows from verifying (a) that each case operation produces the correct result and that (b) each case operation is applied in the correct sequence as defined in (7) or (8).\nOn a final note, we observe that SDP holds for any symbolic case statements; we have not restricted ourselves to rectangular piecewise functions, piecewise linear functions, or even piecewise polynomial functions. As the SDP solution is purely symbolic, SDP applies to any DC-MDPs using bounded symbolic function that can be written in case format! Of course, that is the theory, next we meet practice."
    }, {
      "heading" : "4 Extended ADDs (XADDs)",
      "text" : "In practice, it can be prohibitively expensive to maintain a case statement representation of a value function with explicit partitions. Motivated by the SPUDD [9] algorithm which maintains compact value function representations for finite discrete factored MDPs using algebraic decision diagrams (ADDs) [1], we extend this formalism to handle continuous variables in a data structure we refer to as the XADD. An example XADD for the optimal KNAPSACKvalue function from (1) is provided in Figure 1.\nIn brief we note that an XADD is like an ADD except that (a) the decision nodes can have arbitrary inequalities,\nequalities, or disequalities (one per node) and (b) the leaf nodes can represent arbitrary functions. The decision nodes still have a fixed order from root to leaf and the standard ADD operations to build a canonical ADD (REDUCE) and to perform a binary operation on two ADDs (APPLY) still applies in the case of XADDs.\nWhile exact solutions using symbolic dynamic programming are possible in principle for arbitrary symbolic CSE transition and reward functions, we note that it is much more difficult to devise a canonical and compact form for representations such as (6) in comparison to (5). Hence while we have used general examples throughout the paper to demonstrate the expressiveness of our approach, we will restrict XADDs to use polynomial functions only. We note the main advantage of this for the XADD is that we can put the leaf and decision nodes in a unique, canonical form, which allows us to minimize redundancy in the XADD representation of a case statement.\nIt is fairly straightforward for XADDs to support all case operations required for SDP. Standard operations like unary multiplication, negation,⊕, and⊗ are implemented exactly as they are for ADDs. The fact that the decision nodes have internal structure is irrelevant, although this means that certain paths in the XADD may be inconsistent or infeasible (due to parent decisions). To remedy this, when the XADD has only linear decision nodes, we can use the feasibility checkers of a linear programming solver (e.g., as also done in [14]) to prune unreachable nodes in the XADD; later we show results demonstrating impressive reductions in XADD size using this style of pruning.\nThe only two XADD operations that pose difficulty are substitution and maximization. In principle substitution is simple, the only caveat is that substitutions modify the decision nodes and hence decision nodes may become unordered. We can use the recursive application of ADD binary operations ⊗ and ⊕ as given in Algorithm 1 to correctly reorder the nodes in an XADD F after substitution. A related reordering issue occurs during XADDmaximization; because XADD maximization can introduce new de-\nAlgorithm 1: REORDER(F) input : F (root node for possibly unordered XADD) output: Fr (root node for an ordered XADD)\nbegin //if terminal node, return canonical terminal node if F is terminal node then\nreturn canonical terminal node for polynomial of F ;\n//nodes have a true & false branch and var id if F → Fr is not in Cache then\nFtrue = REORDER(Ftrue) ⊗ I[Fvar ] ; Ffalse = REORDER(Ffalse) ⊗ I[¬Fvar ]; Fr = Ftrue ⊕ Ffalse ; insert F → Fr in Cache;\nreturn Fr; end\ncision nodes (which occurs at the leaf when two leaf functions are compared) and these decision nodes may be out of order w.r.t. the diagram, reordering as defined in Algorithm 1 must also be applied after maximization.\nOn a final note, we mention that an implementation of case statements without any attempt to merge and simplify cases often cannot get past the first or second iteration of SDP; as our results show next, XADDs allow SDP to perform quite well in practice."
    }, {
      "heading" : "5 Empirical Results",
      "text" : "We implemented two versions of our proposed SDP algorithm using XADDs — one that does not prune nodes of the XADD and another that uses a linear programming solver to prune unreachable nodes (for problems with linear XADDs) — and we tested these algorithms on KNAPSACK and two versions of the Mars Rover domain (adapted from [6]) that we call MARS ROVER LINEAR and MARS ROVER NONLINEAR.4"
    }, {
      "heading" : "5.1 Domains",
      "text" : "In a general MARS ROVER domain, a rover is supposed to approach one or more target points and take images of these points. Actions may consume time and energy. There are also some domain constraints, e.g., some pictures can be taken only in a certain time window and can require different levels of energy to be performed. Next we describe the two domain variants we use.\n4While space limitations prevent a self-contained description of all domains, we note that all Java source code and a human/machine readable file format for all domains needed to reproduce the results in this paper can be found online at http://code.google.com/p/xadd-inference.\nMARS ROVER LINEAR This version has two continuous variables, time t and energy e. For each target point i (i = 1 . . . k), there is a boolean variable pi indicating whether the rover is at point i.\nThere are k(k−1) actions movei that move the rover from point i to point j 6= i. There are another k actions take-pici that take a picture at point i, which are conditioned on linear expressions over the time and energy variables. The reward is also a function of time and energy, e.g., the reward for action take-pici is given by:\nRtake-pici(e, t, pi) = 8><>: (e > 3 + 0.0002t) ∧ (t ≥ 3600) ∧ (t ≤ 50400) ∧ pi : 110 otherwise : 0\nwhich shows that to get a reward of 110, the rover must take a picture at point i between times 3600 and 50400 with a required energy reserve that increases as the day progresses.\nMARS ROVER NONLINEAR This version has two different continuous variables — geographic coordinates (x, y) — and k boolean variables hi for each picture point i indicating whether the rover has already taken a picture of point i. There is a single move action in this domain — it simply reduces the distance from the rover to a specific point by 13 of the current distance; for all experiments, this target point was set to (0, 0). The intent of this action is to represent the fact that a rover may move progressively more slowly as it approaches a target position in order to reach the position with high accuracy. take-pici actions are the same from MARS ROVER LINEAR domain but conditioned by nonlinear expressions over the continuous x and y variables. The reward is also a function of x and y, e.g., the reward for action take-pici is given by:\nRtake-pici(x, y, hi) = 8><>: x2 + y2 < 4 ∧ hi : 0 x2 + y2 < 4 ∧ ¬hi : 4− x2 − y2\nx2 + y2 ≥ 4 : 0 (12)\nwhich indicates that if the rover has not already taken a picture of point i and the rover is within a radius of 2 from the picture point (0, 0), then the rover receives a reward that is quadratically proportional to the distance from the picture point. Hence for various points, the rover has to trade-off whether to take each picture at its current position, or to get a larger reward by first moving and potentially getting closer before taking the picture."
    }, {
      "heading" : "5.2 Results",
      "text" : "For the MARS ROVER domains, we have run experiments to evaluate our SDP solution in terms of time and space cost while varying the horizon and problem size.\nBecause the reward and transition functions for MARS ROVER LINEAR use piecewise linear case statements, we note the optimal value function in this domain is also piecewise linear. Hence in this domain, we use a linear constraint feasibility checker to prune unreachable paths in the XADD — later we will compare solutions for MARS ROVER LINEAR with and without this pruning.\nIn Figure 2, for both the MARS ROVER LINEAR and MARS ROVER NONLINEAR domains, we show how the number of nodes of the value function XADD (proportional to the space required to represent the value function) varies for each iteration (horizon) and different problem sizes (given by the number of pictures). We first note that the nonlinear variant appears much harder for SDP (much more time required and larger value functions) than for the linear variant — this is largely due to the fact that the XADD can be optimally pruned in the linear variant. Secondly, we note an apparent superlinear growth in space and time required to solve each problem as a function of the number of picture points — this likely reflects the superlinear growth of combinations of pictures that must be jointly considered as the number of pictures increases. Finally, from these graphs it is hard to summarize general algorithm behavior as a function of horizon, but it appears for the linear problem variant that both the time and space grow linearly as a function of horizon — this will be confirmed in the next experiments.\nFigure 3 shows the amount of time for each iteration of\nSDP vs. horizon for MARS ROVER LINEAR with three picture points for SDP with and without XADD pruning. Here we see an impressive reduction in time and space as a function of horizon with pruning. Without pruning, both time and space grow super-linearly with the horizon, while with pruning time and space appear to grow linearly with the horizon.\nIn Figure 4, we show the exact optimal value function on the vertical axis for three domains, KNAPSACK (from Section 1), MARS ROVER LINEAR and MARS ROVER NONLINEAR, as a function of two continuous state variables shown on the horizontal axes. We notice here that the piecewise boundaries for all three plots clearly demonstrate non-rectangular boundaries. In particular, the value function plot for the MARS ROVER NONLINEAR domain demonstrates nonlinear piecewise boundaries with each piece being a nonlinear function of the state — it has the shape of stacked quadratic cones with each lower cone representing the cost of first moving from points farther away from the picture being receiving the value for taking the picture within the radius limits from (12).\nTo the best of our knowledge, these results demonstrate the first exact analytical solutions for DC-MDPs having optimal value functions with general linear and even nonlinear piecewise boundaries."
    }, {
      "heading" : "6 Related Work",
      "text" : "The most relevant vein of Related work is that of [8] and [11] which can perform exact dynamic programming on DC-MDPs with rectangular piecewise linear reward and transition functions that are delta functions. While SDP can solve these same problems, it removes both the rectangularity and piecewise restrictions on the reward and value functions, while retaining exactness. Heuristic search approaches with formal guarantees like HAO* [13] are an attractive future extension of SDP; in fact HAO* currently uses the method of [8], which could be directly replaced with SDP. While [14] has considered general piecewise functions with linear boundaries (and in fact, we borrow our linear pruning approach from this paper), this work only applied to fully deterministic settings, not DC-MDPs.\nOther work has analyzed limited DC-MDPS having only one continuous variable. Clearly rectangular restrictions are meaningless with only one continuous variable, so it is not surprising that more progress has been made in this restricted setting. One continuous variable can be useful for optimal solutions to time-dependent MDPs (TMDPs) [5]. Or phase transitions can be used to arbitrarily approxi-\nmate one-dimensional continuous distributions leading to a bounded approximation approach for arbitrary single continuous variable DC-MDPs [12]. While this work cannot handle arbitrary stochastic noise in its continuous distribution, it does exactly solve DC-MDPs with multiple continuous dimensions.\nFinally, there are a number of general DC-MDP approximation approaches that use approximate linear programming [10] or sampling in a reinforcement learning style approach [15]. In general, while approximation methods are quite promising in practice for DC-MDPS, the objective of this paper was to push the boundaries of exact solutions; however, in some sense, we believe that more expressive exact solutions may also inform better approximations, e.g., by allowing the use of data structures with nonrectangular piecewise partitions that allow higher fidelity approximations."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In this paper, we introduced a conditional stochastic equation model for the continuous part of the transition function in DC-MDPs. This representation facilitated the use of\nsymbolic dynamic programming techniques to generate exact solutions to DC-MDPs with arbitrary reward functions and expressive nonlinear transition functions that far exceeds the exact solutions possible with existing DC-MDP solvers. In an effort to make SDP practical, we also introduced the novel XADD data structure for representing arbitrary piecewise symbolic value functions and we addressed the complications that SDP induces for XADDs, such as the need for reordering the decision nodes after some operations. All of these are substantial contributions that have contributed to a new level of expressiveness for DC-MDPS that can be exactly solved.\nThere are a number of avenues for future research. First off, it is important examine what generalizations of the transition function used in this work would still permit closedform exact solutions. In terms of better scalability, one avenue would explore the use of initial state focused heuristic search-based value iteration like HAO* [13] that can be readily adapted to use SDP. Another avenue of research would be to adapt the lazy approximation approach of [11] to approximate DC-MDP value functions as piecewise linear XADDs with linear boundaries that may allow for better approximations than current representations that rely on rectangular piecewise functions. Along the same lines, ideas from APRICODD [16] for bounded approximation of discrete ADD value functions by merging leaves could be generalized to XADDs. Altogether the advances made by this work open up a number of potential novel research paths that we believe may help make rapid progress in the field of decision-theoretic planning with discrete and continuous state."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank the anonymous reviewers for their comments that have helped improve the paper. The first author is supported by NICTA; NICTA is funded by the Australian Government as represented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence program. This work has also been supported by the Brazilian agencies FAPESP (under grant 2008/03995-5) and CAPES."
    } ],
    "references" : [ {
      "title" : "Algebraic Decision Diagrams and their applications",
      "author" : [ "R. Iris Bahar", "Erica Frohm", "Charles Gaona", "Gary Hachtel", "Enrico Macii", "Abelardo Pardo", "Fabio Somenzi" ],
      "venue" : "In IEEE /ACM ICCAD,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1993
    }, {
      "title" : "Dynamic Programming",
      "author" : [ "Richard E. Bellman" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1957
    }, {
      "title" : "Decision-theoretic planning: Structural assumptions and computational leverage",
      "author" : [ "Craig Boutilier", "Thomas Dean", "Steve Hanks" ],
      "venue" : "JAIR, 11:1–94,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1999
    }, {
      "title" : "Symbolic dynamic programming for first-order MDPs",
      "author" : [ "Craig Boutilier", "Ray Reiter", "Bob Price" ],
      "venue" : "In IJCAI-01,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2001
    }, {
      "title" : "Exact solutions to time-dependent MDPs",
      "author" : [ "Justin Boyan", "Michael Littman" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2001
    }, {
      "title" : "Planning under continuous time and resource uncertainty: A challenge for ai",
      "author" : [ "John L. Bresina", "Richard Dearden", "Nicolas Meuleau", "Sailesh Ramkrishnan", "David E. Smith", "Richard Washington" ],
      "venue" : "InUncertainty in Artificial Intelligence",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2002
    }, {
      "title" : "A model for reasoning about persistence and causation",
      "author" : [ "Thomas Dean", "Keiji Kanazawa" ],
      "venue" : "Computational Intelligence,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1989
    }, {
      "title" : "Dynamic programming for structured continuous markov decision problems",
      "author" : [ "Zhengzhu Feng", "Richard Dearden", "Nicolas Meuleau", "Richard Washington" ],
      "venue" : "In Uncertainty in Artificial Intelligence",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "SPUDD: Stochastic planning using decision diagrams",
      "author" : [ "Jesse Hoey", "Robert St-Aubin", "Alan Hu", "Craig Boutilier" ],
      "venue" : "In UAI-99,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1999
    }, {
      "title" : "Solving factored mdps with hybrid state and action variables",
      "author" : [ "Branislav Kveton", "Milos Hauskrecht", "Carlos Guestrin" ],
      "venue" : "Journal Artificial Intelligence Research (JAIR),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "Lazy approximation for solving continuous finite-horizon mdps",
      "author" : [ "Lihong Li", "Michael L. Littman" ],
      "venue" : "In National Conference on Artificial Intelligence AAAI-",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "A fast analytical algorithm for solving markov decision processes with real-valued resources",
      "author" : [ "Janusz Marecki", "Sven Koenig", "Milind Tambe" ],
      "venue" : "In International Conference on Uncertainty in Artificial Intelligence IJCAI,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "A heuristic search approach to planning with continuous resources in stochastic domains",
      "author" : [ "Nicolas Meuleau", "Emmanuel Benazera", "Ronen I. Brafman", "Eric A. Hansen", "Mausam" ],
      "venue" : "Journal Artificial Intelligence Research (JAIR),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Temporal planning with continuous change",
      "author" : [ "J. Scott Penberthy", "Daniel S. Weld" ],
      "venue" : "In National Conference on Artificial Intelligence AAAI,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1994
    }, {
      "title" : "Variable resolution discretization in optimal control",
      "author" : [ "Andrew Moore Remi Munos" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2002
    }, {
      "title" : "APRICODD: Approximate policy construction using decision diagrams",
      "author" : [ "Robert St-Aubin", "Jesse Hoey", "Craig Boutilier" ],
      "venue" : "In NIPS-2000,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2000
    }, {
      "title" : "Exploiting causal independence in bayesian network inference",
      "author" : [ "Nevin Lianwen Zhang", "David Poole" ],
      "venue" : "J. Artif. Intell. Res. (JAIR),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "For example, in the MARS ROVER problem [6], a rover must manage bounded continuous resources of battery power and daylight time as it plans scientific discovery tasks for a set of landmarks on a given day.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "While problems such as the MARS ROVER are naturally modeled by discrete and continuous state Markov decision processes (DC-MDPs), little progress seems to have been made in recent years in developing exact solutions for DC-MDPs with multiple continuous state variables beyond the subset of DC-MDPs which have an optimal hyperrectangular piecewise linear value function [8, 11].",
      "startOffset" : 369,
      "endOffset" : 376
    }, {
      "referenceID" : 10,
      "context" : "While problems such as the MARS ROVER are naturally modeled by discrete and continuous state Markov decision processes (DC-MDPs), little progress seems to have been made in recent years in developing exact solutions for DC-MDPs with multiple continuous state variables beyond the subset of DC-MDPs which have an optimal hyperrectangular piecewise linear value function [8, 11].",
      "startOffset" : 369,
      "endOffset" : 376
    }, {
      "referenceID" : 7,
      "context" : "These questions have been affirmatively addressed for the subset of DC-MDPs with transition functions that are mixtures of delta functions and reward functions that are hyper-rectangular piecewise linear, which provably lead to value functions of the same structure [8, 11].",
      "startOffset" : 266,
      "endOffset" : 273
    }, {
      "referenceID" : 10,
      "context" : "These questions have been affirmatively addressed for the subset of DC-MDPs with transition functions that are mixtures of delta functions and reward functions that are hyper-rectangular piecewise linear, which provably lead to value functions of the same structure [8, 11].",
      "startOffset" : 266,
      "endOffset" : 273
    }, {
      "referenceID" : 3,
      "context" : "This is precisely the motivation behind symbolic dynamic programming (SDP) [4] used to solve MDPs with transitions and reward functions defined in first-order logic, except that in prior SDP work, only piecewise constant functions have been used; in this work we introduce techniques for working with arbitrary piecewise symbolic functions.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "For discrete factored MDPs, algebraic decision diagrams (ADDs) [1] have been successfully used in exact algorithms like SPUDD [9] to maintain compact value representations.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 8,
      "context" : "For discrete factored MDPs, algebraic decision diagrams (ADDs) [1] have been successfully used in exact algorithms like SPUDD [9] to maintain compact value representations.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 13,
      "context" : "We also borrow techniques from [14] for constraint-based pruning of XADDs that can be applied when XADDs meet certain expressiveness restrictions.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "We first introduce discrete and continuous state Markov decision processes (DC-MDPs) and then review their finitehorizon solution via dynamic programming following [11].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 2,
      "context" : "DC-MDPs as defined above are naturally factored [3] in terms of state variables (~b, ~x); as such transition structure can be exploited in the form of a dynamic Bayes net (DBN) [7] where the individual conditional probabilities",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "DC-MDPs as defined above are naturally factored [3] in terms of state variables (~b, ~x); as such transition structure can be exploited in the form of a dynamic Bayes net (DBN) [7] where the individual conditional probabilities",
      "startOffset" : 177,
      "endOffset" : 180
    }, {
      "referenceID" : 7,
      "context" : ", Gaussian noise), but we note that this representation effectively allows modeling of continuous variable transitions as a mixture of δ functions, which has been used heavily in previous exact DC-MDP solutions [8, 11, 13].",
      "startOffset" : 211,
      "endOffset" : 222
    }, {
      "referenceID" : 10,
      "context" : ", Gaussian noise), but we note that this representation effectively allows modeling of continuous variable transitions as a mixture of δ functions, which has been used heavily in previous exact DC-MDP solutions [8, 11, 13].",
      "startOffset" : 211,
      "endOffset" : 222
    }, {
      "referenceID" : 12,
      "context" : ", Gaussian noise), but we note that this representation effectively allows modeling of continuous variable transitions as a mixture of δ functions, which has been used heavily in previous exact DC-MDP solutions [8, 11, 13].",
      "startOffset" : 211,
      "endOffset" : 222
    }, {
      "referenceID" : 7,
      "context" : "Furthermore, we note that our representation is more general than [8, 11, 13] in that we do not restrict the equation to be linear, but rather allow it to specify arbitrary functions (e.",
      "startOffset" : 66,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "Furthermore, we note that our representation is more general than [8, 11, 13] in that we do not restrict the equation to be linear, but rather allow it to specify arbitrary functions (e.",
      "startOffset" : 66,
      "endOffset" : 77
    }, {
      "referenceID" : 12,
      "context" : "Furthermore, we note that our representation is more general than [8, 11, 13] in that we do not restrict the equation to be linear, but rather allow it to specify arbitrary functions (e.",
      "startOffset" : 66,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "Now we provide a continuous state generalization of value iteration [2], which is a dynamic programming algorithm for constructing optimal policies.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 3,
      "context" : "As it’s name suggests, symbolic dynamic programming (SDP) [4] is simply the process of performing dynamic programming (in this case value iteration) via symbolic manipulation.",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "While SDP as defined in [4] was previously only used with piecewise constant functions, we now generalize the representation to work with general piecewise functions needed for DC-MDPs in this paper.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "Using variable elimination [17], when marginalizing over xj we can factor out any functions independent of xj — that is, for ∫ xj in (7), one can see that initially, the only functions that can include xj are V ′h and P (xj |~b,~b′, ~x, a) = δ[xj − g(~x)]; hence, the first marginal over xj need only be computed over δ[xj − g(~x)]V ′h.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : "Motivated by the SPUDD [9] algorithm which maintains compact value function representations for finite discrete factored MDPs using algebraic decision diagrams (ADDs) [1], we extend this formalism to handle continuous variables in a data structure we refer to as the XADD.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "Motivated by the SPUDD [9] algorithm which maintains compact value function representations for finite discrete factored MDPs using algebraic decision diagrams (ADDs) [1], we extend this formalism to handle continuous variables in a data structure we refer to as the XADD.",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 13,
      "context" : ", as also done in [14]) to prune unreachable nodes in the XADD; later we show results demonstrating impressive reductions in XADD size using this style of pruning.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 5,
      "context" : "We implemented two versions of our proposed SDP algorithm using XADDs — one that does not prune nodes of the XADD and another that uses a linear programming solver to prune unreachable nodes (for problems with linear XADDs) — and we tested these algorithms on KNAPSACK and two versions of the Mars Rover domain (adapted from [6]) that we call MARS ROVER LINEAR and MARS ROVER NONLINEAR.",
      "startOffset" : 325,
      "endOffset" : 328
    }, {
      "referenceID" : 7,
      "context" : "The most relevant vein of Related work is that of [8] and [11] which can perform exact dynamic programming on DC-MDPs with rectangular piecewise linear reward and transition functions that are delta functions.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 10,
      "context" : "The most relevant vein of Related work is that of [8] and [11] which can perform exact dynamic programming on DC-MDPs with rectangular piecewise linear reward and transition functions that are delta functions.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 12,
      "context" : "Heuristic search approaches with formal guarantees like HAO* [13] are an attractive future extension of SDP; in fact HAO* currently uses the method of [8], which could be directly replaced with SDP.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 7,
      "context" : "Heuristic search approaches with formal guarantees like HAO* [13] are an attractive future extension of SDP; in fact HAO* currently uses the method of [8], which could be directly replaced with SDP.",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 13,
      "context" : "While [14] has considered general piecewise functions with linear boundaries (and in fact, we borrow our linear pruning approach from this paper), this work only applied to fully deterministic settings, not DC-MDPs.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 4,
      "context" : "One continuous variable can be useful for optimal solutions to time-dependent MDPs (TMDPs) [5].",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "Or phase transitions can be used to arbitrarily approximate one-dimensional continuous distributions leading to a bounded approximation approach for arbitrary single continuous variable DC-MDPs [12].",
      "startOffset" : 194,
      "endOffset" : 198
    }, {
      "referenceID" : 9,
      "context" : "Finally, there are a number of general DC-MDP approximation approaches that use approximate linear programming [10] or sampling in a reinforcement learning style approach [15].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 14,
      "context" : "Finally, there are a number of general DC-MDP approximation approaches that use approximate linear programming [10] or sampling in a reinforcement learning style approach [15].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 12,
      "context" : "In terms of better scalability, one avenue would explore the use of initial state focused heuristic search-based value iteration like HAO* [13] that can be readily adapted to use SDP.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 10,
      "context" : "Another avenue of research would be to adapt the lazy approximation approach of [11] to approximate DC-MDP value functions as piecewise linear XADDs with linear boundaries that may allow for better approximations than current representations that rely on rectangular piecewise functions.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "Along the same lines, ideas from APRICODD [16] for bounded approximation of discrete ADD value functions by merging leaves could be generalized to XADDs.",
      "startOffset" : 42,
      "endOffset" : 46
    } ],
    "year" : 2011,
    "abstractText" : "Many real-world decision-theoretic planning problems can be naturally modeled with discrete and continuous state Markov decision processes (DC-MDPs). While previous work has addressed automated decision-theoretic planning for DCMDPs, optimal solutions have only been defined so far for limited settings, e.g., DC-MDPs having hyper-rectangular piecewise linear value functions. In this work, we extend symbolic dynamic programming (SDP) techniques to provide optimal solutions for a vastly expanded class of DCMDPs. To address the inherent combinatorial aspects of SDP, we introduce the XADD — a continuous variable extension of the algebraic decision diagram (ADD) — that maintains compact representations of the exact value function. Empirically, we demonstrate an implementation of SDP with XADDs on various DC-MDPs, showing the first optimal automated solutions to DCMDPs with linear and nonlinear piecewise partitioned value functions and showing the advantages of constraint-based pruning for XADDs.",
    "creator" : "TeX"
  }
}