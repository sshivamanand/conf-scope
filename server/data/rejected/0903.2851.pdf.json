{
  "name" : "0903.2851.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "kamalika@soe.ucsd.edu", "yfreund@ucsd.edu", "djhsu@cs.ucsd.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :0\n90 3.\n28 51\nv1 [\ncs .L\nG ]\n1 6\nM ar\n2 00\nWe propose a completely parameter-free algorithm for learning in this framework. We show theoretically that our algorithm has a regret bound similar to the best bounds achieved by previous algorithms with optimally-tuned learning rates. We also present a few experiments comparing the performance of the algorithm with that of other algorithms for various tunings."
    }, {
      "heading" : "1 Introduction",
      "text" : "In this paper we consider the decision-theoretic framework for online learning (DTOL) proposed by Freund and Schapire [FS97]. DTOL is a variant of the framework of prediction with expert advice introduced by Littlestone and Warmuth [LW94] and Vovk [Vov98]. In this setting, a forecaster repeatedly assigns probabilities to a fixed set of actions. After each assignment, the actual loss associated with each action is revealed. The losses are restricted to the range [0, 1]. The forecaster’s loss on each round is the average loss of actions for that round, where the average is computed according to the forecaster’s current probability assignment. The goal of the forecaster is to achieve, on any sequence of losses, a cumulative loss close to the lowest cumulative loss among all single actions. We call this action the best action; and we call regret the difference between the cumulative loss achieved by the forecaster on a fixed loss sequence, and the cumulative loss of the best action.\nFreund and Schapire [FS97, FS99] use the Hedge algorithm, which assigns to the ith action a probability proportional to exp(−ηXi) where Xi is the cumulative loss of action i and η > 0 is a parameter called the learning rate. By\nappropriately tuning η, Hedge can achieve a regret bounded by O( √ T lnN), where T is the number of iterations and N is the number of actions. A matching lower bound of Ω( √ T lnN) is proven in [FS99]. A disadvantage of the upper bound is that it holds only for particular values of T and N because the learning rate η is set as a function of these parameters. It is clear that setting T in advance is a serious limitation; we would like an algorithm that performs close to optimally for every sequence length. It is less obvious why fixing the number of actions ahead of time is a limitation. To appreciate this, suppose that the actions consist of two equalsized sets where all of the actions in each set have an identical sequence of losses. While the number of actions N might be very large, the effective number of actions is two. Without prior information about the loss sequences we cannot set N correctly and therefore our choice of the learning rate η will be suboptimal. In order to quantify the effect of this phenomenon, we introduce a new notion of regret. We order the cumulative losses of all actions from lowest to highest and define the regret of the forecaster to the top ǫ-quantile to be the difference between the cumulative loss of the forecaster and the ⌊ǫN⌋-th element in the sorted list.\nIn this paper we present a new hedging algorithm for the DTOL problem. This new algorithm has no parameters. We show an upper bound on the regret to the top ǫ-quantile of the form\nO\n(\n√\nT ln 1\nǫ + ln3 N\n)\n,\nwhich holds simultaneously for all T and ǫ. If we set ǫ = 1/N we get a bound on the regret to the best action of the form O ( √ T lnN + ln3 N ) which is just slight worse than\nthe bound achieved by Hedge with optimally-tuned parameters, and which holds only for a specific N and T .\nGoing back to the illustrative example, while N might be very large, our algorithm still gets a bound of the form O( √\nT ln 2 + ln3 N) on the regret of the algorithm to the 1/2-quantile (or, in other words, the median).\nAnother useful property of our algorithm is that it assigns zero probability to any action whose cumulative loss is larger than the cumulative loss of the algorithm itself. In other words, non-zero weights are assigned only to actions which perform better than the algorithm. In most applications of DTOL we expect a small set of the actions to perform significantly better than most of the actions. As the regret of\nthe hedging algorithm is guaranteed to be small, this means that the algorithm will perform better than most of the actions and will therefore assign them zero probability. This can significantly reduce the computational cost when hedging over a large set of actions.\nThe rest of the paper is organized as follows. In Section 2, we review the DTOL setting and present our parameterfree algorithm called Normal-Hedge. Section 3 gives an account of how Normal-Hedge was derived in terms of drifting games [Sch01, FO02]. We review other related work in Section 4. Section 5 describes experiments that illustrate the need for adaptivity with respect to the number of actions N . Finally, the formal regret bounds and analysis are given in Sections 6 and 7."
    }, {
      "heading" : "2 Algorithm",
      "text" : ""
    }, {
      "heading" : "2.1 Setting",
      "text" : "We consider the decision-theoretic framework for online learning. In this setting, the learner is given access to a set of N actions, where N ≥ 2. In round t, the learner chooses a weight distribution pt = (p1,t, . . . , pN,t) over the actions 1, 2, . . . , N . Each action i incurs a loss li,t, and the learner incurs the expected loss\nlA,t = N ∑\ni=1\npi,tli,t.\nThe learner’s instantaneous regret with respect to an action i in round t is ri,t = lA,t − li,t, and the regret with respect to an action i in each of the first t rounds is\nRi,t = t ∑\nτ=1\nri,τ .\nWe assume that the range of the losses li,t is an interval of length 1 (e.g. [0, 1] or [−1/2, 1/2]; the sign of the loss does not matter).\nThe goal of the learner is to minimize this cumulative regret Ri,t for any value of t. The algorithm we present guarantees a bound on the regret to the best action\ni = arg max i′=1,...,N Ri′,t ."
    }, {
      "heading" : "2.2 Normal-Hedge",
      "text" : "Our algorithm, Normal-Hedge, is based on a potential function reminiscent of the half-normal distribution, specifically\nφ(x, c) = exp\n(\n([x]+) 2\n2c\n)\nfor x ∈ R, c > 0 (1)\nwhere [x]+ denotes max{0, x}. It is easy to check that this function is separately convex in x and c, differentiable, and twice-differentiable except at x = 0. Figure 4 catalogues the derivatives.\nIn addition to tracking the cumulative regretsRi,t to each action i after each round t, the algorithm also maintains a scale parameter ct. This is chosen so that the average of the potential, over all actions i, evaluated at Ri,t and ct, remains constant at e:\n1\nN\nN ∑\ni=1\nexp\n(\n([Ri,t]+) 2\n2ct\n)\n= e. (2)\nWe observe that since φ(x, c) is convex in c > 0, we can determine the value of ct with a line search.\nThe weight assigned to i in round t is set proportional to the first-derivative of the potential, evaluated at Ri,t−1 and ct−1:\npi,t ∝ ∂\n∂x φ(x, c)\n∣ ∣ ∣ ∣\nx=Ri,t−1,c=ct−1\n= [Ri,t−1]+\nct−1 exp\n(\n([Ri,t−1]+) 2\n2ct−1\n)\n.\nNotice that the actions for which Ri,t−1 ≤ 0 receive zero weight in round t.\nWe summarize the learning algorithm in Figure 1."
    }, {
      "heading" : "3 Derivation of the algorithm",
      "text" : "Before we present the rigorous analysis of the algorithm, it might be useful to describe the ideas that led us to its design. This work is based on the binomial weights (BW) algorithm [CBFHW96] and on its analysis using drifting games [Sch01, FO02]. Due to space constraints we focus on the differences between Normal-Hedge and the BW algorithm.\nWe first remark on differences in the respective frameworks. The BW algorithm receives as input a bound k on the total number of mistakes that the best action (or expert); this input is not given to Normal-Hedge. In addition, the payoffs in BW are restricted to be either 0 or −1; in Normal-Hedge, they can be any number from an interval of length 1.\nThere are two ideas in common between BW and NormalHedge. The first is a potential function whose average value does not increase between iterations of the game. The second is a simple randomized strategy for the adversary of the hedging algorithm. The adversarial strategy is to select each payoff to be either −1 or 0 with equal probability. The potential function is a function of total payoff and time which is defined by recursion backwards in time. The potential associated with payoff x at time t is the expected potential at time t+ 1 if the adversary uses the randomized strategy.\nThe BW algorithm corresponds to a drifting game with a finite horizon where the potential at the end of the game is one if total payoff is larger than −k and zero otherwise (and this value k is given to the algorithm). Using this potential\nfunction, one can derive a weighting scheme that guarantees a small upper bound on the number of mistakes of the algorithm; this weighting scheme is exactly the BW algorithm.\nNormal-Hedge is based on a drifting game with an unbounded horizon. The potential function depends on the regret relative to the action, rather than on the total payoff of the action. The potential function is designed in such a way that it has the same functional form in all iterations. Intuitively, we want a potential function that increases as fast as possible as a function of the regret, because a faster increasing potential function yields smaller bounds on the regret. On the other hand, if the potential function increases too fast then it will change its functional form between iterations. We therefore search for the fastest increasing potential function under the restriction that it maintains its functional form. Because the payoffs of the adversary are random with mean zero, the total payoff of the optimal algorithm is always zero and the regrets with respect to each action is equal to the total gain of that action. The distribution of the total payoffs is binomial and by the central limit theorem closely approximated by the normal distribution. A potential function that is the reciprocal of the probability distribution of the regrets will maintain its functional form. This yields the potential function defined in Equation (1).\nAnother difference between Normal-Hedge and BW is that payoffs for the actions hedged over by Normal-Hedge can take any value in the range [−1, 0], rather than just the two endpoints. Extending the drifting games analysis to this case can be done by using the continuous time limit described in [FO02]. However, doing this requires changing the setup of the problem to that of prediction in continuous time.1 In this paper, we avoid this change prove our results in the standard discrete time framework."
    }, {
      "heading" : "4 Related work",
      "text" : "A significant portion of the online learning literature is devoted to improving the adaptivity of the learning algorithm. Many of the works are variations of the exponential weights algorithm (e.g. Hedge), which was originally based on Littlestone and Warmuth’s Weighted Majority algorithm [LW94]. In exponential weights, the weight assigned to action i in round t is proportional to eηRi,t−1 . The parameter η is the learning rate that controls the algorithm’s sensitivity to the advantage of one action over another. Here we highlight some of these variations of exponential weights, as well as a few other algorithms, as they relate to Normal-Hedge.\nThe issue of time adaptivity—i.e. not knowing the time horizon in advance—was originally addressed in [CBFH+97] with a doubling trick: guess the horizon (and tune the algorithm accordingly), and double the guess if it turns out incorrect. Time-varying learning rates were considered in [ACBG02] that improve on the doubling trick. Specifically, they show that if ηt = √\n8 ln(N)/t, then using exponential weights with η = ηt in round t guarantees regret bounds of√ 2T lnN + O( √ lnN) for any T . This bound compares favorably to ours, but the algorithm requires tuning with respect to the number of actions N .\n1A different extension of the BW algorithm to the case of continuous outcomes was done by Mukherjee and Schapire [MS08].\nMore recent work in [YEYS04, CBMS07, HK08] give adaptive algorithms with significantly improved bounds when the total loss of the best action or the total variation in the losses is small. That is, the regret bounds of O( √\nL∗T lnN)\nor O( √ V ∗T lnN) only depend implicitly on T throughL ∗ T = mini ∑T t=1 li,t or V ∗ T = maxi ∑T t=1(li,t − ∑T τ=1 li,τ/T ) 2 (the notion of variation in [CBMS07] is slightly different, but the algorithm avoids the need for doubling tricks). One of the techniques in [CBMS07] of setting ηt according to the total loss variation is similar to ours, and thus we believe some of their analysis can be adapted for our algorithm. We stress, however, that these previous algorithms still require learning rates that depend on N .\nBesides exponential weights, time-adaptive methods can be derived from other families of online learning algorithms. The polynomial weights algorithms studied in [GLS01, Gen03, CBL03] are inherently time-adaptive but depend crucially on the number of actions N . The weight assigned to action i in round t is proportional to ([Ri,t−1]+)p for some p > 1; setting p = 2 lnN yields regret bounds of the form √\n2eT (lnN − 0.5) for any T . Note that our algorithm and polynomial weights share the feature that zero weight is given to actions that are performing worse than the algorithm, although the degree of this weight sparsity depends on the performance of the algorithm. Finally, [HP05] derive a timeadaptive variation of the follow-the-(perturbed) leader algorithm [Han57, KV05] by scaling the perturbations by a learning rate that depends on both t and N ."
    }, {
      "heading" : "5 Experiments",
      "text" : "We report a few simple synthetic experiments that illustrate the effect of tuning with respect to the number of actions N . In these experiments, the instantaneous losses of the actions are given by a matrix AN based on the Hadamard matrix (deleting the all-ones row, stacking the result on top of its negation, and then repeating each row infinitely). The loss of action i in round t is the (i, t)-th entry in the matrix AN . We show A6 for concreteness, but it easily generalizes to any N = 2d+1 − 2 using the 2d × 2d Hadamard matrix:\nA6 =\n\n    \n −1/2 +1 −1 +1 −1 +1 −1 +1 . . . −1/2 −1 +1 +1 −1 −1 +1 +1 . . . −1/2 +1 +1 −1 −1 +1 +1 −1 . . . +1/2 −1 +1 −1 +1 −1 +1 −1 . . . +1/2 +1 −1 −1 +1 +1 −1 −1 . . . +1/2 −1 −1 +1 +1 −1 −1 +1 . . .\n(the first column is halved for convenience). Thus, the action(s) with smallest total losses changes in every round. Most learning algorithms will tends towards assigning all actions the same weight.\nWe actually use a slightly different loss matrix Aε,KN , which is the same as AN except that ε is subtracted from every entry in the first K rows. Here, a successful algorithm will recognize that the first K actions perform better than the remaining N −K , and then assign these K actions equal weights.\nWe compare the performance of Normal-Hedge to two other algorithms: “Exp”, a time/variation-adaptive exponential weights algorithm due to [CBMS07]; and “Poly”, polynomial weights with p = 2 lnN [Gen03].\nFigure 2 shows the regrets to the best action versus time. In the plots marked with diamonds, Exp and Poly are given the correct values of N . In the plots marked with circles and squares, the algorithms are told the number of actions is 16N and 1024N , respectively. This simulates the effect of replicating each action 16 or 1024 times. We see that tuning Exp and Poly with respect to the number of actions (including replications) can result in suboptimal behavior: when K = 32 or K = 8, the algorithms are too sensitive to the changes in total loss, so they are slow to stabilize the weights over the best set of actions. When K = 1, Exp and Poly actually perform better when told an inflated value of N , as this causes the slight advantage of the single best action to be magnified. However, this is a fortuitous exception and not the rule. In Figure 3, we plot the regret after t = 32768 rounds versus the replication factor. Even when K = 2, performance degrades as the replication factor increases."
    }, {
      "heading" : "6 Analysis",
      "text" : ""
    }, {
      "heading" : "6.1 Main results",
      "text" : "Our main result is the following theorem.\nTheorem 1. If Normal-Hedge has access to N actions, then for all action loss sequences, for all t, for all 0 < ǫ ≤ 1 and for all 0 < δ ≤ 1/2, the regret of the algorithm to the top ǫ-quantile of the actions is at most √\n(1 + ln(1/ǫ))\n( 3(1 + 50δ)t+ 16 ln2 N δ ( 10.2 δ2 + lnN) ) .\nIn particular, by setting ǫ = 1/N we get that the regret to the best action is at most √\n(1 + lnN)\n( 3(1 + 50δ)t+ 16 ln2 N\nδ ( 10.2 δ2 + lnN)\n)\n.\nThe following corollary illustrates the performance of our algorithm for large values of t.\nCorollary 2. If Normal-Hedge has access toN actions, then, as t → ∞, the regret of Normal-Hedge to the top ǫ-quantile of actions can be made to approach an upper bound of\n√\n3t(1 + ln(1/ǫ)) + o(t) .\nIn particular, the regret of Normal-Hedge to the best action can be made to approach an upper bound of of\n√\n3t(1 + lnN) + o(t) .\nThe proof of Theorem 1 follows from a combination of Lemmas 3, 4, and 5, and is presented in detail at the end of the current section."
    }, {
      "heading" : "6.2 Regret bounds from the potential equation",
      "text" : "The following lemma relates the performance of the algorithm at time t to the scale ct.\nLemma 3. At any time t, the regret to the best action can be bounded as:\nmax i\nRi,t ≤ √ 2ct(lnN + 1)\nMoreover, for any 0 ≤ ǫ ≤ 1 and any t, the regret to the top ǫ-quantile of actions is at most\n√\n2ct(ln(1/ǫ) + 1).\nProof. We use Et to denote the actions that have non-zero weight on iteration t. The first part of the lemma follows from the fact that, for any action i ∈ Et,\nexp\n(\n(Ri,t) 2\n2ct\n)\n= exp\n(\n([Ri,t]+) 2\n2ct\n)\n≤ N ∑\ni′=1\nexp\n(\n([Ri′,t]+) 2\n2ct\n)\n≤ Ne\nwhich implies Ri,t ≤ √\n2ct(lnN + 1). For the second part of the lemma, let Ri,t denote the regret of our algorithm to the action with the ǫN -th highest regret. Then, the total potential of the actions with regrets greater than or equal to Ri,t is at least:\nǫN exp\n(\n([Ri,t]+) 2\n2ct\n)\n≤ Ne\nfrom which the second part of the lemma follows."
    }, {
      "heading" : "6.3 Bounds on the scale ct and proof of Theorem 1",
      "text" : "In Lemmas 4 and 5, we bound the growth of the scale ct as a function of the time t.\nThe main outline of the proof of Theorem 1 is as follows. As ct increases monotonically with t, we can divide the rounds t into two phases, t < t0 and t ≥ t0, where t0 is the first time such that\nct0 ≥ 4 ln2 N\nδ +\n16 lnN\nδ3 ,\nfor some fixed δ ∈ (0, 1/2). We then show bounds on the growth of ct for each phase separately. Lemma 4 shows that ct is not too large at the end of the first phase, while Lemma 5 bounds the per-round growth of ct in the second phase.\nLemma 4. For any time t,\nct+1 ≤ 2ct(1 + lnN) + 3 .\nLemma 5. Suppose that at some time t0, ct0 ≥ 4 ln 2 N δ + 16 lnN δ3 , where 0 ≤ δ ≤ 12 is a constant. Then, for any time t ≥ t0, ct+1 − ct ≤ 3\n2 (1 + 49.19δ) .\nWe now combine Lemmas 4 and 5 together with Lemma 3 to prove the main theorem.\nProof of Theorem 1. Let t0 be the first time at which ct0 ≥ 4 ln2 N\nδ + 16 lnN δ3 . Then, from Lemma 4,\nct0 ≤ 2ct0−1(1 + lnN) + 3, which is at most:\n8 ln3 N\nδ +\n34 ln2 N\nδ3 +\n32 lnN δ3 + 3 ≤ 8 ln\n3 N\nδ +\n81 ln2 N\nδ3 .\nThe last inequality follows because N ≥ 2 and δ ≤ 1/2. By Lemma 5, we have that for any t ≥ t0,\nct ≤ 3\n2 (1 + 49.19δ)(t− t0) + ct0 .\nCombining these last two inequalities yields\nct ≤ 3\n2 (1 + 49.19δ)t+\n8 ln3 N\nδ +\n81 ln2 N\nδ3 .\nNow the theorem follows by applying Lemma 3."
    }, {
      "heading" : "7 Remaining proofs",
      "text" : ""
    }, {
      "heading" : "7.1 Proof of Lemma 4",
      "text" : "Proof of Lemma 4. To show Lemma 4, we first show that, for any t,\n1\nN\n∑\ni\nφ(Ri,t+1, 2ct(1 + lnN) + 3) ≤ e. (3)\nFor any i, Ri,t+1 ≤ Ri,t + 1, the left hand side of the above inequality is at most\n1\nN\n∑\ni\nexp\n(\n(Ri,t + 1) 2\n4ct(1 + lnN) + 6\n)\n.\nThis, in turn, can be upper bounded by\n1\nN\n∑\ni\nexp\n(\nR2i,t 4ct(1 + lnN) + 6\n)\n(4)\n· exp ( 2Ri,t 4ct(1 + lnN) + 6 ) · exp (\n1\n4ct(1 + lnN) + 6\n)\n.\nWe now bound each term in this equation. First, we note that using Lemma 3, the first term can be bounded as\nexp\n(\nR2i,t 4ct(1 + lnN) + 6\n)\n≤ exp ( 2ct(1 + lnN)\n4ct(1 + lnN)\n)\n≤ e1/2.\nThe second term can be written as:\nexp\n(\nRi,t 4ct(1 + lnN) + 6\n) ≤ exp ( √ 2ct(1 + lnN)\n4ct(1 + lnN) + 6\n)\n.\nNow, we observe that for any real number a ≥ 0, 12 √ a + 6√ a ≥ 4 √ 3. This holds, because, for any a ≥ 0, the function 1 2 √ a+ 6√ a is convex, and has a minima at a = 3. Therefore,\n√\n2ct(1 + lnN)\n4ct(1 + lnN) + 6\n≤ 1 2 √ 2ct(1 + lnN) + 6 √ 2ct(1 + lnN) ≤ 1 4 √ 3\nso\nexp\n(\n√\n2ct(1 + lnN)\n4ct(1 + lnN) + 6\n)\n≤ e1/4.\nFinally, the third term is trivially bounded as\nexp\n(\n1\n4ct(1 + lnN) + 6\n)\n≤ e1/6.\nNow combining the bounds for the three terms in (4) gives 1\nN\n∑\ni\nφ(Ri,t+1, 2ct(1 + lnN) + 3) ≤ e.\nSince the quantity ∑\ni φ(Ri,t+1, c) is always increasing with c, Equation (3) implies that ct+1 ≤ 2ct(1 + lnN) + 3. The lemma follows."
    }, {
      "heading" : "7.2 A bootstrap for Lemma 5",
      "text" : "Before we can prove Lemma 5, we first show a somewhat weaker bound on the growth of ct with t (Lemma 6); this bound is used in the proof of Lemma 5 which concludes with the tighter bound on ct+1 − ct. Lemma 6. Suppose that at some time t0, ct0 ≥ 16 lnNδ2 , where 0 ≤ δ ≤ 1/2 is a constant. Then, for any time t ≥ t0,\nct+1 − ct ≤ eδ\n(\n3 2 + δ + lnN\n)\n1− δ2eδ .\nThe main idea behind the proof of Lemma 6 is as follows. First, we use Lemma 7 to show that ct is monotonic in t, and to get an expression for ct+1 − ct as a ratio of some derivatives and double derivatives of the potential function φ. Next, we use Lemma 8 and Corollary 10 to bound the numerator and denominator of this ratio. Combining these bounds gives us a proof of Lemma 6.\nWe denote by Et,t+1 . = Et∪Et+1 the actions relevant to the change of potential between iterations t and t+1 (recall, Et are the actions with non-zero weight on iteration t).\nLemma 7. At any time t, ct+1 − ct ≥ 0. Moreover, ct+1 − ct is at most: ∑\ni∈Et,t+1 (ri,t+1)\n2\n2\n(\n1 ct + ρ2i,t c2t\n) exp ( ρ2i,t 2ct )\n∑ i∈Et+1 (Ri,t+1)2\nε2 t+1\nexp ( (Ri,t+1)2\n2εt+1\n)\nwhere εt+1 lies in between ct and ct+1 and for each i, ρi,t lies between Ri,t and Ri,t+1.\nProof. We consider the change in average potential due to the regrets changing from Ri,t to Ri,t+1 (with the scale fixed at ct), and then the change due to the scale changing from ct to ct+1:\n0 =\nN ∑\ni=1\nφ(Ri,t+1, ct+1)− N ∑\ni=1\nφ(Ri,t, ct)\n=\nN ∑\ni=1\nφ(Ri,t+1, ct)− φ(Ri,t, ct) (5)\n+ N ∑\ni=1\nφ(Ri,t+1, ct+1)− φ(Ri,t+1, ct). (6)\nIt is clear that the sum in (5) can be restricted to i ∈ Et,t+1, and that the sum in (6) can be restricted to i ∈ Et+1. We now will express (6) in terms of ct+1 − ct and employ upper and lower bounds on (5).\nFirst, we derive bounds on (5). Letψ(x) = exp(x2/(2ct)). Then f(x) = φ(x, ct) can be written as\nf(x) =\n{\nψ(x) if x ≥ 0 ψ(0) if x < 0.\nThe function f satisfies the preconditions of Lemma 11 (deferred to the end of the section), so we have f(Ri,t+1)− f(Ri,t) ≥ f ′(Ri,t)ri,t+1 (7) and\nf(Ri,t+1)−f(Ri,t) ≤ f ′(Ri,t)ri,t+1+ ψ′′(ρi,t)\n2 r2i,t+1 (8)\nwhere min{Ri,t, Ri,t+1} ≤ ρi,t ≤ max{Ri,t, Ri,t+1} and ri,t+1 = Ri,t+1 − Ri,t. Now we sum both the lower and upper bounds (Eqs. (7) and (8)) over i ∈ Et,t+1 and apply the fact\n∑\ni∈Et,t+1\nf ′(Ri,t)ri,t+1 =\nN ∑\ni=1\nf ′(Ri,t)ri,t+1 = 0\nwhich follows easily from the fact that the weight assigned to an action i in trial t+ 1 is proportional to f ′(Ri,t). Thus,\n0 ≤ ∑\ni∈Et,t+1\nφ(Ri,t+1, ct)− φ(Ri,t, ct) (9)\n≤ ∑\ni∈Et,t+1\n(\n1 ct + ρ2i,t c2t\n)\nexp\n(\nρ2i,t 2ct\n)\n·(ri,t+1)2. (10) To deal with (6), we view it as a function of ct+1 and then equated via Taylor’s theorem to a first-order expansion around ct\n−(ct+1 − ct) · ∑\ni∈Et+1\n(Ri,t+1) 2\n2ε2t+1 exp\n(\n(Ri,t+1) 2\n2εt+1\n)\nfor some εt+1 between ct and ct+1. Substituting this back into (6), we have\n∑\ni∈Et,t+1\nφ(Ri,t+1, ct)− φ(Ri,t, ct)\n= ∑\ni∈Et+1\n(Ri,t+1) 2\n2ε2t+1 exp\n(\n(Ri,t+1) 2\n2εt+1\n)\n·(ct+1 − ct). (11)\nThe summation on the right-hand side is non-negative, as is the summation on the left-hand side (recall the lower bound (9)), so ct+1 − ct is non-negative as well. This shows the first part of the lemma. The second part follows from rearranging Eq. (11) and applying the upper bound (10).\nLemma 8. Let, for some t = t0, ct0 ≥ 16 lnNδ2 + 1δ , for some 0 ≤ δ ≤ 1. Then, for any t ≥ t0,\n∑\ni∈Et,t+1\nexp\n(\nρ2i,t 2ct\n)\n≤ eδNe\nand also ∑\ni∈Et,t+1\nρ2i,t 2ct exp\n(\nρ2i,t 2ct\n)\n≤ eδ(δ + 1 + lnN)Ne where the ρi,t are the values introduced in Lemma 7.\nProof. Pick any i ∈ Et,t+1. If Ri,t ≥ 0, then |ρi,t| ≤ Ri,t + 1 = [Ri,t]+ + 1. Otherwise Ri,t+1 ≥ 0 > Ri,t. But since |ri,t+1| = |Ri,t+1 − Ri,t| ≤ 1, it must be that |ρi,t| ≤ 1 = [Ri,t]+ + 1. Therefore ρ2i,t ≤ ([Ri,t]+ + 1)2, which in turn implies\nexp\n(\nρ2i,t 2ct\n)\n≤ exp ( ([Ri,t]+ + 1) 2\n2ct\n)\n= exp\n(\n([Ri,t]+) 2\n2ct\n)\n· exp ( 2[Ri,t]+ 2ct ) · exp ( 1 2ct ) .\nTo prove the first claim, it suffices to show that each of the two exponentials in the final product is bounded by eδ/2. Since ct ≥ ct0 = (16 lnN)/δ2+1/δ, we have exp(1/(2ct)) ≤ eδ/2. Also, Lemma 3 imply\nexp\n(\n[Ri,t]+ ct\n) ≤ exp ( √\n2(1 + lnN)√ ct\n)\n≤ exp (\n2δ √ lnN\n4 √ lnN\n)\n≤ eδ/2,\nso the first claim follows. To prove the second claim, we use the first claim to derive the fact\nmax i′∈Et,t+1 ρ2i′,t 2ct ≤ ln ∑\ni′∈Et,t+1\nexp\n(\nρ2i′,t 2ct\n)\n≤ δ + 1 + lnN which in turn is combined again with the first claim to arrive at\n∑\ni∈Et,t+1\nρ2i,t 2ct exp\n(\nρ2i,t 2ct\n)\n≤ (\nmax i′∈Et,t+1 ρ2i′,t 2ct\n)\n∑\ni∈Et,t+1\nexp\n(\nρ2i,t 2ct\n)\n≤ (δ + 1 + lnN)eδNe,\ncompleting the proof.\nLemma 9. Let B ≥ 1. If ∑Ni=1 exi ≥ BN for some x ≥ 0, then f(x) =\n∑N i=1 xie xi ≥ BN lnB.\nProof. We consider minimizing f under the constraint given by the hypothesis. Define the Lagrangian functionL(x, λ) = ∑N\ni=1 xie xi +λ(BN − ∑N i=1 e xi). Then (∂/∂xi)L(x, λ) = (xi + 1 − λ)exi , which is 0 when xi = λ − 1. Let g(λ) = L(x∗, λ) be the dual function, where x∗i = λ − 1. Then g is maximized when λ = 1 + lnB. By weak duality, supλ g(λ) ≤ infx f(x) (with the constraints on x), so f(x) ≥ g(1 + lnB) = BN lnB.\nThe lemma above leads to the following corollary.\nCorollary 10. For any t,\n∑\ni∈Et+1\n(Ri,t+1) 2\n2ct+1 exp\n(\n(Ri,t+1) 2\n2ct+1\n)\n≥ Ne.\nProof. Let xi = (Ri,t+1)2/(2ct+1), so\n∑\ni∈Et+1\nexi = ∑\ni∈Et+1\nexp\n(\n(Ri,t+1) 2\n2ct+1\n)\n=\nN ∑\ni=1\nexp\n(\n([Ri,t+1]+) 2\n2ct+1\n)\n≥ Ne.\nApplying Lemma 9 completes the proof.\nNow we prove Lemma 6.\nProof of Lemma 6. From Lemma 7, ct+1 ≥ ct, and therefore, εt+1 ≥ ct. By Lemma 7 and the fact |ri,t+1| ≤ 1,\nct+1 − ct\n≤ ∑ i∈Et,t+1( 1 2ct + ρ2i,t 2c2t ) exp ( ρ2i,t 2ct )\n∑ i∈Et+1 (Ri,t+1)2\n2c2 t+1\nexp ( (Ri,t+1)2\n2ct+1\n) .\nCombining this with Lemma 8, Corollary 10, we have\nct+1 − ct ≤ ct+1 ct\n· e δ ( 1 2 + δ + 1 + lnN ) Ne\nNe\n=\n(\n1 + ct+1 − ct\nct\n) · eδ ( 1 + 1\n2 + δ + lnN\n)\n.\nRe-arranging, and using the fact that ct ≥ ct0 ≥ (16 lnN)/δ2, we get that,\nct+1 − ct ≤ eδ\n( 1 + 12 + δ + lnN )\n1− e δ(1+ 12+δ+lnN)\nct0\n≤ e δ(32 + δ + lnN)\n1− δ2eδ( 116 lnN + 1 2 +δ 16 lnN + 1 16 )\n.\nThe rest of the lemma follows by plugging in the fact that N ≥ 2, and δ ≤ 1/2."
    }, {
      "heading" : "7.3 Proof of Lemma 5",
      "text" : "Finally, we are ready to prove Lemma 5. As in the proof of Lemma 6, here too, we start with an upper bound on ct+1 − ct, obtained from Lemma 7. We then use this upper bound, and the bound in Lemma 6 to get a finer bound on the quantity ct+1 − ct.\nProof of Lemma 5. We divide the actions into two sets:\nS1 = {i ∈ Et,t+1 : [Ri,t+1]+ + 1 ≤ √\n2ctδ} S2 = {i ∈ Et,t+1 : [Ri,t+1]+ + 1 > √\n2ctδ}. Using the fact that |ri,t+1| ≤ 1, the bound from Lemma 7 can be written as\nct+1 − ct\n≤ 1 2ct\n∑ i∈Et,t+1 exp ( ρ2i,t 2ct )\n∑ i∈Et+1 (Ri,t+1)2\n2c2 t+1\nexp ( (Ri,t+1)2\n2ct+1\n)\n+\n∑ i∈S1 ρ2i,t 2c2t exp ( ρ2i,t 2c2t )\n∑\ni∈Et+1\nR2 i,t+1 2c2 t+1 exp ( ([Ri,t+1]+)2 2c2 t+1 )\n+\n∑ i∈S2 ρ2i,t 2c2t exp ( ρ2i,t 2ct )\n∑ i∈Et+1 (Ri,t+1)2\n2c2 t+1\nexp ( (Ri,t+1)2\n2ct+1\n) .\nWe will upper-bound each of these three terms separately. The first term is bounded by (ct+1/ct)eδ/2 using Lemma 8 and Corollary 10. To bound the second term, the definition of S1 implies\n∑\ni∈S1\n([Ri,t+1]+ + 1) 2\n2ct exp\n(\n([Ri,t+1]+ + 1) 2\n2ct\n)\n≤ N 2ctδ 2ct exp\n(\n2ctδ\n2ct\n)\n≤ δeδN.\nNow Corollary 10 implies a bound of (ct+1/ct)(δeδ/e). Now we bound the third term. Note that since √ 2ctδ ≥ √\n2ct0δ > 1, we have that each i ∈ S2 is also in Et+1. So the third term is bounded above by the largest ratio\nρ2i,t 2c2t exp ( ρ2i,t 2ct )\n(Ri,t+1)2\n2c2 t+1\nexp ( (Ri,t+1)2\n2ct+1\n)\nover all i ∈ Et+1. Since |ρi,t| ≤ Ri,t+1 + 1, each such ratio is at most\nc2t+1 c2t · ( Ri,t+1 + 1 Ri,t+1\n)2\n· exp ( 1\n2ct + Ri,t+1 ct\n)\n· exp ( (Ri,t+1) 2(ct+1 − ct)\n2ctct+1\n)\n.\nWe bound each factor in this product (deferring c2t+1/c 2 t until later).\n• As Ri,t+1+1 ≥ √ 2ctδ and ct ≥ ct0 ≥ 10/δ3, we have√\n2ctδ ≥ 1/δ and (\nRi,t+1 + 1\nRi,t+1\n)2\n≤ 1 (1− δ)2 .\n• By Lemma 3, we have\nRi,t+1 ≤ Ri,t + 1 ≤ √ 2ct(1 + lnN) + 1,\nso\nexp\n(\n1\n2ct + Ri,t+1 ct\n) ≤ exp ( 3\n2ct + Ri,t ct\n)\n≤ exp\n\n\n3\n2ct +\n√\n2(1 + lnN)\nct\n\n ≤ e3δ2/20+3δ/5\nsince ct ≥ ct0 ≥ (16 lnN)/δ2 ≥ 10/δ2. • We use Lemma 6 with ct0 ≥ (16 lnN)/δ2, and δ ≤ 1/2 to obtain the crude bound\nct+1 − ct ≤ eδ(4 + 2 lnN). (12) Now using this bound along with Lemma 3 and δ ≤ 1/2 gives\n(Ri,t+1) 2(ct+1 − ct)\n2ctct+1 ≤ e\nδ(4 + 2 lnN)(1 + lnN)\nct\n≤ 6.2 + 4.7 lnN + 3.1 ln 2 N\nct0\nwhich is at most δ since ct0 ≥ (16 lnN)/δ2+(4 ln2 N)/δ. Therefore, the third term is bounded by\nc2t+1 c2t · exp(1.6δ + 0.15δ 2) (1− δ)2 ≤ c2t+1 c2t · e 2δ (1− δ)2 .\nCollecting the three terms in the bound for ct+1 − ct,\nct+1 − ct ≤ ct+1 ct · e δ 2 + ct+1 ct · δe δ e + c2t+1 c2t · e 2δ (1− δ)2 .\nWe bound the ratio ct+1/ct as\nct+1 ct ≤ 1 + ct+1 − ct ct ≤ 1 + e δ(4 + 2 lnN) ct\n≤ 1 + 4δ 2\n10 +\nδ2\n8 = 1 +\n21δeδ\n40 ≤ 1 + δ\nwhere we have used the bound in (12), ct0 ≥ (16 lnN)/δ2, and δ ≤ 1/2. Therefore, we have\nct+1 − ct ≤ 1 2 · eδ(1 + δ) + 1 · e\n2δ(1 + δ)2 (1− δ)2 + δeδ(1 + δ) e\nTo finish the proof, we use the fact that δ ≤ 12 . Using this condition, and a Taylor series expansion, when 0 ≤ δ ≤ 12 , eδ ≤ 1 +√eδ ≤ 1 + 1.65δ. Using this fact,\n1 2 eδ(1 + δ) + δ e eδ(1 + δ)\nis at most 1\n2 + 3.02δ + 2.63δ2 + 0.61δ3\nwhich in turn is at most 0.5 + 3.49δ. Moreover,\ne2δ(1 + δ)2\n(1 − δ)2 ≤ e 2δ(1 + 4δ)2\nwhich again is at most\n(1 + 3.3δ)(1 + 4δ)2\nUsing the fact that δ ≤ 12 , this is at most 1 + 45.7δ. The lemma follows by combining this with the bound in the previous paragraph.\nLemma 11. Let ψ : R → R be any continuous, twicedifferentiable, convex function such that for some a ∈ R, ψ is non-decreasing on [a,∞) and ψ′(a) = 0. Define f : R → R by\nf(x) =\n{\nψ(x) if x ≥ a ψ(a) if x < a,\nThen for any x0, x ∈ R, f ′(x0)(x − x0) ≤ f(x)− f(x0)\n≤ f ′(x0)(x − x0) + ψ′′(ξ)\n2 (x− x0)2\nfor some min{x0, x} ≤ ξ ≤ max{x0, x}. Proof. The lower bound follows from the convexity of f , which is inherited from the convexity of ψ. For the upper bound, we first consider the case x0 < a and x ≥ a. Then, for some ξ ∈ [a, x],\nf(x)− f(x0) = ψ(x) − ψ(a)\n= ψ′(a)(x − a) + ψ ′′(ξ)\n2 (x− a)2 (13)\n≤ ψ′(a)(x − x0) + ψ′′(ξ)\n2 (x− x0)2 (14)\n= f ′(x0)(x− x0) + ψ′′(ξ)\n2 (x− x0)2\nwhere (13) follows by Taylor’s theorem and (14) follows since x0 ≤ a < x, ψ′(a) ≥ 0, and ψ′′(ξ) ≥ 0. The case x0 ≥ a and x < a is analogous, and the remaining cases are immediate using Taylor’s theorem."
    } ],
    "references" : [ {
      "title" : "Adaptive and self-confident on-line learning algorithms",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "C. Gentile" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "How to use expert advice",
      "author" : [ "N. Cesa-Bianchi", "Y. Freund", "D. Haussler", "D.P. Hembold", "R.E. Schapire", "M. Warmuth" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 1997
    }, {
      "title" : "On-line prediction and conversion strategies",
      "author" : [ "N. Cesa-Bianchi", "Y. Freund", "D.P. Hembold", "M. Warmuth" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 1996
    }, {
      "title" : "Potentialbased algorithms in on-line prediction and game theory",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2003\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2003
    }, {
      "title" : "Improved second-order bounds for prediction with expert advice",
      "author" : [ "N. Cesa-Bianchi", "Y. Mansour", "G. Stoltz" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2007
    }, {
      "title" : "Drifting games and Brownian motion",
      "author" : [ "Y. Freund", "M. Opper" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Freund and Opper.,? \\Q2002\\E",
      "shortCiteRegEx" : "Freund and Opper.",
      "year" : 2002
    }, {
      "title" : "A decisiontheoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Y. Freund", "R.E. Schapire" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Freund and Schapire.,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund and Schapire.",
      "year" : 1997
    }, {
      "title" : "Adaptive game playing using multiplicative weights",
      "author" : [ "Y. Freund", "R.E. Schapire" ],
      "venue" : "Games and Economic Behavior,",
      "citeRegEx" : "Freund and Schapire.,? \\Q1999\\E",
      "shortCiteRegEx" : "Freund and Schapire.",
      "year" : 1999
    }, {
      "title" : "The robustness of p-norm algorithms",
      "author" : [ "C. Gentile" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Gentile.,? \\Q2003\\E",
      "shortCiteRegEx" : "Gentile.",
      "year" : 2003
    }, {
      "title" : "General convergence results for linear discriminant updates",
      "author" : [ "A.J. Grove", "N. Littlestone", "D. Schuurmans" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Grove et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Grove et al\\.",
      "year" : 2001
    }, {
      "title" : "Approximation to bayes risk in repeated play",
      "author" : [ "J. Hannan" ],
      "venue" : "Contributions to the Theory of Games,",
      "citeRegEx" : "Hannan.,? \\Q1957\\E",
      "shortCiteRegEx" : "Hannan.",
      "year" : 1957
    }, {
      "title" : "Extracting certainty from uncertainty: Regret bounded by variation in costs",
      "author" : [ "E. Hazan", "S. Kale" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Hazan and Kale.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2008
    }, {
      "title" : "Adaptive online prediction by following the perturbed leader",
      "author" : [ "M. Hutter", "J. Poland" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hutter and Poland.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hutter and Poland.",
      "year" : 2005
    }, {
      "title" : "Efficient algorithms for the online optimization",
      "author" : [ "A. Kalai", "S. Vempala" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Kalai and Vempala.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kalai and Vempala.",
      "year" : 2005
    }, {
      "title" : "The weighted majority algorithm",
      "author" : [ "N. Littlestone", "M. Warmuth" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "Littlestone and Warmuth.,? \\Q1994\\E",
      "shortCiteRegEx" : "Littlestone and Warmuth.",
      "year" : 1994
    }, {
      "title" : "Learning with continuous experts using drifting games",
      "author" : [ "Indraneel Mukherjee", "Robert E. Schapire" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Mukherjee and Schapire.,? \\Q2008\\E",
      "shortCiteRegEx" : "Mukherjee and Schapire.",
      "year" : 2008
    }, {
      "title" : "A game of prediction witih expert advice",
      "author" : [ "V. Vovk" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Vovk.,? \\Q1998\\E",
      "shortCiteRegEx" : "Vovk.",
      "year" : 1998
    }, {
      "title" : "How to better use expert advice",
      "author" : [ "R. Yaroshinsky", "R. El-Yaniv", "S. Seiden" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Yaroshinsky et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Yaroshinsky et al\\.",
      "year" : 2004
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "In this paper, we consider the decision-theoretic framework for online learning (DTOL) proposed by Freund and Schapire [FS97]. Previous algorithms for learning in this framework have a tunable learning rate parameter. Tuning the learning rate requires prior knowledge about the sequence and severely limits the practicality of the algorithm. While much progress has been made in the past decade for adaptively tuning the learning rate, all of these methods still ultimately rely on some prior information. We propose a completely parameter-free algorithm for learning in this framework. We show theoretically that our algorithm has a regret bound similar to the best bounds achieved by previous algorithms with optimally-tuned learning rates. We also present a few experiments comparing the performance of the algorithm with that of other algorithms for various tunings.",
    "creator" : "LaTeX with hyperref package"
  }
}