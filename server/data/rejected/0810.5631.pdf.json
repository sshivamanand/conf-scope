{
  "name" : "0810.5631.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Temporal Difference Updating without a Learning Rate",
    "authors" : [ "Marcus Hutter", "Shane Legg" ],
    "emails" : [ "RSISE@ANU", "SML@NICTA,", "marcus@hutter1.net", "shane@vetta.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :0\n81 0.\n56 31\nKeywords\nreinforcement learning; temporal difference; eligibility trace; variational principle; learning rate."
    }, {
      "heading" : "1 Introduction",
      "text" : "In the field of reinforcement learning, perhaps the most popular way to estimate the future discounted reward of states is the method of temporal difference learning. It is unclear who exactly introduced this first, however the first explicit version of temporal difference as a learning rule appears to be Witten [Wit77]. The idea is as follows: The expected future discounted reward of a state s is,\nV s := E { rk + γrk+1 + γ 2rk+2 + · · · |sk = s } ,\nwhere the rewards rk, rk+1, . . . are geometrically discounted into the future by γ < 1. From this definition it follows that,\nV s = E { rk + γV sk+1|sk = s } . (1)\nOur task, at time t, is to compute an estimate V ts of V s for each state s. The only information we have to base this estimate on is the current history of state transitions, s1, s2, . . . , st, and the current history of observed rewards, r1, r2, . . . , rt. Equation (1) suggests that at time t + 1 the value of rt + γVst+1 provides us with information on what V ts should be: If it is higher than V t st then perhaps this estimate should be increased, and vice versa. This intuition gives us the following estimation heuristic for state st,\nV t+1st := V t st + α\n(\nrt + γV t st+1 − V tst\n)\n,\nwhere α is a parameter that controls the rate of learning. This type of temporal difference learning is known as TD(0).\nOne shortcoming of this method is that at each time step the value of only the last state st is updated. States before the last state are also affected by changes in the last state’s value and thus these could be updated too. This is what happens with so called temporal difference learning with eligibility traces, where a history, or trace, is kept of which states have been recently visited. Under this method, when we update the value of a state we also go back through the trace updating the earlier states as well. Formally, for any state s its eligibility trace is computed by,\nEts :=\n{\nγλEt−1s if s 6= st, γλEt−1s + 1 if s = st,\nwhere λ is used to control the rate at which the eligibility trace is discounted. The temporal difference update is then, for all states s,\nV t+1s := V t s + αE t s\n(\nr + γV tst+1 − V t st\n)\n. (2)\nThis more powerful version of temporal different learning is known as TD(λ) [Sut88]. The main idea of this paper is to derive a temporal difference rule from statistical principles and compare it to the standard heuristic described above. Superficially,\nour work has some similarities to LSTD(λ) ([LP03] and references therein). However LSTD is concerned with finding a least-squares linear function approximation, it has not yet been developed for general γ and λ, and has update time quadratic in the number of features/states. On the other hand, our algorithm “exactly” coincides with TD/Q/Sarsa(λ) for finite state spaces, but with a novel learning rate derived from statistical principles. We therefore focus our comparison on TD/Q/Sarsa. For a recent survey of methods to set the learning rate see [GP06].\nIn Section 2 we derive a least squares estimate for the value function. By expressing the estimate as an incremental update rule we obtain a new form of TD(λ), which we call HL(λ). In Section 3 we compare HL(λ) to TD(λ) on a simple Markov chain. We then test it on a random Markov chain in Section 4 and a non-stationary environment in Section 5. In Section 6 we derive two new methods for policy learning based on HL(λ), and compare them to Sarsa(λ) and Watkins’ Q(λ) on a simple reinforcement learning problem. Section 7 ends the paper with a summary and some thoughts on future research directions."
    }, {
      "heading" : "2 Derivation",
      "text" : "The empirical future discounted reward of a state sk is the sum of actual rewards following from state sk in time steps k, k + 1, . . ., where the rewards are discounted as they go into the future. Formally, the empirical value of state sk at time k for k = 1, ..., t is,\nvk := ∞ ∑\nu=k\nγu−kru, (3)\nwhere the future rewards ru are geometrically discounted by γ < 1. In practice the exact value of vk is always unknown to us as it depends not only on rewards that have been already observed, but also on unknown future rewards. Note that if sm = sn for m 6= n, that is, we have visited the same state twice at different times m and n, this does not imply that vn = vm as the observed rewards following the state visit may be different each time.\nOur goal is that for each state s the estimate V ts should be as close as possible to the true expected future discounted reward V s. Thus, for each state s we would like Vs to be close to vk for all k such that s = sk. Furthermore, in non-stationary environments we would like to discount old evidence by some parameter λ ∈ (0, 1]. Formally, we want to minimise the loss function,\nL := 1\n2\nt ∑\nk=1\nλt−k(vk − V t sk )2. (4)\nFor stationary environments we may simply set λ = 1 a priori.\nAs we wish to minimise this loss, we take the partial derivative with respect to\nthe value estimate of each state and set to zero,\n∂L\n∂V ts = −\nt ∑\nk=1\nλt−k(vk − V t sk )δsks = V t s\nt ∑\nk=1\nλt−kδsks − t ∑\nk=1\nλt−kδsksvk = 0,\nwhere we could change V tsk into V t s due to the presence of the Kronecker δsks, defined δxy := 1 if x = y, and 0 otherwise. By defining a discounted state visit counter N ts := ∑t k=1 λ t−kδsks we get\nV tsN t s =\nt ∑\nk=1\nλt−kδsksvk. (5)\nSince vk depends on future rewards rk, Equation (5) can not be used in its current form. Next we note that vk has a self-consistency property with respect to the rewards. Specifically, the tail of the future discounted reward sum for each state depends on the empirical value at time t in the following way,\nvk = t−1 ∑\nu=k\nγu−kru + γ t−kvt.\nSubstituting this into Equation (5) and exchanging the order of the double sum,\nV tsN t s =\nt−1 ∑\nu=1\nu ∑\nk=1\nλt−kδsksγ u−kru +\nt ∑\nk=1\nλt−kδsksγ t−kvt\n= t−1 ∑\nu=1\nλt−u u ∑\nk=1\n(λγ)u−kδsksru + t ∑\nk=1\n(λγ)t−kδsksvt\n= Rts + E t svt,\nwhere Ets := ∑t k=1(λγ) t−kδsks is the eligibility trace of state s, and R t s := ∑t−1 u=1 λ\nt−uEus ru is the discounted reward with eligibility. Ets and R t s depend only on quantities known at time t. The only unknown quantity is vt, which we have to replace with our current estimate of this value at time t, which is V tst . In other words, we bootstrap our estimates. This gives us,\nV tsN t s = R t s + E t sV t st . (6)\nFor state s = st, this simplifies to V t st = Rtst/(N t st −Etst). Substituting this back into Equation (6) we obtain,\nV tsN t s = R t s + E t s Rtst N tst − E t st . (7)\nThis gives us an explicit expression for our V estimates. However, from an algorithmic perspective an incremental update rule is more convenient. To derive this we make use of the relations,\nN t+1s = λN t s + δst+1s, E t+1 s = λγE t s + δst+1s, R t+1 s = λR t s + λE t srt,\nwith N0s = E 0 s = R 0 s = 0. Inserting these into Equation (7) with t replaced by t+1,\nV t+1s N t+1 s = R t+1 s + E t+1 s Rt+1st+1 N t+1st+1 − E t+1 st+1\n= λRts + λE t srt + E t+1 s\nRtst+1+ E t st+1 rt N tst+1− γE t st+1 .\nBy solving Equation (6) for Rts and substituting back in,\nV t+1s N t+1 s = λ(V t sN t s − E t sV t st ) + λEtsrt + E t+1 s\nN tst+1V t st+1 − Etst+1V t st + Etst+1rt\nN tst+1 − γE t st+1\n= (λN ts + δst+1s)V t s − δst+1sV t s − λE t sV t st + λEtsrt\n+ Et+1s N tst+1V t st+1 −Etst+1V t st + Etst+1rt\nN tst+1 − γE t st+1\n.\nDividing through by N t+1s (= λN t s+ δst+1s),\nV t+1s = V t s +\n−δst+1sV t s − λE t sV t st + λEtsrt\nλN ts+ δst+1s\n+ (λγEts + δst+1s)(N t st+1 V tst+1− E t st+1 V tst+ E t st+1 rt)\n(N tst+1 − γE t st+1 )(λN ts+ δst+1s) .\nMaking the first denominator the same as the second, then expanding the numerator,\nV t+1s = V t s +\nλEtsrtN t st+1 − λEtsV t st N tst+1 − δst+1sV t sN t st+1 − λγEtst+1E t srt\n(N tst+1 − γE t st+1 )(λN ts+ δst+1s)\n+ λγEtst+1E t sV t st + γEtst+1V t s δst+1s + λγE t sN t st+1 V tst+1 − λγE t sE t st+1 V tst (N tst+1 − γE t st+1 )(λN ts+ δst+1s)\n+ λγEtsE t st+1 rt + δst+1sN t st+1 V tst+1 − δst+1sE t st+1 V tst + δst+1sE t st+1 rt\n(N tst+1 − γE t st+1 )(λN ts+ δst+1s) .\nAfter cancelling equal terms (keeping in mind that in every term with a Kronecker δxy factor we may assume that x = y as the term is always zero otherwise), and factoring out Ets we obtain,\nV t+1s = V t s+\nEts(λrtN t st+1 − λV tstN t st+1 + γV ts δst+1s + λγN t st+1 V tst+1 − δst+1sV t st + δst+1srt)\n(N tst+1 − γE t st+1 )(λN ts+ δst+1s)\nFinally, by factoring out λN tst+1 + δst+1s we obtain our update rule,\nV t+1s = V t s + E t s βt(s, st+1) (rt + γV t st+1 − V tst), (8)\nwhere the learning rate is given by,\nβt(s, st+1) := 1\nN tst+1− γE t st+1 N tst+1 N ts . (9)\nExamining Equation (8), we find the usual update equation for temporal difference learning with eligibility traces (see Equation (2)), however the learning rate α has now been replaced by βt(s, st+1). This learning rate was derived from statistical principles by minimising the squared loss between the estimated and true state value. In the derivation we have exploited the fact that the latter must be self-consistent and then bootstrapped to get Equation (6). This gives us an equation for the learning rate for each state transition at time t, as opposed to the standard temporal difference learning where the learning rate α is either a fixed free parameter for all transitions, or is decreased over time by some monotonically decreasing function. In either case, the learning rate is not automatic and must be experimentally tuned for good performance. The above derivation appears to theoretically solve this problem.\nThe first term in βt seems to provide some type of normalisation to the learning rate, though the intuition behind this is not clear to us. The meaning of second term however can be understood as follows: N ts measures how often we have visited state s in the recent past. Therefore, if N ts ≪ N t st+1\nthen state s has a value estimate based on relatively few samples, while state st+1 has a value estimate based on relatively many samples. In such a situation, the second term in βt boosts the learning rate so that V t+1s moves more aggressively towards the presumably more accurate rt+γV t st+1\n. In the opposite situation when st+1 is a less visited state, we see that the reverse occurs and the learning rate is reduced in order to maintain the existing value of Vs."
    }, {
      "heading" : "3 A simple Markov process",
      "text" : "For our first test we consider a simple Markov process with 51 states. In each step the state number is either incremented or decremented by one with equal probability, unless the system is in state 0 or 50 in which case it always transitions to state 25 in the following step. When the state transitions from 0 to 25 a reward of 1.0 is generated, and for a transition from 50 to 25 a reward of -1.0 is generated. All other transitions have a reward of 0. We set the discount value γ = 0.99 and then computed the true discounted value of each state by running a brute force Monte Carlo simulation.\nWe ran our algorithm 10 times on the above Markov chain and computed the root mean squared error in the value estimate across the states at each time step averaged across each run. The optimal value of λ for HL(λ) was 1.0, which was to be expected given that the environment is stationary and thus discounting old experience is not helpful.\nFor TD(λ) we tried various different learning rates and values of λ. We could find no settings where TD(λ) was competitive with HL(λ). If the learning rate α\nwas set too high the system would learn as fast as HL(λ) briefly before becoming stuck. With a lower learning rate the final performance was improved, however the initial performance was now much worse than HL(λ). The results of these tests appear in Figure 1.\nSimilar tests were performed with larger and smaller Markov chains, and with different values of γ. HL(λ) was consistently superior to TD(λ) across these tests. One wonders whether this may be due to the fact that the implicit learning rate that HL(λ) uses is not fixed. To test this we explored the performance of a number of different learning rate functions on the 51 state Markov chain described above. We found that functions of the form κ\nt always performed poorly, however good\nperformance was possible by setting κ correctly for functions of the form κ√ t and κ3√ t . As the results were much closer, we averaged over 300 runs. These results appear in Figure 2.\nWith a variable learning rate TD(λ) is performing much better, however we were still unable to find an equation that reduced the learning rate in such a way that TD(λ) would outperform HL(λ). This is evidence that HL(λ) is adapting the learning rate optimally without the need for manual equation tuning."
    }, {
      "heading" : "4 Random Markov process",
      "text" : "To test on a Markov process with a more complex transition structure, we created a random 50 state Markov process. We did this by creating a 50 by 50 transition matrix where each element was set to 0 with probability 0.9, and a uniformly random number in the interval [0, 1] otherwise. We then scaled each row to sum to 1. Then to transition between states we interpreted the ith row as a probability distribution over which state follows state i. To compute the reward associated with each transition\nwe created a random matrix as above, but without normalising. We set γ = 0.9 and then ran a brute force Monte Carlo simulation to compute the true discounted value of each state.\nThe λ parameter for HL(λ) was simply set to 1.0 as the environment is stationary. For TD we experimented with a range of parameter settings and learning rate decrease functions. We found that a fixed learning rate of α = 0.2, and a decreasing rate of 1.53√t performed reasonable well, but never as well as HL(λ). The results were generated by averaging over 10 runs, and are shown in Figure 3.\nAlthough the structure of this Markov process is quite different to that used in the previous experiment, the results are again similar: HL(λ) preforms as well or better than TD(λ) from the beginning to the end of the run. Furthermore, stability in the error towards the end of the run is better with HL(λ) and no manual learning tuning was required for these performance gains."
    }, {
      "heading" : "5 Non-stationary Markov process",
      "text" : "The λ parameter in HL(λ), introduced in Equation (4), reduces the importance of old observations when computing the state value estimates. When the environment is stationary this is not useful and so we can set λ = 1.0, however in a non-stationary environment we need to reduce this value so that the state values adapt properly to changes in the environment. The more rapidly the environment is changing, the lower we need to make λ in order to more rapidly forget old observations.\nTo test HL(λ) in such a setting, we used the Markov chain from Section 3, but reduced its size to 21 states to speed up convergence. We used this Markov chain for the first 5,000 time steps. At that point, we changed the reward when transitioning from the last state to middle state to from -1.0 to be 0.5. At time 10,000 we then\nswitched back to the original Markov chain, and so on alternating between the models of the environment every 5,000 steps. At each switch, we also changed the target state values that the algorithm was trying to estimate to match the current configuration of the environment. For this experiment we set γ = 0.9.\nAs expected, the optimal value of λ for HL(λ) fell from 1 down to about 0.9995. This is about what we would expect given that each phase is 5,000 steps long. For TD(λ) the optimal value of λ was around 0.8 and the optimum learning rate was around 0.05. As we would expect, for both algorithms when we pushed λ above its optimal value this caused poor performance in the periods following each switch in the environment (these bad parameter settings are not shown in the results). On the other hand, setting λ too low produced initially fast adaption to each environment switch, but poor performance after that until the next environment change. To get accurate statistics we averaged over 200 runs. The results of these tests appear in Figure 4.\nFor some reason HL(0.9995) learns faster than TD(0.8) in the first half of the first cycle, but only equally fast at the start of each following cycle. We are not sure why this is happening. We could improve the initial speed at which HL(λ) learnt in the last three cycles by reducing λ, however that comes at a performance cost in terms of the lowest mean squared error attained at the end of each cycle. In any case, in this non-stationary situation HL(λ) again performed well."
    }, {
      "heading" : "6 Windy Gridworld",
      "text" : "Reinforcement learning algorithms such as Watkins’ Q(λ) [Wat89] and Sarsa(λ) [RN94, Rum95] are based on temporal difference updates. This suggests that new reinforcement learning algorithms based on HL(λ) should be possible.\nFor our first experiment we took the standard Sarsa(λ) algorithm and modified it in the obvious way to use an HL temporal difference update. In the presentation of this algorithm we have changed notation slightly to make things more consistent with that typical in reinforcement learning. Specifically, we have dropped the t super script as this is implicit in the algorithm specification, and have defined Q(s, a) := V(s,a), E(s, a) := E(s,a) and N(s, a) := N(s,a). Our new reinforcement learning algorithm, which we call HLS(λ) is given in Algorithm 1. Essentially the only changes to the standard Sarsa(λ) algorithm have been to add code to compute the visit counter N(s, a), add a loop to compute the β values, and replace α with β in the temporal difference update.\nTo test HLS(λ) against standard Sarsa(λ) we used the Windy Gridworld environment described on page 146 of [SB98]. This world is a grid of 7 by 10 squares that the agent can move through by going either up, down, left of right. If the agent attempts to move off the grid it simply stays where it is. The agent starts in the 4th row of the 1st column and receives a reward of 1 when it finds its way to the 4th row of the 8th column. To make things more difficult, there is a “wind” blowing the\nAlgorithm 1 HLS(λ)\nInitialise Q(s, a) = 0, N(s, a) = 1 and E(s, a) = 0 for all s, a Initialise s and a repeat\nTake action a, observed r, s′ Choose a′ by using ǫ-greedy selection on Q(s′, ·) ∆ ← r + γQ(s′, a′)−Q(s, a) E(s, a) ← E(s, a) + 1 N(s, a) ← N(s, a) + 1 for all s, a do β((s, a), (s′, a′)) ← 1\nN(s′,a′)−γE(s′,a′) N(s′,a′) N(s,a)\nend for for all s, a do Q(s, a) ← Q(s, a) + β((s, a), (s′, a′))E(s, a)∆ E(s, a) ← γλE(s, a) N(s, a) ← λN(s, a)\nend for s ← s′; a ← a′ until end of run\nagent up 1 row in columns 4, 5, 6, and 9, and a strong wind of 2 in columns 7 and 8. This is illustrated in Figure 5. Unlike in the original version, we have set up this problem to be a continuing discounted task with an automatic transition from the goal state back to the start state.\nWe set γ = 0.99 and in each run computed the empirical future discounted reward at each point in time. As this value oscillated we also ran a moving average through these values with a window length of 50. Each run lasted for 50,000 time steps as this allowed us to see at what level each learning algorithm topped out. These results appear in Figure 6 and were averaged over 500 runs to get accurate statistics.\nDespite putting considerable effort into tuning the parameters of Sarsa(λ), we were unable to achieve a final future discounted reward above 5.0. The settings shown on the graph represent the best final value we could achieve. In comparison HLS(λ) easily beat this result at the end of the run, while being slightly slower than Sarsa(λ) at the start. By setting λ = 0.99 we were able to achieve the same performance as Sarsa(λ) at the start of the run, however the performance at the end of the run was then only slightly better than Sarsa(λ). This combination of superior performance and fewer parameters to tune suggest that the benefits of HL(λ) carry over into the reinforcement learning setting.\nAnother popular reinforcement learning algorithm is Watkins’ Q(λ). Similar to Sarsa(λ) above, we simply inserted the HL(λ) temporal difference update into the usual Q(λ) algorithm in the obvious way. We call this new algorithm HLQ(λ)(not\nshown). The test environment was exactly the same as we used with Sarsa(λ) above. The results this time were more competitive (these results are not shown). Nevertheless, despite spending a considerable amount of time fine tuning the parameters of Q(λ), we were unable to beat HLQ(λ). As the performance advantage was relatively modest, the main benefit of HLQ(λ) was that it achieved this level of performance without having to tune a learning rate."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We have derived a new equation for setting the learning rate in temporal difference learning with eligibility traces. The equation replaces the free learning rate parameter α, which is normally experimentally tuned by hand. In every setting tested, be it stationary Markov chains, non-stationary Markov chains or reinforcement learning, our new method produced superior results.\nTo further our theoretical understanding, the next step would be to try to prove that the method converges to correct estimates. This can be done for TD(λ) under certain assumptions on how the learning rate decreases over time. Hopefully, something similar can be proven for our new method. In terms of experimental results, it would be interesting to try different types of reinforcement learning problems and to more clearly identify where the ability to set the learning rate differently for different state transition pairs helps performance. It would also be good to generalise the result to episodic tasks. Finally, just as we have successfully merged HL(λ) with Sarsa(λ) and Watkins’ Q(λ), we would also like to see if the same can be done with Peng’s Q(λ) [PW96], and perhaps other reinforcement learning algorithms.\nAcknowledgements. This research was funded by the Swiss NSF grant 200020- 107616."
    } ],
    "references" : [ {
      "title" : "Adaptive stepsizes for recursive estimation with applications in approximate dynamic programming",
      "author" : [ "A.P. George", "W.B. Powell" ],
      "venue" : "Journal of Machine Learning,",
      "citeRegEx" : "George and Powell.,? \\Q2006\\E",
      "shortCiteRegEx" : "George and Powell.",
      "year" : 2006
    }, {
      "title" : "Least-squares policy iteration",
      "author" : [ "M.G. Lagoudakis", "R. Parr" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Lagoudakis and Parr.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lagoudakis and Parr.",
      "year" : 2003
    }, {
      "title" : "Increamental multi-step Q-learning",
      "author" : [ "J. Peng", "R.J. Williams" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Peng and Williams.,? \\Q1996\\E",
      "shortCiteRegEx" : "Peng and Williams.",
      "year" : 1996
    }, {
      "title" : "On-line Q-learning using connectionist systems",
      "author" : [ "G.A. Rummery", "M. Niranjan" ],
      "venue" : "Technial Report CUED/F-INFENG/TR 166,",
      "citeRegEx" : "Rummery and Niranjan.,? \\Q1994\\E",
      "shortCiteRegEx" : "Rummery and Niranjan.",
      "year" : 1994
    }, {
      "title" : "Rummery. Problem solving with reinforcement learning",
      "author" : [ "A. G" ],
      "venue" : "PhD thesis, Cambridge University,",
      "citeRegEx" : "G.,? \\Q1995\\E",
      "shortCiteRegEx" : "G.",
      "year" : 1995
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "R. Sutton", "A. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Sutton.,? \\Q1988\\E",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1988
    }, {
      "title" : "Learning from Delayed Rewards",
      "author" : [ "C.J.C.H Watkins" ],
      "venue" : "PhD thesis, King’s College,",
      "citeRegEx" : "Watkins.,? \\Q1989\\E",
      "shortCiteRegEx" : "Watkins.",
      "year" : 1989
    }, {
      "title" : "An adaptive optimal controller for discrete-time markov environments",
      "author" : [ "I.H. Witten" ],
      "venue" : "Information and Control,",
      "citeRegEx" : "Witten.,? \\Q1977\\E",
      "shortCiteRegEx" : "Witten.",
      "year" : 1977
    } ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "We derive an equation for temporal difference learning from statistical principles. Specifically, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibility traces, so called TD(λ), however it lacks the parameter α that specifies the learning rate. In the place of this free parameter there is now an equation for the learning rate that is specific to each state transition. We experimentally test this new learning rule against TD(λ) and find that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins’ Q(λ) and Sarsa(λ) and find that it again offers superior performance without a learning rate parameter.",
    "creator" : "LaTeX with hyperref package"
  }
}