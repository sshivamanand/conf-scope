{
  "name" : "1205.4213.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Structured Prediction via Coactive Learning",
    "authors" : [ "Pannaga Shivaswamy" ],
    "emails" : [ "pannaga@cs.cornell.edu", "tj@cs.cornell.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 5.\n42 13\nv1 [\ncs .L\nG ]\n1 8\nM ay\n2 01\nT ) average regret, even\nthough the learning algorithm never observes cardinal utility values as in conventional online learning. We demonstrate the applicability of our model and learning algorithms on a movie recommendation task, as well as ranking for web-search."
    }, {
      "heading" : "1. Introduction",
      "text" : "In a wide range of systems in use today, the interaction between human and system takes the following form. The user issues a command (e.g. query) and receives a – possibly structured – result in response (e.g. ranking). The user then interacts with the results (e.g. clicks), thereby providing implicit feedback about the user’s utility function. Here are three examples of such systems and their typical interaction patterns:\nWeb-search: In response to a query, a search engine presents the ranking [A,B,C,D, ...] and observes that the user clicks on documents B and D.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nMovie Recommendation: An online service recommends movie A to a user. However, the user rents movie B after browsing the collection. Machine Translation: An online machine translator is used to translate a wiki page from language A to B. The system observes some corrections the user makes to the translated text.\nIn all the above examples, the user provides some feedback about the results of the system. However, the feedback is only an incremental improvement, not necessarily the optimal result. For example, from the clicks on the web-search results we can infer that the user would have preferred the ranking [B,D,A,C, ...] over the one we presented. However, this is unlikely to be the best possible ranking. Similarly in the recommendation example, movie B was preferred over movie A, but there may have been even better movies that the user did not find while browsing. In summary, the algorithm typically receives a slightly improved result from the user as feedback, but not necessarily the optimal prediction nor any cardinal utilities. We conjecture that many other applications fall into this schema, ranging from news filtering to personal robotics.\nOur key contributions in this paper are threefold. First, we formalize Coactive Learning as a model of interaction between a learning system and its user, define a suitable notion of regret, and validate the key modeling assumption – namely whether observable user behavior can provide valid feedback in our model – in a web-search user study. Second, we derive learning algorithms for the Coactive Learning Model, including the cases of linear utility models and convex cost functions, and show O(1/ √ T ) regret bounds in either case with a matching lower bound. The learning algorithms perform structured output prediction (see (Bakir et al., 2007)) and thus can be applied in a wide variety of problems. Several extensions of the model and the algorithm are discussed as well. Third, we provide extensive empirical evaluations of our algorithms on a movie recommendation and a web-search task, showing that the algorithms are highly efficient and effective in practical settings."
    }, {
      "heading" : "2. Related Work",
      "text" : "The Coactive Learning Model bridges the gap between two forms of feedback that have been well studied in online learning. On one side there is the multiarmed bandit model (Auer et al., 2002b;a), where an algorithm chooses an action and observes the utility of (only) that action. On the other side, utilities of all possible actions are revealed in the case of learning with expert advice (Cesa-Bianchi & Lugosi, 2006). Online convex optimization (Zinkevich, 2003) and online convex optimization in the bandit setting (Flaxman et al., 2005) are continuous relaxations of the expert and the bandit problems respectively. Our model, where information about two arms is revealed at each iteration sits between the expert and the bandit setting. Most closely related to Coactive Learning is the dueling bandits setting (Yue et al., 2009; Yue & Joachims, 2009). The key difference is that both arms are chosen by the algorithm in the dueling bandits setting, whereas one of the arms is chosen by the user in the Coactive Learning setting.\nWhile feedback in Coactive Learning takes the form of a preference, it is different from ordinal regression and ranking. Ordinal regression (Crammer & Singer, 2001) assumes training examples (x, y), where y is a rank. In the Coactive Learning model, absolute ranks are never revealed. More closely related is learning with pairs of examples (Herbrich et al., 2000; Freund et al., 2003; Chu & Ghahramani, 2005), since it circumvents the need for ranks; however, existing approaches require an iid assumption and typically perform batch learning. Finally, there is a large body of work on ranking (see (Liu, 2009)). These approaches are different from Coactive Learning as they require training data (x, y) where y is the optimal ranking for query x."
    }, {
      "heading" : "3. Coactive Learning Model",
      "text" : "We now introduce coactive learning as a model of interaction (in rounds) between a learning system (e.g. search engine) and a human (e.g. search user) where both the human and learning algorithm have the same goal (of obtaining good results). At each round t, the learning algorithm observes a context xt ∈ X (e.g. a search query) and presents a structured object yt ∈ Y (e.g. a ranked list of URLs). The utility of yt ∈ Y to the user for context xt ∈ X is described by a utility function U(xt,yt), which is unknown to the learning algorithm. As feedback the user returns an improved object ȳt ∈ Y (e.g. reordered list of URLs), i.e.,\nU(xt, ȳt) > U(xt,yt), (1)\nwhen such an object ȳt exists. In fact, we will also allow violations of (1) when we formally model user feedback in Section 3.1. The process by which the user generates the feedback ȳt can be understood as an approximate utility-maximizing search, but over a user-defined subset Ȳt of all possible Y. This models an approximately and boundedly rational user that may employ various tools (e.g., query reformulations, browsing) to perform this search. Importantly, however, the feedback ȳt is typically not the optimal label\ny∗t := argmaxy∈YU(xt,y). (2)\nIn this way, Coactive Learning covers settings where the user cannot manually optimize the argmax over the full Y (e.g. produce the best possible ranking in websearch), or has difficulty expressing a bandit-style cardinal rating for yt in a consistent manner. This puts our preference feedback ȳt in stark contrast to supervised learning approaches which require (xt,y ∗ t ). But even more importantly, our model implies that reliable preference feedback (1) can be derived from observable user behavior (i.e., clicks), as we will demonstrate in Section 3.2 for web-search. We conjecture that similar feedback strategies also exist for other applications, where users can be assumed to act approximately and boundedly rational according to U .\nDespite the weak preference feedback, the aim of a coactive learning algorithm is to still present objects with utility close to that of the optimal y∗t . Whenever, the algorithm presents an object yt under context xt, we say that it suffers a regret U(xt,y ∗ t )−U(xt,yt) at time step t. Formally, we consider the average regret suffered by an algorithm over T steps as follows:\nREGT = 1\nT\nT ∑\nt=1\n(U(xt,y ∗ t )− U(xt,yt)) . (3)\nThe goal of the learning algorithm is to minimize REGT , thereby providing the human with predictions yt of high utility. Note, however, that a cardinal value of U is never observed by the learning algorithm, but U is only revealed ordinally through preferences (1)."
    }, {
      "heading" : "3.1. Quantifying Preference Feedback Quality",
      "text" : "To provide any theoretical guarantees about the regret of a learning algorithm in the coactive setting, we need to quantify the quality of the user feedback. Note that this quantification is a tool for theoretical analysis, not a prerequisite or parameter to the algorithm. We quantify feedback quality by how much improvement ȳ provides in utility space. In the simplest case, we say that user feedback is strictly α-informative when\nthe following inequality is satisfied:\nU(xt, ȳt)− U(xt,yt) ≥ α(U(xt,y∗t )− U(xt,yt)). (4)\nIn the above inequality, α ∈ (0, 1] is an unknown parameter. Feedback is such that utility of ȳt is higher than that of yt by a fraction α of the maximum possible utility range U(xt,y ∗ t )− U(xt,yt). Violations of the above feedback model are allowed by introducing slack variables ξt ≥ 0:1\nU(xt, ȳt)−U(xt,yt)≥ α(U(xt,y∗t )−U(xt,yt))−ξt. (5)\nWe refer to the above feedback model as α-informative feedback. Note also that it is possible to express feedback of any quality using (5) with an appropriate value of ξt. Our regret bounds will contain ξt, quantifying to what extent the strict α-informative modeling assumption is violated.\nFinally, we will also consider an even weaker feedback model where a positive utility gain is only achieved in expectation over user actions:\nEt[U(xt,ȳt)−U(xt,yt)]≥α(U(xt,y∗t)−U(xt,yt))− ξ̄t. (6)\nWe refer to the above feedback as expected αinformative feedback. In the above equation, the expectation is over the user’s choice of ȳt given yt under context xt (i.e., under a distribution Pxt [ȳt|yt] which is dependent on xt)."
    }, {
      "heading" : "3.2. User Study: Preferences from Clicks",
      "text" : "We now validate that reliable preferences as specified in Equation (1) can indeed be inferred from implicit user behavior. In particular, we focus on preference feedback from clicks in web-search and draw upon data from a user study (Joachims et al., 2007). In their study, subjects (undergraduate students, n = 16) were\n1Strictly speaking, the value of the slack variable depends on the choice of α and the definition of utility. However, for brevity, we do not explicitly show this dependence.\nasked to answer 10 questions – 5 informational, 5 navigational – using the Google search engine. All queries, result lists, and clicks were recorded. For each subject, queries were grouped into query chains by question2. On average, each query chain contained 2.2 queries and 1.8 clicks in the result lists.\nWe use the following strategy to infer a ranking ȳ from the user’s clicks: prepend to the ranking y from the first query of the chain all results that the user clicked throughout the whole query chain. To assess whether U(x, ȳ) is indeed larger than U(x,y) as assumed in our learning model, we measure utility in terms of a standard measure of retrieval quality from Information Retrieval. We use DCG@10(x,y) = ∑10\ni=1 r(x,y[i]) log i+1 ,\nwhere r(x,y[i]) is the relevance score of the i-th document in ranking y (see e.g. (Manning et al., 2008)). To get ground-truth relevance assessments r(x, d), five human assessors were asked to manually rank the set of results encountered during each query chain. We then linearly normalize the resulting ranks to a relative relevance score r(x, d) ∈ [0..5] for each document. We can now evaluate whether the feedback ranking ȳ is indeed better than the ranking y that was originally presented, i.e. DCG@10(x, ȳ) > DCG@10(x,y). Figure 1 plots the Cumulative Distribution functions (CDFs) of DCG@10(x, ȳ) −DCG@10(x,y) for three experimental conditions, as well as the average over all conditions. All CDFs are shifted far to the right of 0, showing that preference feedback from our strategy is highly accurate and informative. Focusing first on the average over all conditions, the utility difference is strictly positive on ∼ 60% of all queries, and strictly negative on only ∼ 10%. This imbalance is significant (binomial sign test, p < 0.0001). Among the remaining ∼ 30% of cases where the DCG@10 difference is zero, 88% are due to ȳ = y (i.e. click only on top 1 or no click). Note that a learning algorithm can easily detect those cases and may explicitly eliminate them as feedback. Overall, this shows that implicit feedback can indeed produce accurate preferences.\nWhat remains to be shown is whether the reliability of the feedback is affected by the quality of the current prediction, i.e., U(xt,yt). In the user study, some users actually received results for which retrieval quality was degraded on purpose. In particular, about one third of the subjects received Google’s top 10 results in reverse order (condition “reversed”) and another third received rankings with the top two positions swapped (condition “swapped”). As Figure 1 shows, we find that users provide accurate preferences across this sub-\n2This was done manually, but can be automated with high accuracy (Jones & Klinkner, 2008).\nAlgorithm 1 Preference Perceptron.\nInitialize w1 ← 0 for t = 1 to T do Observe xt Present yt ← argmaxy∈Yw⊤t φ(xt,y) Obtain feedback ȳt Update: wt+1 ← wt + φ(xt, ȳt)− φ(xt,yt) end for\nstantial range of retrieval quality. Intuitively, a worse retrieval system may make it harder to find good results, but it also makes an easier baseline to improve upon. This intuition is formally captured in our definition of α-informative feedback. The optimal value of the α vs. ξ trade-off, however, will likely depend on many application-specific factors, like user motivation, corpus properties, and query difficulty. In the following, we therefore present algorithms that do not require knowledge of α, theoretical bounds that hold for any value of α, and experiments that explore a large range of α."
    }, {
      "heading" : "4. Coactive Learning Algorithms",
      "text" : "In this section, we present algorithms for minimizing regret in the coactive learning model. In the rest of this paper, we use a linear model for the utility function,\nU(x,y) = w⊤∗ φ(x,y), (7)\nwhere w∗ ∈ RN is an unknown parameter vector and φ : X × Y → RN is a joint feature map such that ‖φ(x,y)‖ℓ2 ≤ R for any x ∈ X and y ∈ Y. Note that both x and y can be structured objects.\nWe start by presenting and analyzing the most basic algorithm for the coactive learning model, which we call the Preference Perceptron (Algorithm 1). The Preference Perceptron maintains a weight vector wt which is initialized to 0. At each time step t, the algorithm observes the context xt and presents an object y that maximizes w⊤t φ(xt,y). The algorithm then observes user feedback ȳt and the weight vector wt is updated in the direction φ(xt, ȳt)− φ(xt,yt).\nTheorem 1 The average regret of the preference perceptron algorithm can be upper bounded, for any α ∈ (0, 1] and for any w∗ as follows:\nREGT ≤ 1\nαT\nT ∑\nt=1\nξt + 2R‖w∗‖ α √ T . (8)\nProof First, consider ‖wT+1‖2, we have,\nw⊤T+1wT+1 = w ⊤ T wT + 2w ⊤ T (φ(xT , ȳT )− φ(xT ,yT ))\n+ (φ(xT , ȳT )− φ(xT ,yT ))⊤(φ(xT , ȳT )− φ(xT ,yT ) ≤ w⊤T wT + 4R2 ≤ 4R2T.\nOn line one, we simply used our update rule from algorithm 1. On line two, we used the fact that w⊤T (φ(xT , ȳT ) − φ(xT ,yT )) ≤ 0 from the choice of yT in Algorithm 1 and that ‖φ(x,y)‖ ≤ R. Further, from the update rule in algorithm 1, we have,\nw⊤T+1w∗ = w ⊤ T w∗ + (φ(xT , ȳT )− φ(xT ,yT ))⊤w∗\n=\nT ∑\nt=1\n(U(xt, ȳt)− U(xt,yt)) . (9)\nWe now use the fact that w⊤T+1w∗ ≤ ‖w∗‖‖wT+1‖ (Cauchy-Schwarz inequality), which implies\nT ∑\nt=1\n(U(xt, ȳt)− U(xt,yt)) ≤ 2R √ T‖w∗‖.\nFrom the α-informative modeling of the user feedback in (5), we have\nα\nT ∑\nt=1\n(U(xt,y ∗ t )− U(xt,yt))−\nT ∑\nt=1\nξt ≤ 2R √ T‖w∗‖,\nfrom which the claimed result follows.\nThe first term in the regret bound denotes the quality of feedback in terms of violation of the strict αinformative feedback. In particular, if the user feedback is strictly α-informative, then all slack variables in (8) vanish and REGT = O(1/ √ T ).\nThough user feedback is modeled via α-informative feedback, the algorithm itself does not require the knowledge of α; α plays a role only in the analysis.\nAlthough the preference perceptron appears similar to the standard perceptron for multi-class classification problems, there are key differences. First, the standard perceptron algorithm requires the true label y∗ as feedback, whereas much weaker feedback ȳ suffices for our algorithm. Second, the standard analysis of the perceptron bounds the number of mistakes made by the algorithm based on margin and the radius of the examples. In contrast, our analysis bounds a different regret that captures a graded notion of utility.\nAn appealing aspect of our learning model is that several interesting extensions are possible. We discuss some of them in the rest of this section."
    }, {
      "heading" : "4.1. Lower Bound",
      "text" : "We now show that the upper bound in Theorem 1 cannot be improved in general.\nLemma 2 For any coactive learning algorithm A with linear utility, there exist xt, objects Y and w∗ such that REGT of A in T steps is Ω(1/ √ T ).\nProof Consider a problem where Y = {−1,+1},X = {x ∈ RT : ‖x‖ = 1}. Define the joint feature map as φ(x,y) = yx. Consider T contexts e1, . . . , eT such that ej has only the j\nth component equal to one and all the others equal to zero. Let y1, . . .yT be the sequence of outputs of A on contexts e1, . . . , eT . Construct w∗ = [−y1/ √ T − y2/ √ T · · · − yT / √ T ]⊤, we have for this construction ‖w∗‖ = 1. Let the user feedback on the tth step be −yt. With these choices, the user feedback is always α-informative with α = 1 since y∗t = −yt. Yet, the regret of the algorithm is 1 T ∑T t=1(w ⊤ ∗ φ(et,y ∗ t )−w⊤∗ φ(et,yt)) = Ω( 1√T )."
    }, {
      "heading" : "4.2. Batch Update",
      "text" : "In some applications, due to high volumes of feedback, it might not be possible to do an update after every round. For such scenarios, it is natural to consider a variant of Algorithm 1 that makes an update every k iterations; the algorithm simply uses wt obtained from the previous update until the next update. It is easy to show the following regret bound for batch updates:\nREGT ≤ 1\nαT\nT ∑\nt=1\nξt + 2R‖w∗‖\n√ k\nα √ T\n."
    }, {
      "heading" : "4.3. Expected α-Informative Feedback",
      "text" : "So far, we have characterized user behavior in terms of deterministic feedback actions. However, if a bound on the expected regret suffices, the weaker model of Expected α-Informative Feedback from Equation (6) is applicable.\nCorollary 3 Under expected α-informative feedback model, the expected regret (over user behavior distribution) of the preference perceptron algorithm can be upper bounded as follows:\nE[REGT ] ≤ 1\nαT\nT ∑\nt=1\nξ̄t + 2R‖w∗‖ α √ T . (10)\nThe above corollary can be proved by following the argument of Theorem 1, but taking expectations over user feedback: E[w⊤T+1wT+1] = E[w ⊤ T wT ] +\nAlgorithm 2 Convex Preference Perceptron.\nInitialize w1 ← 0 for t = 1 to T do Set ηt ← 1√t Observe xt Present yt ← argmaxy∈Yw⊤t φ(xt,y) Obtain feedback ȳt Update: w̄t+1 ← wt + ηtG(φ(xt, ȳt)− φ(xt,yt)) Project: wt+1 ← argminu∈B ‖u− w̄t+1‖2 end for\nE[2w⊤T (φ(xT , ȳT ) − φ(xT ,yT ))] + ET [(φ(xT , ȳT ) − φ(xT ,yT ))\n⊤(φ(xT , ȳT ) − φ(xT ,yT )] ≤ E[w⊤T wT ] + 4R2. In the above, E denotes expectation over all user feedback ȳt given yt under the context xt. It follows that E[w⊤T+1wT+1] ≤ 4TR2. Applying Jensen’s inequality on the concave function\n√·, we get: E[w⊤T w∗] ≤ ‖w∗‖E[‖wT‖] ≤ ‖w∗‖ √ E[w⊤T wT ]. The corollary follows from the definition of expected α-informative feedback."
    }, {
      "heading" : "4.4. Convex Loss Minimization",
      "text" : "We now generalize our results to minimize convex losses defined on the linear utility differences. We assume that at every time step t, there is an (unknown) convex loss function ct : R → R which determines the loss ct(U(xt,yt)−U(xt,y∗t )) at time t. The functions ct are assumed to be non-increasing. Further, subderivatives of the ct’s are assumed to be bounded (i.e., c′t(θ) ∈ [−G, 0] for all t and for all θ ∈ R). The vector w∗ which determines the utility of yt under context xt is assumed from a closed and bounded convex set B whose diameter is denoted as |B|. Algorithm 2 minimizes the average convex loss. There are two differences between this algorithm and Algorithm 1. Firstly, there is a rate ηt associated with the update at time t. Moreover, after every update, the resulting vector w̄t+1 is projected back to the set B. We have the following result for Algorithm 2.\nTheorem 4 For the convex preference perceptron, we have, for any α ∈ (0, 1] and any w∗ ∈ B,\n1\nT\nT ∑\nt=1\nct(U(xt,yt)− U(xt,y∗t )) ≤ 1\nT\nT ∑\nt=1\nct (0)\n+ 2G\nαT\nT ∑\nt=1\nξt + 1\nα ( |B|G 2 √ T + |B|G T + 4R2G√ T ) . (11)\nProof First, we divide the set of time steps into two\ndifferent sets based on the nature of feedback:\nI := {t : U(xt, ȳt)− U(xt,yt)) ≥ 0; 1 ≤ t ≤ T }, J := {t : U(xt, ȳt)− U(xt,yt)) < 0; 1 ≤ t ≤ T }.\nFor brevity we denote φ(xt,y)− φ(xt,yt) by ∆(y,yt) for any y ∈ Y in the rest of this proof. We start by considering the following term for a single time step t:\nct(U(xt,yt)− U(xt,y∗t ))− ct ( w⊤t ∆(yt, ȳt)\nα\n)\n≤ct ( w⊤∗ ∆(yt, ȳt)\nα − ξt α\n) − ct ( w⊤t ∆(yt, ȳt)\nα\n)\n≤ ( (w∗ −wt)⊤∆(yt, ȳt) α − ξt α ) c′t ( w⊤∗ ∆(yt, ȳt) α − ξt α )\n≤\n\n\n\nG(w⊤t ∆(yt, ȳt) + ξt −w⊤∗ ∆(yt, ȳt))/α t ∈ I\nG(w⊤t ∆(yt, ȳt) + ξt)/α t ∈ J.\nIn the above inequalities, the second line follows from α-informative feedback and the fact that ct is nonincreasing. The third line follows from the fact that ct is a convex functions.3 Since −ξt/α ≤ 0 (by definition, slack variables are non-negative) and −w⊤t ∆(yt, ȳt) ≤ 0 (from our choice of yt in the Algorithm) we get the first two terms on line four (irrespective of t ∈ I or t ∈ J). However, w⊤∗ ∆(yt, ȳt) is either positive or negative depending on the feedback which leads to two different cases depending on whether t ∈ I or t ∈ J . Summing the above inequalities from 1 to T , we get:\nT ∑\nt=1\nct(w ⊤ ∗ ∆(yt,y ∗ t ))−\nT ∑\nt=1\nct\n( w⊤t ∆(yt, ȳt)\nα\n)\n≤ G α\nT ∑\nt=1\nw⊤t ∆(yt, ȳt) + G\nα\nT ∑\nt=1\nξt − G\nα\n∑ t∈I w⊤∗ ∆(yt, ȳt)\n≤ G α\nT ∑\nt=1\n(wt −w∗)⊤∆(yt, ȳt) + G\nα\nT ∑\nt=1\nξt\n+ G\nα\n∑ t∈J w⊤∗ ∆(yt, ȳt). (12)\nWe obtained the last line above simply by adding and and subtracting G ∑\nt∈J w ⊤ ∗ ∆(yt, ȳt)/α on the right\nside of the previous inequality.\nTo bound the first term on the right hand side of (12),\n3For any convex function f , f(y)− f(x) ≤ (y − x)f ′(y) where f ′(y) denotes a sub-derivative of f at y.\nconsider the following:\n‖w̄t+1 −w∗‖2 = ‖wt + ηt∆(ȳt,yt)−w∗‖2\n= ‖wt −w∗‖2 + η2t ‖∆(ȳt,yt)‖2 + 2ηt(wt −w∗)⊤∆(ȳt,yt). (13)\nRearranging terms in the above equation, we get:\n(wt −w∗)⊤∆(yt, ȳt)\n= 1\n2ηt ‖wt −w∗‖2 −\n1\n2ηt ‖w̄t+1 −w∗‖2 + ηt 2 ‖∆(ȳt,yt)‖2\n≤ 1 2ηt ‖wt −w∗‖2 − 1 2ηt ‖wt+1 −w∗‖2 + 2ηtR2\nwhere, on the last line, we used the fact that ‖wt+1 − w∗‖2 ≤ ‖w̄t+1 −w∗‖2 since the wt+1 is just the projection of w̄t+1 to the convex set B (which contains the vector w∗). We can now bound the first term in (12) using a standard telescoping argument from (Zinkevich, 2003) as follows:\nT ∑\nt=1\n(\n1\n2ηt ‖wt −w∗‖2 −\n1\n2ηt ‖wt+1 −w∗‖2 + 2ηtR2\n)\n≤ 1 2η1\n‖w1 −w∗‖2 + T ∑\nt=2\n(\n1 2ηt − 1 2ηt−1\n)\n‖wt −w∗‖2\n+ 2R2 T ∑\nt=1\nηt\n≤ 1 2η1\n|B|+ T ∑\nt=2\n(\n1 2ηt − 1 2ηt−1\n) |B|+ 2R2(2 √ T − 1)\n≤ √ T + 1\n2 |B|+ 4R2\n√ T .\nNow, consider the third term on the right hand side of (12):\nw⊤∗ ∆(yt, ȳt)\nα ≤ w⊤∗ ∆(yt,y∗t ) + ξt α ≤ ξt α .\nThe first inequality above follows from α-informative feedback. Whereas the second inequality follows from the fact w⊤∗ ∆(yt,y ∗ t ) ≤ 0 from the definition of y∗t . We obtain ct(0) in the bound (11) from the fact that ct ( w⊤t ∆(yt, ȳt)/α )\n≤ ct(0) from our choice of yt and non-increasing c′ts. Finally, the bound (11) follows from the trivial fact 0 ≤ G\nα\n∑\nt∈I ξt.\nIn the bound (11), ct(0) is the minimum possible convex loss since U(xt,yt) − U(xt,y∗t ) can never be\ngreater than zero by definition of y∗t . Thus the theorem upper bounds the average convex loss via the minimum achievable loss and the quality of feedback. Like the previous result (Theorem 1), under strict αinformative feedback, the average loss approaches the best achievable loss atO(1/ √ T ) albeit with larger constant factors."
    }, {
      "heading" : "5. Experiments",
      "text" : "In this section, we empirically evaluate the Preference Perceptron algorithm on two real-world datasets. The two experiments differ in the nature of prediction and feedback. In the first experiment, the algorithm operates on structured objects (rankings). In the second experiment, atomic items (movies) are presented and received as feedback."
    }, {
      "heading" : "5.1. Structured Feedback: Learning to Rank",
      "text" : "We evaluated our Preference Perceptron algorithm on the Yahoo! learning to rank dataset (Chapelle & Chang, 2011). This dataset consists of query-url feature vectors (denoted as xqi for query q and URL i), each with a relevance rating rqi that ranges from zero (irrelevant) to four (perfectly relevant). To pose ranking as a structured prediction problem, we defined our joint feature map as follows:\nw⊤φ(q,y) =\n5 ∑\ni=1\nw⊤xq yi\nlog(i+ 1) . (14)\nIn the above equation, y denotes a ranking such that yi is the index of the URL which is placed at position i in the ranking. Thus, the above measure considers the top five URLs for a query q and computes a score based on a graded relevance. Note that the above utility function defined via the feature-map is analogous to DCG@5 (see e.g. (Manning et al., 2008)) after replacing the relevance label with a linear prediction based on the features.\nFor query qt at time step t, the Preference Perceptron algorithm presents the ranking yqt that maximizes w⊤t φ(qt,y). Note that this merely amounts to sorting documents by the scores w⊤t x qt i , which can be done very efficiently. The utility regret in Eqn. (3), based on the definition of utility in (14), is given by 1 T ∑T t=1 w ⊤ ∗ (φ(qt,y\nqt∗)−φ(qt,yqt)). Here yqt∗ denotes the optimal ranking with respect to w∗, which is the best least squares fit to the relevance labels from the features using the entire dataset. Query ordering was randomly permuted twenty times and we report average and standard error of the results."
    }, {
      "heading" : "5.1.1. Strong Vs Weak Feedback",
      "text" : "The goal of the first experiment was to see how the regret of the algorithm changes with feedback quality. To get feedback at different quality levels α, we used the following mechanism. Given the predicted ranking yt, the user would go down the list until she found five URLs such that, when placed at the top of the list, the resulting ȳt satisfied the strictly α-informative feedback condition w.r.t. the optimal w∗.\nFigure 2 shows the results for this experiment for two different α values. As expected, the regret with α = 1.0 is lower compared to the regret with respect α = 0.1. Note, however, that the difference between the two curves is much smaller than a factor of ten. This is because strictly α-informative feedback is also strictly β-informative feedback for any β ≤ α. So, there could be several instances where user feedback was much stronger than what was required. As expected from the theoretical bounds, since the user feedback is based on a linear model with no noise, utility regret approaches zero."
    }, {
      "heading" : "5.1.2. Noisy Feedback",
      "text" : "In the previous experiment, user feedback was based on actual utility values computed from the optimal w∗. We next make use of the actual relevance labels provided in the dataset for user feedback. Now, given a ranking for a query, the user would go down the list inspecting the top 10 URLs (or all the URLs if the list is shorter) as before. Five URLs with the highest relevance labels (rqi ) are placed at the top five locations in the user feedback. Note that this produces noisy feedback since no linear model can perfectly fit the relevance labels on this dataset.\nAs a baseline, we repeatedly trained a conventional Ranking SVM4. At each iteration, the previous SVM model was used to present a ranking to the user. The user returned a ranking based on the relevance labels as above. The pairs of examples (qt,y qt svm) and\n4http://svmlight.joachims.org\n(qt, ȳ qt svm) were used as training pairs for the ranking SVMs. Note that training a ranking SVM after each iteration would be prohibitive, since it involves solving a quadratic program and cross-validating the regularization parameter C. Thus, we retrained the SVM whenever 10% more examples were added to the training set. The first training was after the first iteration with just one pair of examples (starting with a random yq1 ), and the C value was fixed at 100 until there were 50 pairs of examples, when reliable cross-validation became possible. After there were more than 50 pairs in the training set, the C value was obtained via five-fold cross-validation. Once the C value was determined, the SVM was trained on all the training examples available at that time. The same SVM model was then used to present rankings until the next retraining.\nResults of this experiment are presented in Figure 3. Since the feedback is now based on noisy relevance labels, the utility regret converges to a non-zero value as predicted by our theoretical results. Over most of the range, the Preference Perceptron performs significantly5 better than the SVM on both the metrics. We conjecture that the regret values for both the algorithms can be improved with better features or kernels, but these extensions are orthogonal to the main focus of this paper. Also note that the perceptron experiment took around 30 minutes to run, whereas the SVM experiment took about 20 hours on the same machine."
    }, {
      "heading" : "5.2. Item Feedback: Movie Recommendation",
      "text" : "In contrast to the structured prediction problem in the previous section, we now evaluate the Preference Perceptron on a task with atomic predictions, namely movie recommendation. In each iteration a movie is presented to the user, and the feedback consists of a movie as well. We use the MovieLens dataset, which consists of a million ratings over 3090 movies rated by 6040 users. The movie ratings ranged from one to five.\nWe randomly divided users into two equally sized sets. The first set was used to obtain a feature vectormj for\n5The error bars are extremely tiny at higher iterations.\neach movie j using the “SVD embedding” method for collaborative filtering (see (Bell & Koren, 2007), Eqn. (15)). The dimensionality of the feature vectors and the regularization parameters were chosen to optimize cross-validation accuracy on the first dataset in terms of squared error. For the second set of users, we then considered the problem of recommending movies based on the movie features mj. This experiment setup simulates the task of recommending movies to a new user based on movie features from old users.\nFor each user i in the second set, we found the best least squares approximation wTi∗mj to the user’s utility functions on the available ratings. This enables us to impute utility values for movies that were not explicitly rated by this user. Furthermore, it allows us to measure regret for each user as 1\nT\n∑T t=1 w ⊤ i∗(mt∗−mt),\nwhich is the average difference in utility between the recommended movie mt and the best available movie mt∗. We denote the best available movie at time t by mt∗, since in this experiment, once a user gave a particular movie as feedback, both the recommended movie and the feedback movie were removed from the set of candidates for subsequent recommendations."
    }, {
      "heading" : "5.2.1. Strong Vs Weak Feedback",
      "text" : "Analogous to the web-search experiments, we first explore how the performance of the Preference Perceptron changes with feedback quality α. In particular, we recommended a movie with maximum utility according to the current wt of the algorithm, and the user returns as feedback a movie with the smallest utility that still satisfied strictly α-informative feedback according to wi∗. For every user in the second set, the algorithm iteratively recommended 1500 movies in this way. Regret was calculated after each iteration and separately for each user, and all regrets were averaged over all the users in the second set.\nFigure 4 shows the results for this experiment. Since the feedback in this case is strictly α-informative, the average regret in all the cases decreases towards zero as expected. Note that even for a moderate value of α, regret is already substantially reduced after 10’s of\niterations. With higher α values, the regret converges to zero at a much faster rate than with lower α values."
    }, {
      "heading" : "5.2.2. Noisy Feedback",
      "text" : "We now consider noisy feedback, where the user feedback does not necessarily match the linear utility model used by the algorithm. In particular, feedback is now given based on the actual ratings when available, or the score u⊤i∗mj rounded to the nearest allowed rating value. In every iteration, the user returned a movie with one rating higher than the one presented to her. If the algorithm already presented a movie with the highest rating, it was assumed that the user gave the same movie as feedback.\nAs a baseline, we again ran a ranking SVM. Like in the web-search experiment, it was retrained whenever 10% more training data was added. The results for this experiment are shown in Figure 5. The regret of the Preference Perceptron is again significantly lower than that of the SVM, and at a small fraction of the computational cost."
    }, {
      "heading" : "6. Conclusions",
      "text" : "We proposed a new model of online learning where preference feedback is observed but cardinal feedback is never observed. We proposed a suitable notion of regret and showed that it can be minimized under our feedback model. Further, we provided several extensions of the model and algorithms. Furthermore, experiments demonstrated its effectiveness for websearch ranking and a movie recommendation task. A future direction is to consider λ-strongly convex functions, and we conjecture it is possible to derive algorithms with O(log(T )/T ) regret in this case.\nAcknowledgements We thank Peter Frazier, Bobby Kleinberg, Karthik Raman and Yisong Yue for helpful discussions. This work was funded in part under NSF awards IIS-0905467 and IIS-1142251."
    } ],
    "references" : [ {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "The non-stochastic multi-armed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R. Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Predicting Structured Data",
      "author" : [ "G.H. Bakir", "T. Hofmann", "B. Schölkopf", "A.J. Smola", "B. Taskar", "Vishwanathan", "S.V.N. (eds" ],
      "venue" : null,
      "citeRegEx" : "Bakir et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bakir et al\\.",
      "year" : 2007
    }, {
      "title" : "Scalable collaborative filtering with jointly derived neighborhood interpolation weights",
      "author" : [ "R.M. Bell", "Y. Koren" ],
      "venue" : "In ICDM,",
      "citeRegEx" : "Bell and Koren,? \\Q2007\\E",
      "shortCiteRegEx" : "Bell and Koren",
      "year" : 2007
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi",
      "year" : 2006
    }, {
      "title" : "Yahoo! learning to rank challenge overview",
      "author" : [ "O. Chapelle", "Y. Chang" ],
      "venue" : "JMLR - Proceedings Track,",
      "citeRegEx" : "Chapelle and Chang,? \\Q2011\\E",
      "shortCiteRegEx" : "Chapelle and Chang",
      "year" : 2011
    }, {
      "title" : "Preference learning with gaussian processes",
      "author" : [ "W. Chu", "Z. Ghahramani" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Chu and Ghahramani,? \\Q2005\\E",
      "shortCiteRegEx" : "Chu and Ghahramani",
      "year" : 2005
    }, {
      "title" : "Pranking with ranking",
      "author" : [ "K. Crammer", "Y. Singer" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Crammer and Singer,? \\Q2001\\E",
      "shortCiteRegEx" : "Crammer and Singer",
      "year" : 2001
    }, {
      "title" : "Online convex optimization in the bandit setting: gradient descent without a gradient",
      "author" : [ "A. Flaxman", "A.T. Kalai", "H.B. McMahan" ],
      "venue" : "In SODA,",
      "citeRegEx" : "Flaxman et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Flaxman et al\\.",
      "year" : 2005
    }, {
      "title" : "An efficient boosting algorithm for combining preferences",
      "author" : [ "Y. Freund", "R.D. Iyer", "R.E. Schapire", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Freund et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Freund et al\\.",
      "year" : 2003
    }, {
      "title" : "Large margin rank boundaries for ordinal regression. In Advances in Large Margin Classifiers",
      "author" : [ "R. Herbrich", "T. Graepel", "K. Obermayer" ],
      "venue" : null,
      "citeRegEx" : "Herbrich et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Herbrich et al\\.",
      "year" : 2000
    }, {
      "title" : "Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search",
      "author" : [ "T. Joachims", "L. Granka", "Pan", "Bing", "H. Hembrooke", "F. Radlinski", "G. Gay" ],
      "venue" : "ACM Transactions on Information Systems (TOIS),",
      "citeRegEx" : "Joachims et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Joachims et al\\.",
      "year" : 2007
    }, {
      "title" : "Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs",
      "author" : [ "R. Jones", "K. Klinkner" ],
      "venue" : "In CIKM,",
      "citeRegEx" : "Jones and Klinkner,? \\Q2008\\E",
      "shortCiteRegEx" : "Jones and Klinkner",
      "year" : 2008
    }, {
      "title" : "Learning to rank for information retrieval",
      "author" : [ "Liu", "T-Y" ],
      "venue" : "Foundations and Trends in Information Retrieval,",
      "citeRegEx" : "Liu and T.Y.,? \\Q2009\\E",
      "shortCiteRegEx" : "Liu and T.Y.",
      "year" : 2009
    }, {
      "title" : "Introduction to Information Retrieval",
      "author" : [ "C. Manning", "P. Raghavan", "H. Schütze" ],
      "venue" : null,
      "citeRegEx" : "Manning et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2008
    }, {
      "title" : "Interactively optimizing information retrieval systems as a dueling bandits problem",
      "author" : [ "Y. Yue", "T. Joachims" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Yue and Joachims,? \\Q2009\\E",
      "shortCiteRegEx" : "Yue and Joachims",
      "year" : 2009
    }, {
      "title" : "The k-armed dueling bandits problem",
      "author" : [ "Y. Yue", "J. Broder", "R. Kleinberg", "T. Joachims" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Yue et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yue et al\\.",
      "year" : 2009
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "M. Zinkevich" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Zinkevich,? \\Q2003\\E",
      "shortCiteRegEx" : "Zinkevich",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "The learning algorithms perform structured output prediction (see (Bakir et al., 2007)) and thus can be applied in a wide variety of problems.",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 17,
      "context" : "Online convex optimization (Zinkevich, 2003) and online convex optimization in the bandit setting (Flaxman et al.",
      "startOffset" : 27,
      "endOffset" : 44
    }, {
      "referenceID" : 8,
      "context" : "Online convex optimization (Zinkevich, 2003) and online convex optimization in the bandit setting (Flaxman et al., 2005) are continuous relaxations of the expert and the bandit problems respectively.",
      "startOffset" : 98,
      "endOffset" : 120
    }, {
      "referenceID" : 16,
      "context" : "Most closely related to Coactive Learning is the dueling bandits setting (Yue et al., 2009; Yue & Joachims, 2009).",
      "startOffset" : 73,
      "endOffset" : 113
    }, {
      "referenceID" : 10,
      "context" : "More closely related is learning with pairs of examples (Herbrich et al., 2000; Freund et al., 2003; Chu & Ghahramani, 2005), since it circumvents the need for ranks; however, existing approaches require an iid assumption and typically perform batch learning.",
      "startOffset" : 56,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "More closely related is learning with pairs of examples (Herbrich et al., 2000; Freund et al., 2003; Chu & Ghahramani, 2005), since it circumvents the need for ranks; however, existing approaches require an iid assumption and typically perform batch learning.",
      "startOffset" : 56,
      "endOffset" : 124
    }, {
      "referenceID" : 11,
      "context" : "In particular, we focus on preference feedback from clicks in web-search and draw upon data from a user study (Joachims et al., 2007).",
      "startOffset" : 110,
      "endOffset" : 133
    }, {
      "referenceID" : 14,
      "context" : "(Manning et al., 2008)).",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 17,
      "context" : "We can now bound the first term in (12) using a standard telescoping argument from (Zinkevich, 2003) as follows:",
      "startOffset" : 83,
      "endOffset" : 100
    }, {
      "referenceID" : 14,
      "context" : "(Manning et al., 2008)) after replacing the relevance label with a linear prediction based on the features.",
      "startOffset" : 0,
      "endOffset" : 22
    } ],
    "year" : 2012,
    "abstractText" : "We propose Coactive Learning as a model of interaction between a learning system and a human user, where both have the common goal of providing results of maximum utility to the user. At each step, the system (e.g. search engine) receives a context (e.g. query) and predicts an object (e.g. ranking). The user responds by correcting the system if necessary, providing a slightly improved – but not necessarily optimal – object as feedback. We argue that such feedback can often be inferred from observable user behavior, for example, from clicks in web-search. Evaluating predictions by their cardinal utility to the user, we propose efficient learning algorithms that have O( 1 √ T ) average regret, even though the learning algorithm never observes cardinal utility values as in conventional online learning. We demonstrate the applicability of our model and learning algorithms on a movie recommendation task, as well as ranking for web-search.",
    "creator" : "gnuplot 4.4 patchlevel 3"
  }
}