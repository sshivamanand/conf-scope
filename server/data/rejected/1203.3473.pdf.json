{
  "name" : "1203.3473.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Lifted Inference for Relational Continuous Models",
    "authors" : [ "Jaesik Choi", "David J. Hill" ],
    "emails" : [ "jaesik@illinois.edu", "eyal@illinois.edu", "ecodavid@rci.rutgers.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Relational Continuous Models (RCMs) represent joint probability densities over attributes of objects, when the attributes have continuous domains. With relational representations, they can model joint probability distributions over large numbers of variables compactly in a natural way. This paper presents a new exact lifted inference algorithm for RCMs, thus it scales up to large models of real world applications. The algorithm applies to Relational Pairwise Models which are (relational) products of potentials of arity 2. Our algorithm is unique in two ways. First, it substantially improves the efficiency of lifted inference with variables of continuous domains. When a relational model has Gaussian potentials, it takes only linear-time compared to cubic time of previous methods. Second, it is the first exact inference algorithm which handles RCMs in a lifted way. The algorithm is illustrated over an example from econometrics. Experimental results show that our algorithm outperforms both a groundlevel inference algorithm and an algorithm built with previously-known lifted methods."
    }, {
      "heading" : "1 Introduction",
      "text" : "Many real world systems are described by continuous variables and relations among them. Such systems include measurements in environmental-sensors networks (Hill et al., 2009), localizations in robotics (Limketkai et al., 2005), and economic forecastings in finance (Niemira & Saaty, 2004). Once a relational model among variables is given, inference algorithms can solve value prediction problems and classification problems.\nAt a ground level, inference with a large number of continuous variables is non-trivial. Typically, inference is the task of calculating a marginal over variables of interest. Suppose that a market index has a relationship with n variables,\nrevenues of n banks. When marginalizing out the market index, the marginal is a function of n variables (revenues of banks), thus marginalizing out remaining variables becomes harder. When n grows, the computation becomes expensive. For example, when relations among variables follow Gaussian distributions, the computational complexity of the inference problem is O(|U|3) (U is a set of ground variables). Thus, the computation with such models is limited to moderate-size models, preventing its use in the many large, real-world applications.\nTo address these issues, Relational Probabilistic Languages (RPLs) (Ng & Subrahmanian, 1992; Koller & Pfeffer, 1997; Pfeffer et al., 1999; Friedman et al., 1999; Poole, 2003; de Salvo Braz et al., 2005; Richardson & Domingos, 2006; Milch & Russell, 2007; Getoor & Taskar, 2007) describe probability distributions at a relational level with the purpose of capturing larger models. RPLs combine probability theory for handling uncertainty and relational models for representing system structures. Thus, they facilitate construction and learning of probabilistic models for large systems. Recently, (Poole, 2003; de Salvo Braz et al., 2005; Milch et al., 2008; Singla & Domingos, 2008) showed that such models enable more efficient inference than possible with propositional graphical models, when inference occurs directly at the relational level.\nPresent exact lifted inference algorithms (Poole, 2003; de Salvo Braz et al., 2006; Milch et al., 2008) and those developed in the efforts above are suitable for discrete domains, thus can in theory be applied to continuous domains through discretization. However, the precision of discretizations deteriorates exponentially in the number of dimensions in the model, and the number of dimensions in relational models is the number of ground random variables. Thus, discretization and usage of discrete lifted inference algorithms is highly imprecise.\nHere, we propose the first exact lifted inference algorithm for Relational Continuous Models (RCMs), a new relational probabilistic language for continuous domains. Our main insight is that, for some classes of potential functions (or potentials), marginalizing out a ground random variable\nin a RCM can yield a RCM representation that does not force other random variables to become propositional (Section 4). Further, relational pairwise models, i.e. products of relational potentials of arity 2, remain relational pairwise models after eliminating out ground random variables in those models. Thus, it leads to the compact representations and the efficient computations. We report Gaussian potentials which satisfy the conditions for relational pairwise models (Section 5). However, we are unsure whether the conditions are only satisfied by Gaussian potentials, yet.\nWe also adapt principles of Inversion Elimination, a method devised by (Poole, 2003), to continuous models. Inversion Elimination’s step essentially takes advantage of an ability to exchange sums and products. The lifted exchange of sums and products translates directly to continuous domains. This is a unique approach to continuous models, even though the insight is brought from discrete models.\nGiven a RCM, our algorithm marginalizes continuous variables by analytically integrating out random variables except query variables. It does so by finding a variable, and eliminating it by Inversion Elimination. If such elimination is not possible, Relational Atom Elimination eliminates each pairwise form in a linear time. If the marginal is not in pairwise form, it converts the marginal into a pairwise form.\nThis paper is organized as follows. Section 2 provides the formal definition of RCMs. Section 3 overviews our inference algorithms. Section 4 presents main intuitions and results in a Gaussian potential. Section 5 provides the generalized algorithm for relational pairwise models. Section 6 provides experimental results followed by related works in Section 7. We conclude in Section 8."
    }, {
      "heading" : "2 Relational Continuous Models",
      "text" : "We present a new relational model for continuous variables, Relational Continuous Models (RCMs). Relations among attributes of objects are represented by Parfactor models. 1 Each parfactor (L,C,AR, φ) is composed of a set of logical variables (L)2, constraints on L (C), a list attributes of objects (AR), and a potential on AR (φ). Here, each attribute is a random variable with a continuous domain.\nWe define a Relational Atom to refer the set of ground attributes compactly. For example, Revenue[B] is a relational atom which refers to revenues of banks (e.g. B = {‘Pacific Bank′, ‘Central Bank′, · · · }). To make the parfactor compact, a list of relational atoms is used for\n1Part of its representation and terms are based on the previous works (Poole, 2003; de Salvo Braz et al., 2005; Milch & Russell, 2007). However, our representaion allows continuous random variables.\n2Instead of objects, we use the general term, logical variables.\nAR. To refer to an individual random variable, we use a substitution θ. For example, if a substitution (B = ‘Pacific Bank′) is applied to a relational atom, then the relational atom Revenue[B] becomes a ground variable Revenue(‘Pacific Bank′).3 Formally, applying a substitution θ to a parfactor g = (L,C,AR, φ) yields a new parfactor gθ = (L′,Cθ,ARθ, φ), where L′ is obtained by renaming the variables in L according to θ. If θ is a ground substitution, gθ is a factor. Θg is a set of all substitution for a parfactor g. The set of groundings of a parfactor g is represented as gr(g) = {gθ : θ ∈ Θgr(L:C)}. We use RV(X) to enumerate the random variables in the relational atom X. Formally, RV(α) = {α[θ] : θ ∈ gr(L)}. LV(g) refers the set of logical variables (L) in g.\nThe joint probability density over random variables is defined by factors in a parfactor. A factor f is composed of Ag and φ. Ag is a list of ground random variables (i.e. (X1(θ), · · · ,XN(θ))). φ is a potential on Ag: a function from range(Ag) = {range(X1(θ)) × · · · × range(XN(θ))} to non-negative real numbers. The factor f defines a weighting function on a valuation (v = (v1, · · · , vm)): w f (v) = φ(v1, · · · , vm)). The weighting function for a parfactor F is the product of weighting function of all factors, wF(v) =∏\nf∈F w f (v). When G is a set of parfactors, the density is\n3Revenue() refers a random variable. Revenue[] refers a relational atom.\nthe product of all factors in G: wG(v) = ∏ g∈G ∏ f∈gr(G) w f (v). (1)\nFor example, consider the model in Figure 1. S and B in L are two logical variables which represent markets and banks respectively. For example, S can be substituted by a specific market sector (e.g. S = ‘stock′). A parfactor f1 = ({Market[S],Gain[S,B]}, φ2) is defined over two relational atoms, Market[S] and Gain[S,B]. Market(s) (one variable in Market[S]) represents the quarterly market change (e.g. Market(auto)=−3.1%). Gain(s, b) represents the gain of bank b in the market s. Given two values, a potential φ1(Market(s),Gain(s, b)) provides a numerical value. Given all valuations of random variables, the product of potentials is the probability density."
    }, {
      "heading" : "3 Algorithm Overview for RCMs",
      "text" : "RCMs model large real-world systems in a compact way. One inference task with such models is to find the conditional density of query variables given observations of some variables.\nOur inference algorithm, FOVE-Continuous (First-Order Variable Elimination), for RCMs recursively eliminates relational atoms. First, it splits (terminology of (Poole, 2003); shattering in (de Salvo Braz et al., 2005))4 relational\n4Please refer (Poole, 2003; de Salvo Braz et al., 2005) for fur-\natoms. The split operation makes groundings (e.g. RV(X) RV(Y)) of every relational atoms (e.g. X Y) disjoint. It introduces observations as observations of groundings of separate relational variables. For example, observing Market(auto) = 30% creates two separate relational atoms: Market(auto), Market(M)M,auto. The ‘M , auto’ then appears in parfactors relating to the latter relational atom. After split, FIND-ELIMINABLE finds a relational atom which satisfies conditions for one of the elimination algorithms: Inversion-Elimination (Section 5.2) and Relational-AtomElimination (Section 5.3). The found atom is eliminated by our ELIMINATE-CONTINUOUS algorithm explained in Sections 4 and 5. It iterates the elimination until only query variables are remained. The procedure is described in Figure 2.\nOur main contributions are focused on the algorithm ELIMINATE-CONTINUOUS, a lifted variable eliminations for continuous variables. We describe details in Sections 4 and 5."
    }, {
      "heading" : "4 Inference with Gaussian Potentials",
      "text" : "This section presents our first main technical contribution, efficient variable elimination algorithms for relational Gaussian models. We focus on the inference problem of computing the posterior of query variables given observations. It is important to efficiently integrating out relational atoms (e.g. Revenue[B] = {Revenue(b1), · · · , Revenue(bm)}) for solving this inference problem. In the following description, we omit the (inequality between logical variables and objects) constraints from parfactors. This allows us to focus on the potential functions inside those parfactors. The treatment below holds with little change for parfactors with such constraints.\nther details."
    }, {
      "heading" : "4.1 Relational Pairwise Potentials",
      "text" : "This section focuses on the product of potentials which we call Relational Normals (RNs). A RN is the following function with arity 2 (Section 5 provides a generalization for arbitrary potentials).:\nφRN(X,Y) = ∏\nx∈X,y∈Y\n1 σ √ 2π exp\n( − (x − y) 2\n2σ2\n)\nThis potential indicates that the difference between two random variables follows Gaussian distributions.\nConsider the models shown in Figure 3 and 4. The models represent the relationships between each market change and revenue of each bank. To simplify notations, we respectively shorten Market(s), Gain(s, b) and Revenue(b) to M(s), G(s, b) and R(b). The potential φ4 in these figures is φRN(M(s),R(b)), and the product of potential is∏\ns∈S,b∈B φRN(M(s),R(b))\nFigure 4 shows that integrating out a random variable R(bi) from the joint density results in the product of RNs again (c and c’ are constants) as follow.\n∫ R(bi ) ∏ s∈S φ4(M(s),R(bi)) = c · exp ( ( ∑ s∈S M(s))2 2σ2 · |S| − ∑ s∈S M(s)2 2σ2 )\n= c · ∏\n1≤i< j≤|S| exp\n( −\n(M(si) −M(s j))2 2σ2 · |S|\n) = c′ · ∏ 1≤i< j≤|S| φ′5(M(si),M(s j)) (2)\nNote that, following equations holds for integration.\n∫ R(bi ) exp ( −aR(bi)2 + 2bR(bi) + c ) = √ π a exp ( b2 a + c ) (3)\nHere, the terms a and b can include random variables except R(bi).\nDefinition 1 (Connected Relational Normal) The product of RNs is connected, when the connectivity graph is a connected component. Each vertex of the connectivity graph is a random variable or a constant in RNs, and each edge is a potential (RN).\nLemma 1 The product of RNs is a probability density function when it is connected, and at least a RN includes a constant argument.\nThe proof is provided in Section 9."
    }, {
      "heading" : "4.2 Constant Time Relational Atom Eliminations",
      "text" : "We provide two constant time elimination algorithms for RNs involving a single relational potential φ (i.e. the product of potentials over different instances of relational atoms). The algorithms eliminate variables, while maintaining the same form, the product of RNs."
    }, {
      "heading" : "4.2.1 Elimination of a relational atom X in φRN(X,Y)",
      "text" : "The first problem is to marginalize a relational atom (X) in the product of RNs with two relational atoms (X, Y): φRN(X,Y). The potential is the product of |X| · |Y| RNs. Note that each random variable in X has a relation with each variable in Y.\nAlgorithm ‘Pairwise Constant1’ It marginalizes xi in X, and converts the marginal into a pairwise form.\n∫ xi ∏ y∈Y exp ( − (xi − y) 2 2σ2 ) = ∏ yi ,yj∈Y,i< j≤|Y| exp ( − (yi − y j)2 2σ2 · |Y| ) (4)\nNote that the marginal over xi ∈ X and the marginal over x j ∈ X (i , j) are identical. Thus, the following result is derived when it marginalizes all variables in X.∫\nx1\n· · · ∫\nx|X| ∏ xi∈X ∏ y∈Y exp ( − (xi − y) 2 2σ2 )\n= ∏ xi∈X  ∫ xi ∏ y∈Y exp ( − (xi − y) 2 2σ2 ) =  ∏yi ,yj∈Y,i< j≤|Y| exp ( − (yi − y j)2 2σ2 · |Y| ) |X| = ∏\nyi ,yj∈Y,i< j≤|Y| exp\n( − |X|(yi − y j)2\n2σ2 |Y|\n) (5)\nThe result of integration is the product of pairwise RNs (φRN(Y,Y)) with the parameter |X| 2σ2·|Y| .\nTheorem 2 For the product of RNs between two relational atoms (φRN(X,Y)), ‘Pairwise Constant1’ eliminates all ground variables of a relational atom in a constant time.\nProof Eliminating a variables xi in X takes a constant time shown as Equation 4. Eliminating other variables in X takes a constant time shown as Equation 5. Thus, the computation takes only a constant time without an iteration."
    }, {
      "heading" : "4.2.2 Elimination of n random variables in φRN(X,X)",
      "text" : "The second problem is to marginalize some (n) variables in a relational atom (X) in the product of RNs within the relational atom: φRN(X,X). The potential is the product of\n|X|·(|X|−1) 2 pairwise RNs between two ground random variables in X.\nAlgorithm ‘Pairwise Constant2’ It updates the marginal after eliminating a random variable without an iteration. When it eliminate xm, it calculates the parameters of φ′′RN given φRN as the following equation.∫\nxm ∏ 1≤i< j≤m φRN(xi, x j) = ∏ 1≤i< j≤m−1 φRN(xi, x j) · ∫ xm ∏ 1≤i≤m−1 exp ( − (xi − xm) 2 2σ2 )\n= ∏\n1≤i< j≤m−1 φRN(xi, x j) · ∏ 1≤i< j≤m−1 exp ( − (xi − x j)2 2σ2 · (m − 1) ) =\n∏ 1≤i< j≤m−1 φRN(xi, x j) · ∏ 1≤i< j≤m−1 φ′RN(xi, x j) = ∏ 1≤i< j≤m−1 φ′′RN(xi, x j)\nThe coefficient of φ′′RN is the sum of coefficient of φRN ( σ2) and coefficient of φ′RN (σ\n2(m − 1)). The sum of two coefficients results in σ2 · m−1m . Similarly, eliminating the next random variable αm−1 results in σ2 m−2m (=σ 2 m−1 m m−2 m−1 ). Thus, eliminating n random variables results in σ2 m−nm without iterations.\nTheorem 3 For the product of RNs with a relational atom (φRN(X,X)), ‘Pairwise Constant2’ eliminates n ground variables of the relational atom in a constant time.\nProof Updating the parameter of φRN(X,X) from σ2 to σ2 m−nm takes only a constant time."
    }, {
      "heading" : "4.3 A Linear Time Relational Atom Elimination",
      "text" : "This section provides a linear time variable elimination algorithm O(|U|) which can be applied to any product of RNs. This algorithm is used when the constant time algorithms of the previous sections are not applicable. 4.3.1 Elimination of multiple atoms in ∏ φRN(Xi,X j)\nThis problem is to marginalize some variables in U, (U = {X1,X2, · · · ,X|N|}) in the product of RNs between two relational atoms: ∏ φRN(Xi,X j). If all relational atoms have pairwise relationships among each other, there are |N|·|N−1|2 pairwise RNs.\nLemma 4 For |U| variables in |N| relational atoms (U = {X1,X2, · · · ,X|N|}) and RN potentials, marginalizing n variables in a ground model takes O(n · |U|2).\nProof Suppose we eliminate a variable x ∈ U. Eliminating a variable x in RN needs updates coefficients of terms (xix j) where xi and x j have relations with the variable x. When x has relations with all other variables in U, the number of terms is bounded by O(|U|2). Thus, eliminating n variables takes O(n · |U|2) because it needs n iterations.\nThus, any inference algorithm in a ground model has an order of O(|U|3) time complexity, when it eliminates all ground variables except a few query variables.\nAlgorithm ‘Pairwise Linear’\nTo reduce the time complexity, our lifted algorithm uses following notations which refer ground variables in an atom X compactly: X[m] = ∑ 1≤i≤m xi; X[m]2 = ∑ 1≤i≤m x2i ;\nand X[m][m] = ∑\n1≤i< j≤m xi · x j. The notations give the following properties (when |X| = m and |Y| = n):(\nX[m] )2 = X[m]2 + 2X[m][m]\nexp ( 2X[m][m] − (m − 1)X[m]2 ) = ∏ xi ,xj∈X exp ( −(xi − x j)2 ) = φ′RN(X,X)\nexp ( 2X[m]Y[n] − nX[m]2 −mY[n]2 ) = ∏ xi∈X,yk∈Y exp ( −(xi − yk)2 ) = φ′′RN(X,Y)\nFor the product of potentials over X, Y, and {x′}, our algorithm marginalizes x′:∫\nx′ φRN(X, x′) · φRN(Y, x′)\n= ∫ x′ exp ( −(m + n)x′2 + 2(X[m] + Y[n])x′ − (X[m]2 + Y[n]2 ) ) = √ π\nm + n · exp\n( (X[m] + Y[n])2\nm + n − (X[m]2 + Y[n]2 ) ) = c · exp ( 2X[m][m] + 2X[m]B[n] + 2Y[n][n] − (m + n − 1)(X[m]2 + Y[n]2 ) m + n\n) = c · φ′RN(X,X) · φ′′RN(X,Y) · φ′′′RN(Y,Y) (6)\nIt iterates until all n variables are eliminated.\nTheorem 5 For |U| variables in |N| relational atoms (U = {X1,X2, · · · ,X|N|}) and potentials in RN, ‘Pairwise Linear’ eliminates n variables in O(n · |N|2).\nProof WLOG, we marginalize a variable x′ ∈ X1. We make an artificial atom Y which includes all relational atoms, when those atoms have relationships with X1.5 Then, {x′} is split from X1 (X1 = X′1 ∪ {x′} and X′1 ∩ {x′} = ∅). When marginalizing x′ out in φRN(X′1, x′) · φRN(Y, x′), the marginal is also the product of RNs shown as Equation 6: φ′RN(X ′ 1,X ′ 1) · φ′′RN(X′1,Y) · φ′′′RN(Y,Y).\nThe marginal can be represented without the artificial atom Y in the following procedures. We convert into φ′′RN(X\n′,Y) and φ′′′RN(Y,Y) as follows. First, φ ′′ RN(X ′ 1,Y) is represented\nas the product of RNs between atoms Xi in Y and X′1:∏ Xi∈Y φ ′′ RN(X ′ 1,Xi). Second, φ ′′′ RN(Y,Y) is also represented\nas the product of RNs between atoms Xi and X j in Y:∏ Xi,X j∈Y φ ′′ RN(Xi,X j).\nFor each elimination, it updates parameters of all possible pairs O(|N|2) among |N| atoms. Thus, the computational complexity to eliminate n variables is the order of O(n · |N|2).\nThus, ‘Pairwise Linear’ has linear time complexity O(|U|) with respect to the number of ground variables.\n5That is, Y = ⋃\ni X′i and X ′ i = { xσi |x ∈ Xi}, when σi is the\nvariance used in φRN(X1,Xi)."
    }, {
      "heading" : "5 Exact Lifted Inference with RCM",
      "text" : "This section presents our algorithm, ELIMINATECONTINUOUS, which generates a new parfactor after eliminating a set of relational atoms given a set of parfactors. A potential of each parfactor is the product of Relational Pairwise Potentials (RPPs):\nφRPP(X,Y) = ∏\nx∈X,y∈Y φRPP(x, y)\nA relational pairwise model is a RCM whose potentials are RPPs. Here, RPPs are not limited to the RNs in Section 4.1."
    }, {
      "heading" : "5.1 Conditions for Exact Lifted Inference",
      "text" : "The lifted ELIMINATE CONTINUOUS algorithm provides the exact solution for potentials of parfactors when the potentials satisfy three conditions: Condition (I), analytically integrable; Condition (II), closed under product operations; and Condition (III), closed under marginalizations, thus represented with the product of relational pairwise potentials again. The RNs are an example that satisfies the conditions. Here, we introduce another potential, a linear Gaussian, which satisfies the conditions.\nLemma 6 The product of RNs with non-zero Means (RNMs) satisfies the three conditions. A RNM has the following form (d is a constant).\nφRN(X,Y) = ∏\nx∈X,y∈Y\n1 σ √ 2π exp\n( − (x − y − d) 2\n2σ2\n)\nThe proof is provided in Section 9."
    }, {
      "heading" : "5.2 Inversion-Elimination",
      "text" : "Inversion elimination is applicable when the set of logical variables in g is same with the set of logical variables in e, LV(e) = LV(g). Let θ1,...,θn be enumeration of Θg.∫\nRV(e) φ(g) = ∫ RV(e) ∏ θ∈Θg φg(Agθ) = ∫ e[θ1] · · · ∫ e[θn ] φg(Agθ1) · · ·φg(Agθn)\n= ∏ θ∈Θg ∫ e[θ] φg(Agθ)(∵ split (Section 3)) = ∏ θ∈Θg ∫ e φg(A′θ, e)\n= ∏ θ∈Θg φ′(A′θ) = φg′\nReturn to the econometric market example, inversion elimination can be applied to G[S,B]. Before an elimination, it combines two parfactors which include φ2 and φ3 respectively. The combined parfactor is g = ({S,B},>, (M[S],G[S,B],R[B]), φ2 · φ3). Then, the elimination procedure is follow.∫\nRV(G) φ(g) = ∫ RV(G) ∏ s∈S,b∈B φg(M(s),G(s, b),R(b))\n= ∏\ns∈{auto,··· ,stock},b∈{b1 ,··· ,bm }\n(∫ G(s,b) φg(M(s),G(s, b),R(b)) )\n= ∏\ns∈{auto,··· ,stock},b∈{b1 ,··· ,bm } φnew(M(s),R(b)) = φnew(M[S],R[b]) = φg′\nNote that, the number of substitutions (|Θg|) is the number of market sectors (|S|) times the number of banks (|B|). Regardless the number of substitutions, we can apply the same integration to eliminate |S| · |B| number of random variables (G(s,b)). Thus, it calculates the integral (= ∫ L φg(M(s),G(s, b),R(b))) one time regardless of specific s and b. The marginal (φnew(M[S],R[B])) becomes the potential of the output parfactor (g′)."
    }, {
      "heading" : "5.3 Relational-Atom-Elimination",
      "text" : "Relational-Atom-Elimination marginalizes atoms when Inversion-Elimination is not applicable. It is a generalized algorithm of those for RN shown in Section 4. It marginalizes each relational atom of a parfactor g according to three cases: (1) variables in the atom e has relationship with an atom (i.e. ‘φ(X,Y)’); (2) variables in the atom e has relationships only each other (i.e. ‘φ(X,X)’); and (3) other general cases (i.e. ‘ ∏ φ(Xi,X j)’).\nFor the case (1), a modified ‘Pairwise Constant1’ eliminates an atom e. In this case, integrating out a random variable in the atom does not affect integrating another variable in the atom as shown in Section 4.2. That is,∫\nRV(e) ∏ θ∈Θg φg(.) = ∏ θe∈Θe ∫ e(θe) ∏ θ∈Θg\\{e} φg(.). Here, E is the set of atoms in g, and E = E \\ {e}, and ΘE is the set of all substitutions for E.∫\nRV(e) φ(g) = ∫ RV(e) ∏ θ∈ΘE φg(Agθ) = ∫ RV(e) ∏ θe∈Θ{e} ∏ θ∈ΘE\\{e} φg(Agθe,Agθ)\n= ∏\nθe∈Θ{e}\n∫ e[θe] ∏ θ∈ΘE\\{e} φg(Agθe,Agθ) = ∏ θe∈Θ{e} φ′(RV(E))(∵ Condition(I))\n= φ′(RV(E))|RV(e)| = φ′′(RV(E))(∵ Condition(II))\nNormally, the marginal φ′′(RV(E)) is not a relational pairwise potential because all random variables in E are arguments of the potential. However, when Condition (III) is satisfied, the marginal can be converted into the product of relational pairwise potentials: φ′′(RV(E)) =∏\nXi,X j∈RV(E) φRPP(Xi,X j).\nIn the financial example, it eliminates R[B] as follow.∫ RV(R) φ(g′) = ∫ RV(R) ∏ s∈S,b∈B φnew(M(s),R(b))\n= ∏ b∈B ∫ R(b) ∏ s∈S φnew(M(s),R(b)) = ∏ b∈B φ′new(M(auto), · · · ,M(stock)) = φ′new(M(auto), · · · ,M(stock))|RV(R)| = φ′′new(M(auto), · · · ,M(stock))\nBeyond Relational Gaussian defined in Section 4.1, any potential function satisfying the Condition III) can convert the potential φ′′new into the pairwise form ∏ φ′′′new.\nφ′′new(M(auto), · · · ,M(stock)) = ∏\ns1 ,s2∈S φ′′′new(M(s1),M(s2))\nLikewise, for the cases (2) and (3), generalized algorithms of ‘Pairwise Constant2’ and ‘Pairwise Linear’ are also applied respectively."
    }, {
      "heading" : "6 Experiments",
      "text" : "We report experiments for the recession model provided in the paper. For experiments, we implemented three algorithms: (A) inference with a grounded model; (B) inference with only Inversion-Elimination; and (C) inference with both Inversion-Elimination and RelationalAtom-Elimination. Our new algorithm (C) is significantly faster than the grounded model (A) and InversionElimination (B). Note that Inversion-Elimination (B) is also our new algorithm for continuous variables, even though comparable elimination methods for discrete variables (de Salvo Braz et al., 2005; Milch et al., 2008; Pfeffer et al., 1999) existed prior to ours. Our experimental results are shown in Figure 5 and 6\nIn the recession model, we provided observations for one market variable (M) and one revenue variable (R).6 Those variables were split from relational atoms. Then, we calculated the marginal density of the Recession variable. We increased the number of markets and the number of banks from 2 to 2048 exponentially. We set an hour of cut-off time. With 512 banks, the grounded inference (A) did not complete within an hour. Meanwhile, the Inversion Elimination (B) and our new algorithm (C) finished computations in almost a constant time even for 2048 banks. With 512 markets, (A) could not finish within an hour, again. With 1024 markets, (B) did not finish in an hour. Meanwhile, our new algorithm (C) finished in a reasonable time (about 151 secs) even with 2048 markets."
    }, {
      "heading" : "7 Related Work",
      "text" : "(Poole, 2003), solves inference problems with the unification which dynamically splits a set of ground nodes and unifies them. With a counting formula, (de Salvo Braz et al., 2005; de Salvo Braz et al., 2006) provides a tractable algorithm. (Milch et al., 2008) applies the counting formula to reduce the size of probability density tables. However, these lifted inference algorithms are hard to apply to continuous domains.\n6Observations are required to make the product of RNs a probability density function. Please refer Lemma 1 for details.\nMLNs (Markov Logic Network) (Richardson & Domingos, 2006) use First-order logic sentences to represent relationships over nodes in a graphical model. In this regard, MLNs also represent graphical models at the relational level. (Singla & Domingos, 2008) provides an approximated lifted inference algorithm over discrete domain. (Singla & Domingos, 2007) makes an analysis for infinitely many discrete variables. However, these achievements are not for continuous domains, too. Although there is an inference algorithm for Hybrid MLNs (Wang & Domingos, 2008), it is an approximated algorithm. Thus, most of achievements are comparable to lifted inferences (de Salvo Braz et al., 2005; Milch et al., 2008; Pfeffer et al., 1999) over discrete domain.\nInference with Gaussian distributions is a traditional problem (Roweis & Ghahramani, 1999). In detail, calculating conditional densities of multivariate Gaussians requires matrix inversions (Kotz et al., 2000) which are intractable for high dimensions. (Lerner & Parr, 2001; Shenoy, 2006) builds inference algorithms for hybrid models with Gaussians. (Paskin, 2003) shows that efficient inference is possible for a linear Gaussian when the treewidth of the model is small. For models with large treewidth, however, those inference algorithms over ground models which would be inefficient.\nRecent advances in inference with relational models (Kisynski & Poole, 2009; Mihalkova & Mooney, 2009) show the promise of the approach in discrete models, and underline the promise of our algorithm in continuous models."
    }, {
      "heading" : "8 Conclusion and Future work",
      "text" : "In this paper, we propose a new exact lifted inference algorithm for Relational Continuous Models (RCMs). This algorithm is an advancement of exact inference in RCMs, since all previous works are restricted to discrete domain. Given a query and observations, our algorithm exactly computes the conditional density of the query, when potentials satisfy specified conditions.\nThere are two limitations in our current algorithm. First,\nfound potentials which satisfy the conditions in Section 5 are variants of Gaussian potentials. Thus, finding potentials beyond Gaussian is a goal of our future works. Second, the current algorithm is designed only for continuous variables. Many real-world models require not only continuous variables but also discrete variables. Thus, making an efficient inference algorithm for hybrid relational models would be a promising direction."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We wish to thank Abner Guzman Rivera and the anonymous reviewers for their valuable comments. This material is supported by NSF IIS 05-46663 and UIUC/NCSA AESIS 251024 grants."
    }, {
      "heading" : "9 Appendix",
      "text" : "Proof of Lemma 1 Here, we prove that the product of RNs integrates to a constant given the conditions. The constant becomes the normalizing factor of the probability density function.\nWe prove this by contradiction. Suppose that the product of RNs does not integrate to a constant. That is, it integrates to infinity.\nAccording to Equation 2, the product of RNs maintains the same form after integrating out a random variable x. Thus, only possible case to be infinity is when the marginal (after an integration over x) is a constant function of another random variable y which is not yet integrated.\nWhen x has relations with more than one variable (e.g. y and z), the condition for infinity is not satisfied. The marginal includes a potential φ(y, z). When x has a relation with only y which has relations with other variables beyond x, the condition for infinity is not satisfied. The marginal is not a constant function of y.\nThus, only φ(x, y) satisfies the condtion for infinity. Given the assumption that at least a RN includes a constant, y can not be a variable. Thus, it contradicts the assumption.\nProof of Lemma 6 First, the product of RNMs is analytically integrated by the rule in Equation 3. Thus, the product of RNMs satisfy the Condition (I).\nSecond, the product of RNMs is closed under product operations and marginalizations. It satisfies the Condition (I) because it is an exponential family. That is, the product of two RNMs (φ′RNM(x, y) and φ ′′ RNM(x, y)) is another RNMs (φ′′′RNM(x, y)). Thus, the product of RNMs satisfies the Condition (II).\nThird, it is also closed under marginalizations. When y j in Equation 4 is substituted with y j−d, the following equation is derived.∫\nxi ∏ y∈Y exp ( − (xi − y − d) 2 2σ2 ) = ∏ yi ,yj∈Y exp ( − (yi − y j − 0))2 2σ2 · |Y| )\nThus, the result is the product of RNMs.\nAs explained in the proof of Theorem 5, the the product of RNMs can be represented as the following form φRNM(X, x′) · φRNM(Y, x′) when x′ is the variable of integration.\nWhen y j ∈ Y in Equation 6 is substituted with y j − d ∈ Y′, the following equation is derived.∫\nx′ φRNM(X, x′) · φRNM(Y, x′) = c · ∫ x′ φRN(X, x′) · φRN(Y′, x′)\n= c′ · φ′RN(X,X) · φ′′RN(X,Y′) · φ′′′RN(Y′,Y′) = c′′ · φ′RNM(X,X) · φ′′RNM(X,Y) · φ′′′RNM(Y,Y)\nThe result is also the product of RNMs. Thus, it is closed under marginalizations."
    } ],
    "references" : [ {
      "title" : "Lifted firstorder probabilistic inference. IJCAI’05: Proceedings of the 19th international joint conference on Artificial intelligence (pp. 1319–1325)",
      "author" : [ "R. de Salvo Braz", "E. Amir", "D. Roth" ],
      "venue" : null,
      "citeRegEx" : "Braz et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Braz et al\\.",
      "year" : 2005
    }, {
      "title" : "Mpe and partial inversion in lifted probabilistic variable elimination. AAAI’06: proceedings of the 21st national conference on Artificial intelligence (pp. 1123–1130)",
      "author" : [ "R. de Salvo Braz", "E. Amir", "D. Roth" ],
      "venue" : null,
      "citeRegEx" : "Braz et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Braz et al\\.",
      "year" : 2006
    }, {
      "title" : "Learning probabilistic relational models. IJCAI’99: Proceedings of the 16th international joint conference on Artificial intelligence (pp. 1300–1307)",
      "author" : [ "N. Friedman", "L. Getoor", "D. Koller", "A. Pfeffer" ],
      "venue" : null,
      "citeRegEx" : "Friedman et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 1999
    }, {
      "title" : "Introduction to statistical relational learning (adaptive computation and machine learning)",
      "author" : [ "L. Getoor", "B. Taskar" ],
      "venue" : null,
      "citeRegEx" : "Getoor and Taskar,? \\Q2007\\E",
      "shortCiteRegEx" : "Getoor and Taskar",
      "year" : 2007
    }, {
      "title" : "Real-time bayesian anomaly detection in streaming environmental data",
      "author" : [ "D.J. Hill", "B.S. Minsker", "E. Amir" ],
      "venue" : "Water Resources Research,",
      "citeRegEx" : "Hill et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2009
    }, {
      "title" : "Lifted aggregation in directed first-order probabilistic models. IJCAI’09",
      "author" : [ "J. Kisynski", "D. Poole" ],
      "venue" : "Proceedings of the 21st international jont conference on Artifical intelligence (pp",
      "citeRegEx" : "Kisynski and Poole,? \\Q2009\\E",
      "shortCiteRegEx" : "Kisynski and Poole",
      "year" : 2009
    }, {
      "title" : "Object-oriented bayesian networks",
      "author" : [ "D. Koller", "A. Pfeffer" ],
      "venue" : "Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Koller and Pfeffer,? \\Q1997\\E",
      "shortCiteRegEx" : "Koller and Pfeffer",
      "year" : 1997
    }, {
      "title" : "Continuous multivariate distributions",
      "author" : [ "S. Kotz", "N. Balakrishnan", "N. Johnson" ],
      "venue" : null,
      "citeRegEx" : "Kotz et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Kotz et al\\.",
      "year" : 2000
    }, {
      "title" : "Inference in hybrid networks: Theoretical limits and practical algorithms",
      "author" : [ "U. Lerner", "R. Parr" ],
      "venue" : null,
      "citeRegEx" : "Lerner and Parr,? \\Q2001\\E",
      "shortCiteRegEx" : "Lerner and Parr",
      "year" : 2001
    }, {
      "title" : "Relational object maps for mobile robots. IJCAI’05: Proceedings of the 19th international joint conference on Artificial intelligence (pp. 1471–1476)",
      "author" : [ "B. Limketkai", "L. Liao", "D. Fox" ],
      "venue" : null,
      "citeRegEx" : "Limketkai et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Limketkai et al\\.",
      "year" : 2005
    }, {
      "title" : "Transfer learning from minimal target data by mapping across relational domains. IJCAI’09",
      "author" : [ "L. Mihalkova", "R.J. Mooney" ],
      "venue" : "Proceedings of the 21st international jont conference on Artifical intelligence (pp",
      "citeRegEx" : "Mihalkova and Mooney,? \\Q2009\\E",
      "shortCiteRegEx" : "Mihalkova and Mooney",
      "year" : 2009
    }, {
      "title" : "First-order probabilistic languages: Into the unknown. 10–24",
      "author" : [ "B. Milch", "S. Russell" ],
      "venue" : null,
      "citeRegEx" : "Milch and Russell,? \\Q2007\\E",
      "shortCiteRegEx" : "Milch and Russell",
      "year" : 2007
    }, {
      "title" : "Lifted probabilistic inference with counting formulas",
      "author" : [ "B. Milch", "L.S. Zettlemoyer", "K. Kersting", "M. Haimes", "L.P. Kaelbling" ],
      "venue" : "Proceedings of the 23rd national conference on Artificial intelligence (pp",
      "citeRegEx" : "Milch et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Milch et al\\.",
      "year" : 2008
    }, {
      "title" : "Probabilistic logic programming",
      "author" : [ "R. Ng", "V.S. Subrahmanian" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "Ng and Subrahmanian,? \\Q1992\\E",
      "shortCiteRegEx" : "Ng and Subrahmanian",
      "year" : 1992
    }, {
      "title" : "An analytic network process model for financial-crisis forecasting",
      "author" : [ "M.P. Niemira", "T.L. Saaty" ],
      "venue" : "International Journal of Forecasting,",
      "citeRegEx" : "Niemira and Saaty,? \\Q2004\\E",
      "shortCiteRegEx" : "Niemira and Saaty",
      "year" : 2004
    }, {
      "title" : "Thin junction tree filters for simultaneous localization and mapping",
      "author" : [ "M.A. Paskin" ],
      "venue" : "IJCAI’03: Proceedings of the 18th international joint conference on Artificial intelligence (pp. 1157–1164). Morgan Kaufmann.",
      "citeRegEx" : "Paskin,? 2003",
      "shortCiteRegEx" : "Paskin",
      "year" : 2003
    }, {
      "title" : "Spook: A system for probabilistic objectoriented knowledge representation",
      "author" : [ "A. Pfeffer", "D. Koller", "B. Milch", "K. Takusagawa" ],
      "venue" : "Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Pfeffer et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Pfeffer et al\\.",
      "year" : 1999
    }, {
      "title" : "First-order probabilistic inference",
      "author" : [ "D. Poole" ],
      "venue" : "IJCAI’03: Proceedings of the 18th international joint conference on Artificial intelligence (pp. 985–991). Morgan Kaufmann.",
      "citeRegEx" : "Poole,? 2003",
      "shortCiteRegEx" : "Poole",
      "year" : 2003
    }, {
      "title" : "A unifying review of linear gaussian models",
      "author" : [ "S. Roweis", "Z. Ghahramani" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Roweis and Ghahramani,? \\Q1999\\E",
      "shortCiteRegEx" : "Roweis and Ghahramani",
      "year" : 1999
    }, {
      "title" : "Inference in hybrid bayesian networks using mixtures of gaussians",
      "author" : [ "P. Shenoy" ],
      "venue" : "Proceedings of the TwentySecond Conference on Uncertainty in Artificial Intelligence (UAI-06) (pp. 428–436). Arlington, Virginia: AUAI Press.",
      "citeRegEx" : "Shenoy,? 2006",
      "shortCiteRegEx" : "Shenoy",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Such systems include measurements in environmental-sensors networks (Hill et al., 2009), localizations in robotics (Limketkai et al.",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 9,
      "context" : ", 2009), localizations in robotics (Limketkai et al., 2005), and economic forecastings in finance (Niemira & Saaty, 2004).",
      "startOffset" : 35,
      "endOffset" : 59
    }, {
      "referenceID" : 16,
      "context" : "To address these issues, Relational Probabilistic Languages (RPLs) (Ng & Subrahmanian, 1992; Koller & Pfeffer, 1997; Pfeffer et al., 1999; Friedman et al., 1999; Poole, 2003; de Salvo Braz et al., 2005; Richardson & Domingos, 2006; Milch & Russell, 2007; Getoor & Taskar, 2007) describe probability distributions at a relational level with the purpose of capturing larger models.",
      "startOffset" : 67,
      "endOffset" : 277
    }, {
      "referenceID" : 2,
      "context" : "To address these issues, Relational Probabilistic Languages (RPLs) (Ng & Subrahmanian, 1992; Koller & Pfeffer, 1997; Pfeffer et al., 1999; Friedman et al., 1999; Poole, 2003; de Salvo Braz et al., 2005; Richardson & Domingos, 2006; Milch & Russell, 2007; Getoor & Taskar, 2007) describe probability distributions at a relational level with the purpose of capturing larger models.",
      "startOffset" : 67,
      "endOffset" : 277
    }, {
      "referenceID" : 17,
      "context" : "To address these issues, Relational Probabilistic Languages (RPLs) (Ng & Subrahmanian, 1992; Koller & Pfeffer, 1997; Pfeffer et al., 1999; Friedman et al., 1999; Poole, 2003; de Salvo Braz et al., 2005; Richardson & Domingos, 2006; Milch & Russell, 2007; Getoor & Taskar, 2007) describe probability distributions at a relational level with the purpose of capturing larger models.",
      "startOffset" : 67,
      "endOffset" : 277
    }, {
      "referenceID" : 17,
      "context" : "Recently, (Poole, 2003; de Salvo Braz et al., 2005; Milch et al., 2008; Singla & Domingos, 2008) showed that such models enable more efficient inference than possible with propositional graphical models, when inference occurs directly at the relational level.",
      "startOffset" : 10,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "Recently, (Poole, 2003; de Salvo Braz et al., 2005; Milch et al., 2008; Singla & Domingos, 2008) showed that such models enable more efficient inference than possible with propositional graphical models, when inference occurs directly at the relational level.",
      "startOffset" : 10,
      "endOffset" : 96
    }, {
      "referenceID" : 17,
      "context" : "Present exact lifted inference algorithms (Poole, 2003; de Salvo Braz et al., 2006; Milch et al., 2008) and those developed in the efforts above are suitable for discrete domains, thus can in theory be applied to continuous domains through discretization.",
      "startOffset" : 42,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "Present exact lifted inference algorithms (Poole, 2003; de Salvo Braz et al., 2006; Milch et al., 2008) and those developed in the efforts above are suitable for discrete domains, thus can in theory be applied to continuous domains through discretization.",
      "startOffset" : 42,
      "endOffset" : 103
    }, {
      "referenceID" : 17,
      "context" : "We also adapt principles of Inversion Elimination, a method devised by (Poole, 2003), to continuous models.",
      "startOffset" : 71,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : "1Part of its representation and terms are based on the previous works (Poole, 2003; de Salvo Braz et al., 2005; Milch & Russell, 2007).",
      "startOffset" : 70,
      "endOffset" : 134
    }, {
      "referenceID" : 17,
      "context" : "First, it splits (terminology of (Poole, 2003); shattering in (de Salvo Braz et al.",
      "startOffset" : 33,
      "endOffset" : 46
    }, {
      "referenceID" : 17,
      "context" : ", 2005))4 relational 4Please refer (Poole, 2003; de Salvo Braz et al., 2005) for fur5 φ : Market(s1), Market(s2),.",
      "startOffset" : 35,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : "Note that Inversion-Elimination (B) is also our new algorithm for continuous variables, even though comparable elimination methods for discrete variables (de Salvo Braz et al., 2005; Milch et al., 2008; Pfeffer et al., 1999) existed prior to ours.",
      "startOffset" : 154,
      "endOffset" : 224
    }, {
      "referenceID" : 16,
      "context" : "Note that Inversion-Elimination (B) is also our new algorithm for continuous variables, even though comparable elimination methods for discrete variables (de Salvo Braz et al., 2005; Milch et al., 2008; Pfeffer et al., 1999) existed prior to ours.",
      "startOffset" : 154,
      "endOffset" : 224
    }, {
      "referenceID" : 17,
      "context" : "(Poole, 2003), solves inference problems with the unification which dynamically splits a set of ground nodes and unifies them.",
      "startOffset" : 0,
      "endOffset" : 13
    }, {
      "referenceID" : 12,
      "context" : "(Milch et al., 2008) applies the counting formula to reduce the size of probability density tables.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 12,
      "context" : "Thus, most of achievements are comparable to lifted inferences (de Salvo Braz et al., 2005; Milch et al., 2008; Pfeffer et al., 1999) over discrete domain.",
      "startOffset" : 63,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : "Thus, most of achievements are comparable to lifted inferences (de Salvo Braz et al., 2005; Milch et al., 2008; Pfeffer et al., 1999) over discrete domain.",
      "startOffset" : 63,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : "In detail, calculating conditional densities of multivariate Gaussians requires matrix inversions (Kotz et al., 2000) which are intractable for high dimensions.",
      "startOffset" : 98,
      "endOffset" : 117
    }, {
      "referenceID" : 19,
      "context" : "(Lerner & Parr, 2001; Shenoy, 2006) builds inference algorithms for hybrid models with Gaussians.",
      "startOffset" : 0,
      "endOffset" : 35
    }, {
      "referenceID" : 15,
      "context" : "(Paskin, 2003) shows that efficient inference is possible for a linear Gaussian when the treewidth of the model is small.",
      "startOffset" : 0,
      "endOffset" : 14
    } ],
    "year" : 2010,
    "abstractText" : "Relational Continuous Models (RCMs) represent joint probability densities over attributes of objects, when the attributes have continuous domains. With relational representations, they can model joint probability distributions over large numbers of variables compactly in a natural way. This paper presents a new exact lifted inference algorithm for RCMs, thus it scales up to large models of real world applications. The algorithm applies to Relational Pairwise Models which are (relational) products of potentials of arity 2. Our algorithm is unique in two ways. First, it substantially improves the efficiency of lifted inference with variables of continuous domains. When a relational model has Gaussian potentials, it takes only linear-time compared to cubic time of previous methods. Second, it is the first exact inference algorithm which handles RCMs in a lifted way. The algorithm is illustrated over an example from econometrics. Experimental results show that our algorithm outperforms both a groundlevel inference algorithm and an algorithm built with previously-known lifted methods.",
    "creator" : "TeX"
  }
}