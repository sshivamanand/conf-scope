{
  "name" : "1206.6405.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bounded Planning in Passive POMDPs",
    "authors" : [ "Roy Fox", "Naftali Tishby" ],
    "emails" : [ "royf@cs.huji.ac.il", "tishby@cs.huji.ac.il" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : ""
    }, {
      "heading" : "1.1. Passive POMDPs Planning",
      "text" : "Planning in Partially Observable Markov Decision Processes (POMDPs) is an important task in reinforcement learning, which models an agent’s interaction with its environment as a discrete-time stochastic process. The environment goes through a sequence of world states W1, . . . ,Wn in a finite domain W. These states are hidden from the agent except for an observation Ot in a finite domain O, distributed by σ(Ot|Wt).\nIn the standard POMDP, the agent then chooses an action, which affects the next world state and incurs a cost. Here we consider Passive POMDPs, in which the action affects the cost, but not the world state. We assume that the world itself is a Markov Chain, with states governed by a time-independent transition probability function p(Wt|Wt−1) and an initial distribution P1(W1).\nThe agent maintains an internal memory state Mt in a finite domainM. In each step the memory state is updated from the previous memory state and the current observation, according to a memory-state transition function qt(Mt|Mt−1, Ot) which serves as an inference policy. Figure 1 summarizes the stochastic process.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nThe agent’s goal is to minimize the average expected cost of its actions. In this paper we take the agent’s memory state to represent the action, and define a cost function d : W ×M → R on the world and memory states. The planning task is then to minimize\n1\nn n∑ t=1 E Wt,Mt d(Wt,Mt)\ngiven the model parameters P1, p, σ and d.\nA Passive POMDP can be viewed as an HMM in which inference quality is measured by a cost function. Examples of Passive POMDPs include various gambling scenarios, such as the stock exchange or horse racing, where the betting does not affect the world state. In some settings, the reward depends directly on the amount of information that the agent has on the world state (Kelly gambling, see Cover & Thomas, 2006).\nWhen the agent is unbounded it has a simple deterministic optimal inference policy. It can maintain a belief Bt(Wt|O(t)), which is the posterior probability of the world state Wt given the observable history O(t) = O1, . . . , Ot. The belief is a minimal sufficient statistic of O(t) for Wt, and therefore keeps all the relevant information. It can be computed sequentially by a forward algorithm, starting with B1(W1|O1) ∝ P1(W1)σ(O1|W1), and at each step updating\nBt(Wt|O(t)) ∝ ∑ wt−1 Bt−1(wt−1|O(t−1))p(Wt|wt−1)σ(Ot|Wt),\nnormalized to be a probability vector."
    }, {
      "heading" : "1.2. Information Constraints",
      "text" : "The sufficiency of the exact belief allows the agent to minimize the external cost, but it incurs significant internal costs. The amount of information which the agent needs to keep in memory can be large, and even each observation can be more than the agent can grasp. Anyway, not all of this information is equally useful in reducing external costs.\nIn general, the agent’s information-processing capacity may be bounded in two ways:\n1. The capacity of the agent’s memory may limit its information rate between Mt−1 and Mt, to RM .\n2. The capacity of the channel from the agent’s sensors to its memory may limit the rate at which the agent is able to process the observation Ot while it is available, to RS .\nThe requirement that the agent keeps sufficient statistics and exact beliefs is unrealistic. Rather, the agent’s memory Mt must be a statistic of O(t) which is not sufficient, but is still ”good” in the sense that it keeps the external cost low. We also want it to be ”minimal” for that level of quality, in terms of information-processing rates, so that the agent keeps only information which is useful enough. For each step individually, this is exactly the notion captured by rate-distortion theory, and here we have a sequential extension of it.\nThe main results of this paper are the formulation of the setting described above, and the introduction of an efficient and simple algorithm to solve it. We prove that the algorithm converges to a local optimum, and demonstrate in simulations the tradeoff of memory and sensing intrinsic to this setting. The application of our results to previously studied problems, and a comparison to existing algorithms, are left for future work.\nThis paper is organized as follows. In section 2 we formulate out setting in information-theoretical terms. In section 3 we solve the problem for one step by finding a variational principle and an efficient optimization algorithm. In section 4 we analyze the complete sequential problem and introduce an algorithm to solve it. In section 5 we show two simulations of our solution."
    }, {
      "heading" : "1.3. Related Work",
      "text" : "Unconstrained planning in Passive POMDPs is easily done by maintaining the exact belief, and choosing each action to minimize the subjective expected cost. Planning in general POMDPs is harder, in one aspect due to the size of the belief space. Many algorithms plan efficiently but approximately by focusing on a subset of this space.\nSeveral works do so by optimizing a finite-state controller of a given size (Poupart & Boutilier, 2003; Amato et al., 2010). The belief represented by each state of the controller is then the posterior probability of the world state given that memory state. A different approach is to explicitly select a subset of beliefs, and use them to guide the iterations (Pineau et al., 2003). Another is to reduce the dimension of the belief space to its principle components (Roy & Gordon, 2002).\nIn this paper we present the novel setting of planning in Passive POMDPs which is constrained by information capacities. This setting allows treatment of reinforcement learning in an information-theoretic framework. It may also provide a principled method for belief approximation in general POMDPs. With a fixed action policy the POMDP becomes a Passive POMDP, and a bounded inference policy can be computed. This reduces the belief space, which in turn guides the action planning. This decoupling is similar to Chrisman (1992), and will be explored in future work.\nSome research treats POMDPs where the cost is the DKL between the distributions of the next world state when it is controlled and uncontrolled (Todorov, 2006; Kappen et al., 2009). This has interesting analogies to our setting. Our information-rate constraints define, in effect, components of the cost which are the DKL between the distribution of the next memory state and its marginals (see section 3.1). Tishby & Polani (2011) combine similar information-rate constraints of perception and action together. Future work will explore and exploit this symmetry in the special case where the memory information rate is unconstrained."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "Assume that the model parameters P1, p, σ and d are given. The agent strives to find an inference policy q(n) such that the average expected cost satisfies\n1\nn n∑ t=1 E Wt,Mt d(Wt,Mt) ≤ D.\nfor the minimal D possible. However, the agent operates under capacity constraints on the channels from Mt−1 and Ot to Mt. The external cost d parallels the distortion in rate-distortion theory, while the internal costs are information rates. The agent actually needs to minimize a combination of these costs.\nNote that the agent will generally have some information on the next observation even before seeing it, i.e. Mt−1 and Ot will not be independent. The agent therefore has some freedom in choosing what part of the information common to Mt−1 and Ot it remembers, and what part it forgets and observes anew.\nThe average information rate in both channels combined cannot exceed their total capacity, that is\n1\nn n∑ t=1 I(Mt−1, Ot;Mt) ≤ RM +RS .\nIn addition, in each step the portion of the above information that is absent from Ot may only be passed on the memory channel, and so\n1\nn n∑ t=1 I(Mt−1;Mt|Ot) ≤ RM .\nSimilarly, information absent from Mt−1 is subject to the sensory channel capacity\n1\nn n∑ t=1 I(Ot;Mt|Mt−1) ≤ RS .\nThe distortion constraint and the three informationrate constraints together form the problem of inference-planning in Passive POMDPs (Problem 1 ).\nThe emergence of three information-rate constraints for two channels is similar in spirit to multiterminal source coding (Berger, 1977). In their terminology, the agent needs to implement in each Mt a lossy coding of the correlated sources Mt−1 and Ot, under capacity constraints, so as to minimize an average expected distortion. The main difference is that here we chose to allow the encoding not to be distributed, in keeping with the ability of memory to interact with perception in biological agents (Laeng & Endestad, 2012)."
    }, {
      "heading" : "3. One-Step Optimization",
      "text" : ""
    }, {
      "heading" : "3.1. Variational Principle",
      "text" : "Before we consider the long-term planning required of the agent in Problem 1, we focus on the choice of qn in the final step, given the other transitions, that is, given the joint distribution of Mn−1, Wn and On. We define the joint belief θn(Mn−1,Wn) to be the joint distribution of Mn−1 and Wn, and have\nPr θn\n(Mn−1,Wn, On) = θn(Mn−1,Wn)σ(On|Wn).\nWe are interested in the rate-distortion region which includes all points (RM , RS , D) which are achievable, that is, for which there exists some qn(Mn|Mn−1, On) with\nDθn(qn) def = E\nWn,Mn d(Wn,Mn) ≤ D\nIC,θn(qn) def = I(Mn−1, On;Mn) ≤ RM +RS\nIM,θn(qn) def = I(Mn−1;Mn|On) ≤ RM\nIS,θn(qn) def = I(On;Mn|Mn−1) ≤ RS .\nFor any information-rate pair (RM , RS), the minimal achievable D lies on the boundary D∗θn(RM , RS) of the rate-distortion region. When θn and qn are clear from context, we refer to these quantities as D, IC , IM , IS and D∗. We find D∗(RM , RS) by minimizing the expected distortion under information-rate constraints. The minimum exists because all our formulas are continuous, and the solution space for qn is closed.\nLet q̄n(Mn|Mn−1), q̄n(Mn|On) and q̄n(Mn) be the marginals of qn(Mn|Mn−1, On). We expand the terms of the problem using these conditional probability distributions, to have\nmin qn,q̄n E Mn−1,Wn,On ∑ mn qn(mn|Mn−1, On)d(Wn,mn)\nE Mn−1,On\nDKL(qn(Mn|Mn−1, On); q̄n(Mn)) ≤ RM +RS\nE Mn−1,On\nDKL(qn(Mn|Mn−1, On); q̄n(Mn|On)) ≤ RM\nE Mn−1,On DKL(qn(Mn|Mn−1, On); q̄n(Mn|Mn−1)) ≤ RS under normalization constraints.1 We may waive the constraints of non-negative probabilities, which will essentially never be active as we shall see later. Also note that we optimize over qn and q̄n as distinct parameters. This is justified by theorem 1 which states that, at the optimum, q̄n are indeed the marginals of qn.\nLet the Lagrange multipliers for the constraints be γC , γM and γS , and their sum γ = γC + γM + γS . Leaving aside terms of log qn, the pointwise terms in the Lagrangian will be\nG(d, q̄n,Mn−1,Wn, On,Mn)\n= d(Wn,Mn)− γC log q̄n(Mn)\n−γM log q̄n(Mn|On)− γS log q̄n(Mn|Mn−1). In the following analysis, several expectations of this function will be useful:\n• Gθn(d, q̄n,Mn−1, On,Mn)\n= E Wn|Mn−1,On G(d, q̄n,Mn−1,Wn, On,Mn),\n• Gqn(d, q̄n,Mn−1,Wn) = E On,Mn|Mn−1,Wn G(d, q̄n,Mn−1,Wn, On,Mn),\n• Gθn,qn(d, q̄n) = E Mn−1,Wn,On,Mn G(d, q̄n,Mn−1,Wn, On,Mn)\n= Dθn(qn) + γCH(q̄n(Mn))\n+γMH(q̄n(Mn|On)) + γSH(q̄n(Mn|Mn−1)), 1The information-rate constraints result from the n-step Problem 1 by fixing the first n−1 steps, if we consider that only two of the constraints are actually used in any instance (see corollary 3).\nwhere H is the entropy function. The Lagrangian of the problem, up to normalization terms and additive constants, can now be written as\nL1(qn, q̄n; θn, γC , γM , γS) = Gθn,qn(d, q̄n)− γH(qn)."
    }, {
      "heading" : "3.2. Properties of the One-Step Lagrangian",
      "text" : "Theorem 1. For any fixed θn, L1 is convex in qn and q̄n. L1 is strictly convex in parameters which are conditional on mn−1 and on with Prθn(mn−1, on) > 0, and at the minimum these satisfy\nqn(Mn|Mn−1, On) (1)\n= exp(−γ−1Gθn(d, q̄n,Mn−1, On,Mn))\nZn(Mn−1, On) ,\nwhere Zn is a normalizing partition function, and q̄n(Mn) = ∑\nmn−1,on\nPr θn\n(mn−1, on)qn(Mn|mn−1, on)\nq̄n(Mn|On) = ∑ mn−1 Pr θn (mn−1|On)qn(Mn|mn−1, On)\nq̄n(Mn|Mn−1) = ∑ on Pr θn (on|Mn−1)qn(Mn|Mn−1, on).\n(2)\nProof. For any fixed θn, L1 is convex since all its terms are convex. Non-zero terms only involve mn−1 and on with Prθn(mn−1, on) > 0. Focusing on these parameters, the distortion terms are linear, and the information terms strictly convex. The unique feasible extremum of L1 is then the global minimum. Differentiating by each parameter gives equations 1 and 2.\nIf follows from theorem 1 that complementary slackness conditions are sufficient for optimality. Table 1 shows these conditions, the information rates (RM , RS) where the solution meets the boundary, and a subgradient of the boundary at that point. For example, if the minimum of L1 with γM = γS = 0 satisfies IC ≥ IM + IS , then for any information-rate pair in the interval [(IC −IS , IS), (IM , IC −IM )] the minimal achievable distortion is D and (−γC ,−γC) is a subgradient of the boundary.\nTheorem 2. For any joint belief θn, the boundary D∗θn(RM , RS) of the rate-distortion region is continuous and convex. Any point (RM , RS , D) on the boundary at which (−αM ,−αS) is a subgradient, is achieved by minimizing L1 for multipliers\n(γC , γM , γS) =  (0, αM , αS) if IC ≤ IM+IS (αM , 0, αS − αM ) if IC ≥ IM+IS and αM ≤ αS (αS , αM − αS , 0) if IC ≥ IM+IS\nand αM ≥ αS\nProof. Let transitions qn and q ′ n achieve the rate-distortion boundary at (RM , RS , D) and (R′M , R ′ S , D\n′), respectively, and let 0 ≤ λ ≤ 1. Then by equations 2 and the convexity of the KullbackLeibler divergence, the transition λqn + (1 − λ)q′n (over-)achieves the rate-distortion constraints λ(RM , RS , D) + (1 − λ)(R′M , R′S , D′). The ratedistortion region is therefore convex, and so is its boundary. The boundary is continuous by the continuity of the problem.\nFor a positive information-rate pair (RM , RS), having Mn independent of Mn−1 and On makes all information-rate constraints inactive. This satisfies the Slater condition, and the multipliers detailed in the theorem are then the Karush-Kuhn-Tucker multipliers necessary for qn to be optimal.\nCorollary 3. Let D∗C , D∗M and D∗S be the boundaries of the rate-distortion regions obtained by keeping each two of the three information-rate constraints. Then D∗ is their maximum."
    }, {
      "heading" : "3.3. Optimization Algorithm",
      "text" : "An algorithm which alternatingly minimizes L1 over each parameter with the others fixed, in the style of Blahut-Arimoto (Cover & Thomas, 2006), will allow us to find the minimum.\nTheorem 4. Algorithm 1 converges2 monotonically to the global minimum of L1.\nProof. L1 is non-increasing in each iteration and is bounded from below, which guarantees its monotonic convergence. That is\n2For the sake of clarity, here and in the rest of this paper strict convexity, uniqueness of minimum and convergence should all be taken with respect to events and transitions of positive probability, as justified by theorem 1.\nAlgorithm 1 Last-Step Optimization\nInput: P1, p, σ, d, γC , γM , γS , θn Output: optimal qn r ← 0 Initialize some suggestion for qrn repeat\nCompute the marginals q̄rn of q r n (eq. 2) Compute a new value for qr+1n from q̄ r n (eq. 1)\nr ← r + 1 until qrn converges\nL1(qrn, q̄rn)− L1(qr+1n , q̄rn) −−−−→ r−→∞ 0.\nBut qr+1n is the unique minimum of the continuous Lagrangian. This implies that qrn also converges to a solution q∗n with marginals q̄ ∗ n. By the continuity of the Lagrangian’s derivatives, they are all 0 at this solution."
    }, {
      "heading" : "4. Sequential Rate-Distortion",
      "text" : ""
    }, {
      "heading" : "4.1. Variational Principle",
      "text" : "Returning to the entire process of Problem 1, the sequence of joint beliefs θ(2,n) = θ2, . . . , θn depends recursively on θ1 and the policy q(n). For each 1 ≤ t < n\nθt+1(Mt,Wt+1) (3) = ∑\nmt−1,wt\nθt(mt−1, wt) Pr qt\n(Mt,Wt+1|mt−1, wt),\nwith θ1 given as the independent distribution of M0 and W1.\nAdding the constraints of equation 3 with multipliers νt,mt,wt+1 , the Lagrangian of Problem 1 is\nLn(q(n), q̄(n), θ(2,n)) = 1\nn n∑ t=1 L1(qt, q̄t; θt, γC , γM , γS)\n− 1 n n−1∑ t=1 ∑ mt,wt+1 νt,mt,wt+1 θt+1(mt, wt+1) −\n∑ mt−1,wt θt(mt−1, wt) Pr qt (mt, wt+1|mt−1, wt)  up to normalization terms and additive constants.\nSolving Ln is much more difficult than L1. Ln is not convex, and each step may affect all future steps. Intuitively, remembering some feature of the sample in one step is less rewarding if this information is discarded in a future step, and vice versa. This leads to Ln having many local minima."
    }, {
      "heading" : "4.2. Local Optimization Algorithm",
      "text" : "Nevertheless, Problem 1 still has some structure which can be insightful to explore. In particular, it has some interesting similarities to the standard POMDP planning problem. Differentiating Ln by qt we now get\nqt(Mt|Mt−1, Ot) (4)\n= exp(−γ−1Gθt(d ~νt t , q̄t,Mt−1, Ot,Mt))\nZt(Mt−1, Ot) ,\nwith\nd~νtt (Wt,Mt) = d(Wt,Mt) + E Wt+1|Wt νt,Mt,Wt+1 ,\nwhere ~νn = 0. qt now depends on the future through the multiplier vector ~νt. Note how the expectation of νt,Mt,Wt+1 given Wt plays a parallel role to that of d(Wt,Mt).\nLn is linear in each θt, and at the optimum must in fact be constant in every non-trivial component of θt. This gives us a recursive formula for computing ~νt−1 from ~νt, qt and q̄t. For 1 < t ≤ n, and whenever 0 < θt(Mt−1,Wt) < 1, we have\nνt−1,Mt−1,Wt = Gqt(d ~νt t , q̄t,Mt−1,Wt) (5)\n−γ E Ot|Wt H(qt(Mt|Mt−1, Ot)) + λt,Wt .\nNote that equation 5 is a linear backward recursion for ~νt. The multipliers ~λt come from the constraints that θt is a probability distribution function. It has no consequence, however, since it is independent of Mt−1, and is normalized out when ~νt−1 is used to compute qt−1 in equation 4.\nAt this point, we can introduce the following generalization of algorithm 1, which finds the optimal transition qt, given the joint belief θt and the policy suffix q(t+1,n) = qt+1, . . . , qn.\nAlgorithm 2 One-Step Optimization\nInput: P1, p, σ, d, γC , γM , γS , θt, q(t+1,n) Output: optimal qt r ← 0 Initialize some suggestion for qrt repeat\nCompute θr(t+1,n) from θt and q r (t,n−1) (eq. 3) Compute the marginals q̄r(t,n) of q r (t,n) (eq. 2) Compute ~νr(t,n−1) recursively backward (eq. 5) Compute qr+1t from θ r t , q̄ r t and ~ν r t (eq. 4)\nr ← r + 1 until qrt converges\nThis is a forward-backward algorithm. In each iteration we compute θ(t+1,n) = θt+1, . . . , θn recursively\nforward, and then ~ν(t,n−1) = ~νt, . . . , ~νn−1 recursively backward. The algorithm is guaranteed to converge monotonically to an optimal solution, since Ln is still strictly convex in each qt separately. In fact, all our theorems and proofs regarding algorithm 1 carry over to this generalization."
    }, {
      "heading" : "4.3. Joint-Belief MDP",
      "text" : "Expanding the recursion of ~νt in equation 5 to a closed form, and disregarding ~λt, we find that for 1 < t ≤ n and consistent parameters3\nLn−t+1(q(t,n); θt) (6)\n= 1 n− t+ 1 ∑\nmt−1,wt\nθt(mt−1, wt)νt−1,mt−1,wt .\nIf we extend the recursion by another step to define ~ν0, we get that our minimization target is\nLn(q(n); θ1) = 1\nn E M0,W1 ν0,M0,W1 .\nThe minimization\nVt(θt) = min q(t,n) E Mt−1,Wt νt−1,Mt−1,Wt\ncan be looked at as the cost-to-go given the joint belief θt before step t. Importantly, the recursive formula 5, when minimized over q(t,n), is a Bellman equation. It contains a recursive term\nE Mt,Wt+1|Mt−1,Wt νt,Mt,Wt+1 ,\nwhich is the expected future cost, and other terms which are the expected immediate costs, internal and external, of implementing qt in step t.\nThis suggests viewing our problem as a joint-belief MDP. Here the states are the joint beliefs θt, the actions are qt, and the next state always follows deterministically according to equation 3. This determinism allows us to use a time-dependent policy q(n), rather than a state-dependent one, and will prove useful in finding a solution.\nThe belief space of a standard POMDP can be looked at as the state space of a belief MDP, with the same actions and observations, and a linear transition function. If memory states are approximate beliefs, then our model is more like a further abstraction, where the MDP state space is the set of distributions over the belief space. Table 2 summarizes the main differences between this joint-belief MDP and the belief-MDP representation of discrete-action finite-horizon POMDPs.\n3When the Lagrangian is written in terms of the policy and the initial joint belief, the other parameters are taken to be consistent with them.\nOne important difference is in the structure of the value function. The expected cost Ln−t+1 of a fixed policy suffix q(t,n) consists of some linear terms of expected distortion, but also some strictly convex terms. The latter all take the form of a Kullback-Leibler divergence between qt′ , for some t\n′ ≥ t, and a marginal q̄t′ , the latter depending on θt through equations 2 and the recursion 3.\nThat this cost is not linear makes the representation of the value function a challenge, but a greater difficulty is the size of the policy space, which is finite in discreteaction finite-horizon POMDPs, but continuous here. Minimizing over it does not yield a piecewise-linear function of the joint belief, although it is still continuous, and the convex mixing of policies shows that it is still concave4. It is unclear how to finitely represent the resulting value function in our case."
    }, {
      "heading" : "4.4. Bounded Planning Algorithm",
      "text" : "Perhaps surprisingly, the determinism of the jointbelief MDP allows us to define a local criterion for optimality. Together with iterations of algorithm 2 which make local improvements, this will guarantee convergence to a local optimum.\nOur algorithm is a simple forward-backward algorithm, with a building block (algorithm 2) which is itself forward-backward. In each iteration we compute recursively forward the joint beliefs θ(n) for the current policy q(n). Then we compute recursively backward a new policy q′(n), by finding in each step t a policy suffix which is locally optimal for θt. The criterion for optimality is that in each step we can use either q′(t+1,n) from the previous step or q(t+1,n) from the previous iteration, and whichever leads to a lower cost is chosen.\nTheorem 5. Algorithm 3 converges monotonically to a limit cost L∗. For any ≥ 0, any qr(n) which costs within of L∗ is also within of a local minimum of\n4If rewards are used instead of costs, the value function is convex.\nAlgorithm 3 Passive POMDP Bounded Planning\nInput: θ1, p, σ, d, γC , γM , γS , n Output: locally optimal q(n) r ← 0 Initialize some suggestion for qr(n) repeat θr1 ← θ1 Compute θr(2,n) from θ r 1 and q r (n−1) (eq. 3)\nfor t← n to 1 do qr+1,t(t,n) ← arg minq(t,n) Ln−t+1(q(t,n); θ r t )\ns.t. q(t+1,n) ∈ { qr+1,t+1(t+1,n) , q r (t+1,n) } (alg. 2)\nend for qr+1(n) ← q r+1,1 (n)\nr ← r + 1 until Ln(qr(n); θ1) converges\nthe bounded-inference-planning problem (section 2), in the sense that for any 1 ≤ t ≤ n, the global minimum given qr(t−1) and q r (t+1,n) is at most better than q r (n).\nProof. In iteration r, qr(n) from the previous iteration is feasible for qr+1(n) . Therefore the cost of q r (n) is nonincreasing in r and converges monotonically to a limit L∗.\nLet qr(n) be within some > 0 of L ∗. Fix any 1 ≤ t ≤ n, and let q∗t achieve the global optimum given q r (t−1) and qr(t+1,n). Then\nLn(qr(n); θ1)− ≤ Ln(q r+1 (n) ; θ1)\n(a) ≤ Ln((qr(t−1), q r+1,t (t,n) ); θ1)\n(b) ≤ Ln((qr(t−1), q ∗ t , q r (t+1,n)); θ1),\nwhere\n(a) follows recursively from (qrt′ , q r+1,t′+1 (t′+1,n) ) being fea-\nsible for θrt′ in iteration r, for each 1 ≤ t′ < t, and\n(b) follows from (q∗t , q r (t+1,n)) being feasible for θ r t in\niteration r.\nWhere algorithm 3 runs algorithm 2, it can initialize qt to q r t from the previous iteration. This may speed up each iteration, particularly when the algorithm has nearly converged. In addition, when running algorithm 3 with different sets of multipliers, it converges much faster if each run is initialized with the previous result. Empirically, this also leads to much better local minima if the runs are sorted in order of decreasing multipliers."
    }, {
      "heading" : "5. Simulations",
      "text" : ""
    }, {
      "heading" : "5.1. Symmetric Channel",
      "text" : "Figure 2 shows the boundary of the rate-distortion region for the 30-step sequential symmetric channel problem. The domains W, O and M are all binary. The agent observes the state correctly with probability 0.8. The state remains the same for the next step independently with probability 0.8. The distortion is the delta function.\nThe boundary consists of three parts as in corollary 3. They have γM = 0 (left), γM = γS = 0 (middle) and γS = 0 (right). Empirically, taking γC = 0 is never feasible, as no optimal solution ever has IC ≤ IM+IS .\nTo clarify this further, figure 3 shows a colored contour map of the boundary. The lower the distortion, the higher the required information rates. The tradeoff between memory and perception is illustrated by the negative slope of the contours."
    }, {
      "heading" : "5.2. Kelly Gambling",
      "text" : "Three horses are running in 10 races. Each horse has a fitness rating fi ∈ {1, 2, 3}, and the winning horse is determined by softmax, i.e. horse i wins with probability proportional to exp(fi). Between the races, the fitness of each horse may independently grow by 1, with probability 0.1 if it is not maxed out, or drop by 1, with probability 0.1 if it is not depleted. Each horse keeps its fitness with the remaining probability.\nThe only observations are side races performed before each race: 2 random horses compete (with softmax) and the identities of the winner and the loser are announced. The memory state is a model of the world, consisting of the presumed fitness f̂i of each horse. The log-optimal proportional gambling strategy is used (Kelly gambling, see Cover & Thomas, 2006), betting on horse i a fraction of the wealth proportional to exp(f̂i). Each bet is double-or-nothing, and the distortion is the expected log return on the portfolio.\nFigure 4 shows the contour map, which is not convex in this instance."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We have presented the problem of planning in Passive POMDPs with information-rate constraints. This problem takes the form of a sequential version of ratedistortion theory, and accordingly we were able to provide algorithms which globally optimize each step individually. Unfortunately, the full problem is not convex, and we expect that it has very hard instance sets.\nNevertheless, typical instances with some locality in their transitions and observations are expected to be easier. We have introduced an efficient and simple algorithm for finding a local minimum, and have used it to illustrate the problem with two simulations. In doing so, we have demonstrated the emergence of a\nmemory-perception tradeoff in the problem.\nOur work has been motivated by the problem of planning in general POMDPs, which may benefit from belief approximation which is principled by information theory. The application of our current results to this problem is left for future work."
    }, {
      "heading" : "7. Acknowledgement",
      "text" : "This project is supported in part by the MSEE DARPA Project and by the Gatsby Charitable Foundation."
    } ],
    "references" : [ {
      "title" : "Optimizing fixed-size stochastic controllers for POMDPs and decentralized POMDPs",
      "author" : [ "Amato", "Christopher", "Bernstein", "Daniel S", "Zilberstein", "Shlomo" ],
      "venue" : "Autonomous Agents and Multi-Agent Systems,",
      "citeRegEx" : "Amato et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Amato et al\\.",
      "year" : 2010
    }, {
      "title" : "Multiterminal source coding",
      "author" : [ "Berger", "Toby" ],
      "venue" : "The Information Theory Approach to Communications,",
      "citeRegEx" : "Berger and Toby.,? \\Q1977\\E",
      "shortCiteRegEx" : "Berger and Toby.",
      "year" : 1977
    }, {
      "title" : "Reinforcement learning with perceptual aliasing: The perceptual distinctions approach",
      "author" : [ "Chrisman", "Lonnie" ],
      "venue" : "In Proceedings of the tenth national conference on Artificial intelligence,",
      "citeRegEx" : "Chrisman and Lonnie.,? \\Q1992\\E",
      "shortCiteRegEx" : "Chrisman and Lonnie.",
      "year" : 1992
    }, {
      "title" : "Optimal control as a graphical model inference problem",
      "author" : [ "Kappen", "Bert", "Gómez", "Vicenç", "Opper", "Manfred" ],
      "venue" : null,
      "citeRegEx" : "Kappen et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kappen et al\\.",
      "year" : 2009
    }, {
      "title" : "Bright illusions reduce the eye’s pupil",
      "author" : [ "Laeng", "Bruno", "Endestad", "Tor" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Laeng et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Laeng et al\\.",
      "year" : 2012
    }, {
      "title" : "Point-based value iteration: An anytime algorithm for POMDPs",
      "author" : [ "Pineau", "Joelle", "Gordon", "Geoffrey J", "Thrun", "Sebastian" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Pineau et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Pineau et al\\.",
      "year" : 2003
    }, {
      "title" : "Bounded finite state controllers",
      "author" : [ "Poupart", "Pascal", "Boutilier", "Craig" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Poupart et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Poupart et al\\.",
      "year" : 2003
    }, {
      "title" : "Exponential family PCA for belief compression in POMDPs",
      "author" : [ "Roy", "Nicholas", "Gordon", "Geoffrey J" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Roy et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Roy et al\\.",
      "year" : 2002
    }, {
      "title" : "Information theory of decisions and actions",
      "author" : [ "Tishby", "Naftali", "Polani", "Daniel" ],
      "venue" : "PerceptionAction Cycle,",
      "citeRegEx" : "Tishby et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Tishby et al\\.",
      "year" : 2011
    }, {
      "title" : "Linearly-solvable Markov decision problems",
      "author" : [ "Todorov", "Emanuel" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Todorov and Emanuel.,? \\Q2006\\E",
      "shortCiteRegEx" : "Todorov and Emanuel.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Several works do so by optimizing a finite-state controller of a given size (Poupart & Boutilier, 2003; Amato et al., 2010).",
      "startOffset" : 76,
      "endOffset" : 123
    }, {
      "referenceID" : 5,
      "context" : "A different approach is to explicitly select a subset of beliefs, and use them to guide the iterations (Pineau et al., 2003).",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 3,
      "context" : "Some research treats POMDPs where the cost is the DKL between the distributions of the next world state when it is controlled and uncontrolled (Todorov, 2006; Kappen et al., 2009).",
      "startOffset" : 143,
      "endOffset" : 179
    }, {
      "referenceID" : 3,
      "context" : "Some research treats POMDPs where the cost is the DKL between the distributions of the next world state when it is controlled and uncontrolled (Todorov, 2006; Kappen et al., 2009). This has interesting analogies to our setting. Our information-rate constraints define, in effect, components of the cost which are the DKL between the distribution of the next memory state and its marginals (see section 3.1). Tishby & Polani (2011) combine similar information-rate constraints of perception and action together.",
      "startOffset" : 159,
      "endOffset" : 431
    } ],
    "year" : 2012,
    "abstractText" : "In Passive POMDPs actions do not affect the world state, but still incur costs. When the agent is bounded by information-processing constraints, it can only keep an approximation of the belief. We present a variational principle for the problem of maintaining the information which is most useful for minimizing the cost, and introduce an efficient and simple algorithm for finding an optimum.",
    "creator" : "LaTeX with hyperref package"
  }
}