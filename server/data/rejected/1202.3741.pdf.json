{
  "name" : "1202.3741.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Noisy Search with Comparative Feedback",
    "authors" : [ "Shiau Hong Lim", "Peter Auer" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We present theoretical results in terms of lower and upper bounds on the query complexity of noisy search with comparative feedback. In this search model, the noise in the feedback depends on the distance between query points and the search target. Consequently, the error probability in the feedback is not fixed but varies for the queries posed by the search algorithm. Our results show that a target out of n items can be found in O(log n) queries. We also show the surprising result that for k possible answers per query, the speedup is not log k (as for k-ary search) but only log log k in some cases."
    }, {
      "heading" : "1 Introduction",
      "text" : "We investigate a form of noisy search that arises in information retrieval systems, e.g. content-based image retrieval [Cox et al., 2000]. Consider a system where a user can search for a target item (unknown to the system) by answering a sequence of “queries” generated by the system. As an example, consider a content-based image retrieval system. The system first presents k images to the user. The user then selects one that is the most similar to the target image. Based on the image chosen by the user, the system presents a new set of k images and the user answers by selecting one of these k images. This continues until the target is found by the system. We assume that the user response in each query is probabilistic and depends on the similarities between the presented images and the target image according to a comparative feedback model as proposed in [Cox et al., 2000] and [Auer and Leung, 2009]. The search process is inherently noisy due to the probabilistic nature of the user feedback. We are interested in the performance of such systems in terms of query complexity, i.e the expected\nnumber of queries needed before a target is found."
    }, {
      "heading" : "1.1 The comparative feedback model",
      "text" : "We consider a finite set of n data points X = {x1, . . . , xn} and a search target T ∈ X . In each query, a set of k (k ≥ 2) distinct data points Q = {q1, . . . , qk} ⊂ X is presented to the user. The search terminates if T ∈ Q, otherwise the user responds by selecting one of the query points qj , j ∈ {1, . . . , k}.\nLet R ∈ {1, . . . , k} be the random user response (note that we use the indices instead of the query points themselves). The comparative feedback model specifies the probability of choosing a particular response R = r as follows:\nPr(R = r|Q, T ) = S(T, qr)∑k j=1 S(T, qj)\nwhere S(·, ·) measures the similarity between data points. This implies that query points close to the target are chosen with larger probability than query points far from the target. Given a distance function d(·, ·) between data points, we consider two types of similarity measures. The first similarity measure decreases polynomially with increasing distance,\nS(x, y) = d(x, y)−θ, (1)\nwhile the second similarity measure decreases exponentially,\nS(x, y) = exp{−θd(x, y)}. (2)\nIn both cases, θ > 0 is a parameter indicating the user “sharpness”. Larger θ implies that the user favors items closer to the target more intensely.1\n1The main difference between the two similarity measures is that for the polynomial similarity measure, the user response depends on the relative differences of the distances to the target, while for the exponential similarity measure the response depends on the absolute differences of the distances. To see this let k = 2. Then Pr(R = 1|q1, q2, T ) ="
    }, {
      "heading" : "1.2 Overview of results",
      "text" : "The results of our analysis provide both lower bounds and upper bounds for the query complexity. The lower bounds are shown to hold for any algorithm, and the upper bounds provide performance guarantees for actual algorithms that are presented in this paper.\nMany formulations of noisy search have been proposed in the literature, each with different assumptions regarding the type of noise/uncertainty present in the user feedback. In the simplest, noise-free case, the search is reduced to standard k-ary search, and the query complexity can be as low as lognlog k where n is the total number of items and k is the number of possible answers to the queries. In the case of binary search where the user makes a mistake with a fixed probability p in each query, a lower bound of logn+o(logn)1−H(p) [Renyi, 1961] and an upper bound of logn+O(log logn δ ) 1−H(p) [Ben-Or and Hassidim, 2008] are known, where H(p) denotes Shannon’s entropy. Similar results for k-ary queries with fixed error probability are also available (see [Ben-Or and Hassidim, 2008]). For a survey of a wide range of noisy search problems, see [Pelc, 2002].\nThe type of uncertainty in the comparative feedback model, however, is unique in the sense that the noise in the user feedback is sensitive to distances among data points. For example, the uncertainty is higher when the query points have relatively similar distances to the target.\nIn the following section we present our theoretical results, while the proofs are given in a separate section. Most of our results are stated for 1-dimensional data. This might not be realistic for certain data (e.g. images) but highlights the properties of the search problem induced by the comparative feedback model. The main result is that with k = 2, O(log n) query complexity is possible for both the polynomial (Eq. 1) and the exponential (Eq. 2) similarity measures. This result carries over also to data of arbitrary dimension, but with a possible cost that is exponential in the dimension. Surprisingly, we can show that larger k helps more for the exponential similarity measure (by a factor log k as expected from k-ary search), while the improvement for the polynomial similarity measure (Eq. 1) is shown to be at most by a factor log log k in some cases.\n1/(1+[ d(q1,T ) d(q2,T ) ]θ) for the polynomial similarity measure and Pr(R = 1|q1, q2, T ) = 1/(1+exp{θ[d(q1, T )−d(q2, T )]}) for the exponential similarity measure."
    }, {
      "heading" : "2 Results",
      "text" : "We assume that the user model and its parameters are known to the search algorithm.\nExcept for the results in Section 2.2 we also assume that X ⊂ R and that the distance between data points is measured as d(xi, xj) = |xi − xj |.\nSince the search algorithm knows the user model, it can maintain the posterior target distribution in respect to the queries and user responses so far. We denote the posterior probability of data point xi being the target by ai and we use a to denote the vector (a1, . . . , an). After receiving a user response R = r to a query Q, this posterior is updated as\na′i ← bi,r := Pr(T = xi|Q, R = r,a)\n= Pr(T = xi|a)Pr(R = r|Q, T = xi)\nPr(R = r|Q,a)\n= aipi,r Ar\nwhere\npi,r = Pr(R = r|Q, T = xi) = S(xi, qr)∑k j=1 S(xi, qj)\nand\nAr = Pr(R = r|Q,a) = n∑ i=1 aipi,r.\nWhen X ⊂ R, we assume x1 < · · · < xn and denote by ci the cumulative probabilities\nci = i∑\ni′=1\nai′ .\nThe quantiles of the cumulative probability are denoted by I(p) where I(p) is the index such that\ncI(p)−1 < p ≤ cI(p).\nWe will use A for (A1, . . . , Ak) and pi for (pi,1, . . . , pi,k). Finally, we denote the entropy function by\nH(a) = − n∑ i=1 ai log ai ,\nand the KL-divergence by\nD(a||a′) = n∑ i=1 ai log ai a′i .\nWe will use base-2 logarithms throughout. Also, for scalar p and p′, we define H(p) = H(p, 1 − p), and similarly D(p||p′) = D ( (p, 1− p)||(p′, 1− p′) ) ."
    }, {
      "heading" : "2.1 Efficient search for the polynomial similarity measure",
      "text" : "In this section we show, for X ⊂ R, how query complexity O(log n) can be achieved in the comparative feedback model with polynomial similarity measure for k = 2. The initial uncertainty about the target is log n when measured by entropy. To guarantee at most O(log n) query iterations, we need to select query points such that a constant information gain is guaranteed. This can be achieved in the following way:\n• Calculate the quantiles is = I(s/4) for s = 0, . . . , 4 (i0 = 1, i4 = n), and consider the resulting 4 intervals (see Fig. 1 for an example, where d1, . . . , d4 mark the intervals).\n• From these four intervals find the one with the smallest length.\n• Query the endpoints of that interval adjacent to the smallest interval, which does not contain x1 or xn.\nFigure 1 shows an example for the selection of the query points. The rationale for this algorithm can be seen by this example: for any data point x in interval d2 we have |x − q1|/|x − q2| ≤ 1/2, while for any data point x in interval d4 we have |x−q1|/|x−q2| ≥ 1. Thus a user response R = 1 significantly indicates that the target is rather in d2 than in d4, while a user response R = 2 mildly indicates that the target is rather in d4 than in d2. This is sufficient to prove the following theorem, the proof is given in Section 3.2.\nTheorem 1. For the comparative feedback model with polynomial similarity measure, the expected number of queries of the above algorithm is at most\n4 log2 n Gρ + 4,\nwhere Gρ = 14 ( D(ρ||φ) +D( 12 ||φ) ) > 0\nwith ρ = 2 θ\n1+2θ and φ = 14 + 1 2ρ. The expectation is\ntaken over targets selected uniformly at random and the random responses of the user.\n2.2 Search in a D-dimensional space\nThe next theorem generalizes Theorem 1 to Ddimensional data points. The drawback is that the constant in the upper bound depends exponentially on the dimension. It can indeed be shown that this is necessary if distances between data points are measured by the ∞-norm.\nWe assume that distances are measured by some pnorm. To mimic the construction of Theorem 1, we need a strategy for choosing query points such that there are two “well-separated” groups of data points with significant probability mass. This can be achieved as follows:\n• Let B be the smallest || · ||p-ball with probability mass ∑ i:xi∈B ai ≥ c for some positive constant\nc (to be determined later). In case of ties, the || · ||p-ball with the largest total probability mass is chosen.\n• Choose an arbitrary point in B as the first query point q1.\n• Let λ be the radius of B. Let B′ be the || · ||p-ball centered on B with radius 7λ.\n• Choose the second query point q2 as the data point not in B′ that is closest to q1.\nBy the above construction we have for all x ∈ B, ||x− q1||p ≤ 2λ and ||x − q2||p ≥ ||q2||p − ||x||p ≥ 6λ, such that\n||x− q1||p ≤ ||x− q2||p/3,\nand for all x 6∈ B′ we have ||x − q2||p ≤ ||x − q1||p + ||q1 − q2||p ≤ 2||x− q1||p such that\n||x− q1||p ≥ ||x− q2||p/2.\nSince (14D)D || · ||p-balls of radius λ/2 are sufficient to cover B′ (2), the probability mass of B′ is at most (14D)Dc (otherwise there would be a || · ||p-ball of radius λ/2 with probability mass at least c). Thus for c = (14D)−D/2 the probability mass of the data points not in B′ is at least 1/2. This is sufficient to prove, similarly to Theorem 1, the following theorem. The detailed proof is omitted.\nTheorem 2. For the comparative feedback model with polynomial similarity measure and D-dimensional data, the expected number of queries of the above algorithm is at most O ( (14D)D log n ) .\n2This can be seen by covering B′ with || · ||∞-balls of radius λ\n2D , and then covering each of these || · ||∞-balls by\na || · ||p-ball of radius λ/2."
    }, {
      "heading" : "2.3 For the polynomial similarity measure large k may not help much",
      "text" : "Using k query points, one could expect that the number of query iterations — similarly as for k-ary search — can be reduced by a factor of log k. Surprisingly, we can show that for some search problems this is not the case in the comparative feedback model with polynomial similarity measure.\nTheorem 3. For the comparative feedback model with polynomial similarity measure, there are data points x1 < · · · < xn ∈ R such that the expected number of queries for any search algorithm is at least\nlog n− log(2k) 2 log log k + 4\n= Ω (\nlog n log log k ) when the target is selected uniformly at random from {x1, . . . , xn}.\nWe will choose increasingly distant data points, xi+1− xi xi − xi−1, such that the similarity of xi to data points with smaller index is not much smaller than the similarity to data points with larger index, S(xi, x1) ≥ S(xi, xi+1)/2. The idea behind this is that then for any query q1 < · · · < qk and any target T = xi, the probability of the user response can be bounded by Pr{R = j|T = xi} ≤ 2/j. Thus query responses with large j are rather unlikely for any target point, which ensures that the expected information gain per query response is only log log k, in contrast to information gain log k for k-ary search with deterministic responses. The full calculation leading to Theorem 3 is given in Section 3.3.\n2.4 The exponential similarity measure allows k-ary search\nIn contrast to the previous section, we show in this section that for the exponential similarity measure it is always possible to find the target within O ( logn log k ) queries. This is achieved by the following selection algorithm: the algorithm subsequently chooses k disjoint intervals I1, . . . , Ik of minimal length such that3\n• each interval has probability mass at least\nc = 1\n14k − 12 ,\ni.e. ∑\ni:xi∈Ij ai ≥ c,\n• and the distance between two intervals Ij = [xj , yj ] and Ij′ = [xj′ , yj′ ] is at least the length\n3In case the construction fails, it is easy to show that there must already exist data points with total probability mass at least 1/2. This will be covered in the proof.\nof the smaller interval, i.e. xj′ − yj ≥ min{yj − xj , yj′ − xj′} for yj < xj′ .\nThe algorithm selects one endpoint from each interval as a query point. For each interval Ij = [xj , yj ], if the first half of the interval [xj , xj+yj 2 ] contains more probability mass than the second half [xj+yj2 , yj ] then xj is selected as the query point, otherwise yj is selected.\nLet denote by X ′ all data points in these intervals that are closer to the query point of the respective interval than to any other query point. By construction, the probability mass of these points is at least c/2 in each interval. For each xi ∈ X ′ let ri be the index of the corresponding query point (i.e. the query point is qri). Furthermore, let δ0 be the minimal distance between any two data points. Then it can be shown that there is a constant β > 0 depending on δ0 and the user sharpness θ (but not on k), such that for any xi ∈ X ′,\npi,ri ≥ β.\nTogether with the observation that the probability mass of X ′ is at least ck/2 ≥ 1/28, this is sufficient to show an information gain of Ω(log k) in each query. This gives the following result.\nTheorem 4. For the comparative feedback model with exponential similarity measure and data points xi ∈ R, the expected number of queries for the above selection algorithm is at most O ( logn log k ) .\nThe proofs are provided in Section 3.4."
    }, {
      "heading" : "3 Proofs",
      "text" : ""
    }, {
      "heading" : "3.1 Preliminaries",
      "text" : "Lemma 1. The expected information gain in each query is given by:\nE [ H(a)−H(a′) ] = n∑ i=1 aiD(pi||A)\nProof. After each query, the expected entropy of the posterior is given by:\nE [ H(a′) ] = k∑ j=1 Pr(R = j)H(b1,j , . . . , bn,j)\n= − k∑\nj=1\nAj n∑ i=1 bi,j log bi,j\n= − k∑\nj=1\nAj n∑ i=1 aipi,j Aj log aipi,j Aj\n= − k∑\nj=1 n∑ i=1 aipi,j ( log ai + log pi,j Aj ) = H(a)−\nn∑ i=1 aiD(pi||A)\nRearranging the terms completes the proof.\nLemma 2. Let φ1, . . . ,φl be any set of l probability distributions on the k query points and α1, . . . , αl be any set of l positive weights. Then, for any distribution r on the k query points,\nl∑ i=1 αiD(φi||r) ≥ l∑ i=1 αiD(φi||φ̄)\nwhere\nφ̄ = ∑l\ni=1 αiφi∑l i=1 αi\n.\nProof.\nl∑ i=1 αiD(φi||r)\n= l∑\ni=1\nαi k∑ j=1 φi,j log ( φ̄j rj )(φi,j φ̄j )\n= k∑\nj=1 ( l∑ i=1 αiφi,j ) log φ̄j rj + l∑ i=1 αi k∑ j=1 φi,j log φi,j φ̄j\n= ( l∑\ni=1\nαi ) k∑ j=1 φ̄j log φ̄j rj + l∑ i=1 αi k∑ j=1 φi,j log φi,j φ̄j\n= ( l∑\ni=l\nαi\n) D(φ̄||r) + l∑ i=1 αiD(φi||φ̄)\n≥ l∑\ni=1\nαiD(φi||φ̄)\nwhere the last inequality is due to non-negativity of relative entropy.\nLemma 3. Let Φ ⊂ {1, . . . , n} be any subset of indices for the data points. The expected information gain in any query is at least∑\ni∈Φ aiD(pi||φ)\nwhere φ = P\ni∈Φ aipiP i∈Φ ai .\nProof. Apply Lemma 2 to Lemma 1."
    }, {
      "heading" : "3.2 Proof of Theorem 1",
      "text" : "Lemma 4. Let Φ,Ψ ⊂ {1, . . . , n} be two disjoint sets of indices such that ∑ i∈Φ ai ≥ α, ∑ i∈Ψ ai ≥ β, and\n(min i∈Φ pi) = p > q = (max i∈Ψ pi) .\nLet p̄ = ∑\ni∈Φ∪Ψ aipi∑ i∈Φ∪Ψ ai and µ = αp+ βq α+ β .\nThen ∑ i∈Φ∪Ψ aiD(pi||p̄) ≥ αD(p||µ) + βD(q||µ) . Proof. Let aΦ = ∑ i∈Φ ai, aΨ = ∑ i∈Ψ ai,\npΦ = ∑ i∈Φ ai aΦ pi and pΨ = ∑ i∈Ψ ai aΨ pi .\nNote that\np̄ = aΦpΦ + aΨpΨ\naΦ + aΨ .\nWe have ∑ i∈Φ∪Ψ aiD(pi||p̄)\n= ∑ i∈Φ aiD(pi||p̄) + ∑ i∈Ψ aiD(pi||p̄)\n= aΦ ∑ i∈Φ ai aΦ D(pi||p̄) + aΨ ∑ i∈Ψ ai aΨ D(pi||p̄) ≥ aΦD(pΦ||p̄) + aΨD(pΨ||p̄)\nwhere the inequality is due to convexity of D.\nLet f(aΦ, aΨ) = aΦD(pΦ||p̄) + aΨD(pΨ||p̄) be a function of aΦ and aΨ. It can be shown, by taking derivatives, that f(aΦ, aΨ) ≥ f(α, β).\nAssume now that aΦ = α and aΨ = β. Let g(pΦ, pΨ) = αD(pΦ||p̄) + βD(pΨ||p̄) be a function of pΦ and pΨ. Taking derivatives with respect to pΦ and pΨ, and using the fact that p > q, it is straightforward to show that the minimum is at g(p, q).\nLemma 5. In each query iteration the algorithm of Section 2.1 achieves at least\nGρ = 14 ( D(ρ||φ) +D( 12 ||φ) ) expected information gain where ρ = 2 θ\n1+2θ and φ =\n1 4+ 1 2ρ, if all four intervals considered by the algorithm have non-zero length. Furthermore, Gρ > 0.\nProof. Lemma 3 allows us to ignore the contribution of subsets of data points in analyzing the information gain. In particular, we will focus attention onto only two of the four intervals of data points, the smallest interval and another interval such that the two query points are between these two intervals. Without loss of generality we assume the configuration of Figure 1, such that these intervals are d2 and d4.\nThe key property of all data points in interval d2 is that the distance to q1 is at most half the distance to q2. Therefore, for any xi ∈ d2,\npi,1 = S(xi, q1)\nS(xi, q1) + S(xi, q2) =\n|xi − q2|θ\n|xi − q1|θ + |xi − q2|θ\n≥ 2 θ\n1 + 2θ = ρ > 1 2 .\nOn the other hand, for any xi ∈ d4, pi,1 < 12 . Since∑ xi∈d2 ai ≥ 1/4 and ∑ xi∈d4 ai ≥ 1/4, applying Lemma 3 and Lemma 4 (setting α = β = 1/4, p = ρ and q = 1/2), the expected information gain is at least∑ i∈Iσ1∪Iσ2 aiD(pi||p̄) ≥ 1 4 D(ρ||φ) + 1 4 D( 1 2 ||φ) .\nFinally, since θ > 0, 12 < φ < ρ, and hence D(ρ||φ) > 0 and D( 12 ||φ) > 0.\nProof of Theorem 1. From Lemma 5 we get that the expected number of query iterations until one of the considered intervals is of zero length, is at most (log n)/Gρ. If there is a zero length interval, then there is a data point xi with ai ≥ 1/4 that is selected as one of the query points q1 or q2. Thus in this case the search terminates with probability at least 1/4.\nNow let τ be an upper bound on the expected total number of query iterations. Then the above reasoning gives the following recursion,\nτ ≤ 1 4 [ log n Gρ + 1 ] + 3 4 [ log n Gρ + 1 + τ ] ,\nsince the search essentially might restart when the query point from a zero length interval is not the target. Solving for τ gives\nτ ≤ 4 log n Gρ + 4."
    }, {
      "heading" : "3.3 Proof of Theorem 3",
      "text" : "The following is a suitable choice of the data points: for all i > 1,\nxi+1 = xi − x1 · 2−1/θ\n1− 2−1/θ\nsuch that ( xi+1 − xi xi+1 − x1 )θ = 1 2 ,\nwhere θ is the parameter in the polynomial similarity measure, S(x, y) = |x− y|−θ. Lemma 6. With the above choice of data points,\nS(x, xi) ≤ 2S(x, x1)\nfor any data points x 6= xi.\nProof. By construction we have S(xi+1, xi) = 2S(xi+1, x1). Since\nS(x,xi) S(x,x1) is decreasing in x, the statement holds for any x ≥ xi+1. Furthermore, for x ≤ xi−1, S(x, xi) = S(xi, x) ≤ 2S(xi, x1) ≤ 2S(x, x1).\nLemma 7. With the above choice of data points, for any query q1 < · · · < qk and any data point x 6∈ {q1, . . . , qk}, the probability of choosing the j-th query point is bounded by\nPr(R = j|T = x) ≤ 2 j\n.\nProof. Let ` be the index of the query points such that q` < x < q`+1 and assume that ` ≤ j. Then\nPr(R = j|T = x) = S(x, qj)Pk j′=1 S(x, qj′)\n= S(x, qj)P` j′=1 S(x, qj′) + Pk j′=`+1 S(x, qj′) ≤ S(x, qj)P` j′=1 S(x, qj′) + Pj j′=`+1 S(x, qj′)\n≤ S(x, qj) ` · S(x, x1) + (j − `) · S(x, qj)\n≤ S(x, qj) j ·min{S(x, x1), S(x, qj)}\n≤ 2 j\nby Lemma 6. If ` > j then\nS(x, qj)∑k j′=1 S(x, qj′) ≤ S(x, qj)∑j j′=1 S(x, qj′) ≤ S(x, qj) j · S(x, x1) ≤ 2 j\nagain by Lemma 6.\nLet Rt ∈ {1, . . . , k} be the random user response to the t-th query and let ω denote a possible randomization of the search algorithm. Then, given a particular sequence of user responses R1 = j1, R2 = j2, . . . , Rt = jt, the set of query points chosen by the algorithm for the (t + 1)-th query is given by a function Q(j1, j2, . . . , jt|ω). Let Qt be the random variable Qt = Q(R1, . . . , Rt−1|ω). We assume that the target T 6∈ Qt if T ∈ Qτ for some τ < t.\nLemma 8. If the target T is chosen uniformly at random from the choice of data points above, then for k ≥ 3,\nPr(T ∈ Qt) ≤ k\nn (4 log k)t−1 .\nProof.\nPr(T ∈ Qt)\n= 1\nn X ω X x Pr(T ∈ Qt|T = x, ω) Pr(ω)\n= 1\nn X ω X x X j1,...,jt−1 Pr(R1 = j1, . . .\n, Rt−1 = jt−1, T ∈ Qt|T = x, ω) Pr(ω)\n= 1\nn X ω X x X j1,...,jt−1 Pr(R1 = j1, . . .\n, Rt−1 = jt−1, x ∈ Q(j1, . . . , jt−1|ω)|T = x, ω) Pr(ω)\n= 1\nn X ω X j1,...,jt−1 X x∈Q(j1,...,jt−1|ω)\nPr(R1 = j1, . . . , Rt−1 = jt−1|T = x, ω) Pr(ω)\n≤ 1 n X ω X j1,...,jt−1 X x∈Q(j1,...,jt−1|ω)“ 2\nj1 ” · · · “ 2 jt−1 ” Pr(ω) (3)\n= k\nn X j1,...,jt−1 “ 2 j1 ” · · · “ 2 jt−1 ” ≤ k\nn (4 log k)t−1\nfor k ≥ 3. Inequality (3) follows from Lemma 7 and the fact that the user responses are independent given the target.\nProof of Theorem 3. Using Lemma 8, we have\nτ∑ t=1 Pr(T ∈ Qt) ≤ k n (4 log k)τ − 1 4 log k − 1 ≤ k n (4 log k)τ ≤ 1 2\nfor\nτ ≤ log n2k\nlog log k + 2 ,\nand the theorem follows from Markov’s inequality."
    }, {
      "heading" : "3.4 Proof of Theorem 4",
      "text" : "The query construction of the selection algorithm fails only if there are k data points with total probability mass ≥ 1/2. We will deal with this case later. For now we assume that the construction succeeds.\nLemma 9. For any xi ∈ X ′,\npi,ri ≥ β\nwhere\nβ = 1\n1 + 2e−θδ0(1 + 1θδ0 ) .\nProof. For xi ∈ X ′,\npi,ri = Pr(R = ri|T = xi)\n= S(xi, qri)∑k j=1 S(xi, qj) = 1\n1 + ∑\nj 6=ri e −θ(|xi−qj |−|xi−qri |)\n≥ 1 1 + ∑ j 6=ri e −θ(|j−ri|δ0) (4) ≥ 1 1 + 2 ∑(k−1)/2 j′=1 e −j′θδ0 (5) ≥ 1 1 + 2e−θδ0(1 + 1θδ0 ) (6)\nFor inequality (4), by construction, the distance between any two adjacent query points is at least δ0 and the distance between any two query points j′ intervals apart is j′δ0. In inequality (5), we rewrite the indices and use the worst case where ri is the query point in the center-most interval, with (k − 1)/2 query points on each side of qri . The inequality (6) is obtained using the integral bound.\nLemma 10. For any query constructed as described in Section 2.4, the expected information gain is Ω(log k).\nProof. Using Lemma 3 we find that the expected information gain is at least∑\ni:xi∈X ′ aiD(pi||φ) (7)\nwhere\nφ = ∑ i:xi∈X ′ aipi∑ i:xi∈X ′ ai .\nGiven the constraint of Lemma 9, (7) is minimized for pi,ri = β, pi,r = (1 − β)/(k − 1) for r 6= ri, and φr = 1/k.4 Thus\nD(pi||φ) = β log(βk) + (1− β) log (1− β)k k − 1\n= β log k + (1− β) log „ 1 + 1\nk − 1\n« −H(β)\n= β log k +O(1).\nSince ∑\ni:xi∈X ′ ai ≥ 1/28, this completes the proof.\n4The minimum can be found using the method of Lagrange multipliers.\nProof of Theorem 4. Using Lemma 10, the proof proceeds similarly to the proof of Theorem 1, observing that if the construction of the selection algorithm fails, then the search terminates with probability ≥ 1/2."
    }, {
      "heading" : "4 Empirical testing of the feedback model",
      "text" : "A question regarding the comparative feedback model is whether it adequately models the uncertainty of user response in real-world applications. We performed statistical goodness-of-fit tests on actual user feedback in image search tasks where distances are measured by Euclidean distance in the image feature space [Auer et al., 2011]. In most of the tasks, the model with polynomial similarity measure fits the data very well while in contrast, the model with exponential similarity measure can be rejected with high confidence. We believe that the distance metric plays a crucial role here and more conclusive results require further investigations.\nThe results in this paper assume that the model parameter θ is known to the algorithm, which is unlikely in practice. However, empirical evidence suggests that search performance degrades rather gracefully under parameter mismatch. To illustrate this, Figure 2 shows the query performance with respect to varying θ assumed by the algorithm while the true θ is indicated by a cross."
    }, {
      "heading" : "5 Open problems",
      "text" : "There are some open questions concerning the necessary number of queries, that we would like to address in future work:\n• Can the exponential dependency on the dimension be avoided for D-dimensional data? Initial progress seems to indicate that for distances measured by p-norms with p <∞, the dependency on the dimension is only polynomial, while for the ∞-norm the dependency is exponential.\n• Is there an algorithm that — for 1-dimensional data — guarantees O\n( logn\nlog log k\n) queries for any\nset of data points? We already can show that for data points as chosen in Section 2.3, indeed only O (\nlogn log log k\n) queries are necessary.\n• Can our analysis — which assumes a uniform (or known) prior distribution on the possible targets and known parameters of the feedback model — be extended to an unknown prior distribution and unknown parameters of the feedback model?"
    }, {
      "heading" : "Acknowledgements",
      "text" : "The research leading to these results has received funding from the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement n◦ 231495 (CompLACS), n◦ 216886 (PASCAL2), n◦ 216529, Personal Information Navigator Adapting Through Viewing, PinView, and the Austrian Federal Ministry of Science and Research."
    } ],
    "references" : [ {
      "title" : "Exploration-exploitation trade-offs with delayed feedback",
      "author" : [ "Auer et al", "P. 2011] Auer", "D. Glowacka", "A. Leung", "S.H. Lim", "A. Medlar", "J. Shawe-Taylor" ],
      "venue" : "PinView Deliverable D4.3,",
      "citeRegEx" : "al. et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2011
    }, {
      "title" : "Relevance feedback models for contentbased image retrieval. Multimedia Analysis, Processing and Communications",
      "author" : [ "Auer", "Leung", "P. 2009] Auer", "A. Leung" ],
      "venue" : null,
      "citeRegEx" : "Auer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2009
    }, {
      "title" : "The bayesian learner is optimal for noisy binary search (and pretty good for quantum as well)",
      "author" : [ "Ben-Or", "Hassidim", "M. 2008] Ben-Or", "A. Hassidim" ],
      "venue" : "In Proceedings of the 2008 49th Annual IEEE Symposium on Foundations of Computer Sci-",
      "citeRegEx" : "Ben.Or et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ben.Or et al\\.",
      "year" : 2008
    }, {
      "title" : "The Bayesian image retrieval system, PicHunter: theory, implementation, and psychophysical experiments",
      "author" : [ "Cox et al", "I. 2000] Cox", "M. Miller", "T. Minka", "T. Papathomas", "P. Yianilos" ],
      "venue" : "IEEE Trans. Image Processing,",
      "citeRegEx" : "al. et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2000
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "We present theoretical results in terms of lower and upper bounds on the query complexity of noisy search with comparative feedback. In this search model, the noise in the feedback depends on the distance between query points and the search target. Consequently, the error probability in the feedback is not fixed but varies for the queries posed by the search algorithm. Our results show that a target out of n items can be found in O(log n) queries. We also show the surprising result that for k possible answers per query, the speedup is not log k (as for k-ary search) but only log log k in some cases.",
    "creator" : "TeX"
  }
}