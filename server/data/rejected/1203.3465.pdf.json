{
  "name" : "1203.3465.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Compiling Possibilistic Networks : Alternative Approaches to Possibilistic Inference",
    "authors" : [ "Raouia Ayachi", "Nahla Ben Amor" ],
    "emails" : [ "raouia.ayachi@gmail.com", "nahla.benamor@gmx.fr", "benferhat@cril.univ-artois.fr", "rolf.haenni@bfh.ch" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Qualitative possibilistic networks, also known as min-based possibilistic networks, are important tools for handling uncertain information in the possibility theory framework. Despite their importance, only the junction tree adaptation has been proposed for exact reasoning with such networks. This paper explores alternative algorithms using compilation techniques. We first propose possibilistic adaptations of standard compilation-based probabilistic methods. Then, we develop a new, purely possibilistic, method based on the transformation of the initial network into a possibilistic base. A comparative study shows that this latter performs better than the possibilistic adaptations of probabilistic methods. This result is also confirmed by experimental results."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "In possibility theory there are two different ways to define the counterpart of Bayesian networks. This is due to the existence of two definitions of possibilistic conditioning: product-based and min-based conditioning (Dubois and Prade, 1988). When we use the product form of conditioning, we get a possibilistic network close to the probabilistic one sharing the same features and having the same theoretical and practical results. However, this is not the case with min-based networks. In this paper, we are interested in the inference problem in multiply connected networks, which is known as a hard problem (Cooper, 1990). More precisely, we propose three compilation methods for min-based possibilistic networks.\nThe compilation of Bayesian networks is always considered as an important area. Recently, researchers\nhave been interested in various kinds of exact and approximate Bayesian networks inference algorithms using compilation techniques (Darwiche, 2003) (Chavira and Darwiche, 2005) (Wachter and Haenni, 2007), etc.\nDespite the importance of possibility theory, there is no compilation that has been proposed for possibilistic networks. This paper analyzes this issue by first adapting well-known compilation-based probabilistic inference approaches, namely the arithmetic circuit method (Darwiche, 2003) and the logical compilation of Bayesian Networks (Wachter and Haenni, 2007). Both of them are based on a network’s encoding into a logical representation and a compilation into a target compilation language, namely Π-DNNF. From there, all possible queries are answered in polynomial time. The third method exploits results obtained on one hand in (Benferhat et al., 2002) that transforms a minbased possibilistic network into a possibilistic knowledge base, and on the other hand results obtained regarding compilation of possibilistic bases (Benferhat et al., 2007) in order to assure inference in polytime. This method that is purely possibilistic is flexible since it permits to exploit efficiently all the existing propositional compilers.\nThe rest of this paper is organized as follows: Section 2 gives a briefly background on possibility theory, possibilistic logic, possibilistic networks and introduces some compilation concepts. Section 3 is dedicated to possibilistic adaptations of compilation-based probabilistic inference methods. Section 4 presents a new inference method in possibilistic networks using compiled possibilistic knowledge bases. Experimental study is presented in Section 5."
    }, {
      "heading" : "2 BASIC CONCEPTS",
      "text" : ""
    }, {
      "heading" : "2.1 POSSIBILITY THEORY",
      "text" : "This subsection briefly recalls some elements of possibility theory, for more details we refer to (Dubois and Prade, 1988). Let V = {X1, X2, ..., XN} be a set of\nvariables. We denote by DXi = {x1, .., xn} the domain associated with the variable Xi. By xi we denote any instance of Xi. Ω denotes the universe of discourse, which is the Cartesian product of all variable domains in V . Each element ω ∈ Ω is called a state of Ω. The notion of possibility distribution denoted by π is a mapping from the universe of discourse to the unit interval [0, 1]. To this scale, two interpretations can be attributed, a quantitative one when values have a real sense and a qualitative one when values reflect only an order between the different states of the world. This paper focuses on the qualitative interpretation of possibility theory.\nGiven a possibility distribution π, we can define a mapping grading the possibility measure of an event φ ⊆ Ω by Π(φ) = maxω∈φπ(ω). Π has a dual measure which is the necessity measure N(φ) = 1−Π(¬φ).\nConditioning consists in modifying our initial knowledge, encoded by a possibility distribution π, by the arrival of a new certain piece of information φ ⊆ Ω. The qualitative interpretation of the scale [0, 1] leads to the well known definition of min-conditioning (Hisdal, 1978), (Dubois and Prade, 1988):\nΠ(ψ | φ) = {\nΠ(ψ ∧ φ) if Π(ψ ∧ φ) < Π(φ) 1 otherwise (1)"
    }, {
      "heading" : "2.2 POSSIBILISTIC LOGIC",
      "text" : "Possibilistic logic (Dubois et al., 1994) handles qualitative uncertainty in a logical setting. A possibilistic logic formula is a pair (p, a) where p is a propositional formula and a its uncertainty degree which estimates to what extent it is certain that p is true. The higher is the weight, the more certain is the formula. A possibilistic knowledge base Σ is made up of a finite set of weighted formulas, i.e.,\nΣ = {(pi, ai), i = 1, .., n} (2)\nwhere ai is the lower bound on N(pi).\nEach possibilistic knowledge base induces a unique possibility distribution such that ∀ ω ∈ Ω and ∀ (pi, ai) ∈ Σ:\nπΣ(ω) = {\n1 if ω |= pi 1−max {ai : ω 2 pi} otherwise\n(3)\nwhere |= is propositional logic entailment."
    }, {
      "heading" : "2.3 POSSIBILISTIC NETWORKS",
      "text" : "A min-based possibilistic network over a set of variables V , denoted by ΠGmin is composed of: - a graphical component that is a DAG (Directed\nAcyclic Graph) where nodes represent variables and edges encode the links between the variables. The parent set of a node Xi is denoted by Ui = {Ui1, Ui2, ..., Uim}. For any ui of Ui we have ui = {ui1, ui2, ..., uim} where m is the number of parents of Xi. In what follows, we use xi, ui, uij to denote, respectively, possible instances of Xi, Ui and Uij . - a numerical component that quantifies different links. For every root node Xi (Ui = ∅), uncertainty is represented by the a priori possibility degree Π(xi) of each instance xi ∈ DXi , such that maxxiΠ(xi) = 1. For the rest of the nodes (Ui 6= ∅) uncertainty is represented by the conditional possibility degree Π(xi|ui) of each instances xi ∈ DXi and ui ∈ DUi . These conditional distributions satisfy the following normalization condition: maxxiΠ(xi|ui) = 1, for any ui.\nThe set of a priori and conditional possibility degrees in a min-based possibilistic network induce a unique joint possibility distribution defined by the following chain rule:\nπmin(X1, .., XN ) = min i=1..N Π(Xi | Ui) (4)"
    }, {
      "heading" : "2.4 COMPILATION CONCEPTS",
      "text" : "A target compilation language is a class of formulas which is tractable for a set of transformations and queries. Compilation languages are compared in terms of their spatial efficiency via the succinctness criteria and also in terms of the set of logical queries and transformations they support in polynomial time (see (Darwiche and Marquis, 2002) for more details).\nWithin the most effective target compilation languages, we cite the Decomposable Negation Normal Form (DNNF) (Darwiche, 2001). This language is universal and presents a number of properties (determinism, smoothness, etc.) that makes it of a great interest. It supports a rich set of polynomial-time logical operations. To define DNNF, the starting point is Negation Normal Form (NNF) which is a set of propositional formulas where possible connectives are conjunctions, disjunctions and negations. A set of important properties may be imposed to NNF, such that: - Decomposability : the conjuncts of any conjunction in NNF do not share variables. - Determinism: two disjuncts of any disjunction in NNF are logically contradictory. - Smoothness: the disjunct of any disjunction in NNF mentions the same variables.\nThese properties lead to a number of interesting subsets of NNF. Within these subsets, the language DNNF (Darwiche, 2001) is one of the most effective target compilation languages that supports the decomposability. We can also mention, the d-DNNF sat-\nisfying determinism, sd-DNNF satisfying smoothness and determinism, etc. Each compilation language supports some queries and transformations in polynomial time. In what follows we are in particular interested by conditioning and forgetting transformations (Darwiche and Marquis, 2002)."
    }, {
      "heading" : "3 POSSIBILISTIC ADAPTATIONS",
      "text" : "OF COMPILATION-BASED PROBABILISTIC INFERENCE METHODS\nThere are several compilation methods which handle the inference problem in probabilistic graphical models. In this section, we first propose an adaptation of the arithmetic circuit method of (Darwiche, 2003). Then we will study one of its variants proposed in (Wachter and Haenni, 2007), namely the logical compilation of Bayesian Networks.\nDNNF has been introduced for propositional language. Recall that in qualitative possibility theory, we basically manipulate two main operators Max and Min. These operators fully make sense when we deal with qualitative plausibility ordering. Therefore, we propose to define concepts of Π-DNNF (resp. Π-d-DNNF, Π-sd-DNNF) as adaptations of the DNNF language (resp. d-DNNF, sd-DNNF) (Darwiche, 2001) in the possibilistic setting (definition 1).\nDefinition 1. A sentence in Π-DNNF is a rooted DAG where each leaf node is labeled with true, false or variable’s instances and each internal node is labeled with max or min operators and can have arbitrarily several children. Roughly speaking, Π-DNNF is the same as the classical DNNF although its operators are max and min instead of ∨ and ∧, respectively. Example 1. Figure 1 depicts a sentence in Π-DNNF. Consider the Min-node (root) in this figure. This node has two children, the first contains variables A, B while the second contains variables C, D. This node is decomposable since its two children do not share variables.\nA sentence in Π-d-DNNF is a sentence in Π-DNNF satisfying decomposability and determinism (viewing\n∨ and ∧ as max and min operators, respectively). A sentence in Π-sd-DNNF is a sentence in Π-DNNF satisfying decomposability, determinism and smoothness."
    }, {
      "heading" : "3.1 INFERENCE USING POSSIBILISTIC CIRCUITS",
      "text" : "In (Darwiche, 2003), authors have focused on inference in compiled Bayesian networks. The main idea is based on representing the network using a polynomial and then retrieving answers to probabilistic queries by evaluating and differentiating the polynomial. This latter itself is exponential in size, so it has been represented efficiently using an arithmetic circuit that can be evaluated and differentiated in time and space linear in the circuit size. In what follows, we propose a direct adaptation of this method in the possibilistic setting. Given a min-based possibilistic network, we first encode it using a possibilistic function fmin defined by two types of variables:\n• Evidence indicators: for each variable Xi in the network , we have a variable λxi for each instance xi ∈ DXi .\n• Network parameters: for each variable Xi and its parents Ui in the network, we have a variable θxi|ui for each instance xi ∈ DXi and ui ∈ DUi .\nfmin = max x min (xi,ui)∼x λxiθxi|ui (5)\nwhere x represents instantiations of all network variables and ui ∼ x denotes the compatibility relationship among ui and x. The possibilistic function fmin of a possibilistic network represents the possibility distribution and allows to compute possibility degrees of variables of interest. Namely, for any piece of evidence e which is an instantiation of some variables E in the network, we can instantiate fmin as it returns the possibility of e, Π(e) (Definition 2 and Proposition 1).\nDefinition 2. The value of the possibilistic function fmin at evidence e, denoted by fmin(e), is the result of replacing each evidence indicator λxi in fmin with 1 if xi is consistent with e, and with 0 otherwise.\nProposition 1. Let ΠGmin be a possibilistic network representing the possibility distribution π and having the possibilistic function fmin. For any evidence e, we have fmin(e) = π(e).\nLet figure 2 be the min-based possibilistic network used throughout the paper.\nThe possibilistic function of the network in figure 2 has 8 terms corresponding to the 8 instantiations of variables F,B,D. Two of these terms are as follows:\nfmin = max(min(λd1 , λf1 , λb1 , θd1|f1,b1 , θf1 , θb1); min (λd1 , λf2 , λb1 , θd1|f2,b1 , θf2 , θb1); · · · )\nIf the evidence e = (d1, b1) then fmin(d1, b1) is obtained by applying the following substitutions to fmin: λd1 = 1, λd2 = 0, λb1 = 1, λb2 = 0, λf1 = λf2 = 1. This leads to Π(e) = 0.7.\nThe possibilistic function fmin is then encoded on a propositional theory (CNF) using λxi and θxi|ui . For each network variable Xi, the encoding contains the following clauses:\nλxi ∨ λxj (6)\n¬λxi ∨ ¬λxj , i 6= j (7)\nMoreover, for each propositional variable θxi|ui , the encoding contains the clause:\nλxi ∧ λui1 ∧ . . . ∧ λuim ↔ θxi|ui (8)\nThe CNF encoding, denoted by Kfmin recovers the min-joint possibility distribution (proposition 2).\nProposition 2. The CNF encoding Kfmin of a possibilistic network encodes the joint distribution of given network.\nOnce the CNF encoding is accomplished, it is then compiled into a Π-DNNF, from which we extract the possibilistic circuit ζp (definition 3) that implements the encoded fmin.\nDefinition 3. A possibilistic circuit ζp encoded by a Π-DNNF sentence ξc is a DAG in which leaf nodes correspond to circuit inputs, internal nodes correspond to max and min operators, and the root corresponds to the circuit output.\nAs in the probabilistic case (Darwiche, 2003), this circuit can be used for linear-time inference. More precisely, computing the possibility degree of an event consists on evaluating ζp by setting each evidence indicator λx to 1 if the event is consistent with x, to 0 otherwise and applying operators in a bottom-up way. This possibility degree corresponds exactly to the one computed from the min-joint possibility distribution (proposition 3). This method referred to Π-DNNFPF\nis outlined by algorithm 1. Note that the suffix PF is added to signify that this method uses a possibilistic function (fmin) before ensuring the CNF encoding.\nAlgorithm 1: Inference using Π-DNNF (Π-DNNFPF )\nData: ΠGmin , instance of interest x, evidence e Result: Π(x|e) begin\nCompilation into Π-DNNF Encode ΠGmin into fmin using equation 5 EncodeCNF of ΠGmin into ξ using equations 6, 7, 8 Compile ξ into ξc ζp ← Possibilistic Circuit of ξc Inference Applying Operators on ζp Π(x, e) ← Root Value (ζp; (x,e)) Π(e) ← Root Value (ζp; e) if Π(x, e) ≺ Π(e) then Π(x|e) ← Π(x, e) else Π(x|e) ← 1 return Π(x|e)\nend\nProposition 3. Let ΠGmin be a possibilistic network. Let πmin be a joint distribution obtained by chain rule. Then for any a ∈ Da and e ∈ DE, we have Π(A = a|E = e) = Πmin(A = a|E = e) where Πmin(A = a|E = e) is obtained from πmin using equation 1 and Π(A = a|E = e) is obtained from algorithm 1.\nThe key point to observe here is that this approach can handle possibilistic circuits of manageable size as in the probabilistic case since some possibility values may have some specific values; for instance, whether they are equal to 0 or 1, and whether some possibilities are equal. In this case, we can say that the network exhibit some local structure. By exploiting it, the produced circuits can be smaller. In fact, the normalization constraint relative to the initial network will mean that we will have several values equal to 1. Thus the idea is to make an advantage from such a local structure which has a particular behavior with the max operator in order to construct more compact possibilistic circuits w.r.t. standard ones as stated by the following proposition: Proposition 4. Let Nbposs and Nbproba be the number of clauses in the possibilistic and probabilistic cases, respectively. Then Nbposs ≤ Nbproba.\nNote that for particular situations where probability values are 1 or 0, we have Nbposs = Nbproba, otherwise Nbposs ≺ Nbproba. Example 2. To illustrate algorithm 1 we will consider the min-based possibilistic network represented in figure 2. We are looking for Π(f2|d1) with f2 as instance of interest and d1 as evidence. First, we encode the network as a possibilistic function and encode it on CNF. This latter is then compiled into Π-DNNF from which a possibilistic circuit is extracted. The possibility degree Π(f2|d1) is computed using this circuit in polynomial time. For instance, Π(f2, d1) is computed using ζp by just replacing\nλf2 = λd1 = λb1 = λb2 = 1 and applying possibilistic operators in a bottom-up way as shown in figure 3. Hence, Π(f2|d1) = Π(f2, d1) = 0.4 since Π(f2, d1) = 0.4 ≺ 1."
    }, {
      "heading" : "3.2 INFERENCE USING POSSIBILISTIC COMPILED REPRESENTATIONS",
      "text" : "DNNF plays an interesting role in compiling propositional knowledge bases. It has been used to compile probabilistic networks. More precisely in (Wachter and Haenni, 2007), authors have been interested in performing a CNF logical encoding of the probability distribution induced by a bayesian network, then a compilation phase from CNF to d-DNNF. In this section, we propose to adapt this encoding in the possibilistic setting by taking into consideration the local structure aspect. This allows to reduce the number of additional variables comparing to the probabilistic encoding. Let ∆ be propositions linked to network’s variables and let θ be propositions linked to the possibility distribution entries (equal to 1). We start by looking at the possibility distribution encoding. The logical representation of a network variable Xi is defined by: ψXi =∧ ui ( ∧ θxi |ui∈ΩθXi |ui ( ui1∧· · ·∧uim∧θxi|ui → xi )) (9)\nBy taking the conjunction of all logical representations of variables, we obtain the network’s representation ψ as follows:\nψ = ∧ Xi∈∆ ψXi (10)\nThe CNF encoding, denoted by Kψ indeed recovers the min-joint possibility distribution (proposition 5). Proposition 5. Let πmin be the joint possibility distribution obtained using the chain rule with the minimum operator and Π be the possibility degree computed\nfrom a function fψ encoding the CNF. Then, we have πmin(xi, ..., xj) = Π(xi, ..., xj), i.e. fψ recovers the min-joint possibility distribution πmin.\nComparing theoretically the probabilistic and the possibilistic case allows us to deduce the following proposition: Proposition 6. The possibilistic encoding of a possibilistic network given by Kψ (equation 10) is more compact than the probabilistic encoding given in (Wachter and Haenni, 2007). In fact, the number of variables used in Kψ is less than the one used in (Wachter and Haenni, 2007). In particular for parameters, our approach uses one variable per different weight, while in the probabilistic encoding one variable per parameter. For each clause in Kψ there exists a clause of the same size in the probabilistic encoding. The converse is false.\nOnce the qualitative network is encoded by Kψ, it is compiled into a compilation language that supports the transformations conditioning and forgetting and the query possibilistic computation. This language is Π-DNNF (proposition 7). Therefore, the CNF encoding is first compiled, and the resulting Π-DNNF is then used to compute efficiently, i.e. in polynomial time a-posteriori possibility degrees (proposition 8). This method referred to Π-DNNF is outlined by algo. 2. Proposition 7. Π-DNNF supports conditioning, forgetting and possibilistic computation.\nAlgorithm 2: Inference using Π-DNNF\nData: ΠGmin , instance of interest x, evidence e Result: Π(x|e) begin\nCompilation into Π-DNNF EncodeCNF of ΠGmin into ψ using equation 10 Compile ψ into ψcp Inference v1 ← Explore Π-DNNF(x ∧ e, ψcp) v2 ← Explore Π-DNNF(e, ψcp) if v1 ≺ v2 then Π(x|e) ← v1 else Π(x|e) ← 1 return Π(x|e)\nend\nProposition 8. Let ΠGmin be a possibilistic network. Let πmin be a joint distribution obtained by chain rule. Then for any a ∈ Da and e ∈ DE, we have Π(A = a|E = e) = Πmin(A = a|E = e) where Πmin(A = a|E = e) is obtained from πmin using equation 1 and Π(A = a|E = e) is obtained from algorithm 2. Example 3. Let us illustrate algorithm 2. In fact, ψ of the network of figure 2 is : ψ = ψF ∧ ψB ∧ ψD = {(θ1 ∨ f2) ∧ (θ2 ∨ b1) ∧ (f2 ∨ b2 ∨ θ2 ∨ d1) ∧ (f2 ∨ b1 ∨ θ1 ∨ d2) ∧ (f1 ∨ b2 ∨ θ3 ∨ d2) ∧ (f1 ∨ b1 ∨ θ4 ∨ d2)} such as θ1, θ2, θ3 and θ4 correspond respectively to 0.8, 0.7, 0.4 and 0.2.\nTo compute Π(f2|d1), we should first compute Π(f2, d1) using algorithm 3. The first step is to check if we have at least\nAlgorithm 3: Explore Π-DNNF\nData: a set of instances x, compiled representation ψcp\nResult: Π(x)\nbegin if ∀ xi ∈ x, θxi|Ui is not a leaf node then\nΠ(x) ← 1 else\ny= {xi | ∀, θxi|Ui is a leaf node ∀ Ui ⊆ x} ψcp|y ← Condition ψcp on y ψcp↓|y ← Forget ∆ from ψcp|y Applying Operators on ψcp↓|y Π(x) ← Root Value of ψcp↓|y\nreturn Π(x)\nend\none θ as a leaf node. In this example, we have θd1|f2,b1 and θd1|f2,b2 as leaf nodes, hence conditioning should be performed. Then, a computation step is required by applying in a bottom-up way Min and Max operators on the forgotten Π-DNNF. Therefore, Π(f2|d1) = Π(f2, d1) = 0.4."
    }, {
      "heading" : "4 NEW POSSIBILISTIC INFERENCE ALGORITHM",
      "text" : "In (Benferhat et al., 2002), authors have been interested in the transition of possibilistic networks into possibilistic logic bases. The starting point is that the possibilistic base associated to a possibilistic network is the result of the fusion of elementary bases. Definition 4 presents the transformation of a min-based possibilistic network into a possibilistic knowledge base.\nDefinition 4. A binary variable Xi of a possibilistic network can be expressed by a local possibilistic knowledge base as follows: ΣXi = {(¬xi ∨ ¬ui, αi) : αi = 1− π(xi|ui) 6= 0}. The possibilistic knowledge base of the whole network is: Σmin = ΣX1 ∪ ΣX2 ∪ · · · ∪ ΣXn .\nIn another angle, researchers in (Benferhat et al., 2007) have focused on the compilation of bases under the possibilistic logic policy in order to be able to process inference from it in a polynomial time. The combination of these methods allows us to propose a new alternative approach to possibilistic inference. This is justified by the fact that the possibilistic logic reasoning machinery can be applied to directed possibilistic networks (Benferhat et al., 2002).\nThe idea is to encode the possibilistic knowledge base Σmin into a classical propositional base (CNF). Let A = {a1, ..., an} with a1 ... an the different weights used in Σmin. A set of additional propositional variables, denoted by Ai, which correspond exactly to the number of different weights, are incorporated and for each formula φi, ai will correspond the propositional formula φi∨Ai. Hence, the propositional\nencoding of Σmin, denoted by KΣ is defined by:\nKΣ = {φi ∨Ai : (φi, ai) ∈ Σmin} (11)\nThe following proposition shows that the CNF encoding KΣ recovers the min-joint possibility distribution. Proposition 9. Let πmin be the joint possibility distribution obtained using the chain rule with the minimum-based conditioning and let KΣ be the propositional base associated with the possibilistic network given by equation 11. Let φi be a propositional formula associated with a degree ai. Then ∀ω ∈ Ω, Π(ω) = 1 iff {¬A1, ...,¬An} ∧ ω ∧ KΣ is consistent. Π(ω) = ai iff {¬A1, ...,¬Ai} ∧ ω ∧KΣ is inconsistent and {¬A1, ...,¬Ai−1} ∧ ω ∧KΣ is consistent.\nThe CNF encoding KΣ is then compiled into a target compilation language in order to compute a-posteriori possibility degrees in an efficient way. Here, we are interested in a particular query useful for possibilistic networks, namely what is the possibility degree of an event A = a given an evidence E = e? Therefore, we propose to adapt the algorithm given in (Benferhat et al., 2007) in order to respond to this query as shown by algorithm 4. Proposition 10 shows that the possibility degree computed using algorithm 4 and the one computed using the min-based joint possibility distribution are equal. Note that this approach is qualified to be flexible since it takes advantage of existing propositional knowledge bases compilation methods (Benferhat et al., 2007). This method referred to DNNF-PKB is outlined by algorithm 4.\nAlgorithm 4: Inference using DNNF\nData: ΠGmin , instance of interest x, evidence e Result: Π(x|e) begin\nTransformation into KΣ Transform ΠGmin into Σmin using definition 4 Transform Σmin into KΣ using equation 11 Inference KcΣ ← Target(KΣ) K ← KcΣ StopCompute ← false i ← 1 Π(x|e) ← 1 while (K 2 Ai ∨ ¬e) and (i ≤ k) and (StopCompute=false) do\nK ← condition (K, ¬Ai) if K ¬x then\nStopCompute← true Π(x|e) ← 1-degree(i)\nelse i← i+ 1\nreturn Π(x|e) end\nProposition 10. Let ΠGmin be a possibilistic network. Let πmin be a joint distribution obtained by\nchain rule. Then for any a ∈ Da and e ∈ DE, we have Π(A = a|E = e) = Πmin(A = a|E = e) where Πmin(A = a|E = e) is obtained from πmin using equation 1 and Π(A = a|E = e) is obtained from Algo. 4.\nExample 4. To illustrate algorithm 4 we will consider the min-based possibilistic network represented in figure 2."
    }, {
      "heading" : "The CNF encoding is as follows : KΣ =",
      "text" : "(d2 ∨ f1 ∨ b2 ∨A1) , (b1 ∨A2) , (d1 ∨ f2 ∨ b2 ∨A2) , (f2 ∨A3) , (d2 ∨ f2 ∨ b1 ∨A3) , (d2 ∨ f1 ∨ b1 ∨A4) such as A1 (0.8), A2 (0.6), A3(0.3) and A4 (0.2) are propositional variables followed by their weights under brackets. Compiling KΣ into DNNF results in: K c Σ = ((b2 ∧A2) ∧ [(A3 ∧ f1) ∨ (f2 ∧ [d2 ∨ (A4 ∧ d1)])]) ∨ (b1 ∧ [[f2 ∧ (d2 ∨ (A1 ∨ d1))] ∨ [(f1 ∧A3) ∧ (d1 ∨ (A2 ∧ d2))]]). The computation of Π(f2|d1) using KcΣ requires two iterations. Therefore, Π(f2|d1) = 1− degree(2) = 0.4.\nDue to the compilation step, this algorithm runs in polynomial time. Moreover, the number of additional variables is low since it corresponds exactly to the number of priority levels existing in the base."
    }, {
      "heading" : "5 COMPARATIVE AND EXPERIMENTAL STUDIES",
      "text" : "The paper analyzes three compilation-based methods, namely DNNF-PKB, Π-DNNF and Π-DNNFPF . The first dimension that differentiates the three approaches proposed in this paper is the CNF encoding. It consists of specifying the number of variables and clauses per approach.\nThe CNF of DNNF-PKB is based on encoding ¬x where x is an instance of interest having a possibility degree different from 1. In Π-DNNF, we write implications relative to instances having 1 as possibility degree. We can notice that the local structure in both methods is exploited in semantically different ways. In DNNF-PKB, the encoding uses the number of different weights as the number of additional variables while the Π-DNNF encoding uses the number of the non-redundant possibility degrees different from 1 in the distributions. Regarding the number of clauses, both methods handle possibility degrees different from 1. This leads us to the following proposition: Proposition 11. The CNF encodings of DNNF-PKB and Π-DNNF have the same number of variables and clauses.\nThe CNF encoding of Π-DNNFPF is different from the ones of DNNF-PKB and Π-DNNF. Proposition 12 shows the difference between Π-DNNFPF and DNNFPKB in terms of number of variables and clauses. Proposition 12. The number of variables and clauses in Π-DNNFPF is more important than those in DNNF-PKB.\nIndeed, in Π-DNNFPF , we associate propositional variables not only to possibility degrees (parameters), but also to each value xi of Xi. While in DNNF-PKB only m new variables are added (one variable per different degree).\nLet us now analyze these three approaches from experimental points of view. Our experimentation is performed on random possibilistic networks. More precisely, we have compared DNNF-PKB and ΠDNNFPF on 100 possibilistic networks having from 10 to 50 nodes. As mentioned that the approaches focus mainly on encoding the possibilistic network as a CNF then compile it into the appropriate language, hence, it should be interesting to compare the CNF parameters (the number of variables and clauses) and the DNNF parameters (the number of nodes and edges) for the two methods."
    }, {
      "heading" : "5.1 CNF PARAMETERS",
      "text" : "First we propose to test the CNF encodings characterized by the number of variables and the number of clauses. Regarding DNNF-PKB, the number of additional variables correspond to the number of weights which are different. While in Π-DNNFPF , variables are both those associated to the possibility degrees of each distribution and those to variable’s instances. The number of clauses for each method is related to the CNF encoding itself. Figure 4 shows the results of this experimentation. Each approach is characterized by a curve for the average number of variables and a curve for the average number of clauses. It is clear that the higher the number of nodes considered in the possibilistic network, the higher the number of variables and clauses. Figure 4 shows that DNNF-PKB has the lower number of variables and clauses comparing to Π-DNNFPF , which confirms the theoretical results detailed above."
    }, {
      "heading" : "5.2 DNNF PARAMETERS",
      "text" : "Once we obtain the CNF encodings, it is important to compare the number of nodes and edges for each compiled base. Figure 5 represents the average size of the compiled bases for the two methods in terms of nodes and edges numbers. We remark that the number of nodes and edges depends deeply on CNF parameters. More precisely, the number of nodes and edges in DNNF-PKB is considered narrow comparing to Π-DNNFPF . This can be explained by the lower number of variables and clauses on CNFs and the local structure which shrinks the sizes of compiled bases. Comparing DNNF-PKB to Π-DNNFPF , the behavior of DNNF-PKB is important."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "This paper proposes algorithms that ensure inference in possibilistic networks using compilation techniques. First, we have proposed possibilistic adaptations of two compilation-based probabilistic methods, namely Π-DNNFPF and Π-DNNF. Then we have developed a new possibilistic inference method DNNF-PKB based on a transformation phase from a possibilistic network into a compiled possibilistic knowledge base. We theoretically show that DNNF-PKB and Π-DNNF share the same number of variables and clauses even they are based on different computations in their inference process since the first is based on necessity degrees and the second on possibility degrees. We have also shown that DNNF-PKB is more compact than Π-DNNFPF which proves the importance of the possibilistic setting versus the probabilistic setting. All these results were confirmed by experimental results. A future work will be to compare these algorithms with the well-known junction tree propagation algorithm. Another future work is to exploit results of this paper in order to infer efficiently interventions in possibilistic causal networks\n(Pearl, 2000) (Benferhat and Smaoui, 2007)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank the anonymous reviewers for many interesting comments and suggestions. Also, we wish to thank Mark Chavira for our valuable discussions on this subject. The third author would like to thank the project ANR Placid."
    }, {
      "heading" : "Benferhat, S., Dubois, D., Garcia, L., and Prade, H.",
      "text" : "(2002). On the transformation between possibilistic logic bases and possibilistic causal networks. International Journal of Approximate Reasoning, 29(2):135– 173."
    }, {
      "heading" : "Benferhat, S. and Smaoui, S. (2007). Possibilistic causal",
      "text" : "networks for handling interventions: A new propagation algorithm. In AAAI, pages 373–378.\nBenferhat, S., Yahi, S., and Drias, H. (2007). On the compilation of stratified belief bases under linear and possibilistic logic policies. In International Journal of Approximate Reasoning, pages 2425–2430."
    }, {
      "heading" : "Chavira, M. and Darwiche, A. (2005). Compiling bayesian",
      "text" : "networks with local structure. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), pages 1306–1312."
    }, {
      "heading" : "Cooper, G. F. (1990). The computational complexity of",
      "text" : "probabilistic inference using bayesian belief networks (research note). Artif. Intell., 42(2-3):393–405.\nDarwiche, A. (2001). Decomposable negation normal form. Journal of the ACM, 48(4):608–647."
    }, {
      "heading" : "Darwiche, A. (2003). A differential approach to inference",
      "text" : "in bayesian networks. Journal of the ACM, 50(3):280– 305.\nDarwiche, A. and Marquis, P. (2002). A knowledge compilation map. Journal of Artificial Intelligence Research, 17:229–264.\nDubois, D., Lang, J., and Prade, H. (1994). Possibilistic logic. In Handbook on Logic in Artificial Intelligence and Logic Programming, volume 3, pages 439– 513. Oxford University press.\nDubois, D. and Prade, H. (1988). Possibility theory:An approach to computerized, Processing of uncertainty. Plenium Press, New York.\nHisdal, E. (1978). Conditional possibilities independence and non interaction. Fuzzy Sets and Systems, 1.\nPearl, J. (2000). Causality: Models, reasonning and inference. Cambridge University Press."
    }, {
      "heading" : "Wachter, M. and Haenni, R. (2007). Logical compilation",
      "text" : "of bayesian networks with discrete variables. In European Conf. on Symbolic and Quantitative Approaches to Reasoning with Uncertainty, pages 536–547."
    } ],
    "references" : [ {
      "title" : "On the transformation between possibilistic logic bases and possibilistic causal networks",
      "author" : [ "S. Benferhat", "D. Dubois", "L. Garcia", "H. Prade" ],
      "venue" : "International Journal of Approximate Reasoning, 29(2):135– 173.",
      "citeRegEx" : "Benferhat et al\\.,? 2002",
      "shortCiteRegEx" : "Benferhat et al\\.",
      "year" : 2002
    }, {
      "title" : "Possibilistic causal networks for handling interventions: A new propagation algorithm",
      "author" : [ "S. Benferhat", "S. Smaoui" ],
      "venue" : "AAAI, pages 373–378.",
      "citeRegEx" : "Benferhat and Smaoui,? 2007",
      "shortCiteRegEx" : "Benferhat and Smaoui",
      "year" : 2007
    }, {
      "title" : "On the compilation of stratified belief bases under linear and possibilistic logic policies",
      "author" : [ "S. Benferhat", "S. Yahi", "H. Drias" ],
      "venue" : "International Journal of Approximate Reasoning, pages 2425–2430.",
      "citeRegEx" : "Benferhat et al\\.,? 2007",
      "shortCiteRegEx" : "Benferhat et al\\.",
      "year" : 2007
    }, {
      "title" : "Compiling bayesian networks with local structure",
      "author" : [ "M. Chavira", "A. Darwiche" ],
      "venue" : "Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), pages 1306–1312.",
      "citeRegEx" : "Chavira and Darwiche,? 2005",
      "shortCiteRegEx" : "Chavira and Darwiche",
      "year" : 2005
    }, {
      "title" : "The computational complexity of probabilistic inference using bayesian belief networks (research note)",
      "author" : [ "G.F. Cooper" ],
      "venue" : "Artif. Intell., 42(2-3):393–405.",
      "citeRegEx" : "Cooper,? 1990",
      "shortCiteRegEx" : "Cooper",
      "year" : 1990
    }, {
      "title" : "Decomposable negation normal form",
      "author" : [ "A. Darwiche" ],
      "venue" : "Journal of the ACM, 48(4):608–647.",
      "citeRegEx" : "Darwiche,? 2001",
      "shortCiteRegEx" : "Darwiche",
      "year" : 2001
    }, {
      "title" : "A differential approach to inference in bayesian networks",
      "author" : [ "A. Darwiche" ],
      "venue" : "Journal of the ACM, 50(3):280– 305.",
      "citeRegEx" : "Darwiche,? 2003",
      "shortCiteRegEx" : "Darwiche",
      "year" : 2003
    }, {
      "title" : "A knowledge compilation map",
      "author" : [ "A. Darwiche", "P. Marquis" ],
      "venue" : "Journal of Artificial Intelligence Research, 17:229–264.",
      "citeRegEx" : "Darwiche and Marquis,? 2002",
      "shortCiteRegEx" : "Darwiche and Marquis",
      "year" : 2002
    }, {
      "title" : "Possibilistic logic",
      "author" : [ "D. Dubois", "J. Lang", "H. Prade" ],
      "venue" : "Handbook on Logic in Artificial Intelligence and Logic Programming, volume 3, pages 439– 513. Oxford University press.",
      "citeRegEx" : "Dubois et al\\.,? 1994",
      "shortCiteRegEx" : "Dubois et al\\.",
      "year" : 1994
    }, {
      "title" : "Possibility theory:An approach to computerized, Processing of uncertainty",
      "author" : [ "D. Dubois", "H. Prade" ],
      "venue" : "Plenium Press, New York.",
      "citeRegEx" : "Dubois and Prade,? 1988",
      "shortCiteRegEx" : "Dubois and Prade",
      "year" : 1988
    }, {
      "title" : "Conditional possibilities independence and non interaction",
      "author" : [ "E. Hisdal" ],
      "venue" : "Fuzzy Sets and Systems, 1.",
      "citeRegEx" : "Hisdal,? 1978",
      "shortCiteRegEx" : "Hisdal",
      "year" : 1978
    }, {
      "title" : "Causality: Models, reasonning and inference",
      "author" : [ "J. Pearl" ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Pearl,? 2000",
      "shortCiteRegEx" : "Pearl",
      "year" : 2000
    }, {
      "title" : "Logical compilation of bayesian networks with discrete variables",
      "author" : [ "M. Wachter", "R. Haenni" ],
      "venue" : "European Conf. on Symbolic and Quantitative Approaches to Reasoning with Uncertainty, pages 536–547.",
      "citeRegEx" : "Wachter and Haenni,? 2007",
      "shortCiteRegEx" : "Wachter and Haenni",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "This is due to the existence of two definitions of possibilistic conditioning: product-based and min-based conditioning (Dubois and Prade, 1988).",
      "startOffset" : 120,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "In this paper, we are interested in the inference problem in multiply connected networks, which is known as a hard problem (Cooper, 1990).",
      "startOffset" : 123,
      "endOffset" : 137
    }, {
      "referenceID" : 6,
      "context" : "Recently, researchers have been interested in various kinds of exact and approximate Bayesian networks inference algorithms using compilation techniques (Darwiche, 2003) (Chavira and Darwiche, 2005) (Wachter and Haenni, 2007), etc.",
      "startOffset" : 153,
      "endOffset" : 169
    }, {
      "referenceID" : 3,
      "context" : "Recently, researchers have been interested in various kinds of exact and approximate Bayesian networks inference algorithms using compilation techniques (Darwiche, 2003) (Chavira and Darwiche, 2005) (Wachter and Haenni, 2007), etc.",
      "startOffset" : 170,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "Recently, researchers have been interested in various kinds of exact and approximate Bayesian networks inference algorithms using compilation techniques (Darwiche, 2003) (Chavira and Darwiche, 2005) (Wachter and Haenni, 2007), etc.",
      "startOffset" : 199,
      "endOffset" : 225
    }, {
      "referenceID" : 6,
      "context" : "This paper analyzes this issue by first adapting well-known compilation-based probabilistic inference approaches, namely the arithmetic circuit method (Darwiche, 2003) and the logical compilation of Bayesian Networks (Wachter and Haenni, 2007).",
      "startOffset" : 151,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "This paper analyzes this issue by first adapting well-known compilation-based probabilistic inference approaches, namely the arithmetic circuit method (Darwiche, 2003) and the logical compilation of Bayesian Networks (Wachter and Haenni, 2007).",
      "startOffset" : 217,
      "endOffset" : 243
    }, {
      "referenceID" : 0,
      "context" : "The third method exploits results obtained on one hand in (Benferhat et al., 2002) that transforms a minbased possibilistic network into a possibilistic knowledge base, and on the other hand results obtained regarding compilation of possibilistic bases (Benferhat et al.",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : ", 2002) that transforms a minbased possibilistic network into a possibilistic knowledge base, and on the other hand results obtained regarding compilation of possibilistic bases (Benferhat et al., 2007) in order to assure inference in polytime.",
      "startOffset" : 178,
      "endOffset" : 202
    }, {
      "referenceID" : 9,
      "context" : "This subsection briefly recalls some elements of possibility theory, for more details we refer to (Dubois and Prade, 1988).",
      "startOffset" : 98,
      "endOffset" : 122
    }, {
      "referenceID" : 10,
      "context" : "The qualitative interpretation of the scale [0, 1] leads to the well known definition of min-conditioning (Hisdal, 1978), (Dubois and Prade, 1988):",
      "startOffset" : 106,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "The qualitative interpretation of the scale [0, 1] leads to the well known definition of min-conditioning (Hisdal, 1978), (Dubois and Prade, 1988):",
      "startOffset" : 122,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "Possibilistic logic (Dubois et al., 1994) handles qualitative uncertainty in a logical setting.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 7,
      "context" : "Compilation languages are compared in terms of their spatial efficiency via the succinctness criteria and also in terms of the set of logical queries and transformations they support in polynomial time (see (Darwiche and Marquis, 2002) for more details).",
      "startOffset" : 207,
      "endOffset" : 235
    }, {
      "referenceID" : 5,
      "context" : "Within the most effective target compilation languages, we cite the Decomposable Negation Normal Form (DNNF) (Darwiche, 2001).",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 5,
      "context" : "Within these subsets, the language DNNF (Darwiche, 2001) is one of the most effective target compilation languages that supports the decomposability.",
      "startOffset" : 40,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : "In what follows we are in particular interested by conditioning and forgetting transformations (Darwiche and Marquis, 2002).",
      "startOffset" : 95,
      "endOffset" : 123
    }, {
      "referenceID" : 6,
      "context" : "In this section, we first propose an adaptation of the arithmetic circuit method of (Darwiche, 2003).",
      "startOffset" : 84,
      "endOffset" : 100
    }, {
      "referenceID" : 12,
      "context" : "Then we will study one of its variants proposed in (Wachter and Haenni, 2007), namely the logical compilation of Bayesian Networks.",
      "startOffset" : 51,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "d-DNNF, sd-DNNF) (Darwiche, 2001) in the possibilistic setting (definition 1).",
      "startOffset" : 17,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : "In (Darwiche, 2003), authors have focused on inference in compiled Bayesian networks.",
      "startOffset" : 3,
      "endOffset" : 19
    }, {
      "referenceID" : 6,
      "context" : "As in the probabilistic case (Darwiche, 2003), this circuit can be used for linear-time inference.",
      "startOffset" : 29,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "More precisely in (Wachter and Haenni, 2007), authors have been interested in performing a CNF logical encoding of the probability distribution induced by a bayesian network, then a compilation phase from CNF to d-DNNF.",
      "startOffset" : 18,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "The possibilistic encoding of a possibilistic network given by Kψ (equation 10) is more compact than the probabilistic encoding given in (Wachter and Haenni, 2007).",
      "startOffset" : 137,
      "endOffset" : 163
    }, {
      "referenceID" : 12,
      "context" : "In fact, the number of variables used in Kψ is less than the one used in (Wachter and Haenni, 2007).",
      "startOffset" : 73,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "In (Benferhat et al., 2002), authors have been interested in the transition of possibilistic networks into possibilistic logic bases.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 2,
      "context" : "In another angle, researchers in (Benferhat et al., 2007) have focused on the compilation of bases under the possibilistic logic policy in order to be able to process inference from it in a polynomial time.",
      "startOffset" : 33,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "This is justified by the fact that the possibilistic logic reasoning machinery can be applied to directed possibilistic networks (Benferhat et al., 2002).",
      "startOffset" : 129,
      "endOffset" : 153
    }, {
      "referenceID" : 2,
      "context" : "Here, we are interested in a particular query useful for possibilistic networks, namely what is the possibility degree of an event A = a given an evidence E = e? Therefore, we propose to adapt the algorithm given in (Benferhat et al., 2007) in order to respond to this query as shown by algorithm 4.",
      "startOffset" : 216,
      "endOffset" : 240
    }, {
      "referenceID" : 2,
      "context" : "Note that this approach is qualified to be flexible since it takes advantage of existing propositional knowledge bases compilation methods (Benferhat et al., 2007).",
      "startOffset" : 139,
      "endOffset" : 163
    }, {
      "referenceID" : 11,
      "context" : "Another future work is to exploit results of this paper in order to infer efficiently interventions in possibilistic causal networks (Pearl, 2000) (Benferhat and Smaoui, 2007).",
      "startOffset" : 133,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : "Another future work is to exploit results of this paper in order to infer efficiently interventions in possibilistic causal networks (Pearl, 2000) (Benferhat and Smaoui, 2007).",
      "startOffset" : 147,
      "endOffset" : 175
    } ],
    "year" : 2010,
    "abstractText" : "Qualitative possibilistic networks, also known as min-based possibilistic networks, are important tools for handling uncertain information in the possibility theory framework. Despite their importance, only the junction tree adaptation has been proposed for exact reasoning with such networks. This paper explores alternative algorithms using compilation techniques. We first propose possibilistic adaptations of standard compilation-based probabilistic methods. Then, we develop a new, purely possibilistic, method based on the transformation of the initial network into a possibilistic base. A comparative study shows that this latter performs better than the possibilistic adaptations of probabilistic methods. This result is also confirmed by experimental results.",
    "creator" : "TeX"
  }
}