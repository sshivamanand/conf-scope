{
  "name" : "1107.0046.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Explicit Learning Curves for Transduction and Application to Clustering and Compression Algorithms",
    "authors" : [ "Philip Derbeko", "Ran El-Yaniv", "Ron Meir" ],
    "emails" : [ "philip@cs.technion.ac.il", "rani@cs.technion.ac.il", "rmeir@ee.technion.ac.il" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The bulk of work in Statistical Learning Theory has dealt with the inductive approach to learning. Here one is given a finite set of labeled training examples, from which a rule is inferred. This rule is then used to label new examples. As pointed out by Vapnik (1998) in many realistic situations one actually faces an easier problem where one is given a training set of labeled examples, together with an unlabeled set of points which needs to be labeled. In this transductive setting, one is not interested in inferring a general rule, but rather only in labeling this unlabeled set as accurately as possible. One solution is of course to infer a rule as in the inductive setting, and then use it to label the required points. However, as argued by Vapnik (1982, 1998), it makes little sense to solve what appears to be an easier\nc©2004 AI Access Foundation. All rights reserved.\nproblem by ‘reducing’ it to a more difficult one. While there are currently no formal results stating that transduction is indeed easier than induction1 it is plausible that the relevant information carried by the test points can be incorporated into an algorithm, potentially leading to superior performance. Since in many practical situations we are interested in evaluating a function only at some points of interest, a major open problem in statistical learning theory is to determine precise relations between induction and transduction.\nIn this paper we present several general error bounds for transductive learning.2 We also present a general technique for establishing error bounds for transductive learning algorithms based on compression and clustering. Our bounds can be viewed as extensions of McAllester’s PAC-Bayesian framework (McAllester, 1999, 2003a, 2003b) to transductive learning. The main advantage of using the PAC-Bayesian approach in transduction, as opposed to induction, is that here prior beliefs on hypotheses can be formed based on the unlabeled test data. This flexibility allows for the choice of “compact priors” (with small support) and therefore, for tight bounds. We use the established bounds and provide tight error bounds for “compression schemes” such as (transductive) SVMs and transductive learning algorithms based on clustering. While precise relations between induction and transduction remain a major challenge, our new bounds and technique offer some new insights into transductive learning.\nThe problem of transduction was formulated as long ago as 1982 in Vapnik’s classic book (Vapnik, 1982), where the precise setting was formulated, and some implicit error bounds were derived.3 In recent years the problem has been receiving an increasing amount of attention, due to its applicability to many real world situations. A non-exhaustive list of recent contributions includes (Vapnik, 1982, 1998; Joachims, 1999; Bennett & Demiriz, 1998; Demiriz & Bennett, 2000; Wu, Bennett, Cristianini, & Shawe-Taylor, 1999; Lanckriet, Cristianini, Ghaoui, Bartlett, & Jordan, 2002), and (Blum & Langford, 2003). Most of this work, with the exception of Vapnik’s bounds (1982, 1998) and the results of Lanckriet et al. (2002), has dealt with algorithmic issues, rather than with performance bounds. Implicit performance bound, in the spirit of Vapnik (1982, 1998) has recently been presented by Blum and Langford (2003). We mention this result again later in Section 2.2.\nWe present explicit PAC-Bayesian bounds for a transductive setting that considers sampling without replacement of the training set from a given ‘full sample’ of unlabeled points. This setting is proposed by Vapnik and it turns out that error bounds for learning algorithms, within this setting, imply the same bounds within another setting which may appear more practical (See Section 2.1 and Theorem 2 for details). Sampling without replacement of the training set leads to the training points being dependent (see Section 2.1 for details). Our first goal is to provide uniform bounds on the deviation between the training error and the test error. To this end, we study two types of bounds that utilize two different bounding techniques. The first approach is based on an observation made in Hoeffding’s classic paper (Hoeffding, 1963). This approach was recently alluded to by Lugosi (2003). As\n1. There may be various ways to state this in a meaningful manner. Essentially, we would like to know if for some learning problems a particular transductive algorithm can achieve better performance than any inductive algorithm, where performance may be characterized by learning rates and/or computational complexity. 2. In the paper we use the terms ‘error bound’ and ‘risk bound’ interchangeably. Strictly speaking, the term ‘generalization bound’ is not appropriate in the transductive setting. 3. Vapnik refers to transduction also as “estimating the values of a function at given points of interest”.\npointed out by Hoeffding (1963), the sampling without replacement problem can be reduced to an equivalent problem involving independent samples, for which standard techniques suffice. We refer to this approach as ‘reduction to independence’. A second approach involves derivations of large deviation bounds for sampling without replacement such as those developed by Serfling (1974), Vapnik (1998), Hush and Scovel (2003), and Dembo and Zeitouni (1998). We refer to such bounds as ‘direct’. We consider these two approaches in Section 3.1 and 3.2, respectively. Using these two approaches we derive general PAC-Bayesian bounds for transduction. It turns out that the direct bounds lead to tighter and more explicit learning curves for transduction.\nWe then show how to utilize PAC-Bayesian transductive bounds to derive error bounds for specific learning schemes. In particular, we show how to choose priors, based on the given unlabeled data, and derive bounds for “compression schemes” and learning algorithms based on clustering. Compression schemes are algorithms that select the same hypothesis using only a subset of the training data. The main example of a compression scheme is a (transductive) Support Vector Machine (SVM). The compression level achieved by such schemes typically depends on the (geometric) complexity of the learning problem at hand, which sometimes does not allow for significant compression. A stronger type of compression can be often achieved using clustering. A natural approach in the context of transduction (and semi-supervised learning) is to apply a clustering algorithm over the set of all available (unlabeled) points and then to use the labeled points to determine the classifications of points in the resulting clusters. We formulate this scheme in the context of transduction and derive for it rather tight error bounds by utilizing an appropriate prior in our transductive PAC-Bayesian bounds. For practical applications a similar but tighter result is obtained by using the implicit bounds of Vapnik (1998) (or the bound of Blum & Langford, 2003)."
    }, {
      "heading" : "2. Problem Setup and Vapnik’s Basic Results",
      "text" : ""
    }, {
      "heading" : "2.1 Problem Setup",
      "text" : "The problem of transduction can be informally described as follows. A learner is given a set of labeled examples {(x1, y1), . . . , (xm, ym)}, xi ∈ X , yi ∈ Y, and a set of unlabeled points {xm+1, . . . , xm+u}. Based on this data, the objective is to label the unlabeled points. In order to formalize the scenario, we consider the setting proposed by Vapnik (1998, Chap. 8), which for simplicity is described in the context of binary classification.4 Let H be a set of binary hypotheses consisting of functions from the input space X to Y = {±1}. Let µ(x, y) be any distribution over X ×Y. For each h ∈ H and a set Z = x1, . . . , x|Z| of samples define\nRh(Z) △ = 1\n|Z|\n|Z| ∑\ni=1\nEµ(y|xi) {ℓ(h(xi), y)} = 1\n|Z|\n|Z| ∑\ni=1\n∫\ny∈Y ℓ(h(xi), y)dµ(y|xi), (1)\nwhere, unless otherwise specified, ℓ(·, ·) is the 0/1 loss function. Vapnik (1998) considers the following two transductive “protocols”.\n4. Note that the bound of Theorem 22 does hold for general bounded loss functions.\nSetting 1 :\n(i) A full sample Xm+u = {x1, . . . , xm+u} consisting of arbitrary m+ u points is given.5\n(ii) We then choose uniformly at random the training sample Xm ⊆ Xm+u and receive its labels Ym where the label y of x is chosen according to µ(y|x); the resulting training set is Sm = (Xm, Ym) and the remaining set Xu is the unlabeled (test) sample, Xu = Xm+u \\Xm.\n(iii) Using both Sm and Xu we select a classifier h ∈ H. The quality of h is measured by Rh(Xu).\nSetting 2 :\n(i) We are given a training set Sm = (Xm, Ym) selected i.i.d according to µ(x, y).\n(ii) An independent test set Su = (Xu, Yu) of u samples is then selected in the same manner.\n(iii) We are required to choose our best h ∈ H based on Sm and Xu so as to minimize\nRm,u(h) △ =\n∫ 1\nu\nm+u∑\ni=m+1\nℓ (h(xi), yi) dµ(x1, y1) · · · dµ(xm+u, ym+u). (2)\nRemark 1 Notice that the choice of the sub-sample Xm in Setting 1 is equivalent to sampling m points from Xm+u uniformly at random without replacement. This leads to the samples being dependent. Also, Setting 1 concerns an “individual sample” Xm+u and there are no assumptions regarding its underlying source. The only element of chance in this setting is the random choice of the training set from the full sample.\nSetting 2 may appear more applicable in some practical situations than Setting 1. However, derivation of theoretical results can be easier within Setting 1. The following useful theorem (Theorem 8.1 in Vapnik, 1998) relates the two transduction settings. For completeness we present Vapnik’s proof in the appendix.\nTheorem 2 (Vapnik) If for some learning algorithm choosing an hypothesis h it is proved within Setting 1 that with probability at least 1− δ, the deviation between the risks Rh(Xm) and Rh(Xu) does not depend on the composition of the full sample and does not exceed ε, then with the same probability, in Setting 2 the deviation between Rh(Xm) and the risk given by formula (2) does not exceed ε.\nRemark 3 The learning algorithm in Theorem 2 is implicitly assumed to be deterministic. The theorem can be extended straightforwardly to the case where the algorithm is randomized and chooses an hypothesis h ∈ H randomly, based on Sm ∪ Xu. A particular type of randomization is the one used by Gibbs algorithms, as discussed in Section 4.1.\n5. The original Setting 1, as proposed by Vapnik, discusses a full sample whose points are chosen independently at random according to some source distribution µ(x).\nRemark 4 The basic quantity of interest in Setting 2 is Rm,u(h) defined in (2). Assuming h is selected based on the sample Sm ∪Xu, one is often interested in its expectation over a random selection of this sample. In inductive learning, one considers the sample Sm and a single new point (x, y), and the average is taken with respect to Sm ∪ {X}. It should be noted that the random variable Rm,u(h) is ‘more concentrated’ around its mean than the random variable ℓ(h(x), y) corresponding to a single new point (x, y). Therefore, one may expect transduction to lead to tighter bounds in Setting 2 as well.\nIn view of Theorem 2 we restrict ourselves in the sequel to Setting 1. Also, for simplicity we focus on the case where there exists a deterministic target function φ : X → Y, so that y = φ(x) is a fixed target label for x; that is, µ(φ(x)|x) = 1 (there is no requirement that φ ∈ H). Note that it is possible to extend our results to the the general case of stochastic targets y ∼ µ(y|x).\nWe make use of the following quantities, which are all instances of (1). The quantity Rh(Xm+u) is called the full sample risk of the hypothesis h, Rh(Xu) is referred to as the transduction risk (of h), and Rh(Xm) is the training error (of h). Note that in the case where our target function φ is deterministic,\nRh(Xm) = 1\nm\nm∑\ni=1\nEµ(y|xi) {ℓ(h(xi), y)} = 1\nm\nm∑\ni=1\nℓ(h(xi), φ(xi)).\nThus, Rh(Xm) is the standard training error denoted interchangeably by R̂h(Sm). It is important to observe that while Rh(Xm+u) is not a random variable, both Rh(Xm) and Rh(Xu) are random variables, due to the random selection of the samples Xm from Xm+u.\nWhile our objective in transduction is to achieve small error over the unlabeled sample (i.e. to minimize Rh(Xu)), we find that it is sometimes easier to derive error bounds for the full sample risk. The following simple lemma translates an error bound on Rh(Xm+u), the full sample risk, to an error bound on the transduction risk Rh(Xu).\nLemma 5 For any h ∈ H and any C\nRh(Xm+u) ≤ R̂h(Sm) + C ⇔ Rh(Xu) ≤ R̂h(Sm) + m+ u\nu · C. (3)\nProof For any h\nRh(Xm+u) = mRh(Xm) + uRh(Xu)\nm+ u . (4)\nSubstituting R̂h(Sm) for Rh(Xm) in (4) and then substituting the result for the left-hand side of (3) we get\nRh(Xm+u) = mR̂h(Sm) + uRh(Xu)\nm+ u ≤ R̂h(Sm) + C.\nThe equivalence (3) is now obtained by isolating Rh(Xu) on the left-hand side.\nRemark 6 In applications of Lemma 5, the term C = C(m) is typically a function m (and of some of the other problem parameters such as δ). In meaningful bounds C(m) → 0 with m → ∞. Observe that in order for the bound on Rh(Xu) to converge it must be that u = ω(mC(m)).\nConsider a hypothesis class H. In the context of transduction, we are only interested in labeling the test set Xu, which is given in advance. Thus, we may in principle regard hypotheses in H which label Xm+u identically as belonging to the same equivalence class. Since for fixed values of m and u the number of equivalence classes is finite (in the case of binary hypotheses, it is at most 2m+u) we may, without loss of generality, restrict ourselves to a finite hypothesis class (see, for example Vapnik, 1998, Sec. 8.5). Note that this freedom is not available in the inductive setting, where the test set is not known in advance.\nRemark 7 The above procedure is clearly not possible in the case of real-valued loss functions. However, in this case one can still use the availability of the full data set Xm+u in order to construct an empirical ǫ-cover of H based on the empirical ℓ1 norm, namely ℓ1(f, g) = (m + u) −1 ∑m+u\ni=1 |f(xi) − g(xi)| for any f, g ∈ H. We restrict ourselves here to the binary case here."
    }, {
      "heading" : "2.2 Vapnik’s Implicit Bounds",
      "text" : "Fix some hypothesis h ∈ H and suppose that h makes kh errors on the full sample (i.e. kh = (m + u)Rh(Xm+u)). Consider a random choice of the training set Sm from the full sample, and let B(r, kh,m, u) be the probability that h makes exactly r errors over the training set Sm. This probability is by definition the hypergeometric distribution, given by\nB(r, kh,m, u) △ =\n( kh r )( m+u−kh m−r )\n( m+u m\n) .\nSince m and u are fixed, throughout this discussion we abbreviate B(r, kh) △\n= B(r, kh,m, u). Define\nC(ε, kh) △ = Pr {Rh(Xu)−Rh(Xm) > ε}\n= Pr\n{ kh − r\nu − r m > ε\n}\n= ∑\nr\nB(r, kh),\nwhere the summation is over all values of r such that max{kh−u, 0} ≤ r ≤ min{m,kh} and\nkh − r u − r m > ε. (5)\nDefine\nΓ(ε) △\n= max k C\n(√\nk\nm+ u · ε, k\n)\n. (6)\nWe now state Vapnik’s implicit bound for transduction. The bound is slightly adapted to incorporate a prior probability over H (the original bound deals with a uniform prior). Also, the original bound is two-sided and the following theorem is one-sided.6\nTheorem 8 (Vapnik 1982) Let δ be given, let p be a prior distribution over H, and let ε∗(h) be the minimal value of ε that satisfies Γ(ε) ≤ p(h)δ. Then, with probability at least 1− δ, for all h ∈ H,\nRh(Xu)−Rh(Xm) √\nRh(Xm+u) < ε∗(h). (7)\nProof Using the union bound we have,\nQ = Pr\n{\n∃h ∈ H : Rh(Xu)−Rh(Xm)√ Rh(Xm+u) > ε∗(h)\n}\n= Pr { ∃h ∈ H : Rh(Xu)−Rh(Xm) > √ Rh(Xm+u)ε ∗(h) }\n= Pr\n{\n∃h ∈ H : Rh(Xu)−Rh(Xm) > √\nkh m+ u ε∗(h)\n}\n≤ ∑\nh∈H\nC\n(√\nkh m+ u ε∗(h), kh\n)\n(8)\n≤ ∑\nh∈H\nΓ(ε∗(h))\n≤ ∑\nh∈H\np(h)δ = δ.\nNote: The convention Rh(Xu)−Rh(Xm)√ Rh(Xm+u) = 0 is used wheneverRh(Xu) = Rh(Xm) = Rh(Xm+u) = 0.\nIt is not hard to convert the bound (7) to the “standard” form (i.e. expressed as empirical error plus some complexity term). Squaring both sides of (7) and then substituting m m+uRh(Xm) + u\nm+uRh(Xu) for Rh(Xm+u) we get a quadratic inequality where the “unknown” variable is Rh(Xu). Solving for Rh(Xu) yields the following result (as in Vapnik, 1998, Equation (8.15)).\nCorollary 9 Under the conditions of Theorem 8,\nRh(Xu) ≤ Rh(Xm) + ε∗(h)2u\n2(m+ u) + ε∗(h)\n√\nRh(Xm) +\n( ε∗(h)u\n2(m+ u)\n)2\n.\nRemark 10 Theorem 8 focuses on the relative deviation of the divergence Rh(Xu)−Rh(Xm), and generates a bound that is particularly tight in cases where the empirical error Rh(Xm) is very small. A very similar but simpler version of Vapnik’s bound concerns the “absolute”\n6. Specifically, in the original bound (Vapnik, 1982) the two-sided condition |kh−r u − r m | > ε is used instead\nof condition (5).\ndeviation is tighter in other cases of interest. The absolute bound is obtained as follows. Define, instead of (6),\nΓ(ε) △\n= max k C (ε, k) ,\nand let ε∗(h) be the minimal value of ε that satisfies Γ(ε) ≤ p(h)δ, for any given δ. Then, the absolute bound states that with confidence at least 1− δ,\nRh(Xu) < Rh(Xm) + ε ∗(h). (9)\nThis result is presented by Bottou, Cortes, and Vapnik (1994).\nRemark 11 The bound of Corollary 9, and the bound (9), are rather tight. Possible sources of slackness are only introduced through the utilization of the union bound in (8) and the definition of Γ in (6). However, note that ε∗(h) is a complicated implicit function of m, u, p(h) and δ leading to a bound that is difficult to interpret and (as noted also by Vapnik) must be tabulated by a computer in order to be used.\nNote that a related result has recently been presented by Blum and Langford (2003). Specifically, Theorem 6 in that paper states a similar implicit bound, based on direct calculation of the hypergeometric tail. However, Vapnik’s bound was originally proved for a uniform prior over the hypothesis class, and the extension to general priors was first proposed by Blum and Langford (2003).7"
    }, {
      "heading" : "3. Concentration Inequalities for Sampling without Replacement",
      "text" : "In this section we present several concentration inequalities that will be used in Section 4 to develop PAC-Bayesian bounds for transduction. As discussed in Section 2 (see Remark 1), sampling without replacement leads to dependent data, precluding direct application of standard large deviation bounds devised for independent samples. Here we present several concentration inequalities for sampling without replacement."
    }, {
      "heading" : "3.1 Inequalities Based on Reduction to Independence",
      "text" : "Even though sampling without replacement leads to dependent samples, Hoeffding (1963) pointed out a simple procedure to transform the problem into one involving independent data. While this procedure leads to non-trivial bounds it involves some loss in tightness (see Section 3.2).\nLemma 12 (Hoeffding 1963) Let C = {c1, . . . , cN} be a finite set with N elements, let {X1, . . . ,Xm} be chosen uniformly at random with replacement from C, and let {Z1, . . . , Zm} be chosen uniformly at random without replacement from C.8 Then, for any continuous and convex real-valued function f(x), Ef (\n∑m i=1 Zi) ≤ Ef ( ∑m i=1Xi) .\n7. Also, the bound of Blum and Langford (2003) may be tighter than the Vapnik’s bounds in some cases of interest. A careful numerical comparison should be conducted to determine the relative advantage of each of these bounds. 8. Note that the variables {X1, . . . ,Xm} are independent, while {Z1, . . . , Zm} are dependent.\nLemma 12 can be used in order to generate standard exponential bounds, as proposed by Hoeffding (1963). We first introduce some notation which will be used in the sequel. Let ν and µ be two real numbers in [0, 1]. We use the following definitions for the binary entropy and binary KL-divergence, respectively.\nH(ν) △ = −ν log ν − (1− ν) log(1− ν),\nD(ν‖µ) △= ν log ν µ + (1− ν) log 1− ν 1− µ.\nTheorem 13 (Hoeffding, 1963) Let C = {c1, . . . , cN} be a finite set of non-negative bounded real numbers, ci ≤ B, and set c̄ = (1/m) ∑m i=1 ci. Let Z1, . . . , Zm, be random variables obtaining their values by sampling C uniformly at random without replacement. Set Z = (1/m)\n∑m i=1 Zi. Then, for any ε ≤ 1− c̄/B,\nPr{Z −EZ ≥ ε} ≤ exp { −mD ( c̄\nB + ε\n∥ ∥ ∥ c̄\nB\n)}\n(10)\n≤ exp { −2mε 2\nB2\n}\n(11)\nSimilar bounds hold for Pr{EZ − Z ≥ ε}.\nThe key to the proof Theorem 13 is the application of Lemma 12 with f( ∑ i Zi) = exp{ ∑\ni(Zi− EZi)} and the utilization of the Chernoff-Hoeffding bounding technique (see Hoeffding, 1963, for the details)."
    }, {
      "heading" : "3.2 Sampling Without Replacement - Direct Inequalities",
      "text" : "In this section we consider approaches which directly establish exponential bounds for sampling without replacement. As opposed to Vapnik’s results (1982, 1998), which provide tight but implicit bounds, we aim at bounds which depend explicitly on all parameters of interest. Note that the bound of Theorem 13 does not depend on all the parameters (in particular, the population size N does not appear and clearly, small population size should affect the convergence rate). One may expect that bounds developed directly for sampling without replacement should be tighter than those based on reduction to independence. The reason for this is as follows. Assume, we have sampled k out of N points without replacement. The next point is to be sampled from a set of N − k rather than N points, which would be the case in sampling with replacement (where the samples are independent). The successive reduction in the size of the sampled set reduces the ‘randomness’ of the newly sampled point as compared to the independent case. This intuition is at the heart of Serfling’s improved bound (Serfling, 1974), which is stated next. This result holds for general bounded loss functions and is established by a careful utilization of martingale techniques combined with Chernoff’s bounding method.\nTheorem 14 (Serfling, 1974) Let C = {c1, . . . , cN} be a finite set of bounded real numbers, |ci| ≤ B. Let Z1, . . . , Zm, be random variables obtaining their values by sampling C uniformly at random without replacement. Set Z = 1\nm ∑m i=1 Zi. Then,\nPr{Z −EZ ≥ ε} ≤ exp { − ( 2mε2\nB2\n)( N\nN −m+ 1\n)}\n, (12)\nand similarly for Pr{EZm − Zm ≥ ε}.\nCompared to the bound of Theorem 13, the bound in (12) is always tighter than Hoeffding’s second bound (11) when N/(N − m + 1) > 1 (i.e. when m > 1). When applied to our transduction setup (see Section 4.1) we take N = m + u and the advantage is maximized when (m+ u)/(u+ 1) is maximized. Thus, considering only the convergence rate obtained by sampling without replacement, one may expect that the fastest rates should be obtained when u assumes the smallest possible value (e.g. u = 1) and not surprisingly, the advantage over the bound of Theorem 13 vanishes as u → ∞.\nIn the case where the ci are binary variables, the bound in Theorem 14 can be improved, by using a proof based on a counting argument. The following theorem and proof is based on a simple consequence of Lemma 2.1.33 by Dembo and Zeitouni (1998).\nTheorem 15 Let C = {c1, . . . , cN}, ci ∈ {0, 1}, be a finite set of binary numbers, and set c̄ = (1/N)\n∑N i=1 ci. Let Z1, . . . , Zm, be random variables obtaining their values by sampling\nC uniformly at random without replacement. Set Z = (1/m)∑mi=1 Zi and β = m/N . Then, if 9 ε ≤ min{1− c̄, c̄(1− β)/β},\nPr {Z −EZ > ε} ≤ exp { −mD(c̄+ ε‖c̄)− (N −m) D (\nc̄− βε 1− β ∥ ∥ ∥ ∥ c̄ ) + 7 log(N + 1) } .\nProof Denote by N0 and N1 the number of appearances in C of ci = 0 and ci = 1, respectively. Let m0 and m1 be integers 0 ≤ m0,m1 ≤ m, such that m0 + m1 = m. The probability of observing m1 appearances of ‘1’ (and thus m0 appearances of ‘0’ ) in a random sub-sample selected without replacement is the number of m-tuples resulting in m0(m1) appearances of 0(1) in the subsample, divided by the overall number of m-tuples,\nPr\n{ ∑\ni=1\nZi = m1\n}\n=\n( N1 m1 )( N0 m0 )\n( N m\n) .\nSetting µ = c̄, the probability that Z = (1/m) ∑m\ni=1 Zi is greater than ν △\n= µ+ ε = m1/m (for some natural number m1) is then given by\nPr\n{\n1\nm\n∑\ni=1\nZi > ν\n}\n= m∑\nm1=⌈mν⌉\n( N1 m1 )( N0 m0 )\n( N m\n) .\nUsing the Stirling bound we have that\nmax 1≤m≤N\n∣ ∣ ∣ ∣ log ( N\nm\n)\n−NH (m\nN\n) ∣ ∣ ∣ ∣ ≤ 2 log(N + 1).\nWe thus find that\nPr\n{\n1\nm\n∑\ni=1\nZi > ν\n}\n≤ m∑\nm1=mν\nexp\n{\nN0H ( m0 N0 ) +N1H ( m1 N1 ) −NH (m N ) + 6 log(N + 1) }\n=\nm∑\nm1=mν\nexp\n{ −mD(ν‖µ)− (N −m)D ( µ− βν 1− β ∥ ∥ ∥ ∥ µ ) + 6 log(N + 1) } .\n9. The second condition, ε ≤ c̄(1 − β)/β, simply guarantees that the number of ‘ones’ in the sub-sample does not exceed their number in the original sample.\nThe claim is concluded by upper bounding the sum by the product of the number of terms in the sum and the maximal term. It is easy to verify that the maximal summand is attained at ν = µ. Assuming that ν > µ, and using the convexity of D(ν‖µ) with respect to ν, we conclude that the largest contribution to the sum is attained at m1 = mν, yielding the bound\nPr\n{\n1\nm\n∑\ni=1\nZi > ν\n}\n≤ m(1− ν) exp { −mD(ν‖µ)− (N −m) D ( µ− βν 1− β ∥ ∥ ∥ ∥ µ ) + 6 log(N + 1) } ≤ exp {\n−mD(ν‖µ)− (N −m)D ( µ− βν 1− β ∥ ∥ ∥ ∥ µ ) + 7 log(N + 1) } ,\nwhich establishes the claim upon setting ν = c̄+ ε.\nNote that the proof of the last bound does not rely on the Chernoff-Hoeffding bounding technique as used by Hoeffding (1963) and in many other derivations, but rather on a direct counting argument.\nRemark 16 We are aware of another concentration inequality for sampling without replacement, which also applies to binary variables. This inequality, by Hush and Scovel (2003), is an extension of a result by Vapnik (1998, Sec. 4.13). While Vapnik’s result concerns the case m = N/2 (u = m in the transduction setup), the Hush and Scovel result considers the general case of arbitrary m and N . The transduction bound we obtain using the Hush and Scovel concentration inequality is more complex but is qualitatively the same as the bound of Corollary 23 that we later present. We therefore omit this bound."
    }, {
      "heading" : "4. PAC-Bayesian Transduction Bounds",
      "text" : "In this section we present general error bounds for transductive learning. Our bounds can be viewed as extensions of McAllester’s PAC-Bayesian inductive bounds (1999, 2003a, 2003b). In Section 4.1 we focus on simple randomized learning algorithms which are typically referred to as ‘Gibbs algorithms’. Then in Section 4.2 we consider a standard deterministic setting. In the case of binary classification the bounds for deterministic learning are comparable to Vapnik’s bounds presented in Section 2.2. Unlike the implicit but tight PACBayesian bound of Theorem 8 (and Corollary 9), the new bounds are somewhat looser but explicit."
    }, {
      "heading" : "4.1 Bounds for Transductive Gibbs Learning",
      "text" : "We present two bounds. The first is a rather immediate extension of McAllester’s bound (2003b, equation (6)), using a reduction to independence, as discussed in Section 3.1. The second bound is based on the ‘direct approach’ and is considerably tighter in many cases of interest.\nWithin the original inductive setting, the selection of the prior distribution p in the PAC-Bayesian bounds must be made prior to observing the data. As we later show, in the present transductive setting it is possible to obtain much more compact (and effective) priors by first observing the full input sample Xm+u and using it to construct a prior p = p(Xm+u). However, as shown by McAllester (2003a), under certain conditions it is\npossible to provide performance guarantees even if we select a “posterior” distribution over the hypothesis space after observing the labels of the training points Xm. The guarantee is provided for a Gibbs algorithm, which is simply a stochastic classifier defined as follows. Let q be any distribution over H. The corresponding Gibbs classifier, denoted by Gq, classifies any new instance using a randomly chosen hypothesis h ∈ H, with h ∼ q (i.e. each new instance is classified with a potentially new random classifier).\nFor Gibbs classifiers we now extend definition (1) as follows. Let Z = x1, . . . , x|Z| be any set of samples and let Gq be a Gibbs classifier over H. The (expected) risk of Gq over Z is\nRGq(Z) △ = Eh∼q\n \n\n1\n|Z|\n|Z| ∑\ni=1\nℓ(h(xi), φ(xi))\n   .\nAs before, when Z = Xm (the training set) we use the standard notation R̂Gq(Sm) = RGq(Xm).\nThe first risk bound we state for transductive Gibbs classifiers is a simple extension of the recent inductive generalization bound for Gibbs classifiers presented by McAllester (2003b). The new transductive bound relies on reduction to independence and its proof follows almost exactly the proof of the original inductive result. We therefore omit the proof but note that the inductive bound relies on the variant of Theorem 13 (inequality (10)) for sampling with replacement. The new bound is obtained by bounding the divergence between the Rh(Xm+u) and R̂h(Sm) now relying on inequality (10), which concerns sampling without replacement. The bound on the transductive risk Rh(Xu) is obtained using the following simple generalization of Lemma 5, stating that for all q and C,\nRGq(Xm+u) ≤ R̂Gq(Sm) + C ⇔ RGq(Xu) ≤ R̂Gq(Sm) + m+ u\nu · C. (13)\nTheorem 17 (Gibbs Classifiers) Let Xm+u = Xm ∪ Xu be the full sample. Let p = p(Xm+u) be a (prior) distribution over H that may depend on the full sample. Let δ ∈ (0, 1) be given. Then with probability at least 1− δ over the choices of Sm (from the full sample) the following bound holds for any distribution q,\nRGq(Xu) ≤ R̂Gq(Sm)+ ( m+ u\nu\n) \n\n√\n2R̂Gq(Sm) ( D(q||p) + ln m\nδ\n) m− 1 + 2 ( D(q||p) + ln m δ ) m− 1\n\n ,\n(14) where D(·||·) is the familiar Kullback-Leibler (KL) divergence (see e.g. Cover & Thomas, 1991).\nNotice that when R̂Gq(Sm) = 0 (the so-called “realizable case”) fast convergence rates of order 1/m are possible when u is sufficiently large (i.e. u = ω(m)).\nThe next risk bound we present for transductive Gibbs binary classifiers relies on the “direct” concentration inequality of Theorem 15, for sampling without replacement. The proof is based on the proof technique recently presented by McAllester (2003b), which, in turn, is based on the results of Langford and Shawe-Taylor (2002) and Seeger (2003).\nTheorem 18 (Binary Gibbs Classifiers) Let the conditions of Theorem 17 hold, and assume the loss is binary. Then with probability at least 1− δ over the choices of Sm (from the full sample) the following bound holds for any distribution q,\nRGq(Xu) ≤ R̂Gq(Sm) +\n√ √ √ √ ( 2R̂Gq(Sm)(m+ u)\nu\n)\nD(q‖p) + ln m δ + 7 log(m+ u+ 1)\nm− 1\n+ 2 ( D(q‖p) + ln m δ + 7 log(m+ u+ 1) )\nm− 1\nBefore we prove Theorem 18 observe that when R̂Gq(Sm) = 0 (the “realizable case”) the bound converges even if u = 1. In contrast, the bound of Theorem 17 diverges in the realizable case for any u = O( √ m).\nFor proving Theorem 18 we quote without proof two results by McAllester (2003b).\nLemma 19 (Lemma 5, McAllester, 2003b) Let X be a random variable satisfying Pr {X > x} ≤ e−mf(x) where f(x) is non-negative. Then E [ e(m−1)f(X) ] ≤ m.\nLemma 20 (Lemma 8, McAllester, 2003b) Ex∼q[f(x)] ≤ D(q‖p) + lnEx∼pef(x).\nProof of Theorem 18: The proof is based on the ideas by McAllester (2003b). Define\nν̂h △ = R̂h(Sm) ; µh △ = Rh(Xm+u).\nLet\nfh(ν) △ = D(ν‖µh) + u\nm D ( µh − βν 1− β ‖µh ) − 7 m log(m+ u+ 1).\n¿From Lemma 20 we have that\nEh∼q [(m− 1)fh(ν)] ≤ D(q‖p) + lnEh∼p [ e(m−1)fh(ν) ] . (15)\nAn upper bound on Eh∼p [ e(m−1)fh(ν) ] may be obtained by the following argument. From Theorem 15 we have that\nPr {ν̂h > ν} ≤ exp{−mfh(ν)}.\nLemma 19 then implies that for any h\nEΣm\n[ e(m−1)fh(ν̂h) ] ≤ m,\nwhich implies that\nEΣmEh∼p\n[ e(m−1)fh(ν̂h) ] ≤ m,\nfrom which we infer that with probability at least 1− δ,\nEh∼p\n[ e(m−1)fh(ν̂h) ] ≤ m δ\nby using Markov’s inequality. Substituting in (15) we find that with probability at least 1− δ,\nEh∼q [(m− 1)fh(ν̂h)] ≤ D(q‖p) + m\nδ . (16)\nSubstituting for fh(ν̂h), and using the convexity of the function x log x, we find that\nD(R̂Gq(Sm)‖RGq(Xm+u)) + u\nm D\n(\nRGq(Xm+u)− βR̂Gq(Sm) 1− β ‖RGq(Xm+u)\n)\n− 7 m log(m+ u+ 1)\n≤ D(q‖p) + ln m δ\nm− 1 . (17)\nIn order to obtain an explicit bound, we use the inequality\nD(ν‖µ) ≥ (ν − µ) 2\n2µ ,\nand substitute this in (17) obtaining\nRGq(Xm+u) ︸ ︷︷ ︸\nz\n≤ R̂Gq(Sm) ︸ ︷︷ ︸\na\n+ √ √ √ √ √ RGq(Xm+u) ︸ ︷︷ ︸\nz\n( 2u\nm+ u\n) D(q‖p) + ln m\nδ + 7 log(m+ u+ 1)\nm− 1 ︸ ︷︷ ︸\nb\n.\nThus we have (with probability at least 1 − δ), z ≤ a + √ zb (where z = RGq(Xm+u)).\nSolving for z we get z ≤ a+ b+ √ ab, and using Lemma 5 yields the desired result. ✷\nRemark 21 It is interesting to compare the bound of Theorem 17, based on the reduction to independence approach, and that of Theorem 18 which is based on a direct concentration inequality for sampling without replacement. The complexity term in Theorem 17 is multiplied by (m + u)/u, while the corresponding term in Theorem 18 is multiplied by √\n(m+ u)/u. This clearly displays the advantage of using the direct concentration bound, even though it does not lead to improved convergence rates in general. More importantly, for the realizable case, R̂Gq(Sm) = 0, the bound of Theorem 18 converges to zero even for u = 1. This is not the case for the bound of Theorem 17."
    }, {
      "heading" : "4.2 Bounds for Deterministic Learning Algorithms",
      "text" : "In this section we present three transductive PAC-Bayesian error bounds for deterministic learning algorithms. Note that the two bounds we present for the (stochastic) Gibbs algorithms in the previous subsection can be specialized to deterministic algorithms. This is done by choosing a “posterior” q which assigns probability 1 to one desired hypothesis h ∈ H. In doing so the term D(q‖p) reduces to log(1/p(h)). For example, the bound of Theorem 17 reduces to\nRh(Xu) ≤ R̂h(Sm) + ( m+ u\nu\n)\n\n  \n√ √ √ √2R̂h(Sm) ( log 1 p(h) + ln m δ ) m− 1 + 2 ( log( 1 p(h) + ln m δ ) m− 1     ,\n(18)\nwhich applies to any bounded loss function. The following bound relies of the Serfling concentration inequality presented in Theorem 14 and applies to any bounded loss function.10\nTheorem 22 Let Xm+u = Xm ∪Xu be the full sample and let p = p(Xm+u) be a (prior) distribution over H that may depend on the full sample. Assume that ℓ(h(x), y) ∈ [0, B] and let δ ∈ (0, 1) be given. Then, with probability at least 1 − δ over choices of Sm (from the full sample) the following bound holds for any h ∈ H,\nRh(Xu) ≤ R̂h(Sm) +B\n√ √ √ √ ( m+ u\nu\n)( u+ 1\nu\n)( ln 1 p(h) + ln 1 δ\n2m\n)\n. (19)\nProof In our transduction setting the set Xm (and therefore Sm) is obtained by sampling the full sample Xm+u uniformly at random without replacement. It is not hard to see that EΣmR̂h(Sm) = Rh(Xm+u). Specifically,\nEΣmR̂h(Sm) = 1\n( m+u m )\n∑\nSm\nR̂h(Sm) = 1\n( m+u m )\n∑\nXm⊆Xm+n\n1\nm\n∑\nx∈Sm\nℓ(h(x), φ(x)). (20)\nBy symmetry, all points x ∈ Xm+u are counted on the right-hand side an equal number of times; this number is precisely\n( m+u m ) − ( m+u−1 m ) = ( m+u−1 m−1 ) . The result is obtained by\nconsidering the definition of Rh(Xm+u) and noting that ( m+u−1 m−1 ) / ( m+u m ) = m\nm+u . Using the fact that our loss function is bounded in [0, B] we apply Theorem 14 (for a fixed h and N = m+ u),\nPrΣm\n{ ER̂h(Sm)− R̂h(Sm) > ε } ≤ e− 2mε2 B2 (m+u u+1 ). (21)\nSetting ε(h) = B\n√\n(u+1)(ln 1 p(h) +ln 1 δ )\n(m+u)2m and using the union bound we find\nPrΣm\n{ ∃h ∈ H s.t. Rh(Xm+u)− R̂h(Sm) > ε(h) } ≤ ∑\nh\nexp\n{\n−2mε(h) 2\nB2\n( m+ u\nu+ 1\n)}\n= ∑\nh\np(h)δ\n= δ.\nWe thus obtain that\nRh(Xm+u) ≤ R̂h(Sm) +B\n√ √ √ √ ( u+ 1\nm+ u\n)( ln 1 p(h) + ln 1 δ\n2m\n)\n. (22)\nThe proof is then completed using Lemma 5.\nFor classification using the 0/1 loss function we present one bound, which is a specialization of Theorem 18.\n10. Although we have not utilized the Serfling inequality for devising a bound for the Gibbs algorithm, it can be done as well.\nCorollary 23 Let the conditions of Theorem 22 hold and assume the loss is binary. Then with probability at least 1 − δ over the choices of Sm (from the full sample) the following bound holds for any distribution q,\nRh(Xu) ≤ R̂h(Sm) +\n√ √ √ √ ( 2R̂h(Sm)(m+ u)\nu\n) log 1\np(h) + ln m δ + 7 log(m+ u+ 1)\nm− 1\n+ 2 ( log 1 p(h) + ln m δ + 7 log(m+ u+ 1) )\nm− 1 .\nFigures 1 and 2 compare the two bounds presented in this section with Vapnik’s bound of Corollary 9. Throughout the discussion here the bound of Theorem 22 is referred to as the “Serfling bound”. Figure 1 focuses on the realizable case (i.e. empirical error = 0). According to the statements of Theorem 22 and Corollary 23, the Serfling bound has a significantly slower rate of convergence in the realizable case. However, the constants (and logarithmic terms) are larger in the bound of Corollary 23. Panels (a) and (b) in Figure 1 indicate that the Serfling bound is significantly better than the bound of Corollary 23 when u = Ω(m) for the range of m we consider. However, even in these cases, we know that the bound of Corollary 23 will eventually outperform the Serfling bound. We also see that the Serfling bound tracks Vapnik’s bound quite well when u = Ω(m). On the other hand, Panels (c) and (d) indicate that the bound of Corollary 23 is significantly better than the Serfling bound when u = o(m). The examples given are u = √ m in Panel (c) and u = 10 in Panel (d). Figure 2 shows these bounds for the case R̂h(Sm) = 0.2. Here again the Serfling bound nicely tracks Vapnik’s bound and we see that the bound of Corollary 23 converges much more slowly. All the curves in Figures 1 and 2 consider the case p(h) = 1. This assignment of the prior eliminates the influence of the union bound that is used to derive these bounds. In Figure 3 we show, for both the Vapnik and Serfling bounds, the complexity term as a function of the prior p(h), with 0.01 ≤ p(h) ≤ 1. Note that such prior assignments are realistic in the case of the transduction algorithm based on clustering that is introduced in Section 5.2.\nWhile these plots indicate that our bounds approximate Vapnik’s bound quite well in many cases of interest, we also see that for small values of m (similar to those considered in the plots) one will gain in applications by using the implicit but tighter Vapnik bound (or the Blum and Langford,2003, bound)."
    }, {
      "heading" : "5. Bounds for Specific Algorithms",
      "text" : "PAC-Bayesian error bounds (both inductive and transductive) are interesting because they provide a very simple yet general formulation of learning. However, in order to provide more concrete statements (e.g. about specific learning algorithms) one must apply such bounds with some concrete priors (and posteriors, in the case of Gibbs learning, see Theorems 17 and 18). In the context of inductive learning, a major obstacle in deriving effective bounds11 using the PAC-Bayesian framework is the construction of “compact priors”. For example,\n11. Informally, we say that a bound is “effective” if its complexity term vanishes with m (the size of the training sample) and it is sufficiently small for “reasonable” values m.\nMcAllester’s generalization bound (McAllester, 1999) contains a complexity term which includes a component of the form ln(1/p(h)) where p is a prior over H (as in Theorem 22 and Corollary 23). The more sophisticated inductive bounds for Gibbs classifiers (McAllester, 2003a, 2003b) include a Kullback-Leibler (KL) divergence complexity component D(q||p), where p is a prior over H and q is a posterior over H (as in Theorems 17 and 18). However, many hypothesis classes of interest are very large and even uncountably infinite. Therefore, despite the fact that these bounds apply in principle to very large H, in a straightforward application of these PAC-Bayesian bounds, when choosing priors with a very large support\n(and possibly a posterior with a small support), the complexity terms in these bounds can diverge or at least be too large to form effective generalization bounds.12\n12. Saying that we should also note that sophisticated prior choices within the inductive PAC-Bayesian framework can also lead to state-of-the-art bounds (McAllester, 2003b).\nIn contrast, the transductive framework provides a very convenient setting for applying PAC-Bayesian bounds. Here priors can be chosen after observing and analyzing the full sample. As already mentioned, in the case of the 0/1-loss, even if we consider a very large hypothesis space H, after observing the full sample, the effective size of equivalence classes of hypotheses in H is always finite and not larger than the number of dichotomies of the full sample (see also Remark 7).\nIn this section we present bounds for specific learning algorithms. The first class of algorithms we consider in Section 5.1 are “compression schemes”. The second class, are algorithms based on clustering. In both cases we show how to form effective priors by considering the structure of the full sample."
    }, {
      "heading" : "5.1 Bounds for Compression Algorithms",
      "text" : "We propose a technique for selecting a prior p(h) over H, based on the full (unlabeled) sample Xm+u. Given m, the learner constructs m “sub-priors” pτ , τ = 1, 2, . . . ,m, based on the full sample Xm+u, and for the final prior, takes a uniform mixture of all these “sub-priors”.\nThis technique generates transductive error bounds for “compression” algorithms. Let A be a learning algorithm. Intuitively, A is a “compression scheme” if it outputs the same hypothesis using a subset of the (labeled) training data.\nDefinition 24 A learning algorithm A (viewed as a function from samples to some hypothesis class) is a compression scheme with respect to a sample Z if there is a sub-sample Z ′, |Z ′| < |Z|, such that A(Z ′) = A(Z).\nObserve that the Support Vector Machine (SVM) approach is a compression scheme, where the set Z ′ is determined by the set of support vectors.\nLet A be a deterministic compression scheme and consider the full sample Xm+u. For each integer τ = 1, . . . ,m, consider all subsets of Xm+u of size τ , and for each subset construct all possible dichotomies of that subset (note that we are not proposing this approach as a useful algorithm, but rather as a means to derive bounds; in practice one need not construct all these dichotomies). A deterministic algorithm A generates at most one hypothesis h ∈ H for each dichotomy.13 For each τ , let the set of hypotheses generated by this procedure be denoted by Hτ . For the rest of this discussion we assume the worst case where |Hτ | = 2τ ( m+u τ ) (i.e. if Hτ does not contains one hypothesis for each dichotomy the bounds we propose below improve). The “sub-prior” pτ is then defined to be a uniform distribution over Hτ .\nIn this way we have m “sub-priors”, p1, . . . ,pm, which are constructed using only Xm+u (and are independent of the labels of the training set Ym; also note that this construction takes place before choosing the subset Xm). Any hypothesis selected by the learning algorithm A based on the labeled sample Sm and on the test set Xu belongs to ∪mτ=1Hτ . The motivation for this construction is as follows. Each τ can be viewed as our “guess” for the maximal number of compression points that will be utilized by a resulting classifier. For\n13. It might be that for some dichotomies the learning algorithm will fail to construct a classifier. For example, a linear SVM in feature space without “soft margin” will fail to classify non linearly-separable dichotomies of Xm+u.\neach such τ the distribution pτ is constructed over all possible classifiers that use τ compression points. By systematically considering all possible dichotomies of τ points we can characterize a relatively small subset of H without observing labels of the training points. Thus, each “sub-prior” pτ represents one such guess. The final prior is\np(h) = 1\nm\nm∑\nτ=1\npτ (h). (23)\nThe following corollary is obtained by an application of Theorem 22 using the prior p(h) in (23). This results characterizes an upper bound on the divergence in terms of the observed size of the compression set of the final classifier.\nCorollary 25 (Transductive Compression Bound) Let the conditions of Theorem 22 hold. Let A be a deterministic learning algorithm leading to a hypothesis h ∈ H based on a compression set of size s. Then, with probability at least 1− δ,\nRh(Xu) ≤ R̂h(Sm) +\n√ √ √ √ √ ( m+ u\nu\n)( u+ 1\nu\n) \n s ln\n( 2e(m+u)\ns\n)\n+ ln(m/δ))\nm\n\n. (24)\nProof Recall that Hs ⊆ H is the support set of ps and that ps(h) = 1/|Hs| for all h ∈ Hs, implying that ln(1/ps(h)) = |Hs|. Using the inequality ( m+u s ) ≤ (e(m + u)/s)s we have that |Hs| = 2s ( m+u s ) ≤ (2e(m + u)/s)s. Using the prior (23) and substituting ln(m/ps(h)) in Theorem 22 leads to the desired result.\nRemark 26 We can use Corollary 23 to get a similar result, which is sometimes tighter. Also note that compression bounds can be easily stated and proved for Gibbs learning.\nThe bound (24) can be easily computed once the classifier is trained. If the size of the compression set happens to be small, we obtain a tight bound. We note that these bounds are applicable to the transductive SVM algorithms discussed by Vapnik (1998), Bennett and Demiriz (1998) and Joachims (1999). However, our bounds motivate a different strategy than the one that drives these algorithms; namely, reduce the number of support vectors! (rather than enlarge the margin, as attempted by those algorithms).\nObserve the conceptual similarity of our bound to Vapnik’s bound for consistent SVMs (Vapnik, 1995, Theorem 5.2), which bounds the generalization error of an SVM by the ratio between the average number of support vectors and the sample size m. However, Vapnik’s bound can only be estimated while this bound is truly data dependent. Finally, it is interesting to compare our result to a recent inductive bound for compression schemes. In this context, Graepel et al. (see Theorem 5.18 in Herbrich, 2002) have derived a bound of the form\nR(SVM) ≤ m m− sR̂(SVM) +\n√\ns log(2em/s) + ln(1/δ) + 2 lnm\n2(m− s) , (25)\nwhere R(SVM) and R̂(SVM) denote the true and empirical errors, respectively, and s is the number of observed support vectors (over the training set)."
    }, {
      "heading" : "5.2 Transductive Learning via Clustering",
      "text" : "Some learning problems do not allow for high compression rates using compression schemes such as SVMs (i.e. the number of support vectors can sometimes be very large, see e.g. Baram et al., 2004, Table 1). A considerably stronger type of compression can often be achieved by clustering algorithms. While there is lack of formal links between entirely unsupervised clustering and classification, within a transductive setting we can provide a principled approach to using clustering algorithms for classification.\nIn particular, we propose the following approach: The learner applies a clustering algorithm (or a number of clustering algorithms) over the unlabeled data to generate several (unsupervised) models. The learner then utilizes the labeled data to guess labels for entire clusters (so that all points in the same cluster have the same label). In this way, a number of hypotheses are generated, one of which is then selected based on a PAC-Bayesian error bound for transduction, applied with an appropriate prior.\nThe natural idea of first clustering the unlabeled data and then assigning labels to clusters has been around for a long time and there are plenty of heuristic procedures that attempt to learn using this approach within a semi-supervised or transductive settings (see, e.g., Seeger, 2002, Sec. 2.1). However, to the best of our knowledge, none of the existing procedures was theoretically justified in terms of provable reduction of the true risk. In contrast, the clustering-based transduction method we propose here relies on a solid theoretical ground.\nLet A be any (deterministic) clustering algorithm which, given the full sample Xm+u, can cluster this sample into any desired number of clusters. We use A to cluster Xm+u into 1, . . . , c clusters where c ≤ m. Thus, the algorithm generates a collection of partitions of Xm+u into τ = 1, 2, . . . , c clusters, where each partition is denoted by Cτ . For each value of τ , let Hτ consist of those hypotheses which assign an identical label to all points in the same cluster of partition Cτ , and define the “sub-prior” pτ (h) = 1/2\nτ for each h ∈ Hτ and zero otherwise (note that there are 2τ possible dichotomies). The final prior is a uniform mixture of all these sub-priors, p(h) = 1\nc\n∑\nτ pτ (h). The learning algorithm selects a hypothesis as follows. Upon observing the labeled sample Sm = (Xm, Ym), for each of the clusterings C1, . . . , Cc constructed above, it assigns a label to each cluster based on the majority vote from the labels Ym of points falling within the cluster (in case of ties, or if no points from Xm belong to the cluster, choose a label arbitrarily). Doing this leads to c classifiers hτ , τ = 1, . . . , c. We now apply Theorem 22 with the above prior p(h) and can choose the classifier (equivalently, number of clusters) for which the best bound holds. We thus have the following corollary of Theorem 22.\nCorollary 27 Let A be any clustering algorithm and let hτ , τ = 1, . . . , c be classifications of the test set Xu as determined by clustering of the full sample Xm+u (into τ clusters) and the training set Sm, as described above. Let δ ∈ (0, 1) be given. Then, with probability at least 1− δ over choices of Sm the following bound holds for all τ ,\nRhτ (Xu) ≤ R̂hτ (Sm) + √ ( m+ u\nu\n)( u+ 1\nu\n)( τ + ln c\nδ\n2m\n)\n(26)\nRemark: Note that when m = u we get the bound\nRhτ (Xu) ≤ R̂hτ (Sm) +\n√ ( 1 + 1\nm\n) ( τ + ln c\nδ\n)\nm .\nAlso, in the case of the 0/1 loss we can use Corollary 23 to obtain significantly faster rates in the realizable case or when the training error is very small. We see that these bounds can be rather tight when the clustering algorithm is successful (e.g. when it captures the class structure in the data using a small number of clusters). Note however, that in practice one can significantly benefit by the faster rates that can be achieved utilizing Vapnik’s implicit bounds presented in Section 2.2 (or the bound of Blum and Langford, 2003). Clearly, any PAC-Bayesian bound for transduction can be plugged-in within this scheme and tighter bounds should yield better performance.\nCorollary 27 can be extended in a number of ways. One simple extension is the use of an ensemble of clustering algorithms. Specifically, we can concurrently apply k different clustering algorithms (using each algorithm to cluster the data into τ = 1, . . . , c clusters). We thus obtain kc hypotheses (partitions of Xm+u). By a simple application of the union bound we can replace ln c\nδ by ln kc δ in Corollary 27 and guarantee that k bounds hold si-\nmultaneously for all the k clustering algorithms (with probability at least 1− δ). We thus choose the hypothesis which minimizes the resulting bound.14 This extension is particularly attractive since typically without prior knowledge we do not know which clustering algorithm will be effective for the dataset at hand.\nTo conclude this section we note that El-Yaniv and Gerzon (2004) recently presented empirical studies of the above clustering approach. These empirical evaluations on a variety of real world datasets demonstrate the effectiveness of the proposed approach."
    }, {
      "heading" : "6. Concluding Remarks",
      "text" : "We presented general explicit PAC-Bayesian bounds for transductive learning. We also developed a new prior construction technique which effectively derives tight data-dependent error bounds for compression schemes and for transductive learning algorithms based on clustering.\nWith the exception of Theorem 22, which holds for any bounded loss function, all our transductive error bounds were presented within the simplest binary classification setting (i.e. with the 0/1-loss function and with non-stochastic labels). However, these results can be easily extended to multi-class problems and to stochastic labels. We hope that these bounds and the prior construction technique will be useful as a starting point for deriving error bounds for other known algorithms and for developing new types of transductive learning algorithms.\nWe emphasize, however, that in the case of classification (i.e. the 0/1 loss), implicit but tighter error bounds for transduction were already known (e.g. Vapnik’s result as stated in Corollary 9). Our bounds are explicit and can therefore be useful for interpreting and characterizing the functional dependency on the problem parameters.\n14. A better approach to combine the k clustering algorithms, especially if we expect that some of the algorithms will generate identical clusterings, is to construct one “big” prior for all of them.\nIn applications, when using compression schemes or our clustering-based transduction approach, one can plug-in any other PAC-Bayesian transductive bound (implicit or explicit). For example, one can benefit by using the tighter Vapnik bound. In (El-Yaniv & Gerzon, 2004) and (Banerjee & Langford, 2004), the authors present empirical studies of the clustering approach of Section 5.2 by plugging in the Vapnik implicit bound and the similar implicit bound from (Blum & Langford, 2003), respectively. These empirical studies indicate that the proposed clustering-based transductive scheme can lead to state-of-the-art algorithms.\nAn interesting feature of any transduction error bound for Setting 1, is that it holds for “individual samples”; that is, the full sample Xm+u need not be sampled i.i.d. from a distribution and moreover, in this setting one cannot assume that it is sampled from a fixed distribution at all! Therefore, results for this setting must hold for any given sample. In this sense, the transductive bounds within Setting 1 are considerably more robust than standard bounds in the inductive setting. We conclude with some open questions and research directions.\n1. All our results are obtained within Vapnik’s “Setting 1” of transduction, which must consider any arbitrary choice of the full sample. Considering Theorem 2 and Remark 4, it would be interesting to see if tighter results are possible within the probabilistic Setting 2.\n2. An interesting direction for future research could be the construction of more sophisticated priors. For example, in our compression bound (Corollary 25), for each number s of compression points we assigned the same prior to each dichotomy of every ssubset. However, in practice, when there is structure in the data, the vast majority of all these subsets and dichotomies cannot “explain” the data and should not be assigned a large prior.\n3. The bounds derived in this paper are based on a contribution from the deviation of a single hypothesis and a utilization of the union bound in the PAC-Bayesian style. More refined approaches, for example those based on McDiarmid’s inequality (McDiarmid, 1989) and the entropy method (Boucheron, Lugosi, & Massart, 2003), are able to eliminate the union bound altogether. It would be interesting to see if such approaches can lead to tighter bounds in the current setting.\n4. When considering arbitrary (bounded) loss functions, the basic observation for transduction (due to Vapnik) that the effective cardinality of the hypothesis class (in the case of classification) is finite, is not necessarily valid. However, it is likely that one can still benefit from the availability of the full sample. One possible approach, mentioned in Remark 7, would be to construct an empirical ǫ-cover of H based on the ℓ1 norm.\n5. Finally, we note that the major challenge, of determining a precise relation between the inductive and transductive learning schemes remains open. Of particular interest would be to determine the relation between the inductive semi-supervised setting (where the learner is also given a set of unlabeled points, but is required to induce\nan hypothesis for the entire space) and transduction. Our bounds suggest that transduction does not allow for learning rates that are faster than induction (as a function of m). On the other hand, it appears that the bounds we obtain for clustering-based transduction can be tighter than any known bound for a specific inductive learning effective algorithm."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The work of Ran El-Yaniv and Ron Meir was partially supported by the Technion V.P.R. fund for the promotion of sponsored research and the partial support of the PASCAL network of excellence. Support from the Ollendorff center of the department of Electrical Engineering at the Technion is also acknowledged. We also thank anonymous referees and Dmitry Pechyony for their useful comments."
    }, {
      "heading" : "Appendix A. Proof of Theorem 2",
      "text" : "Proof The proof we present is identical to Vapnik’s original proof and is provided for the sake of self-completeness. Let A be some learning algorithm choosing an hypothesis hA ∈ H based on Sm ∪Xu. Define\nCA(x1, y1; . . . ;xm+u, ym+u) = ∣ ∣ ∣ ∣ ∣ ∣ 1 m m∑\ni=1\nℓ (yi, hA(xi))− 1\nu\nm+u∑\nj=m+1\nℓ (yi, hA(xi)) ∣ ∣ ∣ ∣ ∣ ∣\n= |RhA(Xm)−RhA(Xu)| .\nConsider Setting 2. The probability that CA deviates from zero by an amount greater than ε is\nP =\n∫\nX ,Y I(CA − ε)dµ(x1, y1) · · · dµ(xm+u, ym+u),\nwhere I is an indicator step function, I(x) = 1 iff x > 0 and I(x) = 0 otherwise. Let Tp, p = 1, . . . , (m+ u)! be the permutation operator for the sample (x1, y1); . . . ; (xm+u, ym+u). It is not hard to see that\nP =\n∫\nX ,Y\n \n\n1\n(m+ u)!\n(m+u)! ∑\np=1\nI (CA(Tp(x1, y1; . . . ;xm+u, ym+u))− ε)\n   dµ(x1, y1) · · · dµ(xm+u, ym+u).\nThe expression in curly braces is the quantity estimated in Setting 1 and by our assumption, for any choice of the full sample, it does not exceed δ. Therefore,\nP ≤ ∫\nX ,Y δdµ(x1, y1) · · · dµ(xm+u, ym+u) = δ."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "Inductive learning is based on inferring a general rule from a finite data set and using it to label new data. In transduction one attempts to solve the problem of using a labeled training set to label a set of unlabeled points, which are given to the learner prior to learning. Although transduction seems at the outset to be an easier task than induction, there have not been many provably useful algorithms for transduction. Moreover, the precise relation between induction and transduction has not yet been determined. The main theoretical developments related to transduction were presented by Vapnik more than twenty years ago. One of Vapnik’s basic results is a rather tight error bound for transductive classification based on an exact computation of the hypergeometric tail. While tight, this bound is given implicitly via a computational routine. Our first contribution is a somewhat looser but explicit characterization of a slightly extended PAC-Bayesian version of Vapnik’s transductive bound. This characterization is obtained using concentration inequalities for the tail of sums of random variables obtained by sampling without replacement. We then derive error bounds for compression schemes such as (transductive) support vector machines and for transduction algorithms based on clustering. The main observation used for deriving these new error bounds and algorithms is that the unlabeled test points, which in the transductive setting are known in advance, can be used in order to construct useful data dependent prior distributions over the hypothesis space.",
    "creator" : "dvips(k) 5.90a Copyright 2002 Radical Eye Software"
  }
}