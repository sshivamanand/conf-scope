{
  "name" : "1206.1458.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dispelling Classes Gradually to Improve Quality of Feature Reduction Approaches",
    "authors" : [ "Shervan Fekri Ershad", "Sattar Hashemi" ],
    "emails" : [ "shfekri@shirazu.ac.ir", "S_hashemi@shirazu.ac.ir" ],
    "sections" : [ {
      "heading" : null,
      "text" : "DOI : 10.5121/acij.2012.3310 95\nFeature reduction is an important concept which is used for reducing dimensions to decrease the computation complexity and time of classification. Since now many approaches have been proposed for solving this problem, but almost all of them just presented a fix output for each input dataset that some of them aren’t satisfied cases for classification. In this we proposed an approach as processing input dataset to increase accuracy rate of each feature extraction methods. First of all, a new concept called dispelling classes gradually (DCG) is proposed to increase separability of classes based on their labels. Next, this method is used to process input dataset of the feature reduction approaches to decrease the misclassification error rate of their outputs more than when output is achieved without any processing. In addition our method has a good quality to collate with noise based on adapting dataset with feature reduction approaches. In the result part, two conditions (With process and without that) are compared to support our idea by using some of UCI datasets."
    }, {
      "heading" : "Keywords",
      "text" : ""
    }, {
      "heading" : "Feature Reduction, Dispelling Classes Gradually, Feature Extraction, Classes Separability",
      "text" : ""
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Since the middle of 20 th century when artificial intelligence science was established, classification methods were too important. By the pass of time, datasets that were used for classification got more complex than past such as geographic or discovery dataset. One of the items that increase the complexity of dataset is the number of dimensions (features). So a new concept called feature reduction was inducted in the literature of AI (Fukunnaga 1991) [4].\nFukunnaga said, If we describe input data as a matrix like X= { … . . } , where \"n\" is the sample number and \"m\" is the original feature dimensions, So the purpose of linear feature extraction is to search for a projection matrix like W ′ that transforms into a desired low-dimensional representation ′ , where ′ W .\nTypically, the projection matrix W is learnt by optimizing a criterion describing certain desired or undesired statistical or geometric properties of the data set. Different criterions lead to different kinds of linear feature exaction algorithms. Among them, Principal Component Analysis (PCA) (Joliffe, 1986) and Linear Discriminant Analysis (LDA) (Fukunnaga, 1991) have been the two most popular ones owing to their simplicity and effectiveness. Another popular technique called Locality Preserving Projections (LPP) (He & Niyogi, 2004) [5], has been proposed for linear feature extraction by preserving the local relationships within the data set. In (Yan et al., 2007) [1], many classical linear feature extraction techniques are unified into\na common framework known as Graph Embedding. To avoid the high time and memory usage associated with eigenvalue decomposition in LDA, the Spectral Regression Discriminant Analysis (SRDA) (Cai et al., 2008) [2], was proposed based on ridge regression.\nNow the output dataset Y is in a low-dimensional so the complexity of classification Y and the time of that are less. Also, computation complexity of Y for any other cases is less than X. It’s easy to know that feature extraction wastes some pieces of the dataset’s information. All of the methods for feature extraction are common in this object, so algorithms try to find a better formula for matrix W to decrease the amount of waste information to decrease the miss classification error rate of output(Y). Almost none of the previous methods have a good generalization for each kind of different datasets. To solve this problem, in this paper we present an approach to adapt the input dataset with every feature extraction methods. First we present a new concept called Dispelling Classes Gradually (DCG) to increase severability and separability of classes based on their labels and then we process the original input dataset with DCG and create a new dataset as an input dataset of the feature extraction methods to decrease the amount of misclassification error rate on its output rather than original situation."
    }, {
      "heading" : "1.1. PAPER ORGANIZION",
      "text" : "The reminder of this paper is organized as follows: Section two is related to the description of Dispelling Classes Gradually technique (DCG) and the way of this estimation. In the section three the proposed approach is described. Section four has justification about quality of proposed approach. In section five necessary parameters are tuned. Section six is related to the description of proposed approach’s noise insensitivity. Finally, the results and conclusion included."
    }, {
      "heading" : "2. DCG CONCEPT",
      "text" : "In the numerical datasets, misclassification error rate decreases with increasing the distance between classes[14]. So it’s necessary and beneficial to increase these distances. In the basic mathematics it’s determined that if two different numbers like be subtracted two another fix numbers like , respectively, and this action get repeated for α times, where , it’s authenticate that in extreme , the distance between will be more than first generation. This principal is shown in figure (1).\nIf we assume class labels to numeric labels and then assign for class labels, separability of classes increases at extreme situation by subtracting each class label from its sample value in available features. This subject is described in (1).\nX αL X αL !\" # X X (1)\nWhere, Xi is the ith sample and Xj is the jth sample and L is the class label of Xi and L ! is the class label of Xj . Figure (2) shows the results of applying (1) on Iris (UCI Dataset) on two features. There is a same authentication for negative numbers of α. If the numbers of α be negative just changes the orientation of classes move, but result of applying DCG is same as positive α values. According to figure (2) and formula (1), the distances between all samples of each class doesn’t change also variance of each class, but separability of classes increase by dispelling mean of the classes."
    }, {
      "heading" : "3. PROPOSED APPROACH",
      "text" : "According to previous part, DCG is too beneficial to increase separability of classes. But DCG does its process based on class labels. So on test set, DCG is not useful because we don’t know sample labels. But for train set, its may be useful. According to introduction part, all of feature extraction methods try to describe an accurate approach to create projection matrix W based on train set. So if DCG increases separability of train set before using to create projection matrix and then dimension reduced output dataset is compute based on projection matrix, then misclassification error rate of output decreases and output set Y classifiers easily get more than output of original input set.\nTo implement this method, sample labels of train set assume to be represented as a matrix C= [$ … . . $ ] % %& , where N is the number of samples and '( is the number of labels and the elements of the indicator vector ) is set to be 1 to '( . Then do *+ ), for - times. This method has 3 important advantages. (A) Adapting dataset with feature extraction method (B) the results never are worse than original manner. (C) Increasing the noise tolerability of dataset.\n(A) According to introduction part, if the input dataset of feature reduction be original just there is a fix output. The misclassification error rate of this fix output probably won’t be\ndesired. But with using our method it’s possible to adapt input set with feature reduction method more than primitive manner. By using optimal - it’s possible to create a better output with lower misclassification error rate than original output. (B) If the number of α be zero, original input and processed input are same so the feature extraction’s outputs of them are same and there is not any difference between\nmisclassification error rates.\n(C) One of the motivations that is presented for feature extraction methods is noise tolerability. In almost the entire feature extraction methods if noise sits on input dataset,\nthe misclassification error rate of output gets more than original manner. But our method can cover this motivation to a large extent. Our method does this action based on two features that are hidden in its formula.\nFirst feature is α. If the input dataset is noisy we can compute the optimal α to adapt feature\nreduction method with noisy dataset. It’s useful to calculate an output with error rate close the situation that input set is not noisy. Second feature is C. one of the methods that is presented to decrease the effect of noise is using fuzzy. In matrix C there is a concept like fuzzy which is hidden. When we subtract sample’s values of their labels in reality we subtract sample’s values of all of the labels just with different coefficient. Because all of labels are coefficients for the other ones.\n+ + -./0 , +1 +1 -./2\nIf ./0 3./2 4 + + 3-./2 (2)\nTo understand better, the flowchart of proposed approach is shown in figure3. Also it is compared with popular feature reduction approaches without train set processing."
    }, {
      "heading" : "4. JUSTFICATION",
      "text" : "The main question which is mentioned in this article is that why applying DCG concept on input dataset could increase the quality of feature reduction approaches. This question will be answered true this section. Almost all feature reduction approaches purpose is to describe new dimensions which have two below mentioned signification based on finding the mean, variance, and eigenvectors of each class in input dataset.\n(A) The first signification is that new dimensions are less than original dimensions (B) The second signification is that separability of classes ability is saved as far as possible.\nAs described in section two after processing the input dataset by DCG, both slope of each eigenvector and variance of each class remain constant and this happened because the transfer amount of each sample is equal to transfer amount of the same class’s samples. Consequently, the main section of feature reduction approaches which is finding slope of eigenvectors, and variance of classes will be remained constant and only mean of each class will be changed. This subject is shown in figure (4). As it is shown in figure (4), the slope of eigenvectors of dataset is not changed after processing the dataset by DCG.\nFigure4. Effect of DCG on Mean and eigenvectors of one of the classes of iris dataset\nAccording to proposed approach, it is asserted by transferring coordination of different classes’s mean, and distance between mean, it is possible to find the new coordination for each means and in this situation classification accuracy rate of feature reduction approach output is higher than original input dataset. Briefly, this article is mainly trying to show that by using DCG for processing input dataset, the feature reduction output error rate will be less than when input dataset is not processed."
    }, {
      "heading" : "5. PARAMETER SELECTION",
      "text" : "To process input dataset of feature extraction methods there is just an input parameter. The number of DCG Loop called α is an important parameter. Α should tune correctly to fulfill our aims. To select α there are two important points to pay attention. (I) according to DCG part, DCG avouch the separability of classes in infinite situation .So there are some values for α at each dataset that α doesn’t dispel classes enough and it’s possible that some of its values decrease the separability of classes. Because the move orientations of classes for dispelling are hidden, maybe the move orientation of a class is like another one and of course their speeds are never the same, so for some of α values, it’s possible that classes come near each other and their distances are less than primitive situation. This range of α is called loop’s problem maker range (LPMR), so we need the values that are out of this range. (II) But the second point to select α refers to feature extraction method. Using DCG method to process input datasets certainly changes the values of samples in each feature so it’s important that these transforms don’t cause\nproblem for feature extraction algorithms. Because in some of the feature extraction algorithms there are some formula that don’t work correctly with every values. For example one of the new feature extraction methods that is presented in ICML 2009 by XiaoTong and Bao-Gang is REDA [8]. This method calculates the best projection matrix W in an iterative manner. In their algorithm there is a formula that computes an item by this form: exp (- 56 ) 5 /8 ,. we know exponential of numbers of more than 30 is too huge and less than - 13 is considered as zero for all cases. So it’s important that new value of X after process don’t be out of this bound. For solving this problem we can compute the good range of α by (3).where\nis the minimum value in samples of 9:; sample’s label\n< *6 -./0 ) , /8 < => (3)\nBecause of using just input dataset, there aren’t any other parameters which are affected on accuracy rate of proposed approach.\nAccording to these points and basic theory of the DCG concept, the misclassification error rates of feature extraction’s outputs are different for different numbers of α. In the table (1) for example we computed the accuracy of the SRDA’s outputs of the input dataset (Haber-man) after applying DCG based on numbers of α between numbers 1 to 30."
    }, {
      "heading" : "SRDA",
      "text" : "According to the table (1), we plan the figure (5) based on numbers of α Between 1 to 60. According to the figure (5), the amount of the feature extraction’s outputs accuracies has tolerance, so there are some local optimums and a global optimum. The authors suggest hill climbing algorithms or simple genetic algorithm (J.Holland 1970’s) with a fitness function based on output set’s misclassification error rate to compute best value of α (Global optimum).\nFigure.5. Tolerance of feature extraction output’s accuracy based on different α"
    }, {
      "heading" : "6. NOISE INSENSITIVITY",
      "text" : "One of the most advantages of the proposed approach is noise insensitivity. According to different problems that occur, some of numeric values in dataset will change, it called noise. So, by occurring noise the quality of feature reduction approaches certainly decrease. By using the proposed approach, after doing some of DCG loops, the noise samples will nearing to another samples which have same labels, so, the effect of noise samples in the performance of feature reduction approaches will decrease."
    }, {
      "heading" : "7. RESULTS",
      "text" : "In this part the misclassification error rate of our idea and original methods are compared. In each table of this part, first we computed the misclassification error rate of feature extraction’s output of one of the original UCI datasets. It is shown in second columns of each row. The feature reduction approaches which are used are SRDA [2], PCA [1] and REDA-SRDA [8]. After that, the DCG is applied on the input datasets. So, the feature reduction approaches are applied on their processed input datasets and next classified. Misclassification rates are shown in fourth columns. Also, Parameter α is computed with a SGA (simple genetic algorithm) for each datasets and highlighted in third columns of each rows. All of the misclassification error rates computed with k-nearest neighborhood classifier [12] and we tried to use best number of k for each dataset to decrease the error rate. The results show, that the proposed approach increases the quality of feature reduction methods. Also, to prove the quality of proposed approach, the SVM classifier was used based on \"one against all\" technique, to compare, which is shown in the tables 6 & 7."
    }, {
      "heading" : "Huber-Man (UCI Dataset)",
      "text" : ""
    }, {
      "heading" : "Lung-Cancer (UCI Dataset)",
      "text" : ""
    }, {
      "heading" : "Glass (UCI Dataset)",
      "text" : ""
    }, {
      "heading" : "Breast-Cancer (UCI Dataset)",
      "text" : ""
    }, {
      "heading" : "8. CONCLUSION",
      "text" : "In this paper we presented a new concept call DCG to increase separability of classes. DCG is provided by subtracting features values of their labels. So we processed the input dataset of feature extraction methods based on DCG. The results showed that output of feature reduction methods have misclassification error rate when the input is processed less than original dataset. Next we presented some ways to compute optimum numbers of DCG’s loops and we mentioned some motivations for that. The results showed the quality of our idea based on various classifications. Some of advantages of proposed approach are:\n1) Suitability with near all of the classifiers based on processing stage 2) Noise insensitivity at the result of using labels 3) Low computational complexity against some of previous approaches 4) The accuracy rate results never be lower by using DCG than when original dataset are\nused"
    }, {
      "heading" : "9. FUTURE WORK",
      "text" : "According to the fifth section, one interesting future research direction is to study how to compute numbers of DCG’s loop without any algorithms just with formula. It will decrease the time and computation complexities."
    } ],
    "references" : [ {
      "title" : "Graph embedding and extensions: A general framework for dimensionality reduction",
      "author" : [ "S. Yan", "D. Xu", "B. Zhang", "H. Zhang", "Q. Yang", "S. Lin" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "An efficient algorithm for large-scale discriminant analysis",
      "author" : [ "D. Cai", "X. He", "J. Han" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Robust Euclidean Embedding",
      "author" : [ "L. Cayton", "S. Dasgupta" ],
      "venue" : "In the Proc of the International Conference on Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Introduction to statistical pattern recognition",
      "author" : [ "K. Fukunnaga" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1991
    }, {
      "title" : "Locality preserving projections",
      "author" : [ "X. He", "P. Niyogi" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2004
    }, {
      "title" : "Feature extraction using information theoretic learning",
      "author" : [ "K. Hild-II", "D. Erdogmus", "K. Torkkola", "C. Principe" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Kernel maximum entropy data transformation and an enhanced spectral clustering algorithm",
      "author" : [ "P. Huber", "R. Jenssen W", "T. Eltoft", "Girolami M", "D. Erdogmus" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "robust feature extraction via information theoretic learning",
      "author" : [ "Y. Xiao-Tong", "H. Bao-gang" ],
      "venue" : "In the Proc of the International Conference on machine learning",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Correntropy: Properties and applications in nongaussian signal processing",
      "author" : [ "W. Liu", "P.P. Pokharel", "J.C. Principe" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "Breakdown points of Cauchy regression-scale estimators",
      "author" : [ "I. Mizera", "C. Muller" ],
      "venue" : "Statistics and Probability Letters,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2002
    }, {
      "title" : "On measures of information and entropy",
      "author" : [ "A. Renyi" ],
      "venue" : "Proc of the 4th Berkeley Symposium on Mathematics, Statistics and Probability,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1961
    }, {
      "title" : "Color texture classification approach based on combination of primitive pattern units and statistical features",
      "author" : [ "Fekri Ershad", "Sh" ],
      "venue" : "International journal of Multimedia and Its application\",",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Feature extraction by nonparametric mutual information maximization",
      "author" : [ "K. Torkkola" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2003
    }, {
      "title" : "To increase quality of feature reduction approaches based on processing input dataset",
      "author" : [ "Fekri Ershad", "Sh", "S. Hashemi" ],
      "venue" : "In Proc of IEEE 3rd International Conference on Communication Software and Networks (ICCSN),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "So a new concept called feature reduction was inducted in the literature of AI (Fukunnaga 1991) [4].",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 4,
      "context" : "Another popular technique called Locality Preserving Projections (LPP) (He & Niyogi, 2004) [5], has been proposed for linear feature extraction by preserving the local relationships within the data set.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 0,
      "context" : ", 2007) [1], many classical linear feature extraction techniques are unified into",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 1,
      "context" : ", 2008) [2], was proposed based on ridge regression.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 13,
      "context" : "DCG CONCEPT In the numerical datasets, misclassification error rate decreases with increasing the distance between classes[14].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "For example one of the new feature extraction methods that is presented in ICML 2009 by XiaoTong and Bao-Gang is REDA [8].",
      "startOffset" : 118,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "The feature reduction approaches which are used are SRDA [2], PCA [1] and REDA-SRDA [8].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "The feature reduction approaches which are used are SRDA [2], PCA [1] and REDA-SRDA [8].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "The feature reduction approaches which are used are SRDA [2], PCA [1] and REDA-SRDA [8].",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 11,
      "context" : "All of the misclassification error rates computed with k-nearest neighborhood classifier [12] and we tried to use best number of k for each dataset to decrease the error rate.",
      "startOffset" : 89,
      "endOffset" : 93
    } ],
    "year" : 2012,
    "abstractText" : "Feature reduction is an important concept which is used for reducing dimensions to decrease the computation complexity and time of classification. Since now many approaches have been proposed for solving this problem, but almost all of them just presented a fix output for each input dataset that some of them aren’t satisfied cases for classification. In this we proposed an approach as processing input dataset to increase accuracy rate of each feature extraction methods. First of all, a new concept called dispelling classes gradually (DCG) is proposed to increase separability of classes based on their labels. Next, this method is used to process input dataset of the feature reduction approaches to decrease the misclassification error rate of their outputs more than when output is achieved without any processing. In addition our method has a good quality to collate with noise based on adapting dataset with feature reduction approaches. In the result part, two conditions (With process and without that) are compared to support our idea by using some of UCI datasets.",
    "creator" : "PScript5.dll Version 5.2.2"
  }
}