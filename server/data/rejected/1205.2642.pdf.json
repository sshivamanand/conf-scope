{
  "name" : "1205.2642.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improved Mean and Variance Approximations for Belief Net Responses via Network Doubling",
    "authors" : [ "Peter Hooper", "Yasin Abbasi-Yadkori", "Russ Greiner", "Bret Hoehn" ],
    "emails" : [ "hooper@stat.ualberta.ca", "hoehn}@cs.ualberta.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "A Bayesian belief network models a joint distribution with an directed acyclic graph representing dependencies among variables and network parameters characterizing conditional distributions. The parameters are viewed as random variables to quantify uncertainty about their values. Belief nets are used to compute responses to queries; i.e., conditional probabilities of interest. A query is a function of the parameters, hence a random variable. Van Allen et al. (2001, 2008) showed how to quantify uncertainty about a query via a delta method approximation of its variance. We develop more accurate approximations for both query mean and variance. The key idea is to extend the query mean approximation to a “doubled network” involving two independent replicates. Our method assumes complete data and can be applied to discrete, continuous, and hybrid networks (provided discrete variables have only discrete parents). We analyze several improvements, and provide empirical studies to demonstrate their effectiveness."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Consider a simple example. Suppose A represents presence/absence of a medical condition while B and Y are test results. Variables B and Y are conditionally independent given A, with A and B binary and Y continuous. The conditional independence assumption is represented by the directed acyclic graph structure in Figure 1(a). Let θa = P (A = a), θb|a = P (B = b |A = a), and let p(y |βa, σa) be the conditional density of Y given A = a, assumed normal with mean βa and variance σ2a. We want to estimate the probability that condition A is present given\nspecified results from the two tests B and Y . Let Θ represent all of the parameters. If Θ were known, we would use the formula:\nq(Θ) = qa|b,y(Θ) = θaθb|ap(y |βa, σa)∑\na1 θa1θb|a1p(y |βa1 , σa1)\n. (1)\nIn the Bayesian paradigm, uncertainty about Θ is quantified by modeling parameters as random variables. It follows that query probabilities such as (1) are also random. A query response is usually estimated by approximating its posterior mean. This approximation is similar to expression (1), but with θa and θb|a replaced by their posterior means and with the normal densities replaced by Student’s t densities.\nOne may want more than just a point estimate. Van Allen et al. (2001, 2008) showed (for discrete networks) how one can approximate the variance and posterior distribution of a query. Their variance derivation employs the delta method; i.e., a first-order Taylor series expansion of the function q(Θ) about the posterior mean of Θ. They provide asymptotic theory and empirical experiments supporting this approach. They also showed how these approximations can be used to construct a Bayesian credible interval (error bars) for q(Θ). Guo and Greiner (2005) applied this delta method approximation as part of a mean squared error (i.e., squared bias + variance) measure designed to estimate the quality of different belief net structures when seeking a best classifier. Lee et al. (2006) provide a technique for combining independent belief net classifiers that involves weighting their respective mean probability values by their inverse variances, and they show that this works well in practice.\nWe propose new approximations for the mean and variance based on a simple trick. Suppose (A1, B1, Y1) and (A2, B2, Y2) are replicates of the network variables, conditionally independent given Θ. We represent the paired replicates as nodes in a “doubled network” with the same structure; see Figure 1. The squared query q(Θ)2 can be expressed as a query in this doubled net-\nFigure 1: A simple Bayesian net.\n1\nwork:\nP (A1 = A2 = a |B1 = B2 = b, Y1 = Y2 = y,Θ) .\nThe method used to approximate the mean of q(Θ) can be extended to the doubled network to approximate the mean of q(Θ)2 and hence to approximate the variance. Unlike the delta method, our approach does not rely on approximate local linearity of q(Θ). It does involve the addition of two incomplete observations to the data set when calculating the posterior mean of q(Θ)2. In some situations, this addition results in under-estimation of the desired variance. This deficiency is largely eliminated by a simple adjustment. A similar adjustment substantially improves the usual query mean approximation.\nSection 2 reviews pertinent models and methods for belief networks. The network doubling technique is described in Section 3 for discrete, continuous, and hybrid networks. Proposed adjustments and numerical results are presented in Sections 4 and 5 for discrete networks. Corresponding work for continuous and hybrid networks is ongoing. Computational issues are discussed in Section 6. Contributions and plans for further work are summarized in Section 7."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : ""
    }, {
      "heading" : "2.1 NETWORK VARIABLES",
      "text" : "We assume network structure is known. Let B denote a discrete network variable taking values b ∈ DomB . Let Y denote a continuous network variable taking values y on the real line. Vectors of variables are denoted by boldface: A for discrete and X for continuous. Let Θ be a random vector comprising all unknown network parameters; i.e., Θ determines all conditional distributions of variables given their parents.\nWe assume that discrete variables have only discrete parents. Suppose pa(B) = A; i.e., the parents of B are the variables comprising the vectorA. The conditional probability that B = b given A = a is denoted\nθb|a = θB=b|A=a = P{B = b |A = a,Θ}.\nVariables associated with values will be clear from context. We employ similar abbreviations for other parameters and hyperparameters. The θb|a parameters are often presented in conditional probability tables (CPtables) with rows indexed by a and columns by b; e.g., see Figure 1. Note that we use superscripts b1, b2 to list the distinct values in DomB . We use subscripts b1, b2 to denote arbitrary values in DomB , often related to replicated variables B1, B2.\nContinuous variables can have both discrete and continuous parents. Suppose pa(Y ) = 〈A,X〉 with X = 〈X1, . . . , Xd〉. The conditional distribution of Y is\n(Y |A = a,X = x,Θ) ∼ N ( (1,xT )βa, σ 2 a ) ; (2)\ni.e., normally distributed, conditional mean related to x by a linear regression model with coefficients depending on a. Here xT is the transpose of the ddimensional column vector x while βa is an (d + 1)- dimensional column vector of regression coefficients (the first entry is the constant term)."
    }, {
      "heading" : "2.2 PRIOR AND POSTERIOR",
      "text" : "The network parameters represented by Θ consist of CPtable parameters θb|a, regression coefficient vectors βa, and variances σ2a. We assume the prior distribution for Θ has the following form; e.g., see Gelman et al. (2003).\n• CPtable rows follow Dirichlet distributions:\nθB|a := 〈θb|a, b ∈ DomB〉 ∼ Dir(αB|a),\nwhere αB|a := 〈αb|a, b ∈ DomB〉 .\n• The regression coefficients and variance together have a normal-(inverse chi-square) distribution:\n(βa |σ2a) ∼ Nd+1 ( µa, σ 2 a(νaΨa) −1) , σ−2a ∼ (τ2aνa)−1χ2νa .\nI.e., dropping subscripts for a moment, β conditioned on σ2 is multivariate normal with mean\nvector µ and covariance matrix σ2(νΨ)−1; and ντ2/σ2 has a χ2ν distribution with ν > 0 (not necessarily an integer). Note that τ2/σ2 has mean 1 and variance 2/ν.\n• Parameters are assumed to be statistically independent except where joint distributions are specified above. In particular, we assume global independence: the parameters determining the conditional distribution of one variable given its parents are independent of all other parameters.\nThe prior is conjugate: given a data set D consisting of n independent replicates of complete tuples of network variables, the prior hyperparameter values are updated as follows. Let nab and na be the number of tuples in D with (A, B) = (a, b) and A = a, respectively. Let (xi, yi) be the observations of (X, Y ) for the na tuples with A = a. Let Xa be the na× (d+1) matrix with rows (1,xTi ). Let ya be the column vector with entries yi. In the five equations below, the prior hyperparameter values appear on the right-hand side and are identified with tildes (e.g., α̃).\nαb|a = α̃b|a + nab νa = ν̃a + na\nνaΨa = ν̃aΨ̃a +XTaXa νaΨaµa = ν̃aΨ̃aµ̃a +X T aya\nνa [ τ2a + µ T aΨaµa ] = ν̃a [ τ̃2a + µ̃ T aΨ̃aµ̃a ] + yTaya\nThe values ∑ a,b αb|a and ∑\na νa are called the effective sample sizes for variables B and Y , respectively. Our adjustments developed in Section 4 are motivated by large m asymptotics, where m is proportional to the effective sample size for each of the variables; i.e.,\nαb|a = mα0b|a and νa = mν 0 a\nwith (α0b|a, ν 0 a,Ψa,µa, τ 2 a) fixed. (3)\nLarge m asymptotics are similar to but not the same as large n asymptotics. As the sample size n increases, the posterior mean E{θb|a | D} = αb|a/α·|a varies and converges to some value. (Here and elsewhere, the dot subscript indicates summation: α·|a = ∑ b αb|a .) Under assumption (3), the posterior mean remains fixed as m varies."
    }, {
      "heading" : "2.3 APPROXIMATING A QUERY MEAN",
      "text" : "Consider a query involving outcomes of hypothesis variables H given values for evidence variables E. It is convenient to represent the query in terms of a function w(H). E.g., suppose H = A, E = (B, Y ), e = (b, y), and\nq(Θ) = P (A = a |B = b, Y = y,Θ) = E{w(A) |B = b, Y = y,Θ} ,\nwhere w(A) = 1 for A = a and w(A) = 0 otherwise.\nFor discrete networks, query responses q(Θ) are usually estimated by q(Θ̂), where Θ̂ := E{Θ | D} is the posterior mean of the parameter vector. This plugin estimate usually differs slightly from the posterior query mean E{q(Θ) | D}. Cooper and Herskovits (1992, expression 19) showed that the plug-in estimate equals E{q(Θ) | D, e}; i.e., the posterior query mean given an augmented data set consisting of D and an additional partial observation of the evidence variables E = e. Cooper and Herskovits (1991) derived a formula for E{q(Θ) | D, e} that is valid for discrete, continuous, and hybrid networks. This formula provides a useful approximation of the less tractable E{q(Θ | D}. The plug-in estimate is a special case of this formula for discrete networks. The formula is important for our network doubling technique, so is reviewed here.\nIn the integral expression below, Z represents all variables not included in (H,E); dh and dz refer to product measures allowing both integration for continuous variables (Lebesgue measure) and summation for discrete variables (counting measure). Some manipulation yields\nE{q(Θ) | D, e} = E{w(H) |E = e,D} = E [E{w(H) |E = e,Θ} |D ] (4)\n= ∫ ∫ w(h) ∫ p(h, e,z |θ)p(θ | D)dθdhdz∫ ∫ ∫\np(h, e,z |θ)p(θ | D)dθdhdz .\nNow p(h, e,z |θ) factors as a product of conditional probabilities and densities, one for each variable in the network. Due to global independence, the integral ∫ p(h, e,z |θ)p(θ | D)dθ factors into a product of integrals, one for each variable. The result is a product of probabilities and densities described in Section 2.4 below. It follows that E{q(Θ | D, e} can be calculated in essentially the same manner as the function q(Θ), but with two modifications.\n• For discrete variables, parameters θb|a are replaced by their posterior means. If all network variables are discrete, then we have the plug-in estimate:\nE{q(Θ) | D, e} = q(E{Θ | D}). (5)\n• For continuous variables, the normal densities are replaced by the St1(η, ω2, ν) densities described below. Note that this is not the same as replacing β and σ2 parameters with their posterior means."
    }, {
      "heading" : "2.4 PREDICTIVE DISTRIBUTIONS",
      "text" : "The predictive distribution of the network variables is obtained by integrating out their joint conditional dis-\ntribution given Θ with respect to the posterior distribution of Θ. Global independence allows this integration to be carried out separately for each conditional distribution of a variable given its parents.\nThe predictive distribution for a discrete variable B is\nπb|a := P (B = b |A = a,D) = E{θb|a | D} = αb|a\nα·|a .\nThe predictive distribution for a continuous variable is a location-scale version of the Student’s t distribution with ν degrees of freedom. We need the multivariate form of this distribution in Section 3, so we define it here. Suppose\nT = η + U−1/2(Z − η),\nwhere Z and U are independent, Z ∼ Np(η,Ω), U ∼ (1/ν)χ2ν , and Ω is a nonsingular covariance matrix. It follows that T has the following density function (Johnson and Kotz, 1972, page 134):\nΓ[(ν + p)/2] /Γ(ν/2) (νπ)p/2|Ω|1/2 [ 1 + 1ν (t− η)TΩ −1(t− η) ](ν+p)/2 .\nWe refer to this as the Stp(η,Ω, ν) distribution. For p = 1, we write St1(η, ω2, ν). Note that St1(0, 1, ν) is Student’s t distribution.\nWe claim that (Y |A = a,X = x,D) ∼ St1(η, ω2, ν) with ν = νa, η = (1,xT )µa, and\nω2 = τ2a { (1,xT )(νaΨa)−1(1,xT )T + 1 } . (6)\nTo see this, let us suppress subscripts for a moment. Let Z1 ∼ N(0, 1) be independent of (β, σ). Put Z2 := σ−1(β − µ) ∼ Nm+1 ( 0, (νΨ)−1 ) . We then have\n(Y |a,x,D) ∼ (1,xT )β + σZ1 ∼ η + (σ/τ)τ { (1,xT )Z2 + Z1 } ."
    }, {
      "heading" : "3 NETWORK DOUBLING",
      "text" : "In Section 2.3 we noted that E{q(Θ) | D} is usually approximated by the more tractable E{q(Θ) | D, e}. Here we propose approximating Var{q(Θ) | D} by Var{q(Θ) | D, e, e}; i.e., the posterior variance given D and additional replicates E1 and E2 of the vector of evidence variables, both having the same value e. We develop a formula for this latter variance by imagining a doubled network; see Figure 1(b). These mean and variance approximations can be improved by adjustments described in Section 4.\nConsider two replicated tuples of network variables, conditionally independent and identically distributed given Θ. Use these to replace each variable in the\noriginal network by a pair of variables; e.g., B is replaced by B∗ := (B1, B2) with possible values b∗ = (b1, b2) ∈ DomB∗ = DomB × DomB . If pa(B) = A, then pa(B∗) = A∗ := (A1,A2). Conditional distributions of doubled variables given parents are obtained by multiplying probabilities or densities for single variables.\nFor discrete variables, we have\nP (B∗ = b∗ |A∗ = a∗,Θ) = θb1|a1θb2|a2 .\nE.g., if A = A, DomA = {a1, a2}, and DomB = {b1, b2}, then the CPtable for B∗ is the 4 × 4 array shown in Figure 1(b). More generally, if a CPtable in the original network involves dr × dc parameters, then corresponding table in the doubled network has d2r × d2c entries. Note that CPtable rows in the doubled network are not independent (local independence does not hold) and do not have Dirichlet distributions. Fortunately, these properties are not needed for the factorization described following (4).\nFor continuous variables, the conditional density of Y ∗ = (Y1, Y2) given (A∗ = a∗,X∗ = x∗,Θ) is the product of the densities for two normal distributions of the form (2) with subscript i = 1, 2 on a and x.\nPut H∗ = (H1,H2), w∗(H∗) = w(H1)w(H2), E∗ = (E1,E2), and e∗ = (e, e). Some manipulation using conditional independence yields\nq(Θ)2 = E{w∗(H∗) |E∗ = e∗,Θ} , q(Θ) = E{w(H1) |E∗ = e∗,Θ} .\nWe thus have\nVar{q(Θ) | D, e, e} (7) = E{q(Θ)2 | D, e, e} − [E{q(Θ) | D, e, e}]2\n= E{w∗(H∗) | e∗,D} − [E{w(H1) | e∗,D}]2 .\nThe doubled network satisfies global independence assumptions, so we can follow the approach of Section 2.3 to evaluate the two expected values in (7). To accomplish this task, we need bivariate predictive distributions for the doubled network.\nFor discrete variables, the calculation follows from the means and covariances of a Dirichlet distribution. Let δb1b2 be the Kronecker delta function. We have\nπ∗b∗|a∗ := P{B ∗ = b∗ |A∗ = a∗,D}\n= E{θb1|a1θb2|a2 | D} = πb1|a1πb2|a2 + δa1a2 πb1|a1(δb1b2 − πb2|a1)\nα·|a1 + 1 .\nIf all network variables are discrete, then we have an identity corresponding to (5). Let Θ∗ be the vector\nof all CPtable entries in the doubled network; e.g., θb1|a1θb2|a2 appears in row a\n∗ and column b∗ for the CPtable of B∗. We then have\nE{q∗(Θ∗) | D, e, e} = q∗(E{Θ∗ | D}) (8)\nwith the entries in E{Θ∗ | D} given by the π∗b∗|a∗ values above. The two expected values in the variance approximation (7) are calculated by applying (8) twice: with q∗(Θ∗) = q(Θ)2 and with q∗(Θ∗) = q(Θ).\nFor continuous variables, we need the density for {(Y1, Y2) |a1,a2,x1,x2,D}. There are two cases to consider.\n• If a1 6= a2, then the parameters (βa1 , σ 2 a1) and\n(βa2 , σ 2 a2) are mutually independent. Consequently, the joint distribution factors as a product of two St1(η, ω2, ν) densities; see expression (6).\n• If a1 = a2 ( = a, say), then the joint distribution is St2(η,Ω, ν) with ν = νa, η =X2µa, and\nΩ = τ2a { X2(νaΨa)−1XT2 + I2 } ,\nwhere X2 is the 2 × (1 + d) matrix whose rows are each (1,xTi ) and I2 is the 2 × 2 identity matrix. The derivation is similar to that following (6). Note that (βa, σ2a) is the same for both Y1 and Y2 in this case."
    }, {
      "heading" : "4 ADJUSTMENTS",
      "text" : "We now narrow our focus to discrete networks and consider the four mean and variance approximations in Table 1. The delta method approximation is\nv̂1 = gTCg , (9)\nwhere g is the gradient vector of q(Θ) and C is the covariance matrix of Θ, both evaluated at E{Θ | D}. The second variance approximation v̂2 is the doubling method introduced in Section 3. The simple adjustments (q̂3, v̂3) and more complex adjustments (q̂4, v̂4) are developed in this section.\nFor conciseness we suppress D in our expressions; i.e., we implicitly assume that expectations are conditioned on D. PutQ = q(Θ) = P (H = h |E = e,Θ) and R = P (E = e |Θ). Note that R is an unconditional query, with hypothesis E = e and no evidence variables. Let µq, µr, σqq, σrr, and σqr denote the means, variances, and covariance for (Q,R). We extend this notation to higher moments; e.g., σqqr = E{(Q− µq)2(R− µr)}.\nWe use approximations for higher moments motivated by large m asymptotics; i.e., a sequence of posterior distributions of the form (3) with m → ∞. One may\nverify that the distribution of √ m(Q− µq, R− µr) converges to bivariate normal by modifying the proof of Theorem 2 in Van Allen et al. (2008). Asymptotic normality implies that\nσqqrr − 2σ2qr − σqqσrr → 0 at rate m−5/2 (10)\nwhile σqrr and σqqr converge to zero at rate m−2. We considered approximating σqqr and σqrr by zero but found that more accurate approximations give better results. Asymptotic bivariate normality suggests\nE{R− µr |Q} ≈ (Q− µq) σqr σqq\nand hence σqqr ≈ σqqqσqr/σqq. Now σqqq = 0 for normal distributions; however, Van Allen et al. (2008) argue that query distributions are usually better approximated by beta distributions. Substituting the third moment of a beta distribution for σqqq, we obtain\nσqqr ≈ 2σqrσqq(1− 2µq) µq(1− µq) + σqq . (11)\nSwitching the roles of Q and R gives\nσqrr ≈ 2σqrσrr(1− 2µr) µr(1− µr) + σrr . (12)\nBefore proceeding, we observe that µr and σrr can be calculated exactly because R can be expressed as a sum of products of independent terms. For queries with this property, all approximations (except v̂1) are exact; i.e., additional observations of evidence variables have no effect on the posterior mean or variance. E.g., given a discrete network with structure E → B → H, we have q(Θ) = ∑ b θh|bθb|e . Since parameters in each product are independent, it follows that q̂2 = q̂1 = µq and v̂2 = σqq.\nWe begin with adjustments to improve q̂1. Bayes rule and some manipulation yields\nq̂1 = E(QR) E(R) = µq + σqr µr\n(13)\nq̂2 = E(QR2) E(R2) = µq + 2µrσqr + σqrr µ2r + σrr .\nIf µr = 1, then set σ̂qr = 0. Otherwise, substituting (12) for σqrr and solving yields σ̂qr =\n(q̂2 − q̂1)µr(µ2r + σrr){µr(1− µr) + σrr} µ3r(1− µr) + µr(1− 2µr)σrr − σ2rr . (14)\nThe formula for q̂4 in Table 1 follows from (13). Now recall that, under condition (3), µr remains fixed while σrr → 0 as m → ∞. It follows that setting σrr = 0 in (14) will have negligible effect for large m. We thus obtain σ̂qr ≈ (q̂2 − q̂1)µr, leading to the simpler q̂3 approximation.\nIn trying to improve v̂2, we began with the idea of replacing q̂2 with µq:\nE{(Q− µq)2 | e, e} = v̂2 + (q̂2 − µq)2 . (15)\nThis suggests an approximation v̂2+4(q̂2− q̂1)2, which does help to reduce the under-estimation problem; however, a greater improvement is obtained by further analysis of (15):\nE{(Q− µq)2R2} E(R2) = µ2rσqq + 2µrσqqr + σqqrr µ2r + σrr . (16)\nWe approximate σqqrr using (10), σqqr by (11), σqr by (14), µq by q̂4, and replace σqq by v̂4. Rearranging terms yields the identity: v̂4 =\n(µ2r + σrr){v̂2 + (q̂2 − q̂4)2} − 2σ̂2qr µ2r + σrr + 4µrσ̂qr(1− 2q̂4)/{q̂4(1− q̂4) + v̂4} . (17)\nNotice that v̂4 appears in the denominator of (17). We initially set this value to v̂2, then iteratively solve for v̂4. The values converge in a few iterations.\nWe observe that replacing σrr by zero has negligible effect on (17) as m → ∞. By also replacing q̂4 by q̂3 and σ̂qr/µr by q̂2 − q̂1, we obtain a simpler identity:\nv̂3 = v̂2 + 2(q̂2 − q̂1)2\n1 + 4(q̂2 − q̂1)(1− 2q̂3)/{q̂3(1− q̂3) + v̂3} . (18)\nWe again initialize by v̂2, then iteratively solve for v̂3. The approximations q̂3 and v̂3 may be preferred to q̂4 and v̂4 since µr and σrr are not required.\nRates of convergence are summarized in Proposition 1 below. The proof of this result follows easily from Van Allen et al. (2008) and the development above.\nProposition 1. Assume a discrete network satisfying (3) and let m→∞. The query mean µq remains constant while the variance σqq approaches zero at rate m−1. The mean approximations have errors q̂j − µq approaching zero at rate m−1 for j = 1 and 2, and at the faster rate m−3/2 for j = 3 and 4. All four variance approximations have relative errors (v̂j−σqq)/σqq approaching zero at rate m−1.\n/Users/peterhooper/Documents/Research/Doubling paper/R figures"
    }, {
      "heading" : "5 NUMERICAL RESULTS",
      "text" : "We evaluated accuracy of approximations q̂j and v̂j using highly accurate empirical estimates of µq and σqq. These estimates q̂0 and v̂0 were obtained by simulating k = 106 replicates of Θ from the posterior distribution, evaluating q(Θ) for each replicate, then calculating the sample mean and sample variance. Computational costs preclude using empirical variance estimates in practice. When m is large, asymptotic normality of q(Θ) implies that the distribution of v̂0/σqq is approximately (1/k)χ2k with variance 2/k. Consequently v̂0/σqq varies over the interval 1± 2 √ 2/k for roughly 95% of samples. Since our variance approximations have relative errors of order m−1, it follows that k should be of order at least m2 for v̂0 to have substantially smaller relative error. When comparing approximate relative errors (v̂j − v̂0)/v̂0 with k = 106, variation in v̂0 has a noticeable effect for m = 500; see Figure 3(f).\nOur examples differ with respect to network structure, posterior distribution, and query. All variables are binary. All posterior distributions satisfy BDe constraints (e.g., see Hooper 2008), so all variables have the same effective sample size m. Hyperparameters are thus determined by m and the poste-\n/Users/peterhooper/Documents/Research/Doubling paper/R figures\nrior means E{Θ | D}. Our examples are from three small networks, each with one vector E{Θ | D} and m ∈ {20, 50, 100, 200, 500}:\n• Two näıve Bayes networks (NB-2 and NB-4 with 2 and 4 features plus the root variable); H = root,\nE = all children of H, e varies over all combinations (22 for NB-2, 24 for NB-4).\n• Diamond network with 4 variables ↙↘ ↘ ↙, all 108\ndistinct queries with one hypothesis variable.\nApproximations for means are compared in Figure 2 and for variances in Figure 3. The errors and relative errors are multiplied by m in these figures to facilitate comparisons across a range of effective sample sizes. Boxplots for m = 20, 100, and 500 are shown. Plots for other values of m are similar. By Proposition 1, relative errors (v̂j−σqq)/σqq should approach zero at rates cj/m, where cj depends implicitly on the network, E(Θ | D), and the query. This theory is supported by Figure 3 and additional plots (not shown) comparing the four methods for individual queries. Our results suggest that c3 ≈ c4 while c1 and c2 tend to be further from zero. Relative errors can be interpreted in terms of variances or standard deviations. If (v̂j − σqq)/σqq = cj/m, then we have\nv̂j σqq = 1 + cj m and\n√ v̂j √ σqq = √ 1 + cj m ≈ 1 + cj 2m ."
    }, {
      "heading" : "6 COMPUTATIONAL ISSUES",
      "text" : "Inference in Bayesian networks is in general an NPcomplete problem (Cooper, 1990). For instance, the complexity of the Variable Elimination (VE) Algorithm is O(dr), where d is an upper bound on the number of values that a variable can take and r is an upper bound on the size of a factor generated by the VE Algorithm (Koller and Friedman, 2008). Network doubling uses essentially the same technique to calculate a variance as that used to evaluate a query, resulting in corresponding computational complexity. The doubled CPtables are larger (squared number of rows and columns), so the computational complexity of VE is increased to O(d2r). The delta method retains O(dr) complexity (Van Allen et al., 2008), so is typically faster in large networks; see Table 2 below.\nIn some cases, we can exploit the structure of the network or query to achieve a polynomial time inference algorithm. For poly-tree Bayesian networks (i.e. networks with at most one undirected path between any pair of nodes), there exist inference algorithms with linear time complexity. Reduced complexity is also available when the query can be expressed in terms of probabilities of hypothesis and evidence nodes conditioned on their Markov blanket; i.e., the parents, the children and the parents of the children. Once again, we have a polynomial time inference algorithm. These techniques translate directly to efficient algorithms for computing all of the variance approximations in Table 1.\nWe empirically compared timing results for the delta and network doubling methods using queries from three networks: 1000 from Näıve Bayes (5 variables) repeated 100 times, 108 from the Diamond network (4 variables) repeated 1000 times, and 100 from the Alarm network (37 variables, Beinlich et al. 1989). The Alarm queries were randomly generated so that queries had on average 3 hypothesis variables, 25 evidence variables, and 9 non-specified variables. The results in Table 2 corroborate the earlier claim that doubling is faster for simple queries and delta is faster for complex queries."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "We plan to extend the implementation of the network doubling and delta methods to continuous and hybrid networks. This should be easy for the doubling method when all continuous variables are evidence variables. We will then compare the accuracies of these methods.\nOur main contributions are summarized as follows.\n• Development of a network doubling method to approximate query variances. This technique exploits the Cooper and Herskovits formula (5) for approximating the query mean and is easily implemented for discrete networks. The technique is also applicable for continuous and hybrid networks, but implementation may be less straightforward.\n• Adjustments to improve accuracy, motivated by asymptotic theory for discrete networks. This theory also provides rates of convergence for the approximations.\n• Numerical comparisons of the network doubling and delta methods, showing superior accuracy of the former in simple networks.\nWe do not recommend that network doubling replace delta in all applications. If the effective sample size is large, then both approaches may provide adequate approximations and the choice between them may depend primarily on computational complexity. If the network is large, then delta may have an advantage. If the effective sample size is small or the network is not large, then doubling may be the better choice."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We are grateful for helpful comments from the anonymous reviewers. We acknowledge support provided by NSERC, iCORE, and the Alberta Ingenuity Centre for Machine Learning."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2009,
    "abstractText" : "A Bayesian belief network models a joint distribution with an directed acyclic graph representing dependencies among variables and network parameters characterizing conditional distributions. The parameters are viewed as random variables to quantify uncertainty about their values. Belief nets are used to compute responses to queries; i.e., conditional probabilities of interest. A query is a function of the parameters, hence a random variable. Van Allen et al. (2001, 2008) showed how to quantify uncertainty about a query via a delta method approximation of its variance. We develop more accurate approximations for both query mean and variance. The key idea is to extend the query mean approximation to a “doubled network” involving two independent replicates. Our method assumes complete data and can be applied to discrete, continuous, and hybrid networks (provided discrete variables have only discrete parents). We analyze several improvements, and provide empirical studies to demonstrate their effectiveness.",
    "creator" : "TeX"
  }
}