{
  "name" : "1206.3261.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning When to Take Advice: A Statistical Test for Achieving A Correlated Equilibrium",
    "authors" : [ "Greg Hines" ],
    "emails" : [ "ggdhines@cs.uwaterloo.ca", "klarson@cs.uwaterloo.ca" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We study a multiagent learning problem where agents can either learn via repeated interactions, or can follow the advice of a mediator who suggests possible actions to take. We present an algorithm that each agent can use so that, with high probability, they can verify whether or not the mediator’s advice is useful. In particular, if the mediator’s advice is useful then agents will reach a correlated equilibrium, but if the mediator’s advice is not useful, then agents are not harmed by using our test, and can fall back to their original learning algorithm. We then generalize our algorithm and show that in the limit it always correctly verifies the mediator’s advice."
    }, {
      "heading" : "1 Introduction",
      "text" : "In settings where agents repeatedly interact with each other (for example, through a repeated game), there are great opportunities for learning since agents are able to adapt their strategies given the history of play. This problem has garnished a lot of attention from several research communities, including the AI community and the game theory community. While many criteria have been proposed for measuring the success of learning approaches, one commonly used measure is whether the agents learn how to best-respond to the strategies being played by the others. That is, does the learning process converge to an equilibrium.\nIn this paper we study the problem of agents interacting with each other in a repeated game setting, but we introduce a third party mediator or advisor who makes strategy suggestions to the agents. Ideally, by following the suggestions of the mediator, agents will be able to learn how to play against each other, possibly even reaching mutually beneficial outcomes which would not have been possible without the mediation. That is, our goal is for the agents to learn and adapt so that they find a correlated equilibrium [1].\nHowever, a mediator is only useful if it can make good sug-\ngestions. Even if a mediator tries to make good suggestions it may be prevented by coding errors, memory limitations, etc. For an agent to accept a mediator’s suggestions, there must be some way for the agent to verify that the suggestions are reasonable. A mediator might not be willing to share its code with the agents, or be aware of its own limitations. Therefore, for a truly robust system, the agents themselves must have a way of checking the mediator’s suggestions.\nThus, this paper introduces a statistical test based on hypothesis testing that, with high probability, can verify the mediator’s suggestions. While hypothesis testing has been proposed in the multiagent learning literature as a tool that agents might use to learn how to play Nash equilibria [5], to the best of our knowledge it has never been applied for validating a mediator’s advice. Based on our test, we propose an algorithm that allows agents to converge to the mediator’s suggestion if it is a correlated equilibrium and otherwise, in the limit, be no worse off for having used our algorithm. We then generalize this algorithm to a more theoretical setting where we show that with probability one, in the limit, our test will always be able to correctly verify the mediator’s suggestions. This provides a method for achieving convergence to a specific correlated equilibrium."
    }, {
      "heading" : "2 Background",
      "text" : "In this section we introduce the key concepts and assumptions used in this paper.\nA n-agent stage game is a tuple G = 〈N, A = A1 × . . . × An, u1, . . . , un〉, where N = {1, . . . , n} is the set of agents, Ai is the set of possible actions for agent i and A is the set of possible joint actions, and ui : A → R is the utility function for agent i. Without loss of generality, we assume that all utilities are greater than or equal to 0. A specific action for agent i is ai ∈ Ai, and a joint action is a = (a1, . . . , an). We assume that A is public knowledge but the agents’ utility functions are private.\nEach agent chooses its actions according to some strategy. A strategy for agent i, σi, is a probability distribution over Ai, stating with what probability the agent will play each\npossible action. The set of all possible strategies for agent i is Σi. The vector σ = (σ1, . . . ,σn) is a strategy profile which specifies a strategy for each agent and Σ is the set of all possible strategy profiles. We use σ−i to denote (σ1, . . . ,σi−1,σi+1, . . . ,σn).\nGiven a strategy profile σ, we define the expected utility for agent i as\nui(σ) = ∑\na=(a1,...,an)∈A\nui(a)Π n j=1σj(aj). (1)\nEach agent’s utility is dependent not just on its own actions, but also on the actions taken by all other agents. We assume agents are rational, i.e., given σ−i, agent i will choose a strategy which maximizes its expected utility.\nIn our model we introduce a third-party mediator, M. The mediator knows the utility functions for all agents, but is not affected by the game’s outcome. Instead M makes suggestions to each agent as to what action it should take, where these suggestions are instantiations of a correlated strategy. Definition 1. A correlated strategy, σA, is a probability distribution over A. We let s ∈ A denote an instantiation of σA. The conditional correlated strategy σA−i(s−i|si) is the conditional probability of the joint signal (si, s−i) given the signal si, and σA−i(si) is the set of all conditional probabilities given si.\nNote that σi is a probability distribution over Ai while σA is a probability distribution over A.\nWe assume that M’s correlated strategy is public knowledge, but the actual instantiation, s, is not. In particular we assume that M sends each agent i a private signal, si, based on s.\nThe agents are under no obligation to follow the mediator’s signals. It is up to the mediator to pick a correlated strategy that a rational agent would be willing to follow. Note that our type of a mediator is different than Monderer and Tennenholtz’s, where agents must agree to follow the mediator’s suggested actions before knowing what they are [11]. Definition 2. A correlated strategy σ∗A = {σA(a)|a ∈ A} is a correlated equilibrium if for every agent i and every si ∈ Ai,\n∑\ns−i∈A−i\nσ∗A−i(s−i|si)ui(si, s−i) (2)\n≥ ∑\ns−i∈A−i\nσ∗A−i(s−i|si)ui(a ′ i, s−i),\nfor all a′i ∈ Ai [1]. The set of all correlated equilibria in G is C(G).\nIf all of agent i’s opponents are following a correlated equilibrium σ∗A, it is rational for agent i to also follow σ∗A.\nIn this paper, we are interested in a setting where agents have the ability to learn and adapt to the actions taken by others. Thus, we study repeated games. A repeated game Gr = (G1, G2, . . .) is an infinite sequence of the stage game G played repeatedly. Agent i’s action at time t is ati and the joint action at time t is at. The history of joint actions, hist(t) = {a1, . . . , at−1}, is a record of the joint action taken at each iteration until time t. The empirical, or observed, percentage of play of joint actions, σhist(t)A , is the percentage of time each joint action has been played as of time t. Agents may learn from previous iterations of the game to try and improve their strategy. Specifically, we assume that agent i has a learning algorithm Li : hist(t) → Σi, that helps agent i select a strategy for time t.\nLet σtA be the actual correlated strategy at time t, i.e. the one agents are actually using and not necessarily the one based on M’s suggestions. We say that σtA converges to a correlated equilibrium if for some σ∗A ∈ C(G), limt→∞ σtA = σ ∗ A. Thus, our algorithm is differentiated from algorithms that achieve convergence to the set of correlated equilibrium, for example [4, 8]."
    }, {
      "heading" : "3 Setup",
      "text" : "The setting for our paper is a repeated game Gr with a mediator, M. As illustrated for the two agent case in Figure 1, time t will begin with the mediator giving each agent a suggested action, sti. Agents will then simultaneously choose their action, ati, which may or may not be sti. If agent i chooses not to follow M’s signal, it can instead use a learning algorithm, Li, which we assume is independent of M’s signals, to select an action. Based on the actual joint action, each agent will then receive some utility and the process repeats. The mediator’s signal to each agent is private information, known only to that agent and the mediator, as is the agent’s utility function. However, the action set for each agent is public knowledge, as is the action taken by each agent during a turn.\nThe mediator’s signals are based on a selected correlated strategy, σMA , which is constant throughout the repeated game. Although ideally the mediator will suggest a correlated strategy that is also a correlated equilibrium, each\nagent still needs to verify that the mediator has actually done so.\nOur aim is to design an algorithm that achieves the following goals.\nFirst goal: If σMA is a correlated equilibrium then σtA, the actual correlated strategy which is not necessarily σMA , will converge to σMA .\nSecond goal: If σMA is not a correlated equilibrium, agents should be no worse off, in the limit, for having used our algorithm.\nIn Section 4, we present an algorithm, Λ, that achieves these goals with high probability. In Section 5, we generalize Λ so that, with probability one, in the limit, it will achieve both goals. Since each agent will be using Λ independently, we refer to Λi as the instance of the algorithm being run by agent i and Λ as the joint algorithm.\nThe algorithm is based on the concept of givingM the benefit of the doubt; until there is reason to believe otherwise, agents assume that σMA is a correlated equilibrium and follow M’s signals. Specifically, agents will assume that the following conditions hold.\nCondition 1: The correlated strategy σMA is a correlated equilibrium.\nCondition 2: All other agents are following the signals based on σMA .\nAgents test whether these conditions hold during an initial period of play called a sampling test which has a fixed length of lT . If, at the beginning of the sampling test, agent i decides that one of the conditions does not hold, it will not follow M’s signals and instead will use an individual “fall-back” strategy, γi, chosen uniformly at random. At the end of the sampling test, all agents who still believe that both conditions hold will continue to follow M’s signals. All other agents will start using their original learning algorithm. The algorithm Λi is correct if and only if, at the end of the sampling test, it correctly determines whether both conditions hold. The joint algorithm, Λ, is correct if and only if Λi is correct for all i."
    }, {
      "heading" : "4 The Initial Algorithm",
      "text" : "In this section, we describe how our initial algorithm works. As a first step in Λi, agent i will check to see if Equation 2 holds for all si ∈ Ai. If Equation 2 does not hold, agent i will know that Condition 1 cannot be true. In this case, agent i will use a “fall-back” strategy, γi ∈ Σi, picked uniformly at random, for the rest of the sampling test. If Equation 2 does hold, agent i must check to see if Condition 2 is true and will continue to follow M’s signals throughout the sampling test.\nSince the utilities for each agent, as well as the signals they receive each turn, are private, there may be no way to prove or disprove Condition 2 with absolute certainty at any finite point during the game. The best Λi can do is reach a probabilistic conclusion. Since joint actions are public knowledge, Λi can compare the empirical percentages of play for the duration of the sampling test against the percentages predicted by σMA . If the difference between these two values is statistically significant, there is a high probability that at least one agent has stopped following the mediator’s signals.\nTo test if there is a difference, agent i assumes there is some fixed but unknown correlated strategy σ̃A that all agents were actually using for the sampling test, where σ̃A may or may not be σMA . We are now able to use hypothesis testing, where our null hypothesis is that σMA is equal to σ̃A, i.e.,\nH0 : σ M A = σ̃A, (3)\nand our alternative hypothesis is that σMA is not equal to σ̃A, i.e.,\nH1 : σ M A '= σ̃A. (4)\nThe test statistic used is Pearson’s χ2 test,\nT = ∑\na∈A′\n(X(a) − E(a))2\nE(a) , (5)\nwhere A′ is any subset of A such that |A′| = |A| − 1, X(a) = lTσ hist(lT ) A (a) is the actual frequency of play of a ∈ A′ during the sampling test, E(a) = lTσMA (a) is the expected frequency of play according to σMA , and where lT is the length of the sampling period [12]. Note that σhist(lT )A is based on a sampling from σ̃A of size lT . For now we assume that σMA (a) > 0 for all a ∈ A. We relax this assumption later. The Pearson’s χ2 test has (in the limit) a probability distribution function of\nχ2df + χ 2 NCP,1, (6)\nwhere the first distribution has df = |A|−2 degrees of freedom, and the second distribution has 1 degree of freedom and a non-centrality parameter of NCP [9].\nIf H0 is true, NCP = 0. Assuming that H0 is true, we choose a significance level for rejection of the null hypothesis of α < 1 and a corresponding critical value of c(α), i.e., we reject the null hypothesis when T ≥ c(α). In this case, the probability of incorrectly rejecting H0 (known as a Type 1 error) is p1 = α. If H1 is actually true, we err when T < c(α) and we do not reject H0 (a Type 2 error). When H1 is true, NCP > 0. Since the non-centrality parameter determines how much the probability distribution in Equation 6 gets adjusted, determining NCP helps determine the probability of a Type 2 error.\nThe equation for NCP is NCP = t ∗ δ, where δ, the sensitivity parameter, is a measure of the difference between σMA and σ̃A given by\nδ(σMA , σ̃A) = ∑\na∈A\n(σ̃A(a) − σMA (a)) 2\nσMA (a) . (7)\nFor a given value of δ, say δ̂, if\nδ(σMA , σ̃A) ≥ δ̂, (8)\nthen the probability of a Type 2 error is bounded by some value β(δ̂) < 1, whose value is normally found via numerical computation [9]. Since β is also a function of lT and α, we refer to it as β(lT ,α, δ).\nSince agents do not know whether their opponents are following the mediator’s suggestions, agents do not know the exact value for σ̃A, and therefore, it is impossible to choose an appropriate value for δ̂ so that Equation 8 is guaranteed to hold. Instead, agents can consider a different question: what is the worst case situation under which Equation 8 does not hold? To answer this question, consider the set of all agents for whom Equation 2 does not hold, NB ⊆ N . Let (σMA−NB , γNB ) be the actual correlated strategy for the duration of the sampling test, i.e., a combination of those agents who will follow M’s signals and those who will rely on their fall-back strategy. Let ΣNB be the set of all possible joint strategies for agents in NB , and\nΣNB (σ M A , δ)\n= {γNB ∈ ΣNB |δ(σ M A , (σ M A−NB , γNB )) < δ} (9)\nbe the set of all possible joint strategies for agents in NB which would result in Equation 8 not holding. Let µ(ΣNB ) and µ(ΣNB (σMA , δ)) be the Lebesgue measures of ΣNB and ΣNB(σMA , δ), respectively. Then, since γi is chosen uniformly at random, the probability of σNB being in ΣNB (σ M A , δ) is\nψ(ΣNB ) = µ(ΣNB (σ M A , δ))\nµ(ΣNB ) . (10)\nSince agents do not know NB , they consider the worst case scenario,\nψ = max N ′⊆N ψ(ΣN ′). (11)\nIf we assume that whenever Equation 8 does not hold and σ̃A '= σMA , a Type 2 error is always made, then the probability of a Type 2 error is at most\np2 ≤ (1 − ψ) · β(δ̂) + ψ. (12)\nThat is, Equation 8 holds with at least a probability of ψ and when it does, the probability of a Type 2 error is at most β(δ̂) and with a probability of at most ψ, Equation 8 does not hold.\nIf we do not assume that σMA (a) > 0 for all a ∈ A, then Equations 5 and 7 may contain division by zero. To deal with this, we ignore all a ∈ A such that σM(a) = 0. If ζ = {a ∈ A|σM(a) = 0}, then the summations in Equations 5 and 7 need to exclude all a ∈ ζ, and df in Equation 6 now equals |A| − 2 − |ζ|. If the null hypothesis is correct then σMA (a) = 0 implies that σ hist(lT ) A (a) = 0 for all a ∈ ζ. Alternatively, if there exists a′ ∈ A such that\nσhist(lT )A (a ′) > 0 while σMA (a′) = 0, the alternative hypothesis must be correct. Hence, both of these cases do not present problems.\nThe only other case is if for all a ∈ A such that σMA (a) = 0, σhist(lT )A (a) = 0 but, unknown to the agents, the alternative hypothesis is correct. In this case, a Type 2 error may occur. To find the probability of this case happening, we first determine the probability of at ∈ ζ. Since any agent who rejects M’s suggested strategy chooses its new strategy uniformly at random, the probability, P , that at ∈ ζ for t ≤ lT is\nP ≥ ∑\na∈ζ\nmin N ′⊆N σA −N′\n(a−N ′) 1\n|AN ′ | , (13)\nwhere minN ′⊆N is considered since agents do not know NB . Therefore, the probability that at '∈ ζ for all t ≤ lT is at most (1 − P)lT and the overall probability of a Type 2 error is at most\np2 ≤ (1 − P) lT [(1 − ψ) · β + ψ] . (14)\nTo accommodate the worst case, we assume equality holds in Equation 14. Note that p1 has not changed. For simplicity, we assume that p1 = p2 = p, and refer to p as the overall probability of error.\nIt is possible to rearrange β(lT ,α, δ) to express lT as a function of α, β and δ, i.e lT (α,β, δ). As a result, lT is the sample size needed to perform the test with at most a probability of error (of either Type 1 or Type 2) of p.\nIf all agents are to use the same value for lT , they must also have the same value for β. This in turn requires them to have the same value for ψ. To achieve this, in Equations 11 and 13, agent i will consider all possible N ′, including those containing agent i."
    }, {
      "heading" : "4.1 Examples",
      "text" : "In this section we provide two examples to illustrate how our test would work.\nExample 1: Consider the game in Figure 2.\nLet A = {(a1,1, a2,1), (a1,1, a2,2), (a2,1, a2,1), (a1,2, a2,2)}. Suppose that M announces a correlated strategy, σMA = {1/18, 5/18, 2/18, 10/18}. Note that σMA is a correlated equilibrium.\nSuppose the agents choose p = 0.1 and δ = 0.01. Agents must now determine the critical value for rejection, c(α),\nand the length of the sampling test, lT . Since p1 = α, α = 0.1. For 3 degrees of freedom, c(α) = 6.25. Since σMA (a) > 0 for all a, we can calculate β by Equation 12. We calculate Equation 11 by numerical computation to find ψ ≈ 0.09429. Therefore, β = 0.0063. In practice, lT (α,β, δ) would now be solved by some method of numerical computation [9]. For simplicity, we used the tables in Cohen to obtain a value of lT = 2100 [2].\nSuppose that after 2100 iterations, we have obtained an empirical frequency of play θhist(2101)A = {96, 601, 224, 1179}. Using Equation 5, we obtain a test statistic value of 4.678. Since this is lower than the critical value, both agents do not reject the null hypothesis and continue to use M’s signals.\nExample 2: Consider a different example based on the same game where M announces a correlated strategy of σMA = {2/18, 10/18, 1/18, 5/18}. In this case, σMA is not a correlated equilibrium. Specifically, while Equation 2 is satisfied for Agent 1, it is not satisfied for Agent 2. Hence, Agent 2 will use a random fall-back strategy. Suppose γ2 = (3/4, 1/4).\nFor this example, the length of the test has not changed. Suppose we find an empirical frequency of θhist(2101)A = {1050, 350, 525, 175} after 2100 turns. Since Agent 2 already knows that σMA is not a correlated equilibrium, it will not perform the test. Agent 1 will obtain a test statistic value of 5953.3. This is well above the critical value and so Agent 1 will reject the null hypothesis, i.e., it will stop following the signals of the mediator.\nNote that, as we have stated our algorithm, Agent 1 will only know that there is a probability of at most 0.1 of incorrectly rejecting the null hypothesis. We have not accounted for the fact that the test statistic value is much higher than the critical value. An additional test that could be run after the null hypothesis is rejected is the calculation of the p-value. The p-value is the smallest α value that would still allow us to reject the hypothesis [12]. In the case of the above example, the p-value would be very small, and Agent 1 could be very certain that σMA is not a correlated equilibrium."
    }, {
      "heading" : "5 Repeated Testing",
      "text" : "The limitation of our basic test is that there is always some positive probability of error. This is due to the need to pick values for 1−p and δ that are both greater than 0. Since we can pick any such values for 1 − p and δ, this is not much of a practical limitation, however we may wish to achieve a stronger theoretical result. Our goal is to have agents converge to playing σMA if it is a correlated equilibrium. If σMA is not a correlated equilibrium, then the agents’ utility should be no worse off for having used our algorithm. This leads to the idea of repeated testing, where throughout the repeated game, agents will use multiple iterations of Λi.\nThe set of repeated sampling tests is R = {R1, R2, . . .},\nwhere Rj = {bRj , lRj}, bRj is the first time period in Rj , and lRj is the length of Rj . The instance of Λi during test Rj is denoted by Λ Rj i . The repeated tests are not contiguous. A simple example is shown in Figure 3, where the timeline represents a repeated game up to 7 iterations. The grey areas represent sampling test iterations. For example, R2 = {bR2 , lR2} = {4, 2}, meaning that the second test iteration begins at time period 4 and lasts for 2 iterations of the repeated game.\nThe parameters, δ and p, can be set to depend on the test iteration, i.e. δ(Rj) and p(Rj). Each test period must be identical for each agent, i.e. Rj must be the same for all agents. This means that δ(Rj) and p(Rj) must be the same for all agents. The parameters are chosen such that\nlim j→∞ δ(Rj) = 0, (15) ∞ ∑\nj=1\np(Rj) < ∞. (16)\nFor example, we can let δ(Rj) = 1/j and p(Rj) = 1/2j. Finally, we assume that each agent’s fall-back strategy is fixed. That is γRji = γ Rj′ i , for all j, j′.\nOur first result is that an agent will not draw the wrong conclusion about the mediator too often. Theorem 1. In the limit, with probability one, there will only be a finite number of tests where ΛRj is incorrect.\nProof. Let σMA be the correlated strategy suggested by M. Consider the following two cases:\nσMA is a correlated equilibrium: For test Rj , the probability of ΛRji making a Type 1 error, p1(Rj), is equal to p(Rj). By the Borel-Cantelli lemma, with probability one, there will only be a finite number of times ΛRji is incorrect, i.e. makes a Type 1 error. 1 This reasoning can be applied to all agents, and therefore with probability one there will only be a finite number of times ΛRj is incorrect.\nσMA is not a correlated equilibrium: If σMA is not a correlated equilibrium, then some subset of agents, N ′ ⊆ N , will use their fall-back strategies instead of following the mediator’s signals. The resulting correlated strategy for every test iteration will be (σMA −N′ , γN ′).\nSince γN ′ is fixed, by Equation 15, there exists a finite j∗\n1Borel-Cantelli Lemma: Let {Et}∞0 be a sequence of independent events and P (Et) be the probability of the event Et occurring. If P ∞\nt=0 P (Et) < ∞, then with probability one, only\na finite number of the events will occur.\nsuch that for all j ≥ j∗,\nδ(σMA , (σ M A −N′ , γN ′)) ≥ δ(Rj). (17)\nLet ψ(Rj) be the value of ψ, according to Equation 11, during the sampling test Rj . Starting at Rj∗ , we know that, with probability one, Equation 8 holds and therefore, since ψ(Rj) is the probability of Equation 8 not holding, ψ(Rj) = 0, for all j ≥ j∗. Therefore, the probability of a Type 2 error starting at Rj∗ is\np2 = ∞ ∑\nj=j∗\n(1 − P)lT β. (18)\nNote that P , lT and β are all functions Rj , however we omit the notation (Rj) for clarity. Since β is less than 1,\np2 ≤ ∞ ∑\nj=j∗\n(1 − P)lT [(1 − ψ) · β + ψ] (19)\n= ∞ ∑\nj=j∗\np(Rj), (20)\nwhere ψ, as calculated by Equation 11, is also a function of Rj . Therefore, by Equation 16 and the Borel-Cantelli lemma, with probability one, there will only be a finite number of times ΛRji is incorrect, i.e. makes a Type 2 error. Again, this reasoning can be generalized to all agents and therefore, there will only be a finite number of times ΛRj is incorrect.\nWe now examine the behaviour of agents between sampling tests. The periods between test iterations are called free periods. The set of free periods is F = {F1, . . .} where Fj = {bFj , lFj}. Thus Gr = {R1, F1, R2, F2, . . .}. For example, in Figure 3, the first free period, F1, would be {bF1 , lF1} = {2, 2}. If Λ Rj i did not reject the null hypothesis, agent i continues to follow M’s signals for all of Fj . If ΛRji did reject the null hypothesis, agent i relies on its learning algorithm Li for Fj . We assume that Li is flexible at the beginning of each free period [3]. Definition 3. The learning algorithm Li is flexible if at the beginning of every free period Fj ,\nLi(hist(bFj )) = Li(hist(1)). (21)\nTherefore, during each free period, Li does not base its actions on what has happened before time bFj .\nFor example, Li may be a trigger strategy, but that trigger may not be based on anything that has happened in a previous sampling test or free period.\nWe require that\nlim j→∞\n∑j j′=1 lRj\n∑j j′=1 lFj\n= 0, (22)\nfor example lFj = l2Rj . This means that, in the limit, the length of the sampling periods is negligible compared to the length of the free periods. We also require that\nlim j→∞ lRj j = ∞. (23)\nThis means that the length of the sampling tests grows at faster than a linear rate. The specific values for lRj and lFj would have to be agreed upon by all agents. Definition 4. Let θexp(t1,t2)A be the expected frequency of play from time t1 to t2, i.e., the expected number of times each joint action a ∈ A gets played between times t1 and t2 inclusive. If t1 is not given, we assume t1 = 1. Similarly, let θexp(Fj ,...,Fj′ )A be the expected frequency of play during the free periods Fj through Fj′ , inclusive.\nSince the frequency of play depends on the algorithms the agents are using, let θexp(t)A (L) be the expected frequency of play from time 1 to t assuming that agents use the joint learning algorithm L for the whole period.\nFor simplicity in all of the following proofs, we assume that t always corresponds to the beginning of a sampling period. Let j(t) be the index of the last free period before t.\nThe first step is to show that if M suggests a correlated equilibrium, agents will converge to it. Theorem 2. If the correlated strategy suggested by M, σMA , is a correlated equilibrium, then with probability one,\nlim t→∞\nσtA = σ M A . (24)\nProof. If σMA is a correlated equilibrium then by Theorem 1, with probability one, after some finite point Λ will always correctly determine that σMA is a correlated equilibrium. As a result, with probability one, after some finite point, all agents will choose to follow the mediator’s signals during the free periods.\nOur next result is a technical lemma which shows that in the limit, agents are not harmed by taking time out to do the sampling tests. Lemma 1. In the limit, there is no difference between the average utility from agents using L for the whole repeated game and just for the free periods, i.e.,\nlim t→∞\n[\nui\n(\nθexp(t)A (L)\nt\n)\n− ui\n(\nθ exp(F1,...,Fj(t)) A (L)\nt\n)]\n= 0.\n(25)\nFurthermore, this is true even when excluding the first j∗ − 1 free periods, for some j∗ > 1, i.e.,\nlim t→∞\n[\nui\n(\nθexp(t)A (L)\nt\n)\n− ui\n(\nθ exp(Fj∗ ,...,Fj(t)) A (L)\nt\n)]\n= 0.\n(26)\nThe proof is given in the Appendix.\nFinally, we need to show that if σMA is not a correlated equilibrium, agents are no worse off, on average, for having used Λ. Theorem 3. If the correlated strategy suggested by M, σMA , is not a correlated equilibrium, then with probability one,\nlim t→∞\n[\nui\n(\nθexp(t)A (Λ)\nt\n)\n− ui\n(\nθexp(t)A (L)\nt\n)]\n≥ 0. (27)\nTherefore, in the limit, agent i will be no worse off for using Λ instead of Li.\nProof. If σMA is not a correlated equilibrium, by Theorem 1, with probability one, starting at some sampling test, say Rj∗ , Λ will always correctly determine that σMA is not a correlated equilibrium.\nConsider θA with respect to some arbitary a ∈ A, denoted by θa. We start by breaking the game down into the sequence of sampling tests and free periods. That is, θexp(t)a (Λ) = θ exp(R1,F1,...,F (t)) a (Λ). For t ≥ t(j∗), the utility can be split up into the utility for the sampling tests and free periods before Rj∗ and for those starting at Rj∗ i.e.,\nlim t→∞\n[\nui\n(\nθ exp(R1,F1,...,Rj∗−1,Fj∗−1) a (Λ)\nt\n)\n+ ui\n(\nθ exp(Rj∗ ,Fj∗ ,...,F (t)) a (Λ)\nt\n)]\nSince θexp(R1,F1,...,Rj∗−1,Fj∗−1)A (Λ) is constant, in the limit, the first term is 0, and so we are interested in\nlim t→∞ ui\n(\nθ (Rj∗ ,Fj∗ ,...,F (t)) a (Λ)\nt\n)\nThe expected frequency can be split up into the expected frequency for the sampling periods and for the free periods. Since Λ always determines that σMA is not a correlated equilibrium, during all the free periods agents will always use L, and so we are interested in\nlim t→∞\n[\nui\n(\nθ (Rj∗ ,...,R(t)) a (Λ)\nt\n)\n+ ui\n(\nθ (Fj∗ ,...,F (t)) a (L)\nt\n)]\nSince we assumed that all utilities are nonnegative, we may discard the first term, and thus have\nlim t→∞ ui\n(\nθ (Fj∗ ,...,F (t) a (L))\nt\n)\nTherefore, by Lemma 1, the theorem follows.\nTogether, Theorems 2 and 3 show that, with probability one, if σMA is a correlated equilibrium, agents will converge to it and if σMA is not a correlated equilibrium, agents will be no worse off in the long run for using Λ."
    }, {
      "heading" : "6 Conclusion",
      "text" : "The setting for this paper was a repeated game with a mediator. The mediator makes suggestions to the agents as to what actions to take. We presented a test that agents could use so that, with high probability, they could determine if the mediator’s suggestion was a correlated equilibrium. We then generalized our algorithm to incorporate repeated testing so that in the limit, with probability one, the test will always correctly determine whether the mediator’s suggested strategy is a correlated equilibrium. As a result, if the mediator suggests a correlated equilibrium, then agents will converge to it, and otherwise, be no worse off in the long run for having used our algorithm.\nWe envision several directions for future research. First, it might be possible to extend our algorithm to work in radically uncoupled environments, where agents are not aware of the existence of others. This would significantly decrease the knowledge requirements of our test. Second, we would like to extend our approach so that the mediator receives feedback from the agents themselves, which can be used to help select appropriate correlated strategies. We believe that the incentive issues in such an approach will be challenging. It may also be interesting to apply our approach to other solution concepts such as mediated equilibria [11].\nIn a more applied direction, it might be possible to generalize our approach so it can be used in a stochastic game setting. Thus, our approach could be combined with methods such as Q-learning [7]. Correlated equilibria have also been used in graphical games, which can be used to model many different settings [10]. Hence, applying our technique to graphical games may yield some interesting results. For example, network games use graphical games to help represent a variety of problems, from public good provision and trade to information collection [6]. These models can be hindered by a “fundamental theoretical problem: even the simplest games played on networks have multiple equilibrium[sic] which display a bewildering range of possible outcomes” [6]. Our model may help integrate correlated equilibria as a possible solution to this problem."
    }, {
      "heading" : "7 Acknowledgements",
      "text" : "Our thanks to Gord Hines for his statistical advice."
    }, {
      "heading" : "A Proof of Lemma 1",
      "text" : "Proof. Consider θ with respect to a ∈ A, denoted by θa. Since j∗ is fixed, θF1,...,Fj∗−1a (L) is constant, and therefore,\nlim t→∞\nθ exp(F1,...,Fj∗−1) a (L)\nt = 0, (28)\nand therefore, Equations 25 and 26 are equivalent.\nSince the utility functions are linear transformations, proving the following is sufficient, although not necessary, to prove that Equation 25 holds,\nlim t→∞\nθexp(t)a (L) − θ exp(F1,...,Fj(t)) a (L)\nt = 0. (29)\nSince L is flexible, it will, in expectation, always behave the same way during each free period. Specifically,\nθ exp(bFj ,bFj +lFj ) a (L) = θ exp(bF j′ ,bF j′ +lFj ) a (L), (30)\nfor all j′ such that lFj′ ≥ lFj . This relationship can be represented graphically, as shown in Figure 4, where for simplicity, we let w(j) = exp(bFj + lFj−1 , bFj + lFj ), where lF0 = 0. Therefore,\nθ exp(F1,...,Fj(t)) a (L) =\nj(t) ∑\nj=1\n(j(t) − j + 1)θw(j)a (L).\nNote that θw(j)a will be “represented” more than θw(j ′)\na for j < j′ and any finite t. In order for Equation 29 to hold, in the limit, all θw(j)a be must represented equally, i.e.\nlim t→∞\nj(t) − j + 1\nt = lim t→∞\nj(t) − j′ + 1\nt , (31)\nfor all j, j′. Consider t(j) = j−1(t), i.e. the first time index after the jth free period has ended:\nt(j) = j ∑\nj′=1\n(lRj + lFj ) ≥ j ∑\nj′=1\nlRj . (32)\nBy Equation 23, limj→∞ t(j)j = ∞, and therefore,\nlim t→∞\nj(t) − j + 1\nt ≤ lim t→∞\nj(t)\nt = 0. (33)\nTherefore, in the limit, all θw(j ′)\na will be represented equally. However, since\n∑j(t) j=1 lFj < t, each θ w(j) a will\nbe “underrepresented” compared to θta(L) for any finite t. However, in the limit, this is not the case since,\nlim t→∞\n∑j(t) j=1 lFj\nt = lim t→∞\n∑j(t) j=1 lFj\n∑j(t) j=1(lRj + lFj )\n= lim t→∞\n1 Pj(t)\nj=1 lRj Pj(t)\nj=1 lFj + 1\n= 1 (by Equation 22). (34)\nTherefore, in the limit θw(j)a will be represented equally compared to θta(L)."
    } ],
    "references" : [ {
      "title" : "Subjectivity and correlation in randomized strategies",
      "author" : [ "R. Aumann" ],
      "venue" : "Journal of Mathematical Economics, 1:67–96,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "Statistical Power Analysis for the Behavioral Sciences",
      "author" : [ "J. Cohen" ],
      "venue" : "2nd edition,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Combining expert advice in reactive environments",
      "author" : [ "D.P.D. Farias", "N. Megiddo" ],
      "venue" : "Journal of the ACM, 53(5):762–799,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Calibrated learning and correlated equilibrium",
      "author" : [ "D.P. Foster", "R. Vohra" ],
      "venue" : "Games and Economic Behavior, 21:40–55,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Learning, hypothesis testing, and Nash equilibrium",
      "author" : [ "D.P. Foster", "H.P. Young" ],
      "venue" : "Games and Economic Behavior, 45:73–96,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Network games",
      "author" : [ "A. Galeotti", "S. Goyal", "M.O. Jackson", "F. Vega- Redondo", "L. Yariv" ],
      "venue" : "Unpublished, Jan",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Correlated Q-learning",
      "author" : [ "A. Greenwald", "K. Hall" ],
      "venue" : "Proceedings of ICML-2003, pages 242–249, Washington, DC, USA,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "A simple adaptive procedure leading to correlated equilibrium",
      "author" : [ "S. Hart", "A. Mas-Colell" ],
      "venue" : "Econometrica, 68:1127–1150,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Continuous Univariate Distributions, volume",
      "author" : [ "N. Johnson", "S. Kotz", "N. Balakrishnan" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1995
    }, {
      "title" : "Correlated equilibria in graphical games",
      "author" : [ "S. Kakade", "M. Kearns", "J. Langford", "L. Ortiz" ],
      "venue" : "EC ’03: Proceedings of the 4th ACM Conference on Electronic Commerce, pages 42–47, New York, NY, USA,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Strong mediated equilibrium",
      "author" : [ "D. Monderer", "M. Tennenholtz" ],
      "venue" : "Proceedings of the 21st American Association of Artificial Intelligence Conference, Boston, MA, USA,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "All of Statistics",
      "author" : [ "L. Wasserman" ],
      "venue" : "Springer,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "That is, our goal is for the agents to learn and adapt so that they find a correlated equilibrium [1].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 4,
      "context" : "While hypothesis testing has been proposed in the multiagent learning literature as a tool that agents might use to learn how to play Nash equilibria [5], to the best of our knowledge it has never been applied for validating a mediator’s advice.",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 10,
      "context" : "Note that our type of a mediator is different than Monderer and Tennenholtz’s, where agents must agree to follow the mediator’s suggested actions before knowing what they are [11].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 0,
      "context" : "for all ai ∈ Ai [1].",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 3,
      "context" : "Thus, our algorithm is differentiated from algorithms that achieve convergence to the set of correlated equilibrium, for example [4, 8].",
      "startOffset" : 129,
      "endOffset" : 135
    }, {
      "referenceID" : 7,
      "context" : "Thus, our algorithm is differentiated from algorithms that achieve convergence to the set of correlated equilibrium, for example [4, 8].",
      "startOffset" : 129,
      "endOffset" : 135
    }, {
      "referenceID" : 11,
      "context" : "where A is any subset of A such that |A| = |A| − 1, X(a) = lTσ hist(lT ) A (a) is the actual frequency of play of a ∈ A during the sampling test, E(a) = lTσ A (a) is the expected frequency of play according to σ A , and where lT is the length of the sampling period [12].",
      "startOffset" : 266,
      "endOffset" : 270
    }, {
      "referenceID" : 8,
      "context" : "The Pearson’s χ2 test has (in the limit) a probability distribution function of χdf + χ 2 NCP,1, (6) where the first distribution has df = |A|−2 degrees of freedom, and the second distribution has 1 degree of freedom and a non-centrality parameter of NCP [9].",
      "startOffset" : 255,
      "endOffset" : 258
    }, {
      "referenceID" : 8,
      "context" : "then the probability of a Type 2 error is bounded by some value β(δ̂) < 1, whose value is normally found via numerical computation [9].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 8,
      "context" : "In practice, lT (α,β, δ) would now be solved by some method of numerical computation [9].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 1,
      "context" : "For simplicity, we used the tables in Cohen to obtain a value of lT = 2100 [2].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 11,
      "context" : "The p-value is the smallest α value that would still allow us to reject the hypothesis [12].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "We assume that Li is flexible at the beginning of each free period [3].",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "It may also be interesting to apply our approach to other solution concepts such as mediated equilibria [11].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 6,
      "context" : "Thus, our approach could be combined with methods such as Q-learning [7].",
      "startOffset" : 69,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "Correlated equilibria have also been used in graphical games, which can be used to model many different settings [10].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "For example, network games use graphical games to help represent a variety of problems, from public good provision and trade to information collection [6].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 5,
      "context" : "These models can be hindered by a “fundamental theoretical problem: even the simplest games played on networks have multiple equilibrium[sic] which display a bewildering range of possible outcomes” [6].",
      "startOffset" : 198,
      "endOffset" : 201
    } ],
    "year" : 2008,
    "abstractText" : "We study a multiagent learning problem where agents can either learn via repeated interactions, or can follow the advice of a mediator who suggests possible actions to take. We present an algorithm that each agent can use so that, with high probability, they can verify whether or not the mediator’s advice is useful. In particular, if the mediator’s advice is useful then agents will reach a correlated equilibrium, but if the mediator’s advice is not useful, then agents are not harmed by using our test, and can fall back to their original learning algorithm. We then generalize our algorithm and show that in the limit it always correctly verifies the mediator’s advice.",
    "creator" : "dvips(k) 5.96.1 Copyright 2007 Radical Eye Software"
  }
}