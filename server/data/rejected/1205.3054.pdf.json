{
  "name" : "1205.3054.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Approximate Modified Policy Iteration",
    "authors" : [ "Bruno Scherrer", "Victor Gabillon", "Mohammad Ghavamzadeh" ],
    "emails" : [ "Bruno.Scherrer@inria.fr", "Victor.Gabillon@inria.fr", "Mohammad.Ghavamzadeh@inria.fr", "Matthieu.Geist@supelec.fr" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Modified Policy Iteration (MPI) (Puterman & Shin, 1978) is an iterative algorithm to compute the optimal policy and value function of a Markov Decision Process (MDP). Starting from an arbitrary value function v0, it generates a sequence of value-policy pairs\nπk+1 = G vk (greedy step) (1) vk+1 = (Tπk+1) mvk (evaluation step) (2)\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nwhere G vk is a greedy policy w.r.t. vk, Tπk is the Bellman operator associated to the policy πk, and m ≥ 1 is a parameter. MPI generalizes the well-known dynamic programming algorithms Value Iteration (VI) and Policy Iteration (PI) for values m = 1 and m =∞, respectively. MPI has less computation per iteration than PI (in a way similar to VI), while enjoys the faster convergence of the PI algorithm (Puterman & Shin, 1978). In problems with large state and/or action spaces, approximate versions of VI (AVI) and PI (API) have been the focus of a rich literature (see e.g. Bertsekas & Tsitsiklis 1996; Szepesvári 2010). The aim of this paper is to show that, similarly to its exact form, approximate MPI (AMPI) may represent an interesting alternative to AVI and API algorithms.\nIn this paper, we propose three implementations of AMPI (Sec. 3) that generalize the AVI implementations of Ernst et al. (2005); Antos et al. (2007); Munos & Szepesvári (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al. (2006); Lazaric et al. (2010); Gabillon et al. (2011). We then provide an error propagation analysis of AMPI (Sec. 4), which shows how the Lp-norm of its performance loss can be controlled by the error at each iteration of the algorithm. We show that the error propagation analysis of AMPI is more involved than that of AVI and API. This is due to the fact that neither the contraction nor monotonicity arguments, that the error propagation analysis of these two algorithms rely on, hold for AMPI. The analysis of this section unifies those for AVI and API and is applied to the AMPI implementations presented in Sec. 3. We detail the analysis of the classification-based implementation of MPI (CBMPI) of Sec. 3 by providing its finite sample analysis in Sec. 5. Our analysis indicates that the parameter m allows us to balance the estimation error of the classifier with the overall quality of the value approximaar X iv :1 20 5.\n30 54\nv2 [\ncs .A\nI] 1\n8 M\nay 2\ntion. We report some preliminary results of applying CBMPI to standard benchmark problems and comparing it with some existing algorithms in (Scherrer et al., 2012, Appendix G)."
    }, {
      "heading" : "2. Background",
      "text" : "We consider a discounted MDP 〈S,A, P, r, γ〉, where S is a state space, A is a finite action space, P (ds′|s, a), for all (s, a), is a probability kernel on S, the reward function r : S × A → R is bounded by Rmax, and γ ∈ (0, 1) is a discount factor. A deterministic policy is defined as a mapping π : S → A. For a policy π, we may write rπ(s) = r ( s, π(s) ) and Pπ(ds ′|s) = P ( ds′|s, π(s) ) . The value of policy π in a state s is defined as the expected discounted sum of rewards received starting from state s and following the policy π, i.e.,vπ(s) = E [ ∑∞ t=0 γ\ntrπ(st)| s0 = s, st+1 ∼ Pπ(·|st) ] . Similarly, the action-value function of a policy π at a state-action pair (s, a), Qπ(s, a), is the expected discounted sum of rewards received starting from state s, taking action a, and then following the policy. Since the rewards are bounded by Rmax, the values and action-values should be bounded by Vmax = Qmax = Rmax/(1 − γ). The Bellman operator Tπ of policy π takes a function f on S as input and returns the function Tπf defined as ∀s, [Tπf ](s) = E [ rπ(s) + γf(s ′) | s′ ∼ Pπ(.|s) ] , or in compact form, Tπf = rπ + γPπf . It is known that vπ is the unique fixed-point of Tπ. Given a function f on S, we say that a policy π is greedy w.r.t. f , and write it as π = G f , if ∀s, (Tπf)(s) = maxa(Taf)(s), or equivalently Tπf = maxπ′(Tπ′f). We denote by v∗ the optimal value function. It is also known that v∗ is the unique fixed-point of the Bellman optimality operator T : v → maxπ Tπv = TG(v)v, and that a policy π∗ that is greedy w.r.t. v∗ is optimal and its value satisfies vπ∗ = v∗."
    }, {
      "heading" : "3. Approximate MPI Algorithms",
      "text" : "In this section, we describe three approximate MPI (AMPI) algorithms. These algorithms rely on a function space F to approximate value functions, and in the third algorithm, also on a policy space Π to represent greedy policies. In what follows, we describe the iteration k of these iterative algorithms."
    }, {
      "heading" : "3.1. AMPI-V",
      "text" : "For the first and simplest AMPI algorithm presented in the paper, we assume that the values vk are represented in a function space F ⊆ R|S|. In any state s, the action πk+1(s) that is greedy w.r.t. vk can be\nestimated as follows:\nπk+1(s) = arg max a∈A\n1\nM ( M∑ j=1 r(j)a + γvk(s (j) a ) ) , (3)\nwhere ∀a ∈ A and 1 ≤ j ≤ M , r(j)a and s(j)a are samples of rewards and next states when action a is taken in state s. Thus, approximating the greedy action in a state s requires M |A| samples. The algorithm works as follows. It first samples N states from a distribution µ, i.e., {s(i)}Ni=1 ∼ µ. From each sampled state s(i), it generates a rollout of size\nm, i.e., ( s(i), a\n(i) 0 , r (i) 0 , s (i) 1 , . . . , a (i) m−1, r (i) m−1, s (i) m\n) , where\na (i) t is the action suggested by πk+1 in state s (i) t , computed using Eq. 3, and r (i) t and s (i) t+1 are the reward and next state induced by this choice of action. For each s(i), we then compute a rollout estimate v̂k+1(s (i)) = ∑m−1 t=0 γ tr (i) t +γ mvk(s (i) m ), which is an un-\nbiased estimate of [( Tπk+1 )m vk ]\n(s(i)). Finally, vk+1 is computed as the best fit in F to these estimates, i.e.,\nvk+1 = FitF ({( s(i), v̂k+1(s (i)) )}N\ni=1\n) .\nEach iteration of AMPI-V requires N rollouts of size m, and in each rollout any of the |A| actions needs M samples to compute Eq. 3. This gives a total of Nm(M |A|+1) transition samples. Note that the fitted value iteration algorithm (Munos & Szepesvári, 2008) is a special case of AMPI-V when m = 1."
    }, {
      "heading" : "3.2. AMPI-Q",
      "text" : "In AMPI-Q, we replace the value function v : S → R with an action-value function Q : S × A → R. The Bellman operator for a policy π at a state-action pair (s, a) can then be written as\n[TπQ](s, a) = E [ rπ(s, a) +γQ(s ′, π(s′))|s′ ∼ P (·|s, a) ] ,\nand the greedy operator is defined as\nπ = GQ ⇔ ∀s π(s) = arg max a∈A Q(s, a).\nIn AMPI-Q, action-value functions Qk are represented in a function space F ⊆ R|S×A|, and the greedy action w.r.t. Qk at a state s, i.e., πk+1(s), is computed as\nπk+1(s) ∈ arg max a∈A Qk(s, a). (4)\nThe evaluation step is similar to that of AMPI-V, with the difference that now we work with stateaction pairs. We sample N state-action pairs from a distribution µ on S × A and build a rollout set\nDk = {(s(i), a(i))}Ni=1, (s(i), a(i)) ∼ µ. For each (s(i), a(i)) ∈ Dk, we generate a rollout of size m, i.e., ( s(i), a(i), r\n(i) 0 , s (i) 1 , a (i) 1 , · · · , s (i) m , a (i) m\n) , where the\nfirst action is a(i), a (i) t for t ≥ 1 is the action suggested by πk+1 in state s (i) t computed using Eq. 4, and r (i) t and s (i) t+1 are the reward and next state induced by this choice of action. For each (s(i), a(i)) ∈ Dk, we then compute the rollout estimate\nQ̂k+1(s (i), a(i)) = m−1∑ t=0 γtr (i) t + γ mQk(s (i) m , a (i) m ),\nwhich is an unbiased estimate of[ (Tπk+1) mQk ] (s(i), a(i)). Finally, Qk+1 is the best fit to these estimates in F , i.e.,\nQk+1 = FitF ({( (s(i), a(i)), Q̂k+1(s (i), a(i)) )}N\ni=1\n) .\nEach iteration of AMPI-Q requires Nm samples, which is less than that for AMPI-V. However, it uses a hypothesis space on state-action pairs instead of states. Note that the fitted-Q iteration algorithm (Ernst et al., 2005; Antos et al., 2007) is a special case of AMPI-Q when m = 1."
    }, {
      "heading" : "3.3. Classification-Based MPI",
      "text" : "The third AMPI algorithm presented in this paper, called classification-based MPI (CBMPI), uses an ex-\nplicit representation for the policies πk, in addition to the one used for value functions vk. The idea is similar to the classification-based PI algorithms (Lagoudakis & Parr, 2003; Fern et al., 2006; Lazaric et al., 2010; Gabillon et al., 2011) in which we search for the greedy policy in a policy space Π (defined by a classifier) instead of computing it from the estimated value or action-value function (like in AMPI-V and AMPI-Q).\nIn order to describe CBMPI, we first rewrite the MPI formulation (Eqs. 1 and 2) as\nvk = (Tπk) mvk−1 (evaluation step) (5) πk+1 = G [ (Tπk) mvk−1 ] (greedy step) (6)\nNote that in the new formulation both vk and πk+1 are functions of (Tπk)\nmvk−1. CBMPI is an approximate version of this new formulation. As described in Fig. 1, CBMPI begins with arbitrary initial policy π1 ∈ Π and value function v0 ∈ F .1 At each iteration k, a new value function vk is built as the best approximation of the m-step Bellman operator (Tπk)\nmvk−1 in F (evaluation step). This is done by solving a regression problem whose target function is (Tπk)\nmvk−1. To set up the regression problem, we build a rollout set Dk by sampling n states i.i.d. from a distribution µ.2 For each state s(i) ∈ Dk, we generate a rollout ( s(i), a\n(i) 0 , r (i) 0 , s (i) 1 , . . . , a (i) m−1, r (i) m−1, s (i) m\n) of size m,\nwhere a (i) t = πk(s (i) t ), and r (i) t and s (i) t+1 are the reward and next state induced by this choice of action. From this rollout, we compute an unbiased estimate v̂k(s\n(i)) of [ (Tπk) mvk−1 ] (s(i)) as\nv̂k(s (i)) = m−1∑ t=0 γtr (i) t + γ mvk−1(s (i) m ), (7)\nand use it to build a training set {( s(i), v̂k(s (i)) )}n i=1\n. This training set is then used by the regressor to compute vk as an estimate of (Tπk) mvk−1.\nThe greedy step at iteration k computes the policy πk+1 as the best approximation of G [ (Tπk) mvk−1 ]\nby solving a cost-sensitive classification problem. From the definition of a greedy policy, if π = G [ (Tπk) mvk−1 ] , for each s ∈ S, we have[ Tπ(Tπk) mvk−1 ] (s) = max\na∈A\n[ Ta(Tπk) mvk−1 ] (s). (8)\nBy defining Qk(s, a) = [ Ta(Tπk) mvk−1 ] (s), we may\n1Note that the function space F and policy space Π are automatically defined by the choice of the regressor and classifier, respectively.\n2Here we used the same sampling distribution µ for both regressor and classifier, but in general different distributions may be used for these two components.\nrewrite Eq. 8 as Qk ( s, π(s) ) = max\na∈A Qk(s, a). (9)\nThe cost-sensitive error function used by CBMPI is of the form\nLΠπk,vk−1(µ;π) = ∫ S [ max a∈A Qk(s, a)−Qk ( s, π(s) )] µ(ds).\nTo simplify the notation we use LΠk instead of LΠπk,vk−1 . To set up this cost-sensitive classification problem, we build a rollout set D′k by sampling N states i.i.d. from a distribution µ. For each state s(i) ∈ D′k and each action a ∈ A, we build M independent rollouts of size m+ 1, i.e.,3( s(i), a, r\n(i,j) 0 , s (i,j) 1 , a (i,j) 1 , . . . , a (i,j) m , r (i,j) m , s (i,j) m+1 )M j=1 ,\nwhere for t ≥ 1, a(i,j)t = πk(s (i,j) t ), and r (i,j) t and s (i,j) t+1 are the reward and next state induced by this choice of action. From these rollouts, we compute an unbiased estimate of Qk(s (i), a) as Q̂k(s (i), a) =\n1 M ∑M j=1R j k(s (i), a) where\nRjk(s (i), a) = m∑ t=0 γtr (i,j) t + γ m+1vk−1(s (i,j) m+1).\nGiven the outcome of the rollouts, CBMPI uses a costsensitive classifier to return a policy πk+1 that minimizes the following empirical error\nL̂Πk (µ̂;π) = 1\nN N∑ i=1 [ max a∈A Q̂k(s (i), a)− Q̂k ( s(i), π(s(i)) )] ,\nwith the goal of minimizing the true error LΠk (µ;π).\nEach iteration of CBMPI requires nm+M |A|N(m+1) (or M |A|N(m + 1) in case we reuse the rollouts, see Footnote 3) transition samples. Note that when m tends to ∞, we recover the DPI algorithm proposed and analyzed by Lazaric et al. (2010)."
    }, {
      "heading" : "4. Error propagation",
      "text" : "In this section, we derive a general formulation for propagation of error through the iterations of an AMPI algorithm. The line of analysis for error propagation is different in VI and PI algorithms. VI analysis is based on the fact that this algorithm computes the fixed point of the Bellman optimality operator, and this operator is a γ-contraction in max-norm (Bertsekas & Tsitsiklis, 1996; Munos, 2007). On the other\n3We may implement CBMPI more sample efficient by reusing the rollouts generated for the greedy step in the evaluation step.\nhand, it can be shown that the operator by which PI updates the value from one iteration to the next is not a contraction in max-norm in general. Unfortunately, we can show that the same property holds for MPI when it does not reduce to VI (i.e., m > 1).\nProposition 1. If m > 1, there exists no norm for which the operator that MPI uses to update the values from one iteration to the next is a contraction.\nProof. Consider a deterministic MDP with two states {s1, s2}, two actions {change, stay}, rewards r(s1) = 0, r(s2) = 1, and transitions Pch(s2|s1) = Pch(s1|s2) = Pst(s1|s1) = Pst(s2|s2) = 1. Consider the following two value functions v = ( , 0) and v′ = (0, ) with > 0. Their corresponding greedy policies are π = (st, ch) and π′ = (ch, st), and the next iterates of v and v′ can\nbe computed as (Tπ) mv =\n( γm\n1 + γm\n) and (Tπ′)\nmv′ =( γ−γm 1−γ + γ\nm 1−γm 1−γ + γ m\n) . Thus, (Tπ′) mv′−(Tπ)mv = ( γ−γm 1−γ γ−γm 1−γ ) while v′− v = ( − ) . Since can be arbitrarily small,\nthe norm of (Tπ′) mv′−(Tπ)mv can be arbitrarily larger than the norm of v − v′ as long as m > 1.\nWe also know that the analysis of PI usually relies on the fact that the sequence of the generated values is non-decreasing (Bertsekas & Tsitsiklis, 1996; Munos, 2003). Unfortunately, it can easily be shown that for m finite, the value functions generated by MPI may decrease (it suffices to take a very high initial value). It can be seen from what we just described and Proposition 1 that for m 6= 1 and∞, MPI is neither contracting nor non-decreasing, and thus, a new line of proof is needed for the propagation of error in this algorithm.\nTo study error propagation in AMPI, we introduce an abstract algorithmic model that accounts for potential errors. AMPI starts with an arbitrary value v0 and at each iteration k ≥ 1 computes the greedy policy w.r.t. vk−1 with some error ′ k, called the greedy step error. Thus, we write the new policy πk as\nπk = Ĝ ′kvk−1. (10)\nEq. 10 means that for any policy π′,\nTπ′vk−1 ≤ Tπkvk−1 + ′k.\nAMPI then generates the new value function vk with some error k, called the evaluation step error\nvk = (Tπk) mvk−1 + k. (11)\nBefore showing how these two errors are propagated through the iterations of AMPI, let us first define them\nin the context of each of the algorithms presented in Section 3 separately.\nAMPI-V: k is the error in fitting the value function vk. This error can be further decomposed into two parts: the one related to the approximation power of F and the one due to the finite number of samples/rollouts. ′k is the error due to using a finite number of samples M for estimating the greedy actions.\nAMPI-Q: ′k = 0 and k is the error in fitting the state-action value function Qk.\nCBMPI: This algorithm iterates as follows:\nvk = (Tπk) mvk−1 + k\nπk+1 = Ĝ ′k+1 [(Tπk) mvk−1]\nUnfortunately, this does not exactly match with the model described in Eqs. 10 and 11. By introducing the auxiliary variable wk ∆ = (Tπk)\nmvk−1, we have vk = wk + k, and thus, we may write\nπk+1 = Ĝ ′k+1 [wk] . (12)\nUsing vk−1 = wk−1 + k−1, we have\nwk = (Tπk) mvk−1 = (Tπk) m(wk−1 + k−1)\n= (Tπk) mwk−1 + (γPπk) m k−1. (13)\nNow, Eqs. 12 and 13 exactly match Eqs. 10 and 11 by replacing vk with wk and k with (γPπk) m k−1.\nThe rest of this section is devoted to show how the errors k and ′ k propagate through the iterations of an AMPI algorithm. We only outline the main arguments that will lead to the performance bound of Thm. 1 and report most proofs in (Scherrer et al., 2012). We follow the line of analysis developped by Thiery & Scherrer (2010). The results are obtained using the following three quantities:\n1) The distance between the optimal value function and the value before approximation at the kth iteration: dk ∆ = v∗ − (Tπk)mvk−1 = v∗ − (vk − k).\n2) The shift between the value before approximation and the value of the policy at the kth iteration: sk ∆ = (Tπk) mvk−1 − vπk = (vk − k)− vπk .\n3) The Bellman residual at the kth iteration: bk ∆ = vk − Tπk+1vk.\nWe are interested in finding an upper bound on the loss lk ∆ = v∗ − vπk = dk + sk. To do so, we will upper bound dk and sk, which requires a bound on the Bellman residual bk. More precisely, the core of our analysis is to prove the following point-wise inequalities for our three quantities of interest.\nLemma 1 (Proof in (Scherrer et al., 2012, Appendix A)). Let k ≥ 1, xk ∆ = (I − γPπk) k + ′k+1 and yk ∆ = −γPπ∗ k + ′k+1. We have:\nbk ≤ (γPπk)mbk−1 + xk,\ndk+1 ≤ γPπ∗dk + yk + m−1∑ j=1 (γPπk+1) jbk,\nsk = (γPπk) m(I − γPπk)−1bk−1.\nSince the stochastic kernels are non-negative, the bounds in Lemma 1 indicate that the loss lk will be bounded if the errors k and ′ k are controlled. In fact, if we define as a uniform upper-bound on the errors | k| and | ′k|, the first inequality in Lemma 1 implies that bk ≤ O( ), and as a result, the second and third inequalities gives us dk ≤ O( ) and sk ≤ O( ). This means that the loss will also satisfy lk ≤ O( ).\nOur bound for the loss lk is the result of careful expansion and combination of the three inequalities in Lemma 1. Before we state this result, we introduce some notations that will ease our formulation.\nDefinition 1. For a positive integer n, we define Pn as the set of transition kernels that are defined as follows:"
    }, {
      "heading" : "1) for any set of n policies {π1, . . . , πn},",
      "text" : "(γPπ1)(γPπ2) . . . (γPπn) ∈ Pn,\n2) for any α ∈ (0, 1) and (P1, P2) ∈ Pn × Pn, αP1 + (1− α)P2 ∈ Pn.\nFurthermore, we use the somewhat abusive notation Γn for denoting any element of Pn. For example, if we write a transition kernel P as P = α1Γ i + α2Γ jΓk = α1Γ i+α2Γ\nj+k, it should be read as there exist P1 ∈ Pi, P2 ∈ Pj, P3 ∈ Pk, and P4 ∈ Pk+j such that P = α1P1 + α2P2P3 = α1P1 + α2P4.\nUsing the notation introduced in Definition 1, we now derive a point-wise bound on the loss.\nLemma 2 (Proof in (Scherrer et al., 2012, Appendix B)). After k iterations, the losses of AMPI-V and AMPI-Q satisfy\nlk ≤ 2 k−1∑ i=1 ∞∑ j=i Γj | k−i|+ k−1∑ i=0 ∞∑ j=i Γj | ′k−i|+ h(k),\nwhile the loss of CBMPI satisfies\nlk ≤ 2 k−2∑ i=1 ∞∑ j=i+m Γj | k−i−1|+ k−1∑ i=0 ∞∑ j=i Γj | ′k−i|+ h(k),\nwhere h(k) ∆ = 2 ∑∞ j=k Γ j |d0| or h(k) ∆ = 2 ∑∞ j=k Γ j |b0|.\nRemark 1. A close look at the existing point-wise error bounds for AVI (Munos, 2007, Lemma 4.1) and API (Munos, 2003, Corollary 10) shows that they do not consider error in the greedy step (i.e., ′k = 0) and that they have the following form:\nlim supk→∞lk ≤ 2 lim supk→∞ k−1∑ i=1 ∞∑ j=i Γj | k−i|.\nThis indicates that the bound in Lemma 2 not only unifies the analysis of AVI and API, but it generalizes them to the case of error in the greedy step and to a finite horizon k. Moreover, our bound suggests that the way the errors are propagated in the whole family of algorithms VI/PI/MPI does not depend on m at the level of the abstraction suggested by Definition 1.4\nThe next step is to show how the point-wise bound of Lemma 2 can turn to a bound in weighted Lp-norm, which for any function f : S → R and any distribution µ on S is defined as ‖f‖p,µ ∆ = (∫ |f(x)|pµ(dx) )1/p . Munos (2003; 2007); Munos & Szepesvári (2008), and the recent work of Farahmand et al. (2010), which provides the most refined bounds for API and AVI, show how to do this process through quantities, called concentrability coefficients, that measure how a distribution over states may concentrate through the dynamics of the MDP. We now state a lemma that generalizes the analysis of Farahmand et al. (2010) to a larger class of concentrability coefficients. We will discuss the potential advantage of this new class in Remark 4. We will also show through the proofs of Thms. 1 and 3, how the result of Lemma 3 provides us with a flexible tool for turning point-wise bounds into Lp-norm bounds. Thm. 3 in (Scherrer et al., 2012, Appendix D) provides an alternative bound for the loss of AMPI, which in analogy with the results of Farahmand et al. (2010) shows that the last iterations have the highest impact on the loss (the influence exponentially decreases towards the initial iterations).\nLemma 3 (Proof in (Scherrer et al., 2012, Appendix C)). Let I and (Ji)i∈I be sets of positive integers, {I1, . . . , In} be a partition of I, and f and (gi)i∈I be functions satisfying\n|f | ≤ ∑ i∈I ∑ j∈Ji Γj |gi| = n∑ l=1 ∑ i∈Il ∑ j∈Ji Γj |gi|.\nThen for all p, q and q′ such that 1q + 1 q′ = 1, and for all distributions ρ and µ, we have\n‖f‖p,ρ ≤ n∑ l=1 ( Cq(l) )1/p sup i∈Il ‖gi‖pq′,µ ∑ i∈Il ∑ j∈Ji γj ,\n4Note however that the dependence on m will reappear if we make explicit what is hidden in the terms Γj .\nwith the following concentrability coefficients\nCq(l) ∆ =\n∑ i∈Il ∑ j∈Ji γ\njcq(j)∑ i∈Il ∑ j∈Ji γ j ,\nwith the Radon-Nikodym derivative based quantity\ncq(j) ∆ = max π1,··· ,πj ∥∥∥∥d(ρPπ1Pπ2 · · ·Pπj )dµ ∥∥∥∥ q,µ\n(14)\nWe now derive a Lp-norm bound for the loss of the AMPI algorithm by applying Lemma 3 to the pointwise bound of Lemma 2.\nTheorem 1 (Proof in (Scherrer et al., 2012, Appendix D)). Let ρ and µ be distributions over states. Let p, q, and q′ be such that 1q + 1 q′ = 1. After k iterations, the loss of AMPI satisfies\n‖lk‖p,ρ ≤ 2(γ − γk)\n( C1,k,0q ) 1 p\n(1− γ)2 sup1≤j≤k−1 ‖ j‖pq′,µ (15)\n+ (1− γk)\n( C0,k,0q ) 1 p\n(1− γ)2 sup1≤j≤k ‖ ′j‖pq′,µ + g(k),\nwhile the loss of CBMPI satisfies\n‖lk‖p,ρ ≤ 2γm(γ − γk−1)\n( C2,k,mq ) 1 p\n(1− γ)2 sup1≤j≤k−2 ‖ j‖pq′,µ\n(16)\n+ (1− γk)\n( C1,k,0q ) 1 p\n(1− γ)2 sup1≤j≤k ‖ ′j‖pq′,µ + g(k),\nwhere for all q, l, k and d, the concentrability coefficients Cl,k,dq are defined as\nCl,k,dq ∆ =\n(1− γ)2 γl − γk k−1∑ i=l ∞∑ j=i γjcq(j + d),\nwith cq(j) given by Eq. 14, and g(k) is defined as\ng(k) ∆ = 2γ\nk 1−γ ( Ck,k+1q ) 1 p min ( ‖d0‖pq′,µ, ‖b0‖pq′,µ ) .\nRemark 2. When p tends to infinity, the first bound of Thm. 1 reduces to ‖lk‖∞ ≤ 2(γ − γk) (1− γ)2 sup1≤j≤k−1 ‖ j‖∞ + 1− γk (1− γ)2 sup1≤j≤k ‖ ′j‖∞\n+ 2γk\n1− γ min(‖d0‖∞, ‖b0‖∞). (17)\nWhen k goes to infinity, Eq. 17 gives us a generalization of the API (m = ∞) bound of Bertsekas & Tsitsiklis (1996, Prop. 6.2), i.e.,\nlim sup k→∞\n‖lk‖∞ ≤ 2γ supj ‖ j‖∞ + supj ‖ ′j‖∞\n(1− γ)2 .\nMoreover, since our point-wise analysis generalizes those of API and AVI (as noted in Remark 1), the Lp-bound of Eq. 15 unifies and generalizes those for API (Munos, 2003) and AVI (Munos, 2007).\nRemark 3. Canbolat & Rothblum (2012) recently (and independently) developped an analysis of an approximate form of MPI. Also, as mentionned, the proof technique that we used is based on that of Thiery & Scherrer (2010). While Canbolat & Rothblum (2012) only consider the error in the greedy step and Thiery & Scherrer (2010) that in the value update, our work is more general in that we consider both sources of error – this is required for the analysis of CBMPI. Thiery & Scherrer (2010) and Canbolat & Rothblum (2012) provide bounds when the errors are controlled in maxnorm, while we consider the more general Lp-norm. At a more technical level, Th. 2 in (Canbolat & Rothblum, 2012) bounds the norm of the distance v∗ − vk while we bound the loss v∗−vπk . If we derive a bound on the loss (using e.g., Th. 1 in (Canbolat & Rothblum, 2012)), this leads to a bound on the loss that is looser than ours. In particular, this does not allow to recover the standard bounds for AVI/API, as we managed to (c.f. Remark 2).\nRemark 4. We can balance the influence of the concentrability coefficients (the bigger the q, the higher the influence) and the difficulty of controlling the errors (the bigger the q′, the greater the difficulty in controlling the Lpq′ -norms) by tuning the parameters q and q′, given the condition that 1q + 1 q′ = 1. This potential leverage is an improvement over the existing bounds and concentrability results that only consider specific values of these two parameters: q = ∞ and q′ = 1 in Munos (2007); Munos & Szepesvári (2008), and q = q′ = 2 in Farahmand et al. (2010).\nRemark 5. For CBMPI, the parameter m controls the influence of the value function approximator, cancelling it out in the limit when m tends to infinity (see Eq. 16). Assuming a fixed budget of sample transitions, increasing m reduces the number of rollouts used by the classifier and thus worsens its quality; in such a situation, m allows to make a trade-off between the estimation error of the classifier and the overall value function approximation."
    }, {
      "heading" : "5. Finite-Sample Analysis of CBMPI",
      "text" : "In this section, we focus on CBMPI and detail the possible form of the error terms that appear in the bound of Thm. 1. We select CBMPI among the proposed algorithms because its analysis is more general than the others as we need to bound both greedy and evaluation step errors (in some norm), and also because it displays an interesting influence of the parameter m (see Remark 5). We first provide a bound on the greedy step error. From the definition of ′k for CBMPI (Eq. 12) and the description of the greedy step in CBMPI, we can easily observe that ‖ ′k‖1,µ = LΠk−1(µ;πk).\nLemma 4 (Proof in (Scherrer et al., 2012, Appendix E)). Let Π be a policy space with finite VCdimension h = V C(Π) and µ be a distribution over the state space S. Let N be the number of states in D′k−1 drawn i.i.d. from µ, M be the number of rollouts per state-action pair used in the estimation of Q̂k−1, and πk = argminπ∈Π L̂Πk−1(µ̂, π) be the policy computed at iteration k − 1 of CBMPI. Then, for any δ > 0, we have\n‖ ′k‖1,µ = LΠk−1(µ;πk) ≤ inf π∈Π LΠk−1(µ;π) + 2( ′1 + ′2),\nwith probability at least 1− δ, where\n′1(N, δ) = 16Qmax\n√ 2\nN\n( h log eN\nh + log\n32\nδ\n) ,\n′2(N,M, δ) = 8Qmax\n√ 2\nMN\n( h log eMN\nh + log\n32\nδ\n) .\nWe now consider the evaluation step error. The evaluation step at iteration k of CBMPI is a regression problem with the target (Tπk)\nmvk−1 and a training set {( s(i), v̂k(s (i)) )}n i=1 in which the states s(i) are i.i.d. samples from µ and v̂k(s (i)) are unbiased estimates of the target computed according to Eq. 7. Different function spaces F (linear or non-linear) may be used to approximate (Tπk)\nmvk−1. Here we consider a linear architecture with parameters α ∈ Rd and bounded (by L) basis functions {ϕj}dj=1, ‖ϕj‖∞ ≤ L. We denote by φ : X → Rd, φ(·) = ( ϕ1(·), . . . , ϕd(·)\n)> the feature vector, and by F the linear function space spanned by the features ϕj , i.e., F = {fα(·) = φ(·)>α : α ∈ Rd}. Now if we define vk as the truncation (by Vmax) of the solution of the above linear regression problem, we may bound the evaluation step error using the following lemma.\nLemma 5 (Proof in (Scherrer et al., 2012, Appendix F)). Consider the linear regression setting described above, then we have\n‖ k‖2,µ ≤ 4 inf f∈F ‖(Tπk)mvk−1 − f‖2,µ + 1 + 2,\nwith probability at least 1− δ, where\n1(n, δ) = 32Vmax\n√ 2 n log (27(12e2n)2(d+1) δ ) ,\n2(n, δ) = 24 ( Vmax + ‖α∗‖2 · sup\nx ‖φ(x)‖2 )√ 2 n log 9 δ ,\nand α∗ is such that fα∗ is the best approximation (w.r.t. µ) of the target function (Tπk) mvk−1 in F .\nFrom Lemmas 4 and 5, we have bounds on ‖ ′k‖1,µ and ‖ k‖1,µ ≤ ‖ k‖2,µ. By a union bound argument,\nwe thus control the r.h.s of Eq. 16 in L1 norm. In the context of Th. 1, this means p = 1, q′ = 1 and q =∞, and we have the following bound for CBMPI:\nTheorem 2. Let d′ = supg∈F,π′ infπ∈Π LΠπ′,g(µ;π) and dm = supg∈F,π inff∈F ‖(Tπ)mg − f‖2,µ. With the notations of Th. 1 and Lemmas 4-5, after k iterations, and with probability 1 − δ, the expected loss Eµ[lk] = ‖lk‖1,µ of CBMPI is bounded by\n2γm(γ − γk−1)C2,k,m∞ (1− γ)2\n( dm + 1(n, δ\n2k ) + 2(n,\nδ\n2k ) ) +\n(1− γk)C1,k,0∞ (1− γ)2\n( d′ + ′1(N, δ\n2k ) + ′2(N,M,\nδ\n2k )\n) + g(k).\nRemark 6. This result leads to a quantitative version of Remark 5. Assume that we have a fixed budget for the actor and the critic B = nm = NM |A|m. Then, up to constants and logarithmic factors, the bound has the form ‖lk‖1,µ ≤\nO ( γm ( dm + √ m B ) + d′ + √ M |A|m B ) . It shows the\ntrade-off in the tuning of m: a big m can make the influence of the overall (approximation and estimation) value error small, but that of the estimation error of the classifier bigger."
    }, {
      "heading" : "6. Summary and Extensions",
      "text" : "In this paper, we studied a DP algorithm, called modified policy iteration (MPI), that despite its generality that contains the celebrated policy and value iteration methods, has not been thoroughly investigated in the literature. We proposed three approximate MPI (AMPI) algorithms that are extensions of the wellknown ADP algorithms: fitted-value iteration, fittedQ iteration, and classification-based policy iteration. We reported an error propagation analysis for AMPI that unifies those for approximate policy and value iteration. We also provided a finite-sample analysis for the classification-based implementation of AMPI (CBMPI), whose analysis is more general than the other presented AMPI methods. Our results indicate that the parameter of MPI allows us to control the balance of errors (in value function approximation and estimation of the greedy policy) in the final performance of CBMPI. Although AMPI generalizes the existing AVI and classification-based API algorithms, additional experimental work and careful theoretical analysis are required to obtain a better understanding of the behaviour of its different implementations and their relation to the competitive methods. Extension of CBMPI to problems with continuous action space is another interesting direction to pursue."
    }, {
      "heading" : "Ernst, D., Geurts, P., and Wehenkel, L. Tree-based batch",
      "text" : "mode reinforcement learning. Journal of Machine Learning Research, 6:503–556, 2005."
    }, {
      "heading" : "Farahmand, A., Munos, R., and Szepesvári, Cs. Error",
      "text" : "propagation for approximate policy and value iteration. In Proceedings of NIPS, pp. 568–576, 2010.\nFern, A., Yoon, S., and Givan, R. Approximate Policy Iteration with a Policy Language Bias: Solving Relational Markov Decision Processes. Journal of Artificial Intelligence Research, 25:75–118, 2006.\nGabillon, V., Lazaric, A., Ghavamzadeh, M., and Scherrer, B. Classification-based policy iteration with a critic. In Proceedings of ICML, pp. 1049–1056, 2011."
    }, {
      "heading" : "Lagoudakis, M. and Parr, R. Reinforcement Learning as",
      "text" : "Classification: Leveraging Modern Classifiers. In Proceedings of ICML, pp. 424–431, 2003."
    }, {
      "heading" : "Lazaric, A., Ghavamzadeh, M., and Munos, R. Analysis",
      "text" : "of a Classification-based Policy Iteration Algorithm. In Proceedings of ICML, pp. 607–614, 2010."
    }, {
      "heading" : "Munos, R. Error Bounds for Approximate Policy Iteration.",
      "text" : "In Proceedings of ICML, pp. 560–567, 2003.\nMunos, R. Performance Bounds in Lp-norm for Approximate Value Iteration. SIAM J. Control and Optimization, 46(2):541–561, 2007."
    }, {
      "heading" : "Munos, R. and Szepesvári, Cs. Finite-Time Bounds for",
      "text" : "Fitted Value Iteration. Journal of Machine Learning Research, 9:815–857, 2008.\nPuterman, M. and Shin, M. Modified policy iteration algorithms for discounted Markov decision problems. Management Science, 24(11), 1978."
    }, {
      "heading" : "Scherrer, Bruno, Gabillon, Victor, Ghavamzadeh, Mohammad, and Geist, Matthieu. Approximate Modified Policy",
      "text" : "Iteration. Technical report, INRIA, May 2012."
    }, {
      "heading" : "Szepesvári, Cs. Reinforcement Learning Algorithms for",
      "text" : "MDPs. In Wiley Encyclopedia of Operations Research. Wiley, 2010."
    }, {
      "heading" : "Thiery, Christophe and Scherrer, Bruno. Performance",
      "text" : "bound for Approximate Optimistic Policy Iteration. Technical report, INRIA, 2010.\nSupplementary Material for Approximate Modified Policy Iteration"
    }, {
      "heading" : "A. Proof of Lemma 1",
      "text" : "Before we start, we recall the following definitions:\nbk = vk−Tπk+1vk, dk = v∗− (Tπk)mvk−1 = v∗− (vk− k), sk = (Tπk)mvk−1− vπk = (vk− k)− vπk ."
    }, {
      "heading" : "Bounding bk",
      "text" : "bk = vk − Tπk+1vk = vk − Tπkvk + Tπkvk − Tπk+1vk (a) ≤ vk − Tπkvk + ′k+1\n= vk − k − Tπkvk + γPπk k + k − γPπk k + ′k+1 (b) = vk − k − Tπk(vk − k) + (I − γPπk) k + ′k+1. (18)\nUsing the definition of xk, i.e.,\nxk ∆ = (I − γPπk) k + ′k+1, (19)\nwe may write Eq. (18) as\nbk ≤ vk − k − Tπk(vk − k) + xk (c) = (Tπk) mvk−1 − Tπk(Tπk)mvk−1 + xk = (Tπk)mvk−1 − (Tπk)m(Tπkvk−1) + xk = (γPπk) m(vk−1 − Tπkvk−1) + xk = (γPπk)mbk−1 + xk. (20)\n(a) From the definition of ′k+1, we have ∀π′ Tπ′vk ≤ Tπk+1vk + ′k+1, thus this inequality holds also for π′ = πk. (b) This step is due to the fact that for every v and v′, we have Tπk(v + v ′) = Tπkv + γPπkv ′.\n(c) This is from the definition of k, i.e., vk = (Tπk) mvk−1 + k."
    }, {
      "heading" : "Bounding dk",
      "text" : "dk+1 = v∗ − (Tπk+1)mvk = Tπ∗v∗ − Tπ∗vk + Tπ∗vk − Tπk+1vk + Tπk+1vk − (Tπk+1)mvk (a)\n≤ γPπ∗(v∗ − vk) + ′k+1 + gk+1 = γPπ∗(v∗ − vk) + γPπ∗ k − γPπ∗ k + ′k+1 + gk+1\n(b) = γPπ∗ ( v∗ − (vk − k) ) + yk + gk+1 = γPπ∗dk + yk + gk+1 (c) = γPπ∗dk + yk + m−1∑ j=1 (γPπk+1) jbk. (21)\n(a) This step is from the definition of ′k+1 (see step (a) in bounding bk) and by defining gk+1 as follows:\ngk+1 ∆ = Tπk+1vk − (Tπk+1)mvk. (22)\n(b) This is from the definition of yk, i.e.,\nyk ∆ = −γPπ∗ k + ′k+1. (23)\n(c) This step comes from rewriting gk+1 as\ngk+1 = Tπk+1vk − (Tπk+1)mvk = m−1∑ j=1 [ (Tπk+1) jvk − (Tπk+1)j+1vk ] = m−1∑ j=1 [ (Tπk+1) jvk − (Tπk+1)j(Tπk+1vk) ]\n= m−1∑ j=1 (γPπk+1) j(vk − Tπk+1vk) = m−1∑ j=1 (γPπk+1) jbk. (24)\nBounding sk With some slight abuse of notation, we have\nvπk = (Tπk) ∞vk\nand thus:\nsk = (Tπk) mvk−1 − vπk (a) = (Tπk) mvk−1 − (Tπk)∞vk−1 = (Tπk)mvk−1 − (Tπk)m(Tπk)∞vk−1\n= (γPπk) m ( vk−1 − (Tπk)∞vk−1 ) = (γPπk) m ∞∑ j=0 [ (Tπk) jvk−1 − (Tπk)j+1vk−1 ]\n= (γPπk) m ( ∞∑ j=0 [ (Tπk) jvk−1 − (Tπk)jTπkvk−1 ] = (γPπk) m ( ∞∑ j=0 (γPπk) j ) (vk−1 − Tπkvk−1) = (γPπk) m(I − γPπk)−1(vk−1 − Tπkvk−1) = (γPπk)m(I − γPπk)−1bk. (25)\n(a) For any v, we have vπk = (Tπk) ∞v. This step follows by setting v = vk−1, i.e., vπk = (Tπk) ∞vk−1."
    }, {
      "heading" : "B. Proof of Lemma 2",
      "text" : "We begin by focusing our analysis on AMPI. Here we are interested in bounding the loss lk = v∗−vπk = dk +sk.\nBy induction, from Eqs. (20) and (21), we obtain\nbk ≤ k∑ i=1 Γm(k−i)xi + Γ mkb0, (26)\ndk ≤ k−1∑ j=0 Γk−1−j ( yj + m−1∑ l=1 Γlbj ) + Γkd0. (27)\nin which we have used the notation introduced in Definition 1. In Eq. (27), we also used the fact that from Eq. (24), we may write gk+1 = ∑m−1 j=1 Γ jbk. Moreover, we may rewrite Eq. (25) as\nsk = Γ m ∞∑ j=0 Γjbk−1 = ∞∑ j=0 Γm+jbk−1. (28)\nBounding lk From Eqs. (26) and (27), we may write\ndk ≤ k−1∑ j=0 Γk−1−j\n( yj +\nm−1∑ l=1 Γl ( j∑ i=1 Γm(j−i)xi + Γ mjb0 )) + Γkd0\n= k∑ i=1 Γi−1yk−i + k−1∑ j=0 m−1∑ l=1 j∑ i=1 Γk−1−j+l+m(j−i)xi + zk, (29)\nwhere we used the following definition\nzk ∆ = k−1∑ j=0 m−1∑ l=1 Γk−1+l+j(m−1)b0 + Γ kd0 = mk−1∑ i=k Γib0 + Γ kd0.\nThe triple sum involved in Eq. (29) may be written as\nk−1∑ j=0 m−1∑ l=1 j∑ i=1 Γk−1−j+l+m(j−i)xi = k−1∑ i=1 k−1∑ j=i m−1∑ l=1 Γk−1+l+j(m−1)−mixi = k−1∑ i=1 mk−1∑ j=mi+k−i Γj−mixi\n= k−1∑ i=1 m(k−i)−1∑ j=k−i Γjxi = k−1∑ i=1 mi−1∑ j=i Γjxk−i. (30)\nUsing Eq. (30), we may write Eq. (29) as\ndk ≤ k∑ i=1 Γi−1yk−i + k−1∑ i=1 mi−1∑ j=i Γjxk−i + zk. (31)\nSimilarly, from Eqs. (28) and (26), we have\nsk ≤ ∞∑ j=0 Γm+j ( k−1∑ i=1 Γm(k−1−i)xi + Γ m(k−1)b0 ) = ∞∑ j=0 ( k−1∑ i=1 Γm+j+m(k−1−i)xi + Γ m+j+m(k−1)b0 )\n= k−1∑ i=1 ∞∑ j=0 Γj+m(k−i)xi + ∞∑ j=0 Γj+mkb0 = k−1∑ i=1 ∞∑ j=0 Γj+mixk−i + ∞∑ j=mk Γjb0 = k−1∑ i=1 ∞∑ j=mi Γjxk−i + z ′ k, (32)\nwhere we used the following definition\nz′k ∆ = ∞∑ j=mk Γjb0.\nFinally, using the bounds in Eqs. (31) and (32), we obtain the following bound on the loss\nlk ≤ dk + sk ≤ k∑ i=1 Γi−1yk−i + k−1∑ i=1 (mi−1∑ j=i Γj + ∞∑ j=mi Γj ) xk−i + zk + z ′ k\n= k∑ i=1 Γi−1yk−i + k−1∑ i=1 ∞∑ j=i Γjxk−i + ηk, (33)\nwhere we used the following definition\nηk ∆ = zk + z ′ k = ∞∑ j=k Γjb0 + Γ kd0. (34)\nNote that we have the following relation between b0 and d0\nb0 = v0 − Tπ1v0 = v0 − v∗ + Tπ∗v∗ − Tπ∗v0 + Tπ∗v0 − Tπ1v0 ≤ (I − γPπ∗)(−d0) + ′1, (35)\nIn Eq. (35), we used the fact that v∗ = Tπ∗v∗, 0 = 0, and Tπ∗v0 − Tπ1v0 ≤ ′1 (this is because the policy π1 is ′1-greedy w.r.t. v0). As a result, we may write |ηk| either as\n|ηk| ≤ ∞∑ j=k Γj [ (I−γPπ∗)|d0|+ | ′1| ] +Γk|d0| ≤ ∞∑ j=k Γj [ (I+Γ1)|d0|+ | ′1| ] +Γk|d0| = 2 ∞∑ j=k Γj |d0|+ ∞∑ j=k Γj | ′1|, (36)\nor using the fact that from Eq. (35), we have d0 ≤ (I − γPπ∗)−1(−b0 + ′1), as\n|ηk| ≤ ∞∑ j=k Γj |b0|+ Γk ∞∑ j=0 (γPπ∗) j ( |b0|+ | ′1| ) = ∞∑ j=k Γj |b0|+ Γk ∞∑ j=0 Γj ( |b0|+ | ′1| ) = 2 ∞∑ j=k Γj |b0|+ ∞∑ j=k Γj | ′1|. (37)\nNow, using the definitions of xk and yk in Eqs. (19) and (23), the bound on |ηk| in Eq. (36) or (37), and the fact that 0 = 0, we obtain |lk| ≤ k∑ i=1 Γi−1 [ Γ1| k−i|+ | ′k−i+1| ] + k−1∑ i=1 ∞∑ j=i Γj [ (I + Γ1)| k−i|+ | ′k−i+1| ] + |ηk|\n= k−1∑ i=1 ( Γi + ∞∑ j=i (Γj + Γj+1) ) | k−i|+ Γk| 0|+ k−1∑ i=1 ( Γi−1 + ∞∑ j=i Γj ) | ′k−i+1|+ Γk−1| ′1|+ ∞∑ j=k Γj | ′1|+ h(k)\n= 2 k−1∑ i=1 ∞∑ j=i Γj | k−i|+ k−1∑ i=1 ∞∑ j=i−1 Γj | ′k−i+1|+ ∞∑ j=k−1 Γj | ′1|+ h(k) = 2 k−1∑ i=1 ∞∑ j=i Γj | k−i|+ k−1∑ i=0 ∞∑ j=i Γj | ′k−i|+ h(k),\n(38)\nwhere we used the following definition\nh(k) ∆ = 2 ∞∑ j=k Γj |d0|, or h(k) ∆ = 2 ∞∑ j=k Γj |b0|.\nWe end this proof by adapting the error propagation to CBMPI. As expressed by Eqs. 12 and 13 in Sec. 4, an analysis of CBMPI can be deduced from that we have just done by replacing vk with the auxiliary variable\nwk = (Tπk) mvk−1 and k with (γPπk) m k−1 = Γ m k−1. Therefore, using the fact that 0 = 0, we can rewrite the bound of Eq. 38 for CBMPI as follows:\nlk ≤ 2 k−1∑ i=1 ∞∑ j=i Γj+m| k−i−1|+ k−1∑ i=0 ∞∑ j=i Γj | ′k−i|+ h(k)\n= 2 k−2∑ i=1 ∞∑ j=m+i Γj | k−i−1|+ k−1∑ i=0 ∞∑ j=i Γj | ′k−i|+ h(k). (39)"
    }, {
      "heading" : "C. Proof of Lemma 3",
      "text" : "For any integer t and vector z, the definition of Γt and the Hölder’s inequality imply that\nρΓt|z| = ∥∥Γt|z|∥∥\n1,ρ ≤ γtcq(t)‖z‖q′,µ = γtcq(t)\n( µ|z|q ′ ) 1 q′ . (40)\nWe define\nK ∆ = n∑ l=1 ξl ∑ i∈Il ∑ j∈Ji γj  , where {ξl}nl=1 is a set of non-negative numbers that we will specify later. We now have\n‖f‖pp,ρ = ρ|f |p\n≤ Kpρ\n(∑n l=1 ∑ i∈Il ∑ j∈Ji Γ\nj |gi| K\n)p = Kpρ ∑nl=1 ξl∑i∈Il∑j∈Ji Γj ( |gi| ξl ) K p\n(a) ≤ Kpρ\n∑n l=1 ξl ∑ i∈Il ∑ j∈Ji Γ j ( |gi| ξl )p K = Kp ∑n l=1 ξl ∑ i∈Il ∑ j∈Ji ρΓ j ( |gi| ξl )p K\n(b) ≤ Kp ∑n l=1 ξl ∑ i∈Il ∑ j∈Ji γ jcq(j) ( µ ( |gi| ξl )pq′) 1q′ K\n= Kp\n∑n l=1 ξl ∑ i∈Il ∑ j∈Ji γ jcq(j) ( ‖gi‖pq′,µ ξl )p K\n≤ Kp ∑n l=1 ξl (∑ i∈Il ∑ j∈Ji γ jcq(j) )( supi∈Il ‖gi‖pq′,µ ξl )p K\n(c) = Kp\n∑n l=1 ξl (∑ i∈Il ∑ j∈Ji γ j ) Cq(l) ( supi∈Il ‖gi‖pq′,µ ξl )p K ,\nwhere (a) results from Jensen’s inequality, (b) from Eq. 40, and (c) from the definition of Cq(l). Now, by setting ξl = ( Cq(l) )1/p supi∈Il ‖gi‖pq′,µ, we obtain\n‖f‖pp,ρ ≤ Kp ∑n l=1 ξl (∑ i∈Il ∑ j∈Ji γ j )\nK = Kp,\nwhere the last step follows from the definition of K."
    }, {
      "heading" : "D. Proof of Theorem 1 & other Bounds on the Loss",
      "text" : "Proof. We only detail the proof for AMPI (the proof being similar for CBMPI). We define I = {1, 2, · · · , 2k}, the partition I = {I1, I2, I3} as I1 = {1, . . . , k − 1}, I2 = {k, . . . , 2k − 1}, and I3 = {2k}, and for each i ∈ I\ngi =  2 k−i if 1 ≤ i ≤ k − 1, ′k−(i−k) if k ≤ i ≤ 2k − 1, 2d0 (or 2b0) if i = 2k,\nand Ji =  {i, i+ 1, · · · } if 1 ≤ i ≤ k − 1,{i− k, i− k + 1, · · · } if k ≤ i ≤ 2k − 1,{k, k + 1, · · · } if i = 2k. Note that here we have divided the terms in the point-wise bound of Lemma 2 into three groups: the evaluation error terms { j}k−1j=1 , the greedy step error terms { ′j}kj=1, and finally the residual term h(k). With the above definitions and the fact that the loss lk is non-negative, Lemma 2 may be rewritten as\n|lk| ≤ 3∑ l=1 ∑ i∈Il ∑ j∈Ji Γj |gi|.\nThe result follows by applying Lemma 3 and noticing that ∑k−1 i=i0 ∑∞ j=i γ j = γ i0−γk (1−γ)2 .\nHere in oder to show the flexibility of Lemma 3, we group the terms differently and derive an alternative Lpbound for the loss of AMPI and CBMPI. In analogy with the results of Farahmand et al. (2010), this new bound shows that the last iterations have the highest influence on the loss (the influence exponentially decreases towards the initial iterations).\nTheorem 3. With the notations of Theorem 1, after k iterations, the loss of AMPI satisfies\n‖lk‖p,ρ ≤ 2 k−1∑ i=1 γi 1− γ ( Ci,i+1q ) 1 p ‖ k−i‖pq′,µ + k−1∑ i=0 γi 1− γ ( Ci,i+1q ) 1 p ‖ ′k−i‖pq′,µ + g(k).\nwhile the loss of CBMPI satisfies\n‖lk‖p,ρ ≤ 2γm k−2∑ i=1 γi 1− γ ( Ci,i+1q ) 1 p ‖ k−i−1‖pq′,µ + k−1∑ i=0 γi 1− γ ( Ci,i+1q ) 1 p ‖ ′k−i‖pq′,µ + g(k).\nProof. Again, we only detail the proof for AMPI (the proof being similar for CBMPI). We define I, (gi) and (Ji) as in the proof of Theorem 1. We then make as many groups as terms, i.e., for each n ∈ {1, 2, . . . , 2k − 1}, we define In = {n}. The result follows by application of Lemma 3."
    }, {
      "heading" : "E. Proof of Lemma 4",
      "text" : "The proof of this lemma is similar to the proof of Theorem 1 in Lazaric et al. (2010). Before stating the proof, we report the following two lemmas that are used in the proof.\nLemma 6. Let Π be a policy space with finite VC-dimension h = V C(Π) < ∞ and N be the number of states in the rollout set Dk−1 drawn i.i.d. from the state distribution µ. Then we have\nPDk−1 [\nsup π∈Π ∣∣∣LΠk−1(µ̂;π)− LΠk−1(µ;π)∣∣∣ > ] ≤ δ , with = 16Qmax √ 2 N ( h log eNh + log 8 δ ) .\nProof. This is a restatement of Lemma 1 in Lazaric et al. (2010).\nLemma 7. Let Π be a policy space with finite VC-dimension h = V C(Π) <∞ and s(1), . . . , s(N) be an arbitrary sequence of states. At each state we simulate M independent rollouts of the form , then we have\nP sup π∈Π ∣∣∣ 1 N N∑ i=1 1 M M∑ j=1 Rjk−1 ( s(i,j), π(s(i,j)) ) − 1 N N∑ i=1 Qk−1 ( s(i,j), π(s(i,j)) )∣∣∣ >  ≤ δ ,\nwith = 8Qmax\n√ 2\nMN ( h log eMNh + log 8 δ ) .\nProof. The proof is similar to the one for Lemma 6.\nProof. (Lemma 4) Let a∗(·) = argmaxa∈AQk−1(·, a) be the greedy action. To simplify the notation, we remove the dependency of a∗ on states and use a∗ instead of a∗(xi) in the following. We prove the following series of inequalities:\nLΠk−1(µ;πk) (a) ≤ LΠk−1(µ̂;πk) + ′1 w.p. 1− δ′\n= 1\nN N∑ i=1 [ Qk−1(xi, a ∗)−Qk−1 ( xi, πk(xi) )] + ′1\n(b) ≤ 1 N N∑ i=1 [ Qk−1(xi, a ∗)− Q̂k−1 ( xi, πk(xi) )] + ′1 + ′ 2 w.p. 1− 2δ′\n(c) ≤ 1 N N∑ i=1 [ Qk−1(xi, a ∗)− Q̂k−1 ( xi, π ∗(xi) )] + ′1 + ′ 2\n≤ 1 N N∑ i=1 [ Qk−1(xi, a ∗)−Qk−1 ( xi, π ∗(xi) )] + ′1 + 2 ′ 2 w.p. 1− 3δ′ = LΠk−1(µ̂;π∗) + ′1 + 2 ′2 ≤ LΠk−1(µ;π∗) + 2( ′1 + ′2) w.p. 1− 4δ′ = inf π∈Π LΠk−1(µ;π) + 2( ′1 + ′2).\nThe statement of the theorem is obtained by δ′ = δ/4.\n(a) This follows from Lemma 6.\n(b) Here we introduce the estimated action-value function Q̂k−1 by bounding\nsup π∈Π\n[ 1\nN N∑ i=1 Q̂k−1 ( s(i), π(s(i)) ) − 1 N N∑ i=1 Qk−1 ( s(i), π(s(i)) )]\nusing Lemma 7. (c) From the definition of πk in CBMPI, we have\nπk = argmin π∈Π L̂Πk−1(µ̂;π) = argmax π∈Π\n1\nN N∑ i=1 Q̂k−1 ( s(i), π(s(i)) ) ,\nthus, −1/N ∑N i=1 Q̂k−1 ( s(i), πk(s (i)) ) can be maximized by replacing πk with any other policy, particularly with\nπ∗ = argmin π∈Π ∫ S ( max a∈A Qk−1(s, a)−Qk−1 ( s, π(s) )) µ(ds)."
    }, {
      "heading" : "F. Proof of Lemma 5",
      "text" : "Let us define two n-dimensional vectors z = ([\n(Tπk) mvk−1\n] (s(1)), . . . , [ (Tπk) mvk−1 ] (s(n)) )> and y =(\nv̂k(s (1)), . . . , v̂k(s (n)) )>\nand their orthogonal projections onto the vector space Fn as ẑ = Π̂z and ŷ = Π̂y = ( ṽk(s (1)), . . . , ṽk(s (n)) )>\n, where ṽk is the result of linear regression and its truncation (by Vmax) is vk, i.e., vk = T(ṽk) (see Figure 2). What we are interested is to find a bound on the regression error ‖z − ŷ‖ (the difference between the target function z and the result of the regression ŷ). We may decompose this error as\n‖z − ŷ‖n ≤ ‖ẑ − ŷ‖n + ‖z − ẑ‖n = ‖ξ̂‖n + ‖z − ẑ‖n, (41)\nwhere ξ̂ = ẑ − ŷ is the projected noise (estimation error) ξ̂ = Π̂ξ, with the noise vector ξ = z − y defined as ξi = [ (Tπk) mvk−1 ] (s(i)) − v̂k(s(i)). It is easy to see that noise is zero mean, i.e., E[ξi] = 0 and is bounded by 2Vmax, i.e., |ξi| ≤ 2Vmax. We may write the estimation error as\n‖ẑ − ŷ‖2n = ‖ξ̂‖2n = 〈ξ̂, ξ̂〉 = 〈ξ, ξ̂〉,\nwhere the last equality follows from the fact that ξ̂ is the orthogonal projection of ξ. Since ξ̂ ∈ Fn, let fα ∈ F be any function whose values at {s(i)}ni=1 equals to {ξi}ni=1. By application of a variation of Pollard’s inequality (Györfi et al., 2002), we obtain\n〈ξ, ξ̂〉 = 1 n n∑ i=1 ξifα(s (i)) ≤ 4Vmax‖ξ̂‖n\n√ 2\nn log\n( 3(9e2n)d+1\nδ′\n) ,\nwith probability at least 1− δ′. Thus, we have\n‖ẑ − ŷ‖n = ‖ξ̂‖n ≤ 4Vmax\n√ 2\nn log\n( 3(9e2n)d+1\nδ′\n) . (42)\nFrom Eqs. 41 and 42, we have\n‖(Tπk)mvk−1 − ṽk‖µ̂ ≤ ‖(Tπk)mvk−1 − Π̂(Tπk)mvk−1‖µ̂ + 4Vmax\n√ 2\nn log\n( 3(9e2n)d+1\nδ′\n) , (43)\nwhere µ̂ is the empirical norm induced from the n i.i.d. samples from µ. Now in order to obtain a random design bound, we first define fα̂∗ ∈ F as fα̂∗(s(i)) = [ Π̂(Tπk) mvk−1 ] (s(i)), and then define fα∗ = Π(Tπk) mvk−1 that is the best approximation (w.r.t. µ) of the target function (Tπk)\nmvk−1 in F . Since fα̂∗ is the minimizer of the empirical loss, any function in F different than fα̂∗ has a bigger empirical loss, thus we have\n‖fα̂∗ − (Tπk)mvk−1‖µ̂ ≤ ‖fα∗ − (Tπk)mvk−1‖µ̂ ≤ 2‖fα∗ − (Tπk)mvk−1‖µ\nApproximate Modified Policy Iteration\n+ 12 ( Vmax + ‖α∗‖2 sup\nx ‖φ(x)‖2 )√ 2 n log 3 δ′ , (44)\nwith probability at least 1− δ′, where the second inequality is the application of a variation of Theorem 11.2 in the book by Györfi et al., (2002) with ‖fα∗ − (Tπk)mvk−1‖∞ ≤ Vmax + ‖α∗‖2 supx ‖φ(x)‖2. Similarly, we can write the left-hand-side of Equation 43 as\n2‖(Tπk)mvk−1 − ṽk‖µ̂ ≥ 2‖(Tπk)mvk−1 − T(ṽk)‖µ̂ ≥ ‖(Tπk)mvk−1 − T(ṽk)‖µ − 24Vmax\n√ 2\nn Λ(n, d, δ′), (45)\nwith probability at least 1− δ′, where Λ(n, d, δ′) = 2(d+ 1) log n+ log eδ′ + log ( 9(12e)2(d+1) ) . Putting together Equations 43, 44, and 45 and using the fact that T(ṽk) = vk, we obtain\n‖ηk‖2,µ = ‖(Tπk)mvk−1 − vk‖µ ≤ 2 ( 2‖(Tπk)mvk−1 − fα∗‖µ + 12 ( Vmax + ‖α∗‖2 sup\nx ‖φ(x)‖2 )√ 2 n log 3 δ′\n+ 4Vmax\n√ 2\nn log\n( 3(9e2n)d+1\nδ′\n)) + 24Vmax √ 2\nn Λ(n, d, δ′).\nThe result follows by setting δ = 3δ′ and some simplification."
    }, {
      "heading" : "G. Experimental Results",
      "text" : "In this section, we report the empirical evaluation of CBMPI and compare it to DPI and LSPI. In the experiments, we show that CBMPI, by combining policy and value function approximation, can improve over DPI and LSPI. In these experiments, we are using the same setting as in Gabillon et al. (2011) to facilitate the comparison."
    }, {
      "heading" : "G.1. Setting",
      "text" : "We consider the mountain car (MC) problem with its standard formulation in which the action noise is bounded in [−1, 1] and γ = 0.99. The value function is approximated using a linear space spanned by a set of radial basis functions (RBFs) evenly distributed over the state space.\nEach CBMPI-based algorithm is run with the same fixed budget B per iteration. CBMPI splits the budget into a rollout budget BR = B(1 − p) used to build the training set of the greedy step and a critic budget BC = Bp used to build the training set of the evaluation step , where p ∈ (0, 1) is the critic ratio. The rollout budget is divided into M rollouts of length m for each action in A and each state in the rollout set D′, i.e., BR = mMN |A|. The critic budget is divided into one rollout of length m for each action in A and each state in the rollout set D, i.e., BC = mn|A|.\nIn Fig. 3, we report the performance of DPI, CBMPI, and LSPI. In MC, the performance is evaluated as the number of steps-to-go with a maximum of 300. The results are averaged over 1000 runs. We report the performance of DPI and LSPI at p = 0 and p = 1, respectively. DPI can be seen as a special case of CBMPI where p = 0. We tested the performance of DPI and CBMPI on a wide range of parameters (m,M,N, n) but we only report their performance for the best choice of M (M = 1 was the best choice in all the experiments) and different values of m."
    }, {
      "heading" : "G.2. Experiments",
      "text" : "As discussed in Remark 5, the parameter m balances between the error in evaluating the value function and the error in evaluating the policy. The value function approximation error tends to zero for large values of m. Although this would suggest to have large values for m, the size of the rollout sets would correspondingly decrease as N = O(B/m) and n = O(B/m), thus decreasing the accuracy of both the regression and classification problems. This leads to a trade-off between long rollouts and the number of states in the rollout sets. The solution to this trade-off strictly depends on the capacity of the value function space F . A rich value function space would lead to solve the trade-off for small values of m. On the other hand, when the value function space is poor, or as in the DPI case, m should be selected in a way to guarantee a sufficient number of informative rollouts, and\nat the same time, a large enough rollout sets.\nFigure 3 shows the learning results in MC with budget B = 200. On the left panel, the function space is rich enough to approximate v∗. Therefore LSPI has almost optimal results (about 80 steps to reach the goal). On the other hand, DPI achieves a poor performance of about 150 steps, which is obtained by setting m = 12 and N = 5. We also report the performance of CBMPI for different values of m and p. When p is large enough, the value function approximation becomes accurate enough so that the best solution is to have m = 1. This both corresponds to rollouts built almost entirely on the basis of the approximated value function and to a large number of states in the training set N . For m = 1 and p ≈ 0.8, CBMPI achieves a slightly better performance than LSPI.\nIn the next experiment, we show that CBMPI is able to outperform both DPI and LSPI when F has a lower accuracy. The results are reported on the right panel of Figure 3. The performance of LSPI now worsens to 190 steps. Simultaneously one can notice m = 1 is no longer the best choice for CBMPI. Indeed in the case where m = 1, CBMPI becomes an approximated version of the value iteration algorithm relying on a function space not rich enough to approximate v∗. Notice that relying on this space is still better than setting the value function to zero which is the case in DPI. Therefore, we notice an improvement of CBMPI over DPI for m = 4 which trade-off between the estimates of the value function and the rewards collected by the rollouts. Combining those two, CBMPI also improves upon LSPI."
    } ],
    "references" : [ {
      "title" : "Fitted Qiteration in continuous action-space MDPs",
      "author" : [ "A. Antos", "R. Munos", "Szepesvári", "Cs" ],
      "venue" : "In Proceedings of NIPS, pp",
      "citeRegEx" : "Antos et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Antos et al\\.",
      "year" : 2007
    }, {
      "title" : "approximate) iterated successive approximations algorithm for sequential decision processes",
      "author" : [ "Canbolat", "Pelin", "Rothblum", "Uriel" ],
      "venue" : "Annals of Operations Research, pp",
      "citeRegEx" : "Canbolat et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Canbolat et al\\.",
      "year" : 2012
    }, {
      "title" : "Tree-based batch mode reinforcement learning",
      "author" : [ "D. Ernst", "P. Geurts", "L. Wehenkel" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Ernst et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ernst et al\\.",
      "year" : 2005
    }, {
      "title" : "Error propagation for approximate policy and value iteration",
      "author" : [ "A. Farahmand", "R. Munos", "Szepesvári", "Cs" ],
      "venue" : "In Proceedings of NIPS,",
      "citeRegEx" : "Farahmand et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Farahmand et al\\.",
      "year" : 2010
    }, {
      "title" : "Approximate Policy Iteration with a Policy Language Bias: Solving Relational Markov Decision Processes",
      "author" : [ "A. Fern", "S. Yoon", "R. Givan" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Fern et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Fern et al\\.",
      "year" : 2006
    }, {
      "title" : "Classification-based policy iteration with a critic",
      "author" : [ "V. Gabillon", "A. Lazaric", "M. Ghavamzadeh", "B. Scherrer" ],
      "venue" : "In Proceedings of ICML,",
      "citeRegEx" : "Gabillon et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gabillon et al\\.",
      "year" : 2011
    }, {
      "title" : "Reinforcement Learning as Classification: Leveraging Modern Classifiers",
      "author" : [ "M. Lagoudakis", "R. Parr" ],
      "venue" : "In Proceedings of ICML, pp",
      "citeRegEx" : "Lagoudakis and Parr,? \\Q2003\\E",
      "shortCiteRegEx" : "Lagoudakis and Parr",
      "year" : 2003
    }, {
      "title" : "Analysis of a Classification-based Policy Iteration Algorithm",
      "author" : [ "A. Lazaric", "M. Ghavamzadeh", "R. Munos" ],
      "venue" : "In Proceedings of ICML, pp",
      "citeRegEx" : "Lazaric et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lazaric et al\\.",
      "year" : 2010
    }, {
      "title" : "Error Bounds for Approximate Policy Iteration",
      "author" : [ "R. Munos" ],
      "venue" : "In Proceedings of ICML, pp",
      "citeRegEx" : "Munos,? \\Q2003\\E",
      "shortCiteRegEx" : "Munos",
      "year" : 2003
    }, {
      "title" : "Performance Bounds in Lp-norm for Approximate Value Iteration",
      "author" : [ "R. Munos" ],
      "venue" : "SIAM J. Control and Optimization,",
      "citeRegEx" : "Munos,? \\Q2007\\E",
      "shortCiteRegEx" : "Munos",
      "year" : 2007
    }, {
      "title" : "Finite-Time Bounds for Fitted Value Iteration",
      "author" : [ "R. Munos", "Szepesvári", "Cs" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Munos et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Munos et al\\.",
      "year" : 2008
    }, {
      "title" : "Modified policy iteration algorithms for discounted Markov decision problems",
      "author" : [ "M. Puterman", "M. Shin" ],
      "venue" : "Management Science,",
      "citeRegEx" : "Puterman and Shin,? \\Q1978\\E",
      "shortCiteRegEx" : "Puterman and Shin",
      "year" : 1978
    }, {
      "title" : "Approximate Modified Policy Iteration",
      "author" : [ "Scherrer", "Bruno", "Gabillon", "Victor", "Ghavamzadeh", "Mohammad", "Geist", "Matthieu" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Scherrer et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Scherrer et al\\.",
      "year" : 2012
    }, {
      "title" : "Reinforcement Learning Algorithms for MDPs",
      "author" : [ "Szepesvári", "Cs" ],
      "venue" : "In Wiley Encyclopedia of Operations Research. Wiley,",
      "citeRegEx" : "Szepesvári and Cs.,? \\Q2010\\E",
      "shortCiteRegEx" : "Szepesvári and Cs.",
      "year" : 2010
    }, {
      "title" : "Performance bound for Approximate Optimistic Policy Iteration",
      "author" : [ "Thiery", "Christophe", "Scherrer", "Bruno" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Thiery et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Thiery et al\\.",
      "year" : 2010
    }, {
      "title" : "Let Π be a policy space with finite VC-dimension h = V C(Π) <∞ and s",
      "author" : [ "Lazaric" ],
      "venue" : null,
      "citeRegEx" : "Lazaric,? \\Q2010\\E",
      "shortCiteRegEx" : "Lazaric",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "3) that generalize the AVI implementations of Ernst et al. (2005); Antos et al.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "(2005); Antos et al. (2007); Munos & Szepesvári (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "(2005); Antos et al. (2007); Munos & Szepesvári (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al.",
      "startOffset" : 8,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "(2005); Antos et al. (2007); Munos & Szepesvári (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al.",
      "startOffset" : 8,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "(2005); Antos et al. (2007); Munos & Szepesvári (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al. (2006); Lazaric et al.",
      "startOffset" : 8,
      "endOffset" : 146
    }, {
      "referenceID" : 0,
      "context" : "(2005); Antos et al. (2007); Munos & Szepesvári (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al. (2006); Lazaric et al. (2010); Gabillon et al.",
      "startOffset" : 8,
      "endOffset" : 169
    }, {
      "referenceID" : 0,
      "context" : "(2005); Antos et al. (2007); Munos & Szepesvári (2008) and the classification-based API algorithm of Lagoudakis & Parr (2003); Fern et al. (2006); Lazaric et al. (2010); Gabillon et al. (2011). We then provide an error propagation analysis of AMPI (Sec.",
      "startOffset" : 8,
      "endOffset" : 193
    }, {
      "referenceID" : 2,
      "context" : "Note that the fitted-Q iteration algorithm (Ernst et al., 2005; Antos et al., 2007) is a special case of AMPI-Q when m = 1.",
      "startOffset" : 43,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "Note that the fitted-Q iteration algorithm (Ernst et al., 2005; Antos et al., 2007) is a special case of AMPI-Q when m = 1.",
      "startOffset" : 43,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "The idea is similar to the classification-based PI algorithms (Lagoudakis & Parr, 2003; Fern et al., 2006; Lazaric et al., 2010; Gabillon et al., 2011) in which we search for the greedy policy in a policy space Π (defined by a classifier) instead of computing it from the estimated value or action-value function (like in AMPI-V and AMPI-Q).",
      "startOffset" : 62,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : "The idea is similar to the classification-based PI algorithms (Lagoudakis & Parr, 2003; Fern et al., 2006; Lazaric et al., 2010; Gabillon et al., 2011) in which we search for the greedy policy in a policy space Π (defined by a classifier) instead of computing it from the estimated value or action-value function (like in AMPI-V and AMPI-Q).",
      "startOffset" : 62,
      "endOffset" : 151
    }, {
      "referenceID" : 5,
      "context" : "The idea is similar to the classification-based PI algorithms (Lagoudakis & Parr, 2003; Fern et al., 2006; Lazaric et al., 2010; Gabillon et al., 2011) in which we search for the greedy policy in a policy space Π (defined by a classifier) instead of computing it from the estimated value or action-value function (like in AMPI-V and AMPI-Q).",
      "startOffset" : 62,
      "endOffset" : 151
    }, {
      "referenceID" : 7,
      "context" : "Note that when m tends to ∞, we recover the DPI algorithm proposed and analyzed by Lazaric et al. (2010).",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 9,
      "context" : "VI analysis is based on the fact that this algorithm computes the fixed point of the Bellman optimality operator, and this operator is a γ-contraction in max-norm (Bertsekas & Tsitsiklis, 1996; Munos, 2007).",
      "startOffset" : 163,
      "endOffset" : 206
    }, {
      "referenceID" : 8,
      "context" : "We also know that the analysis of PI usually relies on the fact that the sequence of the generated values is non-decreasing (Bertsekas & Tsitsiklis, 1996; Munos, 2003).",
      "startOffset" : 124,
      "endOffset" : 167
    }, {
      "referenceID" : 12,
      "context" : "1 and report most proofs in (Scherrer et al., 2012).",
      "startOffset" : 28,
      "endOffset" : 51
    }, {
      "referenceID" : 12,
      "context" : "1 and report most proofs in (Scherrer et al., 2012). We follow the line of analysis developped by Thiery & Scherrer (2010). The results are obtained using the following three quantities: 1) The distance between the optimal value function and the value before approximation at the k iteration: dk ∆ = v∗ − (Tπk)vk−1 = v∗ − (vk − k).",
      "startOffset" : 29,
      "endOffset" : 123
    }, {
      "referenceID" : 7,
      "context" : "Munos (2003; 2007); Munos & Szepesvári (2008), and the recent work of Farahmand et al.",
      "startOffset" : 0,
      "endOffset" : 46
    }, {
      "referenceID" : 3,
      "context" : "Munos (2003; 2007); Munos & Szepesvári (2008), and the recent work of Farahmand et al. (2010), which provides the most refined bounds for API and AVI, show how to do this process through quantities, called concentrability coefficients, that measure how a distribution over states may concentrate through the dynamics of the MDP.",
      "startOffset" : 70,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "Munos (2003; 2007); Munos & Szepesvári (2008), and the recent work of Farahmand et al. (2010), which provides the most refined bounds for API and AVI, show how to do this process through quantities, called concentrability coefficients, that measure how a distribution over states may concentrate through the dynamics of the MDP. We now state a lemma that generalizes the analysis of Farahmand et al. (2010) to a larger class of concentrability coefficients.",
      "startOffset" : 70,
      "endOffset" : 407
    }, {
      "referenceID" : 3,
      "context" : "Munos (2003; 2007); Munos & Szepesvári (2008), and the recent work of Farahmand et al. (2010), which provides the most refined bounds for API and AVI, show how to do this process through quantities, called concentrability coefficients, that measure how a distribution over states may concentrate through the dynamics of the MDP. We now state a lemma that generalizes the analysis of Farahmand et al. (2010) to a larger class of concentrability coefficients. We will discuss the potential advantage of this new class in Remark 4. We will also show through the proofs of Thms. 1 and 3, how the result of Lemma 3 provides us with a flexible tool for turning point-wise bounds into Lp-norm bounds. Thm. 3 in (Scherrer et al., 2012, Appendix D) provides an alternative bound for the loss of AMPI, which in analogy with the results of Farahmand et al. (2010) shows that the last iterations have the highest impact on the loss (the influence exponentially decreases towards the initial iterations).",
      "startOffset" : 70,
      "endOffset" : 853
    }, {
      "referenceID" : 8,
      "context" : "15 unifies and generalizes those for API (Munos, 2003) and AVI (Munos, 2007).",
      "startOffset" : 41,
      "endOffset" : 54
    }, {
      "referenceID" : 9,
      "context" : "15 unifies and generalizes those for API (Munos, 2003) and AVI (Munos, 2007).",
      "startOffset" : 63,
      "endOffset" : 76
    }, {
      "referenceID" : 7,
      "context" : "This potential leverage is an improvement over the existing bounds and concentrability results that only consider specific values of these two parameters: q = ∞ and q′ = 1 in Munos (2007); Munos & Szepesvári (2008), and q = q′ = 2 in Farahmand et al.",
      "startOffset" : 175,
      "endOffset" : 188
    }, {
      "referenceID" : 7,
      "context" : "This potential leverage is an improvement over the existing bounds and concentrability results that only consider specific values of these two parameters: q = ∞ and q′ = 1 in Munos (2007); Munos & Szepesvári (2008), and q = q′ = 2 in Farahmand et al.",
      "startOffset" : 175,
      "endOffset" : 215
    }, {
      "referenceID" : 3,
      "context" : "This potential leverage is an improvement over the existing bounds and concentrability results that only consider specific values of these two parameters: q = ∞ and q′ = 1 in Munos (2007); Munos & Szepesvári (2008), and q = q′ = 2 in Farahmand et al. (2010). Remark 5.",
      "startOffset" : 234,
      "endOffset" : 258
    }, {
      "referenceID" : 3,
      "context" : "In analogy with the results of Farahmand et al. (2010), this new bound shows that the last iterations have the highest influence on the loss (the influence exponentially decreases towards the initial iterations).",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 7,
      "context" : "Proof of Lemma 4 The proof of this lemma is similar to the proof of Theorem 1 in Lazaric et al. (2010). Before stating the proof, we report the following two lemmas that are used in the proof.",
      "startOffset" : 81,
      "endOffset" : 103
    }, {
      "referenceID" : 7,
      "context" : "This is a restatement of Lemma 1 in Lazaric et al. (2010). Lemma 7.",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "In these experiments, we are using the same setting as in Gabillon et al. (2011) to facilitate the comparison.",
      "startOffset" : 58,
      "endOffset" : 81
    } ],
    "year" : 2012,
    "abstractText" : "Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide error propagation analyses that unify those for approximate policy and value iteration. On the last classification-based implementation, we develop a finite-sample analysis that shows that MPI’s main parameter allows to control the balance between the estimation error of the classifier and the overall value function approximation.",
    "creator" : "LaTeX with hyperref package"
  }
}