{
  "name" : "1001.0921.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Graph Quantization",
    "authors" : [ "Brijnesh J. Jain", "Klaus Obermayer" ],
    "emails" : [ "jbj@cs.tu-berlin.de", "oby@cs.tu-berlin.de" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Vector quantization is a classical technique from signal processing suitable for lossy data compression, density estimation, and prototype-based clustering [7, 14, 30]. The problem of optimal vector quantizer design is to find a codebook consisting of a finite set of prototypes such that an expected distortion with respect to some (differentiable) distortion measure is minimized.\nSince the probability distribution of the input patterns is usually unknown, vector quantizer design techniques use empirical data. Extensively studied design techniques are, for example, k-means and simple competitive learning. The kmeans algorithm is also commonly referred to as the Linde-Buzo-Gray (LBG) algorithm [24] the generalized Lloyd algorithm [25]. This algorithm is a local optimizer of the empirical sum-of-squared-error distortion without any global optimal or consistency guarantees. In contrast to k-means, competitive learning directly minimizes the expected distortion and is a consistent learner under very general conditions in the sense that it almost surely converges to a local optimal solution of the expected distortion.\nOne limitation of VQ is its restriction to patterns that are represented by vectors. For patterns that are more naturally represented by finite combinatorial structures, the theoretical framework of VQ as well its design techniques are no longer applicable. Examples of such structures include, for example, point patterns, strings, trees, and graphs arising from diverse application areas like proteomics, chemoinformatics, and computer vision.\nTo overcome this limitation, we generalize vector quantization to quantization of graphs. A number of graph quantizer design techniques for the purpose of\nar X\niv :1\n00 1.\n09 21\nv1 [\ncs .A\nI] 6\nJ an\n2 01\n0\nprototype-based clustering have already been proposed. Examples include competitive learning algorithms in the domain of graphs [16–20, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29]. Related clustering method are presented in [3, 26, 31]. Due to a lack of an appropriate theoretical framework, all these graph quantizer design techniques (or clustering methods) have been developed in order to minimize an empirical distortion function without justifying whether the solutions found are statistically consistent estimators of the true but unknown solutions. In addition, it is unclear whether the nearest neighbor and centroid condition, which are also referred to as the Lloyd-Max conditions, are necessary conditions for optimality.\nIn this contribution, we propose graph quantization in a mathematically principled way as an extension of vector quantization, where we consider the graph edit distance as an underlying graph distortion measure. The key results of this contribution are consistency statements for estimators based on empirical distortion measures and estimators based on stochastic optimization. Furthermore, we prove that the Llyod-Max conditions are also necessary condition for optimal graph quantizers. In order to achieve the consistency results and the Lloyd-Max conditions, we isometrically embed – without loss of structural information – graphs as points into some Riemannian orbifold. An orbifold is the quotient of a manifold by a finite group action and therefore generalizes the notion of manifold. Using orbifolds we can define geometric and analytic concept such as length, angle, derivative, gradient, and integral locally to a Euclidean space. This construction forms the basis for extending consistency results from Euclidean vector spaces to the domain of graphs.\nThe proposed approach has the following properties: First, it can be applied to finite combinatorial structures other than graphs like, for example, point patterns, sequences, trees, and hypergraphs. For the sake of concreteness, we restrict our attention exclusively to the domain of graphs. Second, for graphs consisting of a single vertex with feature vectors as attributes, graph quantization coincides with vector quantization. Third, the proposed consistency results justify some of the above referenced graph clustering methods as statistically consistent learners. Fourth, the underlying mathematical framework can be applied in order to link other structural pattern recognition methods that directly operate in the domain of graphs to methods from statistical pattern recognition.\nThe paper is organizes as follows. Section 2 describes the problem of graph quantizer design. Section 3 introduces Riemannian orbifolds. In Section 4, we extend VQ to GQ and present consistency result for GQ design techniques. Section 5 briefly discusses the case of general graph edit distance functions. Finally, Section 6 concludes."
    }, {
      "heading" : "2 The Problem of Graph Quantizer Design",
      "text" : "This section aims at outlining the problem of extending VQ to the quantization of graphs."
    }, {
      "heading" : "2.1 Attributed Graphs",
      "text" : "To begin with, we first describe the structures we want to quantize.\nLet A be a set of attributes and let ε ∈ A be a distinguished element denoting the null or void element. An attributed graph is a tuple X = (V, α) consisting of a finite nonempty set V of vertices and an attribute function α : V × V → A. Elements of the set\nE = {(i, j) ∈ V × V : i 6= j and α(i, j) 6= ε}\nare the edges of X. By GA we denote the set of all attributed graphs with attributes from A. The vertex set of an attributed graph X is often referred to as VX and its attribute function as αX .\nAn alignment of a graph X is a graph X ′ with VX ⊆ VX′ and\nαX′(i, j) =\n{ αX(i, j) : (i, j) ∈ VX × VX\nε : otherwise\nfor all i, j ∈ VX′ . Thus, we obtain an alignment of X by adding isolated vertices with null-attribute. The set VX′ \\ VX is the set of aligned vertices. By A(X) we denote the (infinite) set of all alignments of X.\nA pairwise alignment of graphs X and Y is a triple (φ,X ′, Y ′) consisting of alignments X ′ ∈ A(X) and Y ′ ∈ A(Y ) together with a bijective mapping\nφ : VX′ → VY ′ , i 7→ iφ.\nBy A(X,Y ) we denote the set of all pairwise alignments between X and Y . Sometimes we briefly write φ instead of (φ,X ′, Y ′)."
    }, {
      "heading" : "2.2 The Graph Edit Distance",
      "text" : "Fundamental for quantizing data is the notion of distortion. This section briefly introduces the graph edit distance functions as our choice of distortion measure. For a more detailed definition of the graph edit distance, we refer to [2]. In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33]. For sake of convenience, we assume that all distances are metrics.\nEach pairwise alignment (φ,X ′, Y ′) ∈ A(X,Y ) can be regarded as an edit path with cost\ndφ (X,Y ) = ∑\ni,j∈VX′\ndA ( αX′(i, j), αY ′(i φ, jφ) ) ,\nwhere dA : A×A → R+ is a distance function defined on the set A of attributes. Observe that deletion (insertion) of vertices also deletes (inserts) all edges the respective vertices are incident to.\nThe graph edit distance of X and Y is then defined as the edit path with minimal cost\nd(X,Y ) = min {dφ (X,Y ) : φ ∈ A(X,Y )} .\nNote that the set A(X,Y ) of pairwise alignments is of infinite cardinality. But since dA(ε, ε) = 0, we actually take the minimum over a finite subset by ignoring all pairwise alignments that map aligned vertices with null-attributes onto each other.\nNext, we consider an important example of the graph edit distance based on a generalization of the concept of maximum common subgraph. We derive this graph metric from a similarity measure in the same way the Euclidean distance is derived from an inner product.\nSuppose that kA : A×A → R with kA(·, ε) = 0 is a positive definite kernel. We measure the quality of a pairwise alignment φ ∈ A(X,Y ) by\nkφ(X,Y ) = ∑\ni,j∈VX\nkA ( αX(i, j), αY (i φ, jφ) ) .\nAn optimal alignment kernel is a graph similarity measure of the form\nk(X,Y ) = max {kφ(X,Y ) : φ ∈ A(X,Y )} . (1)\nNote that k (·|·) is symmetric but indefinite as a pointwise maximizer of a set of positive definite kernels.\nThe distance metric on GA induced by an optimal alignment kernel k (·|·) is defined by\nd(X,Y ) = √ l(X)2 − 2k(X,Y ) + l(Y )2, (2)\nwhere l(X) = √ k(X,X) denotes the length of an attributed graph X. As shown in [23], d is indeed a metric and can be expressed as a graph edit distance."
    }, {
      "heading" : "2.3 The Problem of Graph Quantizer Design",
      "text" : "Let (GA, d) be a graph distance space, where d (·|·) is a graph edit distance. Optimal graph quantization design aims at minimizing the expected distortion\nD(C) = ∫ GA d(X,Q(X)) dP (X),\nwhere Q : GA → C is a graph quantizer, C = {Y1, . . . , Yk} a codebook consisting of k code graphs, and P = PGA is a probability measure defined on some appropriate measurable space (GA, ΣGA).\nAs opposed to vector quantization, the following factors complicate designing an optimal graph quantizer in a statistically consistent way:\n1. The graph distance d(X,Y ) is in general non-convex and non-differentiable. 2. Neither a well-defined addition on graphs nor the notion of derivative for\nfunctions on graphs is known.\nTo overcome these difficulties, we isometrically embed graphs as points into a Riemannian orbifold in order to apply methods that generalize gradient descent techniques and methods from stochastic optimization for non-convex and nondifferentiable distortion functions."
    }, {
      "heading" : "3 Riemannian Orbifolds",
      "text" : "Orbifolds generalize the notion of manifold as locally being a quotient of Rn by finite group actions. Consequently, learning on orbifolds generalizes learning on Euclidean spaces and Riemannian manifolds. This section introduces Riemannian orbifolds and their intrinsic metric structure. Proofs for new results are delegated to Section B.1. For all other proofs we refer to [4, 21]."
    }, {
      "heading" : "3.1 Riemannian Orbifolds",
      "text" : "To keep the treatment simple, we assume that X = Rn is the n-dimensional Euclidean vector space, and Γ is a permutation group acting on X . In a more general setting, however, we can assume that X is a Riemannian manifold, and Γ is a finite group of isometries acting effectively on X .\nThe binary operation\n· : Γ ×X → X , (γ,x) 7→ γ(x)\nis a group action of Γ on X . For x ∈ X , the orbit of x is the set defined by\n[x] = {γ(x) : γ ∈ Γ} .\nThe quotient set XΓ = X/Γ = {[x] : x ∈ X}\nconsisting of all all orbits carries the structure of a Riemannian orbifold. Its orbifold chart is the surjective continuous mapping\nπ : X → XΓ , x 7→ [x]\nthat projects each point x to its orbit [x]. In the following, an orbifold is a triple Q = (X , Γ, π) consisting of an Euclidean space X , a permutation group Γ acting on X and its orbifold chart π. With Γ = {id} being the trivial permutation group consisting of the identity only, a manifold X is also an orbifold. In general, however, the underlying space XΓ of an orbifold is not a manifold. Thus, orbifolds generalize the notion of manifold. The points at which an orbifold XΓ is locally not homeomorphic to a manifold are its singular points. We call the elements of XΓ structures, since they represent combinatorial structures like attributed graphs. We use capital letters X,Y, Z, . . . to denote structures from XΓ and write, by abuse of notation, x ∈ X if π(x) = X. Each vector x ∈ X is a vector representation of structure X and the set X of all vector representation is the representation space of XΓ .\nExample 1. Let X = R2 and let Γ be the group generated by reflections across the main-diagonal of the x-y-plane. Then Q = (XΓ , Γ, π) is a Riemannian orbifold with\nπ : X → XΓ , x = (x1, x2) 7→ [x] = {(x1, x2), (x2, x1)} .\nThe singular points of XΓ are all structuresX represented by vectors x = (x1, x2) with x1 = x2."
    }, {
      "heading" : "3.2 The Riemannian Orbifold of Attributed Graphs",
      "text" : "In this section, we show that attributes graphs can be identified with points in some Riemannian orbifold.\nRiemannian orbifolds of attributed graphs arise by considering equivalence classes of matrices representing the same graph. To identify graphs with points in a Riemannian orbifold without loss of structural information, some technical assumptions and restrictions to simplify the mathematical treatment are necessary. For this, let (GA, d) be a graph distance space with graph edit distance d(·|·). Then we make the following assumptions:\nP1 There is a feature map Φ : A → H of the attributes into some finite dimensional Euclidean feature space H and a distance function dH : H×H → R+ such that Φ(ε) = 0 ∈ H and\ndA(a, a ′) = dH(Φ(a), Φ(a ′))\nfor all attributes a, a′ ∈ A. P2 All graphs are finite of bounded order n, where n is a sufficiently large\nnumber. Graphs X of order less than n, say m < n, are aligned to graphs X ′ of order n by inserting p = n−m isolated vertices with null attribute ε.\nBefore discussing the impact of both assumptions for practical application, we first restate our first assumptions for graph metrics induced by optimal alignment kernels. By definition kA : A×A → R is a positive definite kernel corresponding to an inner product kA(x, y) = 〈Φ(x), Φ(y)〉 in some feature space H. Our first assumption requires thatH is a finite dimensional Euclidean space and Φ(ε) = 0.\nNow let us consider the above assumptions in more detail. Both conditions do not effect the graph edit distance, provided an appropriate feature map for the attributes can be found. Restricting to finite dimensional Euclidean feature spaces H is necessary for deriving consistency results and for applying methods from stochastic optimization. Limiting the maximum size of the graphs to some arbitrarily large number n and aligning smaller graphs to graphs of oder n are purely technical assumptions to simplify mathematics. For machine learning problems, this limitation should have no practical impact, because neither the bound n needs to be specified explicitly nor an extension of all graphs to an identical order needs to be performed. When applying the theory, all we actually require is that the order of the graphs is bounded.\nWith both assumptions in mind, we construct the Riemannian orbifold of attributed graphs. Let X = Hn×n be the set of all (n×n)-matrices with elements from feature space H. A graph X is completely specified by a representation matrix X = (xij) from X with elements\nxij =  φ (µX(i)) : i = jφ (νX(i, j)) : (i, j) ∈ E 0 : otherwise\nfor all i, j ∈ VX . The form of a representation matrix X of X is generally not unique and depends on how the vertices are arranged in the diagonal of X.\nNow suppose that Πn be the set of all (n × n)-permutation matrices. For each P ∈ Πn we define a mapping\nγP : X → X , X 7→ P TXP .\nThen Γ = {γP : P ∈ Πn} is a permutation group acting on X . Regarding an arbitrary matrix X as a representation of some graph X, then the orbit [X] consists of all possible matrices that can represent X. By identifying the orbits of XΓ with attributed graphs, the set GA of attributed graphs of bounded order n is a Riemannian orbifold."
    }, {
      "heading" : "3.3 Metric Structures",
      "text" : "Let Q = (X , Γ, π) be an orbifold. We derive an intrinsic metric that enables us to do Riemannian geometry. In the case of a Riemannian orbifold of attributed graphs the intrinsic metric coincides with the graph metric of (2) induced by an optimal alignment kernel.\nAny inner product 〈·, ·〉 on X gives rise to a maximizer of the form\nk : XΓ ×XΓ → R, (X,Y ) 7→ max {〈x,y〉 : x ∈ X,y ∈ Y } .\nWe call the kernel function k(·|·) optimal alignment kernel, induced by the inner product 〈·, ·〉. Note that the maximizer of a set of positive definite kernels is an indefinite kernel in general. Since Γ is a group, we find that\nk(X,Y ) = max {〈x,y〉 : x ∈ X} .\nwhere y is an arbitrary but fixed vector representation of Y . In general, we have\nk(X,Y ) ≥ 〈x,y〉\nfor all x ∈ X and y ∈ Y .\nExample 2. Consider the Riemannian orbifold (X , Γ, π) of Example 1, where X = R2 and Γ = {id, γ} is the group generated by reflections across the x-yplane. Suppose that x = (1, 2) is a vector representation of X and y = (3, 2) is a\nvector representation of Y . Then the optimal alignment kernel k (X,Y ) induced by the standard inner product of X is given by\nk(X,Y ) = max {〈x,y〉, 〈γ(x),y〉, 〈x, γ(y〉), 〈γ(x), γ(y〉)}\nEvaluating the inner products yields\n〈x,y〉 = 〈(1, 2), (3, 2)〉 = 7 〈γ(x),y〉 = 〈(2, 1), (3, 2)〉 = 8 〈x, γ(y)〉 = 〈(1, 2), (2, 3)〉 = 8\n〈γ(x), γ(y)〉 = 〈(2, 1), (2, 3)〉 = 7.\nThus, we have k(X,Y ) = 8.\nExample 3. Suppose that X and Y are attributed graphs where edges have attribute 1 and vertices have attribute 0. The optimal alignment kernel k (X,Y ) induced by the standard inner product of X is the number of edges of a maximum common subgraph of X and Y .\nExample 4. More generally, if property P1 is satisfied, then any optimal alignment kernel on a bounded set of attributed graphs as defined in (1) is also an optimal assignment kernel of some Riemannian orbifold.\nSuppose that X ∈ XΓ . Since k(X,X) = 〈x,x〉 for all x ∈ X, we can define the length of X by l(X) = √ k(X,X).\nThe optimal alignment kernel together with the length satisfies the CauchySchwarz inequality\n|k(X,Y )| ≤ l(X) · l(Y ).\nSince the Cauchy-Schwarz inequality is valid, the geometric interpretation of k(·|·) is that it computes the cosine of a well-defined angle between X and X ′ provided they are normalized to length 1.\nLikewise, k(·|·) gives rise to a distance function defined by d(X,Y ) = √ l(X)2 − 2k(X,Y ) + l(Y ).\nFrom the definition of k(·|·) follows that d is a metric. In addition, we have\nd(X,Y ) = min {‖x− y‖ : x ∈ X,y ∈ Y }, (3)\nwhere ‖·‖ denotes the Euclidean norm induced by the inner product 〈·, ·〉 of the Euclidean space X .\nExample 5. Consider the Riemannian orbifold (X , Γ, π) of Example 1 and 2. Suppose that x = (1, 2) is a vector representation of X and y = (3, 2) is a vector representation of Y . Then the squared lengths of X and Y are l(X)2 = 5 and l(Y )2 = 13. Since k(X,Y ) = 8 according to Example 2, the distance is d(X,Y ) = √ 5− 16 + 13 = √ 2.\nExample 6. If properties P1 and P2 are satisfied, then the graph metric (2) coincides with the intrinsic orbifold metric (3).\nEquation (3) states that d (·|·) is the length of a minimizing geodesic of X and Y and therefore an intrinsic metric, because it coincides with the infimum of the length of all admissible curves from X to Y . In addition, we find that the topology of XΓ induced by the metric d coincides with the quotient topology induced by the topology of the Euclidean space X ."
    }, {
      "heading" : "3.4 Orbifold Functions",
      "text" : "Suppose that Q = (X , Γ, π) is an orbifold. An orbifold function is a mapping\nf : XΓ → R.\nThe lift of f is a function f̃ : X → R\nsatisfying f̃ = f ◦ π. The lift f̃ is invariant under group actions of Γ , that is f̃(x) = f̃ (γ(x)) for all γ ∈ Γ .\nWe say, an orbifold function f : XΓ → R is continuous (locally Lipschitz, differentiable, generalized differentiable) at X ∈ XΓ if its lift f̃ is continuous (locally Lipschitz, differentiable, generalized differentiable) at some vector representation x ∈ X. The definition is independent of the choice of the vector representation that projects to X (see Section B.1, Prop. 1 – Prop. 4). For a definition of generalized differentiable functions and their basic properties we refer to Section A.\nExample 7. Consider the Riemannian orbifold (X , Γ, π) of Example 1-5. The function\nfY : XΓ → R, X 7→ k(X,Y )\nfor some Y ∈ XΓ is an orbifold function with lift\nf̃Y : X → R, x 7→ max {〈x,y〉, 〈x, γ(y)〉},\nwhere y ∈ Y . Analytical properties of f such as continuity and differentiability can be investigated using the lift f̃ of f . For example, if f̃ is differentiable at x ∈ X then it is also differentiable at γ(x) according to Prop. 3. Hence, differentiability of the orbifold function f is well-defined at X."
    }, {
      "heading" : "3.5 Gradients and Generalized Gradients of Orbifold Functions",
      "text" : "We extend the notion of gradient and generalized gradient to differentiable and generalized differentiable orbifold functions.\nGradient of Differentiable Orbifold Functions. Suppose that f : XΓ → R is differentiable at X ∈ XΓ . Then its lift f̃ : X → R is differentiable at all vector representations that project to X. The gradient ∇f(X) of f at X is defined by the projection\n∇f(X) = π ( ∇f̃(x) ) of the gradient ∇f̃(x) of f̃ at a vector representation x ∈ X. This definition is independent of the choice of the vector representation. We have\n∇f̃(γ(x)) = γ ( ∇f̃(x) ) for all γ ∈ Γ . This implies that the gradients of f̃ at x and γ(x) are vector representations of the same structure, namely the gradient∇f(X) of the orbifold function f at X. Thus, the gradient of f at X is a well-defined structure pointing to the direction of steepest ascent (see Section B.1, Prop. 3).\nSubdifferential of Generalized Differentiable Orbifold Functions. Suppose that f : XΓ → R is generalized differentiable at X ∈ XΓ . Then its lift f̃ : X → R is generalized differentiable at all vector representations that project to X. The subdifferential ∂f(X) of f at X is defined by the projection\n∂f(X) = π ( ∂f̃(x) ) of the subdifferential ∂f̃(x) of f̃ at a vector representation x ∈ X. This definition is independent of the choice of the vector representation. We have\n∂f̃(γ(x)) = γ ( ∂f̃(x) ) for all γ ∈ Γ . This implies that the subdifferentials ∂f̃(x) ⊆ X and ∂f̃(γ(x)) ⊆ X are subsets that project to the same subset of XΓ , namely the subdifferential ∂f(X) (see Section B.1, Prop. 4).\nThe properties of generalized differentiable function as listed in Section A carry over to generalized differentiable orbifold functions via their lifts. For example, a generalized differentiable orbifold function is locally Lipschitz and therefore differentiable almost everywhere.\nExample 8. Let (GA, d) be a graph space, where\nd(X,Y ) = min φ∈A(X,Y ) dφ(X,Y )\nis a graph edit distance. We can identify GA with a Riemannian orbifold Q = (X , Γ, π) and the graph edit distance d (·|·) with a distance function defined on XΓ . Suppose that the cost functions dφ (·|·) of the edit paths are continuously differentiable (generalized differentiable). Then the distance d (·|·) is generalized differentiable.\nExample 9. Let Q be a Riemannian orbifold of attributed graphs. Then (i) an optimal assignment kernel k (·|·), (ii) the intrinsic metric d (·|·) induced by k (·|·), and (iii) the squared metric d (·|·)2 are generalized differentiable."
    }, {
      "heading" : "3.6 Integration on Orbifolds",
      "text" : "Suppose that Q = (X , Γ, π) is a Riemannian orbifold with singular set SQ. In order to integrate orbifold functions f : XΓ → R by the Lebesgue integral, we need to construct an appropriate measurable space together with an orbifold measure. The measurable space is defined by the Borel set B(XΓ ) generated by the open sets of XΓ . From the orbifold measure we expect that it is compatible with the local Riemannian measures. In addition, we demand that the singular set SQ has measure 0. This is motivated by the following fact: The singular set is covered locally by the finite union of totally geodesic submanifolds, which has measure 0 relative to the local canonical Riemannian measure. Since the projection to the orbifold is distance decreasing, it is reasonable to ask for an orbifold measure that assigns measure 0 to the singular set SQ.\nLet B (XΓ \\ SQ) denote the Borel set generated by the open sets of XΓ \\ SQ. Then there exists a complete canonical measure µ on the the Borel set B (XΓ \\ SQ) given by a unique volume form on XΓ \\ SQ. The measure µ can be extended to a complete measure ν on the Borel set B(XΓ ) such that\nν (A) = µ (A \\ SQ) = ∫ A\\SQ dµ.\nIn particular, we have ν(A) = 0 for any subset A ⊆ SQ. For proofs we refer to [4].\nIn the following we write∫ UΓ f(X)dX = ∫ UΓ fdν\nfor the integral of an orbifold function f : UΓ → R defined on a measurable subset UΓ ⊆ XΓ . We tacitly assume that all integrals occurring in the following sections exist."
    }, {
      "heading" : "4 Graph Quantization",
      "text" : "This section extends vector quantization to quantization of graphs."
    }, {
      "heading" : "4.1 The Basics",
      "text" : "Suppose that Q = (X , Γ, π) is a Riemannian orbifold. A graph quantizer of size k is a mapping of the form\nQ : XΓ → C\nwhere C = {Y1, . . . , Yk} ⊆ XΓ is a finite set, called codebook. The elements Yj ∈ C are the code graphs. The graph quantizer Q partitions the input space XΓ into k disjoint regions\nRj = {X ∈ XΓ : Q(X) = Yj}\nsuch that their union covers XΓ . By PQ we denote the partition of Q consisting of all k regions Rj .\nSuppose that J = {1, . . . , k}. The basic operation of a vector quantizer Q can be written as a composition Q = dQ ◦ eQ of an encoder eQ : XΓ → J and a decoder dQ : J → C. The encoder assigns each input graph to a region via the index set J . The decoder maps indices of J referring to regions to code graphs."
    }, {
      "heading" : "4.2 Graph Quantizer Performance",
      "text" : "We measure the performance of a graph quantizer Q by the expected distortion D(Q) = EX [d (X,Q(X))] = ∫ XΓ d(X,Q(X))dP (X),\nwhere X ∈ XΓ is a random variable with probability measure P = PXΓ representing the observable graphs to be quantized. The expectation EX is taken with respect to some probability space (XΓ , ΣXΓ , PXΓ ). The quantity d(X,Y ) measures the distortion of the random input graph X and code graph Y . Here we consider graph distortion measures that are graph edit distances. An example is the squared metric induced by an optimal alignment kernel\nd (X,Y ) = min x∈X,y∈Y\n‖x− y‖2\nUsing the codebook and partition for the given quantizer Q, we can rewrite the expected distortion by\nD(C) = k∑ j=1 ∫ Rj d(X,Y )dP (X)."
    }, {
      "heading" : "4.3 The Problem of Optimal Graph Quantizer Design",
      "text" : "The problem of optimal graph quantizer design is stated as follows: Find a codebook C specifying the decoder dQ and a partition PQ specifying the encoder eQ such that the expected distortionD(Q) is minimized. The composite mapping Q = dQ ◦ eQ of the resulting encoder and decoder is then an optimal graph quantizer.\nAn optimal graph quantizer satisfies the following necessary conditions, also known as the Lloyd-Max conditions:\n1. Nearest Neighbor Condition. Given a fixed codebook C, a graph quantizer Q is optimal, if the code vector Q(X) of an input pattern X satisfies the nearest neighbor rule\nQ(X) = argmin Y ∈C d (X,Y )\nfor all X ∈ XΓ , where ties are resolved according to some rule. A proof is given in Section B.2, Theorem 3.\n2. Centroid Condition. Given a fixed partition PQ, a vector quantizer Q is optimal, if each code vector Yj is the centroid of region Rj , that is\nYj = arg min Y ∈XΓ\nE [d (X,Y ) |X ∈ Rj ]\nfor all Y ∈ XΓ and all j ∈ J . A proof is given in Section B.2, Theorem 4.\nNote that Yj with\nYj = arg min Y ∈XΓ\nE [d (X,Y ) |X ∈ Rj ]\nis called a centroid of region Rj . The centroids may not be unique. This also holds for squared metrics induced by some optimal assignment kernel, which are the counterparts of squared Euclidean distances."
    }, {
      "heading" : "4.4 Graph Quantizer Design",
      "text" : "Since the distribution P = PXΓ of the observable graphs is usually unknown, the expected distortion D(C) can neither be computed nor be minimized directly. Instead, we design (estimate) an optimal quantizer from empirical data. For vectors, prominent methods for designing an optimal quantizer are k-means and simple competitive learning. Both methods, k-means and simple competitive learning have been extended for designing graph quantizers in the context of prototype based clustering. To derive consistency results for k-means and simple competitive learning in the domain of graphs, we consider estimators based on empirical distortions and on stochastic approximation.\nEstimators based on Empirical Distortion Measures. In order to derive consistency results, we restrict the set of feasible codebooks to a compact subspace\nW ⊂ X kΓ = XΓ × · · · × XΓ︸ ︷︷ ︸ k-times\nof the topological space X kΓ . The problem of designing an optimal quantizer for graphs is then of the form\nmin C∈W D(C) = k∑ j=1 ∫ Rj d(X,Y )dP (X).\nwhere the minimum is taken over the compact set W rather than X kΓ . Let\n1. D∗ be the set of minimal values of the expected distortion D(C), 2. W∗ = {C ∈ W : D(C) = D∗} be the set of true (optimal) codebooks, and 3. W∗ε = {C ∈ W : D(C) ≤ D∗ + ε} be the set of approximate solutions.\nTo design an optimal graph quantizer, we minimize the empirical distortion\nD̂N (C) = 1\nN N∑ i=1 min j∈J d (Xi, Yj) ,\nwhere C ∈ W and S = {X1, . . . , XN} is a training set consisting of N independent graphs Xi drawn from XΓ . Let\n1. D̂∗N be the set of minimal values of the empirical distortion D̂N (C), 2. W∗N = {C ∈ W : D̂N (C) = D̂∗N} be the set of empirical codebooks, and 3. W∗Nε = {C ∈ W : D̂N (C) ≤ D̂∗N + ε} be the set of approximate solutions.\nThe next result shows that estimators based on empirical distortions are consistent estimators.\nTheorem 1. Suppose that Q = (X , Γ, π) is a Riemannian orbifold, d(X,Y ) is a locally Lipschitz metric on XΓ with integrable Lipschitz constant, and W ⊆ X kΓ is compact. Then we have\nlim N→∞\nD̂∗N (ω) = D ∗\nlim N→∞\nW∗N (ω) =W∗\nlim N→∞\nW∗ N (ω) =W∗\nalmost surely.\nThe proof follows from [8] applied to the lift d̃ of distortion d. Examples of locally Lipschitz distance metrics on XΓ with integrable Lipschitz constants are metrics induced by an optimal alignment kernel\nd(X,Y ) = min x∈X,y∈Y\n‖x− y‖\nas well as d(X,Y )2.\nK-Means. In order to extend the standard k-means method to graphs for constructing an empirical codebook, we use the following update rule\nyt+1j = 1\nN tj N∑ i=1 qtijxi,\nwhere t > 0 is the iteration, xi ∈ Xi and ytj ∈ Y tj are vector representations that are optimally aligned,1 and Qt = ( qtij ) is the matrix representation of the nearest neighbor quantizer Qt restricted to the training set S. The elements of Qt are of the form\nqtij =\n{ 1 : Qt(Xi) = Y t j\n0 : otherwise .\n1 Recall that two vector representations x ∈ X and y ∈ Y are optimally aligned if ‖x− y‖ = d(X,Y )\nThe quantity N tj denotes the number of elements from the training sets that are quantized by code graph Y tj .\nAs for vectors, a drawback of k-means for graphs is that it is a local optimization technique for which existing consistency theorems are inapplicable, because Theorem 1 assumes global instead of local minimizers of the empirical distortion as estimators.\nEstimators based on Stochastic Optimization. Suppose that W = X kΓ . Stochastic optimization methods directly minimize the expected distortion\nD (C) = k∑ j=1 ∫ Rj d (X,Yj) dP (X)\n= k∑ j=1 ∫ XΓ min 1≤j≤k d (X,Yj) dP (X),\nusing a training set S = {X1, . . . , XN} of N independent graphs Xi drawn from XΓ . We assume that the loss function\nL(X, C) = min 1≤j≤k d (X,Yj)\nis generalized-differentiable, hence L(X, C) is differentiable almost everywhere.\nExample 10. If he graph distortion d(·|·) is generalized differentiable, then the loss function L(X, C) is also generalized differentiable by calculus of generalized differentiable functions. This holds for graph distortions of Example 8 and 9.\nSince the interchange of integral and generalized gradient remains valid for generalized differentiable loss functions, that is\n∂D(C) = EX [∂L(X, C)]\nunder mild assumptions (see [11, 27]), we can minimize the expected distortion D(C) according to the following stochastic generalized gradient (SGG) method:\nyt+1 = yt + ηt (xt − yt), (4)\nwhere xt is a vector representation of input pattern Xt ∈ S, which is optimally aligned to vector representation yt of a code graph Yt closest to Xt. The random elements st = xt − yt ∈ St are vector representations of stochastic generalized gradients St, i.e. random variables defined on the probability space (XΓ , ΣXΓ , PXΓ ) ∞ such that\nE [St | C0, . . . , Ct] ∈ ∂D (C) . (5)\nWe consider the following conditions for almost sure convergence of stochastic optimization:\nA1 The sequence (ηt)t≥0 of step sizes satisfies\nηt > 0, lim t→∞ ηt = 0, ∞∑ t=1 ηt =∞, ∞∑ t=1 η2t <∞.\nA2 The stochastic generalized gradients (St)t≥0 satisfy (5). A3 The expected squared norm of stochastic generalized gradients (St)t≥0 is\nbounded by E [ ‖St‖2 ] < +∞.\nThe next result shows that the SGG method is a consistent estimator. Theorem 2. Let Q = (X , Γ, π) be a Riemannian orbifold and let d(X,Y ) be a generalized differentiable metric on XΓ . Suppose that assumptions (A1) − (A3) hold. Then the sequence (Ct)t≥0 generated by the SGG method converges almost surely to graphs satisfying necessary extremum conditions\nW∗ = {C ∈ W : 0 ∈ ∂D(C)} .\nBesides the sequence (D(Ct))t≥0 converges almost surely and we have\nlim t→∞\nD(Ct) ∈ D(W∗).\nThe proof is a direct consequence of Ermoliev and Norkin’s Theorem [11] applied on the lift d̃ (·|·) of d (·|·)."
    }, {
      "heading" : "5 Remarks to GQ using the Graph Edit Distance",
      "text" : "In many applications, the graph edit distance is discontinuous. Examples include edit distances with constant non-zero deletion and/or insertion cost. A necessary (but not sufficient) condition for the consistency results stated in Theorem 1 and 2 is that the underlying graph distortion is locally Lipschitz. Hence, both consistency results are inapplicable for discontinuous graph distortions. Let us consider both cases separately.\nEstimators based on Empirical Distortion Measures. Estimators based on empirical distortion measures aim at approximating the expected distortion D(C) by its empirical mean\nmin C∈W\nD̂N (C) = 1\nN N∑ i=1 min j∈J d (Xi, Yj) .\nAs shown in [10], minimizing the empirical distortion is often meaningless, if the underlying graph edit distance function d (·|·) and thus D̂N (C) is discontinuous, even if the expectation D(C) may be continuously differentiable. Since the local solutions of D̂N (C) may have nothing in common with the local solutions of the original problem, estimators based on the empirical distortion D̂N (C) can be statistically inconsistent. Hence, minimizing D̂N (C) with underlying discontinuous graph edit distance using global or local optimization techniques like, for example, k-means lacks theoretical support.\nEstimators based on Stochastic Optimization. The situation is better for estimators based on methods from stochastic optimization. For discontinuous graph edit distances d (·|·) the expected distortion can be minimized in a statistically consistent way, for example, by methods based on approximations of d (·|·) via averaged functions obtained by convolution with so-called mollifiers. For details, we refer to [9]."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This contribution proposes a theoretical sound foundation of graph quantization generalizing the ideas of vector quantizations to the domain of attributed graph. We presented consistency results for graph quantizer design, where the underlying graph edit distances is generalized differentiable. As for vectors, estimators based on empirical distortion and stochastic optimization are statistically consistent. If the underlying distortion measure is a discontinuous graph edit distance, estimators based on empirical distortion measures lack theoretical justification. Thus, the proposed consistency results justify existing research on prototype-based clustering in the domain of graphs. In addition, we showed that the Lloyd-Max conditions are necessary conditions for optimality of GQ.\nThe mathematical framework that enables us to derive consistency results are Riemannian orbifolds. Identifying graphs with points in a Riemannian orbifold provides us locally access to a Euclidean space. This in turn allows us to introduce geometrical and analytical concepts for extending vector quantization to the domain of graphs. The implication of this approach is that it provides us a template for consistently linking methods from structural pattern recognition other than GQ to statistical pattern recognition methods.\nAcknowledgments. The first author is very grateful to Vladimir Norkin for his kind support and valuable comments."
    }, {
      "heading" : "A Generalized Differentiable Functions",
      "text" : "Let X = Rn be a finite-dimensional Euclidean space. A function f : X → R is generalized differentiable at x ∈ X in the sense of Norkin [27] if there is a multi-valued map ∂f : X → 2X in a neighborhood of x such that\n1. ∂f(x) is a convex and compact set; 2. ∂f(x) is upper semicontinuous at x, that is, if yi → x and gi ∈ ∂f(yi) for\neach i ∈ N, then each accumulation point g of (gi) is in ∂f(x); 3. for each y ∈ X there is a g ∈ ∂f(y) with f(y) = f(x)+〈g,y − x〉+o (x,y, g),\nwhere lim i→∞ |o (x,yi, gi)| ‖yi − x‖ = 0\nfor all sequences yi → y and gi → g with gi ∈ ∂f (yi).\nWe call f generalized differentiable if it is generalized differentiable at each point x ∈ X . The set ∂f(x) is the subdifferential of f at x and its elements are called generalized gradients.\nGeneralized differentiable functions have the following properties [27]:\n(GD1) Generalized differentiable functions are locally Lipschitz and therefore continuous and differentiable almost everywhere. (GD2) Continuously differentiable, convex, and concave functions are generalized differentiable. (GD3) Suppose that f1, . . . , fn : X → R are generalized differentiable at x ∈ X . Then\nf∗(x) = min(f1(x), . . . , fm(x)) f∗(x) = max(f1(x), . . . , fm(x))\nare generalized differentiable at x ∈ X . (GD4) Suppose that f1, . . . , fm : X → R are generalized differentiable at x ∈ X\nand f0 : Rm → R is generalized differentiable at y = (f1(x), . . . , fm(x)) ∈ Rm. Then f(x) = f0(f1(x), . . . , fm(x)) is generalized differentiable at x ∈ X . The subdifferential of f at x is of the form\n∂f(x) = con { g ∈ X : g = [ g1g2 . . . gm ] g0,\ng0 ∈ ∂f0(y), gi ∈ ∂fi(x), 1 ≤ i ≤ m } .\nwhere [g1g2 . . . gm] is a (N ×m)-matrix. (GD5) Suppose that F (x) = Ez [f(x, z)], where f(·, z) is generalized differen-\ntiable. Then F is generalized differentiable and its subdifferential at x ∈ X is of the form ∂F (x) = Ez [∂f(x, z)]."
    }, {
      "heading" : "B Proofs",
      "text" : "Suppose thatQ = (X , Γ, π) is a Riemannian orbifold. By Uδ(x) = {x′ : ‖x′‖ < δ} we denote the open ball with center x and radius δ > 0. Note that Uδ(γ(x)) = γ (Uδ(x)) for all γ ∈ Γ .\nB.1 Orbifold Functions\nContinuous Orbifold Functions\nProposition 1. Let f : XΓ → R be an orbifold function. Suppose that its lift f̃ : X → R is continuous at a vector representation x that projects to X ∈ XΓ . Then f̃ is continuous at γ(x) for all γ ∈ Γ .\nProof. Let γ ∈ Γ be a permutation and x′ = γ(x). Suppose that (y′i)i∈N is a sequence with y′i → x′. Then there is a sequence (yi)i∈N with γ(yi) = y′i for each i ∈ N. Since permutations are homeomorphisms, we find that\nlim i→∞ yi = lim i→∞\nγ−1(y′i) = γ −1(x′) = x.\nFrom continuity of f̃ at x follows that f̃(yi)→ f̃(x). Since f̃ is invariant under group actions from Γ , we have f̃(x) = f̃(x′) and f̃(yi) = f̃(y′i) for each i ∈ N. We obtain\nlim i→∞ f̃ (y′i) = lim i→∞ f̃ (yi) = f̃(x) = f̃(x ′).\nThis proves that f̃ is continuous at each vector representation that projects to X. ut\nLocally Lipschitz Orbifold Functions\nProposition 2. Let f : XΓ → R be an orbifold function. Suppose that its lift f̃ : X → R is locally Lipschitz at a vector representation x that projects to X ∈ XΓ . Then f̃ is locally Lipschitz at γ(x) for all γ ∈ Γ .\nProof. Since f̃ is locally Lipschitz at x there is a L ≥ 0 and δ > 0 such that∣∣∣f̃(y)− f̃(z)∣∣∣ ≤ L ‖y − z‖ for all y, z ∈ Uδ(x). Let γ ∈ Γ be a permutation and x′ = γ(x). Since γ is an isometric homeomorphism, we have Uδ(x′) = γ (Uδ(x)). From Γ -invariance of f̃ and the isometric property of γ follows∣∣∣f̃(y′)− f̃(z′)∣∣∣ = ∣∣∣f̃(y)− f̃(z)∣∣∣ ≤ L ‖y − z‖ = L ‖y′ − z′‖ for all y′, z′ ∈ Uδ(x′), where y = γ−1(y′) ∈ Uδ(x) and z = γ−1(z) ∈ Uδ(x). This proves that f̃ is locally Lipschitz at each vector representation that projects to X. ut\nDifferentiable Orbifold Functions\nProposition 3. Let f : XΓ → R be an orbifold function. Suppose that its lift f̃ : X → R is differentiable at a vector representation x that projects to X ∈ XΓ . Then f̃ is differentiable at γ(x) for all γ ∈ Γ . The gradient of f̃ at γ(x) is of the form\n∇f̃(γ(x)) = γ ( ∇f̃(x) ) .\nProof. Since the lift f̃ of f is differentiable at x, there is a δ > 0 such that f̃(x+ h) = f̃(x) + 〈 ∇f̃ (x),h 〉 + o(h)\nfor all h ∈ Uδ(0). Let x′ be an arbitrary vector representation that projects to X. Then there is a γ ∈ Γ with x′ = γ(x). Since f̃ is invariant under the group actions of Γ , we have f̃(x′) = f̃(x). Then for each h′ ∈ Uδ(0), we find that\nf̃(x′ + h′)− f̃(x′) = f̃(x+ h)− f̃(x) = 〈 ∇f̃ (x),h 〉 + o(h),\nwhere h ∈ X with γ(h) = h′. Since the elements of Γ are isometries, we have ‖h‖ = ‖h′‖ giving h ∈ Uδ(0). In addition, from isometry of γ follows\n〈fx,h〉 = 〈 γ ( ∇f̃ (x) ) , γ(h) 〉 = 〈 γ ( ∇f̃ (x) ) ,h′ 〉 .\nWe obtain f̃(x′ + h′)− f̃(x′) = 〈 γ ( ∇f̃ (x) ) ,h′ 〉 + o′(h′),\nwhere o′(h′) = o ◦ γ−1(h′) satisfies\nlim h′→0\no′(h′)\n‖h′‖ = lim h′→0\no(γ−1(h′))\n‖h′‖ = lim h′→0\no(γ−1(h′)) ‖γ−1(h′)‖ = 0.\nThis proves that f̃ is differentiable at each vector representation that projects to X. In addition, from the proof follows that the gradient of f̃ at x′ = γ(x) is of the form\n∇f̃ (x′) = γ ( ∇f̃ (x) ) .\nut\nGeneralized Differentiable Orbifold Functions\nProposition 4. Let f : XΓ → R be an orbifold function. Suppose that its lift f̃ : X → R is generalized differentiable at a vector representation x that projects to X ∈ XΓ . Then f̃ is generalized differentiable at γ(x) for all γ ∈ Γ and\n∂f̃(γ(x)) = γ ( ∂f̃(x) ) .\nis a subdifferential of f̃ at γ(x) for all γ ∈ Γ .\nProof. Since f̃ is generalized differentiable at x, there is a multi-valued mapping ∂f̃ : Uδ(x)→ 2X defined on some neighborhood Uδ(x). Let γ ∈ Γ be an arbitrary permutation and x′ = γ(x). Then\n∂f̃ : Uδ(x′)→ 2X , y′ = γ(y) 7→ γ ( ∂f̃(y) ) is a multi-valued mapping in a neighborhood of x′.\nSince γ is a homeomorphic linear map, we find that γ(∂f̃(x)) = ∂f̃(x′) is a convex and compact set.\nNext we show that f̃ is upper semicontinuous at x′. Suppose that y′i → x′, g′i ∈ f̃c(y′i) for each i ∈ N, and g′ is an accumulation point of (g′i)i∈N. Then there is a i0 ∈ N such that y′i ∈ Uδ(x′) for all i ≥ i0. From\nUδ(x′) = Uδ(γ(x)) = γ (Uδ(x))\nfollows that there are vector representations yi ∈ Uδ(x) with γ(yi) = y′i for each i ≥ i0. From continuity of γ−1 follows that yi → x. By construction of ∂f̃ follows that\ng′i ∈ ∂f̃ (y′i) = ∂f̃ (γ (yi)) = γ ( ∂f̃ (yi) ) for each i ≥ i0. Hence, there are vector representations gi ∈ ∂f̃(yi) with γ(gi) = g′i for each i ≥ i0. Since f̃ is upper semicontinuous at x, we find that g ∈ ∂f̃(x). Again by construction of ∂f̃ follows that\ng′ = γ(g) ∈ γ ( ∂f̃(x) ) = ∂f̃ (γ(x)) = ∂f̃(x′).\nThis proves upper semicontinuity of ∂f̃ at all vector representations projecting to X = π(x).\nFinally, we prove that f̃ satisfies the subderivative property at x′. Suppose that y′,y ∈ X with y′ = γ(y). By Γ -invariance of f̃ , we have f̃(y′) = f̃(y). Since f̃ is generalized differentiable at x, we find a g ∈ ∂f̃(y) such that\nf̃(y′) = f̃(y) = f̃(x) + 〈g,y − x〉+ o(x,y, g)\nwith o(x,y, g) tending faster to zero than ‖y − x‖. Let g′ = γ(g). Exploiting Γ -invariance of f̃ as well as isometry and linearity of γ yields\nf̃(y′) = f̃(γ(x)) + 〈γ(g), γ(y − x)〉+ o(x,y, g) = f̃(x′) + 〈g′,y′ − x′〉+ o(x,y, g).\nWe define o′(x′,y′, g′) = o ◦ γ−1(x′,y′, g′) = o(x,y, g) showing that o′ tends faster to zero than normy′ − x. This proves the subderivative property of f̃ at all vector representations projecting to X = π(x).\nPutting all results together yields that f̃ is generalized differentiable at γ(x) for all γ ∈ Γ . ut\nB.2 Lloyd-Max Necessary Conditions for Optimality\nDue to the comparable nice analytical properties of Riemannian orbifolds, the proofs for the nearest neighbor and centroid condition of optimal graph quantizers are similar to their respective counterparts in vector quantization.\nTheorem 3 (Nearest Neighbor Condition). Suppose that C is a fixed codebook. Any graph quantizer Q : XΓ → C with\nQ(X) = argmin Y ∈C d (X,Y )\nfor all X ∈ XΓ , where ties are resolved according to some rule, has minimal expected distortion.\nProof. Suppose that Q′ : XΓ → C is a graph quantizer with arbitrary regions. Then we have\nd(X,Q′(X)) ≥ min Y ∈Y d(X,Y ) = d(X,Q(X))\nfor all X ∈ XΓ . This implies\nD(Q′) = EX [d (X,Q′(X))] ≥ EX [d (X,Q(X))] = D(Q).\nut\nTheorem 4 (Nearest Neighbor Condition). Suppose that PQ is a fixed partition and Q : XΓ → C a graph quantizer with codebook C satisfying\nYj = arg min Y ∈XΓ\nE [d (X,Y ) |X ∈ Rj ]\nfor all Y ∈ XΓ and all j ∈ J . Then Q has minimal expected distortion.\nProof. Let Pj = P (X ∈ Rj). Suppose that Q′ is a quantizer with partition {R1, . . . ,Rk} and arbitrary codebook C = {Y ′1 , . . . , Y ′k}. Then we have\nE [d(X,Q′(X))] = k∑ j=1 PjE [d(X,Q′(X)) |X ∈ Rj ]\n= k∑ j=1 PjE [ d(X,Y ′j ) |X ∈ Rj ] ≥\nk∑ j=1 Pj min Y ∈XΓ E [d(X,Y ) |X ∈ Rj ]\n= k∑ j=1 PjE [d(X,Yj) |X ∈ Rj ] = E [d(X,Q(X))]\nut"
    }, {
      "heading" : "1. H. Almohamad and S. Duffuaa, \"A linear programming approach for the weighted",
      "text" : "graph matching problem\", IEEE Transactions on Pattern Analysis and Machine Intelligence, 15(5)522–525, 1993. 2. H. Bunke and B.T. Messmer, \"Similarity measures for structured representations\", Lecture Notes in Computer Science, 837.106–118, 1994. 3. H. Bunke, P. Foggia, C. Guidobaldi, and M. Vento, \"Graph clustering using the weighted minimum common supergraph\" Graph Based Representations in Pattern Recognition, Lecture Notes in Computer Science, 2726:235–246, 2003 4. J.E. Borzellino, Riemannian geometry of orbifolds, PhD thesis, University of California, Los Angelos, 1992. 5. T.S. Caetano, L. Cheng, Q.V. Le, and A.J. Smola, \"Learning graph matching\" International Conference on Computer Vision, p. 1–8, 2007.\n6. T. Cour, P. Srinivasan, and J. Shi, \"Balanced graph matching\", NIPS 2006 Conference Proceedings, 2006. 7. R.O. Duda, P.E. Hart, and D.G. Stork Pattern Classification, Wiley & Sons, 2000. 8. Y.M. Ermoliev and V.I. Norkin, \"Normalized convergence in stochastic optimiza-\ntion\", Annals of Operations Research, 30:187–198, 1991, 9. Y.M. Ermoliev, V.I. Norkin, and R. Wets, \"The minimization of discontinuous func-\ntions: mollifier subgradients\", SIAM Journal on Control and Optimization, 33:149– 167, 1995. 10. Y.M. Ermoliev and V.I. Norkin, \"On nonsmooth and discontinuous problems of stochastic systems optimization\", European Journal of Operational Research, 101:230–244, 1997. 11. Y. M. Ermoliev and V.I. Norkin, \"Stochastic generalized gradient method for nonconvex nonsmooth stochastic optimization\", Cybernetics and Systems Analysis, 34(2), 196–215, 1998. 12. M. Ferrer, Theory and algorithms on the median graph. application to graph-based classification and clustering, PhD Thesis, Univ. Aut‘onoma de Barcelona, 2007. 13. M. Ferrer, E. Valveny, F. Serratosa, I. Bardají, and H. Bunke, \"Graph-Based kMeans Clustering: A Comparison of the Set Median versus the Generalized Median Graph\" CAIP 2009 Conference Proceedings, 2009. 14. A. Gersho and R.M. Gray, Vector Quantization and Signal Compression, Kluwer Academic Publishers, 1992. 15. S. Gold and A. Rangarajan, \"Graduated Assignment Algorithm for Graph Matching\", IEEE Trans. Pattern Analysis and Machine Intelligence, 18:377–388, 1996. 16. S. Gold, A. Rangarajan, and E. Mjolsness, \"Learning with preknowledge: clustering with point and graph matching distance measures\" Neural Computation, 8(4):787– 804, 1996. 17. S. Günter and H. Bunke, \"Self-organizing map for clustering in the graph domain\", Pattern Recognition Letters, 23(4):405–417, 2002. 18. M. Hagenbuchner, A. Sperduti, and A.C. Tsoi, ÒA Self-Organizing Map for Adaptive Processing of Structured Data,Ó IEEE Transaction on Neural Networks, 14:491–505, 2003. 19. B. Jain and F. Wysotzki, \"Central Clustering of Attributed Graphs\", Machine Learning, 56, 169–207, 2004. 20. B. Jain and K. Obermayer, \"On the sample mean of graphs\", IJCNN 2008 Conference Proceedings, p. 993–1000, 2008. 21. B. Jain and K. Obermayer, \"Structure Spaces\", Journal of Machine Learning Research, 10:2667–2714, 2009. 22. B. Jain and K. Obermayer, \"Accelerating Competitive Learning Graph Quantization\", Computer Vision and Image Understanding, 2009 (submitted). 23. B. Jain and K. Obermayer, \"Elkan’s k-Means for Graphs\", arXiv:0912.4598v1 [cs.AI], 2009. 24. Y. Linde, A. Buzo, and R. M. Gray, ÒAn algorithm for vector quantizer design,Ó IEEE Transactions on Communications, 28:84–95, 1980. 25. S.P. Lloyd, ÒLeast squares quantization in PCMÓ, IEEE Transactions on Information Theory, 28:129–137, 1982, reprint of 1957. 26. M.A. Lozano and F. Escolano, \"ACM attributed graph clustering for learning classes of images\", Graph Based Representations in Pattern Recognition, Lecture Notes in Computer Science, 2726:247–258, 2003 27. V.I. Norkin, \"Stochastic generalized-differentiable functions in the problem of nonconvex nonsmooth stochastic optimization\", Cybernetics, 22(6), 804–809, 1986."
    }, {
      "heading" : "28. A. Schenker, M. Last, H. Bunke, and A. Kandel, \"Clustering of web documents",
      "text" : "using a graph model\", Web Document Analysis: Challenges and Opportunities, p. 1–16, 2003. 29. A. Schenker, M. Last, H. Bunke, and A. Kandel, Graph-Theoretic Techniques for Web Content Mining, World Scientific Publishing, 2005. 30. S. Theodoridis and K. Koutroumbas, Pattern Recognition, Elsevier, 2009. 31. A. Torsello and E.R. Hancock, \"Learning shape-classes using a mixture of\ntree-unions\", IEEE Transactions on Pattern Analysis and Machine Intelligence, 28(6):954-967, 2006. 32. S. Umeyama, \"An eigendecomposition approach to weighted graph matching problems\", IEEE Transactions on Pattern Analysis and Machine Intelligence, 10(5):695– 703, 1988. 33. M. Van Wyk, M. Durrani, and B. Van Wyk, \"A RKHS interpolator-based graph matching algorithm\", IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(7):988–995, 2002."
    } ],
    "references" : [ {
      "title" : "Duffuaa, \"A linear programming approach for the weighted graph matching problem",
      "author" : [ "S.H. Almohamad" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1993
    }, {
      "title" : "Messmer, \"Similarity measures for structured representations",
      "author" : [ "B.T.H. Bunke" ],
      "venue" : "Lecture Notes in Computer Science,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1994
    }, {
      "title" : "Graph clustering using the weighted minimum common supergraph\" Graph Based Representations in Pattern Recognition, Lecture",
      "author" : [ "H. Bunke", "P. Foggia", "C. Guidobaldi", "M. Vento" ],
      "venue" : "Notes in Computer Science,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Riemannian geometry of orbifolds",
      "author" : [ "J.E. Borzellino" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1992
    }, {
      "title" : "Learning graph matching",
      "author" : [ "T.S. Caetano", "L. Cheng", "Q.V. Le", "A.J. Smola" ],
      "venue" : "International Conference on Computer Vision, p",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2007
    }, {
      "title" : "Balanced graph matching",
      "author" : [ "T. Cour", "P. Srinivasan", "J. Shi" ],
      "venue" : "NIPS 2006 Conference Proceedings,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Norkin, \"Normalized convergence in stochastic optimization",
      "author" : [ "V.I.Y.M. Ermoliev" ],
      "venue" : "Annals of Operations Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1991
    }, {
      "title" : "The minimization of discontinuous functions: mollifier subgradients",
      "author" : [ "Y.M. Ermoliev", "V.I. Norkin", "R. Wets" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1995
    }, {
      "title" : "Norkin, \"On nonsmooth and discontinuous problems of stochastic systems optimization",
      "author" : [ "V.I.Y.M. Ermoliev" ],
      "venue" : "European Journal of Operational Research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1997
    }, {
      "title" : "Norkin, \"Stochastic generalized gradient method for nonconvex nonsmooth stochastic optimization",
      "author" : [ "V.I.Y.M. Ermoliev" ],
      "venue" : "Cybernetics and Systems Analysis,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1998
    }, {
      "title" : "Theory and algorithms on the median graph. application to graph-based classification and clustering",
      "author" : [ "M. Ferrer" ],
      "venue" : "PhD Thesis, Univ. Aut‘onoma de Barcelona,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2007
    }, {
      "title" : "Graph-Based kMeans Clustering: A Comparison of the Set Median versus the Generalized Median Graph",
      "author" : [ "M. Ferrer", "E. Valveny", "F. Serratosa", "I. Bardají", "H. Bunke" ],
      "venue" : "CAIP 2009 Conference Proceedings,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Vector Quantization and Signal Compression",
      "author" : [ "A. Gersho", "R.M. Gray" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1992
    }, {
      "title" : "Graduated Assignment Algorithm for Graph Matching",
      "author" : [ "S. Gold", "A. Rangarajan" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1996
    }, {
      "title" : "Learning with preknowledge: clustering with point and graph matching distance measures",
      "author" : [ "S. Gold", "A. Rangarajan", "E. Mjolsness" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1996
    }, {
      "title" : "Self-organizing map for clustering in the graph domain",
      "author" : [ "S. Günter", "H. Bunke" ],
      "venue" : "Pattern Recognition Letters,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2002
    }, {
      "title" : "ÒA Self-Organizing Map for Adaptive Processing of Structured Data,Ó",
      "author" : [ "M. Hagenbuchner", "A. Sperduti", "A.C. Tsoi" ],
      "venue" : "IEEE Transaction on Neural Networks,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2003
    }, {
      "title" : "Central Clustering of Attributed Graphs",
      "author" : [ "B. Jain", "F. Wysotzki" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2004
    }, {
      "title" : "On the sample mean of graphs",
      "author" : [ "B. Jain", "K. Obermayer" ],
      "venue" : "IJCNN 2008 Conference Proceedings,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    }, {
      "title" : "Structure Spaces",
      "author" : [ "B. Jain", "K. Obermayer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2009
    }, {
      "title" : "Accelerating Competitive Learning Graph Quantization",
      "author" : [ "B. Jain", "K. Obermayer" ],
      "venue" : "Computer Vision and Image Understanding,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2009
    }, {
      "title" : "Elkan’s k-Means for Graphs",
      "author" : [ "B. Jain", "K. Obermayer" ],
      "venue" : "[cs.AI],",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2009
    }, {
      "title" : "ÒLeast squares quantization in PCMÓ",
      "author" : [ "S.P. Lloyd" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1982
    }, {
      "title" : "ACM attributed graph clustering for learning classes of images",
      "author" : [ "M.A. Lozano", "F. Escolano" ],
      "venue" : "Graph Based Representations in Pattern Recognition, Lecture Notes in Computer Science,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2003
    }, {
      "title" : "Stochastic generalized-differentiable functions in the problem of nonconvex nonsmooth stochastic optimization",
      "author" : [ "V.I. Norkin" ],
      "venue" : "Cybernetics, 22(6),",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1986
    }, {
      "title" : "Clustering of web documents using a graph model\", Web Document Analysis: Challenges and Opportunities",
      "author" : [ "A. Schenker", "M. Last", "H. Bunke", "A. Kandel" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2003
    }, {
      "title" : "Graph-Theoretic Techniques for Web Content Mining, World",
      "author" : [ "A. Schenker", "M. Last", "H. Bunke", "A. Kandel" ],
      "venue" : "Scientific Publishing,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2005
    }, {
      "title" : "Learning shape-classes using a mixture of tree-unions",
      "author" : [ "A. Torsello", "E.R. Hancock" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2006
    }, {
      "title" : "An eigendecomposition approach to weighted graph matching problems",
      "author" : [ "S. Umeyama" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 1988
    }, {
      "title" : "A RKHS interpolator-based graph matching algorithm",
      "author" : [ "M. Van Wyk", "M. Durrani", "B. Van Wyk" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Vector quantization is a classical technique from signal processing suitable for lossy data compression, density estimation, and prototype-based clustering [7, 14, 30].",
      "startOffset" : 156,
      "endOffset" : 167
    }, {
      "referenceID" : 22,
      "context" : "The kmeans algorithm is also commonly referred to as the Linde-Buzo-Gray (LBG) algorithm [24] the generalized Lloyd algorithm [25].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 14,
      "context" : "Examples include competitive learning algorithms in the domain of graphs [16–20, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 15,
      "context" : "Examples include competitive learning algorithms in the domain of graphs [16–20, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "Examples include competitive learning algorithms in the domain of graphs [16–20, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 17,
      "context" : "Examples include competitive learning algorithms in the domain of graphs [16–20, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 18,
      "context" : "Examples include competitive learning algorithms in the domain of graphs [16–20, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 20,
      "context" : "Examples include competitive learning algorithms in the domain of graphs [16–20, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].",
      "startOffset" : 73,
      "endOffset" : 84
    }, {
      "referenceID" : 10,
      "context" : "Examples include competitive learning algorithms in the domain of graphs [16–20, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].",
      "startOffset" : 129,
      "endOffset" : 157
    }, {
      "referenceID" : 11,
      "context" : "Examples include competitive learning algorithms in the domain of graphs [16–20, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].",
      "startOffset" : 129,
      "endOffset" : 157
    }, {
      "referenceID" : 17,
      "context" : "Examples include competitive learning algorithms in the domain of graphs [16–20, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].",
      "startOffset" : 129,
      "endOffset" : 157
    }, {
      "referenceID" : 18,
      "context" : "Examples include competitive learning algorithms in the domain of graphs [16–20, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].",
      "startOffset" : 129,
      "endOffset" : 157
    }, {
      "referenceID" : 21,
      "context" : "Examples include competitive learning algorithms in the domain of graphs [16–20, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].",
      "startOffset" : 129,
      "endOffset" : 157
    }, {
      "referenceID" : 25,
      "context" : "Examples include competitive learning algorithms in the domain of graphs [16–20, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].",
      "startOffset" : 129,
      "endOffset" : 157
    }, {
      "referenceID" : 26,
      "context" : "Examples include competitive learning algorithms in the domain of graphs [16–20, 22] and k-means as well as k-medoids algorithms [12, 13, 19, 20, 23, 28, 29].",
      "startOffset" : 129,
      "endOffset" : 157
    }, {
      "referenceID" : 2,
      "context" : "Related clustering method are presented in [3, 26, 31].",
      "startOffset" : 43,
      "endOffset" : 54
    }, {
      "referenceID" : 23,
      "context" : "Related clustering method are presented in [3, 26, 31].",
      "startOffset" : 43,
      "endOffset" : 54
    }, {
      "referenceID" : 27,
      "context" : "Related clustering method are presented in [3, 26, 31].",
      "startOffset" : 43,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "For a more detailed definition of the graph edit distance, we refer to [2].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].",
      "startOffset" : 200,
      "endOffset" : 221
    }, {
      "referenceID" : 4,
      "context" : "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].",
      "startOffset" : 200,
      "endOffset" : 221
    }, {
      "referenceID" : 5,
      "context" : "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].",
      "startOffset" : 200,
      "endOffset" : 221
    }, {
      "referenceID" : 13,
      "context" : "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].",
      "startOffset" : 200,
      "endOffset" : 221
    }, {
      "referenceID" : 28,
      "context" : "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].",
      "startOffset" : 200,
      "endOffset" : 221
    }, {
      "referenceID" : 29,
      "context" : "In addition, we present an important graph metric based on a generalization of the concept of maximum common subgraph, which arises in various different guises as a common choice of proximity measure [1, 5, 6, 15, 32, 33].",
      "startOffset" : 200,
      "endOffset" : 221
    }, {
      "referenceID" : 21,
      "context" : "As shown in [23], d is indeed a metric and can be expressed as a graph edit distance.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 3,
      "context" : "For all other proofs we refer to [4, 21].",
      "startOffset" : 33,
      "endOffset" : 40
    }, {
      "referenceID" : 19,
      "context" : "For all other proofs we refer to [4, 21].",
      "startOffset" : 33,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "For proofs we refer to [4].",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 6,
      "context" : "The proof follows from [8] applied to the lift d̃ of distortion d.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "under mild assumptions (see [11, 27]), we can minimize the expected distortion D(C) according to the following stochastic generalized gradient (SGG) method: yt+1 = yt + ηt (xt − yt), (4)",
      "startOffset" : 28,
      "endOffset" : 36
    }, {
      "referenceID" : 24,
      "context" : "under mild assumptions (see [11, 27]), we can minimize the expected distortion D(C) according to the following stochastic generalized gradient (SGG) method: yt+1 = yt + ηt (xt − yt), (4)",
      "startOffset" : 28,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "The proof is a direct consequence of Ermoliev and Norkin’s Theorem [11] applied on the lift d̃ (·|·) of d (·|·).",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "As shown in [10], minimizing the empirical distortion is often meaningless, if the underlying graph edit distance function d (·|·) and thus D̂N (C) is discontinuous, even if the expectation D(C) may be continuously differentiable.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 7,
      "context" : "For details, we refer to [9].",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 24,
      "context" : "A function f : X → R is generalized differentiable at x ∈ X in the sense of Norkin [27] if there is a multi-valued map ∂f : X → 2X in a neighborhood of x such that",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 24,
      "context" : "Generalized differentiable functions have the following properties [27]:",
      "startOffset" : 67,
      "endOffset" : 71
    } ],
    "year" : 2010,
    "abstractText" : "Vector quantization(VQ) is a lossy data compression technique from signal processing, which is restricted to feature vectors and therefore inapplicable for combinatorial structures. This contribution presents a theoretical foundation of graph quantization (GQ) that extends VQ to the domain of attributed graphs. We present the necessary Lloyd-Max conditions for optimality of a graph quantizer and consistency results for optimal GQ design based on empirical distortion measures and stochastic optimization. These results statistically justify existing clustering algorithms in the domain of graphs. The proposed approach provides a template of how to link structural pattern recognition methods other than GQ to statistical pattern recognition.",
    "creator" : "TeX"
  }
}