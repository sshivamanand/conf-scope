{
  "name" : "1106.3457.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n10 6.\n34 57\nv1 ["
    }, {
      "heading" : "1 Introduction",
      "text" : "The two most prominent declarative paradigms, namely logic and functional programming, differ radically in an important aspect: logic programming is traditionally first-order while functional programming encourages and promotes the use of higher-order functions and constructs. One problem is that even second-order logic fails in terms of vital properties such as completeness and compactness. It would seem, on the face of it, that there would be no hope of finding a complete resolution proof procedure for higher-order logic programming.\nThe initial attitude of logic programmers towards higher-order logic programming was somewhat skeptical: it was often argued (see for example [War82]) that there exist ways of encoding or simulating higher-order programming inside Prolog itself. However ease of use is a primary criterion for a programming language, and the fact that higher-order features can be simulated or encoded does not mean that it is practical to do so.\n∗An early version of this paper appears in: A. Charalambidis, K. Handjopoulos, P. Rondogiannis, W. W. Wadge. Extensional Higher-Order Logic Programming. Proceedings of the 12th European Conference on Logics in Artificial Intelligence (JELIA). LNCS 6341, Springer, pages 91-103, 2010.\n†The research of A. Charalambidis has been co-financed by the European Union (European Social Fund - ESF) and Greek national funds through the Operational Program “Education and Lifelong Learning” of the National Strategic Reference Framework (NSRF) - Research Funding Program: Heracleitus II. Investing in knowledge society through the European Social Fund.\nEventually extensions with genuine higher-order capabilities were introduced - roughly speaking, extensions which allow predicates to be applied but also passed as parameters. The two most prominent such languages are λProlog [MN86, Nad87] and HiLog [CKW89, CKW93]. These two systems share a common idea, namely they are both intensional. Intuitively speaking, an intensional language places almost no restraints on the way in which a predicate can be passed and used. In an intensional language it is possible that two co-extensional predicates are not considered equal. In other words, a predicate in such a language is more than just the set of arguments for which it is true.\nHowever, for many applications intensionality appears to be appropriate. Suppose, for example, we have a database of professions, both of their membership and their status. We might have rules such as:\nengineer(tom). engineer(sally). programmer(harry).\nwith engineer and programmer used as predicates. But in intensional higher-order logic programming we could also have rules in which these are arguments, eg:\nprofession(engineer). profession(programmer).\nNow suppose tom and sally are also avid users of Twitter. We could have rules:\ntweeter(tom). tweeter(sally).\nIn the absence of other rules, it is clear that the tweeters are exactly the engineers; but the query:\n?-profession(tweeter).\nfails. This failure contradicts the extensionality principle, which holds that predicates that succeed for exactly the same instances are equal. However, in this context the failure of extensionality does not seem unnatural.\nNevertheless, the failure of extensionality cannot in general be taken lightly. It means that we cannot use our mathematicians’ intuitions of relations, intuitions based on hundreds of years of mathematical development. It raises doubts that rules like those just given can have a simple declarative meaning. Moreover, there are many applications that call for higherorder logic (predicates used as arguments) but do not involve intensional notions. As a simple example, consider the predicate allmembers(L,P) which asserts that all elements of the list L have property P. Predicate allmembers raises no foundational issues and the corresponding rules seem obvious:\nallmembers([],P). allmembers([H|T],P):-P(H),allmembers(T,P).\nHowever this is not legitimate Prolog, and to write these rules we currently have no choice but to use an intensional higher-order language, even though the logic behind allmembers is entirely extensional. For example, if the query:\n?-allmembers([a,b,c],p).\nsucceeds and q is co-extensional with p, we can be sure that the query\n?-allmembers([a,b,c],q).\nwill also succeed.\nAre there more modest higher-order extensions of logic programming that do not entail intensionality? After all, higher-order extensions of functional programming are almost all extensional. This question was first raised by W. W. Wadge in [Wad91] and answered in the affirmative. Wadge discovered a simple syntactic restriction which, though it limited the applicability of the language, ensured that compliant programs have an extensional declarative reading. The restriction forbids a user-defined predicate to appear as an argument in the head of a clause. For example, of the rules cited already,\nprofession(engineer).\nviolates the restriction, but allmembers([],P).\ncomplies with it. Roughly speaking, the restriction says that rules about predicates can state general principles but cannot pick out a particular predicate for special treatment. Wadge gave several examples of useful extensional higher-order programs and outlined the proof of a minimum-model result. He also showed that in this model the denotations of program predicates are monotonic and continuous. Continuity, in this context, is a kind of finitaryness. For example, if foo(p) succeeds and foo is continuous, it means there is a finite set of arguments {a1, . . . , ak} for which p(a1), . . . , p(ak) all succeed; moreover, if q(a1), . . . , q(ak) also succeed, then foo(q) succeeds.\nContributions: In this paper we extend the study initiated in [Wad91] and derive the first, to our knowledge, complete theoretical framework for extensional higher-order logic programming, both from a semantic as-well-as from a proof theoretic point of view.\nOur first contribution is the development of a novel extensional semantics for higher-order logic programming that is based on algebraic lattices (see for example [Gra78]), a subclass of the familiar complete lattices that have traditionally been used in the theory of first-order logic programming. For every predicate type of our language, algebraic lattices single out a subset of “finite” objects of that type. In other words, the proposed semantics reflects in a direct way the finitary nature of continuity that is implicit in [Wad91]. The benefit of the new approach compared to that of [Wad91] is that all basic properties and results of classical logic programming are now transferred in the higher-order setting in a natural way. Moreover, the new semantics leads to a relatively simple sound and complete proof procedure (see below) even for a language that is genuinely more powerful than the one considered in [Wad91]. More details on the connections between the two approaches will be given in 5.3.\nOur second contribution fixes a major shortcoming of Wadge’s language by allowing clause bodies and program goals to have uninstantiated higher-order variables. To understand the importance of this extension, consider the following axiom for bands (musical ensembles):\nband(B):-singer(S),B(S),drummer(D),B(D),guitarist(G),B(G).\nThis says that a band is a group that has at least a singer, a drummer, and a guitarist. Suppose that we also have a database of musicians:\nsinger(sally). singer(steve). drummer(dave). guitarist(george). guitarist(grace).\nOur extensional higher-order language allows the query ?-band(B). At first sight a query like this is impractical if not impossible to implement. Since a band is a set, bands can be very large and there can be many, possibly uncountably infinitely many of them. In existing\nintensional systems such queries fail since the program does not provide any information about any particular band.\nHowever, in an extensional context the finitary behavior of the predicates of our language, saves us. If the predicate band declares a relation to be a band, then (due to the finitaryness described above) it must have examined only finitely many members of the relation. Therefore we can enumerate the bands by enumerating finite bands, and this collection is countable (in this particular example it is actually finite). Actually, as we are going to see, this enumeration can be performed in a careful way so as that it avoids producing all finite relations one by one (see the discussion in Section 2 that follows).\nOur final contribution, and not the least, is a relatively simple proof procedure for extensional higher-order logic programming, which extends classical SLD-resolution. We demonstrate that the new proof procedure is sound and complete with respect to the proposed semantics. In particular, the derived completeness theorems generalize the well-known such theorems for first-order logic programming. This result may, at first sight, also seem paradoxical, given the well-known failure of completeness for even second-order logic. But the paradox is resolved by recalling that we are dealing with a restricted subset of higher-order logic and that the denotations of the types of our language are not arbitrary sets but instead algebraic lattices (which have a much more refined structure).\nOne very important benefit of the proof procedure is that it gives us an operational semantics for our language. This means in turn that we could probably extend it with cut, negation and other operational features not easily specified in terms of model theory alone.\nThe rest of the paper is organized as follows: Section 2 presents in a more detailed manner the basic ideas developed in this paper. Section 3 introduces the syntax of the higher-order logic programming language H. Section 4 introduces the key lattice-theoretic notions that will be needed in the development of the semantics. Sections 5 and 6 develop the semantics and the minimum Herbrand model semantics of H; the main properties of the semantics are also established. Section 7 introduces an SLD-resolution proof procedure for H and establishes its soundness and completeness. Section 8 presents a brief description of related approaches to higher-order logic programming. Section 9 briefly discusses implementation issues and presents certain interesting topics for future work. The lengthiest among the proofs have been moved to corresponding appendices in order to enhance the readability of the paper."
    }, {
      "heading" : "2 The Proposed Approach: an Intuitive Overview",
      "text" : "As discussed in the previous section, the purpose of this paper is to develop a purely extensional theoretical framework for higher-order logic programming which will generalize the familiar theory of first-order logic programming. The first problem we consider is to bypass one important restriction of [Wad91], namely the inability to handle programs in which clauses contain uninstantiated predicate variables. The following example illustrates these ideas:\nExample 2.1 Consider the following higher-order program written in an extended Prolog-like syntax:\np(Q):-Q(0),Q(s(0)). nat(0). nat(s(X)):-nat(X).\nThe Herbrand universe of the program is the set of natural numbers in successor notation. According to the semantics of [Wad91], the least Herbrand model of the program assigns to predicate p a continuous relation which is true of all unary relations that contain at least 0 and s(0). Consider now the query:\n?-p(R).\nwhich asks for all relations that satisfy p. Such a query seems completely unreasonable, since there exist uncountably many relations that must be substituted and tested in the place of R.\nThe above example illustrates why uninstantiated predicate variables in clauses were disallowed in [Wad91]. From a theoretical point of view, one could extend the semantics to cover such cases, but the problem is mainly a practical one: “how can one implement such programs and queries?”.\nIn more formal terms, the least Herbrand model of a higher-order program under the semantics of [Wad91] is in general an uncountable set; in our example, this is evidenced by the fact that there exists an uncountable number of unary relations over the natural numbers that contain both 0 and s(0). This observation comes in contrast with the semantics of first-order logic programming in which the least Herbrand model of a program is a countable set. How can one define a proof procedure that is sound and complete with respect to this semantics? The key idea for bypassing these problems was actually anticipated in the concluding section of [Wad91]:\nOur higher order predicates, however, are continuous: if a relation satisfies a predicate, then some finite subset satisfies it. This means that we have to examine only finite relations.\nIn the above example, despite the fact that there exists an infinite number of relations that satisfy p, all these relations are supersets of the finite relation {0, s(0)}. In some sense, this finite relation represents all the relations that satisfy p. But how can we make the notion of “finiteness” more explicit? In order to define a sound and complete proof procedure for an interesting extensional higher-order logic programming language, our semantics must in some sense reflect the above “finitary” concepts more explicitly.\nAn idea that springs to mind is to define an alternative semantics in which variables (like Q in Example 2.1) range over finite relations (and not over arbitrary relations as in [Wad91]). Of course, the notion of “finite” should be appropriately defined for every predicate type. But then an immediate difficulty appears to arise. Given again the program in Example 2.1, and the query\n?-p(nat).\nit is not immediately obvious what the meaning of the above is. Since we have assumed that Q ranges over finite relations, how can p be applied to an infinite one? To overcome this problem, observe that in order for the predicate p to succeed for its argument Q, it only has to examine a “finite number of facts” about Q (namely whether Q is true of 0 and s(0)). This remark suggests that the meaning of p(nat) can be established following a non-standard interpretation of application: we apply the meaning of p to all the “finite approximations” of the meaning of nat, ie., to all finite subsets of the set {0, s(0), s(s(0)), . . .}. In our case p(nat) will be true since there exists a finite subset of the meaning of nat for which the meaning of p is true (namely the set {0, s(0)}).\nNotice that the new semantical approach outlined above, heavily relies on the idea that the meaning of predicates (like nat) can be expressed as the least upper bound of a set of simpler (in this case, finite) relations. Actually, this is an old and well-known assumption in the area of denotational semantics, as the following excerpt from [Sto77][page 98] indicates:\nSo we may reasonably demand of all the value spaces in which we hope to compute that they come equipped with a particular countable subset of elements from which all the other elements may be built up.\nAs we are going to demonstrate, the meaning of every predicate defined in our language possesses the property just mentioned, and this allows us to use the new non-standard semantics\nof application. In fact, as we are going to see, for every predicate type of our language, the set of possible meanings of this type forms an algebraic lattice [Gra78]; then, the above property is nothing more than the key property which characterizes algebraic lattices (see Proposition 4.14), namely that “every element of an algebraic lattice is the least upper bound of the compact elements of the lattice that are below it”. More importantly, for the algebraic lattices we consider, it is relatively easy to identify these compact elements and to enumerate them one by one. Based on the above semantics, we are able to derive for higher-order logic programs many properties that are either identical or generalize the familiar ones from first-order logic programming (see Section 6).\nThe new semantics allows us to introduce a relatively simple, sound and complete proof procedure which applies to programs and queries that may contain uninstantiated predicate variables. This is due to the fact that the set of “finite” relations is now countable, and as we are going to see, there exist interesting ways of producing and enumerating them. The key idea can be demonstrated by continuing Example 2.1. Given the query:\n?-p(R).\none (inefficient and tedious) approach would be to enumerate all possible finite relations of the appropriate type over the Herbrand universe. Instead of this, we use an approach which is based on what we call basic templates: a basic template for R is (intuitively) a finite set whose elements are individual variables. This saves us from having to enumerate all finite sets consisting of ground terms from the Herbrand universe. For example1, assume that we instantiate R with the template {X, Y}. Then, the resolution proceeds as follows:\n?-p(R) ?-p({X, Y}) ?-{X, Y}(0), {X, Y}(s(0)) ?-{0, Y}(s(0)) ✷\nand the proof procedure will return the answer R = {0, s(0)}. The proof procedure will also return other finite solutions, such as R = {0, s(0), Z1}, R = {0, s(0), Z1, Z2}, and so on. However, a slightly optimized implementation (see Section 9) can be created that returns only the answer R = {0, s(0)}, which represents all the finite relations produced by the proof procedure. The intuition behind the above answer is that the given query succeeds for all unary relations that contain at least 0 and s(0). Similarly, for the band example of Section 1, the implementation will systematically assemble all the minimal three-member bands from the talents available."
    }, {
      "heading" : "3 The Higher-Order Language H: Syntax",
      "text" : "In this section we introduce the higher-order logic programming language H, which extends classical first-order logic programming to a higher-order setting. The language H is based on a simple type system that supports two base types: o, the boolean domain, and ι, the domain of individuals (data objects). The composite types are partitioned into three classes: functional (assigned to function symbols), predicate (assigned to predicate symbols) and argument (assigned to parameters of predicates).\nDefinition 3.1 A type can either be functional, argument, or predicate:\nσ := ι | (ι → σ) ρ := ι | π π := o | (ρ → π)\n1The notation we use for representing basic templates will slightly change in Section 7.\nWe will use τ to denote an arbitrary type (either functional, argument or predicate one).\nAs usual, the binary operator → is right-associative. A functional type that is different than ι will often be written in the form ιn → ι, n ≥ 1. Moreover, it can be easily seen that every predicate type π can be written in the form ρ1 → · · · → ρn → o, n ≥ 0 (for n = 0 we assume that π = o).\nWe can now proceed to the definition of H, starting from its alphabet:\nDefinition 3.2 The alphabet of the higher-order language H consists of the following:\n1. Predicate variables of every predicate type π (denoted by capital letters such as P,Q,R, . . .).\n2. Predicate constants of every predicate type π (denoted by lowercase letters such as p, q, r, . . .).\n3. Individual variables of type ι (denoted by capital letters such as X,Y,Z, . . .).\n4. Individual constants of type ι (denoted by lowercase letters such as a, b, c, . . .).\n5. Function symbols of every functional type σ 6= ι (denoted by lowercase letters such as f, g, h, . . .).\n6. The following logical constant symbols: the propositional constants 0 and 1 of type o; the equality constant ≈ of type ι → ι → o; the generalized disjunction and conjunction constants ∨ π and ∧ π of type π → π → o, for every predicate type π; the generalized\ninverse implication constants ←π, of type π → π → o, for every predicate type π; the existential quantifier ∃ρ, of type (ρ → o) → o, for every argument type ρ.\n7. The abstractor λ and the parentheses “(” and “)”.\nThe set consisting of the predicate variables and the individual variables of H, will be called the set of argument variables of H. Argument variables will be usually denoted by V and its subscripted versions.\nThe existential quantifier in higher-order logic is usually introduced in a different way than in first-order logic. So, for example, in order to express the quantification of the argument variable V of type ρ over the expression E one writes (∃ρ (λV.E)). For simplicity, we will use in this paper the more familiar notation (∃ρVE).\nWe proceed by defining the set of positive expressions of H:\nDefinition 3.3 The set of positive expressions of the higher-order language H is recursively defined as follows:\n1. Every predicate variable (respectively, predicate constant) of type π is a positive expression of type π; every individual variable (respectively, individual constant) of type ι is a positive expression of type ι; the propositional constants 0 and 1 are positive expressions of type o.\n2. If f is an n-ary function symbol and E1, . . . ,En are positive expressions of type ι, then (f E1 · · ·En) is a positive expression of type ι.\n3. If E1 is a positive expression of type ρ → π and E2 is a positive expression of type ρ, then (E1E2) is a positive expression of type π.\n4. If V is an argument variable of type ρ and E is a positive expression of type π, then (λV.E) is a positive expression of type ρ → π.\n5. If E1,E2 are positive expressions of type π, then (E1 ∧ π E2) and (E1 ∨\nπ E2) are positive expressions of type π.\n6. If E1,E2 are positive expressions of type ι, then (E1 ≈ E2) is a positive expression of type o.\n7. If E is an expression of type o and V is an argument variable of any type ρ, then (∃ρVE) is a positive expression of type o.\nThe notions of free and bound variables of an expression are defined as usual. An expression is called closed if it does not contain any free variables.\nThe set of clausal expressions of H can now be specified:\nDefinition 3.4 The set of clausal expressions of the higher-order language H is defined as follows:\n1. If p is a predicate constant of type π and E is a closed positive expression of type π then p ←π E is a clausal expression of type o of H, also called a program clause.\n2. If E is a positive expression of type o, then 0 ←o E (usually denoted by ←o E or just ← E) is a clausal expression of type o of H, also called a goal clause.\nNotice that (following the tradition of first-order logic programming) we will often talk about the “empty clause” which is denoted by ✷ and is equivalent to the propositional constant 1.\nThe union of the sets of positive and clausal expressions of H will be called the set of expressions of H. To denote that an expression E has type τ , we will often write E : τ ; additionally, we write type(E) to denote the type of expression E. Expressions of type ι will be called terms and of type o will be called formulas. We will write ←, ∧ and ∨ instead of ←o, ∧ o\nand ∨\no. Moreover, instead of ∃ρ we will often write ∃. When writing an expression, in order to avoid the excessive use of parentheses, certain usual conventions will be adopted (such as for example the usual priorities between logical constants, the convention that application is left-associative and that lambda abstraction extends as far to the right as possible, and so on). Given an expression E, we denote by FV (E) the set of all free variables of E. By overloading notation, we will also write FV (S), where S is a set of expressions.\nNotice that in Definition 3.4 above, a goal clause may contain two types of occurrences of variables that serve a similar purpose, namely free argument variables and argument variables that are existentially quantified. From a semantic point of view, these two types of variables are essentially the same. However, in a later section we will distinguish them from an operational point of view: the free argument variables that appear in a goal are the ones for which an answer is sought for by the proof procedure; the argument variables that are existentially quantified are essentially free variables for which an answer is not sought for (something like the underscored variables in Prolog systems). This distinction is not an important one, and we could have proceeded in a different way (eg. by disallowing existentially quantified variables from goals).\nDefinition 3.5 A program of H is a set of program clauses of H.\nExample 3.6 The following is a higher-order program that computes the closure of its input binary relation R. The type of closure is π = (ι → ι → o) → ι → ι → o.\nclosure←π λR.λX.λY.(R X Y) closure←π λR.λX.λY.∃Z((R X Z)∧(closure R Z Y))\nor even more compactly:\nclosure←π (λR.λX.λY.(R X Y)) ∨ π (λR.λX.λY.∃Z((R X Z)∧(closure R Z Y)))\nA possible query could be: ← (closure R a b) (which intuitively requests for those binary relations such that the pair (a, b) belongs to their transitive closure). In a Prolog-like extended syntax, the above program would have been written as:\nclosure(R,X, Y) :- R(X, Y). closure(R,X, Y) :- R(X, Z), closure(R,Z, Y).\nand the corresponding query as ← closure(R,a, b).\nExample 3.7 We define a predicate ordered which checks whether its second argument (a list) is ordered according to its first argument (a binary relation). The type of ordered is π = (ι → ι → o) → ι → o (notice that the type of a list is also ι since a list is nothing more than a term). In Prolog-like syntax, the program is the following:\nordered(R,[ ]). ordered(R,[X]). ordered(R,[X,Y|T]):- R(X,Y), ordered(R,[Y|T]).\nIn the syntax of H (slightly extended with the standard notation for lists), the above program can be written as follows:\nordered ←π λR.λL.(L≈[]) ordered ←π λR.λL.(∃X(L≈[X])) ordered ←π λR.λL.(∃X∃Y∃T((L≈[X,Y|T])∧(R X Y)∧(ordered R [Y|T])))\nAssume that we have also defined a binary relation less which succeeds if its first argument (eg. a natural number) is less than the second one. Then, the query ← ordered less [1,4,7,10] is expected to succeed. On the other hand, the query ← ordered R [a,b,c,d] requests for all binary relations under which the list [a,b,c,d] is ordered. As it will become clear in the subsequent sections of the paper, this is a meaningful question which can obtain a reasonable answer."
    }, {
      "heading" : "4 Algebraic Lattices",
      "text" : "In order to develop the semantics of H, we first need to introduce certain lattice-theoretic concepts. As it is well-known, the standard semantics of classical (first-order) logic programming, is based on complete lattices (see for example [Llo87]). As we are going to see, the development of the semantics of H is based on a special class of complete lattices, namely algebraic lattices (see for example [Gra78]). An algebraic lattice is a complete lattice in which every element can be created by using certain compact (intuitively, “simple”) elements of the lattice. In our setting, these compact elements will be the ones that the proof procedure will generate in order to answer queries that involve uninstantiated predicate variables. We should mention at this point that algebraic partially ordered sets are widely used in domain theory (see for example [AJ94]).\nWe start by introducing some mathematical preliminaries concerning lattice theory. Since the bibliography on partially ordered sets is huge, certain results appear in one form or another in various contexts, and they are often hard to locate in the exact form needed. Propositions 4.7, 4.10 and 4.14 fall into this category; for reasons of completeness, we have included short proofs for them. On the other hand, Lemma 4.17 is, to our knowledge, new. We start with some basic definitions:\nDefinition 4.1 A set P with a binary relation ⊑P is called a partially ordered set or poset if ⊑P is reflexive, transitive and antisymmetric.\nUsually, the subscript P in ⊑P will be omitted when it is obvious from context.\nDefinition 4.2 Let P be a poset. An element x ∈ P is called an upper bound for a subset A ⊆ P , if for every y ∈ A, y ⊑ x. If the set of upper bounds of A has a least element, then this element is called the least upper bound (or lub) of A and is denoted by ⊔ A. Symmetrically, one can define the notions of lower bound and greatest lower bound (or glb) of A (this last notion denoted by d A).\nThe following proposition (see for example [AJ94][Proposition 2.1.4]) will prove useful later in the paper:\nProposition 4.3 Let P be a poset and let A,B, (Ai)i∈I be subsets of P . Then, the following statements hold (provided the glbs and lubs occurring in the formulas exist):\n1. A ⊆ B implies ⊔ A ⊑ ⊔ B. 2. If A = ⋃ i∈I Ai, then ⊔ A = ⊔ i∈I( ⊔ Ai).\nDefinition 4.4 Let P be a poset. A subset A of P is directed, if it is nonempty and each pair of elements of A has an upper bound in A.\nDefinition 4.5 Let P and Q be posets. A function f : P → Q is called monotonic if for all x, y ∈ P with x ⊑P y, we have f(x) ⊑Q f(y). The set of all monotonic functions from P to Q is denoted by [P m → Q].\nNotice that monotonicity can be generalized in the obvious way for functions f : Pn → Q, n > 0, since Pn is also a poset (where the partial order in this case is defined in a point-wise way).\nWe are particularly interested in one type of posets, namely complete lattices:\nDefinition 4.6 A poset L in which every subset has a least upper bound and a greatest lower bound, is called a complete lattice.\nIn fact, there is a symmetry here: the existence of all least upper bounds suffices to prove that a poset is indeed a complete lattice, a fact that we will freely use throughout the paper.\nProposition 4.7 Let P be a poset, L a complete lattice and let f : P ×P → L be a monotonic function. Then, ⊔ x∈P,y∈P f(x, y) = ⊔ x∈P f(x, x).\nProof. An easy proof using basic properties of posets (see for example the corresponding proof for domains [Ten91][Lemma 5.3, page 92]).\nDefinition 4.8 Let L be a complete lattice. A function f : L → L is called continuous if it is monotonic and for every directed subset A of L, we have f( ⊔ A) = ⊔ f(A).\nWe will write ⊥L for the greatest lower bound of a complete lattice L (called the bottom element of L). A very useful tool in lattice theory, is Kleene’s fixpoint theorem:\nTheorem 4.9 Let L be a complete lattice. Then, every continuous function f : L → L has a least fixpoint lfp(f) given by lfp(f) = ⊔ n<ω f n(⊥L).\nLet A be an arbitrary set and L be a complete lattice. Then, a partial order can be defined on A → L: for all f, g ∈ A → L, we write f ⊑A→L g if for all a ∈ A, f(a) ⊑L g(a). We will often use the following proposition:\nProposition 4.10 Let A be a poset, L a complete lattice and let F ⊆ [A m → L]. Then, for all a ∈ A, ( ⊔ F )(a) = ⊔ f∈F f(a) and ( d F )(a) = d f∈F f(a). Therefore, [A m → L] is a complete lattice.\nProof. We give the proof for ⊔ (the proof for d\nis symmetrical). Let h ∈ A → L such that h(a) = ⊔ f∈F f(a), where the least upper bound is well-defined because L is a complete lattice. Notice that h is obviously an upper bound of F . Now let g be an arbitrary upper bound of F . For each a ∈ A, it holds that g(a) is an upper bound of {f(a) | f ∈ F}, thus ⊔ f∈F f(a) ⊑ g(a)\nwhich means that h ⊑ g. Therefore, h = ⊔ F .\nIt remains to show that h is monotonic. Consider x, y ∈ A such that x ⊑ y. For all f ∈ F we have f(x) ⊑ f(y) due to the monotonicity of f . Since ⊔ f∈F f(y) is an upper bound of\n{f(y) | f ∈ F}, it is also an upper bound of {f(x) | f ∈ F}. Therefore, ⊔ f∈F f(x) ⊑ ⊔ f∈F f(y) and consequently h is monotonic.\nWe will be interested in a certain type of complete lattices in which every element can be “created” by using a set of compact (intuitively, “simple”) elements of the lattice:\nDefinition 4.11 Let L be a complete lattice and let c ∈ L. Then c is called compact if for every A ⊆ L such that c ⊑ ⊔ A, there exists finite A′ ⊆ A such that c ⊑ ⊔ A′. The set of all compact elements of L is denoted by K(L).\nWe can now define the notion of algebraic lattice (see for example [Gra78]), which will prove to be the key lattice-theoretic concept applicable to our context.\nDefinition 4.12 A complete lattice L is called algebraic if every element of L is the least upper bound of a set of compact elements of L.\nThe name “algebraic lattice” is due to G. Birkhoff [Bir67] (who did not assume completeness at that time). In the literature, algebraic lattices are also called compactly generated lattices.\nExample 4.13 The set L = {0, 1} under the usual numerical ordering is an algebraic lattice with K(L) = {0, 1}.\nLet S be a set. Then, 2S, the set of all subsets of S, forms a complete lattice under set inclusion. It is easy to see that this is an algebraic lattice whose compact elements are the finite subsets of S.\nLet P be a poset. Given B ⊆ P and x ∈ P , we write B[x] = {b ∈ B | b ⊑P x}. We have the following easy proposition:\nProposition 4.14 Let L be an algebraic lattice. Then, for every x ∈ L, x = ⊔\nK(L)[x].\nProof. Obviously it holds that ⊔ K(L)[x] ⊑ x. We show that x ⊑ ⊔\nK(L)[x]. By Definition 4.12, there exists A ⊆ K(L) such that x = ⊔ A. Obviously, A ⊆ K(L)[x]. Therefore, by Proposition 4.3, ⊔ A ⊑ ⊔ K(L)[x] and consequently x ⊑ ⊔ K(L)[x].\nGiven an algebraic lattice L, the set K(L) will be called the basis of L. If additionally, K(L) is countable, then L will be called an ω-algebraic lattice.\nIn the rest of this section we will define a particular class of algebraic lattices that arise in our semantics of higher-order logic programming. This class will be characterized by Lemma 4.17 that follows. We first need to define the notion of “step functions” (see for example [AJ94]) which are used to build the compact elements of our algebraic lattices.\nDefinition 4.15 Let A be a poset and L be an algebraic lattice. For each a ∈ A and c ∈ K(L), we define the function (a ց c) : A → L as\n(a ց c)(x) = { c, if a ⊑A x ⊥L, otherwise\nThe functions of the above form will be called the step functions of A → L.\nExample 4.16 Consider a non-empty set A equipped with the trivial partial order that relates every element of A to itself, ie., a ⊑A a for all a. Moreover, let L = {0, 1} (which by Example 4.13 is an algebraic lattice). Then, for every a ∈ A, (a ց 1) is the function that returns 1 iff its argument is equal to a. In other words (a ց 1) is the singleton set {a}. On the other hand, for every a, (a ց 0) corresponds to the empty set.\nAs a second example, assume that A is the set of finite subsets of N and that L = {0, 1}. Then, for any finite set a ∈ A, (a ց 1) is the function that given any finite set x such that x ⊇ a, (a ց 1)(x) = 1. In other words, (a ց 1) is a set consisting of a and all its (finite) supersets. On the other hand, for every a, (a ց 0) is the function that given any finite set x, (a ց 0)(x) = 0, ie., it corresponds to the empty set (of sets).\nThe following lemma (which we have not seen explicitly stated before) identifies a class of algebraic lattices that will play the central role in the development of the semantics of higherorder logic programming. An important characteristic of these lattices is that they have a simple characterization of their basis. The proof of the lemma is given in Appendix A.\nLemma 4.17 Let A be a poset and L be an algebraic lattice. Then, [A m → L] is an algebraic lattice whose basis is the set of all least upper bounds of finitely many step functions from A to L. If, additionally, A is countable and L is an ω-algebraic lattice then [A m → L] is an ω-algebraic lattice.\nWe can now outline the reasons why algebraic lattices play such an important role in our context. As we have already mentioned, one of the contributions of the paper is that it allows the treatment of queries with uninstantiated predicate variables. The results of [Wad91] indicate that (due to continuity), if a relation satisfies a predicate, then some “finite representative” of this relation also satisfies it. This gives the idea of defining a semantics which makes these “finite representatives” more explicit. Intuitively, these finite representatives are the compact elements of an algebraic lattice. From an operational point of view, restricting attention to the compact elements allows us to answer queries with uninstantiated variables: if the set of compact elements is enumerable then we can try them one by one examining in each case whether the query is satisfied.\nMore formally, since our lattices are algebraic and satisfy the conditions of Lemma 4.17, we have a relatively easy characterization of their sets of compact elements (as suggested by Lemma 4.17). Moreover, as we are going to see, if we restrict attention to Herbrand interpretations (see Section 6), then the lattices that we have to consider are all ω-algebraic and therefore their sets of compact elements are countable. For these lattices it turns out that we can devise an effective procedure for enumerating their compact elements which leads us to an effective proof procedure for our higher-order language."
    }, {
      "heading" : "5 The Semantics of H",
      "text" : "The semantics of H is built upon the notion of algebraic lattice. Recall that an algebraic lattice is a complete lattice L with the additional property that every element x of L is the least upper bound of K(L)[x]."
    }, {
      "heading" : "5.1 The Semantics of Types",
      "text" : "Before specifying the semantics of expressions of H we need to provide the set-theoretic meaning of the types of expressions of H with respect to a set D (where D is later going to be the domain of our interpretations). The fact that a given type τ denotes a set [[τ ]]D will mean that an expression of type τ denotes an element of [[τ ]]D. In other words, the semantics of types help us understand what are the meanings of the expressions of our language. In the following definition we define simultaneously and recursively two things: the semantics [[τ ]]D of a type τ and the corresponding partial order ⊑τ 2.\nDefinition 5.1 Let D be a non-empty set. Then:\n• [[ι]]D = D, and ⊑ι is the trivial partial order such that d ⊑ι d, for all d ∈ D.\n• [[ιn → ι]]D = D n → D. A partial order for this case will not be needed.\n• [[o]]D = {0, 1}, and ⊑o is the partial order defined by the numerical ordering on {0, 1}.\n• [[ι → π]]D = D → [[π]]D, and ⊑ι→π is the partial order defined as follows: for all f, g ∈ [[ι → π]]D, f ⊑ι→π g if and only if f(d) ⊑π g(d), for all d ∈ D.\n• [[π1 → π2]]D = [K([[π1]]D) m → [[π2]]D], and ⊑π1→π2 is the partial order defined as follows:\nfor all f, g ∈ [[π1 → π2]]D, f ⊑π1→π2 g if and only if f(d) ⊑π2 g(d), for all d ∈ K([[π1]]D).\nIt is not immediately obvious that the last case in the above definition is well-defined. More specifically, in order for the quantity K([[π1]]D) to make sense, [[π1]]D must be a complete lattice. This is ensured by the following lemma:\nLemma 5.2 Let D be a non-empty set. Then, for every π, [[π]]D is an algebraic lattice (ωalgebraic if D is countable).\nProof. The proof is by a simple induction on the structure of π. The basis case is for π = o and holds trivially (see Example 4.13). For the induction step, we distinguish two cases. The first case is for π = ι → π1. Then, [[ι → π1]]D = D → [[π1]]D. Notice now that D is partially ordered by the trivial partial order ⊑ι, and it holds that D → [[π1]]D = [D m → [[π1]]D] (monotonicity is trivial in this case). By the induction hypothesis and Lemma 4.17 it follows that [[π]]D is an algebraic lattice (ω-algebraic if D is countable). The second case is for π = π1 → π2, and the result follows by the induction hypothesis and Lemma 4.17.\nThe following definition gives us a convenient shorthand when we want to refer to an object that is either a compact element or a member of the domain D of our interpretations. This shorthand will be used in various places of the paper.\n2Notice that we are writing ⊑τ instead of the more accurate ⊑[[τ]] D . In the following, for brevity reasons we\nwill often use the former (simpler) notation. Similarly, we will often write ⊥π instead of ⊥[[π]] D .\nDefinition 5.3 Let D be a non-empty set and let ρ be an argument type. Define:\nFD(ρ) = { D, if ρ = ι K([[ρ]]D), otherwise\nThe set FD(ρ) will be called the set of basic elements of type ρ (with respect to the set D).\nExample 5.4 Consider the type ι → o (a first-order predicate with one argument has this type). By Definition 5.1, [[ι → o]]D is the set of all functions from D to {0, 1} (or equivalently, of arbitrary subsets of D).\nAs a second example, consider the type (ι → o) → o. This is the type of a predicate which takes as its only parameter another predicate which is first-order; for example, p in Example 2.1 has this type. Then, it can be verified using Lemma 4.17 and Example 4.16 that the set K([[ι → o]]D) is the set of all finite functions from D to {0, 1} (or equivalently, of finite subsets of D). By Definition 5.1, [[(ι → o) → o]]D is the set of all monotonic functions from finite subsets of D (ie., elements of K([[ι → o]]D)) to {0, 1}. In other words, in the semantics of H, a predicate of type (ι → o) → o will denote a monotonic function from finite subsets of D to {0, 1}. The role that monotonicity plays in this context can be intuitively explained by considering again Example 2.1: if p is true of a finite set, then this set must contain both 0 and s(0). But then, p will also be true for every superset of this set (since every superset also contains both 0 and s(0)). As we are going to see, the meaning of all the higher-order predicates that are defined in a program will possess the monotonicity property.\nIt should be noted at this point that the semantics of types of our language is in some sense a finitary version of the one given in [Wad91], where the denotation of a type of the form π1 → π2 is the set of all continuous functions from the denotation of π1 to the denotation of π2 (more details on the connections between the two approaches will be given in Section 5.3). Notice now that in our interpretation of types, only monotonicity is required; actually, continuity is not applicable in our interpretation: given a type π1 → π2, it would be meaningless to talk about the continuous functions from K([[π1]]D) to [[π2]]D because K([[π1]]D) is not in general a complete lattice3 as required by the definition of continuity. However, as we are going to see, monotonicity suffices in order to establish that the immediate consequence operator of every program is continuous (Lemma 6.10) and therefore has a least fixed-point.\nAs a last remark, we should mention that the interpretation of types given in Definition 5.1, does not apply to the inverse implication operator ←π of H, whose denotation is not monotonic (for example, notice that negation can be implicitly defined with the use of implication). However, since the use of ←π is not allowed inside positive expressions, the non-monotonicity of ←π does not create any semantic problems."
    }, {
      "heading" : "5.2 The Semantics of Expressions",
      "text" : "We can now proceed to give meaning to the expressions of H. This is performed by first defining the notions of interpretation and state for H:\nDefinition 5.5 An interpretation I of H consists of:\n1. a nonempty set D, called the domain of I\n2. an assignment to each individual constant symbol c, of an element I(c) ∈ D\n3To see this, take π1 = ι → o and let D be an infinite set. Then, K([[ι → o]]D) consists of all finite subsets of D and is not a complete lattice (since the least upper bound of a set of finite sets can itself be infinite).\n3. an assignment to each predicate constant p of type π, of an element I(p) ∈ [[π]]D\n4. an assignment to each function symbol f of type ιn → ι, of a function I(f) ∈ Dn → D.\nDefinition 5.6 Let D be a nonempty set. Then, a state s of H over D is a function that assigns to each argument variable V of type ρ of H an element s(V) ∈ FD(ρ).\nIn the following, s[d/V] is used to denote a state that is identical to s the only difference being that the new state assigns to V the value d.\nBefore we proceed to formally define the semantics of expressions of H, a short discussion on the semantics of application is needed. The key technical difficulty we have to confront can be explained by reconsidering Example 2.1 in the more formal context that we have now developed.\nExample 5.7 Consider again the program from Example 2.1:\np(Q):-Q(0),Q(s(0)). nat(0). nat(s(X)):-nat(X).\nConsider also the query ← p(nat). The type of p is (ι → o) → o, while the type of nat is ι → o. Let I be an interpretation with underlying domain D. Then, according to Definition 5.5, I(p) must be a monotonic function from K([[ι → o]]D) to {0, 1}. Moreover, according to Example 5.4, FD(ι → o) consists of all the finite sets of elements of D. But I(nat) is a member of [[ι → o]]D and can therefore be an infinite set. How can we apply I(p) to I(nat)? To overcome this problem, observe that in order for the predicate p to succeed for its argument Q, it only has to examine a “finite number of facts” about Q (namely whether Q is true of 0 and s(0)). This remark suggests that the meaning of p(nat) can be established following a non-standard interpretation of application: we apply I(p) to all the “finite approximations” of I(nat), ie., to all elements of K([[ι → o]]D)[I(nat)], and then take the least upper bound of the results. Notice that our approach heavily relies on the fact that our semantic domains are algebraic lattices: every element of such a lattice (like I(nat) in our example) is the least upper bound of the compact elements of the lattice that are below it (the finite subsets of I(nat) in our case).\nWe can now proceed to present the semantics of H:\nDefinition 5.8 Let I be an interpretation of H, let D be the domain of I, and let s be a state over D. Then, the semantics of expressions of H with respect to I and s, is defined as follows:\n1. [[0]]s(I) = 0\n2. [[1]]s(I) = 1\n3. [[c]]s(I) = I(c), for every individual constant c\n4. [[p]]s(I) = I(p), for every predicate constant p\n5. [[V]]s(I) = s(V), for every argument variable V\n6. [[(f E1 · · ·En)]]s(I) = I(f) [[E1]]s(I) · · · [[En]]s(I), for every n-ary function symbol f 7. [[(E1E2)]]s(I) = ⊔\nb∈B([[E1]]s(I)(b)), where B = FD(type(E2))[[[E2]]s(I)]\n8. [[(λV.E)]]s(I) = λd.[[E]]s[d/V](I), where d ranges over FD(type(V))\n9. [[(E1 ∨ π E2)]]s(I) = ⊔ π{[[E1]]s(I), [[E2]]s(I)}, where ⊔\nπ is the least upper bound function on [[π]]D\n10. [[(E1 ∧ π E2)]]s(I) = d π{[[E1]]s(I), [[E2]]s(I)}, where d\nπ is the greatest lower bound function on [[π]]D\n11. [[(E1≈E2)]]s(I) = { 1, if [[E1]]s(I) = [[E2]]s(I) 0, otherwise\n12. [[(∃VE)]]s(I) =\n{ 1, if there exists d ∈ FD(type(V)) such that [[E]]s[d/V](I) = 1\n0, otherwise\n13. [[(p ←π E)]]s(I) = { 1, if [[E]]s(I) ⊑π I(p) 0, otherwise\n14. [[(← E)]]s(I) = { 1, if [[E]]s(I) = 0 0, otherwise\nFor closed expressions E we will often write [[E]](I) instead of [[E]]s(I) (since, in this case, the meaning of E is independent of s).\nWe need to demonstrate that the semantic valuation function [[·]] assigns to every expression of H an element of the corresponding semantic domain. More formally, we need to establish that for every interpretation I with domain D, for every state s over D and for all expressions E : ρ, it holds that [[E]]s(I) ∈ [[ρ]]D. In order to prove this, the following definition is needed:\nDefinition 5.9 Let SH,D be the set of states of H over the nonempty set D. We define the following partial order on SH,D: for all s1, s2 ∈ SH,D, s1 ⊑SH,D s2 iff for every argument variable V : ρ of H, s1(V) ⊑ρ s2(V).\nThe following lemma states that Definition 5.8 assigns to expressions elements of the corresponding semantic domain. Notice that in order to establish this, we must also prove simultaneously that the meaning of positive expressions is monotonic with respect to states.\nLemma 5.10 Let E : ρ be an expression of H and let D be a nonempty set. Moreover, let s, s1, s2 be states over D and let I be an interpretation over D. Then:\n1. [[E]]s(I) ∈ [[ρ]]D.\n2. If E is positive and s1 ⊑SH,D s2 then [[E]]s1(I) ⊑ρ [[E]]s2(I).\nThe proof of the lemma is given in Appendix B.\nWe can now define the important notion of a model of a set of formulas:\nDefinition 5.11 Let S be a set of formulas of H and let I be an interpretation of H. We say that I is a model of S if for every F ∈ S and for every state s over the domain of I, [[F]]s(I) = 1.\nWe close this section with the definitions of the notions of unsatisfiability and of logical consequence of a set of formulas.\nDefinition 5.12 Let S be a set of formulas of H. We say that S is unsatisfiable if no interpretation of H is a model for S.\nDefinition 5.13 Let S be a set of formulas and F a formula of H. We say that F is a logical consequence of S if, for every interpretation I of H, I is a model of S implies that I is a model of F."
    }, {
      "heading" : "5.3 A Comparison with the Continuous Semantics",
      "text" : "In this subsection we give a brief comparison of the proposed semantics with the semantics introduced in [Wad91]. A complete presentation of such a comparison would require a detailed presentation of the approach introduced in [Wad91] and its adaptation to the richer language H introduced in this paper. We avoid such an extensive comparison by outlining the main points in an intuitive way.\nAs already mentioned, the source language considered in [Wad91] is restricted compared to H. However, the semantics of [Wad91] can be appropriately extended to apply to H as well. Given a non-empty set D, let us denote by [[ρ]]∗D the semantics of an argument type ρ in D under the approach of [Wad91]. Then, the semantics of types is defined as follows:\n• [[ι]]∗D = D.\n• [[ιn → ι]]∗D = D n → D.\n• [[o]]∗D = {0, 1}.\n• [[ι → π]]∗D = D → [[π]] ∗ D.\n• [[π1 → π2]] ∗ D = [[[π1]] ∗ D c → [[π2]] ∗ D].\nwhere by [A c → B] we denote the set of continuous functions from A to B. The corresponding partial orders can be easily defined as in Definition 5.1. The semantics of expressions can be defined in an analogous way as in Definition 5.8, the main difference being that the semantics of application is the standard one. Roughly speaking, one can say that the semantics of [Wad91] is the logic programming analogue of the standard denotational semantics of functional programming languages [Ten91]. In the following, we will refer to the semantics of [Wad91] as the “continuous semantics”.\nIt is relatively easy to show that for every argument type ρ of H there is a bijection between the sets [[ρ]]D and [[ρ]] ∗ D. Similarly, there is a bijection between the set of interpretations of H under the proposed semantics and the set of interpretations of H under the continuous semantics. Then, the following proposition can be established:\nProposition 5.14 Let P be a program and let F be a formula of H. Then, F is a logical consequence of P under the proposed semantics iff F is a logical consequence of P under the continuous semantics.\nIn other words, the two semantics, despite their differences, are closely related. The key advantage of the proposed semantics is that it is much closer to the SLD-resolution proof procedure that will be introduced in Section 7. More specifically:\n• The compact elements of our algebraic lattices correspond to the basic expressions that are a vital characteristic of the proposed proof procedure (see Subsection 7.1).\n• The notion of answer and correct answer for a query (see Definitions 7.16 and 7.17) can now be accurately defined. Notice that the notion of correct answer must be quite close to that of computed answer in order to be able to state the main completeness theorem.\nIn conclusion, the proposed semantics allows us to define an SLD-resolution proof procedure and it helps us formalize and prove its completeness. It is unclear to us whether (and how) this could have been accomplished by relying on the continuous semantics."
    }, {
      "heading" : "6 Minimum Herbrand Model Semantics",
      "text" : "Herbrand interpretations constitute a special form of interpretations that have proven to be a cornerstone of first-order logic programming. Analogously, we have:\nDefinition 6.1 The Herbrand universe UH of H is the set of all terms that can be formed out of the individual constants and the function symbols of H.\nDefinition 6.2 A Herbrand interpretation I of H is an interpretation such that:\n1. The domain of I is the Herbrand universe UH of H.\n2. For every individual constant c, I(c) = c.\n3. For every predicate constant p of type π, I(p) ∈ [[π]]UH.\n4. For every n-ary function symbol f and all terms t1, . . . , tn of UH, I(f) t1 · · · tn = f t1 · · · tn.\nSince all Herbrand interpretations have the same underlying domain, we will often refer to a “Herbrand state s”, meaning a state whose underlying domain is UH. As it is a standard practice in logic programming, we will often refer to an “interpretation of a set of formulas S” rather than of the underlying language H. In this case, we will implicitly assume that the set of individual constants and function symbols are those that appear in S. Under this assumption, we will often talk about the “Herbrand universe US of a set of formulas S”.\nWe should also note that since the Herbrand universe is a countable set, by Lemma 5.2, for every predicate type π, [[π]]UH is an ω-algebraic lattice (ie., it has a countable basis).\nWe can now proceed to examine properties of Herbrand interpretations. In the following we denote the set of Herbrand interpretations of a program P with IP .\nDefinition 6.3 Let P be a program. We define the following partial order on IP: for all I, J ∈ IP, I ⊑IP J iff for every π and for every predicate constant p : π of P, I(p) ⊑π J(p).\nLemma 6.4 Let P be a program and let I ⊆ IP. Then, for every predicate p of P, ( ⊔\nI)(p) =⊔ I∈I I(p) and ( d I)(p) = d I∈I I(p). Therefore, IP is a complete lattice under ⊑IP.\nProof. We give the proof for ⊔ ; the proof for d is symmetrical and omitted. Let J ∈ IP\nsuch that for every p : π in P, J(p) = ⊔ I∈I I(p). Notice that ⊔\nI∈I I(p) is well-defined since [[π]]UP is a complete lattice. Notice also that J is an upper-bound for I because for every I ∈ I, I ⊑IP J . Let J ′ be an arbitrary upper bound of I. Then, for every p : π, it holds that J ′(p)\nis an upper bound of {I(p) | I ∈ I}, and therefore ⊔\nI∈I I(p) ⊑π J ′(p), which implies that\nJ ⊑IP J ′.\nIn the following we denote with ⊥IP the greatest lower bound of IP, ie., the interpretation which for every π, assigns to each predicate p : π of P the element ⊥π.\nThe properties of monotonicity and continuity of the semantic valuation function will prove vital:\nLemma 6.5 (Monotonicity of Semantics) Let P be a program and let E : ρ be a positive expression of P. Let I, J be Herbrand interpretations and s a Herbrand state of P. If I ⊑IP J then [[E]]s(I) ⊑ρ [[E]]s(J).\nThe proof of the lemma is given in Appendix C.\nLemma 6.6 (Continuity of Semantics) Let P be a program and let E be a positive expression of P. Let I be a directed set of Herbrand interpretations and s a Herbrand state of P. Then, [[E]]s( ⊔ I) = ⊔ I∈I [[E]]s(I).\nThe proof of the lemma is given in Appendix D.\nAll the basic properties of first-order logic programming extend naturally to the higherorder case:\nTheorem 6.7 (Model Intersection Theorem) Let P be a program and M a non-empty set of Herbrand models of P. Then, d M is a Herbrand model for P.\nProof. By Lemma 6.4, d M is well-defined. Assume that d M is not a model for P. Then, there exists a rule p ←π E in P and b1, . . . , bn of the appropriate types such that ( d M)(p) b1 · · · bn = 0 while [[E]]( d\nM) b1 · · · bn = 1. Since for every M ∈ M we haved M ⊑ M , using Lemma 6.5 we conclude that for all M ∈ M, [[E]](M) b1 · · · bn = 1. Moreover, since ( d M)(p) b1 · · · bn = 0, by Lemma 6.4 we get that ( d\nM(p)) b1 · · · bn = 0. By Proposition 4.10 we conclude that for some M ∈ M, M(p) b1 · · · bn = 0. But then there exists M ∈ M that does not satisfy the rule p ←π E, and therefore is not a model of P (contradiction).\nIt is straightforward to check that every higher-order program P has at least one Herbrand model I, namely the one which for every predicate constant p of P and for all basic elements b1, . . . , bn of the appropriate types, I(p) b1 · · · bn = 1. Notice that this model generalizes the familiar idea of “Herbrand Base” that is used in the theory of first-order logic programming.\nSince the set of models of a higher-order logic program is non-empty, the intersection (glb) of all Herbrand models is well-defined, and by the above theorem is a model of the program. We will denote this model by MP.\nDefinition 6.8 Let P be a program. The mapping TP : IP → IP is defined as follows for every p : π in P and for every I ∈ IP:\nTP(I)(p) = ⊔\n(p←πE)∈P\n[[E]](I)\nThe mapping TP will be called the immediate consequence operator for P.\nThe fact that TP is well-defined is verified by the following lemma:\nLemma 6.9 Let P be a program and let p : π be a predicate constant of P. Then, for every I ∈ IP, TP(I)(p) ∈ [[π]]UP.\nProof. The result follows directly by the definition of TP, Lemma 5.10 and the fact that [[π]]UP is a complete lattice.\nThe key property of TP is that it is continuous:\nLemma 6.10 Let P be a program. Then the mapping TP is continuous.\nProof. Straightforward using Lemma 6.6.\nThe following property of TP generalizes the corresponding well-known property from firstorder logic programming:\nLemma 6.11 Let P be a program and let I ∈ IP. Then I is a model of P if and only if TP(I) ⊑IP I.\nProof. An interpretation I ∈ IP is a model of P iff [[E]](I) ⊑π I(p) for every clause p ←π E in P iff ⊔ (p←πE)∈P [[E]](I) ⊑π I(p) iff TP(I)(p) ⊑π I(p).\nDefine now the following sequence of interpretations:\nTP ↑ 0 = ⊥IP TP ↑ (n + 1) = TP(TP ↑ n) TP ↑ ω = ⊔ {TP ↑ n | n < ω}\nWe have the following theorem (which is entirely analogous to the one for the first-order case):\nTheorem 6.12 Let P be a program. Then MP = lfp(TP) = TP ↑ ω.\nProof. Using exactly the same reasoning as in the first-order case (see for example the corresponding proof in [Llo87])."
    }, {
      "heading" : "7 Proof Procedure",
      "text" : "In this section we propose a sound and complete proof-procedure for H. One important aspect we initially have to resolve, is how to represent basic elements (see Definition 5.3) in our source language. In the following subsection we introduce a class of positive expressions, namely basic expressions, which are the syntactic analogues of basic elements. Basic expressions will be used in order to formalize the notion of answer (to a given query) as-well-as in our development of the SLD-resolution for H."
    }, {
      "heading" : "7.1 Basic Expressions",
      "text" : "As we have already seen, basic elements have played an important role in the development of the semantics of our higher-order logic programming language. In order to devise a sound and complete proof procedure for our language, we first need to find a syntactic representation for basic elements. Since the definition of basic elements uses the operator ⊑ (see Lemma 4.17, Definition 4.15 and Definition 5.3), it is not immediately obvious how one can construct a positive expression whose meaning coincides with a given basic element. Basic expressions introduced below, solve this apparent difficulty:\nDefinition 7.1 The set of basic expressions of H is recursively defined as follows. Every expression of H of type ι is a basic expression of type ι. Every predicate variable of H of type π is a basic expression of type π. The propositional constants 0 and 1 are basic expressions of type o. A non-empty finite union of expressions each one of which has the following form, is a basic expression of type ρ1 → · · · → ρn → o (where V1 : ρ1, . . . ,Vn : ρn):\n1. λV1. · · · λVn.0\n2. λV1. · · · λVn.(A1 ∧ · · · ∧ An), where each Ai is either\n(a) (Vi ≈ Bi), if Vi : ι and Bi : ι is a basic expression where Vj 6∈ FV (Bi) for all j, or\n(b) the constant 1 or Vi, if Vi : o, or\n(c) the constant 1 or Vi(B11) · · · (B1r)∧· · ·∧Vi(Bm1) · · · (Bmr), m > 0, if type(Vi) 6= ι, o and the Bkl’s are basic expressions of the appropriate types, where Vj 6∈ FV (Bkl) for all j, k, l.\nThe Bi and Bkl above will be called the basic subexpressions of B.\nThe following example illustrates the ideas behind the above definition.\nExample 7.2 We consider various cases of the above definition:\n• The terms a, f(a,b), X and f(X,h(Y)), are basic expressions of type ι.\n• Assume X : ρ. Then, λX.0 is a basic expression of type ρ → o. Intuitively, it corresponds to the basic element ⊥ρ→o.\n• Assume X : ι. Then, λX.(X≈a) is a basic expression of type ι → o. Intuitively, it corresponds to the basic element (a ց 1) or more simply to the finite set {a}.\n• Assume X : ι and Y : ι. Then, λX.λY.(X≈a)∧(Y≈b) is a basic expression of type ι → ι → o. Intuitively, it corresponds to the basic element (a ց (b ց 1)) or more simply to the singleton binary relation {(a, b)}. • Assume X : ι. Then, (λX.(X≈a)) ∨\nι→o(λX.(X≈b)) is a basic expression of type ι → o. It corresponds to the basic element ⊔ {(a ց 1), (b ց 1)}, or more simply to the finite set {a, b}.\n• Assume Q : ι → o. Then, λQ.(Q(a)∧Q(b))is a basic expression of type (ι → o) → o. Intuitively, it corresponds to the basic element ( ⊔ {(a ց 1), (b ց 1)}) ց 1. More simply,\nit corresponds to the set of all finite sets that contain both a and b.\nThe proof procedure that will be developed later in this section, relies on a special form of basic expressions:\nDefinition 7.3 The set of basic templates of H is the subset of the set of basic expressions of H defined as follows:\n• The propositional constants 0 and 1 are basic templates.\n• Every non-empty finite union of basic expressions (of the form presented in items 1 and 2 of Definition 7.1) in which all the basic subexpressions involved are distinct variables, is a basic template.\nThe variables mentioned above, will be called template variables.\nExample 7.4 Assume in the following expressions that X, Y, Z, W : ι, Q, Q1, Q2 : ι → o and R : ((ι → o) → o) → o. The expression λX.(X≈Z) is a basic template of type ι → o. The expression λX.λY.(X≈Z)∧(Y≈W) is a basic template of type ι → ι → o; the template variables in this case are Z and W. The expression λQ.(Q(Z)∧Q(W))is a basic template of type (ι → o) → o with template variables Z and W. The expression λR.(R(Q1)∧R(Q2)) is a basic template of type ((ι → o) → o) → o with template variables Q1 and Q2.\nNotice from the above example that the structure of basic templates is in general much simpler than that of basic expressions (due to the fact that a template variable can represent an arbitrary basic expression of the same type). For this reason, basic templates are much simpler to enumerate than arbitrary basic expressions.\nThe following two lemmas establish the connections between basic elements and basic expressions.\nLemma 7.5 For every basic expression B : ρ, for every Herbrand interpretation I of H, and for every Herbrand state s, [[B]]s(I) ∈ FUH(ρ).\nProof. The proof is by induction on the type of B. The basis case is for basic expressions of type ι and o and holds trivially. We demonstrate that the lemma holds for basic expressions of type ρ = ρ1 → · · · → ρn → o, assuming that it holds for all basic expressions that have simpler types than ρ. If the basic expression is a predicate variable, the result is immediate; otherwise, we have to distinguish the following cases:\nCase 1: B = λV1. · · · λVn.0. Then, the corresponding basic element in FUH(ρ) is the bottom element of type ρ1 → · · · → ρn → o (ie., ⊥ρ1→···→ρn→o).\nCase 2: B = λV1. · · · λVn.(A1∧· · ·∧An). Then, the corresponding basic element is the element b1 ց (b2 ց · · · ց (bn ց 1) · · · ), where the bi are defined as follows:\n• If Vi : ι, then by Definition 7.1, Ai = (Vi ≈ Bi). In this case, bi = [[Bi]]s(I).\n• If Vi : o then Ai is either equal to 1 or to Vi; in the former case bi = 0 and in the latter case bi = 1.\n• If Vi is of any other type then Ai is either equal to 1 or to Vi(B11) · · · (B1r) ∧ · · · ∧ Vi(Bm1) · · · (Bmr), where m > 0. In the former case it is bi = 0; in the latter case bi = ⊔ 1≤j≤m([[Bj1]]s(I) ց ([[Bj2]]s(I) ց · · · ց ([[Bjr]]s(I) ց 1) · · · )).\nCase 3: B is a finite union of lambda abstractions. Then, for each term of the finite union we can create (as above) a basic element. By taking the finite union of these elements, we create the basic element that corresponds to B.\nIt can be easily verified that for every basic expression B, [[B]]s(I) coincides with the cor-\nresponding basic element defined as above.\nThe converse of the above lemma holds, as the following lemma demonstrates.\nLemma 7.6 Let ρ be any argument type and let b ∈ FUH(ρ). Then, there exists a closed basic expression B : ρ such that for every Herbrand interpretation I, [[B]](I) = b.\nProof. The proof is by induction on the structure of argument types. The basis case is for argument types ι and o, and holds trivially. We demonstrate that the lemma holds for type ρ = ρ1 → · · · → ρn → o, assuming that it holds for all subtypes of ρ. Assume now that b is a basic element of type ρ, consisting of a finite union of step functions.\nIf the union is empty, then B = λV1. · · · λVn.0. Assume now that the union is non-empty. Then, the basic expression corresponding to b will simply be the union of the basic expressions corresponding to the step functions that comprise b.\nLet b1 ց (b2 ց · · · ց (bn ց 1) · · · ) be one of the step functions that constitute b. We create the basic expression: B = λV1. · · ·λVn.(A1 ∧ · · ·An) where each Ai can be created as follows:\n• If bi is of type ι and bi = t ∈ UH, then Ai = (Vi ≈ t).\n• If bi is of type o and bi = 0, then Ai = 1.\n• If bi is of type o and bi = 1, then Ai = 0.\n• Otherwise, bi is a finite union of m > 0 basic elements of the form bj1 ց (bj2 ց · · · ց (bjr ց 1) · · · ), 1 ≤ j ≤ m. Then, Ai = Vi(B11) · · · (B1r) ∧ · · · ∧ Vi(Bm1) · · · (Bmr), where Bj1, . . . ,Bjm are the expressions that correspond (by the induction hypothesis) to bj1, . . . , bjm.\nIt is easy to verify that the resulting basic expression B satisfies [[B]](I) = b.\nThe above two lemmas suggest that basic expressions are the syntactic analogues of basic elements."
    }, {
      "heading" : "7.2 Substitutions and Unifiers",
      "text" : "Substitutions are vital in the development of the proof procedure for H:\nDefinition 7.7 A substitution θ is a finite set of the form {V1/E1, . . . ,Vn/En}, where the Vi’s are different argument variables of H and each Ei is a positive expression of H having the same type as Vi. We write dom(θ) = {V1, . . . ,Vn} and range(θ) = {E1, . . . ,En}. A substitution is called basic if all Ei are basic expressions. A substitution is called zero-order, if type(Vi) = ι, for all i ∈ {1, . . . , n} (notice that every zero-order substitution is also basic). The substitution corresponding to the empty set will be called the identity substitution and will be denoted by ǫ.\nWe are now ready to define what it means to apply a substitution θ to an expression E. Such definitions are usually complicated by the fact that one has to often rename the bound variable before applying θ to the body of a lambda abstraction. In order to simplify matters, we follow the simple approach suggested in [Bar84][pages 26-27], which consists of the following two conventions:\n• The α-congruence convention: Expressions that are α-congruent will be considered identical (expression E1 is α-congruent with expression E2 if E2 results from E1 by a series of changes of bound variables). For example, λQ.Q(a) is α-congruent to λR.R(a).\n• The variable convention: If expressions E1, . . . ,En occur in a certain mathematical context (eg., definition, proof), then in these expressions all bound variables are chosen to be different from the free variables.\nUsing the variable convention, we have the following simple definition:\nDefinition 7.8 Let θ be a substitution and let E be a positive expression. Then, Eθ is an expression obtained from E as follows:\n• Eθ = E, if E is 0, 1, c, or p.\n• Vθ = θ(V) if V ∈ dom(θ); otherwise, Vθ = V.\n• (f E1 · · ·En)θ = (f E1θ · · ·Enθ).\n• (E1E2)θ = (E1θ E2θ).\n• (λV.E1)θ = (λV.(E1θ)).\n• (E1 ∨ π E2)θ = (E1θ ∨ π E2θ). • (E1 ∧ π E2)θ = (E1θ ∧ π E2θ).\n• (E1 ≈ E2)θ = (E1θ ≈ E2θ).\n• (∃VE1)θ = (∃V (E1θ)).\nNotice that in the case of lambda abstraction (and similarly in the case of existential quantification), it is not needed to say “provided V 6∈ FV (range(θ)) and V 6∈ dom(θ)”. By the variable convention this is the case.\nDefinition 7.9 Let θ = {V1/E1, . . . ,Vm/Em} and σ = {V ′ 1/E ′ 1, . . . ,V ′ n/E ′ n} be substitutions. Then the composition θσ of θ and σ is the substitution obtained from the set\n{V1/E1σ, . . . ,Vm/Emσ,V ′ 1/E ′ 1, . . . ,V ′ n/E ′ n}\nby deleting any Vi/Eiσ for which Vi = Eiσ and deleting any V ′ j/E ′ j for which V ′ j ∈ {V1, . . . ,Vm}.\nThe following proposition is easy to establish:\nProposition 7.10 Let θ, σ and γ be substitutions. Then:\n1. θǫ = ǫθ = θ.\n2. For all positive expressions E, (Eθ)σ = E(θσ).\n3. (θσ)γ = θ(σγ).\nWe will use the notions of unifier and most general unifier, which in our case have exactly the same meaning as in the case of classical (first-order) logic programming:\nDefinition 7.11 Let S be a set of terms of H (ie., expressions of type ι). A zero-order substitution θ will be called a unifier of the expressions in S if the set Sθ = {Eθ | E ∈ S} is a singleton. The substitution θ will be called a most general unifier of S (denoted by mgu(S)), if for every unifier σ of the expressions in S, there exists a zero-order substitution γ such that σ = θγ.\nWe now have the following Substitution Lemma (see for example [Ten91] for a corresponding lemma in the case of functional programming). The Substitution Lemma shows that given a basic substitution θ, the meaning of Eθ is that of E in a certain state definable from θ. The lemma will be later used in the proof of soundness of the proposed proof procedure.\nLemma 7.12 (Substitution Lemma) Let P be a program, let I be an interpretation of P and let s be a state over the domain of I. Let θ be a basic substitution and E be a positive expression. Then, [[Eθ]]s(I) = [[E]]s′(I), where s ′(V) = [[θ(V)]]s(I) if V ∈ dom(θ) and s ′(V) = s(V), otherwise.\nProof. By structural induction on E.\nThe following lemmas, that also involve the notion of substitution, can be easily demonstrated and will prove useful in the sequel.\nLemma 7.13 Let θ1, . . . , θn be basic substitutions. Then, θ1 · · · θn is also a basic substitution.\nProof. By induction on n and using Definitions 7.1 and 7.9.\nLemma 7.14 Let P be a program, let I be an interpretation of P and let s be a state over the domain of I. Let λV.E1 and E2 be positive expressions of type ρ → π and ρ respectively. Then, [[(λV.E1)E2]]s(I) = [[E1{V/E2}]]s(I).\nProof. By structural induction on E1.\nLemma 7.15 Let P be a program, I a Herbrand interpretation of P and s a Herbrand state. Let E be a positive expression. Then, there exists a basic substitution θ such that [[E]]s(I) = [[Eθ]]s′(I) for every Herbrand state s ′.\nProof. Define θ such that if V ∈ FV (E), θ(V) = B, where B is a closed basic expression such that [[B]](I) = s(V) (the existence of such a B is ensured by Lemma 7.6). The lemma follows by a structural induction on E.\nIt is important to note that in the rest of the paper, the substitutions that we will use will be basic ones (unless otherwise stated). Actually, the only place where a non-basic substitution will be needed, is when we perform a β-reduction step (see for example the rule for λ in the forthcoming Definition 7.18)."
    }, {
      "heading" : "7.3 SLD-Resolution",
      "text" : "We now proceed to define the notions of answer and correct answer.\nDefinition 7.16 Let P be a program and G a goal. An answer for P∪{G} is a basic substitution for (certain of the) free variables of G.\nDefinition 7.17 Let P be a program, G =← A a goal clause and θ an answer for P ∪ {G}. We say that θ is a correct answer for P ∪ {G} if for every model M of P and for every state s over the domain of M , [[Aθ]]s(M) = 1.\nDefinition 7.18 Let P be a program and let G =← A and G′ =← A′ be goal clauses. Then, we say that A′ is derived in one step from A using basic substitution θ (or equivalently that G′ is derived in one step from G using θ), and we denote this fact by A θ → A′ (respectively, G θ → G′) if one of the following conditions applies:\n1. p E1 · · ·En ǫ → E E1 · · · En, where p ←π E is a rule in P.\n2. Q E1 · · ·En θ → (Q E1 · · · En)θ, where θ = {Q/Bt} and Bt a basic template.\n3. (λV.E) E1 · · ·En ǫ → (E{V/E1})E2 · · ·En. 4. (E′ ∨\nπ E ′′) E1 · · ·En ǫ → E′ E1 · · ·En.\n5. (E′ ∨\nπ E ′′) E1 · · ·En ǫ → E′′ E1 · · ·En.\n6. (E′ ∧\nπ E ′′) E1 · · ·En ǫ → (E′ E1 · · ·En) ∧ (E ′′ E1 · · ·En), where π 6= o.\n7. (E1 ∧ E2) θ → (E′1 ∧ (E2θ)), if E1 θ → E′1.\n8. (E1 ∧ E2) θ → ((E1θ) ∧ E ′ 2), if E2 θ → E′2.\n9. (✷ ∧ E) ǫ → E\n10. (E ∧ ✷) ǫ → E\n11. (E1 ≈ E2) θ → ✷, where θ is an mgu of E1 and E2.\n12. (∃VE) ǫ → E\nMoreover, we write A θ ։ A′ if A = A0 θ1→ A1 θ2→ · · · θn→ An = A ′, n ≥ 1, where θ = θ1 · · · θn (and similarly for G θ ։ G′).\nDefinition 7.19 Let P be a program and G a goal. An SLD-derivation of P∪{G} is a (finite or infinite) sequence G0 = G,G1, . . . of goals and a sequence θ1, θ2, . . . of basic substitutions such that each Gi+1 is derived in one step from Gi using θi+1. Moreover, for all i, if θi = {V/Bt}, then the free variables of Bt are disjoint from all the variables that have already appeared in the derivation up to Gi−1.\nDefinition 7.20 Let P be a program and G a goal. Assume that P ∪ {G} has a finite SLDderivation G0 = G,G1, . . . ,Gn with basic substitutions θ1, . . . , θn, such that Gn = ✷. Then, we will say that P∪ {G} has an SLD-refutation of length n using basic substitution θ = θ1 · · · θn.\nDefinition 7.21 Let P be a program, G a goal and assume that P∪{G} has an SLD-refutation using basic substitution θ. Then, a computed answer σ for P ∪ {G} is the basic substitution obtained by restricting θ to the free variables of G.\nExample 7.22 Consider the program of Example 3.6. An SLD-refutation of the goal ← closure Q a b is given below (where we have omitted certain simple steps involving lambda abstractions):\nclosure Q a b θ1 = ǫ (λR.λX.λY.(R X Y)) Q a b θ2 = ǫ Q a b θ3 = {Q/(λX.λY.(X≈X0)∧(Y≈Y0))} (λX.λY.(X≈X0)∧(Y≈Y0)) a b θ4 = ǫ (a≈X0)∧(b≈Y0) θ5 = {X0/a} ∧(b≈Y0) θ6 = ǫ (b≈Y0) θ7 = {Y0/b}\nIf we restrict the composition θ1 · · · θ7 to the free variables of the goal, we get the computed answer σ1 = {Q/λX.λY.(X≈a)∧(Y≈b)}. Intuitively, σ1 assigns to Q the relation {(a, b)} (for which the query is obviously true). Notice that by substituting Q with different basic templates, one can get answers that are “similar” to the above one, such as for example {(a, b), (Z1, Z2)} or {(a, b), (Z1, Z2), (Z3, Z4)}, and so on. Answers of this type are in some sense “represented” by the answer {(a, b)}. Actually, one can easily optimize the proof procedure so as to avoid enumerating such superfluous answers (see the discussion in Section 9).\nHowever, there exist other answers to our original query that are genuinely different from {(a, b)} and can be obtained by making different clause choices. For example, another answer to our query is σ2 = {Q/(λX.λY.(X≈a)∧(Y≈Z)) ∨ π (λX.λY.(X≈Z)∧(Y≈b))}, which corresponds to the relations of the form {(a, Z), (Z, b)}, for every Z in the Herbrand universe. Similarly, one can get the answer {(a, Z1), (Z1, Z2), (Z2, b)}, and so on.\nIn other words, we observe that by performing different choices in the selection of a basic template for Q and making an appropriate use of the two rules of the program for closure, we get an infinite (but countable) number of computed answers to our original query."
    }, {
      "heading" : "7.4 Soundness of SLD-resolution",
      "text" : "In this subsection we establish the soundness of the SLD-resolution proof procedure. The following lemmas are very useful in the proof of the soundness theorem:\nLemma 7.23 Let P be a program, let I be an interpretation of P and let s be a state over the domain of I. Let E1 and E2 be positive expressions of type ρ → π and E expression of type ρ. If [[E1]]s(I) ⊑ρ→π [[E2]]s(I), then [[(E1 E)]]s(I) ⊑π [[(E2 E)]]s(I).\nProof. Straightforward using the definition of application.\nLemma 7.24 Let P be a program, let G =← A and G′ =← A′ be goals and let θ be a basic substitution such that A θ → A′. Then, for every model M of P and for every state s over the domain of M , it holds that [[Aθ]]s(M) ⊒ [[A ′]]s(M).\nProof. First, observe that in all cases A is of the form E E1 · · ·Ek, k ≥ 0, where E is an expression of predicate type. We perform a structural induction on E.\nInduction Basis: We distinguish two cases, namely E = p and E = Q. For the first case it suffices to show that [[(pE1 · · · Ek)θ]]s(M) ⊒ [[(Ep E1 · · ·Ek)θ]]s(M), where θ = ǫ and p ←π Ep is a clause in P. This follows easily by the fact that M is a model of P and using Lemma 7.23. The second case is trivial.\nInduction Step: We examine the two most interesting cases (the rest are straightforward):\nCase 1: E = (λV.E′). In this case θ is the empty substitution, and therefore it suffices to show that [[(λV.E′)E1 · · ·Ek]]s(M) ⊒ [[E\n′{V/E1}E2 · · ·Ek]]s(M). By Lemma 7.14 we have that [[(λV.E′)E1]]s(M) = [[E ′{V/E1}]]s(M), and the result follows by Lemma 7.23.\nCase 2: E = (E′ ∧ E′′). Moreover, assume that E′ θ → E′1. Then, (E ′ ∧ E′′) derives in one step the expression (E′1 ∧ (E ′′θ)). It suffices to show that [[(E′ ∧ E′′)θ]]s(M) ⊒ [[E ′ 1 ∧ (E\n′′θ)]]s(M), or equivalently that [[(E′θ) ∧ (E′′θ)]]s(M) ⊒ [[E ′ 1 ∧ (E\n′′θ)]]s(M). But this holds since by the induction hypothesis we have that [[E′θ]]s(M) ⊒ [[E ′ 1]]s(M).\nLemma 7.25 Let P be a program and G =← A be a goal. Let G0 = G,G1 =← A1, . . . ,Gn =← An be an SLD-refutation of length n using basic substitutions θ1, . . . , θn. Then, for every model M of P and for every state s over the domain of M , [[Aθ1 · · · θn]]s(M) ⊒ [[An]]s(M).\nProof. Using Lemma 7.24, Lemma 7.12 and induction on n.\nTheorem 7.26 (Soundness) Let P be a program and G =← A a goal. Then, every computed answer for P ∪ {G} is a correct answer for P ∪ {G}.\nProof. The result is a direct consequence of Lemma 7.25 for Gn = ✷."
    }, {
      "heading" : "7.5 Completeness of SLD-resolution",
      "text" : "In order to establish the completeness of the proposed SLD-resolution, we need to first demonstrate a result that is analogous to the lifting lemma of the first-order case (see [Llo87]). We first state (and prove in the appendix) a more technical lemma, which has as a special case the desired lifting lemma.\nIn the rest of this subsection, whenever we refer to a “substitution” we mean a “basic substitution”.\nLemma 7.27 Let P be a program, G a goal and θ a substitution. Suppose that there exists an SLD-refutation of P ∪ {Gθ} using substitution σ. Then, there exists an SLD-refutation of P ∪ {G} using a substitution δ, where for some substitution γ it holds that δγ ⊇ θσ and dom(δγ − θσ) is a (possibly empty) set of template variables that are introduced during the refutation of P ∪ {G}.\nThe proof of the above lemma is by a straightforward (but tedious) induction on the length of the SLD-refutation of P ∪ {Gθ}, and is given in Appendix E.\nLemma 7.28 (Lifting Lemma) Let P be a program, G a goal and θ a substitution. Suppose that there exists an SLD-refutation of P ∪ {Gθ} using substitution σ. Then, there exists an SLD-refutation of P ∪ {G} using a substitution δ, where for some substitution γ it holds that Gδγ = Gθσ.\nProof. By Lemma 7.27, δγ and θσ differ only in template variables that are introduced during the refutation. By the restriction mentioned in Definition 7.19, these variables are different from the variables in the goal G. Therefore, δγ and θσ agree on the expressions they assign to the free variables of G.\nNotice that the above lifting lemma differs slightly from the corresponding lemma for classical logic programming, where we actually have the equality δγ = θσ. This difference is due to the existence of template variables in the higher-order resolution proof procedure. Of course, if we restrict the higher-order proof procedure to apply to first-order logic programs, then it behaves like classical SLD-resolution and the usual lifting lemma holds.\nExample 7.29 Consider any program P of our higher-order language and consider the goal clause G =← R(Z), where Z is of type ι and R of type ι → o. Let θ = {R/λX.(X ≈ a), Z/a}. Then, Gθ =← (λX.(X ≈ a))(a). We have the following SLD-refutation:\n(λX.(X ≈ a))(a) ǫ → (a ≈ a) ǫ → ✷\nTherefore, Gθ has an SLD-refutation with substitution σ = ǫ. On the other hand, we have the following SLD-refutation of G:\nR(Z) {R/λX.(X ≈ X0)} → (λX.(X ≈ X0))(Z) ǫ → (Z ≈ X0) {X0/Z} → ✷\nTherefore, G has an SLD-refutation with substitution δ which is equal to the composition of the substitutions {R/λX.(X ≈ X0)}, ǫ and {X0/Z}, ie., δ = {R/λX.(X ≈ Z), X0/Z}. Let γ = {Z/a}. Then, δγ = {R/λX.(X ≈ a), X0/a, Z/a} while θ = {R/λX.(X ≈ a), Z/a}. We see that δγ ⊇ θσ and dom(δγ − θσ) = {X0} (which is a template variable). Moreover, it holds Gθσ = Gδγ.\nBefore we derive the first completeness result, we need certain definitions and lemmas.\nDefinition 7.30 Let P be a program and let E be a positive expression or a goal clause. We define SE to be the set of all expressions that can be obtained from E by substituting zero or more occurrences of every predicate constant p in E with the expression E1 ∨ π · · · ∨ π Ek, where p ←π Ei are all the clauses 4 for p in P. Moreover, Ê ∈ SE is the expression obtained from E by substituting every predicate symbol occurrence with the corresponding expression.\nLemma 7.31 Let P be a program, E a positive expression or a goal clause, I a Herbrand\ninterpretation of P and s a Herbrand state. Then, [[E]]s(TP(I)) = [[Ê]]s(I).\nProof. The proof is by structural induction on E. Assume that E is a positive expression (the proof for the case of goal clause is similar). For the induction basis we need to consider the cases where E is an argument variable V, an individual constant c, a propositional constant (0, 1), or a predicate constant p. Except for the last one, all other cases are straightforward because the meaning of E is independent of TP(I) and I. For the last case assume that E1, . . . ,Ek are all the bodies of the rules defining p in P. By definition of the TP operator, it holds that [[p]]s(TP(I)) =⊔\n(p←πEi)∈P [[Ei]]s(I). Moreover, [[Ê]]s(I) = [[E1 ∨ π · · · ∨ π Ek]]s(I) = ⊔ (p←πEi)∈P [[Ei]]s(I). This\ncompletes the basis case. For the induction step, all cases are immediate.\nLemma 7.32 Let P a program, G,G′ goals and G′ ∈ SG. If G ′ θ→ H′ then G θ ։ H, where H′ ∈ SH.\nProof. The proof is by induction on the number m of top-level subexpressions of the goal G that are connected with the logical constant ∧. The basis case is for m = 1, ie., it applies to goal clauses G that do not contain a top-level ∧. Assume that G =← A. The cases we need to examine for A for the induction basis are the following: (p A1 · · ·An), (QA1, . . .An), ((λV.A′) A1 · · ·An), ((A ′ ∨ π A ′′) A1 · · ·An), ((A ′ ∧ π A ′′) A1 · · ·An), (A1 ≈ A2), and (∃VA). The only non-trivial case is A = (p A1 · · ·An), which we demonstrate. Assume that p is defined in P with a set of k rules with right-hand sides E1, . . . ,Ek. Let Ep = E1 ∨ π · · · ∨ π Ek. Since G =← (pA1 · · ·An), we have that G ′ =← (A′ A′1 · · ·A ′ n), with A ′ ∈ Sp,A ′ 1 ∈ SA1 , . . . ,A ′ n ∈ SAn . We distinguish three cases for A′:\n• A′ = p. Then G′ ǫ → H′, where H′ =← (Ej A ′ 1 · · ·A ′ n) for some j. We also have that G ǫ → H,\nwhere H =← (Ej A1 · · ·An). Obviously, it holds that H ′ ∈ SH.\n• A′ = Ep and Ep contains more than one disjunct. Then G ′ ǫ→ H′, where H′ =←\n(Ej A ′ 1 · · ·A ′ n). We also have that G ǫ → H, where H =← (Ej A1 · · ·An). Again, it holds that H′ ∈ SH.\n• A′ = Ep and Ep contains exactly one disjunct. Then this disjunct must be a lambda\nabstraction of the form (λV.A′′). This implies that G′ =← ((λV.A′′)A′1 · · ·A ′ n) and G\n′ ǫ→\nH′, where H′ =← (A′′{V/A′1}A ′ 2 · · ·A ′ n). On the other hand, G ǫ → H1, where H1 =← ((λV.A′′)A1 · · ·An), and H1 ǫ → H, where H =← (A′′{V/A1}A2 · · ·An). Therefore, G ǫ ։ H where H′ ∈ SH.\nThe above completes the proof for the basis case. For the induction step, the goal must be of the form G =← (A1 ∧ A2). Then, G ′ =← (A′1 ∧ A ′ 2) where A ′ 1 ∈ SA1 and A ′ 2 ∈ SA2 . Since G′ θ → H′, we conclude without loss of generality that A′1 θ → H′1 and H ′ =← (H′1 ∧ A ′ 2θ). By the\n4We may assume without loss of generality that each predicate symbol p that is used in P, has a definition in P: if no such definition exists, we can add to the program the clause p ←π E, where E is a basic expression corresponding to the basic element ⊥π.\ninduction hypothesis, since A′1 θ → H′1, we get that A1 θ ։ H1, where H ′ 1 ∈ SH1 . But then this easily implies that (A1 ∧ A2) θ ։ (H1 ∧ A2θ), ie., G θ ։ H, where H′ ∈ SH. This completes the proof for the induction step and the lemma.\nLemma 7.33 Let P be a program, G,G′ goals and G′ ∈ SG. If there exists an SLD-refutation for P ∪ {G′} using substitution θ, then there also exists an SLD-refutation for P ∪ {G} using the same substitution θ.\nProof. The proof is by induction on the length n of the refutation of P∪{G′}. The induction basis is for n = 1 and includes the following cases for G: (✷∧✷), (✷∨E1), (E1∨✷), (E1 ≈ E2), ((λV.✷)E) and (∃V✷). It can be easily verified that the lemma holds for all these cases.\nSuppose now that the result holds for n − 1. We demonstrate that it also holds for n. Let G′ = G′0,G ′ 1, . . . ,G ′ n be the derived goals of the SLD-refutation of G ′ using the sequence of substitutions θ1, . . . , θn. Since G ′ θ1→ G′1, by Lemma 7.32 there exists a goal G1 such that G θ1 ։ G1 and G ′ 1 ∈ SG1 . By the induction hypothesis, P ∪ {G1} has an SLD-refutation using θ2 · · · θn. It follows that P ∪ {G} also has an SLD-refutation using θ = θ1 · · · θn.\nCorollary 7.34 Let P be a program and G a goal. If there exists an SLD-refutation for\nP ∪ {Ĝ} using substitution θ, then there also exists an SLD-refutation for P ∪ {G} using the same substitution θ.\nLemma 7.35 Let P be a program and G =← A be a goal such that [[A]]s(⊥IP) = 1 for all Herbrand states s. Then, there exists an SLD-refutation for P ∪ {G} with computed answer equal to the identity substitution.\nThe proof of the lemma can be found in Appendix F.\nAs in the first-order case, we have various forms of completeness. We can now prove the analogue of a theorem due to Apt and van Emden (see [Apt90][Lemma 3.17] or [Llo87][Theorem 8.3]).\nTheorem 7.36 Let P be a program, G =← A a goal and assume that [[A]]s(MP) = 1 for all Herbrand states s. Then, there exists an SLD-refutation for P ∪ {G} with computed answer equal to the identity substitution.\nProof. We prove by induction on n that if [[A]]s(TP ↑ n) = 1 for all Herbrand states s, then P ∪ {G} has an SLD-refutation with computed answer equal to the identity substitution. For n = 0 the proof is a direct consequence of Lemma 7.35.\nNow suppose that the result holds for n− 1. For the induction step assume that [[A]]s(TP ↑ n) = 1 for all s. By Lemma 7.31, [[Â]]s(TP ↑ (n− 1)) = 1. By the induction hypothesis there exists an SLD-refutation for P∪{Ĝ} with computed answer equal to the identity substitution. Let θ be the composition of the substitutions that are used during the SLD-refutation of\nP∪{Ĝ}. By Corollary 7.34, P∪{G} also has an SLD-refutation using the same substitution θ. The restriction of θ to the free variables of G is equal to the restriction of θ to the free variables\nof Ĝ which is equal to the empty substitution. Therefore, P∪ {G} has an SLD-refutation with computed answer equal to the identity substitution.\nThe following theorem generalizes a result of Hill [H74] (see also [Apt90][Theorem 3.15]):\nTheorem 7.37 Let P be a program and G =← A a goal. Suppose that P∪{G} is unsatisfiable. Then, there exists an SLD-refutation of P ∪ {G}.\nProof. Since P ∪ {G} is unsatisfiable and since MP is a model of P, we conclude that [[G]]s(MP) = 0, for some state s. Therefore, [[A]]s(MP) = 1. By Lemma 7.15 we can construct a substitution θ such that [[Aθ]]s′(MP) = 1 for all states s\n′. By Theorem 7.36, there exists an SLD-refutation for P ∪ {Gθ}. By Lemma 7.28 there exists an SLD-refutation for P ∪ {G}.\nFinally, the following theorem is a generalization of Clark’s theorem [Cla79] (see also the more accessible [Apt90][Theorem 3.18]) for the higher-order case:\nTheorem 7.38 (Completeness) Let P be a program and G =← A a goal. For every correct answer θ for P∪ {G}, there exists an SLD-refutation for P∪ {G} with computed answer δ and a substitution γ such that Gθ = Gδγ.\nProof. Since θ is a correct answer for P ∪ {← A}, it follows that [[Aθ]]s(MP) = 1 for all Herbrand states s. By Theorem 7.36, P ∪ {Gθ} has an SLD-refutation with computed answer equal to the identity substitution. This means that if σ is the composition of the substitutions used in the refutation of P ∪ {Gθ}, then Gθσ = Gθ. By Lemma 7.28 there exists an SLDrefutation for P ∪ {G} using substitution δ′ such that for some substitution γ, Gδ′γ = Gθσ. Let δ be δ′ restricted to the variables in G. Then, it also holds that Gδ′γ = Gδγ, and therefore Gδγ = Gθσ = Gθ."
    }, {
      "heading" : "8 Related Work",
      "text" : "As already discussed in Section 1, research in higher-order logic programming can be categorized in two main streams: the extensional approaches and the intensional ones.\nWork on extensional higher-order logic programming is rather limited. Apart from the results of [Wad91]5, the only other work that has come to our attention is that of M. Bezem [Bez99, Bez01], who considers higher-order logic programming languages with syntax similar to that of [Wad91]. In [Bez01] a notion of extensionality is defined (called the extensional collapse) and it is demonstrated that many logic programs are extensional under this notion; however, this notion appears to differ from classical extensionality and has a more proof-theoretical flavor.\nOn the other hand, work on intensional higher-order logic programming is much more extended. The two main existing approaches in this area are represented by the languages λProlog and HiLog. Both systems have mature implementations and have been tested in various application domains. It should be noted that both λProlog and HiLog encourage a form of higher-order programming that extends in various ways the higher-order programming capabilities that are supported by functional programming languages. For a more detailed discussion on this issue, see [NM98][section 7.4].\nIn the rest of this section, we give a very brief presentation of certain characteristics of these two systems that are related to their intensional behavior (ie., characteristics that will help the reader further clarify the differences between the intensional and extensional approaches). A detailed discussion on the syntax, semantics, implementation and applications of the two languages, is outside the scope of this paper (and the interested reader can consult the relevant bibliography).\n5The work in [Wad91] has also been used in order to define a higher-order extension of Datalog [KRW05].\nλProlog: The language was initially designed in the late 1980s [MN86, Nad87, NM90] in order to provide a proof theoretic basis for logic programming. The syntax of λProlog is based on the intuitionistic theory of higher-order hereditary Harrop formulas. The resulting language is a powerful one, that allows the programmer to quantify over function and predicate variables, to use λ-abstractions in terms, and so on. The semantics of λProlog is not extensional (see for example the discussion in [NM98]). The following simple example illustrates this idea.\nExample 8.1 Consider the λProlog program (we omit type declarations):\nr p. p X :- q X. q X :- p X.\nThe goal ?-r q. fails for the above program.\nIn the following we briefly discuss the behavior of λProlog with respect to queries that contain uninstantiated higher-order variables (because, from a user point of view, this is a key concept that can differentiate an intensional system from an extensional one). Consider for example the query:\n?-(R john mary).\nThe above goal is not a meaningful one for λProlog because there exist too many suitable answer substitutions (ie., predicate terms) for R that one could think of (see the relevant discussion in [NM98][page 50]). One way to get answers to such queries is to use an extra predicate, say rel, which specifies a collection of predicate terms that are relevant to consider as substitutions. In this case, the above query will be written as:\n?-(rel R),(R john mary).\nand the implementation will return those terms for which the query succeeds. In other words, an answer for the above query is a predicate term (such as for example married, father, a lambda expression, and so on). As remarked in [CKW93], “equality in (a fragment of) λProlog corresponds to λ-equivalence and is not extensional: there may exist predicates that are not λ-equivalent but still extensionally equal”. This is a key difference from the extensional approach presented in this paper, in which an answer to a query is a set.\nHiLog: The language possesses a higher-order syntax and a first-order semantics [CKW89, CKW93]. It extends classical logic programming quite naturally, and allows the programmer to write in a concise way programs that would be rather awkward to code in Prolog. It has been used in various application domains (eg. deductive and object-oriented databases, modular logic programming, and so on).\nAs it is the case with λProlog, HiLog is also an intensional higher-order language. The examples with uninstantiated higher-order variables mentioned for λProlog have a somewhat similar behavior in HiLog. However, consider the program:\nmarried(john,mary).\nThen, the query: ?-R(john,mary).\nis a meaningful one for HiLog, and the interpreter will respond with R = married. Intuitively, the interpreter searches the program for possible candidate relations and tests them one by one. Of course, if there is no binary relation defined in the program, the above query will fail.\nThe above program behavior can be best explained by the following comment from [CKW93]: “in HiLog predicates and other higher-order syntactic objects are not equal unless they (ie., their names) are equated explicitly”."
    }, {
      "heading" : "9 Implementation and Future Work",
      "text" : "A prototype implementation of the proposed proof procedure has been performed in Haskell6. A detailed description of the implementation is outside the scope of this paper. However, in the following we outline certain points that we feel are important.\nThe main difference in comparison to a first-order implementation, is that the proof procedure has to generate an infinite (yet enumerable) number of basic templates. In order to make more efficient the production of the basic templates, one main optimization has been adopted. As we have already mentioned in Definition 7.3, a basic template is a non-empty finite union of basic expressions of a particularly simple form. In the implementation, the members of this union are generated in a “demand-driven way”, as the following examples illustrate.\nExample 9.1 Consider the query ← (R a b),(R c d). The proposed proof procedure would try some basic templates until it finds one that satisfies the query. However, if it first tries the basic template (λX.λY.(X≈Z)∧(Y≈W)) then this will obviously not lead to an answer (since a relation that satisfies the above query must contain at least two pairs of elements). In order to avoid such cases, our implementation initially produces a basic expression that consists of the union of a basic template with an uninstantiated variable (say L) of the same type as the template; intuitively, L represents a (yet undetermined) set of basic templates that may be needed later during resolution and which need not yet be explicitly generated. In our example, the implementation starts with the production of an expression of the form (λX.λY.(X≈Z)∧(Y≈W)) ∨ L. When the second application in the goal is reached, then a second basic template will be generated together with a new uninstantiated variable (say L1). The final answer to the query will be an expression of the form: (λX.λY.(X≈a)∧(Y≈b)) ∨ (λX.λY.(X≈c)∧(Y≈d)) ∨\nL1. The intuitive meaning of the above answer is that the query is satisfied by all relations that contain at least the pairs (a, b) and (c, d).\nNotice that an important practical advantage of the above optimization is that a unique answer to the given query is generated. Notice also that if the formal proof procedure of the previous sections was followed faithfully in the implementation, then an infinite number of answers would be generated: an answer representing the two-element relation {(a, b), (c, d)}, an answer representing all three-element relations {(a, b), (c, d), (X1, X2)}, an answer representing the four-element relations {(a, b), (c, d), (X1, X2), (X3, X4)}, and so on.\nExample 9.2 Consider the ordered predicate of Example 3.7 and let ← ordered R [1,2,3] be a query. Following the same ideas as in the previous example, the implementation will produce the unique answer (λX.λY.(X≈1)∧(Y≈2)) ∨ (λX.λY.(X≈2)∧(Y≈3)) ∨ L. Intuitively, this answer states that the list [1,2,3] is ordered under any relation of the form {(1, 2), (2, 3)} ∪ L.\nFinally, consider Example 3.6 defining the closure predicate. Consider also the query ← closure Q a b. Then, the implementation will enumerate the following (infinite set of) answers:\nQ = (λX.λY.(X≈a)∧(Y≈b)) ∨ L Q = (λX.λY.(X≈a)∧(Y≈Z)) ∨ (λX.λY.(X≈Z)∧(Y≈b)) ∨ L\n. . .\nwhich intuitively correspond to relations of the following forms:\nQ = {(a, b)} ∪ L Q = {(a, Z), (Z, b)} ∪ L\n. . .\n6The code can be retrieved from http://code.haskell.org/hopes\nIntuitively, the above answers state that the pair (a, b) belongs to the transitive closure of all relations that contain at least the pair (a, b); moreover, it also belongs to the transitive closure of all relations that contain at least two pairs of the form (a, Z) and (Z, b) for any Z, and so on.\nWe are currently considering issues regarding an extended WAM-based implementation of the ideas presented in the paper. We believe that ideas originating from graph-reduction [FH88] will also prove vital in the development of this extended implementation.\nAnother interesting direction for future research is the extension of our higher-order fragment with negation-as-failure. The semantics of negation in a higher-order setting could probably be captured model-theoretically using the recent purely logical characterization of the well-founded semantics through an appropriate infinite-valued logic [RW05].\nAcknowledgments: We would like to thank Costas Koutras for valuable discussions regarding algebraic lattices and the reading group on programming languages at the University of Athens for many insightful comments and suggestions."
    }, {
      "heading" : "A Proof of Lemma 4.17",
      "text" : "In order to establish Lemma 4.17, we first demonstrate the following auxiliary propositions:\nProposition A.1 Let A be a poset and L be an algebraic lattice. Then, for each step function (a ց c) and for every f : [A m → L] it holds that (a ց c) ⊑ f if and only if c ⊑ f(a).\nProof. If (a ց c) ⊑ f , by applying both functions to a we get c ⊑ f(a). Now suppose that c ⊑ f(a) and consider an arbitrary x ∈ A. In case a ⊑ x, we have (a ց c)(x) = c thus, since c ⊑ f(a) and f is monotonic, (a ց c)(x) ⊑ f(x). Otherwise, (a ց c)(x) =⊥L thus (a ց c)(x) ⊑ f(x). It follows that (a ց c) ⊑ f .\nProposition A.2 Let L be a complete lattice and assume there exists B ⊆ K(L) such that for every x ∈ L, x = ⊔ B[x]. Then L is an algebraic lattice (ω-algebraic if B is countable) with\nbasis K(L) = { ⊔ M | M is a finite subset of B}.\nProof. It is immediate that L is algebraic, since by assumption every element of L can be written as the least upper bound of a set of compact elements of L. The nontrivial part is establishing the relation between K(L) and B.\nGiven x ∈ L, we let ∆(x) be the set { ⊔\nM | M is a finite subset of B[x]}. Notice that B[x] = ⋃ {M | M is a finite subset of B[x]}. Using Proposition 4.3(2), we have that ⊔ B[x] =\n⊔ ∆(x) and thus ⊔ ∆(x) = x. We show that for each x ∈ L it holds that K(L)[x] = ∆(x) by proving that each set is a subset of the other one.\nFirst consider an arbitrary c ∈ K(L)[x] and recall that c = ⊔\n∆(c). By the compactness of c, there exists a finite A ⊆ ∆(c) such that c ⊑ ⊔ A. But then ⊔ A ⊑ c because c is an upper bound of ∆(c), and therefore c = ⊔\nA. By the definition of ∆(c) and the fact that A ⊆ ∆(c), we get that c = ⊔ { ⊔ M1, . . . , ⊔\nMr}, where M1, . . . ,Mr are finite subsets of B[c]. By Proposition 4.3(2), c = ⊔ (M1 ∪ · · · ∪Mr). In other words there exists a finite set\nM = M1 ∪ · · · ∪Mr such that M ⊆ B[c] ⊆ B[x] and c = ⊔ M , which means that c ∈ ∆(x).\nOn the other hand, consider a finite set M = {c1, . . . , cn} ⊆ B[x] such that ⊔\nM ∈ ∆(x). Let A be a subset of L such that ⊔ M ⊑ ⊔ A. Due to the compactness of each ci, by\nci ⊑ ⊔ A we get ci ⊑ ⊔ Ai for some finite Ai ⊆ A. But then, for every i, ci ⊑ ⊔\nAi ⊑⊔ { ⊔ A1, . . . , ⊔ An} = ⊔ (A1 ∪ · · · ∪ An). In other words, ⊔ M ⊑ ⊔ (A1 ∪ · · · ∪ An), which\nimplies that ⊔\nM is compact. Moreover, since x is an upper bound of M , we have that⊔ M ∈ K(L)[x]. Hence, K(L)[x] = ∆(x).\nTo complete the proof, simply take x = ⊔\nL in the equality K(L)[x] = ∆(x). If, additionally, B is countable, the cardinality of K(L) is bounded by the number of finite subsets of a countable set, which is countable. Hence, L is an ω-algebraic lattice in this case.\nWe can now proceed to the proof of Lemma 4.17:\nLemma 4.17 Let A be a poset and L be an algebraic lattice. Then, [A m → L] is an algebraic lattice whose basis is the set of all least upper bounds of finitely many step functions from A to L. If, additionally, A is countable and L is an ω-algebraic lattice then [A m → L] is an ω-algebraic lattice.\nProof. Let B denote the set of all step functions from A to L. Recall that [A m → L] forms a complete lattice by Proposition 4.10. Let (a ց c) ∈ B be an arbitrary step function. We show that (a ց c) is compact. Consider a set F of monotonic functions from A to L such that (a ց c) ⊑ ⊔ F . By Propositions A.1 and 4.10 we get that c ⊑ ⊔ f∈F f(a). By the compactness\nof c, there exists a finite F ′ ⊆ F such that c ⊑ ⊔\nf∈F ′ f(a). Let f ′ =\n⊔ F ′. Then, c ⊑ f ′(a), or\nequivalently by Proposition A.1, (a ց c) ⊑ f ′ = ⊔ F ′. Hence, (a ց c) is compact.\nWe now show that every monotonic function f ∈ [A m → L] is the least upper bound of B[f ]. Since f is an upper bound of this set, we let g be an upper bound of B[f ] and prove that f ⊑ g. In fact, we consider an arbitrary x ∈ A and prove that f(x) ⊑ g(x). Suppose Sx is the set of all step functions hc = (x ց c) for every compact element c ∈ K(L)[f(x)]. By Proposition A.1, we have that for all step functions hc ∈ Sx, are hc ⊑ f ; thus Sx is a subset of B[f ]. Since g is an upper bound of B[f ], it must also be an upper bound of Sx, therefore it holds that hc ⊑ g for each hc ∈ Sx. Applying this inequality for x we get that c ⊑ g(x) for each c ∈ K(L)[f(x)], therefore ⊔ K(L)[f(x)] ⊑ g(x). Since L is an algebraic lattice, f(x) is the least upper bound of K(L)[f(x)], thus f(x) ⊑ g(x). Hence, f is the least upper bound of B[f ].\nOn the whole, we have shown that B is a subset of K([A m → L]) such that each monotonic function f from A to L is the least upper bound of B[f ]. Notice that if, additionally, A is countable and L is an ω-algebraic lattice, then B is countable because its cardinality is equal to that of the cartesian product of two countable sets. Now apply Proposition A.2."
    }, {
      "heading" : "B Proof of Lemma 5.10",
      "text" : "Lemma 5.10 Let E : ρ be an expression of H and let D be a nonempty set. Moreover, let s, s1, s2 be states over D and let I be an interpretation over D. Then:\n1. [[E]]s(I) ∈ [[ρ]]D.\n2. If E is positive and s1 ⊑SH,D s2 then [[E]]s1(I) ⊑ρ [[E]]s2(I).\nProof. The two statements are established simultaneously by a structural induction on E.\nInduction Basis: The cases for E being 0, 1, c, p or V, are all straightforward.\nInduction Step: The interesting cases are E = (E1E2) and E = (λV.E1). The other cases are easier and omitted.\nCase 1: E = (E1E2). We examine the two statements of the lemma:\nStatement 1: Assume that E1 : ρ1 → π2 and E2 : ρ1. Then, it suffices to demonstrate that [[(E1E2)]]s(I) ∈ [[π2]]D, or equivalently that ⊔ b∈B([[E1]]s(I)(b)) ∈ [[π2]]D, where B = FD(type(E2))[[[E2]]s(I)] = {b ∈ FD(type(E2)) | b ⊑ [[E2]]s(I)}. By the induction hypothesis, [[E1]]s(I) ∈ [[ρ1 → π2]]D and [[E2]]s(I) ∈ [[ρ1]]D. But then, for every b ∈ B, [[E1]]s(I)(b) ∈ [[π2]]D and since [[π2]]D is a complete lattice, we get that ⊔ b∈B([[E1]]s(I)(b)) ∈ [[π2]]D.\nStatement 2: It suffices to demonstrate that [[(E1E2)]]s1(I) ⊑ [[(E1E2)]]s2(I), or equivalently\nthat ⊔\nb2∈B2 ([[E1]]s1(I)(b2)) ⊑ ⊔ b′ 2 ∈B′ 2\n([[E1]]s2(I)(b ′ 2)), where B2 = FD(type(E2))[[[E2]]s1 (I)] and\nB′2 = FD(type(E2))[[[E2]]s2 (I)] . Notice that by definition, B2 = {b ∈ FD(type(E2)) | b ⊑ [[E2]]s1(I)} and B ′ 2 = {b ∈ FD(type(E2)) | b ⊑ [[E2]]s2(I)}. By the induction hypothesis we have [[E2]]s1(I) ⊑ [[E2]]s2(I), and therefore B2 ⊆ B ′ 2. By the induction hypothesis we also have that [[E1]]s1(I) ⊑ [[E1]]s2(I). By the induction hypothesis for the first statement of the lemma, both [[E1]]s1(I) and [[E1]]s2(I) are monotonic functions since they belong to [[ρ1 → π2]]D. Therefore,⊔\nb2∈B2 ([[E1]]s1(I)(b2)) ⊑ ⊔ b′ 2 ∈B′ 2 ([[E1]]s2(I)(b ′ 2)), or equivalently [[(E1E2)]]s1(I) ⊑ [[(E1E2)]]s2(I).\nCase 2: E = (λV.E1). We examine the two statements of the lemma:\nStatement 1: Assume that V : ρ1 and E1 : π1. We show that [[(λV.E1)]]s(I) ∈ [[ρ1 → π1]]D. We distinguish two cases, namely ρ1 = ι and ρ1 = π. If ρ1 = ι then the result follows easily using the induction hypothesis for the first statement of the lemma. If ρ1 = π, then we must demonstrate that [[(λV.E1)]]s(I) ∈ [[π → π1]]D = [K([[π]]D) m → [[π1]]D]. In other words, we need to show that the function λd.[[E1]]s[d/V](I) is monotonic. But this follows directly from the induction hypothesis for the second statement of the lemma.\nStatement 2: It suffices to show that [[(λV.E1)]]s1(I) ⊑ [[(λV.E1)]]s2(I). By the semantics of lambda abstraction, it suffices to show that λd.[[E1]]s1[d/V](I) ⊑ λd.[[E1]]s2[d/V](I), or that for every d, [[E1]]s1[d/V](I) ⊑ [[E1]]s2[d/V](I), which holds by the induction hypothesis."
    }, {
      "heading" : "C Proof of Lemma 6.5",
      "text" : "Lemma 6.5 Let P be a program and let E : ρ be a positive expression of P. Let I, J be Herbrand interpretations and s a Herbrand state of P . If I ⊑IP J then [[E]]s(I) ⊑ρ [[E]]s(J).\nProof. The proof is by a structural induction on E.\nInduction Basis: The cases for E being 0, 1, c, p or V, are all straightforward.\nInduction Step: The interesting cases are E = (E1E2) and E = (λV.E1). The other cases are easier and omitted.\nCase 1: E = (E1E2). It suffices to demonstrate that [[(E1E2)]]s(I) ⊑ [[(E1E2)]]s(J), or equivalently that ⊔ b2∈B2 ([[E1]]s(I)(b2)) ⊑ ⊔ b′ 2 ∈B′ 2 ([[E1]]s(J)(b ′ 2)), where B2 = FD(type(E2))[[[E2]]s(I)] and B′2 = FD(type(E2))[[[E2]]s(J)] . Notice that by definition, B2 = {b ∈ FD(type(E2)) | b ⊑ [[E2]]s(I)} and B ′ 2 = {b ∈ FD(type(E2)) | b ⊑ [[E2]]s(J)}. By the induction hypothesis we have [[E2]]s(I) ⊑ [[E2]]s(J), and therefore B2 ⊆ B ′ 2. By the induction hypothesis we also have that\n[[E1]]s(I) ⊑ [[E1]]s(J). Therefore, ⊔ b2∈B2 ([[E1]]s(I)(b2)) ⊑ ⊔ b′ 2 ∈B′ 2 ([[E1]]s(J)(b ′ 2)), or equivalently [[(E1E2)]]s(I) ⊑ [[(E1E2)]]s(J).\nCase 2: E = (λV.E1). It suffices to show that [[(λV.E1)]]s(I) ⊑ [[(λV.E1)]]s(J). By the semantics of lambda abstraction, it suffices to show that λd.[[E1]]s[d/V](I) ⊑ λd.[[E1]]s[d/V](J), or that for every d, [[E1]]s[d/V](I) ⊑ [[E1]]s[d/V](J), which holds by the induction hypothesis."
    }, {
      "heading" : "D Proof of Lemma 6.6",
      "text" : "Lemma 6.6 Let P be a program and let E be any positive expression of P. Let I be a directed set of Herbrand interpretations and s be a Herbrand state of P. Then, [[E]]s( ⊔ I) =⊔\nI∈I [[E]]s(I).\nProof. The proof can be performed in two steps: we first show that [[E]]s( ⊔ I) ⊒ ⊔\nI∈I [[E]]s(I) and then that [[E]]s( ⊔ I) ⊑ ⊔ I∈I [[E]]s(I).\nFor the first of these two statements observe that by Lemma 6.5, we have that [[E]]s( ⊔\nI) ⊒ [[E]]s(I), for all I ∈ I. But then [[E]]s( ⊔ I) is an upper bound of the set {[[E]]s(I) | I ∈ I}, and\ntherefore [[E]]s( ⊔ I) ⊒ ⊔ I∈I [[E]]s(I). It remains to show that [[E]]s( ⊔ I) ⊑ ⊔\nI∈I [[E]]s(I). The proof is by a structural induction on E.\nInduction Basis: The cases for E being 0, 1, c, p or V, are all straightforward.\nInduction Hypothesis: Assume that for given expressions E1,E2 it holds that [[Ei]]s( ⊔\nI) =⊔ I∈I [[Ei]]s(I), i ∈ {1, 2}. Notice that we assume equality. This is due to the fact that the one direction has already been established for all expressions while the other direction is assumed.\nInduction Step: We distinguish the following cases:\nCase 1: E = f E1 · · · En. This case is straightforward since for every interpretation I and for every state s, the value of [[f E1 · · ·En]]s(I) only depends on s (since the expressions E1, . . . ,En are of type ι and do not contain predicate symbols).\nCase 2: E = (E1E2). Assume that E2 : ρ. Then:\n[[(E1E2)]]s( ⊔ I) =\n= ⊔ b∈B([[E1]]s( ⊔ I)(b)), where B = {b ∈ FD(ρ) | b ⊑ [[E2]]s( ⊔\nI)} (Semantics of application)\n= ⊔ b∈B(( ⊔ I∈I [[E1]]s(I))(b)), where B = {b ∈ FD(ρ) | b ⊑ [[E2]]s( ⊔\nI)} (Induction hypothesis)\n= ⊔ b∈B( ⊔ I∈I [[E1]]s(I)(b)), where B = {b ∈ FD(ρ) | b ⊑ [[E2]]s( ⊔\nI)} (Proposition 4.10)\n= ⊔ {[[E1]]s(I)(b) | I ∈ I, b ∈ FD(ρ), b ⊑ [[E2]]s( ⊔ I)}\n(Proposition 4.3(2))\n= ⊔ {[[E1]]s(I)(b) | I ∈ I, b ∈ FD(ρ), b ⊑ ⊔ I∈I [[E2]]s(I)}\n(Induction hypothesis)\n= ⊔ {[[E1]]s(I)(b) | I ∈ I, b ∈ FD(ρ), b ⊑ ⊔ J∈F [[E2]]s(J), F finite subset of I}\n(Since b is either a compact element or a member of D)\n⊑ ⊔ {[[E1]]s(I)(b) | I ∈ I, b ∈ FD(ρ), b ⊑ [[E2]]s(J)}, for some J ∈ I\n(Because I is directed and [[E2]]s is monotonic by Lemma 6.5)\n⊑ ⊔ {[[E1]]s(I)(b) | I ∈ I, J ∈ I, b ∈ FD(ρ), b ⊑ [[E2]]s(J)}\n(Proposition 4.3(1))\n⊑ ⊔\nI∈I,J∈I ⊔ {[[E1]]s(I)(b) | b ∈ FD(ρ), b ⊑ [[E2]]s(J)}\n(Proposition 4.3(2))\n⊑ ⊔\nI∈I ⊔ {[[E1]]s(I)(b) | b ∈ FD(ρ), b ⊑ [[E2]]s(I)}\n(Proposition 4.7)\n= ⊔\nI∈I [[(E1E2)]]s(I) (Semantics of application)\nCase 3: E = (λV.E1). We show that [[(λV.E1)]]s( ⊔ I) ⊑ ⊔\nI∈I [[(λV.E1)]]s(I). Consider b ∈ FD(type(V)). By the semantics of lambda abstraction we get that [[(λV.E1)]]s( ⊔ I)(b) =\n[[E1]]s[b/V]( ⊔ I); by the induction hypothesis this is equal to ⊔ I∈I [[E1]]s[b/V](I), which by Propo-\nsition 4.10 is equal to ( ⊔\nI∈I [[(λV.E1)]]s(I))(b).\nCase 4: E = (E1 ∨ π E2). We show that [[(E1 ∨ π E2)]]s( ⊔ I) ⊑ ⊔ I∈I [[(E1 ∨\nπ E2)]]s(I), ie., that for all b1, . . . , bn, if [[(E1 ∨ π E2)]]s( ⊔ I) b1 · · · bn = 1 then ( ⊔ I∈I [[(E1 ∨ π E2)]]s(I)) b1 · · · bn = 1.\nBy the semantics of ∨ π we get that if [[(E1 ∨ π E2)]]s( ⊔ I) b1 · · · bn = 1 then [[E1]]s( ⊔\nI) b1 · · · bn = 1 or [[E2]]s( ⊔ I) b1 · · · bn = 1. By the induction hypothesis and Proposition 4.10 we get that\neither ⊔ I∈I([[E1]]s(I) b1 · · · bn) = 1 or ⊔\nI∈I([[E2]]s(I) b1 · · · bn) = 1. Then there must exist I ∈ I such that either [[E1]]s(I) b1 · · · bn = 1 or [[E2]]s(I) b1 · · · bn = 1. By the semantics of ∨ π we get\nthat [[(E1 ∨ π E2)]]s(I) b1 · · · bn = 1 and therefore ( ⊔ I∈I [[(E1 ∨ π E2)]]s(I)) b1 · · · bn = 1. Case 5: E = (E1 ∧ π E2). We show that [[(E1 ∧ π E2)]]s( ⊔ I) ⊑ ⊔ I∈I [[(E1 ∧\nπ E2)]]s(I). In other words, it suffices to show that for all b1, . . . , bn, if [[(E1 ∧ π E2)]]s( ⊔ I) b1 · · · bn = 1\nthen ( ⊔ I∈I [[(E1 ∧ π E2)]]s(I)) b1 · · · bn = 1. But if [[(E1 ∧ π E2)]]s( ⊔ I) b1 · · · bn = 1, then by\nthe semantics of ∧ π we get that [[E1]]s( ⊔ I) b1 · · · bn = 1 and [[E2]]s( ⊔\nI) b1 · · · bn = 1. By the induction hypothesis and Proposition 4.10 this implies that ⊔ I∈I([[E1]]s(I) b1 · · · bn) = 1\nand ⊔\nI∈I([[E2]]s(I) b1 · · · bn) = 1. This means that there must exist I1, I2 ∈ I such that [[E1]]s(I1) b1 · · · bn = 1 and [[E2]]s(I2) b1 · · · bn = 1. Since I is directed, we get that I = ⊔ {I1, I2} exists in I and it holds that [[E1]]s(I) b1 · · · bn = 1 and [[E2]]s(I) b1 · · · bn = 1. By the semantics of ∧ π, [[(E1 ∧ π E2)]]s(I) b1 · · · bn = 1 and therefore ( ⊔ I∈I [[(E1 ∧ π E2)]]s(I)) b1 · · · bn = 1. Case 6: E = (E1 ≈ E2). It suffices to show that [[(E1 ≈ E2)]]s( ⊔ I) ⊑ ⊔\nI∈I [[(E1 ≈ E2)]]s(I). This is straightforward since the value of [[(E1 ≈ E2)]] only depends on s (since the expressions E1,E2 do not contain predicate symbols). Case 7: E = (∃VE1). We show that [[(∃E1)]]s( ⊔ I) ⊑ ⊔\nI∈I [[(∃VE1)]]s(I) or equivalently that if [[(∃VE1)]]s( ⊔ I) = 1 then ⊔ I∈I [[(∃VE1)]]s(I) = 1. Notice now that if [[(∃VE1)]]s( ⊔ I) = 1\nthen there exists b such that [[E1]]s[b/V]( ⊔\nI) = 1, which by the induction hypothesis gives⊔ I∈I [[E1]]s[b/V](I) = 1. But this last statement implies that ⊔ I∈I [[(∃VE1)]]s(I) = 1."
    }, {
      "heading" : "E Proof of Lemma 7.27",
      "text" : "Lemma 7.27 Let P be a program, G a goal and θ a substitution. Suppose that there exists an SLD-refutation of P ∪ {Gθ} using substitution σ. Then, there exists an SLD-refutation of P ∪ {G} using a substitution δ, where for some substitution γ it holds that δγ ⊇ θσ and dom(δγ − θσ) is a (possibly empty) set of template variables that are introduced during the refutation of P ∪ {G}.\nProof. The proof is by induction on the length n of the SLD-refutation of P ∪ {Gθ}.\nInduction Basis: The basis case is for n = 1. We need to distinguish cases based on the structure of G. The most interesting case is G = (E1 ≈ E2) (the rest are simpler and omitted). By assumption, it holds that (E1θ ≈ E2θ) σ → ✷, where σ is an mgu of E1θ and E2θ. But then we also have that (E1 ≈ E2) δ → ✷, where δ is an mgu of E1 and E2. Since θσ is a unifier of E1,E2, there exists substitution γ such that θσ = δγ.\nInduction Step: We demonstrate the statement for SLD-refutations of length n + 1. We distinguish cases based on the structure of G.\nCase 1: G =← (pE1 · · ·Ek). Then, Gθ =← (p (E1θ) · · · (Ekθ)). By Definition 7.18 we get that p (E1θ) · · · (Ekθ) ǫ → E (E1θ) · · · (Ekθ), where p ← E is a rule in P. By assumption, E (E1θ) · · · (Ekθ) has an SLD-refutation of length n using σ. Consider now the goal G. By Definition 7.18, we get that (pE1 · · · Ek) ǫ → (EE1 · · ·Ek). Notice now that since E is a closed lambda expression, it holds that (EE1 · · · Ek)θ = (E (E1θ) · · · (Ekθ)). Moreover, since (E (E1θ) · · · (Ekθ)) has an SLD-refutation of length n using σ, we get by the induction hypothesis that (EE1 · · ·Ek) has an SLD-refutation using substitution δ, where for some substitution γ it holds that δγ ⊇ θσ and dom(δγ − θσ) is a set of template variables that are introduced during the refutation of (EE1 · · · Ek). But then, (pE1 · · · Ek) has an SLD-refutation which satisfies the requirements of the lemma.\nCase 2: G =← (QE1 · · ·Ek). Consider first the case where θ(Q) = B, for some basic expression B. Then, Gθ =← (B (E1θ) · · · (Ekθ)). Notice now that B can be either a higher-order predicate variable or a finite-union of lambda abstractions. We examine the case where B is a single lambda abstraction (the other two cases are similar). Since B is a lambda abstraction, assume that B = λV.C. By Definition 7.18 we get that B (E1θ) · · · (Ekθ) ǫ → C{V/(E1θ)} (E2θ) · · · (Ekθ).\nBy assumption, C{V/(E1θ)} (E2θ) · · · (Ekθ) has an SLD-refutation of length n using σ. Consider now the goal G. By Definition 7.18, we get that (QE1 · · ·Ek) {Q/Bt} → Bt (E1{Q/Bt}) · · · (Ek{Q/Bt}), where Bt = λV.Ct, and B = Btγ1, for some substitution γ1 with dom(γ1) = FV (Bt). We assume without loss of generality that the set dom(γ1) is disjoint from FV (G) and from dom(θ) ∪ FV (range(θ)). By Definition 7.18 we get that Bt (E1{Q/Bt}) · · · (Ek{Q/Bt}) ǫ → Ct{V/E1{Q/Bt}} (E2{Q/Bt}) · · · (Ek{Q/Bt}). Notice now that:\n((Ct{V/E1{Q/Bt}}) (E2{Q/Bt}) · · · (Ek{Q/Bt}))θγ1 = (C{V/E1θ}) (E2θ) · · · (Ekθ)\nThen, since (C{V/E1θ}) (E2θ) · · · (Ekθ) has an SLD-refutation of length n using σ, we get by the induction hypothesis that (Ct{V/E1{Q/Bt}}) (E2{Q/Bt}) · · · (Ek{Q/Bt}) has an SLDrefutation using substitution δ′, where for some substitution γ it holds δ′γ ⊇ θγ1σ and dom(δ′γ− θγ1σ) is a set of template variables that are introduced during this SLD-refutation. From the above discussion we conclude that (QE1 · · · Ek) has an SLD-refutation using substitution δ = {Q/Bt}δ ′. Moreover, it holds that δγ = {Q/Bt}δ ′γ ⊇ {Q/Bt}θγ1σ ⊇ θσ and dom(δγ − θσ) is a set of template variables that are introduced during the refutation of (QE1 · · ·Ek).\nConsider now the case where θ(Q) is undefined. Then, Gθ =← (Q (E1θ) · · · (Ekθ)). By Definition 7.18 we get that Q (E1θ) · · · (Ekθ) {Q/Bt} → Bt (E1θ{Q/Bt}) · · · (Ekθ{Q/Bt}). We may assume without loss of generality that the set FV (Bt) is disjoint from FV (G) and from dom(θ)∪ FV (range(θ)). By assumption, Bt (E1θ{Q/Bt}) · · · (Ekθ{Q/Bt}) has an SLD-refutation of length n using σ′, where σ = {Q/Bt}σ ′. Consider now the goal G. By Definition 7.18, we get that (QE1 · · · Ek) {Q/Bt} → Bt (E1{Q/Bt}) · · · (Ek{Q/Bt}). Notice now that:\n(Bt (E1{Q/Bt}) · · · (Ek{Q/Bt}))θ{Q/Bt} = Bt (E1θ{Q/Bt}) · · · (Ekθ{Q/Bt})\nThen, since Bt (E1θ{Q/Bt}) · · · (Ekθ{Q/Bt}) has an SLD-refutation of length n using σ ′, we get by the induction hypothesis that Bt (E1{Q/Bt}) · · · (Ek{Q/Bt}) has an SLD-refutation using substitution δ′, where for some substitution γ it holds δ′γ ⊇ θ{Q/Bt}σ\n′ and dom(δ′γ − θ{Q/Bt}σ\n′) is a set of template variables that are introduced during this SLD-refutation; notice that these template variables can be chosen to be different than the variables in FV (Bt). From the above discussion we conclude that (QE1 · · ·Ek) has an SLD-refutation using substitution δ = {Q/Bt}δ ′. Moreover, it holds that δγ = {Q/Bt}δ ′γ ⊇ {Q/Bt}θ{Q/Bt}σ ′ = θ{Q/Bt}σ ′ = θσ and dom(δγ − θσ) is a set of template variables that are introduced during the refutation of (QE1 · · ·Ek).\nCase 3: G =← ((λV.E)E1 · · ·Ek). Then, Gθ =← ((λV.Eθ) (E1θ) · · · (Ekθ)). By Definition 7.18 we get that (λV.Eθ) (E1θ) · · · (Ekθ) ǫ → (Eθ{V/(E1θ)}) (E2θ) · · · (Ekθ). Moreover, by assumption, (Eθ{V/(E1θ)}) (E2θ) · · · (Ekθ) has an SLD-refutation of length n using σ. Consider now the goal G. By Definition 7.18, we get that (λV.E)E1 · · ·Ek ǫ → (E{V/E1})E2 · · ·Ek. Notice now that ((E{V/E1})E2 · · ·Ek)θ = (Eθ{V/(E1θ)}) (E2θ) · · · (Ekθ), and since the latter expression has an SLD-refutation of length n using σ, we get by the induction hypothesis that (E{V/E1})E2 · · ·Ek has an SLD-refutation using a substitution δ, where for some substitution γ it holds δγ ⊇ θσ and dom(δγ − θσ) is a set of template variables that are introduced during this refutation. But then, ((λV.E)E1 · · ·Ek) has an SLD-refutation using substitution δ which satisfies the requirements of the lemma. Case 4: G =← ((E′ ∨\nπ E ′′)E1 · · · Ek). Then, Gθ =← ((E\n′θ ∨\nπ E ′′θ) (E1θ) · · · (Ekθ)). By Defini-\ntion 7.18 we get that (E′θ ∨\nπ E ′′θ) (E1θ) · · · (Ekθ) ǫ → (E′θ) (E1θ) · · · (Ekθ) (and symmetrically for\nE′′). By assumption, either (E′θ) (E1θ) · · · (Ekθ) or (E ′′θ) (E1θ) · · · (Ekθ) has an SLD-refutation of length n using σ. Assume, without loss of generality, that (E′θ) (E1θ) · · · (Ekθ) has an SLDrefutation of length n using σ. Consider now the goal G. By Definition 7.18, we get that\n(E′ ∨\nπ E ′′)E1 · · ·Ek ǫ → E′ E1 · · · Ek. Notice now that (E ′ E1 · · ·Ek)θ = (E ′θ) (E1θ) · · · (Ekθ), and\nsince the latter expression has an SLD-refutation of length n using σ, we get by the induction hypothesis that E′ E1 · · ·Ek has an SLD-refutation using a substitution δ, where for some substitution γ it holds δγ ⊇ θσ and dom(δγ−θσ) is a set of template variables that are introduced during this refutation. But then, (E′ ∨ π E\n′′)E1 · · · Ek has an SLD-refutation using substitution δ which satisfies the requirements of the lemma. Case 5: G =← ((E′ ∧\nπ E ′′)E1 · · · Ek). Then, Gθ =← ((E\n′θ ∧\nπ E ′′θ) (E1θ) · · · (Ekθ)). By Defini-\ntion 7.18 we get (E′θ ∨\nπ E ′′θ) (E1θ) · · · (Ekθ) ǫ → ((E′θ) (E1θ) · · · (Ekθ)) ∧ ((E ′′θ) (E1θ) · · · (Ekθ)). By assumption, ((E′θ) (E1θ) · · · (Ekθ))∧ ((E ′′θ) (E1θ) · · · (Ekθ)) has an SLD-refutation of length n using σ. Consider now the goal G. By Definition 7.18, we get that (E′ ∧\nπ E ′′)E1 · · ·Ek\nǫ →\n(E′ E1 · · ·Ek) ∧ (E ′′ E1 · · · Ek). Notice now that it holds that ((E ′ E1 · · ·Ek) ∧ (E ′′ E1 · · · Ek))θ = ((E′θ) (E1θ) · · · (Ekθ))∧((E ′′θ) (E1θ) · · · (Ekθ)); since the latter expression has an SLD-refutation of length n using σ, we get by the induction hypothesis that (E′ E1 · · ·Ek) ∧ (E ′′ E1 · · ·Ek) has an SLD-refutation using a substitution δ, where for some substitution γ it holds δγ ⊇ θσ and dom(δγ − θσ) is a set of template variables that are introduced during this refutation. But then, (E′ ∧ π E\n′′)E1 · · · Ek has an SLD-refutation using substitution δ which satisfies the requirements of the lemma.\nCase 6: G =← (✷∧ E). Then, Gθ =← (✷∧ (Eθ)). By Definition 7.18 we get (✷∧ (Eθ)) ǫ → Eθ. By assumption, Eθ has an SLD-refutation of length n using σ. Consider now the goal G. By Definition 7.18, we get that (✷ ∧ E) ǫ → E. Since Eθ has an SLD-refutation of length n using σ, we get by the induction hypothesis that E has an SLD-refutation using a substitution δ, where for some substitution γ it holds δγ ⊇ θσ and dom(δγ−θσ) is a set of template variables that are introduced during this refutation. But then, (✷ ∧ E) has an SLD-refutation using substitution δ which satisfies the requirements of the lemma.\nCase 7: G =← (E ∧ ✷). Almost identical to the previous case.\nCase 8: G =← (∃VE). Then, Gθ =← (∃V (Eθ)). By Definition 7.18 we get (∃V (Eθ)) ǫ → Eθ. By assumption, Eθ has an SLD-refutation of length n using σ. Consider now the goal G. By Definition 7.18, we get that (∃VE) ǫ → E. Since Eθ has an SLD-refutation of length n using σ, we get by the induction hypothesis that E has an SLD-refutation using a substitution δ, where for some substitution γ it holds δγ ⊇ θσ and dom(δγ−θσ) is a set of template variables that are introduced during this refutation. But then, (∃VE) has an SLD-refutation using substitution δ which satisfies the requirements of the lemma.\nCase 9: G =← (E1 ∧ E2). We may assume without loss of generality that given the goal Gθ =← (E1θ ∧ E2θ), the first step in the refutation will take place due to the subexpression E1θ. Moreover, again without loss of generality, due to the associativity of ∧, we assume that E1 is not an expression that contains a top-level ∧ (ie., it is not of the form A1∧A2). The proof could be easily adapted to circumvent the two assumptions just mentioned (but this would result in more cumbersome notation). We perform a case analysis on E1:\nSubcase 9.1: E1 = (A1 ≈ A2). Then, we have ((A1 ≈ A2)θ ∧ E2θ) σ1→ (✷ ∧ E2θσ1), where σ1 is an mgu of A1θ and A2θ. By assumption, (✷∧ E2θσ1) has an SLD-refutation of length n using σ′, where σ = σ1σ ′. Consider now (E1 ∧ E2). By Definition 7.18, it holds that (A1 ≈ A2) δ1→ ✷, where δ1 is an mgu of A1,A2. By Definition 7.18 we get that ((A1 ≈ A2) ∧ E2) δ1→ (✷ ∧ E2δ1). Since θσ1 is a unifier of A1,A2, there exists θ ′ such that θσ1 = δ1θ\n′, and since (✷ ∧ E2θσ1) has an SLD-refutation of length n using σ′, we get that (✷ ∧ E2δ1θ ′) = (✷ ∧ E2δ1)θ ′ has an SLD-refutation of length n using σ′. By the induction hypothesis we get that (✷∧E2δ1) has an SLD-refutation using δ′, where δ′γ ⊇ θ′σ′ and dom(δ′γ−θ′σ′) is a set of template variables that are introduced during the refutation of this goal. But then, (E1 ∧ E2) has an SLD-refutation\nusing substitution δ = δ1δ ′. Moreover, it holds that δγ = δ1δ ′γ ⊇ δ1θ ′σ′ = θσ1σ ′ = θσ.\nSubcase 9.2: E1 = (QA1 · · ·Ar). Consider first the case where θ(Q) = B, for some basic expression B. Notice now that B can be either a higher-order predicate variable or a finiteunion of lambda abstractions. We examine the case where B is a single lambda abstraction (the other two cases are similar). Since B is a lambda abstraction, we have that E1θ ǫ → E′1, where E ′ 1 is the resulting expression after performing the outer beta reduction in E1θ. By Definition 7.18 we have that (E1 ∧ E2)θ ǫ → E′1 ∧ E2θ. By assumption, E ′ 1 ∧ E2θ has an SLD-refutation of length n using σ. Consider now (E1 ∧ E2). By Definition 7.18, it holds that E1 {Q/Bt} → E′′1 , where E′′1 = E1{Q/Bt} and B = Btγ1, for some substitution γ1, with dom(γ1) = FV (Bt). We may assume without loss of generality that the set dom(γ1) is disjoint from FV (G) and from dom(θ) ∪ FV (range(θ)). By Definition 7.18, we also get that E′′1 ǫ → E′′′1 , where E ′′′ 1 is the expression that results after performing the outer beta reduction in E′′1 . Then, by Definition 7.18 we get that E1∧E2 {Q/Bt} → E′′1 ∧E2{Q/Bt} and E ′′ 1 ∧E2{Q/Bt} ǫ → E′′′1 ∧E2{Q/Bt}. Notice now that (E′′′1 ∧ E2{Q/Bt})θγ1 = E ′ 1 ∧ E2θ, and since E ′ 1 ∧ E2θ has an SLD-refutation of length n using σ, we get by the induction hypothesis that (E′′′1 ∧ E2{Q/Bt}) has an SLDrefutation using δ′, where for some substitution γ it holds δ′γ ⊇ θγ1σ and dom(δ\n′γ − θγ1σ) is a set of template variables that are introduced during this SLD-refutation. But then, E1 ∧ E2 has an SLD-refutation using substitution δ = {Q/Bt}δ\n′. Moreover, it holds that δγ = {Q/Bt}δ\n′γ ⊇ {Q/Bt}θγ1σ ⊇ θσ and dom(δγ − θσ) is a set of template variables that are introduced during the refutation of G.\nConsider now the case where θ(Q) is undefined, ie., there does not exist a binding for Q\nin θ. Then, we have that E1θ {Q/Bt} → E′1, where E ′ 1 = Bt (A1θ{Q/Bt}) · · · (Arθ{Q/Bt}). We may assume without loss of generality that the set FV (Bt) is disjoint from FV (G) and from dom(θ)∪FV (range(θ)). By Definition 7.18 we have that (E1 ∧ E2)θ {Q/Bt} → E′1 ∧ (E2θ{Q/Bt}). By assumption, E′1 ∧ (E2θ{Q/Bt}) has an SLD-refutation of length n using σ ′, where σ = {Q/Bt}σ ′. Consider now (E1 ∧ E2). By Definition 7.18, it holds that E1 {Q/Bt} → E′′1, where E′′1 = E1{Q/Bt}. By Definition 7.18 we get that E1 ∧ E2 {Q/Bt} → E′′1 ∧ E2{Q/Bt}. Notice now that (E′′1 ∧ E2{Q/Bt})θ{Q/Bt} = E ′ 1 ∧ E2θ{Q/Bt}, and since E ′ 1 ∧ E2θ{Q/Bt} has an SLDrefutation of length n using σ′, we get by the induction hypothesis that (E′′1 ∧ E2{Q/Bt}) has an SLD-refutation using δ′, where for some substitution γ it holds that δ′γ ⊇ θ{Q/Bt}σ\n′, and dom(δ′γ − θ{Q/Bt}σ\n′) is a set of template variables that are introduced during this SLDrefutation; notice that these template variables can be chosen to be different than the variables in FV (Bt). Then, E1 ∧ E2 has an SLD-refutation using substitution δ = {Q/Bt}δ\n′. Moreover, it holds that δγ = {Q/Bt}δ ′γ ⊇ {Q/Bt}θ{Q/Bt}σ ′ = θ{Q/Bt}σ\n′ = θσ and dom(δγ − θσ) is a set of template variables that are introduced during the refutation of G. Subcase 9.3: E1 =← ((A ′ ∨ π A ′′)A1 · · ·Ar). Then, E1θ = (A ′θ ∨ π A ′′θ) (A1θ) · · · (Arθ). By\nDefinition 7.18 we get that (A′θ ∨\nπ A ′′θ) (A1θ) · · · (Arθ) ǫ → (A′θ) (A1θ) · · · (Arθ) (and symmet-\nrically for A′′). By Definition 7.18 we have (E1θ ∧ E2θ) ǫ → ((A′θ) (A1θ) · · · (Arθ)) ∧ E2θ and (E1θ∧E2θ) ǫ → ((A′′θ) (A1θ) · · · (Arθ))∧E2θ. By assumption, either ((A\n′θ) (A1θ) · · · (Arθ))∧E2θ or ((A′′θ) (A1θ) · · · (Arθ)) ∧ E2θ has an SLD-refutation of length n using σ. Assume, without loss of generality, that ((A′θ) (A1θ) · · · (Arθ))∧ E2θ has an SLD-refutation of length n using σ. Notice now that by Definition 7.18, we have that (A′ ∨\nπ A ′′)A1 · · ·Ar ǫ → A′ A1 · · ·Ar. More-\nover, notice that ((A′ A1 · · ·Ar) ∧ E2)θ = ((A ′θ) (A1θ) · · · (Arθ)) ∧ E2θ, and since the latter expression has an SLD-refutation of length n using σ, we get by the induction hypothesis that ((A′ A1 · · ·Ar)∧ E2) has an SLD-refutation using a substitution δ, where for some substitution γ it holds δγ ⊇ θσ and dom(δγ − θσ) is a set of template variables that are introduced during this refutation. But then, ((A′ ∨ π A\n′′)A1 · · ·Ar)∧E2 has an SLD-refutation using substitution δ which satisfies the requirements of the lemma.\nSubcase 9.4: E1 has any other form except for the ones examined in the previous three subcases. Then, it can be verified that in all these subcases it holds that E1θ ǫ → E′1, for some E ′ 1. By Definition 7.18 we have that (E1 ∧ E2)θ ǫ → E′1 ∧ (E2θ). By assumption, E ′ 1 ∧ (E2θ) has an SLDrefutation of length n using σ. Consider now (E1∧E2). By Definition 7.18 and by examination of all the possible cases for E1 it can be seen that E1 ǫ → E′′1, where E ′ 1 = E ′′ 1θ. By Definition 7.18 we get that E1∧E2 ǫ → E′′1∧E2. Notice now that (E ′′ 1∧E2)θ = E ′ 1∧E2θ, and since E ′ 1∧E2θ has an SLD-refutation of length n using σ, we get by the induction hypothesis that (E′′1 ∧ E2) has an SLD-refutation using δ, where for some substitution γ it holds that δγ ⊇ θσ, and dom(δγ−θσ) is a set of template variables that are introduced during this SLD-refutation. Then, E1 ∧ E2 has an SLD-refutation using substitution δ which satisfies the requirements of the lemma."
    }, {
      "heading" : "F Proof of Lemma 7.35",
      "text" : "Lemma 7.35 Let P be a program and G =← A be a goal such that [[A]]s(⊥IP) = 1 for all Herbrand states s. Then, there exists an SLD-refutation for P ∪ {G} with computed answer equal to the identity substitution.\nProof. We establish a stronger statement which has the statement of the lemma as a special case. Let us call a substitution θ closed if every expression in range(θ) is closed. We demonstrate that for every closed basic substitution θ, if [[Aθ]]s(⊥IP) = 1 for all Herbrand states s, then there exists an SLD-refutation for P ∪ {Aθ} with computed answer equal to the identity substitution. The statement of the lemma is then a direct consequence for θ = ǫ.\nWe start by noting that A is always of the form (A0 A1 · · ·An), n ≥ 0 (if n = 0 then A0 is of type o). We perform induction on the type ρ1 → · · · ρn → o of A0.\nOuter Induction Basis: The outer induction basis is for n = 0, ie., for type(A0) = o, and in order to establish it we need to perform an inner structural induction on A0.\nInner Induction Basis: For the inner induction basis we need to examine the cases where A0 is 0, 1, (E1 ≈ E2) and Q, where type(Q) = o. The first case is not applicable since [[0θ]]s(⊥IP) = 0. The second case is immediate. We examine the latter two cases:\nCase 1: A0 = (E1 ≈ E2). Since for all s it holds that [[(E1 ≈ E2)θ]]s(⊥IP) = 1, we get that for all s, [[E1θ]]s(⊥IP) = [[E2θ]]s(⊥IP). By the fact that ⊥IP is a Herbrand interpretation, we conclude that E1θ and E2θ must be identical expressions of type ι, and therefore they are unifiable using the identity substitution.\nCase 2: A0 = Q, with type(Q) = o. If θ(Q) = 0 then it can not be the case that [[A0]]s(⊥IP) = 1, and therefore this case is not applicable. If θ(Q) = 1, the result is trivial. If on the other hand Q 6∈ dom(θ), then this case is not applicable since it is not possible to have [[A0θ]]s(⊥IP) = 1, for all s (eg. choose s such that s(Q) = 0).\nInner Induction Step: We distinguish the following cases:\nCase 1: A0 = (∃QE). We can assume without loss of generality that Q 6∈ dom(θ). Since for all s it holds that [[(∃QE)θ]]s(⊥IP) = 1, it follows that [[Eθ]]s[b/Q](⊥IP) = 1 for some b ∈ FUP(type(Q)). Let θ ′ = {Q/B} where B is a closed basic expression such that [[B]](⊥IP) = b (the existence of such an expression B is ensured by Lemma 7.6). Then it is easy to see that [[Eθθ′]]s(⊥IP) = 1 for all states s. By the induction hypothesis there exists an SLD-refutation for P ∪ {Eθθ′} using some substitution σ and with computed answer equal to the identity substitution. Using Lemma 7.27, it follows that there exists an SLD-refutation of P ∪ {Eθ} using substitution δ where for some substitution γ it holds that δγ ⊇ {Q/B}σ; moreover,\ndom(δγ − {Q/B}σ) is a set of template variables that are introduced during the refutation of P∪ {Eθ}. Since the restriction of σ to the free variables of Eθθ′ is the identity substitution, it follows that the restriction of δ to the free variables of Eθ will either be empty or it will only contain the binding Q/B. We conclude that there exists a refutation of P ∪ {(∃QE)θ} using substitution ǫδ = δ. The computed answer is the identity substitution since Q is not a free variable of (∃QE)θ.\nCase 2: A0 = (E1 ∧ E2). By assumption, [[(E1 ∧ E2)θ]]s(⊥IP) = 1, for all s. Then, it holds [[E1θ]]s(⊥IP) = 1 and [[E2θ]]s(⊥IP) = 1. By the induction hypothesis there exist SLD-refutations for P∪{← E1θ} and P∪{← E2θ} with computed answers equal to the identity substitution. Let θ1 and θ2 be the compositions of the substitutions used for the refutations of P∪{← E1θ} and P∪{← E2θ} respectively. Now, since the computed answer of the refutation for P∪{E1θ} is the identity, this implies that the free variables of E2θ that also appear free in E1θ do not belong to dom(θ1). Moreover, the rest of the free variables of E2θ do not belong to dom(θ1), because the variables of θ1 have been obtained by using resolution steps that only involve “fresh” variables. In conclusion, the restriction of θ1 to the free variables of (E1∧E2)θ is the identity substitution (and similarly for θ2). These observations imply that E2θθ1 = E2θ. But then, we can construct a refutation for P ∪ {← (E1θ ∧ E2θ)} by first deriving ✷ from E1θ and then deriving ✷ from E2θθ1 = E2θ. The substitution used for the refutation of P ∪ {← (E1 ∧ E2)θ} is θ1θ2 and the computed answer is equal to the restriction of θ1θ2 to the free variables of (E1 ∧ E2)θ, which gives the identity substitution.\nCase 3: A0 = (E1 ∨ E2). By assumption, [[(E1 ∨ E2)θ]]s(⊥IP) = 1, for all s. Then, it either holds that [[E1θ]]s(⊥IP) = 1 or [[E2θ]]s(⊥IP) = 1. Without loss of generality, assume that [[E1θ]]s(⊥IP) = 1. By the induction hypothesis there exists an SLD-refutation for P∪{← E1θ} with computed answer equal to the identity substitution. But then P∪{← (E1θ∨E2θ)} has an SLD-refutation whose first step leads to ← (E1θ) using ǫ and then proceeds according to the SLD-refutation of ← (E1θ). The computed answer of this refutation is obviously the identity substitution.\nOuter Induction Step: Assume the lemma holds when A0 has type ρ1 → · · · ρn−1 → o, n ≥ 1. We establish the lemma for the case where A0 has type π = ρ1 → · · · ρn → o. We distinguish the following cases:\nCase 1: A0 = p (ie., A = pA1 · · ·An). This case is not applicable since ⊥IP(p) =⊥π and therefore [[A]]s(⊥IP) = 0, for all s.\nCase 2: A0 = Q (ie., A = QA1 · · ·An). If Q 6∈ dom(θ) then this case is not applicable since it is not possible to have [[A]]s(⊥IP) = 1, for all s (eg. take s(Q) =⊥π). If on the other hand Q ∈ dom(θ), then θ(Q) is a basic expression of type π, ie., it is a nonempty finite union of lambda abstractions. We demonstrate the case where θ(Q) is a single lambda abstraction; the more general case is similar and omitted. Assume therefore that θ(Q) = λV.E. Then, since [[(λV.E) (A1θ) · · · (Anθ)]]s(⊥IP) = 1, by Lemma 7.14 we get that [[(E{V/A1θ}) (A2θ) · · · (Anθ)]]s(⊥IP) = 1. By assumption, θ is a closed substitution and therefore the only free variable that appears in E is V. Therefore, [[((E{V/A1})A2 · · ·An)θ]]s(⊥IP) = 1. By the outer induction hypothesis we get that P ∪ {← ((E{V/A1})A2 · · ·An)θ} has an SLD-refutation using substitution δ, with computed answer equal to the identity substitution. By the definition of SLD-resolution we get that P ∪ {← (λV.E) (A1θ) · · · (Anθ))} has an SLD-refutation using the substitution ǫδ = δ; the computed answer of this refutation is the restriction of δ to the free variables of ((λV.E)A1 · · ·An)θ which (by our previous discussion) gives the identity substitution.\nCase 3: A0 = λV.E (ie., A = (λV.E)A1 · · ·An. We can assume without loss of generality that V 6∈ dom(θ) ∪ FV (range(θ)). Then, since [[(λV.Eθ) (A1θ) · · · (Anθ)]]s(⊥IP) = 1, by\nLemma 7.14 we get that [[(Eθ{V/A1θ}) (A2θ) · · · (Anθ)]]s(⊥IP) = 1. By the outer induction hypothesis P ∪ {← (Eθ{V/A1θ}) (A2θ) · · · (Anθ)} has an SLD-refutation using substitution δ, with computed answer equal to the identity substitution. By the definition of SLD-resolution we get that P ∪ {← ((λV.Eθ) (A1θ) · · · (Anθ))} has an SLD-refutation using the substitution ǫδ = δ; the computed answer of this refutation is the restriction of δ to the free variables of ((λV.E)A1 · · ·An)θ which (by our previous discussion) gives the identity substitution. Case 4: A0 = (E ′ ∨ π E ′′) (ie., A = (E′ ∨ π E\n′′)A1 · · ·An), where π 6= o. The proof for this case follows easily using the outer induction hypothesis. Case 5: A0 = (E ′ ∧ π E ′′) (ie., A = (E′ ∧ π E\n′′)A1 · · ·An), where π 6= o. The proof for this case follows easily using the outer induction hypothesis."
    } ],
    "references" : [ {
      "title" : "Domain theory",
      "author" : [ "S. Abramsky", "A. Jung" ],
      "venue" : "Handbook of Logic in Computer Science III,",
      "citeRegEx" : "Abramsky and Jung.,? \\Q1994\\E",
      "shortCiteRegEx" : "Abramsky and Jung.",
      "year" : 1994
    }, {
      "title" : "Logic Programming",
      "author" : [ "K.R. Apt" ],
      "venue" : "In: J. van Leeuwen, editor, Handbook of Theoretical Computer Science vol. B, Elsevier Science Publishers B.V.",
      "citeRegEx" : "Apt90",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "The Lambda Calculus: Its Syntax and Semantics",
      "author" : [ "H.P. Barendregt" ],
      "venue" : "North-Holland",
      "citeRegEx" : "Bar84",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Extensionality of Simply Typed Logic Programs",
      "author" : [ "M. Bezem" ],
      "venue" : "International Conference on Logic Programming (ICLP), pages 395–410",
      "citeRegEx" : "Bez99",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "An Improved Extensionality Criterion for Higher-Order Logic Programs",
      "author" : [ "M. Bezem" ],
      "venue" : "Computer Science Logic (CSL), pages 203–216",
      "citeRegEx" : "Bez01",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Lattice Theory",
      "author" : [ "G. Birkhoff" ],
      "venue" : "American Mathematical Society",
      "citeRegEx" : "Bir67",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "HiLog as a Platform for Database Languages",
      "author" : [ "W.C. Chen", "M. Kifer", "D.S. Warren" ],
      "venue" : "IEEE Data Eng. Bull., 12(3): 37–44",
      "citeRegEx" : "CKW89",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "HILOG: A Foundation for Higher-Order Logic Programming",
      "author" : [ "W.C. Chen", "M. Kifer", "D.S. Warren" ],
      "venue" : "J. of Logic Programming, 15(3):187–230",
      "citeRegEx" : "CKW93",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Predicate Logic as a Computational Formalism",
      "author" : [ "K.L. Clark" ],
      "venue" : "Research Report DOC 79/59, Dept. of Computing, Imperial College, London",
      "citeRegEx" : "Cla79",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Functional Programming",
      "author" : [ "A.J. Field", "P. Harrison" ],
      "venue" : "Addison-Wesley",
      "citeRegEx" : "FH88",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "General Lattice Theory",
      "author" : [ "G. Grätzer" ],
      "venue" : "Academic Press",
      "citeRegEx" : "Gra78",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "LUSH-Resolution and its Completeness",
      "author" : [ "R. Hill" ],
      "venue" : "DCL Memo 78, Dept. of Artificial Intelligence, University of Edinburgh",
      "citeRegEx" : "H74",
      "shortCiteRegEx" : null,
      "year" : 1974
    }, {
      "title" : "Introduction to Combinators and λ-calculus",
      "author" : [ "J.R. Hindley", "J.P. Seldin" ],
      "venue" : "London Mathematical Society",
      "citeRegEx" : "HS86",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Extensional Higher-Order Datalog",
      "author" : [ "V. Kountouriotis", "P. Rondogiannis", "W.W. Wadge" ],
      "venue" : "short paper proceedings of the 12th International Conference on Logic for Programming, Artificial Intelligence and Reasoning (LPAR-12), pages 1–5",
      "citeRegEx" : "KRW05",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Foundations of Logic Programming",
      "author" : [ "J. Lloyd" ],
      "venue" : "Springer-Verlag",
      "citeRegEx" : "Llo87",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Higher Order Logic Programming",
      "author" : [ "D.A. Miller", "G. Nadathur" ],
      "venue" : "Proceedings of the Third International Conference on Logic Programming, pages 448–462",
      "citeRegEx" : "MN86",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "A Higher-Order Logic as the Basis for Logic Programming",
      "author" : [ "G. Nadathur" ],
      "venue" : "Ph.D. thesis, University of Pennsylvania",
      "citeRegEx" : "Nad87",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Higher-Order Horn Clauses",
      "author" : [ "G. Nadathur", "D. Miller" ],
      "venue" : "Journal of the ACM, 37(4):777–814",
      "citeRegEx" : "NM90",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Higher-Order Logic Programming",
      "author" : [ "G. Nadathur", "D. Miller" ],
      "venue" : "Dov M. Gabbay, C. J. Hogger, and J. A. Robinson, editors, Handbook of Logics for Artificial Intelligence and Logic Programming, pages 499–590. Clarendon Press, Oxford",
      "citeRegEx" : "NM98",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Minimum Model Semantics for Logic Programs with Negation-as-Failure",
      "author" : [ "P. Rondogiannis", "W.W. Wadge" ],
      "venue" : "ACM TOCL, 6(2): 441–467",
      "citeRegEx" : "RW05",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Denotational Semantics: the Scott-Strachey Approach to Programming Language Theory",
      "author" : [ "J.E. Stoy" ],
      "venue" : "MIT Press",
      "citeRegEx" : "Sto77",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Semantics of Programming Languages",
      "author" : [ "R.D. Tennent" ],
      "venue" : "Prentice Hall",
      "citeRegEx" : "Ten91",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Higher-Order Horn Logic Programming",
      "author" : [ "W.W. Wadge" ],
      "venue" : "Proceedings of the International Symposium on Logic Programming, pages 289–303",
      "citeRegEx" : "Wad91",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Higher-Order Extensions to Prolog: are they needed? Machine Intelligence",
      "author" : [ "D.H.D. Warren" ],
      "venue" : "10:441–454",
      "citeRegEx" : "War82",
      "shortCiteRegEx" : null,
      "year" : 1982
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "The initial attitude of logic programmers towards higher-order logic programming was somewhat skeptical: it was often argued (see for example [War82]) that there exist ways of encoding or simulating higher-order programming inside Prolog itself.",
      "startOffset" : 142,
      "endOffset" : 149
    }, {
      "referenceID" : 22,
      "context" : "Wadge in [Wad91] and answered in the affirmative.",
      "startOffset" : 9,
      "endOffset" : 16
    }, {
      "referenceID" : 22,
      "context" : "Contributions: In this paper we extend the study initiated in [Wad91] and derive the first, to our knowledge, complete theoretical framework for extensional higher-order logic programming, both from a semantic as-well-as from a proof theoretic point of view.",
      "startOffset" : 62,
      "endOffset" : 69
    }, {
      "referenceID" : 10,
      "context" : "Our first contribution is the development of a novel extensional semantics for higher-order logic programming that is based on algebraic lattices (see for example [Gra78]), a subclass of the familiar complete lattices that have traditionally been used in the theory of first-order logic programming.",
      "startOffset" : 163,
      "endOffset" : 170
    }, {
      "referenceID" : 22,
      "context" : "In other words, the proposed semantics reflects in a direct way the finitary nature of continuity that is implicit in [Wad91].",
      "startOffset" : 118,
      "endOffset" : 125
    }, {
      "referenceID" : 22,
      "context" : "The benefit of the new approach compared to that of [Wad91] is that all basic properties and results of classical logic programming are now transferred in the higher-order setting in a natural way.",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 22,
      "context" : "Moreover, the new semantics leads to a relatively simple sound and complete proof procedure (see below) even for a language that is genuinely more powerful than the one considered in [Wad91].",
      "startOffset" : 183,
      "endOffset" : 190
    }, {
      "referenceID" : 22,
      "context" : "The first problem we consider is to bypass one important restriction of [Wad91], namely the inability to handle programs in which clauses contain uninstantiated predicate variables.",
      "startOffset" : 72,
      "endOffset" : 79
    }, {
      "referenceID" : 22,
      "context" : "According to the semantics of [Wad91], the least Herbrand model of the program assigns to predicate p a continuous relation which is true of all unary relations that contain at least 0 and s(0).",
      "startOffset" : 30,
      "endOffset" : 37
    }, {
      "referenceID" : 22,
      "context" : "The above example illustrates why uninstantiated predicate variables in clauses were disallowed in [Wad91].",
      "startOffset" : 99,
      "endOffset" : 106
    }, {
      "referenceID" : 22,
      "context" : "In more formal terms, the least Herbrand model of a higher-order program under the semantics of [Wad91] is in general an uncountable set; in our example, this is evidenced by the fact that there exists an uncountable number of unary relations over the natural numbers that contain both 0 and s(0).",
      "startOffset" : 96,
      "endOffset" : 103
    }, {
      "referenceID" : 22,
      "context" : "How can one define a proof procedure that is sound and complete with respect to this semantics? The key idea for bypassing these problems was actually anticipated in the concluding section of [Wad91]:",
      "startOffset" : 192,
      "endOffset" : 199
    }, {
      "referenceID" : 22,
      "context" : "1) range over finite relations (and not over arbitrary relations as in [Wad91]).",
      "startOffset" : 71,
      "endOffset" : 78
    }, {
      "referenceID" : 20,
      "context" : "Actually, this is an old and well-known assumption in the area of denotational semantics, as the following excerpt from [Sto77][page 98] indicates:",
      "startOffset" : 120,
      "endOffset" : 127
    }, {
      "referenceID" : 10,
      "context" : "In fact, as we are going to see, for every predicate type of our language, the set of possible meanings of this type forms an algebraic lattice [Gra78]; then, the above property is nothing more than the key property which characterizes algebraic lattices (see Proposition 4.",
      "startOffset" : 144,
      "endOffset" : 151
    }, {
      "referenceID" : 14,
      "context" : "As it is well-known, the standard semantics of classical (first-order) logic programming, is based on complete lattices (see for example [Llo87]).",
      "startOffset" : 137,
      "endOffset" : 144
    }, {
      "referenceID" : 10,
      "context" : "As we are going to see, the development of the semantics of H is based on a special class of complete lattices, namely algebraic lattices (see for example [Gra78]).",
      "startOffset" : 155,
      "endOffset" : 162
    }, {
      "referenceID" : 21,
      "context" : "An easy proof using basic properties of posets (see for example the corresponding proof for domains [Ten91][Lemma 5.",
      "startOffset" : 100,
      "endOffset" : 107
    }, {
      "referenceID" : 10,
      "context" : "We can now define the notion of algebraic lattice (see for example [Gra78]), which will prove to be the key lattice-theoretic concept applicable to our context.",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "Birkhoff [Bir67] (who did not assume completeness at that time).",
      "startOffset" : 9,
      "endOffset" : 16
    }, {
      "referenceID" : 22,
      "context" : "The results of [Wad91] indicate that (due to continuity), if a relation satisfies a predicate, then some “finite representative” of this relation also satisfies it.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 22,
      "context" : "It should be noted at this point that the semantics of types of our language is in some sense a finitary version of the one given in [Wad91], where the denotation of a type of the form π1 → π2 is the set of all continuous functions from the denotation of π1 to the denotation of π2 (more details on the connections between the two approaches will be given in Section 5.",
      "startOffset" : 133,
      "endOffset" : 140
    }, {
      "referenceID" : 22,
      "context" : "In this subsection we give a brief comparison of the proposed semantics with the semantics introduced in [Wad91].",
      "startOffset" : 105,
      "endOffset" : 112
    }, {
      "referenceID" : 22,
      "context" : "A complete presentation of such a comparison would require a detailed presentation of the approach introduced in [Wad91] and its adaptation to the richer language H introduced in this paper.",
      "startOffset" : 113,
      "endOffset" : 120
    }, {
      "referenceID" : 22,
      "context" : "As already mentioned, the source language considered in [Wad91] is restricted compared to H.",
      "startOffset" : 56,
      "endOffset" : 63
    }, {
      "referenceID" : 22,
      "context" : "However, the semantics of [Wad91] can be appropriately extended to apply to H as well.",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "Given a non-empty set D, let us denote by [[ρ]]∗D the semantics of an argument type ρ in D under the approach of [Wad91].",
      "startOffset" : 113,
      "endOffset" : 120
    }, {
      "referenceID" : 22,
      "context" : "Roughly speaking, one can say that the semantics of [Wad91] is the logic programming analogue of the standard denotational semantics of functional programming languages [Ten91].",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "Roughly speaking, one can say that the semantics of [Wad91] is the logic programming analogue of the standard denotational semantics of functional programming languages [Ten91].",
      "startOffset" : 169,
      "endOffset" : 176
    }, {
      "referenceID" : 22,
      "context" : "In the following, we will refer to the semantics of [Wad91] as the “continuous semantics”.",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 14,
      "context" : "Using exactly the same reasoning as in the first-order case (see for example the corresponding proof in [Llo87]).",
      "startOffset" : 104,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "In order to simplify matters, we follow the simple approach suggested in [Bar84][pages 26-27], which consists of the following two conventions:",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 21,
      "context" : "We now have the following Substitution Lemma (see for example [Ten91] for a corresponding lemma in the case of functional programming).",
      "startOffset" : 62,
      "endOffset" : 69
    }, {
      "referenceID" : 14,
      "context" : "In order to establish the completeness of the proposed SLD-resolution, we need to first demonstrate a result that is analogous to the lifting lemma of the first-order case (see [Llo87]).",
      "startOffset" : 177,
      "endOffset" : 184
    }, {
      "referenceID" : 1,
      "context" : "We can now prove the analogue of a theorem due to Apt and van Emden (see [Apt90][Lemma 3.",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "17] or [Llo87][Theorem 8.",
      "startOffset" : 7,
      "endOffset" : 14
    }, {
      "referenceID" : 11,
      "context" : "The following theorem generalizes a result of Hill [H74] (see also [Apt90][Theorem 3.",
      "startOffset" : 51,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "The following theorem generalizes a result of Hill [H74] (see also [Apt90][Theorem 3.",
      "startOffset" : 67,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "Finally, the following theorem is a generalization of Clark’s theorem [Cla79] (see also the more accessible [Apt90][Theorem 3.",
      "startOffset" : 70,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "Finally, the following theorem is a generalization of Clark’s theorem [Cla79] (see also the more accessible [Apt90][Theorem 3.",
      "startOffset" : 108,
      "endOffset" : 115
    }, {
      "referenceID" : 22,
      "context" : "Apart from the results of [Wad91]5, the only other work that has come to our attention is that of M.",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "Bezem [Bez99, Bez01], who considers higher-order logic programming languages with syntax similar to that of [Wad91].",
      "startOffset" : 108,
      "endOffset" : 115
    }, {
      "referenceID" : 4,
      "context" : "In [Bez01] a notion of extensionality is defined (called the extensional collapse) and it is demonstrated that many logic programs are extensional under this notion; however, this notion appears to differ from classical extensionality and has a more proof-theoretical flavor.",
      "startOffset" : 3,
      "endOffset" : 10
    }, {
      "referenceID" : 18,
      "context" : "For a more detailed discussion on this issue, see [NM98][section 7.",
      "startOffset" : 50,
      "endOffset" : 56
    }, {
      "referenceID" : 22,
      "context" : "The work in [Wad91] has also been used in order to define a higher-order extension of Datalog [KRW05].",
      "startOffset" : 12,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : "The work in [Wad91] has also been used in order to define a higher-order extension of Datalog [KRW05].",
      "startOffset" : 94,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "The semantics of λProlog is not extensional (see for example the discussion in [NM98]).",
      "startOffset" : 79,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : ", predicate terms) for R that one could think of (see the relevant discussion in [NM98][page 50]).",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : "As remarked in [CKW93], “equality in (a fragment of) λProlog corresponds to λ-equivalence and is not extensional: there may exist predicates that are not λ-equivalent but still extensionally equal”.",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "The above program behavior can be best explained by the following comment from [CKW93]: “in HiLog predicates and other higher-order syntactic objects are not equal unless they (ie.",
      "startOffset" : 79,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "We believe that ideas originating from graph-reduction [FH88] will also prove vital in the development of this extended implementation.",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 19,
      "context" : "The semantics of negation in a higher-order setting could probably be captured model-theoretically using the recent purely logical characterization of the well-founded semantics through an appropriate infinite-valued logic [RW05].",
      "startOffset" : 223,
      "endOffset" : 229
    } ],
    "year" : 2011,
    "abstractText" : "We propose a purely extensional semantics for higher-order logic programming. In this semantics program predicates denote sets of ordered tuples, and two predicates are equal iff they are equal as sets. Moreover, every program has a unique minimum Herbrand model which is the greatest lower bound of all Herbrand models of the program and the least fixed-point of an immediate consequence operator. We also propose an SLD-resolution proof procedure which is proven sound and complete with respect to the minimum model semantics. In other words, we provide a purely extensional theoretical framework for higher-order logic programming which generalizes the familiar theory of classical (firstorder) logic programming.",
    "creator" : "LaTeX with hyperref package"
  }
}