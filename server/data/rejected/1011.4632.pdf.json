{
  "name" : "1011.4632.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n01 1.\n46 32\nv1 [\ncs .A\nI] 2\n1 N\nov 2\n01 0\n√ t or −1/ √ t with equal\nprobability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results."
    }, {
      "heading" : "1 Introduction",
      "text" : "The k-means clustering algorithm [16] was recently recognized as one of the top ten data mining tools of the last fifty years [20]. In parallel, random projections (RP) or the so-called Johnson-Lindenstrauss type embeddings [12] became popular and found applications in both theoretical computer science [2] and data analytics [4]. This paper focuses on the application of the random projection method (see Section 2.3) to the k-means clustering problem (see Definition 1). Formally, assuming as input a set of n points in d dimensions, our goal is to randomly project the points into d̃ dimensions, with d̃ ≪ d, and then apply a k-means clustering algorithm (see Definition 2) on the projected points. Of course, one should be able to compute the projection fast without distorting significantly the “clusters” of the original point set. Our algorithm (see Algorithm 1) satisfies both conditions by computing the embedding in time linear in the size of the input and by distorting the “clusters” of the dataset by a factor of at most 2 + ε, for some ε ∈ (0, 1/3) (see Theorem 1). We believe that the high dimensionality of modern data will render our algorithm useful and attractive in many practical applications [9].\nDimensionality reduction encompasses the union of two different approaches: feature selection, which embeds the points into a low-dimensional space by selecting actual dimensions of the data, and feature extraction, which finds an embedding by constructing new artificial features that are, for example, linear combinations of the original features. Let A be an n × d matrix containing n d-dimensional points (A(i) denotes the i-th point of the set), and let k be the number of clusters (see also Section 2.2 for more notation). We slightly abuse notation by also denoting by A the n-point set formed by the rows of A. We say that an embedding f : A → Rd̃ with f(A(i)) = Ã(i) for all i ∈ [n] and some d̃ < d, preserves the clustering structure of A within a factor φ, for some φ ≥ 1, if finding an optimal clustering in Ã and plugging it back to A is only a factor of φ worse than finding the optimal clustering directly in A. Clustering optimality and approximability are formally presented in Definitions 1 and 2, respectively. Prior efforts on designing provably accurate dimensionality reduction methods for k-means clustering include: (i) the Singular Value Decomposition (SVD), where one finds an embedding with image Ã = UkΣk ∈ Rn×k such that the clustering structure is preserved within a factor of two; (ii) random projections, where one projects the input points into t = Ω(log(n)/ε2) dimensions such that with constant probability the clustering structure is preserved within a factor of 1+ε (see Section 2.3); (iii) SVD-based feature selection, where one can use the SVD to find c = Ω(k log(k/ε)/ε2) actual features, i.e. an embedding with image Ã ∈ Rn×c containing (rescaled) columns fromA, such that with constant probability the clustering structure is preserved within a factor of 2 + ε. These results are summarized in Table 1. A head-to-head comparison of our algorithm with existing results allows us to claim the following improvements: (i)\nreduce the running time by a factor of min{n, d}⌈ε2 log(d)/k⌉, while losing only a factor of ε in the approximation accuracy and a factor of 1/ε2 in the dimension of the embedding; (ii) reduce the dimension of the embedding and the running time by a factor of log(n)/k while losing a factor of one in the approximation accuracy; (iii) reduce the dimension of the embedding by a factor of log(k/ε) and the running time by a factor of min{n, d}⌈ε2 log(d)/k⌉, respectively. Finally, we should point out that other techniques, for example the Laplacian scores [10] or the Fisher scores [7], are very popular in applications (see also surveys on the topic [8, 13]). However, they lack a theoretical worst case analysis of the form we describe in this work."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We start by formally defining the k-means clustering problem using matrix notation. Later in this section, we precisely describe the approximability framework adopted in the k-means clustering literature and fix the notation.\nDefinition 1. [THE K-MEANS CLUSTERING PROBLEM] Given a set of n points in d dimensions (rows in an n× d matrix A) and a positive integer k denoting the number of clusters, find the n× k indicator matrix Xopt such that\nXopt = arg min X∈X\n∥ ∥A−XX⊤A ∥ ∥ 2\nF . (1)\nHere X denotes the set of all n×k indicator matrices X . The functional F (A,X) = ∥ ∥A−XX⊤A ∥ ∥ 2\nF is the so-called\nk-means objective function. An n × k indicator matrix has exactly one non-zero element per row, which denotes cluster membership. Equivalently, for all i = 1, . . . , n and j = 1, . . . , k, the i-th point belongs to the j-th cluster if and only if Xij = 1/ √ zj , where zj denotes the number of points in the corresponding cluster. Note that X⊤X = Ik, where Ik is the k × k identity matrix.\n2.1 Approximation Algorithms for k-means clustering\nFinding Xopt is an NP-hard problem even for k = 2 [3], thus research has focused on developing approximation algorithms for k-means clustering. The following definition captures the framework of such efforts.\nDefinition 2. [K-MEANS APPROXIMATION ALGORITHM] An algorithm is a “γ-approximation” for the k-means clustering problem (γ ≥ 1) if it takes inputs A and k, and returns an indicator matrix Xγ that satisfies with probability at least 1− δγ ,\n∥ ∥A−XγX⊤γ A ∥ ∥ 2 F ≤ γ min\nX∈X\n∥ ∥A−XX⊤A ∥ ∥ 2\nF . (2)\nIn the above, δγ ∈ [0, 1) is the failure probability of the γ-approximation k-means algorithm.\nFor our discussion, we fix the γ-approximation algorithm to be the one presented in [14], which guarantees γ = 1+ ε′ for any ε′ ∈ (0, 1] with running time O(2(k/ε′)O(1)dn)."
    }, {
      "heading" : "2.2 Notation",
      "text" : "Given an n × d matrix A and an integer k with k < min{n, d}, let Uk ∈ Rn×k (resp. Vk ∈ Rd×k) be the matrix of the top k left (resp. right) singular vectors of A, and let Σk ∈ Rk×k be a diagonal matrix containing the top\nk singular values of A in non-increasing order. If we let ρ be the rank of A, then Aρ−k is equal to A − Ak, with Ak = UkΣkV ⊤ k . By A(i) we denote the i-th row of A. For an index i taking values in the set {1, . . . , n} we write i ∈ [n]. We denote, in non-increasing order, the non-negative singular values of A by σi(A) with i ∈ [ρ]. ‖A‖F and ‖A‖2 denote the Frobenius and the spectral norm of a matrix A, respectively. A† denotes the pseudo-inverse of A, i.e. the unique d × n matrix satisfying A = AA†A, A†AA† = A†, (AA†)⊤ = AA†, and (A†A)⊤ = A†A. Note also that ∥ ∥A†\n∥ ∥ 2 = σ1(A\n†) = 1/σρ(A) and ‖A‖2 = σ1(A) = 1/σρ(A†). A useful property of matrix norms is that for any two matrices C and T of appropriate dimensions, ‖CT ‖F ≤ ‖C‖F ‖T ‖2; this is a stronger version of the standard submultiplicavity property. We call P a projector matrix if it is square and P 2 = P . We use E [Y ] and Var [Y ] to take the expectation and the variance of a random variable Y and P (e) to take the probability of an event e. We abbreviate “independent identically distributed” to “i.i.d.” and “with probability” to “w.p.”. Finally, all logarithms are base two."
    }, {
      "heading" : "2.3 Random Projections",
      "text" : "A classical result of Johnson and Lindenstrauss states that any n-point set in d dimensions - rows in a matrix A ∈ Rn×d - can be linearly projected into t = Ω(log(n)/ε2) dimensions while preserving pairwise distances within a factor of 1±ε using a random orthonormal matrix [12]. Subsequent research simplified the proof of the above result by showing that such a projection can be generated using a d× t random Gaussian matrix R, i.e., a matrix whose entries are i.i.d. Gaussian random variables with zero mean and variance 1/ √ t [11]. More precisely, the following inequality holds with high probability over the randomness of R,\n(1− ε) ∥ ∥A(i) −A(j) ∥ ∥ 2 ≤ ∥ ∥A(i)R−A(j)R ∥ ∥ 2 ≤ (1 + ε) ∥ ∥A(i) −A(j) ∥ ∥ 2 . (3)\nNotice that such an embedding Ã = AR preserves the metric structure of the point-set, so it also preserves, within a factor of 1 + ε, the optimal value of the k-means objective function of A. Achlioptas proved that even a (rescaled) random sign matrix suffices in order to get the same guarantees as above [1], an approach that we adopt here (see step two in Algorithm 1). Moreover, in this paper we will heavily exploit the structure of such a random matrix, and obtain, as an added bonus, savings on the computation of the projection.\n3 A random-projection-type k-means algorithm Algorithm 1 takes as inputs the matrix A ∈ Rn×d, the number of clusters k, an error parameter ε ∈ (0, 1/3), and some γ-approximation k-means algorithm. It returns an indicator matrix Xγ̃ determining a k-partition of the rows of A.\nInput: n× d matrix A (n points, d features), number of clusters k, error parameter ε ∈ (0, 1/3), and γ-approximation k-means algorithm. Output: Indicator matrix Xγ̃ determining a k-partition on the rows of A.\n1. Set t = Ω(k/ε2), i.e. set t = to ≥ ck/ε2 for a sufficiently large constant c. 2. Compute a random d× t matrix R as follows. For all i ∈ [d], j ∈ [t]\nRij =\n{ +1/ √ t,w.p. 1/2,\n−1/ √ t,w.p. 1/2.\n3. Compute the product Ã = AR. 4. Run the γ-approximation algorithm on Ã to obtain Xγ̃ ; Return the indicator matrix Xγ̃\nAlgorithm 1: A random projection algorithm for k-means clustering."
    }, {
      "heading" : "3.1 Running time analysis",
      "text" : "Algorithm 1 reduces the dimensions of A by post-multiplying it with a random sign matrix R. Interestingly, any “random projection matrix” R that respects the properties of Lemma 2 with t = Ω(k/ε2) can be used in this step. If R is constructed as in Algorithm 1, one can employ the so-called mailman algorithm for matrix multiplication [15] and\ncompute the productAR in O(nd⌈ε−2k/ log(d)⌉) time. Indeed, the mailman algorithm computes (after preprocessing 1) a matrix-vector product of any d-dimensional vector (row of A) with an d × log(d) sign matrix in O(d) time. By partitioning the columns of our d × t matrix R into ⌈t/ log(d)⌉ blocks, the claim follows. Notice that when k = O(log(d)), then we get an - almost - linear time complexity O(nd/ε2). The latter assumption is reasonable in our setting since the need for dimension reduction in k-means clustering arises usually in high-dimensional data (large d). Other choices of R would give the same approximation results; the time complexity to compute the embedding would be different though. A matrix where each entry is a random Gaussian variable with zero mean and variance 1/ √ t would imply an O(knd/ε2) time complexity (naive multiplication). In our experiments in Section 5 we experiment with the matrix R described in Algorithm 1 and employ MatLab’s matrix-matrix BLAS implementation to proceed in the third step of the algorithm. We also experimented with a novel MatLab/C implementation of the mailman algorithm but, in the general case, we were not able to outperform MatLab’s built-in routines (see section 5.2).\nFinally, note that any γ-approximation algorithm may be used in the last step of Algorithm 1. Using, for example, the algorithm of [14] with γ = 1 + ε would result in an algorithm that preserves the clustering within a factor of 2 + ε, for any ε ∈ (0, 1/3), running in time O(nd⌈ε−2k/ log(d)⌉ + 2(k/ε)O(1)kn/ε2). In practice though, the Lloyd algorithm [16, 17] is very popular and although it does not admit a worst case theoretical analysis, it empirically does well. We thus employ the Lloyd algorithm for our experimental evaluation of our algorithm in Section 5. Note that, after using the proposed dimensionality reduction method, the cost of the Lloyd heuristic is only O(nk2/ε2) per iteration. This should be compared to the cost of O(knd) per iteration if applied on the original high dimensional data."
    }, {
      "heading" : "4 Main Theorem",
      "text" : "Theorem 1 is our main quality-of-approximation result for Algorithm 1. Notice that if γ = 1, i.e. if the k-means problem with inputs Ã and k is solved exactly, Algorithm 1 guarantees a distortion of at most 2 + ε, as advertised.\nTheorem 1. Let the n × d matrix A and the positive integer k < min{n, d} be the inputs of the k-means clustering problem. Let ε ∈ (0, 1/3) and assume access to a γ-approximation k-means algorithm. Run Algorithm 1 with inputs"
    }, {
      "heading" : "A, k, ε, and the γ-approximation algorithm in order to construct an indicator matrix Xγ̃ . Then with probability at",
      "text" : "least 0.97− δγ ,\n∥ ∥A−Xγ̃X⊤γ̃ A ∥ ∥ 2 F ≤ (1 + (1 + ε)γ) ∥ ∥A−XoptX⊤optA ∥ ∥ 2 F . (4)\nProof of Theorem 1\nThe proof of Theorem 1 employs several results from [19] including Lemma 6, 8 and Corollary 11. We summarize these results in Lemma 2 below. Before employing Corollary 11, Lemma 6, and Lemma 8 from [19] we need to make sure that the matrix R constructed in Algorithm 1 is consistent with Definition 1 and Lemma 5 in [19]. Theorem 1.1 of [1] immediately shows that the random sign matrix R of Algorithm 1 satisfies Definition 1 and Lemma 5 in [19].\nLemma 2. Assume that the matrix R is constructed by using Algorithm 1 with inputs A, k and ε.\n1. Singular Values Preservation: For all i ∈ [k] and w.p. at least 0.99, |1− σi(V ⊤k R)| ≤ ε.\n2. Matrix Multiplication: For any two matrices S ∈ Rn×d and T ∈ Rd×k,\nE [∥ ∥ST − SRR⊤T ∥ ∥ 2\nF\n]\n≤ 2 t ‖S‖2F ‖T ‖ 2 F .\n3. Moments: For any C ∈ Rn×d: E [ ‖CR‖2F ] = ‖C‖2F and Var [‖CR‖F] ≤ 2 ‖C‖ 4 F /t.\nThe first statement above assumes c being sufficiently large (see step 1 of Algorithm 1). We continue with several novel results of general interest.\n1Reading the input d × log d sign matrix requires O(d log d) time. However, in our case we only consider multiplication with a random sign matrix, therefore we can avoid the preprocessing step by directly computing a random correspondence matrix as discussed in [15, Preprocessing Section].\nLemma 3. Under the same assumptions as in Lemma 2 and w.p. at least 0.99, ∥ ∥ ∥(V ⊤k R)\n† − (V ⊤k R)⊤ ∥ ∥ ∥ 2 ≤ 3ε. (5)\nProof. Let Φ = V ⊤k R; note that Φ is a k× t matrix and the SV D of Φ is Φ = UΦΣΦV ⊤Φ , where UΦ and ΣΦ are k×k matrices, and VΦ is a t× k matrix. By taking the SVD of (V ⊤k R) † and (V ⊤k R)\n⊤ we get ∥ ∥ ∥(V ⊤k R)\n† − (V ⊤k R)⊤ ∥ ∥ ∥ 2 = ∥ ∥VΦΣ −1 Φ U ⊤ Φ − VΦΣΦU⊤Φ ∥ ∥ 2 = ∥ ∥VΦ(Σ −1 Φ − ΣΦ)U⊤Φ ∥ ∥ 2 = ∥ ∥Σ−1Φ − ΣΦ ∥ ∥ 2 ,\nsince VΦ and U⊤Φ can be dropped without changing any unitarily invariant norm. Let Ψ = Σ −1 Φ − ΣΦ; Ψ is a k × k diagonal matrix. Assuming that, for all i ∈ [k], σi(Φ) and τi(Ψ) denote the i-th largest singular value of Φ and the i-th diagonal element of Ψ, respectively, it is\nτi(Ψ) = 1− σ2i (Φ) σi(Φ) .\nSince Ψ is a diagonal matrix,\n‖Ψ‖2 = max1≤i≤k τi(Ψ) = max1≤i≤k 1− σ2i (Φ) σi(Φ) .\nThe first statement of Lemma 2, our choice of ε ∈ (0, 1/3), and elementary calculations suffice to conclude the proof.\nLemma 4. Under the same assumptions as in Lemma 2 and for any n× d matrix C w.p. at least 0.99, ‖CR‖F ≤ √ (1 + ε) ‖C‖F . (6)\nProof. Notice that there exists a sufficiently large constant c such that t ≥ ck/ε2. Then, setting Z = ‖CR‖2F, using the third statement of Lemma 2, the fact that k ≥ 1, and Chebyshev’s inequality we get\nP\n( |Z − E [Z] | ≥ ε ‖C‖2F ) ≤ Var [Z] ε2 ‖C‖4F ≤ 2 ‖C‖ 4 F tε2 ‖C‖4F ≤ 2 ck ≤ 0.01.\nThe last inequality follows assuming c sufficiently large. Finally, taking square root on both sides concludes the proof.\nLemma 5. Under the same assumptions as in Lemma 2 and w.p. at least 0.97,\nAk = (AR)(V ⊤ k R) † V ⊤k + E, (7)\nwhere E is an n× d matrix with ‖E‖F ≤ 4ε ‖A−Ak‖F.\nProof. Since (AR)(V ⊤k R) † V ⊤k is an n × d matrix, let us write E = Ak − (AR)(V ⊤k R) † V ⊤k . Then, setting A = Ak +Aρ−k, and using the triangle inequality we get\n‖E‖F ≤ ∥ ∥ ∥Ak −AkR(V ⊤k R) † V ⊤k ∥ ∥ ∥\nF +\n∥ ∥ ∥Aρ−kR(V ⊤ k R) † V ⊤k ∥ ∥ ∥\nF .\nThe first statement of Lemma 2 implies that rank(V ⊤k R) = k thus (V ⊤ k R)(V ⊤ k R) † = Ik, where Ik is the k×k identity matrix. Replacing Ak = UkΣkV ⊤k and setting (V ⊤ k R)(V ⊤ k R) † = Ik we get that\n∥ ∥ ∥Ak −AkR(V ⊤k R) † V ⊤k ∥ ∥ ∥\nF =\n∥ ∥ ∥Ak − UkΣkV ⊤k R(V ⊤k R) † V ⊤k ∥ ∥ ∥\nF =\n∥ ∥Ak − UkΣkV ⊤k ∥ ∥ F = 0.\nTo bound the second term above, we drop V ⊤k , add and subtract the matrix Aρ−kR(V ⊤ k R) ⊤V ⊤k , and use the triangle inequality and submultiplicativity:\n∥ ∥ ∥Aρ−kR(V ⊤ k R) † V ⊤k ∥ ∥ ∥\nF ≤\n∥ ∥Aρ−kR(V ⊤ k R) ⊤ ∥ ∥ F + ∥ ∥ ∥Aρ−kR((V ⊤ k R) † − (V ⊤k R)⊤) ∥ ∥ ∥\nF\n≤ ∥ ∥Aρ−kRR ⊤Vk ∥ ∥\nF + ‖Aρ−kR‖F ∥ ∥ ∥(V ⊤k R) † − (V ⊤k R)⊤ ∥ ∥ ∥ 2 .\nNow we will bound each term individually. A crucial observation for bounding the first term is that Aρ−kVk = Uρ−kΣρ−kV ⊤ ρ−kVk = 0 by orthogonality of the columns of Vk and Vρ−k. This term now can be bounded using the second statement of Lemma 2 with S = Aρ−k and T = Vk. This statement, assuming c sufficiently large, and an application of Markov’s inequality on the random variable ∥ ∥Aρ−kRR ⊤Vk −Aρ−kVk ∥ ∥\nF give that w.p. at least 0.99,\n∥ ∥Aρ−kRR ⊤Vk ∥ ∥\nF ≤ 0.5ε ‖Aρ−k‖F . (8)\nThe second two terms can be bounded using Lemma 3 and Lemma 4 on C = Aρ−k. Hence by applying a union bound on Lemma 3, Lemma 4 and Inq. (8), we get that w.p. at least 0.97,\n‖E‖F ≤ ∥ ∥Aρ−kRR ⊤Vk ∥ ∥ F + ‖Aρ−kR‖F ∥ ∥ ∥(V ⊤k R) † − (V ⊤k R)⊤ ∥ ∥ ∥ 2\n≤ 0.5ε ‖Aρ−k‖F + √\n(1 + ε) ‖Aρ−k‖F · 3ε ≤ 0.5ε ‖Aρ−k‖F + 3.5ε ‖Aρ−k‖F = 4ε · ‖Aρ−k‖F .\nThe last inequality holds thanks to our choice of ε ∈ (0, 1/3). Proposition 6. A well-known property connects the SVD of a matrix and k-means clustering. Recall Definition 1, and notice that XoptX⊤optA is a matrix of rank at most k. From the SVD optimality we immediately get that\n‖Aρ−k‖2F = ‖A−Ak‖ 2 F ≤ ∥ ∥A−XoptX⊤optA ∥ ∥ 2 F . (9)\n4.1 The proof of Eqn. (4) of Theorem 1\nWe start by manipulating the term ∥ ∥A−Xγ̃X⊤γ̃ A ∥ ∥ 2 F in Eqn. (4). Replacing A by Ak + Aρ−k , and using the Pythagorean theorem (the subspaces spanned by the components Ak − Xγ̃X⊤γ̃ Ak and Aρ−k − Xγ̃X⊤γ̃ Aρ−k are perpendicular) we get\n∥ ∥A−Xγ̃X⊤γ̃ A ∥ ∥ 2\nF =\n∥ ∥(I −Xγ̃X⊤γ̃ )Ak ∥ ∥ 2 F ︸ ︷︷ ︸\nθ21\n+ ∥ ∥(I −Xγ̃X⊤γ̃ )Aρ−k ∥ ∥ 2\nF ︸ ︷︷ ︸\nθ22\n. (10)\nWe first bound the second term of Eqn. (10). Since I − Xγ̃X⊤γ̃ is a projector matrix, it can be dropped without increasing a unitarily invariant norm. Now Proposition 6 implies that\nθ22 ≤ ‖Aρ−k‖ 2 F ≤ ∥ ∥A−XoptX⊤optA ∥ ∥ 2 F . (11)\nWe now bound the first term of Eqn. (10):\nθ1 ≤ ∥ ∥ ∥(I −Xγ̃X⊤γ̃ )AR(VkR)†V ⊤k ∥ ∥ ∥\nF + ‖E‖F (12)\n≤ ∥ ∥(I −Xγ̃X⊤γ̃ )AR ∥ ∥\nF\n∥ ∥ ∥(VkR) † ∥ ∥ ∥ 2 + ‖E‖F (13)\n≤ √γ ∥ ∥(I −XoptX⊤opt)AR ∥ ∥\nF\n∥ ∥ ∥(VkR) † ∥ ∥ ∥ 2 + ‖E‖F (14)\n≤ √γ √ (1 + ε) ∥ ∥(I −XoptX⊤opt)A ∥ ∥\nF\n1 1− ε + 4ε ∥ ∥(I −XoptX⊤opt)A ∥ ∥ F (15)\n≤ √γ(1 + 2.5ε) ∥ ∥(I −XoptX⊤opt)A ∥ ∥ F + √ γ 4ε ∥ ∥(I −XoptX⊤opt)A ∥ ∥ F (16) ≤ √γ(1 + 6.5ε) ∥ ∥(I −XoptX⊤opt)A ∥ ∥ F (17)\nIn Eqn. (12) we used Lemma 5, the triangle inequality, and the fact that I − X̃γX̃⊤γ is a projector matrix and can be dropped without increasing a unitarily invariant norm. In Eqn. (13) we used submultiplicativity (see Section 2.2) and the fact that V ⊤k can be dropped without changing the spectral norm. In Eqn. (14) we replaced Xγ̃ by Xopt and the factor √ γ appeared in the first term. To better understand this step, notice that Xγ̃ gives a γ-approximation to the optimal k-means clustering of the matrix AR, and any other n × k indicator matrix (for example, the matrix Xopt) satisfies\n∥ ∥ ( I −Xγ̃X⊤γ̃ ) AR ∥ ∥ 2 F ≤ γ min\nX∈X\n∥ ∥(I −XX⊤)AR ∥ ∥ 2\nF ≤ γ ∥ ∥ ( I −XoptX⊤opt ) AR ∥ ∥ 2 F .\nIn Eqn. (15) we used Lemma 4 with C = (I − XoptX⊤opt)A, Lemma 3 and Proposition 6. In Eqn. (16) we used the fact that γ ≥ 1 and that for any ε ∈ (0, 1/3) it is ( √ 1 + ε)/(1− ε) ≤ 1 + 2.5ε. Taking squares in Eqn. (17) we get\nθ21 ≤ γ(1 + 28ε) ∥ ∥(I −XoptX⊤opt)A ∥ ∥ 2\nF .\nFinally, rescaling ε accordingly and applying the union bound on Lemma 5 and Definition 2 concludes the proof."
    }, {
      "heading" : "5 Experiments",
      "text" : "This section describes an empirical evaluation of Algorithm 1 on a face images collection. We implemented our algorithm in MatLab and compared it against other prominent dimensionality reduction techniques such as the Local Linear Embedding (LLE) algorithm and the Laplacian scores for feature selection. We ran all the experiments on a Mac machine with a dual core 2.26 Ghz processor and 4 GB of RAM. Our empirical findings are very promising indicating that our algorithm and implementation could be very useful in real applications involving clustering of large-scale data."
    }, {
      "heading" : "5.1 An application of Algorithm 1 on a face images collection",
      "text" : "We experiment with a face images collection. We downloaded the images corresponding to the ORL database from [21]. This collection contains 400 face images of dimensions 64 × 64 corresponding to 40 different people. These images form 40 groups each one containing exactly 10 different images of the same person. After vectorizing each 2-D image and putting it as a row vector in an appropriate matrix, one can construct a 400 × 4096 image-by-pixel matrix A. In this matrix, objects are the face images of the ORL collection while features are the pixel values of the images. To apply the Lloyd’s heuristic on A, we employ MatLab’s function kmeans with the parameter determining the maximum number of repetitions setting to 30. We also chose a deterministic initialization of the Lloyd’s iterative E-M procedure, i.e. whenever we call kmeans with inputs a matrix Ã ∈ R400×d̃, with d̃ ≥ 1, and the integer k = 40, we initialize the cluster centers with the 1-st, 11-th,..., 391-th rows of Ã, respectively. Note that this initialization corresponds to picking images from the forty different groups of the available collection, since the images of every group are stored sequentially in A. We evaluate the clustering outcome from two different perspectives. First, we measure and report the objective function F of the k-means clustering problem. In particular, we report a normalized version of F , i.e. F̃ = F/||A||2F . Second, we report the mis-classification accuracy of the clustering result. We denote this number by P (0 ≤ P ≤ 1), where P = 0.9, for example, implies that 90% of the objects were assigned to the correct cluster after the application of the clustering algorithm. In the sequel, we first perform experiments by running Algorithm 1 with everything fixed but t, which denotes the dimensionality of the projected data. Then, for four representative values of t, we compare Algorithm 1 with three other dimensionality reduction methods as well with the approach of running the Lloyd’s heuristic on the original high dimensional data.\nWe run Algorithm 1 with t = 5, 10, ..., 300 and k = 40 on the matrix A described above. Figure 1 depicts the results of our experiments. A few interesting observations are immediate. First, the normalized objective function F̃ is a piece-wise non-increasing function of the number of dimensions t. The decrease in F̃ is large in the first few choices\nof t; then, increasing the number of dimensions t of the projected data decreases F̃ by a smaller value. The increase of t seems to become irrelevant after around t = 90 dimensions. Second, the mis-classification rate P is a piece-wise non-decreasing function of t. The increase of t seems to become irrelevant again after around t = 90 dimensions. Another interesting observation of these two plots is that the mis-classification rate is not directly relevant to the objective function F . Notice, for example, that the two have different behavior from t = 20 to t = 25 dimensions. Finally, we report the running time T of the algorithm which includes only the clustering step. Notice that the increase in the running time is - almost - linear with the increase of t. The non-linearities in the plot are due to the fact that the number of iterations that are necessary to guarantee convergence of the Lloyd’s method are different for different values of t. This observation indicates that small values of t result to significant computational savings, especially when n is large. Compare, for example, the one second running time that is needed to solve the k-means problem when t = 275 against the 10 seconds that are necessary to solve the problem on the high dimensional data. To our benefit, in this case, the multiplication AR takes only 0.1 seconds resulting to a total running time of 1.1 seconds which corresponds to an almost 90% speedup of the overall procedure.\nWe now compare our algorithm against other dimensionality reduction techniques. In particular, in this paragraph we present head-to-head comparisons for the following five methods: (i) SVD: the Singular Value Decomposition (or Principal Components Analysis) dimensionality reduction approach - we use MatLab’s svds function; (ii) LLE: the famous Local Linear Embedding algorithm of [18] - we use the MatLab code from [23] with the parameter K determining the number of neighbors setting equal to 40; (iii) LS: the Laplacian score feature selection method of [10] - we use the MatLab code from [22] with the default parameters2; (v) HD: we run the k-means algorithm on the High Dimensional data; and (vi) RP: the random projection method we proposed in this work - we use our own MatLab implementation. The results of our experiments on A, k = 40 and t = 10, 20, 50, 100 are shown in Table 2. In terms of computational complexity, for example t = 50, the time (in seconds) needed for all five methods (only the dimension reduction step) are TSVD = 5.9, TLLE = 4.4, TLS = 0.32, THD = 0, and TRP = 0.03. Notice that our algorithm is much faster than the other approaches while achieving worse (t = 10, 20), slightly worse (t = 50) or slightly better (t = 100) approximation accuracy results."
    }, {
      "heading" : "5.2 A note on the mailman algorithm for matrix-matrix and matrix-vector multiplication",
      "text" : "In this section, we compare three different implementations of the third step of Algorithm 1. As we already discussed in Section 3.1, the mailman algorithm is asymptotically faster than naively multiplying the two matrices A and R. In this section we want to understand whether this asymptotic behavior of the mailman algorithm is indeed achieved in a practical implementation. We compare three different approaches for the implementation of the third step of our algorithm: the first is MatLab’s function times(A,R) (MM1); the second exploits the fact that we do not need to explicitly store the whole matrix R, and that the computation can be performed on the fly (column-by-column) (MM2); the last is the mailman algorithm [15] (see Section 3.1 for more details). We implemented the last two algorithms in C using MatLab’s MEX technology. We observed that when A is a vector (n = 1), then the mailman algorithm is indeed faster than (MM1) and (MM2) as it is also observed in the numerical experiments of [15]. Moreover, it’s worth-noting that (MM2) is also superior compared to (MM1). On the other hand, our best implementation of the mailman algorithm for matrix-matrix operations is inferior to both (MM1) and (MM2) for any 10 ≤ n ≤ 10, 000. Based on these findings, we chose to use (MM1) for our experimental evaluations.\nAcknowledgments: Christos Boutsidis was supported by NSF CCF 0916415 and a Gerondelis Foundation Fellowship; Petros Drineas was partially supported by an NSF CAREER Award and NSF CCF 0916415.\n2In particular, we run W = constructW (A); Scores = LaplacianScore(A,W );"
    } ],
    "references" : [ {
      "title" : "Database-friendly random projections: Johnson-Lindenstrauss with binary coins",
      "author" : [ "D. Achlioptas" ],
      "venue" : "Journal of Computer and System Science,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2003
    }, {
      "title" : "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform",
      "author" : [ "N. Ailon", "B. Chazelle" ],
      "venue" : "In ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2006
    }, {
      "title" : "NP-hardness of Euclidean sum-of-squares clustering",
      "author" : [ "D. Aloise", "A. Deshpande", "P. Hansen", "P. Popat" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Random projection in dimensionality reduction: applications to image and text data",
      "author" : [ "E. Bingham", "H. Mannila" ],
      "venue" : "In ACM SIGKDD international conference on Knowledge discovery and data mining (KDD),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2001
    }, {
      "title" : "Unsupervised feature selection for the k-means clustering problem",
      "author" : [ "C. Boutsidis", "M.W. Mahoney", "P. Drineas" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Clustering in large graphs and matrices",
      "author" : [ "P. Drineas", "A. Frieze", "R. Kannan", "S. Vempala", "V. Vinay" ],
      "venue" : "In ACM- SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1999
    }, {
      "title" : "An optimal set of discriminant vectors",
      "author" : [ "D. Foley", "J. Sammon" ],
      "venue" : "IEEE Transactions on Computers,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1975
    }, {
      "title" : "An introduction to variable and feature selection",
      "author" : [ "I. Guyon", "A. Elisseeff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2003
    }, {
      "title" : "Result analysis of the NIPS 2003 feature selection challenge",
      "author" : [ "I. Guyon", "S. Gunn", "A. Ben-Hur", "G. Dror" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2005
    }, {
      "title" : "Laplacian score for feature selection",
      "author" : [ "X. He", "D. Cai", "P. Niyogi" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2006
    }, {
      "title" : "Approximate nearest neighbors: towards removing the curse of dimensionality",
      "author" : [ "P. Indyk", "R. Motwani" ],
      "venue" : "In ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1998
    }, {
      "title" : "Extensions of Lipschitz mappings into a Hilbert space",
      "author" : [ "W. Johnson", "J. Lindenstrauss" ],
      "venue" : "Contemporary mathematics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1984
    }, {
      "title" : "A simple linear time (1+ε)-approximation algorithm for k-means clustering in any dimensions",
      "author" : [ "A. Kumar", "Y. Sabharwal", "S. Sen" ],
      "venue" : "In IEEE Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2004
    }, {
      "title" : "The Mailman algorithm: A note on matrix-vector multiplication",
      "author" : [ "E. Liberty", "S. Zucker" ],
      "venue" : "Information Processing Letters,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2009
    }, {
      "title" : "Least squares quantization in PCM",
      "author" : [ "S. Lloyd" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1982
    }, {
      "title" : "The effectiveness of Lloyd-type methods for the k-means problem",
      "author" : [ "R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy" ],
      "venue" : "In IEEE Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2006
    }, {
      "title" : "Nonlinear dimensionality reduction by locally linear embedding",
      "author" : [ "S. Roweis", "L. Saul" ],
      "venue" : "Science, 290:5500,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2000
    }, {
      "title" : "Improved approximation algorithms for large matrices via random projections",
      "author" : [ "T. Sarlos" ],
      "venue" : "In IEEE Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Top 10 algorithms in data mining",
      "author" : [ "X. Wu" ],
      "venue" : "Knowledge and Information Systems,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "The k-means clustering algorithm [16] was recently recognized as one of the top ten data mining tools of the last fifty years [20].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 18,
      "context" : "The k-means clustering algorithm [16] was recently recognized as one of the top ten data mining tools of the last fifty years [20].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : "In parallel, random projections (RP) or the so-called Johnson-Lindenstrauss type embeddings [12] became popular and found applications in both theoretical computer science [2] and data analytics [4].",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 1,
      "context" : "In parallel, random projections (RP) or the so-called Johnson-Lindenstrauss type embeddings [12] became popular and found applications in both theoretical computer science [2] and data analytics [4].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 3,
      "context" : "In parallel, random projections (RP) or the so-called Johnson-Lindenstrauss type embeddings [12] became popular and found applications in both theoretical computer science [2] and data analytics [4].",
      "startOffset" : 195,
      "endOffset" : 198
    }, {
      "referenceID" : 8,
      "context" : "We believe that the high dimensionality of modern data will render our algorithm useful and attractive in many practical applications [9].",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 5,
      "context" : "Description Dimensions Time Accuracy 1999 [6] SVD - feature extraction k O(ndmin{n, d}) 2 - Folklore RP - feature extraction Ω(log(n)/ε) O(nd⌈ε−2 log(n)/ log(d)⌉) 1 + ε 2009 [5] SVD - feature selection Ω(k log(k/ε)/ε) O(ndmin{n, d}) 2 + ε 2010 This paper RP - feature extraction Ω(k/ε) O(nd⌈ε−2k/ log(d)⌉) 2 + ε",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 4,
      "context" : "Description Dimensions Time Accuracy 1999 [6] SVD - feature extraction k O(ndmin{n, d}) 2 - Folklore RP - feature extraction Ω(log(n)/ε) O(nd⌈ε−2 log(n)/ log(d)⌉) 1 + ε 2009 [5] SVD - feature selection Ω(k log(k/ε)/ε) O(ndmin{n, d}) 2 + ε 2010 This paper RP - feature extraction Ω(k/ε) O(nd⌈ε−2k/ log(d)⌉) 2 + ε",
      "startOffset" : 174,
      "endOffset" : 177
    }, {
      "referenceID" : 9,
      "context" : "Finally, we should point out that other techniques, for example the Laplacian scores [10] or the Fisher scores [7], are very popular in applications (see also surveys on the topic [8, 13]).",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "Finally, we should point out that other techniques, for example the Laplacian scores [10] or the Fisher scores [7], are very popular in applications (see also surveys on the topic [8, 13]).",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 7,
      "context" : "Finally, we should point out that other techniques, for example the Laplacian scores [10] or the Fisher scores [7], are very popular in applications (see also surveys on the topic [8, 13]).",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 2,
      "context" : "Finding Xopt is an NP-hard problem even for k = 2 [3], thus research has focused on developing approximation algorithms for k-means clustering.",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 12,
      "context" : "For our discussion, we fix the γ-approximation algorithm to be the one presented in [14], which guarantees γ = 1+ ε for any ε ∈ (0, 1] with running time O(2(k/ε′)O(1)dn).",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "A classical result of Johnson and Lindenstrauss states that any n-point set in d dimensions - rows in a matrix A ∈ R - can be linearly projected into t = Ω(log(n)/ε) dimensions while preserving pairwise distances within a factor of 1±ε using a random orthonormal matrix [12].",
      "startOffset" : 270,
      "endOffset" : 274
    }, {
      "referenceID" : 10,
      "context" : "Gaussian random variables with zero mean and variance 1/ √ t [11].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "Achlioptas proved that even a (rescaled) random sign matrix suffices in order to get the same guarantees as above [1], an approach that we adopt here (see step two in Algorithm 1).",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "If R is constructed as in Algorithm 1, one can employ the so-called mailman algorithm for matrix multiplication [15] and",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 12,
      "context" : "Using, for example, the algorithm of [14] with γ = 1 + ε would result in an algorithm that preserves the clustering within a factor of 2 + ε, for any ε ∈ (0, 1/3), running in time O(nd⌈ε−2k/ log(d)⌉ + 2(k/ε)O(1)kn/ε2).",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : "In practice though, the Lloyd algorithm [16, 17] is very popular and although it does not admit a worst case theoretical analysis, it empirically does well.",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 15,
      "context" : "In practice though, the Lloyd algorithm [16, 17] is very popular and although it does not admit a worst case theoretical analysis, it empirically does well.",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 17,
      "context" : "The proof of Theorem 1 employs several results from [19] including Lemma 6, 8 and Corollary 11.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 17,
      "context" : "Before employing Corollary 11, Lemma 6, and Lemma 8 from [19] we need to make sure that the matrix R constructed in Algorithm 1 is consistent with Definition 1 and Lemma 5 in [19].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : "Before employing Corollary 11, Lemma 6, and Lemma 8 from [19] we need to make sure that the matrix R constructed in Algorithm 1 is consistent with Definition 1 and Lemma 5 in [19].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 0,
      "context" : "1 of [1] immediately shows that the random sign matrix R of Algorithm 1 satisfies Definition 1 and Lemma 5 in [19].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 17,
      "context" : "1 of [1] immediately shows that the random sign matrix R of Algorithm 1 satisfies Definition 1 and Lemma 5 in [19].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 16,
      "context" : "In particular, in this paragraph we present head-to-head comparisons for the following five methods: (i) SVD: the Singular Value Decomposition (or Principal Components Analysis) dimensionality reduction approach - we use MatLab’s svds function; (ii) LLE: the famous Local Linear Embedding algorithm of [18] - we use the MatLab code from [23] with the parameter K determining the number of neighbors setting equal to 40; (iii) LS: the Laplacian score feature selection method of [10] - we use the MatLab code from [22] with the default parameters2; (v) HD: we run the k-means algorithm on the High Dimensional data; and (vi) RP: the random projection method we proposed in this work - we use our own MatLab implementation.",
      "startOffset" : 302,
      "endOffset" : 306
    }, {
      "referenceID" : 9,
      "context" : "In particular, in this paragraph we present head-to-head comparisons for the following five methods: (i) SVD: the Singular Value Decomposition (or Principal Components Analysis) dimensionality reduction approach - we use MatLab’s svds function; (ii) LLE: the famous Local Linear Embedding algorithm of [18] - we use the MatLab code from [23] with the parameter K determining the number of neighbors setting equal to 40; (iii) LS: the Laplacian score feature selection method of [10] - we use the MatLab code from [22] with the default parameters2; (v) HD: we run the k-means algorithm on the High Dimensional data; and (vi) RP: the random projection method we proposed in this work - we use our own MatLab implementation.",
      "startOffset" : 478,
      "endOffset" : 482
    }, {
      "referenceID" : 13,
      "context" : "We compare three different approaches for the implementation of the third step of our algorithm: the first is MatLab’s function times(A,R) (MM1); the second exploits the fact that we do not need to explicitly store the whole matrix R, and that the computation can be performed on the fly (column-by-column) (MM2); the last is the mailman algorithm [15] (see Section 3.",
      "startOffset" : 348,
      "endOffset" : 352
    }, {
      "referenceID" : 13,
      "context" : "We observed that when A is a vector (n = 1), then the mailman algorithm is indeed faster than (MM1) and (MM2) as it is also observed in the numerical experiments of [15].",
      "startOffset" : 165,
      "endOffset" : 169
    } ],
    "year" : 2013,
    "abstractText" : "This paper discusses the topic of dimensionality reduction for k-means clustering. We prove that any set of n points in d dimensions (rows in a matrix A ∈ R) can be projected into t = Ω(k/ε) dimensions, for any ε ∈ (0, 1/3), in O(nd⌈ε−2k/ log(d)⌉) time, such that with constant probability the optimal k-partition of the point set is preserved within a factor of 2 + ε. The projection is done by post-multiplying A with a d × t random matrix R having entries +1/ √ t or −1/ √ t with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.",
    "creator" : "LaTeX with hyperref package"
  }
}