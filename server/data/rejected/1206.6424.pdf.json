{
  "name" : "1206.6424.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Anytime Marginal Maximum a Posteriori Inference",
    "authors" : [ "Denis Deratani Mauá", "Cassio Polpo de Campos" ],
    "emails" : [ "denis@idsia.ch", "cassio@idsia.ch" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The maximum a posteriori (MAP) assignment problem consists in finding an assignment that maximizes the posterior probability of a given set of variables. To facilitate modeling, the model often includes latent variables that are neither to be maximized nor observed, but marginalized. It is this more general form of the problem (a.k.a. partial or marginal MAP) that we tackle in this paper. Moreover, we assume that the probability distribution is represented as a discrete graphical model, which allows for compactness.\nComputationally, this is a very hard problem. It is NPPP-hard even if all variables are binary, and NP-hard if either the underlying graph has bounded treewidth or there are no latent variables (Park & Darwiche, 2004). Also producing a provably good approximate solution is NP-hard, even if the treewidth of the underlying graph is bounded (Park & Darwiche, 2004). A positive result has recently been given by de Campos (2011), which derived a fully polynomial-time approximation scheme when both treewidth and number of states per variable are bounded.\nMAP assignment problems can be seen as a composition of two different tasks: the computation of marginal probabilities and the combinatorial search\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nover assignments. The former is responsible for evaluating the quality of a candidate assignment produced by the latter. When the marginal probability inference is tractable, standard combinatorial search approaches such as branch-and-bound for exact solutions and local search for approximate results have been used (Park & Darwiche, 2003; Yuan et al., 2004). When it is hard, researchers have resorted to loopy belief propagation algorithms (Liu & Ihler, 2011; Jiang et al., 2011) and factor decomposition (Meek & Wexler, 2011).\nIn this paper, we present a new anytime algorithm to perform marginal MAP inference in graphical models of bounded treewidth. The algorithm implements a clique-tree propagation scheme that propagates sets of factors instead of single factors. Efficiency is achieved by verifying sub-optimality locally. We show empirically that the algorithm compares well to the systematic search algorithm of Park & Darwiche (2003). We derive theoretical bounds for the error produced by the algorithm within a given amount of computational resources (time and memory), and show that this error can be made arbitrarily small with enough resources."
    }, {
      "heading" : "2. Notation",
      "text" : "A finite integer set {1, 2, . . . , n} is denoted by [n]. Random variables are represented by capital letters, e.g., X, Y , Z; real-valued functions by greek letters, e.g., φ, ψ; sets by calligraphic letters, e.g., I=[3], P={φ, ψ}, S = {I,P}; vectors in boldface, e.g., V = (X,Y, Z). The number of elements in a set X is denoted by |X |. We identify a variable with its sample space. Hence, the finite set of values a variable X can assume is also denoted by X. Given a vector of variables X = (X1, . . . , Xn), we write X =X1 ×· · · × Xn to denote the space of configurations or assignments of the variables in X, where × denotes the Cartesian product. We also identify a vector of variables to its joint sample space, so that the notation x ∈ X is welldefined, and |X| denotes the number of assignments x\nto X and not the number of variables in the vector. For X = (X1, . . . , Xn) and I ⊆ [n], the notation XI denotes the vector (Xi)i∈I . We write xI to denote the vector (xi)i∈I obtained by projecting x ∈ X onto XI .\nA factor φ over a vector of variables X=(X1, . . . , Xn) is a |X|-dimensional vector of non-negative real values. The value of the factor corresponding to a particular assignment x ∈ X is denoted by φ(x). Given an assignment z for a vector of variables X, the indicator factor δz assigns value one for x=z and zero for all others. Product and sum-marginalization of factors are defined as usual: (φψ)(x) = φ(xI)ψ(xJ ), for X=(X1, . . . , Xn), I∪J =[n], φ defined over XI and ψ defined over XJ ; (∑ XI φ ) (y)= ∑\nx∈X φ(x)δy(xI) for φ defined over X=(X1, . . . , Xn), I ⊆ [n] and y ∈ XI .\nGiven a tree T over [n] and a root node r ∈ [n], we say that a node p is the parent of a neighboring node i if p is closer to r than i, in which case, we say that i is a child of p. The parent and the children of a node i are denoted by pa(i) and ch(i), respectively. The set of descendants of i (i.e., its children, the children of its children, and so on) is denoted by de(i). Nodes containing no children are called leaves, and nodes containing at least one child are called internal."
    }, {
      "heading" : "3. Graphical Models",
      "text" : "Let X = (X1, . . . , Xn) be a vector of discrete random variables, J1, . . . ,Jm be a collection of index sets satisfying J1 ∪· · · ∪ Jm=[n], and P={φ1, . . . , φm} be a set of factors over vectors XJ1 , . . . ,XJm , respectively. We call P a graphical model if it specifies a joint probability distribution over assignments x ∈ X by\nPr(X=x) = 1\nZ ∏ i∈[m] φi(xJi) ,\nwhere Z = ∑\nX ∏ φ∈P φ is a normalizing constant\nknown as the partition function. The graph in the left-hand side of Figure 1 depicts a graphical model.\nLet D and H be index sets partitioning the variables into decision and latent variables, respectively. The MAP assignment problem consists in finding\nd∗ = argmax d∈XD Pr(XD=d)\n= argmax d∈XD ∑ h∈XH Pr(XD=d,XH=h)\n= argmax d∈XD ∑ X ∏ i∈[m] φi ∏ j∈D δdj . (1)\nFor each fixed assignment d, we can represent the factorization in (1) by a new graphical model Pd =P ∪\n⋃ j∈D{δdj}. The partition function of this new model\nsatisfies Zd = ∑\nX ∏ i∈[m] φi ∏ j∈D δdj . This way, we\ncan re-state the MAP assignment problem as a search over graphical models Pd. Assume without loss of generality that D={1, . . . , d} and H={d+ 1, . . . , n}, and define Ki = {φi} for i = 1, . . . ,m, and Ki+m = {δxi : xi ∈ Xi} for each decision i ∈ D. Each combination of factors φ1, . . . , φm+d from sets K1, . . . ,Km+d, respectively, specifies the graphical model Pd corresponding to an assignment d. Let M= {{φ1, . . . , φm+d} : φi ∈ Ki} denote all graphical models obtained in such a way. Finding a MAP assignment is equivalent to finding a graphical model P∗= argmaxP∈M ∑ X ∏ φ∈P φ. An assignment d∗ is a MAP assignment iff it satisfies d∗=argmaxd∈XD ∏m+d i=m+1 φi(di) for some optimal P∗.\nExample 1. Consider the graphical model in Figure 1, and assume that variables are binary, D={1, 2} and H= {3, 4}. We denote the values a binary variable Xi can assume by xi and x̃i, and reformulate this MAP assignment problem as a search over graphical models as follows. Let K1 = {φ1}, K2 = {φ2}, K3 = {φ3}, K4 = {φ4}, K5 = {φ5}, K6 = {φ6}, K7 = {φ7}, K8 = {φ8}, K9 = {δx1 , δx̃1} and K10 = {δx2 , δx̃2}. Each combination of factors φ1, . . . , φ10 ∈ K1, . . . ,K10 corresponds to the graphical model induced by the assignment d = argmaxx φ9(x1)φ10(x2). Suppose that P∗ = {φ1, . . . , φ8, δx1 , δx̃2} is a solution to argmaxP∈M ∑ X1,X2,X3,X4 ∏ i∈[10] φi. Then d ∗ = (x1, x̃2) = argmaxd δx1(d1)δx̃2(d2) is a MAP assignment."
    }, {
      "heading" : "4. Clique-Tree Computation",
      "text" : "Let T be a tree over [m], I1, . . . , Im be a collection of index sets satisfying I1 ∪ · · · ∪ Im = [n] for some positive integer n. We call T a clique tree if for i = 1, . . . , n the subgraph obtained by removing from T all nodes j such that i /∈ Ij remains a tree. Clique trees\nare so called because the index sets usually represent the cliques in the triangulated underlying graph of a graphical model. Let P be a graphical model whose factors φ1, . . . , φk are defined over sets XJ1 , . . . ,XJk , respectively, and J1∪· · ·∪Jk=[n]. We say that T is a clique tree for P if for i = 1, . . . , k there is 1 ≤ j ≤ m such that Ji ⊆ Ij . In the following, we assume for ease of exposition and without loss of generality that if T is a clique tree for P then m = k and Ji ⊆ Ii for all i, which allows us to unambiguously associate each factor φi to the clique tree node i. The tree on the right-hand side in Figure 1 is a clique tree for the graphical model on the left.\nThe width of a clique tree is the cardinality of the largest index set minus one. For example, the width of the tree in Figure 1 is two. Since the complexity of algorithms that operate on clique trees is (at least) exponential in the tree width, one usually seeks to obtain a clique tree of low width. Finding a minimum-width clique tree for a given graphical model is an NP-hard problem, and one usually resorts to heuristics to obtain low-width trees.\nThe basic computation scheme with clique trees is the factor-elimination procedure in Algorithm 1, which computes the partition function of a graphical model P = {φ1, . . . , φm} associated to a clique tree T over [m].1 In the algorithm, we assume that each factor φi is assigned to node i in the clique tree (hence its associated index set Ji ⊆ Ii). In a nutshell, the algorithm roots the tree in an arbitrary node r, and then propagates messages from the leaves towards the root. For ease of exposition, we assume in line 5 that Ipa(r) =∅. The propagation of messages halts when the root receives a message from every child, in which case the partition function is obtained by Z =µr. The algorithm runs in O(msw+1) time, where s= maxi |Xi| is the maximum number of values a variable in the model can assume, and w=maxi |Ii| − 1 is the width of the clique tree. Thus when the width w is bounded, the computations take polynomial time.\nLet h(i) = ⋃ j∈de(i)∪{i} Ij \\ Ipa(i). It can be shown that for i = 1, . . . ,m the factor µi satisfies µi = ∑ Xh(i) φi ∏ j∈de(i) φj (Koller & Friedman, 2009). Since h(r) = [n] by definition of clique trees, the correctness of the computations follows easily by applying this result to the root: Z=µr= ∑ X ∏ i∈[m] φi. Hence, we can evaluate the quality of a candidate solution d to the MAP assignment problem by building a clique tree T for the corresponding graphical model Pd and then running factor-elimination, which produces\n1The name collect algorithm has also been used to describe the same algorithm.\nAlgorithm 1 factor-elimination Require: A clique tree T over a graphical model P Ensure: Z= ∑ X ∏ φ∈P φ\n1: select a node r as root 2: label all nodes as inactive 3: while there is an inactive node i do 4: select an inactive node i with all children active 5: compute µi= ∑ XIi\\Ipa(i) φi ∏ j∈ch(i) µj 6: label i as active 7: end while 8: Z=µr\nZd = ∑\nX ∏ φ∈Pd φ. Note that the same clique tree\ncan be used to evaluate different candidates.\nExample 2. Consider the graphical model and clique tree in Figure 1 and assume again that variables are binary D = {1, 2} and H = {3, 4}. We can evaluate the assignment d = (x1, x̃2) to (X1, X2) by replacing φ1 and φ2 with φ ′ 1 = φ1δx1 and φ ′ 2 = φ2δx̃2 , respectively, and then running factor-elimination, which obtains Z(x1,x̃2) = ∑ X1,X2,X3,X4 ∏8 i=1 φiδx1δx̃2 ∝ Pr(X1 =x1, X2 = x̃2).\nThe algorithm can be straightforwardly modified to find a MAP assignment when there are no latent variables (i.e., when H = ∅) by substituting sums with maximizations in the computation of factors µi (Koller & Friedman, 2009). This naturally suggests an approach to the computation of MAP assignments in the presence of latent variables (i.e., when H 6= ∅), which consists in redefining the factors µi so that latent variables are summed out while decision variables are maximized. A factor-max-elimination version of the algorithm thus obtains factors µi = maxXDi ∑ XHi φi ∏ j∈ch(i) µj , where Di = (Ii ∩ D) \\ Ipa(i) and Hi=(Ii ∩ H) \\ Ipa(i). Variants of this procedure have recently been justified as an approximation by variational inference (Liu & Ihler, 2011; Jiang et al., 2011). These approaches retain the efficiency of message-passing algorithms, but produce only an upper bound to the real value, unless the root node r contains all decision variables. Enforcing the clique tree to contain a node over all decision variables results in an exponential complexity in the number of decision variables (Park & Darwiche, 2004), unless the factors in the root node are factorized (Meek & Wexler, 2011).\nAnother simple but often effective approach to the MAP assignment problem is to perform a search over the space of assignments, and to use factorelimination to evaluate candidate solutions. An upper bound for any partial assignment can be obtained by running factor-max-elimination, which poten-\nAlgorithm 2 factor-set-elimination Require: A clique tree T over the sets of factors K1, . . . ,Km and positive integers k1, . . . , km Ensure: Zl ≤ Z∗ ≤ Zu 1: select a node r as root and let σ be an empty\ndictionary 2: for all leaf node i do 3: let Mi be an empty set 4: for all φi ∈ Ki do 5: add µi= ∑ XIi\\Ipa(i)\nφi to Mi 6: set σ(µi)← µi 7: end for 8: Li=prune(Mi, σi, ki) 9: end for\n10: label leaves as active and internal nodes as inactive 11: while there is an inactive node do 12: select an inactive node i whose children are all active 13: let Mi be empty sets 14: for all φi ∈ Ki, µj ∈ Lj , j ∈ ch(i) do 15: add µi= ∑ XIi\\Ipa(i) φi ∏ j∈ch(i) µj to Mi\n16: set σ(µi)← ∑ XIi\\Ipa(i) φi ∏ j∈ch(i) σ(µj) 17: end for 18: Li=prune(Mi, σ, ki) 19: label i as active 20: end while 21: Zl=max{µr : µr ∈ Lr} 22: Zu=max{σ(µr) : µr ∈ Lr}\ntially narrows the search space. The algorithm of Park & Darwiche (2003), against which we compare the algorithm we devise here, builds on this idea."
    }, {
      "heading" : "5. Propagating Sets",
      "text" : "Recall from the previous section that we can compare the quality of different candidate solutions to the MAP assignment problem by running factor-elimination with the same clique tree structure but different indicator factors. More generally, let K1, . . . ,Km be a collection of sets of factors such that each P = {φ1, . . . , φm} obtained by selecting a factor φi from Ki, i = 1, . . . ,m, is a graphical model. Let P be a graphical model obtained in this way, and let T be a clique tree for this model. Then T is also a clique tree for any other graphical model induced byK1, . . . ,Km. This insight is the base of the factor-set-elimination procedure in Algorithm 2, which performs a search over the space of assignments while it propagates sets of factors over the clique tree.\nThe algorithm resembles factor-elimination, but\ninstead of propagating factors µi, it propagates sets of factors Li ⊆Mi={ ∑ XIi\\Ipa(i) φi ∏ j∈ch(i) µj : φi ∈ Ki, µj ∈ Lj}. The elements σ(µi) obtained in lines 6 and 16 are local upper bounds which we discuss later on. The pruning operations in lines 8 and 18 return a subset Li ⊆ Mi of cardinality ki and recompute the upper bounds σ(µi) to account for the discarded elements. So, if ki ≥ |Mi|, then the pruning operation returns Li=Mi. The algorithm outputs lower and upper bounds Zl and Zu, respectively, to the maximum partition function Z∗= max{ ∑ X ∏ i∈[m] φi : φi ∈ Ki} of a graphical model induced by the sets in the input. The following result shows the correspondence of factors µi computed by this algorithm to those computed with factor-elimination.\nTheorem 1. For i = 1, . . . ,m, any µi ∈ Li satisfies µi = ∑ Xh(i) φi ∏ j∈de(i) φj for some combination of φi ∈ Ki and φj ∈ Kj for all j ∈ de(i).\nProof. First, note that the definition of µi in factorset-elimination is identical to the definition in factor-elimination. Assume the prunining operations are not performed, that is, that prune(Mi, σi, ki) returns Mi. Then it is not difficult to see that µi matches the computation in factor-elimination for some graphical model induced by K1, . . . ,Km. But since the pruning operation returns a subset of Mi, this holds also for any µi ∈ Li.\nThe following result follows immediately from the above theorem.\nCorollary 1. Zl = ∑\nX ∏ i∈[m] φi for some combina-\ntion of factors (φ1, . . . , φm) ∈ K1 × · · · × Km.\nIf the algorithm is run with factor sets K1, . . . ,Km that induce graphical models corresponding to different assignments to decision variables as explained in Section 3, the numbers Zl and Zu returned are lower and upper bounds for the MAP assignment probability Z∗=maxd Pr(XD=d). In fact, if ki= |Mi| for all i = 1, . . . ,m, the algorithm is equivalent to an exhaustive search over the space of assignments, and thus returns Zl = Z\n∗. Moreover, the value of Zl is actually achieved by some assignment, and hence denotes the value of a feasible solution. The assignment corresponding to Zl can be obtained by tracking back the indicator factors δi, i ∈ D, that were propagated to generate the number µr=Zl.\nThe complexity of the algorithm is determined by the number of additions and multiplications needed to compute each factor µi in a set Mi plus the complexity of the pruning operation. Similarly to factorelimination, the complexity of computing each µi\nis O(msw+1). Let k be the maximum of k1, . . . , km and |K1|, . . . , |Km|. By design, each set Mi contains |Ki| ∏ j∈ch(i) |Lj | = |Ki| ∏ j∈ch(i) kj ≤ kc elements, where c is the maximum number of neighbors of a node. Hence, the algorithm runs in O(kcmsw). If the clique tree given as input contains a bounded number of children for each node and bounded width, the algorithm runs in time polynomial in the inputs k1, . . . , km and K1, . . . ,Km. Note that for any given any graphical model of bounded treewidth we can obtain a clique tree of bounded width and bounded number of children per node (e.g., a binary clique tree)."
    }, {
      "heading" : "5.1. Pruning",
      "text" : "The pruning operations are responsible for reducing the size of the propagated sets, enabling efficient inference. The trade-off between the quality of the solution and the computation time is determined by the parameters k1, . . . , km in the input. In the following, we discuss how the pruning operations are implemented.\nConsider a set of factors µ (1) i , . . . , µ (k) i which we wish to discard to reduce the size of a set Mi produced during factor-set-elimination. Our first insight is that convex combinations can be safely removed, as they are certainly outperformed by some extrema.\nA factor µ (1) i is a convex combination of factors µ (2) i and µ (3) i if there is a real 0 ≤ λ ≤ 1 such that µ (1) i = λµ (2) i + (1 − λ)µ (3) i . Given a set of factors Mi, we say that µi ∈ Mi is an extreme if it is not a convex combination of any two other elements in the set. Nonextreme factors can be safely removed fromMi, as the following result shows.\nTheorem 2. Let µ (1) i , µ (2) i and µ (3) i be three different factors in a set Mi such that µ(1)i is a convex combination of µ\n(2) i and µ (3) i . Then any solution value µ (1) r\ndifferent from µ (2) r and µ (3) r , where µ (`) r is obtained by propagating µ (`) i up to the root, is not an optimal solution.\nProof. Let µ (1) j = ∑ XIj\\Ip φjµ (1) i ∏ k∈ch(j)\\{i} µk, µ (2) j = ∑ XIj\\Ip φjµ (2) i ∏ k∈ch(j)\\{i} µk and µ\n(3) j =∑\nXIj\\Ip φjµ\n(3) i ∏ k∈ch(j)\\{i} µk be factors in Mj ,\nwhere j = pa(i) and p = pa(j). Then µ (1) j is a convex combination of µ (2) j and µ (3) j . By induction in the nodes of the clique tree, we find that any number µ (1) r ∈Mr obtained by propagating µ(1)i up to the root is a convex combination of numbers µ (2) r and µ (3) r obtained by propagating µ (2) i and µ (3) i , respectively, up\nto the root. Hence, µ (1) r is necessarily (strictly) less than max{µ(2)r , µ(3)r }, which is less than or equal to the optimal solution Z∗.\nThere is also another condition between factors which if verified allows us to safely discard a factor from Mi. Let µ(1)i and µ (2) i be two factors in Mi. We say that µ (2) i (weakly Pareto-)dominates µ (1) i , and write µ (2) i ≥ µ (1) i , if µ (2) i (x) ≥ µ (1) i (x) for all x ∈ XIi∩Ipa(i) . As the following result shows, we can safely remove dominated factors.\nTheorem 3. Let µ (1) i and µ (2) i be two different factors in a set Mi such that µ(2)i ≥ µ (1) i . Then any solution µ (1) r 6=µ(2)r , where µ(`)r is obtained by propagating µ(`)i up to the root, is not an optimal solution.\nProof. Let µ (1) j = ∑ XIj\\Ip φjµ (1) i ∏ k∈ch(j)\\{i} µk and µ (2) j = ∑ XIj\\Ip φjµ (2) i ∏ k∈ch(j)\\{i} µk be factors in Mj , where j = pa(i) and p = pa(j). Since the factors contain only nonnegative values, it follows that µ (2) j ≥ µ (1) j . By induction in the nodes of the clique tree, we find that any number µ (1) r ∈ Mr generated by propagating µ (1) i up to the root is dominated by a number µ (2) r obtained by propagating µ (2) i , and therefore (strictly) less than the optimal solution Z∗.\nThe pruning operation prune(Mi, σ, ki) first discards non-extreme and dominated factors from Mi. Albeit accurate, these operations are seldom enough to produce a set Li whose cardinality is less than the desired ki. To be able to meet the cardinality constraint, we partition the remaining factors in Mi (after nonextreme and dominated elements have been removed) in ki clusters C(1)i . . . , C (ki) i , and obtain Li by selecting one representative factor µ(`) i\nin each cluster C(`)i . These representatives are valid solutions in that they can be produced from combination of factors from the input sets. Hence, they provide attainable lower bounds for the optimal solution. To account for the (worst-case) errors introduced by the pruning operations we introduce upper bound factors σ(µi) for each discarded factor µi ∈ C(`)i \\ {µ(`)i }. We first discuss how to obtain upper bounds for discarded factors.\nConsider a set of factors µ (1) i , . . . , µ (k) i which we intend to discard, and let µi be a factor such that µi(x) = max{µ(1)i (x), . . . , µ (k) i (x)} for all x ∈ XIi∩Ipa(i) . Then µi ≥ µ (`) i for ` = 1, . . . , k, and it follows from Theorem 3 that any value µr obtained by propagating µi up to the root is greater than or equal to a solution\nµ (`) r obtained by propagating µ (`) i up to the root, for ` = 1, . . . , k. Thus, we can use the factor µi as an upper bound of the factors we wish to discard. We could introduce one upper bound for each discarded factor, but this would cause the propagation of an exponential number of upper bounds (therefore more than the limit ki). On the other extreme, we might produce a single upper bound for all factors discarded fromMi, but this would create too loose a bound. Instead, we generate and propagate one upper bound for each cluster. Let µ(`)\ni be the representative of a clus-\nter C(`)i . To account for the removal of the elements in the cluster, we update the upper bound σ(µ(`)\ni ) to be\nmax{σ(µi) : µi ∈ C(`)i }. Figure 2 depicts the pruning of a set Mi = {µ(1)i , µ (2) i , µ (3) i , µ (4) i , µ (5) i }, and the induced upper bounds. Let µ (`) r be a solution obtained by propagating the representative µ(`) i\nof cluster C(`)i , and let σ(µ (`) r ) be the corresponding propagated upper bound. Then it follows that µ (`) r ≤ Z∗ ≤ σ(µ(`)r ), where Z∗ is the optimal solution of the problem.\nThere still remains to decide how to select good representatives. To this end, we define the following divergence metric 〈µ(1)i , µ (2) i 〉 that assesses the quality of “representing” a factor µ (1) i by a factor µ (2) i as 〈µ(1)i , µ (2) i 〉=max{µ (1) i (x)/µ (2) i (x) : x ∈ X}. The metric matches the worst-case (multiplicative) error in discarding µ (1) i while selecting µ (2) i as representative, that is µ (1) i ≤ µ (2) i 〈µ (1) i , µ (2) i 〉. Note that the divergence is asymmetric, and that it is greater than one if and only if µ (1) i is not dominated by µ (2) i .\nGiven a set of representatives Vi={µ(1)i , . . . , µ (ki) i } in Mi, we assign each factor µi ∈ Mi to a cluster C(`)i such that 〈µi, µ(`)i 〉 = minl∈[ki]〈µi, µ (`) i 〉. The overall performance of the clustering can be conservatively measured by the largest divergence within a cluster:\n(Vi) = max `∈[ki] max{〈µi, µ(`)i 〉 : µi ∈ C (`) i } . (2)\nIdeally, we would like to find a set Vi ⊆ Mi of ki representatives that obtains the minimum (Vi) over all sets. However, this would add an extra complexity to the computations. Instead, we use a greedy search that at each step attempts to replace a factor inMi\\Vi with a factor in Vi such that (Vi) is decreased.\nThe following result shows that the the solution found by the algorithm improves monotonically by improving the clusterings at any node of the clique tree. Theorem 4. The outputs Zl and Zu satisfy Zu ≤ Zl ∏ i∈[m] (Vi).\nProof. Consider some inactive node i whose children j are all active, and assume by inductive hypothesis that for any µj ∈ Lj it holds that σ(µj) ≤ µjej , where ej is defined as (Vj) ∏ k∈de(j) (Vk). Then any\nµi ∈ Mi satisfies σ(µi)= ∑ XIi\\Ip φj ∏ j∈ch(i) σ(µj) ≤∏\nk∈de(i) (Vk)[ ∑ XIi\\Ip φj ∏ j∈ch(i) µj ] = µiei/ (Vi), where p = pa(i) and µj ∈ Lj . Let µi be the representative of a cluster Ci ⊆ Mi, with σ(µi) = max{µi : µi ∈ Ci}. It follows from (2) that σ(µi) ≤ (Vi)µi. After the clustering, the new upper bound assigned to µ i\nis (by design) given by µi = max{σ(µi) : µi ∈ Ci}, which satisfies µi ≤ σ(µi)ei/ (Vi) ≤ eiµi.\nThe above result guarantees that the algorithm finds lower and upper bounds whose ratio is not worse than the product of the clustering quality measures∏ i∈[m] (Vi). The quality of each cluster (Vi) can be improved by increasing the maximum allowed number of elements ki in the set. Since each set cannot have more than |K1|· · · |Km| elements, the algorithm is guaranteed to converge to the optimum in finite time. In fact, each maximum set size ki needs only to be as high as the number of extrema and non-dominated factors in Mi, since these are shown to lead to exact computations. These remarks lead naturally to the anytime algorithm we present in the next section."
    }, {
      "heading" : "6. Anytime Inference",
      "text" : "An anytime algorithm is a procedure that can be interrupted at any time with a meaningful solution whose quality is a monotonic function of runtime. Hence, anytime algorithms allow a trade-off between computation time and quality of solutions.\nWe can easily transform factor-set-elimination into an anytime algorithm that continuously improve the lower and upper bounds by increasing the maximum set cardinalities k1, . . . , km. The procedure is described in Algorithm 3. The anytime algorithm starts by running factor-set-elimination with all maxi-\nAlgorithm 3 anytime-inference Require: A clique tree T over sets K1, . . . ,Km and integer c\n1: let k (0) 1 = 1, . . . , k (0) m = 1, Z (0) l =0 and Z (0) u =1 2: set t← 0 3: while Z\n(t) l < Z (t) u and not interrupted do\n4: find the node i with highest (Vi) 5: run factor-set-elimination with\nk (t) 1 , . . . , k (t) m and let (Zl, Zu) be its output\n6: set Z (t+1) l = max{Zl, Z (t) l }, Z (t+1) u =\nmin{Zu, Z(t)u } and k(t+1)i =k (t) i + c, i = 1, . . . ,m\n7: set t← t+ 1 8: end while\nmum set cardinalities k (0) 1 , . . . , k (0) m set to one. This produces an arbitrary (but feasible) lower bound Z (0) l , and an upper bound Z (0) u that matches the value returned by factor-max-elimination. Then, for each time step, the algorithm increases the maximum set cardinality ki of the node i with poorest clustering quality (Vi) by a given constant c. In principle, even if we improve the clustering quality we might obtain a worse solution, as the metric that evaluates clustering quality optimizes worst case. This can be circumvented by enlarging each set Li incrementally."
    }, {
      "heading" : "7. Experiments",
      "text" : "We performed experiments with three groups of graphical models, which range from simple to very challenging problems. The first group, which appears in the top five lines of Table 1, consists of benchmark Bayesian networks used in real applications.2 In these networks, the MAP inference asks for optimum assignments of the root nodes given some evidence on every leaf. This creates MAP problems where every variable in the network is relevant to the solution and obtaining an exact solution by factor-max-elimination would take time (at least) exponential in the number of decision variables. The second group (lines 6–8 of the table) contains graphical models designed to solve multiple knapsack problems with three bags and varying number of items (20, 50 and 100). The graphs are structured in a chain of latent variables with root decision nodes as parents. Besides the importance of the multiple knapsack itself, this group allows us to evaluate the performance of the methods when the search space is large but the treewidth is low. Finally, the third group (last seven lines of the table) consists of grid-structured graphical models whose pa-\n2At the time of this submission, they were available at http://www.cs.huji.ac.il/site/labs/compbio/Repository/.\nrameters were uniformly sampled. Each Grid-x-y-z model contains x rows, y columns and z planes. For z = 2, variables are quaternary and the grid has two planes: one is the grid itself and the other is formed by decision variables that are linked to grid variables in a one-to-one correspondence; for z= 1, the models are usual planar binary grids, with all border variables chosen as decision variables. These experiments allow us to better evaluate how the performance is affected by the treewidth and the size of the search space.\nWe compare our algorithm against SamIam’s implementation of the systematic search algorithm of Park & Darwiche (2003), which we refer to as SI. We chose SI because (i) it is a state-of-the-art algorithm, (ii) its implementation is publicly available, (iii) it is an anytime procedure, and (iv) it returns feasible solutions.\nTable 1 shows the results of the experiments, comparing the proposed method (named AFSE for short) and SI. The table presents names, total number of variables, number of decision variables, amount of time that SI and AFSE, respectively, spent to solve the instances, and errors of the obtained solution (in case one of the methods was unable to solve the instances in a reasonable amount of time and memory). The error corresponds to the worst of the two methods and it was obtained by calculating the ratio of the returned value and the optimum (the worst of the two methods can be identified in the columns corresponding to the time they spent, indicated by a “> t”, where t is the time-limit used for the given method).\nSome results from Table 1 deserve an additional discussion. Firstly, models in the second group and the two-plane grids of the third group of experiments in-\ndicate that AFSE is by far faster when treewidth is small. Still, SI was able to find the best solution in the models of the second group (even though it was not aware of it, so the search have not stopped), but clearly degrades in the two-plane grids, as can be seen in the error column of the table, which reaches 55% in Grid-4-30-2. This means that not only the algorithm did not finish but the best solution found was far from the optimum. Such situation justifies the use of methods that can provide anytime lower and upper bounds for the solution. Secondly, AFSE performed similarly to SI in (real) Bayesian networks, with the largest differences in the Barley and Pigs networks (the former favorable to SI, the latter favorable to AFSE). We see on the squared grids of the third group that SI can handle better the increase of treewidth, indeed a known characteristic of SI. The exception is Grid-1818-1, where SI exhausted the 8 GB of memory granted without being able to produce a (candidate) solution. Finally, the time-accuracy trade-off of the algorithms can be seen in Figure 3, which shows the accuracy of AFSE and SI on models Grid-4-30-2 and Grid-4-25-2 as a function of time. Lower and upper bounds converge to the optimal solution, and while SI starts with a better lower bound, it gets stuck in the search and does not converge within the allowed time."
    }, {
      "heading" : "8. Conclusion",
      "text" : "We present a new anytime algorithm for the marginal MAP assignment problem in graphical models. We show theoretically that the algorithm produces feasible solutions whose quality are a function of the amount of computational resources granted. The convergence and error bounds are analyzed.\nBy performing experiments with real and synthetic graphical models, we show that the proposed algorithm is competitive against the systematic search of Park & Darwiche (2003). In particular, our algorithm compares favorably when the problems exhibit moderate treewidth but large search space. Unfortunately, as the treewidth increases, the bounds returned by the algorithm become too loose. This could be mitigated by decomposing the propagated factors into smaller domains, as in the work of Meek & Wexler (2011).\nUnderstanding how the numerical parameters of the input affect the complexity of the algorithm is an important question that remains open. Finally, in the spirit of the result by de Campos (2011) it is possible to show that the anytime algorithm is also a fully polynomial-time approximation scheme for graphical models if we assume that both the treewidth and the number of values a variable can assume are bounded."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partially supported by the Swiss NSF grants no. 200020 134759/1 and 200020 132252."
    } ],
    "references" : [ {
      "title" : "New complexity results for MAP in Bayesian networks",
      "author" : [ "C.P. de Campos" ],
      "venue" : "In IJCAI",
      "citeRegEx" : "Campos,? \\Q2011\\E",
      "shortCiteRegEx" : "Campos",
      "year" : 2011
    }, {
      "title" : "Message-passing for approximate MAP inference with latent variables",
      "author" : [ "J. Jiang", "P. Rai", "III", "H. Daume" ],
      "venue" : "In NIPS",
      "citeRegEx" : "Jiang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2011
    }, {
      "title" : "Probabilistic Graphical Models: Principles and Techniques",
      "author" : [ "D. Koller", "N. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Koller and Friedman,? \\Q2009\\E",
      "shortCiteRegEx" : "Koller and Friedman",
      "year" : 2009
    }, {
      "title" : "Variational algorithms for marginal MAP",
      "author" : [ "Q. Liu", "A. Ihler" ],
      "venue" : "In UAI",
      "citeRegEx" : "Liu and Ihler,? \\Q2011\\E",
      "shortCiteRegEx" : "Liu and Ihler",
      "year" : 2011
    }, {
      "title" : "Approximating max-sumproduct problems using multiplicative error bounds",
      "author" : [ "C. Meek", "Y. Wexler" ],
      "venue" : "Bayesian Statistics,",
      "citeRegEx" : "Meek and Wexler,? \\Q2011\\E",
      "shortCiteRegEx" : "Meek and Wexler",
      "year" : 2011
    }, {
      "title" : "Solving MAP exactly using systematic search",
      "author" : [ "J.D. Park", "A. Darwiche" ],
      "venue" : "In UAI",
      "citeRegEx" : "Park and Darwiche,? \\Q2003\\E",
      "shortCiteRegEx" : "Park and Darwiche",
      "year" : 2003
    }, {
      "title" : "Complexity results and approximation strategies for MAP explanations",
      "author" : [ "J.D. Park", "A. Darwiche" ],
      "venue" : "JAIR, 21:101–133,",
      "citeRegEx" : "Park and Darwiche,? \\Q2004\\E",
      "shortCiteRegEx" : "Park and Darwiche",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "A positive result has recently been given by de Campos (2011), which derived a fully polynomial-time approximation scheme when both treewidth and number of states per variable are bounded.",
      "startOffset" : 48,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "When it is hard, researchers have resorted to loopy belief propagation algorithms (Liu & Ihler, 2011; Jiang et al., 2011) and factor decomposition (Meek & Wexler, 2011).",
      "startOffset" : 82,
      "endOffset" : 121
    }, {
      "referenceID" : 1,
      "context" : "Variants of this procedure have recently been justified as an approximation by variational inference (Liu & Ihler, 2011; Jiang et al., 2011).",
      "startOffset" : 101,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "Finally, in the spirit of the result by de Campos (2011) it is possible to show that the anytime algorithm is also a fully polynomial-time approximation scheme for graphical models if we assume that both the treewidth and the number of values a variable can assume are bounded.",
      "startOffset" : 43,
      "endOffset" : 57
    } ],
    "year" : 2012,
    "abstractText" : "This paper presents a new anytime algorithm for the marginal MAP problem in graphical models of bounded treewidth. We show asymptotic convergence and theoretical error bounds for any fixed step. Experiments show that it compares well to a state-of-the-art systematic search algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}