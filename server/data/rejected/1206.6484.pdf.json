{
  "name" : "1206.6484.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Apprenticeship Learning for Model Parameters of  Partially Observable Environments",
    "authors" : [ "Takaki Makino", "Johane Takeuchi" ],
    "emails" : [ "mak@sat.t.u-tokyo.ac.jp", "johane.takeuchi@jp.honda-ri.com" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Learning from Demonstration (LfD) is a framework for learning to perform a complex task by observing demonstration (task execution) by an expert (Argall et al., 2009). LfD is particularly useful for domains where the expert knowledge of the domain is limited or difficult to represent, because demonstrations are much easier than designing a controller for the task.\nApprenticeship Learning via Inverse Reinforcement Learning (Abbeel & Ng, 2004), which is an application of LfD for reinforcement learning, is an algorithm that learns the reward function of the environment un-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nder the assumption that the expert is trying to maximize the reward. The idea is that, although reinforcement learning can produce an optimal policy with respect to a given reward function, designing a reward function that captures the desired task behavior is not always obvious and requires expert knowledge of the domain. Moreover, learning the reward function from demonstration requires much less amount of demonstration compared to learning the policy directly from the demonstration, because the reinforcement learning combines the reward function with the environment model for optimizing the policy for future rewards. Inverse reinforcement learning is successfully applied to tasks where the environment is fully observable, including aerobatic helicopter flight (Abbeel et al., 2010), robot hand control (Boularias et al., 2011) and prediction of linguistic structures (Neu & Szepesvári, 2009). Inverse reinforcement learning in partially observable environments when an exact model is available has also been studied (Ziebart et al., 2010; Henry et al., 2010; Choi & Kim, 2011).\nHowever, the design bottleneck is not limited to the reward function. In many tasks, how to model the environment is not obvious as well, and requires expert knowledge of the domain, especially when the environment is partially observable. For example, dialogue system tasks are often represented as a Partially Observable Markov Decision Process (POMDP) in which the user’s mental state is situated as a hidden state (Williams et al., 2005; Kim et al., 2008; Meguro et al., 2010), but designing such a model requires a considerable amount of work by domain experts, such as annotating dialogue corpus. Thus, there is a need for a way to estimate uncertain parameters of an environment model from non-annotated demonstration data.\nOne obvious way to estimate environmental parameters from the demonstration is to extract the environmental reaction to the expert’s action (Thomson et al.,\n2010). In case of POMDP environment, This reduces the problem into a parameter estimation of an InputOutput Hidden Markov Model (IO-HMM) (Bengio & Frasconi, 1996) (Fig. 1(a)). However, this approach assumes nothing about the demonstrator, and it is applicable to cases where the demonstration is generated from the learning agent or even from a naive random policy. Our claim is that demonstration by an expert contains much richer information about the environment that comes from the expert’s knowledge, and by extracting this information, we can reduce the burden of designing a model suitable for the task.\nOur proposal is to apply the framework of apprenticeship learning to estimate uncertain parameters of the environment (Fig. 1(b))1 . Assuming that the expert’s behavior is based on a stochastic optimal policy with knowledge of the perfect POMDP model for the target environment, we can extract the expert’s knowledge regarding the POMDP parameters from his demonstration. The extracted information of expert knowledge can be combined with IO-HMM estimation from the environmental response, to provide a better estimate of the POMDP parameters.\nWe present two straightforward estimation algorithms, maximum a posteriori (MAP) estimator and posterior sampler by Markov chain Monte Carlo (MCMC), combined with planning algorithms to achieve modelparameter apprenticeship learning In the experiments with short demonstrations, we show that our algorithms can achieve more accurate estimates of\n1We can use this approach for any environment model, such as fully-observable MDPs. However, the most effective cases are for POMDPs because they are hard to be learned from environmental reactions, so we focus on POMDPs in this paper.\nPOMDP parameters and better policies than can existing methods based on IO-HMM estimation."
    }, {
      "heading" : "2. POMDP and its Parameterization",
      "text" : "An Input-Output Hidden Markov Model (IO-HMM) (Bengio & Frasconi, 1996) is a framework for representing environments consisting of hidden states, inputs (actions that may affect the states), and outputs (observations from the states). Formally, an IO-HMM is defined as a tuple ⟨S,A,Z, T,O, b0⟩, where S is the finite set of states, A is the finite set of actions, Z is the finite set of observations, T is the state transition function such that T (s, a, s′) denotes probability P (s′|s, a) of changing to state s′ by taking action a at state s, O is the observation function such that O(a, s, z) denotes probability P (z|a, s) of perceiving observation z as a result of taking action a and arriving in state s, and b0 is the vector of initial state distribution such that b0(s) denotes the probability of starting in state s.\nSince the true state is hidden, we construct a belief about the state. We denote belief by a vector b where b(s) denotes the probability that the state is s at the current time step. The following update formula can be used to calculate the belief baz for the next time step from the belief at the current time step, given the action a at the current time step and the observation z at the next time step:\nbaz(s ′) ∝ O(a, s′, z) ∑ s T (s, a, s′)b(s) . (1)\nA partially observable Markov decision process (POMDP) is a formulation of an action selection problem on an IO-HMM. A POMDP is defined as a tuple P = ⟨S,A,Z, T,O, b0, R, γ⟩, where S,A,Z, T,O, b0 are\ndefined as in the IO-HMM, R is the reward function so that R(s, a) denotes the immediate reward of taking action a in state s, and γ ∈ [0, 1) is the discount factor. The goal of an agent is to maximize the expected discounted total reward E[ ∑∞ t=0 γ\ntR(st, at)] by choosing a policy.\nSince the true state is hidden, a policy of agent action must be defined over past actions and observations. If a POMDP is specified, we can use a belief as a sufficient statistic of past actions and observations, where π(b, a) = P (a|b) is a probability of taking action a at belief b. A policy π induces a value function Vπ(b) that represents the expected discounted total reward of executing policy π starting from b. It is known (Smallwood & Sondik, 1973) that Vπ∗ , the value function associated with the optimal greedy policy π∗, can be approximated with an arbitrary accuracy by a convex, piecewise-linear function\nQ(b, a) = max α∈Γ(a) (α · b) V (b) = max a Q(b, a) , (2)\nwhere Γ(a) is a finite set of vectors called α-vectors associated to action a, and α ·b is the inner product of a α-vector and vector b. We consider soft-max policy for a given set of α-vectors:\nπ̃(b, a) = exp(βQ(b, a))∑ a′ exp(βQ(b, a ′)) (3)\nwhere β is the inverse temperature parameter that controls the orderedness of the policy. We denote the softmax policy from the optimal action-value function Q∗ as soft-max optimal policy π̃∗.\nIn general, computing an approximately optimal solution within a given error bound ϵ is NP-hard. However, it is known that given a set of balls of radius δ ≤ O(ϵ) over beliefs that cover an optimal reachable space, an approximated solution can be computed in a polynomial time (Hsu et al., 2008). SARSOP (Kurniawati et al., 2008) is one of approximated POMDP solvers that implements elaborated point selection and a pruning algorithm.\nIn this paper, we consider situations in which some part of the environment model is uncertain. We introduce a K-element parameter vector θ with its prior distribution p(θ), and consider a POMDP Pθ = ⟨S,A,Z, Tθ, Oθ, b0,θ, Rθ, γ⟩, where T , O, b0 and R are determined according to the given parameter θ. An L-length sequence D = (a1z1 · · · aLzL) of demonstration by an expert is given, assuming that the expert knows θtrue, the true parameter of the environment, and is following a soft-max optimal policy π̃∗θtrue under POMDP Pθtrue with inverse temperature β.2 What we\n2For notational simplicity we assume β is known and\nwant is to calculate p(θ|D), the posterior distribution of the parameter, and to find an optimal policy over the posterior."
    }, {
      "heading" : "3. Inferring Posterior",
      "text" : "Bayes’ theorem gives posterior distribution p(θ|D) of parameter θ given demonstration D = (a1z1 · · · aLzL):\np(θ|D) ∝ p(D|θ)p(θ) . (4)\nLikelihood p(D|θ) of the demonstration is the result of marginalizing expert’s policy π:\np(D|θ) = ∫ p(D|θ, π)p(π|θ)dπ . (5)\nNote that, from our assumption, the expert’s policy πθ is equal to the soft-max optimal policy π̃ ∗ θ for the POMDP Pθ with parameter θ, thus p(π = π̃∗θ|θ) = 1.\nWe can further refactor the likelihood p(D|θ, πθ) as follows:\np(D|θ, π) = p(a1|θ, π)p(z1|θ, a1)p(a2|θ, π, a1z1) · · · (6) = p(a1 · · · aL|θ, π, z1 · · · zL−1)\n· p(z1 · · · zL|θ, a1 · · · aL) . (7)\nThe first factor of Eq. 7 corresponds to the likelihood that the expert performs action ai given the policy πθ:\np(a1 · · · aL|π, z1 · · · zL−1) = L∏\ni=1\nπ(bi,θ,D, ai) , (8)\nwhere bi,θ,D is the belief at time step i in a POMDP Pθ with history D, calculated by applying Eq. 1 repeatedly to b0,θ.\nOn the other hand, the second factor corresponds to the likelihood that the environment responds with observation zi to the performed actions:\np(z1 · · · zL|θ, a1 · · · aL) = L∏\ni=1 ∑ s∈S bθ,D,i−1(s) ∑ s′∈S T (s, ai, s ′)O(ai, s ′, zi) . (9)\nTo our knowledge, previous studies that use the benefit of the first factor in the inference of parameter θ only consider the change of the reward function. In cases where only the rewards are uncertain, the inference is relatively easy since the value function Vπ for a given policy is given as a linear function of the reward\nfixed. However it is easy to apply our methods to cases with unknown β, because it is equivalent to a fixed β with an unknown scaling parameter of the reward function.\nvalues (Ramachandran & Amir, 2007). However, if we consider cases where transition and observation probabilities are uncertain, the inference is complex because of the nonlinear dependence between the parameters and the value function."
    }, {
      "heading" : "3.1. Maximum A Posteriori Inference",
      "text" : "Maximum a posteriori (MAP) inference is to find θ that maximizes the posterior (Eq. 4). Unfortunately, it is not easy to use sophisticated optimization techniques using gradients because changes in beliefs complicates obtaining gradients for either factor in Eq. 7. This is the major difference from the setting of inverse reinforcement learning, in which we can evaluate the gradient of expert action likelihood, and the observation likelihood is constant given D.\nWe take a straightforward approach to optimization by using the COBYLA algorithm (Powell, 1998), which does not require gradients. For each candidate parameter value θ, we call a POMDP solver for POMDP Pθ to obtain the optimal action-value function Q∗, which gives the soft-max optimal policy π̃∗θ for the POMDP that is used for evaluating expert action likelihood (Eq. 8). As for the observation likelihood (Eq. 9), we apply the standard forward algorithm for IO-HMM to POMDP Pθ and sequence D.\nThis algorithm has no guarantee to find the MAP parameter because it is based on a local search. However, in practice it seems to find a reasonably good solution, and calculation is quick compared to the sampling approach which we will describe next."
    }, {
      "heading" : "3.2. Inference by Sampling",
      "text" : "We also employ a Markov chain Monte Carlo (MCMC) sampling approach (Gilks et al., 1996) to infer the posterior distribution of θ. Unlike MAP inference, The approximation calculated by MCMC can be arbitrarily accurate with a sufficient computational time.\nThe traditional way of sampling parameters for IOHMM is to use the Markov chain Monte Carlo approach; that is by alternately sampling the hidden state sequence s given parameter θ, and θ given s. By using a conjugate prior for the parameters, we can easily sample θ from the posterior given s.\nTo make the sample distribution follow the expert action likelihood, we introduce the Metropolis algorithm (Metropolis et al., 1953), which accepts the proposed sample θ′ with the probability min(1, p′/p), where p′ and p are the expert action likelihood for the proposed sample and for the previous sample, respectively.\nAlgorithm 1 MCMC sampler for posterior p(θ|D) Require: D: demonstration 1: sample θ from the prior 2: p := infinitesimal positive value 3: loop 4: sample s = (s1 · · · sL) from p(s|D,θ) 5: for k := 1 to K, in random order do 6: θ′ := θ 7: replace θ′k by a sampled value from IO-HMM\nposterior given s, D 8: call POMDP solver to find π̃∗θ′ 9: p′ := p(a1 · · · aL|π̃∗θ′ , z1 · · · zL) {Eq. 8}\n10: if with probability min(1, p′/p) then 11: p := p′, θ := θ′ {accept the sample} 12: end if 13: end for 14: end loop\nAlgorithm 1 shows the sampler for the posterior of model parameters. The algorithm is similar to the sampling algorithm for model parameters from IOHMM posterior, which deals with only the likelihood of an environmental response (the second term in Eq. 7). The difference lies in lines 8–12, that performs Metropolis algorithm for the expert action likelihood (the first term in Eq. 7). We run the algorithm until specified numberM of samples are collected, excluding burn-ins and interval samples."
    }, {
      "heading" : "4. Planning with a Sampled Posterior",
      "text" : "Our goal is to achieve model-parameter apprenticeship learning; that is, to make an optimal policy for the learned posterior of POMDP model parameters. In this section, we describe how to produce an optimal policy based on the sampled results. Note that, in case of a MAP estimate, we can obtain a policy by applying a solver to POMDP Pθ̂ with estimated parameter θ̂.\nExisting planning methods for POMDP with Bayesian uncertainty (Ross et al., 2008) are not applicable, because they require that the uncertainty be represented in conjugate priors, which cannot represent the posterior distribution of parameters after observing demonstration. Instead, we employed a method to develop a POMDP policy based on the sampled parameters. The idea is to extend the hidden state of POMDP with a variable m, which is an index of the sampled parameters θm (m = 1, . . . ,M). At the beginning m is uniformly distributed, and never changes. This extended POMDP can be solved by a standard POMDP solver. We expect that the belief over sample index m converges to the index of the most likely parame-\nter while the agent interacts with the environment. In case the target POMDP is episodic, we want to retain belief overm beyond episodes, so we convert the target POMDP into non-episodic POMDPs before extension.\nFormally, given M sampled parameters θ1, . . . ,θM for the target POMDP, we create an extended POMDP P̃ = ⟨S̃, A, Z, T̃ , Õ, b̃0, R̃, γ⟩, where\nS̃ = S × {1, . . . ,M} Õ([s,m], a, z) = Oθm(s, a, z) b̃0([s,m]) = b0,θm(s)/M R̃([s,m], a) = Rθm(s, a)\nT̃ ([s,m], a, [s′,m′]) =\n{ Tθm(s, a, s ′) m = m′\n0 m ̸= m′ .\nNote that the optimal policy of the extended POMDP becomes a good policy in the target POMDP only if the samples represent the target well. If we need a agent that learns by exploring the uncertainty in the target POMDP, we will need scheduled resampling as has been done in fully observable environments by the BOSS algorithm (Asmuth et al., 2009). In this paper we chose not to resample, because our purpose is to evaluate the posterior distribution p(θ|D) obtained from demonstration."
    }, {
      "heading" : "5. Experiments",
      "text" : "To evaluate the proposed model-parameter apprenticeship learning algorithms, we performed experiments with two tasks: one is a simple environment based on the well-known Tiger problem (Kaelbling et al., 1998), and the other is a task designed for a dialog system. In the following experiments, we used APPL Toolkit which implements the SARSOP algorithm (Kurniawati et al., 2008) as a POMDP solver. We used COBYLA implementation in the NLopt library (Johnson, 2008)."
    }, {
      "heading" : "5.1. Bayesian Tiger Problem",
      "text" : "We introduced four unknown parameters to the Tiger problem, whose prior is represented as pi ∼ Beta(3, 3), pl, pr ∼ Beta(5, 3), rt ∼ N (−50, 502) as follows. An agent is standing in front of two doors. A tiger is hidden behind the left door with probability pi and behind the right door with probability 1 − pi. The agent can open one of the doors, and obtain reward rt if the agent sees the tiger and reward 10 otherwise. Alternatively, the agent can choose to listen with reward −1: if the tiger is behind the left door, the agent hears the tiger from the left with probability pl and from the right with probability 1 − pl; if the tiger is behind the right door, the agent hears it from the right with probability pr and from the left with probability 1− pr.\nWe set the true environment as pi = 0.6, pl = pr = 0.85 and rt = −100, and we used γ = 0.9. We generated 100 demonstrations by the experts with soft-max policy β = 0.3, each consisting of 100 steps of actions and observations (which contained 22 episodes on average). For each demonstration, we applied one of the learning algorithms to the demonstration to estimate the posterior. From the estimated posterior, an optimal greedy policy was derived, and tested by simulating 100,000 steps in the true environment, and the average reward was measured. For sampling algorithms, 1,000 MCMC steps were performed including 100-step burn-in, and parameters were sampled for every 10 steps (total M = 90 samples) to generate a greedy policy.\nTable 1 shows the distribution of the estimated parameters. Both of the proposed methods produce more accurate estimates of parameters compared to the meth-\nods based on IO-HMM3. We can see that the proposed methods provide better RMSE than do the IO-HMM methods. The estimates from IO-HMM methods are closer to the prior mean, suggesting that the provided demonstration is too short to obtain an accurate estimate. On the other hand, the estimates from the proposed methods are closer to the true value, which indicates that the proposed methods provide a better estimate using the same length of demonstration. We can also see that the proposed sampler produces a narrower posterior distribution (i.e., smaller standard deviation of the samples) than that of the IO-HMM\n3Note that we compare only state transition parameters because the rewards cannot be estimated by IO-HMM methods.\nsampler.\nAs shown in Figure 2, having an accurate estimate leads to better results in simulation by the learned policy. The results of the policies based on estimated posterior with our methods are not much worse than those of the expert policy who knows the true parameter values. On the other hand, policies based on IOHMM estimation occasionally result in very bad policies, as shown in “Less” average rewards in the figure. Considering that the demonstration is short and noisy, these results indicate that the model-parameter apprenticeship learning methods prevent agents from critical failures in learning to follow the demonstrated task."
    }, {
      "heading" : "5.2. Dialog System",
      "text" : "To show the effectiveness of our methods in a more realistic scenario, we developed a new task of dialog management for a ticket-vending system. A user asks the agent for a ticket with a certain origin and destination via an unreliable voice recognition interface; the task of the agent is to repeat the order correctly, and issue the ticket. We expect that the expert demonstration is useful to determine parameters, which represents user’s preferred ticket routes and way of talking.\nThe task consists of 13 observations from voice recognition, including three place names and SIL (silence). The agent can choose from 11 actions, consisting of uttering one of 9 words, waiting for next word from the user, or issuing a ticket. The dialog is managed by a 32-state POMDP (Fig. 3) for each of 3×2 = 6 ticket routes, resulting in the total of 192 hidden states.\nThe POMDP is parameterized with a 15-dimensional vector θ; 4 parameters are assigned to route preferences (initial state distribution), 9 to ways of talking (transition probabilities), and 2 to voice recognition errors (observation probabilities). The agents are required to estimate the parameters from a 300-step demonstration generated by an expert. In the experiments we didn’t use samplers since they require too much computational resources.\nWe generated 12 demonstrations by the experts (the result of solving the true POMDP model) with softmax policy β = 0.4, each consisting of 300 steps of actions and observations (which contained 27 episodes on average). Using the learned parameters, we applied SARSOP POMDP solver to obtain a greedy policy, and measured average reward by testing the policy on the original environment. Since calculating the exact solution of the POMDP is too expensive, we set the timeout of 40 CPU seconds for each parameter can-\ndidate during MAP search, and 600 CPU seconds for calculating the expert policy and the final policy based on the estimated parameters;\nFigure 4 shows the results. We found that the agents based on the parameters estimated by the proposed MAP algorithm perform significantly better than the agents based on the parameters estimated by IO-HMM (P < .05). However, in this setting, we couldn’t obtain the expert-level performance by the apprenticeship learning. One possible reason is that the optimization algorithm is disturbed by the approximation error of expert action likelihood, which is caused by the short timeout of the POMDP solver and random searching strategy of SARSOP. We believe that the\nresult can be improved if we use more computational resources; or, if we use a POMDP solver that can be started from the result of a similar POMDP, we may be able to improve the optimization process."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We have shown that the apprenticeship learning approach can be used to estimate parameters of an unknown POMDP environment. Assuming that an expert knowing the perfect POMDP model of the target environment will try to maximize the reward, we can extract the expert’s knowledge about the environment from his demonstration in terms of the posterior distribution of unknown parameters. Our proposed algorithms are simple but are capable of estimating POMDP parameters accurately even if the demonstration is short. We also showed that the extracted knowledge can be used to develop a policy that can act reasonably well in the target environment.\nOur approach is a generalization of inverse reinforcement learning, in a sense that the unknown parameters are not limited to those for the reward function but can also be for transition and observation functions. This approach can be particularly useful in the domain of applications that interact with human beings, whose model is unknown but demonstration by experts is available. One direct extension of the approach is to estimate other parameters, such as the discount factor of the expert, from the demonstration. Future work should also include the development of more efficient algorithms as has been done in the context of inverse reinforcement learning."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This research is supported by the Aihara Innovative Mathematical Modelling Project, the Japan Society for the Promotion of Science (JSPS) through the “Funding Program for World-Leading Innovative R&D on Science and Technology (FIRST Program),” initiated by the Council for Science and Technology Policy (CSTP), and by JSPS Grant-in-Aid for Young Scientists (B) (20700126)."
    } ],
    "references" : [ {
      "title" : "Apprenticeship learning via inverse reinforcement learning",
      "author" : [ "P. Abbeel", "A. Ng" ],
      "venue" : "Proc. of 21st International Conference on Machine Learning (ICML",
      "citeRegEx" : "Abbeel and Ng,? \\Q2004\\E",
      "shortCiteRegEx" : "Abbeel and Ng",
      "year" : 2004
    }, {
      "title" : "A survey of robot learning from demonstration",
      "author" : [ "B.D. Argall", "S. Chernova", "M. Veloso", "B. Browning" ],
      "venue" : "Robotics and Autonomous Systems,",
      "citeRegEx" : "Argall et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Argall et al\\.",
      "year" : 2009
    }, {
      "title" : "A bayesian sampling approach to exploration in reinforcement learning",
      "author" : [ "J. Asmuth", "L. Li", "M. Littman", "A. Nouri", "D. Wingate" ],
      "venue" : "In Proc. of the 25th Annual Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Asmuth et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Asmuth et al\\.",
      "year" : 2009
    }, {
      "title" : "Input-output HMM’s for sequence processing",
      "author" : [ "Y. Bengio", "P. Frasconi" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Bengio and Frasconi,? \\Q1996\\E",
      "shortCiteRegEx" : "Bengio and Frasconi",
      "year" : 1996
    }, {
      "title" : "Relative entropy inverse reinforcement learning",
      "author" : [ "A. Boularias", "J. Kober", "J. Peters" ],
      "venue" : "Journal of Machine Learning Research: Workshop and Conference Proceedings (AISTATS",
      "citeRegEx" : "Boularias et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Boularias et al\\.",
      "year" : 2011
    }, {
      "title" : "Inverse reinforcement learning in partially observable environments",
      "author" : [ "J. Choi", "Kim", "K.-E" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Choi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning to navigate through crowded environments",
      "author" : [ "P. Henry", "C. Vollmer", "B. Ferris", "D. Fox" ],
      "venue" : "In Proc. of 2010 IEEE International Conference of Robotics and Automation (ICRA",
      "citeRegEx" : "Henry et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Henry et al\\.",
      "year" : 2010
    }, {
      "title" : "What makes some POMDP problems easy to approximate",
      "author" : [ "D. Hsu", "W.S. Lee", "N. Rong" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Hsu et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hsu et al\\.",
      "year" : 2008
    }, {
      "title" : "The NLopt nonlinear-optimization package",
      "author" : [ "S.G. Johnson" ],
      "venue" : "http://ab-initio.mit.edu/nlopt,",
      "citeRegEx" : "Johnson,? \\Q2008\\E",
      "shortCiteRegEx" : "Johnson",
      "year" : 2008
    }, {
      "title" : "Planning and acting in partially observable stochastic domains",
      "author" : [ "L.P. Kaelbling", "M.L. Littman", "A.R. Cassandra" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Kaelbling et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Kaelbling et al\\.",
      "year" : 1998
    }, {
      "title" : "A frame-based probabilistic framework for spoken dialog management using dialog examples",
      "author" : [ "K. Kim", "C. Lee", "S. Jung", "G.G. Lee" ],
      "venue" : "In Proc. of the 9th SIGdial Workshop on Discourse and Dialogue,",
      "citeRegEx" : "Kim et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2008
    }, {
      "title" : "SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces",
      "author" : [ "H. Kurniawati", "D. Hsu", "W.S. Lee" ],
      "venue" : "In Proc. Robotics: Science and Systems,",
      "citeRegEx" : "Kurniawati et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kurniawati et al\\.",
      "year" : 2008
    }, {
      "title" : "Controlling Listening-oriented Dialogue using Partially Observable Markov Decision Processes",
      "author" : [ "T. Meguro", "R. Higashinaka", "Y. Minami", "K. Dohsaka" ],
      "venue" : "In Proc. of the 23rd International Conference on Computational Linguistics (COLING",
      "citeRegEx" : "Meguro et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Meguro et al\\.",
      "year" : 2010
    }, {
      "title" : "Equations of state calculations by fast computing machines",
      "author" : [ "N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller" ],
      "venue" : "Journal of Chemical Physics,",
      "citeRegEx" : "Metropolis et al\\.,? \\Q1953\\E",
      "shortCiteRegEx" : "Metropolis et al\\.",
      "year" : 1953
    }, {
      "title" : "Training parsers by inverse reinforcement learning",
      "author" : [ "G. Neu", "C. Szepesvári" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Neu and Szepesvári,? \\Q2009\\E",
      "shortCiteRegEx" : "Neu and Szepesvári",
      "year" : 2009
    }, {
      "title" : "Direct search algorithms for optimization calculations",
      "author" : [ "M.J.D. Powell" ],
      "venue" : "Acta Numerica,",
      "citeRegEx" : "Powell,? \\Q1998\\E",
      "shortCiteRegEx" : "Powell",
      "year" : 1998
    }, {
      "title" : "Bayesian inverse reinforcement learning",
      "author" : [ "D. Ramachandran", "E. Amir" ],
      "venue" : "In Proc. of International Joint Conference of Artifical Intelligence",
      "citeRegEx" : "Ramachandran and Amir,? \\Q2007\\E",
      "shortCiteRegEx" : "Ramachandran and Amir",
      "year" : 2007
    }, {
      "title" : "The optimal control of partially observable Markov processes over a finite horizon",
      "author" : [ "R. Smallwood", "E. Sondik" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "Smallwood and Sondik,? \\Q1973\\E",
      "shortCiteRegEx" : "Smallwood and Sondik",
      "year" : 1973
    }, {
      "title" : "Parameter learning for POMDP spoken dialogue models",
      "author" : [ "B. Thomson", "F. Jurč́ıček", "M. Gašić", "S. Keizer", "F. Mairesse", "K. Yu", "S. Young" ],
      "venue" : "In Proc. of the 3rd IEEE Workshop on Spoken Language Technology (SLT",
      "citeRegEx" : "Thomson et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Thomson et al\\.",
      "year" : 2010
    }, {
      "title" : "Partially observable Markov decision processes with continuous observations for dialogue management",
      "author" : [ "J.D. Williams", "P. Poupart", "S. Young" ],
      "venue" : "In Proc. of the 6th SIGdial Workshop on Discourse and Dialogue,",
      "citeRegEx" : "Williams et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Williams et al\\.",
      "year" : 2005
    }, {
      "title" : "Modeling interaction via the principle of maximum causal entropy",
      "author" : [ "B. Ziebart", "J.A. Bragnell", "A.K. Dey" ],
      "venue" : "Proc. of the 27th International Conference on Machine Learning (ICML",
      "citeRegEx" : "Ziebart et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ziebart et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Learning from Demonstration (LfD) is a framework for learning to perform a complex task by observing demonstration (task execution) by an expert (Argall et al., 2009).",
      "startOffset" : 145,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : ", 2010), robot hand control (Boularias et al., 2011) and prediction of linguistic structures (Neu & Szepesvári, 2009).",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 20,
      "context" : "Inverse reinforcement learning in partially observable environments when an exact model is available has also been studied (Ziebart et al., 2010; Henry et al., 2010; Choi & Kim, 2011).",
      "startOffset" : 123,
      "endOffset" : 183
    }, {
      "referenceID" : 6,
      "context" : "Inverse reinforcement learning in partially observable environments when an exact model is available has also been studied (Ziebart et al., 2010; Henry et al., 2010; Choi & Kim, 2011).",
      "startOffset" : 123,
      "endOffset" : 183
    }, {
      "referenceID" : 19,
      "context" : "For example, dialogue system tasks are often represented as a Partially Observable Markov Decision Process (POMDP) in which the user’s mental state is situated as a hidden state (Williams et al., 2005; Kim et al., 2008; Meguro et al., 2010), but designing such a model requires a considerable amount of work by domain experts, such as annotating dialogue corpus.",
      "startOffset" : 178,
      "endOffset" : 240
    }, {
      "referenceID" : 10,
      "context" : "For example, dialogue system tasks are often represented as a Partially Observable Markov Decision Process (POMDP) in which the user’s mental state is situated as a hidden state (Williams et al., 2005; Kim et al., 2008; Meguro et al., 2010), but designing such a model requires a considerable amount of work by domain experts, such as annotating dialogue corpus.",
      "startOffset" : 178,
      "endOffset" : 240
    }, {
      "referenceID" : 12,
      "context" : "For example, dialogue system tasks are often represented as a Partially Observable Markov Decision Process (POMDP) in which the user’s mental state is situated as a hidden state (Williams et al., 2005; Kim et al., 2008; Meguro et al., 2010), but designing such a model requires a considerable amount of work by domain experts, such as annotating dialogue corpus.",
      "startOffset" : 178,
      "endOffset" : 240
    }, {
      "referenceID" : 7,
      "context" : "However, it is known that given a set of balls of radius δ ≤ O(ε) over beliefs that cover an optimal reachable space, an approximated solution can be computed in a polynomial time (Hsu et al., 2008).",
      "startOffset" : 180,
      "endOffset" : 198
    }, {
      "referenceID" : 11,
      "context" : "SARSOP (Kurniawati et al., 2008) is one of approximated POMDP solvers that implements elaborated point selection and a pruning algorithm.",
      "startOffset" : 7,
      "endOffset" : 32
    }, {
      "referenceID" : 15,
      "context" : "We take a straightforward approach to optimization by using the COBYLA algorithm (Powell, 1998), which does not require gradients.",
      "startOffset" : 81,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "To make the sample distribution follow the expert action likelihood, we introduce the Metropolis algorithm (Metropolis et al., 1953), which accepts the proposed sample θ′ with the probability min(1, p′/p), where p′ and p are the expert action likelihood for the proposed sample and for the previous sample, respectively.",
      "startOffset" : 107,
      "endOffset" : 132
    }, {
      "referenceID" : 2,
      "context" : "If we need a agent that learns by exploring the uncertainty in the target POMDP, we will need scheduled resampling as has been done in fully observable environments by the BOSS algorithm (Asmuth et al., 2009).",
      "startOffset" : 187,
      "endOffset" : 208
    }, {
      "referenceID" : 9,
      "context" : "To evaluate the proposed model-parameter apprenticeship learning algorithms, we performed experiments with two tasks: one is a simple environment based on the well-known Tiger problem (Kaelbling et al., 1998), and the other is a task designed for a dialog system.",
      "startOffset" : 184,
      "endOffset" : 208
    }, {
      "referenceID" : 11,
      "context" : "In the following experiments, we used APPL Toolkit which implements the SARSOP algorithm (Kurniawati et al., 2008) as a POMDP solver.",
      "startOffset" : 89,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "We used COBYLA implementation in the NLopt library (Johnson, 2008).",
      "startOffset" : 51,
      "endOffset" : 66
    } ],
    "year" : 2012,
    "abstractText" : "We consider apprenticeship learning — i.e., having an agent learn a task by observing an expert demonstrating the task — in a partially observable environment when the model of the environment is uncertain. This setting is useful in applications where the explicit modeling of the environment is difficult, such as a dialogue system. We show that we can extract information about the environment model by inferring action selection process behind the demonstration, under the assumption that the expert is choosing optimal actions based on knowledge of the true model of the target environment. Proposed algorithms can achieve more accurate estimates of POMDP parameters and better policies from a short demonstration, compared to methods that learns only from the reaction from the environment.",
    "creator" : "LaTeX with hyperref package"
  }
}