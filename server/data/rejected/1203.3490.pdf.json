{
  "name" : "1203.3490.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Anytime Planning for Decentralized POMDPs using Expectation Maximization",
    "authors" : [ "Akshat Kumar" ],
    "emails" : [ "akshat@cs.umass.edu", "shlomo@cs.umass.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Decentralized POMDPs provide an expressive framework for multi-agent sequential decision making. While finite-horizon DECPOMDPs have enjoyed significant success, progress remains slow for the infinite-horizon case mainly due to the inherent complexity of optimizing stochastic controllers representing agent policies. We present a promising new class of algorithms for the infinite-horizon case, which recasts the optimization problem as inference in a mixture of DBNs. An attractive feature of this approach is the straightforward adoption of existing inference techniques in DBNs for solving DEC-POMDPs and supporting richer representations such as factored or continuous states and actions. We also derive the Expectation Maximization (EM) algorithm to optimize the joint policy represented as DBNs. Experiments on benchmark domains show that EM compares favorably against the state-of-the-art solvers."
    }, {
      "heading" : "1 Introduction",
      "text" : "Decentralized partially observable MDPs (DECPOMDPs) have emerged in recent years as an important framework for modeling sequential decision making by a team of agents [5]. Their expressive power makes it possible to tackle coordination problems in which agents must act based on different partial information about the environment and about each other to maximize a global reward function. Applications of DEC-POMDPs include coordinating the operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor agents [13]. However, the rich model comes with a price–optimally solving a finite-horizon DEC-POMDP\nis NEXP-Complete [5]. In contrast, finite-horizon POMDPs are PSPACE-complete [12], a strictly lower complexity class that highlights the difficulty of solving DEC-POMDPs.\nRecently, a multitude of point-based approximate algorithms have been proposed for solving finite-horizon DEC-POMDPs [11, 8, 16]. However, unlike their point-based counterparts in POMDPs ([15, 17]), they cannot be easily adopted for the infinite-horizon case due to a variety of reasons. For example, POMDP algorithms represent the policy compactly as α-vectors, whereas all DEC-POMDP algorithms explicitly store the policy as a mapping from observation sequences to actions, making them unsuitable for the infinitehorizon case. In POMDPs, the Bellman equation forms the basis of most point-based solvers, but as Bernstein et. al. [4] highlight, no analogous equation exists for DEC-POMDPs.\nTo alleviate such problems, most infinite-horizon algorithms represent agent policies as finite-state controllers [1, 4]. So far, only two algorithms have shown promise for effectively solving infinite-horizon DEC-POMDPs–decentralized bounded policy iteration (DEC-BPI) [4] and a non-linear programming based approach (NLP) [1]. However, both of these algorithms have significant drawbacks in terms of the representative class of problems that can be handled. For example, solving DEC-POMDPs with continuous state or action spaces is not supported by either of these approaches. Scaling up to structured representations such as factored or hierarchical state-space is difficult due to convergence issues in DEC-BPI and a potential increase in the number of non-linear constraints in the NLP solver. Further, none of the above approaches have been shown to work for more than 2 agents, a significant bottleneck for solving practical problems.\nTo address these shortcomings, we present a promising new class of algorithms which amalgamates planning with probabilistic inference and opens the door\nto the application of rich inference techniques to solving infinite-horizon DEC-POMDPs. Our technique is based on Toussaint et. al.’s approach of transforming the planning problem to its equivalent mixture of dynamic Bayes nets (DBNs) and using likelihood maximization in this framework to optimize the policy value [20, 19]. Earlier work on planning by probabilistic inference can be found in [2]. Such approaches have been successful in solving MDPs and POMDPs [19]. They also easily extend to factored or hierarchical structures [18] and can handle continuous action and state spaces thanks to advanced probabilistic inference techniques [10]. We show how DEC-POMDPs, which are much harder to solve than MDPs or POMDPs, can also be reformulated as a mixture of DBNs. We then present the Expectation Maximization algorithm (EM) to maximize the reward likelihood in this framework. The EM algorithm naturally has the desirable anytime property as it is guaranteed to improve the likelihood (and hence the policy value) with each iteration. We also discuss its extension to large multiagent systems. Our experiments on benchmark domains show that EM compares favorably against the state-of-the-art algorithms, DEC-BPI and NLP-based optimization. It always produces better quality policies than DEC-BPI and for some instances, it nearly doubles the solution quality of the NLP solver. Finally, we discuss potential pitfalls, which are inherent in the EM based approach."
    }, {
      "heading" : "2 The DEC-POMDP model",
      "text" : "In this section, we introduce the DEC-POMDP model for two agents [5]. Note that finite-horizon DECPOMDPs are NEXP complete even for two agents.\nThe set S denotes the set of environment states, with a given initial state distribution b0. The action set of agent 1 is denoted by A and agent 2 by B. The state transition probability P (s′|s, a, b) depends upon the actions of both the agents. Upon taking the joint action 〈a, b〉 in state s, agents receive the joint reward R(s, a, b). Y is the finite set of observations for agent 1 and Z for agent 2. O(s, ab, yz) denotes the probability P (y, z|s, a, b) of agent 1 observing y ∈ Y and agent 2 observing z ∈ Z when the joint action 〈a, b〉 was taken and resulted in state s.\nTo highlight the differences between a single agent POMDP and a DEC-POMDP, we note that in a POMDP an agent can maintain a belief over the environment state. However, in a DEC-POMDP, an agent is not only uncertain about the environment states but also about the actions and observations of the other agent. Therefore in a DEC-POMDP a belief over the states cannot be maintained during execution time.\nThis added uncertainty about other agents in the system make DEC-POMDPs NEXP complete [5].\nWe are concerned with solving infinite-horizon DECPOMDPs with a discount factor γ. We represent the stationary policy of each agent using a fixed size, stochastic finite-state controller (FSC) similar to [1]. An FSC can be described by a tuple 〈N, π, λ, ν〉. N denotes a finite set of controller nodes n; π : N → ∆A represents the actions selection model or the probability πan = P (a|n); λ : N × Y → ∆N represents the node transition model or the probability λn′ny = P (n\n′|n, y); ν : N → ∆N represents the initial node distribution νn = P (n). We adopt the convention that nodes of agent 1’s controller are denoted by p and agent 2’s by q. Other problem parameters such as observation function P (y, z|s, a, b) are represented using subscripts as Pyzsab. The value for starting the controllers in nodes 〈p, q〉 at state s is given by:\nV (p, q, s) = ∑ a,b πapπbq [ Rsab+\nγ ∑ s′ Ps′sab ∑ y,z Pyzs′ab ∑ p′,q′ λp′pyλq′qzV (p ′, q′, s′) ] .\nThe goal is to set the parameters 〈π, λ, ν〉 of the agents’ controllers (of some given size) that maximize the expected discounted reward for the initial belief b0:\nV (b0) = ∑ p,q,s νpνqb0(s)V (p, q, s)"
    }, {
      "heading" : "3 DEC-POMDPs as mixture of DBNs",
      "text" : "In this section, we describe how DEC-POMDPs can be reformulated as a mixture of DBNs such that maximizing the reward likelihood (to be defined later) in this framework is equivalent to optimizing the joint policy. Our approach is based on the framework proposed in [19, 20] to solve Markovian planning problems using probabilistic inference. First we informally describe the intuition behind this reformulation (for details please refer to [19]) and then we describe in detail the steps specific to DEC-POMDPs.\nA DEC-POMDP can be described using a single DBN where the reward is emitted at each time step. However, in our approach, it is described by an infinite mixture of a special type of DBNs where reward is emitted only at the end. For example, Fig. 1(a) describes the DBN for time t = 0. The key intuition is that for the reward emitted at any time step T , we have a separate DBN with the general structure as in Fig. 1(b). Further, to simulate the discounting of rewards, probability of time variable T is set as P (T = t) = γt(1− γ). This ensures that ∑∞ t=0 pt = 1. In addition, the random variable r shown in Fig. 1(a,b)\n1\n1\n1\nis a binary variable with its conditional distribution (for any time T ) described using the normalized immediate reward as R̂sab = P (r = 1|sT = s, aT = a, bT = b) = (Rsab − Rmin)/(Rmax − Rmin). This scaling of the reward is the key to transforming the optimization problem from the realm of planning to likelihood maximization as stated below. θ denotes the parameters 〈π, λ, ν〉 for each agent’s controller.\nTheorem 1. Let the CPT of binary rewards r be such that R̂sab ∝ Rsab and the discounting time prior be set as P (T ) = γT (1−γ). Then, maximizing the likelihood Lθ = P (r = 1; θ) in the mixture of DBNs is equivalent to optimizing the DEC-POMDP policy. Furthermore, the joint-policy value relates linearly to the likelihood as V θ = (Rmax −Rmin)Lθ/(1− γ) + ∑ T γ TRmin\nThe proof is omitted as it is very similar to that of MDPs and POMDPs [19]. Before detailing the EM algorithm, we describe the DBN representation of DECPOMDPs–the basis for any inference technique.\nThe DBN for any time step T is shown in Fig. 1(b). Every node is a random variable with subscripts indicating time. pi denotes controller nodes for agent 1 and qi for agent 2. The remaining nodes represent the states, actions, and observations. There are four kinds of dependencies induced by the DEC-POMDP model that the DBN must represent:\n• State transitions: State transitions as a result of the joint action of both agents and the previous state, shown by the DBN’s middle layer.\n• Controller node transitions (λ): These transitions depend on the last controller state and the most recent individual observation received. They are shown in the top and bottom layers.\n• Action probabilities (π): The action taken at any time step t depends on the current controller state. The links between controller nodes (pi or qi) and action nodes (ai or bi) model this. • Observation probabilities: First, the probability of receiving joint observation yizi depends on the joint action of both agents and the domain\nstate. This relationship is modeled by the DBN nodes labeled yizi. Second, the individual observation each agent receives is a deterministic function of the joint observation. That is Pyy′z′ = P (y|y′z′) = 1 if y = y′ else 0. This is modeled by a link between yizi and the nodes yi and zi.\nTo highlight the differences from a POMDP, Fig. 1(c) shows the DBN for a POMDP. The sheer scale of interactions present in a DEC-POMDP DBN become clear from this comparison, also highlighting the difficulty of solving DEC-POMDPs even approximately. In a POMDP, an agent receives the observation which is affected by the environment state, whereas in a DECPOMDP agents only perceive the individual part of the joint observation yizi. Such differences in the interaction structure make the E and M steps of a DECPOMDP EM very different from that of a POMDP, despite sharing the same high-level principles."
    }, {
      "heading" : "4 EM algorithm for DEC-POMDPs",
      "text" : "This section describes the EM algorithm [7] for maximizing the reward likelihood in the mixture of DBNs representing DEC-POMDPs. In the corresponding DBNs, only the binary reward is treated as observed (r = 1); all other variables are latent. While maximizing the likelihood, EM yields the DEC-POMDP joint-policy parameters θ. EM also possesses the desirable anytime characteristic as the likelihood (and the policy value which is proportional to the likelihood) is guaranteed to increase per iteration until convergence. We note that EM is not guaranteed to converge to the global optima. However, in the experiments we show that EM almost always achieves similar values as the state-of-the-art NLP based solver [1] and much better than DEC-BPI [4]. The main advantage of using EM lies in its ability to easily generalize to much richer representations than currently possible for DECPOMDPs such as factored or hierarchical controllers, continuous state and action spaces. Another important advantage is the ability to generalize the solver to larger multi-agent systems with more than 2 agents.\nThe E step we derive next is generic as any probabilistic inference technique can be used."
    }, {
      "heading" : "4.1 E-step",
      "text" : "In the E-step, for the fixed parameter θ, forward messages α and backward messages β are propagated. First, we define the following Markovian transitions on the (p, q, s) state in the DBN of Fig. 1(b). These transitions are independent of the time t due to the stationary joint policy. We also adopt the convention that for any random variable v, v′ refers to the next time slice and v̄ refers to the previous time slice. For any group of variables v, Pt(v,v ′) refers to P (vt = v,vt+1 = v ′). P (p′, q′, s′|p, q, s) =∑ aby′z′ λp′py′λq′qz′Py′z′abs′πapπbqPs′sab (1)\nαt is defined as Pt(p, q, s; θ). It might appear that we need to propagate α messages for each DBN separately, but as pointed out in [19], only one sweep is required as the head of the DBN is shared among all the mixture components. That is, α2 is the same for all the T-step DBNs with T ≥ 2. We will omit using θ as long as it is unambiguous.\nα0(p, q, s) = νpνqb0(s)\nαt(p ′, q′, s′) = ∑ p,q,s P (p′, q′, s′|p, q, s)αt−1(p, q, s)\nIntuitively, α messages compute the probability of visiting a particular (p, q, s) state in the DBN as per the current policy. The β messages are similar to computing the value of starting the controllers in nodes 〈p, q〉 at state s using dynamic programming. They are propagated backwards and are defined as Pt(r = 1|p, q, s). However, this particular definition would require separate inference for each DBN as for T and T ′ step DBN, βt will be different due to difference in the time-to-go (T − t and T ′ − t). To circumvent this problem, β messages are indexed backward in time as βτ (p, q, s) = PT−τ (r = 1|p, q, s) using the index τ such that τ = 0 denotes the time slice t = T . Hence we get:\nβ0(p, q, s) = ∑ ab Rsabπapπbq\nβτ (p, q, s) = ∑ p′,q′,s′ βτ−1(p ′, q′, s′)P (p′, q′, s′|p, q, s)\nBased on the α and β messages we also calculate two more quantities α̂(p, q, s) = ∑ t P (T = t)α(p, q, s) and\nβ̂(p, q, s) = ∑ t P (T = t)β(p, q, s), which will be used in the M-step. The cut-off time for message propagation can either be fixed a priori or be more flexible based on the likelihood accumulation. If α messages\nare propagated for t-steps and β-messages for τ steps, then the likelihood for T = t+ τ is given by\nLθt+τ = P (r=1|T = t+ τ ; θ) = ∑ p,q,s αt(p, q, s)βτ (p, q, s)\nIf both α and β messages are propagated for k steps and Lθ2k ∑2k−1 T=0 γ\nTLθT , then the message propagation can be stopped."
    }, {
      "heading" : "4.1.1 Complexity",
      "text" : "Calculating the Markov transitions on the (p, q, s) chain has complexity O(N4S2A2Y 2), where N is the maximum number of nodes for a controller. The message propagation has complexity O(TmaxN\n4S2). Techniques to effectively reduce this complexity without sacrificing accuracy will be discussed later."
    }, {
      "heading" : "4.2 M-step",
      "text" : "In the DBNs of Fig. 1(a,b) every variable is hidden except the reward variable. After each M-step, EM provides better estimates of these variables, improving the likelihood Lθ and hence the policy value. For details of EM, we refer to [7]. The parameters to estimate are 〈π, λ, ν〉 for each agent. For a particular DBN for time T , let L̃ = (P,Q,A,B, S) denote the latent variables, where each variable denotes a sequence of length T . That is, P = p0:T . EM maximizes the following expected complete log-likelihood for the DEC-POMDP DBN mixture. θ denotes the previous parameters and θ? denotes new parameters.\nQ(θ, θ?) = ∑ T ∑ L̃ P (r=1, L̃, T ; θ) logP (r=1, L̃, T ; θ?)\nIn the rest of the section, all the derivations refer to the general DBN structure of the DEC-POMDP as in Fig. 1(b). The joint probability of all the variables is: P (r = 1, L̃, T ; θ) = P (T ) [ Rsab ] t=T [ T∏ t=1 πapπbqPss̄āb̄\nPyyzPzyzPyzsāb̄λpp̄yλqq̄z ][ πapπbqνpνqb0(s) ] t=0\n(2)\nwhere brackets indicate the time slices, i.e.,[ Rsab ] t=T = R(sT , aT , bT ). Taking the log, we get:\nlogP (r = 1, L̃, T ) = . . .+ T∑ t=0 log πatpt + T∑ t=0 log πbtqt\n+ T∑ t=1 log λptpt−1yt + T∑ t=1 log λqtqt−1zt\n+ log νp0 + log νq0 (3)\nwhere the missing terms represents the quantities independent of θ. As all the policy parameters 〈π, λ, ν〉 get separated out for each agent in the log above, we first derive the action updates for an agent by substituting Eq. 3 in Q(θ, θ?)"
    }, {
      "heading" : "4.2.1 Action updates",
      "text" : "The update for action parameters π?ap for agent 1 can be derived by simplifying Q(θ, θ?) as follows:\nQ(θ, θ?) = ∞∑ T=0 P (T ) T∑ t=0 ∑ a,p [ P (r=1, a, p|T ; θ) ] t log π?ap\nBy breaking the above summation between t = T and t = 0 to T − 1, we get ∞∑ T=0 P (T ) ∑ apqbs RsabπapπbqαT (p, q, s) log π ? ap+ ∞∑ T=0 P (T )\nT−1∑ t=0 ∑ app′q′s′ βT−t−1(p ′, q′, s′)Pt(a, p, p ′, q′, s′) log π?ap\nIn the above equation, we marginalized the last time slice over the variables (q, b, s). For the intermediate time slice t, we condition upon the variables (p′, q′, s′) in the next time slice t+ 1. We now use the definition of α̂ and move the summation over time T inside for the last time slice and further marginalize over the remaining variables (q, s) in the intermediate slice t: = ∑\na,p,q,b,s\nRsabπapπbqα̂(p, q, s) log π ? ap +\n∞∑ T=0 P (T ) T−1∑ t=0 ∑ ap log π?ap ∑ p′q′s′sq βT−t−1(p ′, q′, s′)πap\nP (p′, q′, s′|a, p, q, s)αt(p, q, s)\nUpon further marginalizing over the joint observations y′z′ and simplifying we get: = ∑ ap πap log π ? ap ∑ qs [∑ b Rsabπbqα̂(p, q, s) +\n∑ p′q′s′y′z′ ∞∑ T=0 P (T ) T−1∑ t=0 βT−t−1(p ′, q′, s′)P (s′|a, q, s)\nλp′py′λq′qz′P (y ′z′|a, q, s′)αt(p, q, s) ] We resolve the above time summation, as in [19], based\non the fact that ∑∞ T=0 ∑T−1 t=0 f(T − t − 1)g(t) can be\nrewritten as ∑∞ t=0 ∑∞ T=t+1 f(T − t − 1)g(t) and then\nsetting τ = T − t− 1 to get ∑∞ t=0 g(t) ∑∞ τ=0 f(τ). Finally we get: = ∑ ap πap log π ? ap ∑ qs α̂(p, q, s) [∑ b Rsabπbq + γ\n1− γ∑ p′q′s′y′z′ β̂(p′, q′, s′)λp′py′λq′qz′P (s ′|a, q, s)P (y′z′|a, q, s′) ]\nThe product P (s′|a, q, s)P (y′z′|a, q, s′) can be further simplified by marginalizing out over actions b of agent 2 as follows: = ∑ ap πap log π ? ap ∑ qs α̂(p, q, s) [∑ b Rsabπbq + γ\n1− γ∑ p′q′s′y′z′ β̂(p′, q′, s′)λp′py′λq′qz′ ∑ b Py′z′s′abπbqPs′sab ]\nThe above expression is maximized by setting the parameter π?ap to be:\nπ?ap = πap Cp ∑ qs α̂(p, q, s) [∑ b Rsabπbq + γ\n1− γ∑ p′q′s′y′z′ β̂(p′, q′, s′)λp′py′λq′qz′ ∑ b Py′z′s′abπbqPs′sab ] (4)\nwhere Cp is a normalization constant. The action parameters π?bq of the other agent can be found similarly by the analogue of the previous equation."
    }, {
      "heading" : "4.2.2 Controller node transition updates",
      "text" : "The update for controller node transition parameters λpp̄y for agent 1 can be found by maximizing Q(θ, θ\n?) w.r.t λ?pp̄y as follows.\nQ(θ, θ?)= ∞∑ T=0 P (T ) T∑ t=1 ∑ pp̄y [ P (r=1, p, p̄, y|T ; θ) ] t log λ?pp̄y\nBy marginalizing over the variables (q, s) for the current time slice t, we get\n= ∞∑ T=0 P (T ) T∑ t=1 ∑ pp̄ysq log λ?pp̄yβT−t(p, q, s)Pt(p, p̄, y, s, q|T ; θ)\nBy further marginalizing over the variables (s̄, q̄) for the previous time slice of t and over the observations z of the other agent, we get = ∑ pp̄y λpp̄y log λ ? pp̄y ∞∑ T=0 P (T ) T∑ t=1 ∑ sqs̄q̄z βT−t(p, q, s)λqq̄z\nP (yz|p̄, q̄, s)P (s|p̄, q̄, s̄)αt−1(p̄, q̄, s̄)\nThe above equation can be further simplified by marginalizing the product P (yz|p̄, q̄, s)P (s|p̄, q̄, s̄) over actions a and b of both the agents as follows: = ∑ pp̄y λpp̄y log λ ? pp̄y ∞∑ T=0 P (T ) T∑ t=1 ∑ sqs̄q̄z βT−t(p, q, s)λqq̄z\nαt−1(p̄, q̄, s̄) ∑ ab PyzsabPss̄abπap̄πbq̄\nUpon resolving the time summation as before, we get the final M-step estimate:\nλ?pp̄y = λpp̄y Cp̄y ∑ sqs̄q̄z\nα̂(p̄, q̄, s̄)β̂(p, q, s)λqq̄z∑ ab PyzsabPss̄abπap̄πbq̄ (5)\nThe parameters λ?qq̄z for the other agent can be found in an analogous way."
    }, {
      "heading" : "4.2.3 Initial node distribution",
      "text" : "The initial node distribution ν for controller nodes of agent 1 and 2 can be updated as follows. We do not show the complete derivation as it is similar to that of the other parameters.\nν?p = νp Cp ∑ qs β̂(p, q, s)νqPsb0(s) (6)"
    }, {
      "heading" : "4.2.4 Complexity and implementation issues",
      "text" : "The complexity of updating all action parameters is O(N4S2AY 2). Updating node transitions requires O(N4S2Y 2 + N2S2Y 2A2). This is relatively high when compared to the POMDP updates requiring O(N2S2AY ) mainly due to the scale of the interactions present in DEC-POMDPs.\nIn our experimental settings, we observed that having a relatively small sized controller (N ≤ 5) suffices to yield good quality solutions. The main contributor to the complexity is the factor S2 as we experimented with large domains having nearly 250 states. The good news is that the structure of the E and M-step equations provides a way to effectively reduce this complexity by significant factor without sacrificing accuracy. For a given state s, joint action 〈a, b〉 and joint observation 〈y, z〉, the possible next states can be calculated as follows: succ(s, a, b, y, z) = {s′|P (s′|s, a, b)P (y, z|s′, a, b) > 0}. For most of the problems, the size of this set is typically a constant k < 10. Such simple reachability analysis and other techniques could speed up the EM algorithm by more than an order of magnitude for large problems. The effective complexity reduces to O(N4SAY 2k) for the action updates and O(N4SY 2k+N2SY 2A2k) for node transitions. Other enhancements of the EM implementation are discussed in Section 6."
    }, {
      "heading" : "5 Experiments",
      "text" : "We experimented with several standard 2-agent DECPOMDP benchmarks with discount factor 0.9. Complete details of these problems can be found in [1, 4].\nWe compare our approach with the decentralized bounded policy iteration (DEC-BPI) algorithm [4] and a non-convex optimization solver (NLP) [1]. The DEC-BPI algorithm iteratively improves the parameters of a node using a linear program while keeping the other nodes’ parameters fixed. The NLP approach recasts the policy optimization problem as a non-linear program and uses an off-the-shelf solver, Snopt [9], to obtain a solution. We implemented the EM algorithm in JAVA. All our experiments were on a Mac with 4GB RAM and 2.4GHz CPU. Each data point is an average of 10 runs with random initial controller parameters. In terms of solution quality, EM is always better than DEC-BPI and it achieves similar or higher solution quality than NLP. We note that our current implementation is mainly a proof-of-concept; we have not yet implemented several enhancements (discussed later) that could improve the performance of the EM approach. In contrast, the NLP solver [9] is an optimized package and therefore for larger problems is currently faster than the EM approach. The fact that a crude implementation of the EM approach works so well is very encouraging.\nTable 1 shows results for the broadcast channel problem, which has 4 states, 2 actions per agent and 5 observations. This is a networking problem where agents must decide whether or not to send a message on a shared channel and must avoid collision to get a reward. We tested with different controller sizes. On this problem, all the algorithms compare reasonably well, with EM being better than DEC-BPI and very close in value to NLP. The time for NLP is also ≈ 1s.\nFig. 2(a) compares the solution quality of the EM approach against DEC-BPI and NLP for varying controller sizes on the recycling robots problem. In this problem, two robots have the task of picking up cans in an office building. They can search for a small can, a big can or recharge the battery. The large item is only retrievable by the joint action of the two robots. Their goal is to coordinate their actions to maximize the joint reward. EM(2) and NLP(2) show the results with controller size 2 for both agents in Fig. 2(a). For this problem, EM works much better than both DEC-BPI and the NLP approach. EM achieves a value of ≈ 62 for all controller sizes, providing nearly 12% improvement over DEC-BPI (= 55) and 20% improvement over NLP (= 51). Fig. 2(b) shows the time comparisons for EM with different controller sizes. Both the NLP and DEC-BPI take nearly 1s to converge. EM\nwith controller size 2 has comparable performance, but as expected, EM with 4-node controllers takes longer as the complexity of EM is proportional to O(N4).\nFig. 2(c) compares the solution quality of EM on the meeting on a grid problem. In this problem, agents start diagonally across in a 2 × 2 grid and their goal is to take actions such that they meet each other (i.e., share the same square) as much as possible. As the figure shows, EM provides much better solution quality than the NLP approach. EM achieves a value of ≈ 7, which nearly doubles the solution quality achieved by NLP (= 3.3). DEC-BPI results are not plotted as it performs much worse and achieves a solution quality of 0, essentially unable to improve the policy at all even for large controllers. Both DEC-BPI and NLP take around 1s to converge. Fig. 2(d) shows the time comparison for EM versions. EM with 2-node controllers is very fast and takes < 1s to converge (50 iterations). Also note that in both the cases, EM could run with much larger controller sizes (≈10), but the increase in size did not provide tangible improvement in solution quality.\nFig. 3 shows the results for the multi-agent tiger problem, involving two doors with a tiger behind one door and a treasure behind the other. Agents should coordinate to open the door leading to the treasure [1]. Fig. 3(a) shows the quality comparisons. EM does not perform well in this case; even after increasing the controller size, it achieves a value of −19. NLP works better with large controller sizes. However, this experiment presents an interesting insight into the workings of EM as related to the scaling of the rewards. Recalling the relation between the likelihood and the policy value from Theorem 1, the equation for this problem\nis: V θ = 1210Lθ − 1004.5. For EM to achieve the same solution as the best NLP setting (= −3), the likelihood should be .827. Fig. 3(b) shows that the likelihood EM converges to is .813. Therefore, from EM’s perspective, it is finding a really good solution. Thus, the scaling of rewards has a significant impact (in this case, adverse) on the policy value. This is a potential drawback of the EM approach, which applies to other Markovian planning problems too when using the technique of [19]. Incidently, DEC-BPI performs much worse on this problem and gets a quality of −77.\nFig. 4 shows the results for the two largest DECPOMDP domains–box pushing and Mars rovers. In the box pushing domain, agents need to coordinate and push boxes into a goal area. In the Mars rovers domain, agents need to coordinate their actions to perform experiments at multiple sites. Fig. 4(a) shows that EM performs much better than DEC-BPI for every controller size. For controller size 2, EM achieves better quality than NLP with comparable runtime (Fig. 4(b), 500 iterations). However, for the larger controller size (= 3), it achieves slightly lower quality than NLP. For the largest Mars rovers domain (Fig. 4(c)), EM achieves better solution quality (= 9.9) than NLP (= 8.1). However, EM also takes many more iterations to converge than for previous problems and hence, requires more time than NLP. EM is also much better than DEC-BPI, which achieves a quality of −1.18 and takes even longer to converge (Fig. 4(d))."
    }, {
      "heading" : "6 Conclusion and future work",
      "text" : "We present a new approach to solve DEC-POMDPs using inference in a mixture of DBNs. Even a simple implementation of the approach provides good results. Extensive experiments show that EM is always better than DEC-BPI and compares favorably with the stateof-the-art NLP solver. The experiments also highlight two potential drawbacks of the EM approach: the adverse effect of reward scaling on solution quality and slow convergence rate for large problems. We are currently addressing the runtime issue by parallelizing the algorithm. For example, α and β can be propagated in parallel. Even updating each node’s parameters can\nbe done in parallel for each iteration. Furthermore, the structure of EM’s update equations is very amenable to Google’s Map-Reduce paradigm [6], allowing each parameter to be computed by a cluster of machines in parallel using Map-Reduce. Such scalable techniques will certainly make our approach many times faster than the current serial implementation. We are also investigating how a different scaling of rewards affects the convergence properties of EM.\nThe main benefit of the EM approach is that it opens up the possibility of using powerful probabilistic inference techniques to solve decentralized planning problems. Using a graphical DBN structure, EM can easily generalize to richer representations such as factored or hierarchical controllers, or continuous state and action spaces. Unlike the existing techniques, EM can easily extend to larger multi-agent systems with more than 2 agents. The ND-POMDP model [13] is a class of DECPOMDPs specifically designed to support large multiagent systems. It makes some restrictive yet realistic assumptions such as locality of interaction among agents, and transition and observation independence. EM can naturally exploit such independence structure in the DBN and scale to larger multi-agent systems, something that current infinite-horizon algorithms fail to achieve. Hence the approach we introduce offers great promise to overcome the shortcomings of the prevailing approaches to multi-agent planning."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Support for this work was provided in part by the National Science Foundation Grant IIS-0812149 and by the Air Force Office of Scientific Research Grant FA9550-08-1-0181."
    } ],
    "references" : [ {
      "title" : "Optimizing fixed-size stochastic controllers for POMDPs and decentralized POMDPs",
      "author" : [ "C. Amato", "D.S. Bernstein", "S. Zilberstein" ],
      "venue" : "JAAMAS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Planning by probabilistic inference",
      "author" : [ "H. Attias" ],
      "venue" : "In Workshop on AISTATS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "Solving transition independent decentralized markov decision processes",
      "author" : [ "R. Becker", "S. Zilberstein", "V. Lesser", "C.V. Goldman" ],
      "venue" : "JAIR, 22:423–455,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Policy iteration for decentralized control of Markov decision processes",
      "author" : [ "D.S. Bernstein", "C. Amato", "E.A. Hansen", "S. Zilberstein" ],
      "venue" : "JAIR, 34:89–132,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "The complexity of decentralized control of Markov decision processes",
      "author" : [ "D.S. Bernstein", "R. Givan", "N. Immerman", "S. Zilberstein" ],
      "venue" : "J. MOR,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "MapReduce: a flexible data processing",
      "author" : [ "J. Dean", "S. Ghemawat" ],
      "venue" : "tool. CACM,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Maximum likelihood from incomplete data via the EM algorithm",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.B. Rubin" ],
      "venue" : "Journal of the Royal Statistical society, Series B,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1977
    }, {
      "title" : "Point-based incremental pruning heuristic for solving finite-horizon DEC-POMDPs",
      "author" : [ "J.S. Dibangoye", "A.-I. Mouaddib", "B. Chaib-draa" ],
      "venue" : "In AAMAS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "SNOPT: An SQP algorithm for large-scale constrained optimization. SIOPT",
      "author" : [ "P.E. Gill", "W. Murray", "M.A. Saunders" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2002
    }, {
      "title" : "New inference strategies for solving Markov decision processes using reversible jump MCMC",
      "author" : [ "M. Hoffman", "H. Kueck", "N. de Freitas", "A. Doucet" ],
      "venue" : "In UAI,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Point based backup for decentralized POMDPs: Complexity and new algorithms",
      "author" : [ "A. Kumar", "S. Zilberstein" ],
      "venue" : "In AAMAS,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Complexity of finite-horizon Markov decision process problems",
      "author" : [ "M. Mundhenk", "J. Goldsmith", "C. Lusena", "E. Allender" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2000
    }, {
      "title" : "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs",
      "author" : [ "R. Nair", "P. Varakantham", "M. Tambe", "M. Yokoo" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2005
    }, {
      "title" : "Optimal and approximate Q-value functions for decentralized POMDPs",
      "author" : [ "F.A. Oliehoek", "M.T.J. Spaan", "N.A. Vlassis" ],
      "venue" : "JAIR, 32:289–353,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Anytime pointbased approximations for large POMDPs",
      "author" : [ "J. Pineau", "G. Gordon", "S. Thrun" ],
      "venue" : "JAIR, 27:335–380,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2006
    }, {
      "title" : "Memory-bounded dynamic programming for DEC-POMDPs",
      "author" : [ "S. Seuken", "S. Zilberstein" ],
      "venue" : "In IJCAI, pages 2009–2015,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2007
    }, {
      "title" : "Heuristic search value iteration for POMDPs",
      "author" : [ "T. Smith", "R. Simmons" ],
      "venue" : "In UAI,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "Hierarchical POMDP controller optimization by likelihood maximization",
      "author" : [ "M. Toussaint", "L. Charlin", "P. Poupart" ],
      "venue" : "In UAI,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2008
    }, {
      "title" : "Probabilistic inference for solving (PO)MDPs",
      "author" : [ "M. Toussaint", "S. Harmeling", "A. Storkey" ],
      "venue" : "Technical Report EDIINF-RR-0934,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Probabilistic inference for solving discrete and continuous state markov decision processes",
      "author" : [ "M. Toussaint", "A.J. Storkey" ],
      "venue" : "In ICML,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Decentralized partially observable MDPs (DECPOMDPs) have emerged in recent years as an important framework for modeling sequential decision making by a team of agents [5].",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 2,
      "context" : "Applications of DEC-POMDPs include coordinating the operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor agents [13].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 13,
      "context" : "Applications of DEC-POMDPs include coordinating the operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor agents [13].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 4,
      "context" : "Applications of DEC-POMDPs include coordinating the operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor agents [13].",
      "startOffset" : 166,
      "endOffset" : 169
    }, {
      "referenceID" : 12,
      "context" : "Applications of DEC-POMDPs include coordinating the operation of planetary exploration rovers [3], coordinating firefighting robots [14], broadcast channel protocols [5] and target tracking by a team of sensor agents [13].",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 4,
      "context" : "However, the rich model comes with a price–optimally solving a finite-horizon DEC-POMDP is NEXP-Complete [5].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : "In contrast, finite-horizon POMDPs are PSPACE-complete [12], a strictly lower complexity class that highlights the difficulty of solving DEC-POMDPs.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 10,
      "context" : "Recently, a multitude of point-based approximate algorithms have been proposed for solving finite-horizon DEC-POMDPs [11, 8, 16].",
      "startOffset" : 117,
      "endOffset" : 128
    }, {
      "referenceID" : 7,
      "context" : "Recently, a multitude of point-based approximate algorithms have been proposed for solving finite-horizon DEC-POMDPs [11, 8, 16].",
      "startOffset" : 117,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "Recently, a multitude of point-based approximate algorithms have been proposed for solving finite-horizon DEC-POMDPs [11, 8, 16].",
      "startOffset" : 117,
      "endOffset" : 128
    }, {
      "referenceID" : 14,
      "context" : "However, unlike their point-based counterparts in POMDPs ([15, 17]), they cannot be easily adopted for the infinite-horizon case due to a variety of reasons.",
      "startOffset" : 58,
      "endOffset" : 66
    }, {
      "referenceID" : 16,
      "context" : "However, unlike their point-based counterparts in POMDPs ([15, 17]), they cannot be easily adopted for the infinite-horizon case due to a variety of reasons.",
      "startOffset" : 58,
      "endOffset" : 66
    }, {
      "referenceID" : 3,
      "context" : "[4] highlight, no analogous equation exists for DEC-POMDPs.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "To alleviate such problems, most infinite-horizon algorithms represent agent policies as finite-state controllers [1, 4].",
      "startOffset" : 114,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "To alleviate such problems, most infinite-horizon algorithms represent agent policies as finite-state controllers [1, 4].",
      "startOffset" : 114,
      "endOffset" : 120
    }, {
      "referenceID" : 3,
      "context" : "So far, only two algorithms have shown promise for effectively solving infinite-horizon DEC-POMDPs–decentralized bounded policy iteration (DEC-BPI) [4] and a non-linear programming based approach (NLP) [1].",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "So far, only two algorithms have shown promise for effectively solving infinite-horizon DEC-POMDPs–decentralized bounded policy iteration (DEC-BPI) [4] and a non-linear programming based approach (NLP) [1].",
      "startOffset" : 202,
      "endOffset" : 205
    }, {
      "referenceID" : 19,
      "context" : "’s approach of transforming the planning problem to its equivalent mixture of dynamic Bayes nets (DBNs) and using likelihood maximization in this framework to optimize the policy value [20, 19].",
      "startOffset" : 185,
      "endOffset" : 193
    }, {
      "referenceID" : 18,
      "context" : "’s approach of transforming the planning problem to its equivalent mixture of dynamic Bayes nets (DBNs) and using likelihood maximization in this framework to optimize the policy value [20, 19].",
      "startOffset" : 185,
      "endOffset" : 193
    }, {
      "referenceID" : 1,
      "context" : "Earlier work on planning by probabilistic inference can be found in [2].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 18,
      "context" : "Such approaches have been successful in solving MDPs and POMDPs [19].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "They also easily extend to factored or hierarchical structures [18] and can handle continuous action and state spaces thanks to advanced probabilistic inference techniques [10].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "They also easily extend to factored or hierarchical structures [18] and can handle continuous action and state spaces thanks to advanced probabilistic inference techniques [10].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 4,
      "context" : "In this section, we introduce the DEC-POMDP model for two agents [5].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "This added uncertainty about other agents in the system make DEC-POMDPs NEXP complete [5].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "We represent the stationary policy of each agent using a fixed size, stochastic finite-state controller (FSC) similar to [1].",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 18,
      "context" : "Our approach is based on the framework proposed in [19, 20] to solve Markovian planning problems using probabilistic inference.",
      "startOffset" : 51,
      "endOffset" : 59
    }, {
      "referenceID" : 19,
      "context" : "Our approach is based on the framework proposed in [19, 20] to solve Markovian planning problems using probabilistic inference.",
      "startOffset" : 51,
      "endOffset" : 59
    }, {
      "referenceID" : 18,
      "context" : "First we informally describe the intuition behind this reformulation (for details please refer to [19]) and then we describe in detail the steps specific to DEC-POMDPs.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 18,
      "context" : "The proof is omitted as it is very similar to that of MDPs and POMDPs [19].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "This section describes the EM algorithm [7] for maximizing the reward likelihood in the mixture of DBNs representing DEC-POMDPs.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "However, in the experiments we show that EM almost always achieves similar values as the state-of-the-art NLP based solver [1] and much better than DEC-BPI [4].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "However, in the experiments we show that EM almost always achieves similar values as the state-of-the-art NLP based solver [1] and much better than DEC-BPI [4].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 18,
      "context" : "It might appear that we need to propagate α messages for each DBN separately, but as pointed out in [19], only one sweep is required as the head of the DBN is shared among all the mixture components.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "For details of EM, we refer to [7].",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 18,
      "context" : "We resolve the above time summation, as in [19], based on the fact that ∑∞ T=0 ∑T−1 t=0 f(T − t − 1)g(t) can be rewritten as ∑∞ t=0 ∑∞ T=t+1 f(T − t − 1)g(t) and then setting τ = T − t− 1 to get ∑∞ t=0 g(t) ∑∞ τ=0 f(τ).",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "Complete details of these problems can be found in [1, 4].",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "Complete details of these problems can be found in [1, 4].",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 3,
      "context" : "We compare our approach with the decentralized bounded policy iteration (DEC-BPI) algorithm [4] and a non-convex optimization solver (NLP) [1].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 0,
      "context" : "We compare our approach with the decentralized bounded policy iteration (DEC-BPI) algorithm [4] and a non-convex optimization solver (NLP) [1].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : "The NLP approach recasts the policy optimization problem as a non-linear program and uses an off-the-shelf solver, Snopt [9], to obtain a solution.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "In contrast, the NLP solver [9] is an optimized package and therefore for larger problems is currently faster than the EM approach.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "Agents should coordinate to open the door leading to the treasure [1].",
      "startOffset" : 66,
      "endOffset" : 69
    }, {
      "referenceID" : 18,
      "context" : "This is a potential drawback of the EM approach, which applies to other Markovian planning problems too when using the technique of [19].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 5,
      "context" : "Furthermore, the structure of EM’s update equations is very amenable to Google’s Map-Reduce paradigm [6], allowing each parameter to be computed by a cluster of machines in parallel using Map-Reduce.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 12,
      "context" : "The ND-POMDP model [13] is a class of DECPOMDPs specifically designed to support large multiagent systems.",
      "startOffset" : 19,
      "endOffset" : 23
    } ],
    "year" : 2010,
    "abstractText" : "Decentralized POMDPs provide an expressive framework for multi-agent sequential decision making. While finite-horizon DECPOMDPs have enjoyed significant success, progress remains slow for the infinite-horizon case mainly due to the inherent complexity of optimizing stochastic controllers representing agent policies. We present a promising new class of algorithms for the infinite-horizon case, which recasts the optimization problem as inference in a mixture of DBNs. An attractive feature of this approach is the straightforward adoption of existing inference techniques in DBNs for solving DEC-POMDPs and supporting richer representations such as factored or continuous states and actions. We also derive the Expectation Maximization (EM) algorithm to optimize the joint policy represented as DBNs. Experiments on benchmark domains show that EM compares favorably against the state-of-the-art solvers.",
    "creator" : "TeX"
  }
}