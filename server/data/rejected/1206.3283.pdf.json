{
  "name" : "1206.3283.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Observation Subset Selection as Local Compilation of Performance Profiles",
    "authors" : [ "Yan Radovilsky" ],
    "emails" : [ "yanr@cs.bgu.ac.il", "shimony@cs.bgu.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Deciding what to sense is a crucial task, made harder by dependencies and by a nonadditive utility function. We develop approximation algorithms for selecting an optimal set of measurements, under a dependency structure modeled by a tree-shaped Bayesian network (BN).\nOur approach is a generalization of composing anytime algorithm represented by conditional performance profiles. This is done by relaxing the input monotonicity assumption, and extending the local compilation technique to more general classes of performance profiles (PPs). We apply the extended scheme to selecting a subset of measurements for choosing a maximum expectation variable in a binary valued BN, and for minimizing the worst variance in a Gaussian BN."
    }, {
      "heading" : "1 Introduction",
      "text" : "A typical diagnostic system consists of two types of variables: tests (observable) and hypotheses (unobservable), with statistical dependencies among variables. Each test, if performed, consumes resources (time or money), and provides a measurement of one or more test variables. After obtaining the values of the selected tests, the distribution of the model is updated. An objective function specifies a reward given to the system for the posterior distribution. The system should make its selection so as to optimize the objective function, a hard problem in the general case. Observation subset selection (OSS) is a restricted version of this problem, where all measurements must be selected in advance, prior to any observations. In this paper we develop approximation algorithms for some settings of the OSS problem for tree-shaped dependency structures.\nTo tackle this problem we present OSS as a variant of the following well-known meta-reasoning problem. In systems composed of several computational components (CCs), the meta-level controller should reason about allocation of available computational resources for different CCs in order to optimize the overall performance of the entire system. This task is usually referred to in the research literature as the meta-level resource allocation (MRA) problem (see for example [11]). The standard approach used to optimize the MRA task was proposed by S. Zilberstein [10, 11, 12], the technique of local compilation (LC). This technique is applied to individual CCs, represented in a form of conditional performance profiles, and generates the optimal time allocation scheme for the entire system (see Section 2). However, local compilation requires the input monotonicity assumption and is, therefore, restricted to deterministic performance profiles with scalar output quality.\nIn this paper we relax the input monotonicity assumption and extend the LC technique to more general classes of PPs. We then apply the extended approximation scheme (Section 4) to find an approximate solution to the OSS problem, under two settings, both for dependencies modeled as a tree-shaped BN: a) finding a maximum expectation variable in a binary valued BN (Section 4.1), and b) minimizing the worst variance in a Gaussian BN (Section 4.2)."
    }, {
      "heading" : "2 Background",
      "text" : "Flexible computation refers to procedures that allow a graceful trade-off to be made between the quality of results and allocation of costly resources, such as time, memory, or information [3]. Since time is usually the main computational resource, there are several alternative terms used for reference to flexible computation in the research literature: continual computation [4, 5], anytime computation [8], and anytime algorithms [1, 10, 12].\nTo predict the quality of the result which depends on the amount of allocated time, a statistical model called a performance profile (PP) is employed. The most simple version of such a PP called an expected performance profile (EPP), a mapping from consumed time to an expected result quality, Q : T → Q. Given anytime algorithm A described by EPP QA and time-dependent utility function U : T ×Q → R, optimal time allocation t∗ can be derived as follows:\nt∗ = argmax t U(t, QA(t)) (1)\nWhen an algorithm operates with some inexact inputs, the quality of its result may also strongly depend on the quality of the inputs. To address this point, a more flexible model, known as a conditional performance profile (CPP), is used. For example, an anytime algorithm with 2 inputs and one output can be represented by a CPP Q : T×Q2 → Q.\nIn a complex system, several CCs (or anytime algorithms) can be composed to achieve a required result. In this case, their PPs should be compiled in order to obtain an appropriate performance prediction for the entire system. Such a system is usually described in a graphical form by a directed acyclic graph (DAG), where each node corresponds to a single CC and is associated with an appropriate PP, while edges represent dependencies between input/output qualities of different CCs (Figure 1). This model is referred to as a composition graph (CG).\nA time allocation scheme (TAS) t̂ ∈ Tn specifies allocation of time for each of n elementary CCs of a composite system. Output quality of the whole system can be expressed as a function of a TAS in a form of composite expression (CE) φ : Tn ×Qk → Q, where k is a number of external inputs. For example, the system in figure 1 has the following CE:\nφ(t̂, qin) = Q1([t̂]1, Q2([t̂]2, Q3([t̂]3, qin)), Q3([t̂]3, qin))\nThe MRA problem for a composite system is to optimize a time-dependent utility function by selecting an appropriate TAS. Formally, given a composite system represented by CE φ, external input quality qin, and utility function U , the goal is to find TAS t̂∗ ∈ Tn which maximizes the utility:\nt̂∗ = arg max t̂∈Tn U(‖t̂‖, φ(t̂, qin)), (2)\nwhere operator ‖ · ‖ denotes a summation over all elements of its argument vector: ‖t̂‖ = ∑\ni[t̂]i.\nAn intuitive approach to tackle the MRA problem has been proposed in [10]. This approach, called compilation of performance profiles, involves construction of an appropriate PP for the entire system:\nQc(t, qin) = max t̂∈Tn:‖t̂‖=t φ(t̂, qin), (3)\nEach entry in Qc is additionally associated with a corresponding TAS π(t, qin) = t̂ s.t. φ(t̂, qin) = Q\nc(t, qin) and ‖t̂‖ = t.\nOnce such a model is constructed, the optimal solution for the MRA problem can be derived from this model as t̂∗ = π(t∗, qin), where t\n∗ is the optimal total time allocation computed using equation 1.\nAlgorithm 1: method RLC for in-tree shaped composite systems\nInput : s - a node in a composition graph. Output: Composed PP for the subtree rooted in s. if (Pred (s) = ∅) then\nreturn Qs; foreach si ∈ Pred (s) do\nLet Qcsi ← RLC (si); Let L ← {Qcsi : si ∈ Pred (s)}; return Compose (Qs, L);\nIn general, the task of (global) compilation is computationally hard even for approximate solution [10]. However, for some restricted topologies (e.g. trees) S. Zilberstein [10] proposed an efficient algorithm based on the local compilation (LC) technique, summarized as RLC in Algorithm 1. In this technique a composite PP of each subtree is generated based on the CPP of its root and composed PPs of all its predecessors. Method Compose performs the basic composition operation, which results in the following composite PP:\nQcs(t, q1, . . . , qk) = (4)\nmax t0,...,tk∈T: P ti=t Qs(t0, Q\nc s1 (t1, q1), . . . , Q c sk (tk, qk))\nwhere s1, . . . , sk are the predecessors of node s.\nFor systems, where all CCs are represented by deterministic CPPs, RLC is proved to produce the correct result (equals to one obtained by global compilation) when the following assumptions hold:\n• A tree-structured CG - each node has only one output which serves as an input for one successor node except for the root node, whose output is the resulting system output.\n• Input monotonicity (IM) - the utility function and all involved CPPs are non-decreasing functions of input quality, i.e. ∀q ≥ p : Q(t, q) ≥ Q(t, p).\nSuch restrictive assumptions are required to enable independent compilation of each subtree followed by greedy selection of one local TAS for each total allocation without harming optimality of the resulting solution. However, in practice, the IM assumption may be too restrictive. For example, a diagnostic system with a dependency model represented by Gaussian BN and a utility function depending on posterior marginal precision of several hypothesis nodes, violates this assumption. In this work we propose an extension to the CPP model, which allows relaxing the IM assumption.\nRecent work exists in another line directly related to the OSS task. In [7] the authors proved that the OSS problem is NPPP -hard even for tree-structured BNs. They proposed a polynomial time algorithm, based on dynamic programming, which constructs an optimal solution to a version of OSS restricted to chain topology, exact observation, and additive reward. In [9] a similar technique was used beyond the exact observation assumption, and determined a theoretical bound for the worst-case loss in expected reward. Our work can be seen as an extention of the latter research. Another approximation method based on greedy selection of test nodes is applicable when the reward function exhibits property of sub-modularity [6]."
    }, {
      "heading" : "3 Generalized Local Compilation",
      "text" : "This section generalizes the notion of a performance profile to a reachable performance profile, and adapts the LC technique to handle the extended model.\nDefinition 3.1 (Conditional performance). Conditional performance (CP) of a CC (either elementary or composite) is a tuple (t̂, p, q), where t̂ is a local TAS (t-component), p is a required input quality (pcomponent), and q is an expected output quality, obtained by the CC when applied to inputs of quality p with TAS t̂ (q-component).\nEither p or q may be represented by scalars or by vectors, while t̂ is assumed to be a complete assignment of time allocations to all CCs of the system.\nDefinition 3.2 (Reachable performance profile). Reachable performance profile (RPP) of a CC is a set of CPs achievable by this CC.\nThe RPP model can be derived from a CPP as follows:\nRA = {(πA(t, p), p,QA(t, p))}, (5)\nwhere QA is a CPP, and RA is an appropriate RPP. The converse is not always possible, because a RPP\ncan contain more than one output quality for the same pair of total time allocation and input quality (each with a different local TAS).\nWe further extend our model by allowing backward conditioning, which means that output quality of a CC may depend on output quality of its successor. The general form of a CP in this case is (t̂, (psuc, ppred), (qsuc, qpred)), where the p-component has two parts: psuc is a required output quality of the successor, and ppred is a vector of required output qualities of all the predecessors; the q-component has two parts as well: an output quality towards its successor (qsuc), and a vector of qualities towards all its predecessors (qpred). Such a form of CPs is useful in applications for BNs, where posterior probability distribution of a node (and its local reward) depends on the messages coming from all its neighbors (details appear in section 4).\nIn order to adapt the RLC algorithm to the extended settings, we need to modify the Compose method. This method is now applied to a list of profiles in the RPP representation, and its output should be in the RPP form as well. Moreover, the greedy selection reflected in the max operator in equation 3 strongly relies on the IM assumption. Since this assumption fails in the RPP case, we propose another approach. The composition process comprises two parts: the first part (Construct) considers all combinations of input CPs, and collects appropriate resulting CPs.\nQ = {(t̂0 + t̂1 + . . .+ t̂k, p0, q0) : (t̂i, pi, qi) ∈ Qsi ,\n(t̂0, (p0, q1, . . . , qk), (q0, p1, . . . , pk)) ∈ Qs}. (6)\nThe second part, called Purge is applied to the set Q. Purge exploits domination and equivalence among reachable CPs in order to filter out irrelevant CPs.1 The idea of pruning irrelevant CPs is very similar to one used in the Incremental Pruning algorithm [2] for filtering irrelevant α-vectors. The resulting RPP Qc keeps one representative for each CP in Q:\n∀a ∈ Q ∃b ∈ Qcs.t. (b ≻ a) ∨ (b ≈ a).\nwhere ≻ and ≈ are domination and equivalence operators respectively. These operators can be defined based on partial orders within each performance component. Assuming only a natural partial order in the t-component we obtain:\n(t̂, p, q) ≈ (t̂′, p′, q′) ⇔ (‖t̂‖ = ‖t̂′‖)∧ (p P ≈ p′)∧ (q Q ≈ q′)\nand\n(t̂, p, q) ≻ (t̂′, p′, q′) ⇔ (‖t̂‖ < ‖t̂′‖)∧ (p P ≈ p′)∧ (q Q ≈ q′)\n1The filtering can be applied after Construct terminates, but it is usually more efficient to perform filtering on-the-fly.\nwhere P ≈ and Q ≈ are equivalence operators defined over the p and q components, respectively."
    }, {
      "heading" : "4 Observation Selection in BNs.",
      "text" : "In this section we describe how our framework can be applied to OSS in BNs. We use the following notation:\n• X = {Xi : 1 ≤ i ≤ n} - set of all state variables;\n• XH ⊆ X - set of hypothesis state variables;\n• XM ⊆ X - set of measurable state variables;\n• Y = {Yi : Xi ∈ XM} - set of test variables;\n• N - BN over X ∪Y;\n• R : Pr(XH) → R - reward function;\n• T (E) = ∑\nYi∈E τi - additive time consumption;\n• B - time budget (maximum time for observation).\nDefinition 4.1 (OSS optimization problem). The OSS optimization problem is: given a tuple (N,R, T,B), select a subset of observation variables E ⊆ Y which maximizes the expected reward:\nR̂(XH |E) = ∑\ne∈Dom(E)\nPr(E = e)R(XH |E = e) (7)\nsubject to the budget limit T (E) ≤ B.\nWe assume BNs with an out-tree shaped dependency graph rooted in nodeX1 (as shown in figure 2). LetX I s denote a subset of X, which consists of all nodes Xi in the subtree rooted in Xs (all descendants ofXs including Xs itself), and let E ⊆ Y be any given subset of observation (“evidence”) nodes. We use the following\nnotation to refer to other relevant subsets of nodes:\nXOs = X \\X I s ; YIs = {Yi : Xi ∈ (XM ∩X I s)}; YOs = Y \\Y I s ; EIs = E ∩Y I s ; EOs = E ∩Y O s ."
    }, {
      "heading" : "4.1 OSS in discrete BNs",
      "text" : "We now apply our approach to OSS in tree-shaped BNs with discrete variables. Since solving this problem for the general setting (even for the tree-structured topology) is proved to be NPPP -hard [7], we restrict our focus to BNs with boolean variables (Dom(Xi) = Dom(Yi) = {0, 1}), and consider the following reward function, defined for an arbitrary set of nodes A:\nR(A|E = e) = max Xi∈A Ri(Xi|E = e),\nwhere eachRi : Pr(Xi) → R is a local reward function:\nRi(Xi|E = e) =\n{\nPr(Xi = 1|E = e) : if(Xi ∈ XH),\n0 : otherwise.\n(8) We refer to this version of OSS as Boolean OSS (BOSS).\nThe BN can be specified by the following parameters:\nαi =\n{\nPr(Xi = 1) : if Xi is a root,\nPr(Xi = 1|XPrev(i) = 1) : otherwise;\nβi =\n{\nPr(Xi = 1) : if Xi is a root,\nPr(Xi = 1|XPrev(i) = 0) : otherwise;\nθi =\n{\nPr(Yi = 0|Xi = 1) : if(Xi ∈ XM ),\n1 : otherwise;\nζi =\n{\nPr(Yi = 1|Xi = 0) : if(Xi ∈ XM ),\n0 : otherwise;\nWe make the following simplifying assumptions about the involved observation process:\n1. Probability of a false positive observation result for all nodes is bounded by a small constant ζmax (that is ∀i : ζi ≤ ζmax);\n2. Only hypothesis nodes have directly attached observations (XM ⊆ XH).\nHenceforth this set of assumptions is called the restricted false positive property. In the extreme case (ζmax = 0) we get a false-positive-free observation process. Despite the relatively restricted setting, we have the following complexity result:\nTheorem 1 (Hardness of BOSS). Finding an exact solution to the BOSS problem is NP-hard even when all state variables are independent (αi = βi) and all observations are exact (θi = ζi = 0).\nProof is by reduction from Knapsack, which is a wellknown NP -complete problem. Below, we show how the BOSS problem can be reduced to a special case of MRA and then solved (approximately) by the RLC algorithm.\nIn order to apply the RLC algorithm we must specify the problem in terms of a composite system. Deriving the corresponding CG is straightforward: the graph has in-tree form and can be obtained from the dependency graph by simply reversing directions of all arcs.\nCareful inspection of the false-positive-free property yields that observing one positive value at any observation node (Yi = 1) provides a sufficient evidence for determining the reward value (R(XH |Yi = 1, E = e) = Ri(Xi|Yi = 1) = 1), regardless of other observations. We employ this fact to obtain a recursive decomposition of the expected reward.\nWe specify output quality (q-component) of exploring subset E ⊆ Y w.r.t. subtree XIs as the triple (f, g, r):\nf = Pr(êIs|Xs = 1), g = Pr(êIs|Xs = 0), r = R(XIs|ê),\nwhere ê, êIs, and ê O s denote assignments of all zeros to E, EIs , and E O s respectively. While f and g components depend only on observations inside the subtree (êIs), to determine the value of the r-component we need additional information from outside the subtree, provided by the p-component: p = Pr(Xs = 1|ê O s ).\nFor each quality component we define one corresponding domain set (of relevant values):\nPs = { Pr(Xs = 1|E O s = ê O s ) : E ⊆ Y } Fs = { Pr(EIs = ê I s|Xs = 1) : E ⊆ Y } Gs = { Pr(EIs = ê I s|Xs = 0) : E ⊆ Y } Rs = { R(XIs|E = ê) : E ⊆ Y } .\nWe also define combined domain sets Qs = Fs×Gs× Rs. Finally, all alternative assignments to a number of observations (measurements) in nodeXs is represented by set Ms. In the basic OSS setting we have at most one observation per node:\nMs =\n{\n{0, 1} : if (Xs ∈ XM ),\n{0} : otherwise.\nHowever, the model can be easily extended to multiple observations (by specifying appropriate Ms sets).\nRPPs of observation nodes contain CPs with no condition (denoted by ∅ in the p-component):\nQYs = {(mτ̂s, ∅,m) : m ∈ Ms}, (9)\nwhere τ̂s is an assignment of τs time units to s and 0 to all the other CCs.\nAll leaf X-nodes are associated with RPPs of the following form:\nQXs = {(0̂, u, ψs(u)) : u ∈ Ps ×Ms} (10)\nwhere 0̂ denotes a zero time allocation (to all CCs), and ψs : Ps ×Ms → Qs is a vector function defined as follows:\nψs(p,m) = (f, g, r), (11)\nf = Pr(êIs|Xs = 1) = θ m s , g = Pr(êIs|Xs = 0) = (1− ζs) m,\nr = Rs(Xs|ê) =\n{\npf L(f,g,p) : if (Xs ∈ XH), 0 : otherwise\nIn our notation L(·, ·, ·) stands for the operator of linear interpolation defined as follows:\nL(a, b, c) = ca+ (1− c)b. (12)\nRPP of any non-leaf node Xs with k children (Xs1 , . . . , Xsk) is specified as follows:\nQXs = {(0̂, u, ψs(u)) : u ∈ Ps ×Ms ×Qs1 × · · · ×Qsk} (13) where ψs : Ps×Ms×Qs1 ×· · ·×Qsk → Qs×Ps1 ×· · ·×Psk is a vector function defined as follows:\nψs(p,m, (f1, g1, r1), . . . , (fk, gk, rk)) = (14)\n= ((f, g, r), p1, . . . , pk),\nr = max{r0, r1, . . . , rk},\nr0 = Rs(Xs|ê) =\n{\npf L(f,g,p) : if (Xs ∈ XH), 0 : otherwise\nf = Pr(êIs|Xs = 1) = θ m s\n∏\n1≤i≤k\nf ′i ,\ng = Pr(êIs|Xs = 0) = (1− ζs) m\n∏\n1≤i≤k\ng′i,\npi = Pr(Xsi = 1|ê O si ) =\n= L(αsi , βsi , pθ m s\n∏\nj 6=i f ′ j)\nL(θms ∏ j 6=i f ′ j, (1− ζs) m ∏ j 6=i g ′ j , p)\n,\nf ′i = Pr(ê I si |Xs = 1) = L(fi, gi, αsi), g′i = Pr(ê I si |Xs = 0) = L(fi, gi, βsi).\nWhile all QYs RPPs can be specified explicitly (as a list of CPs), the QXs RPPs generally cannot, due to a possibly exponential size of domains Ps, Fs, Gs and Rs in their input condition. Instead, we represent them in an implicit form by providing (parameters of) the involved ψs functions.\nDuring the compilation process (according to the RLC algorithm) method Compose is applied to lists of RPPs. At each application one composed RPP Qcs (which represents the whole subtree XIs) is generated. Due to backward conditioning on elements ofPs which can contain exponential number of elements), such a RPP cannot be derived exactly (and explicitly). There is also no obvious way to specify it implicitly by providing some predefined functions (as we do in case of individual RPPs). To address this problem, we propose to approximate all Ps sets by one uniform grid Dp defined as follows:\nDp =\n{(\nk + 1\n2\n)\nǫp : k ∈ 0, . . . , dp − 1\n}\n(15)\nwhere 0 < ǫp < 1 is a small constant, and dp = ⌈\n1 ǫp\n⌉\nis a number of intervals of size ǫp in the range [0, 1]. Thus, setDp has a fixed number of elements (dp) which makes efficient enumeration possible.\nWe define discretization function λp, which maps any value p ∈ Ps to an appropriate point in set Dp:\nλp(p) =\n(⌊\np\nǫp\n⌋\n+ 1\n2\n)\nǫp (16)\nThis discretization induces approximate equivalence relation P ≈ among elements of Ps defined as follows:\np P ≈ p′ ⇔ λp(p) = λp(p ′) (17)\nWe apply a similar discretization technique to other domain sets (Fs, Gs, andRs) with discretization steps ǫf , ǫg and ǫr respectively. The appropriate grids (Df , Dg, Dr), discretization functions (λf , λg, λr) and equivalence operators ( F ≈, G ≈, R ≈) are defined accordingly. The composed equivalence operator Q ≈ is defined as follows:\n(f, g, r) Q ≈ (f ′, g′, r′) ⇔ (f F ≈ f ′) ∧ (g G ≈ g′) ∧ (r R ≈ r′)\nTo complete the specification, we define the following comprehensive utility function:\nU(t̂, p, (f, g, r)) = (18)\n=\n{\nL(r, 1, L(f, g, α1)) : if (p P ≈ α1 ∧ ‖t̂‖ ≤ B),\n−∞ : otherwise,\nAfter compiling RPPs of the entire tree, the resulting RPP Qc1 can be used to select a near-optimal TAS w.r.t. this utility function. Let E∗ denote the optimal observation subset, and let E be a subset corresponding to the TAS selected based on the resulting RPP.\nTheorem 2 (Approximation quality of RLC for BOSS). The RLC algorithm applied to a transformed instance of BOSS with out-tree topology and a restricted false positive observation process approximates the optimal solution within additive factor of ∆u, which is bounded as follows:\n∆u = R̂(X|E ∗)− R̂(X|E) ≤ hǫp + 2nǫs + ǫr + nζmax (19) where h is a height of the tree, and ǫs = max{ǫf , ǫg}.\nSelection of the appropriate values for the grid steps depends on the required precision of the solution. To ensure approximation with ∆u ≤ ǫ+ nζmax (in worst case) we can select ǫp = ǫ 3h , ǫf = ǫg = ǫ 6n , and ǫr = ǫ 3 .\nDue to monotonicity in total time, any composed RPP can be represented by the 4-dimensional table (Dp × Df × Dg × Dr), where each entry is associated with at most one appropriate partial TAS. The number of entries (CPs) in such a table is bounded as follows:\n|Qcs| ≤ dpdfdgdr =\n⌈\n3h\nǫ\n⌉⌈\n6n\nǫ\n⌉2 ⌈ 3\nǫ\n⌉\n. (20)\nWorst-case complexity for the complete run of RLC is O ( n4\nǫ4\n) time, O ( n3\nǫ4\n)\nspace for a chain topology, and\nO (\nn2c+1hc ǫ2c+2\n) time, O (\nn2ch2c ǫ2c+2\n)\nspace for a tree with a\nmaximum branching factor of c."
    }, {
      "heading" : "4.2 OSS in Gaussian Bayesian networks",
      "text" : "Gaussian Bayesian network (GBN) is a special case of BN, where the conditional probability distributions of the variables are Normal (Gaussian) distributions:\nXi|Pa(Xi) ∼ N\n\nµi + ∑\nXj∈Pa(Xi)\nai,j(Xj − µj), σ 2 i\n\n\nWe parametrize our GBN model as follows:\nαi = a 2 i,Prev(i), βi = Prec(Xi|XPrev(i)) = 1\nσ2i ,\nθi =\n{\nPrec(Yi|Xi) : if (Xi ∈ XM ),\n0 : otherwise.\nIn our notation Prec(·) denotes the precision operator, which is reciprocal to variance:\nPrec(Xi|E) = 1\nV ar(Xi|E) . (21)\nFor Gaussian OSS (GOSS) we consider the following reward function:\nR(A|E = e) = min Xi∈A Ri(Xi|E),\nwhere Ri : Pr(Xi) → R are local reward functions:\nRi(Xi|E) =\n{\nloga,b(Prec(Xi|E)) : if(Xi ∈ XH), 1 : otherwise.\n(22) Here a and b are two parameters that determine a range of distinguishable (for reward) values of precision, and loga,b(·) denotes a normalized (and truncated at its extreme points) log operator, defined as follows:\nloga,b(p) =\n\n \n \n0 : if (p ≤ a),\n1 : if (p ≥ b), log p−log a log b−log a : otherwise.\n(23)\nTheorem 3 (Hardness of GOSS). Finding an exact solution for a general instance of the GOSS problem is NP-hard even for a tree-shaped GBN.\nProof is by reduction from Knapsack. A polynomial scheme for approximate solution of GOSS, similar to one presented for BOSS, follows.\nTo apply the RLC algorithm we specify the problem in terms of a composite system. The CG is as for BOSS. All domain sets (except for Ms, that remain the same) should be redefined as follows:\nPs = {Prec(Xs|E O s ) : E ⊆ Y}, (24) Fs = {Prec(Xs|E I s ) : E ⊆ Y}, (25) Rs = {Rs(Xs|E) : E ⊆ Y}, (26)\nQs = Fs ×Rs. (27)\nWe need to reformulate, in the definition of RPPs, the specification of the ψs functions. For the leaf nodes ψs is defined as follows:\nψs(p,m) = (f, r), (28)\nf = Prec(Xs|E I s ) = mθs,\nr = Rs(Xs|E) =\n{\nloga,b(p+ f) : if (Xs ∈ XH), 1 : otherwise,\nFor the non-leaf nodes we have:\nψs(p,m, (f1, r1), . . . , (fk, rk)) = ((f, r), p1, . . . , pk);\nf = Prec(Xs|E I s ) = mθs +\n∑\n1≤i≤k\nf ′i ,\nr = min{r0, r1, . . . , rk},\nr0 = Rs(Xs|E) =\n{\nloga,b(p+ f) : if (Xs ∈ XH), 1 : otherwise,\npi = Prec(Xsi |E O si ) = J(βsi , αsi(mθs + p+\n∑\nj 6=i\nf ′j)),\nf ′i = Prec(Xs|E I si ) =\nJ(fi, βsi)\nαsi , (29)\n(30)\nIn our notation J(·, ·) stands for the operator of precision propagation defined as follows:\nJ(a, b) =\n{\n0 : if (a = b = 0), ab a+b : otherwise.\n(31)\nAs in case of BOSS, to prevent exponential growth of the composed RPPs we apply discretization to all domains by appropriate grids. Grid Dr and corresponding discretization function λr are defined exactly as in BOSS. To define Dp we use its projection D ′ p to the [0, 1] interval (D′p is defined exactly asDp in the BOSS case):\nDp = {p : loga,b(p) ∈ D ′ p}, (32)\nWe express the discretization function λp through its projected version λ′p (which is defined as λp in BOSS):\nλp(p) = λ ′ p(loga,b(p)). (33)\nGrid Df and the corresponding discretization function λf are similarly defined.\nEquivalence operators P ≈, F ≈, and R ≈ are defined as in BOSS. The composed equivalence operator Q ≈ is:\n(f, r) Q ≈ (f ′, r′) ⇔ (f F ≈ f ′) ∧ (r R ≈ r′)\nThe comprehensive utility function is as follows:\nU(t̂, p, (f, r)) =\n{\nr : if ((p P ≈ β1) ∧ (‖t̂‖ ≤ B)),\n−∞ : otherwise (34)\nAfter compiling the composite system (using the RLC algorithm), a near-optimal TAS can be selected from the resulting RPP Qc1 w.r.t. this utility function. Let E∗ denote an optimal observation subset, and E a subset corresponding to the TAS selected from Qc1.\nTheorem 4 (Approximation quality of RLC for GOSS). The RLC algorithm applied to a transformed instance of GOSS problem with out-tree topology approximates the optimal solution E∗ within additive factor of ∆u, bounded as follows:\n∆u = R̂(XH |E ∗)− R̂(XH |E) ≤ hǫp + hǫf + ǫr (35)\nTo ensure approximation with ∆u ≤ ǫ (in worst case) we can select ǫp = ǫf = ǫ h , and ǫr = ǫ.\nAny composed RPP Qcs can be represented by a 3- dimensional (Dp ×Df ×Dr) table with a number of entries bounded as follows:\n|Qcs| ≤ dpdfdr =\n⌈\nh\nǫ\n⌉2 ⌈ 1\nǫ\n⌉\n. (36)\nThe appropriate worst-case complexity for the complete run of the RLC algorithm is O ( nh2\nǫ2\n)\ntime,\nO ( h2\nǫ3\n) space for a chain topology, and O (\nnhc+1c ǫc+2\n)\ntime, O (\nhc+2c ǫc+2\n)\nspace for a tree with maximum\nbranching factor of c."
    }, {
      "heading" : "5 Summary",
      "text" : "In this paper we extended the concept of CPP, and presented an efficient technique for compiling a composite system beyond the input monotonicity assumption. The extended scheme has been applied to optimizing a set of measurements in two different settings (for choosing a maximum expectation variable in a binary valued BN, and for minimizing the worst variance in a Gaussian BN). Polynomial time methods have been presented for both problems, and quality of approximation has been theoretically determined.\nApplying our framework to real-world domains as an empirical evaluation is underway. Further extending the framework to deal with more general system topologies, tractable strategies for active monitoring are possible directions for future research."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Partially supported by the IMG4 consortium (under the Ministry of Industry, Trade and Labor of Israel MAGNET program), by the Lynn and William Frankel Center for Computer Sciences, and by the Paul Ivanier Center for Robotics."
    } ],
    "references" : [ {
      "title" : "Solving time-dependent planning problems",
      "author" : [ "M. Boddy", "T. Dean" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1989
    }, {
      "title" : "Incremental Pruning: A simple, fast, exact method for partially observable Markov decision processes",
      "author" : [ "A. Cassandra", "M.L. Littman", "N.L. Zhang" ],
      "venue" : "In Proceedings of (UAI–97),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1997
    }, {
      "title" : "Reasoning about beliefs and actions under computational resource constraints",
      "author" : [ "E. Horvitz" ],
      "venue" : "In UAI,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1987
    }, {
      "title" : "Models of continual computation",
      "author" : [ "E. Horvitz" ],
      "venue" : "In AAAI/IAAI, pages 286–293,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "Continual computation policies for allocating offline and real-time resources",
      "author" : [ "E. Horvitz" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1999
    }, {
      "title" : "Near-optimal nonmyopic value of information in graphical models",
      "author" : [ "A. Krause", "C. Guestrin" ],
      "venue" : "In UAI,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2005
    }, {
      "title" : "Optimal nonmyopic value of information in graphical models - efficient algorithms and theoretical limits",
      "author" : [ "A. Krause", "C. Guestrin" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2005
    }, {
      "title" : "Knowledgebased anytime computation",
      "author" : [ "A.I. Mouaddib", "S. Zilberstein" ],
      "venue" : "In IJCAI, pages 775–783,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1995
    }, {
      "title" : "Efficient deterministic approximation algorithm for nonmyopic value of information in graphical models",
      "author" : [ "Y. Radovilsky", "G. Shattah", "E.S. Shimony" ],
      "venue" : "In SMC Conference,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "Operational rationality through compilation of anytime algorithms",
      "author" : [ "S. Zilberstein" ],
      "venue" : "Technical report, Computer Science Division, University of California at Berkeley,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1993
    }, {
      "title" : "Optimizing decision quality with contract algorithms",
      "author" : [ "S. Zilberstein" ],
      "venue" : "In IJCAI, pages 1576–1582,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1995
    }, {
      "title" : "Using anytime algorithms in intelligent systems",
      "author" : [ "S. Zilberstein" ],
      "venue" : "AI Magazine,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "This task is usually referred to in the research literature as the meta-level resource allocation (MRA) problem (see for example [11]).",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "Zilberstein [10, 11, 12], the technique of local compilation (LC).",
      "startOffset" : 12,
      "endOffset" : 24
    }, {
      "referenceID" : 10,
      "context" : "Zilberstein [10, 11, 12], the technique of local compilation (LC).",
      "startOffset" : 12,
      "endOffset" : 24
    }, {
      "referenceID" : 11,
      "context" : "Zilberstein [10, 11, 12], the technique of local compilation (LC).",
      "startOffset" : 12,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "Flexible computation refers to procedures that allow a graceful trade-off to be made between the quality of results and allocation of costly resources, such as time, memory, or information [3].",
      "startOffset" : 189,
      "endOffset" : 192
    }, {
      "referenceID" : 3,
      "context" : "Since time is usually the main computational resource, there are several alternative terms used for reference to flexible computation in the research literature: continual computation [4, 5], anytime computation [8], and anytime algorithms [1, 10, 12].",
      "startOffset" : 184,
      "endOffset" : 190
    }, {
      "referenceID" : 4,
      "context" : "Since time is usually the main computational resource, there are several alternative terms used for reference to flexible computation in the research literature: continual computation [4, 5], anytime computation [8], and anytime algorithms [1, 10, 12].",
      "startOffset" : 184,
      "endOffset" : 190
    }, {
      "referenceID" : 7,
      "context" : "Since time is usually the main computational resource, there are several alternative terms used for reference to flexible computation in the research literature: continual computation [4, 5], anytime computation [8], and anytime algorithms [1, 10, 12].",
      "startOffset" : 212,
      "endOffset" : 215
    }, {
      "referenceID" : 0,
      "context" : "Since time is usually the main computational resource, there are several alternative terms used for reference to flexible computation in the research literature: continual computation [4, 5], anytime computation [8], and anytime algorithms [1, 10, 12].",
      "startOffset" : 240,
      "endOffset" : 251
    }, {
      "referenceID" : 9,
      "context" : "Since time is usually the main computational resource, there are several alternative terms used for reference to flexible computation in the research literature: continual computation [4, 5], anytime computation [8], and anytime algorithms [1, 10, 12].",
      "startOffset" : 240,
      "endOffset" : 251
    }, {
      "referenceID" : 11,
      "context" : "Since time is usually the main computational resource, there are several alternative terms used for reference to flexible computation in the research literature: continual computation [4, 5], anytime computation [8], and anytime algorithms [1, 10, 12].",
      "startOffset" : 240,
      "endOffset" : 251
    }, {
      "referenceID" : 9,
      "context" : "An intuitive approach to tackle the MRA problem has been proposed in [10].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 9,
      "context" : "In general, the task of (global) compilation is computationally hard even for approximate solution [10].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 9,
      "context" : "Zilberstein [10] proposed an efficient algorithm based on the local compilation (LC) technique, summarized as RLC in Algorithm 1.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 6,
      "context" : "In [7] the authors proved that the OSS problem is NP -hard even for tree-structured BNs.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 8,
      "context" : "In [9] a similar technique was used beyond the exact observation assumption, and determined a theoretical bound for the worst-case loss in expected reward.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 5,
      "context" : "Another approximation method based on greedy selection of test nodes is applicable when the reward function exhibits property of sub-modularity [6].",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 1,
      "context" : "The idea of pruning irrelevant CPs is very similar to one used in the Incremental Pruning algorithm [2] for filtering irrelevant α-vectors.",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "Since solving this problem for the general setting (even for the tree-structured topology) is proved to be NP -hard [7], we restrict our focus to BNs with boolean variables (Dom(Xi) = Dom(Yi) = {0, 1}), and consider the following reward function, defined for an arbitrary set of nodes A: R(A|E = e) = max Xi∈A Ri(Xi|E = e), where eachRi : Pr(Xi) → R is a local reward function:",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "is a number of intervals of size ǫp in the range [0, 1].",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "To define Dp we use its projection D ′ p to the [0, 1] interval (Dp is defined exactly asDp in the BOSS case): Dp = {p : loga,b(p) ∈ D ′ p}, (32) We express the discretization function λp through its projected version λp (which is defined as λp in BOSS): λp(p) = λ ′ p(loga,b(p)).",
      "startOffset" : 48,
      "endOffset" : 54
    } ],
    "year" : 2008,
    "abstractText" : "Deciding what to sense is a crucial task, made harder by dependencies and by a nonadditive utility function. We develop approximation algorithms for selecting an optimal set of measurements, under a dependency structure modeled by a tree-shaped Bayesian network (BN). Our approach is a generalization of composing anytime algorithm represented by conditional performance profiles. This is done by relaxing the input monotonicity assumption, and extending the local compilation technique to more general classes of performance profiles (PPs). We apply the extended scheme to selecting a subset of measurements for choosing a maximum expectation variable in a binary valued BN, and for minimizing the worst variance in a Gaussian BN.",
    "creator" : null
  }
}