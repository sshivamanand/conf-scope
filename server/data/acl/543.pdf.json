{
  "name" : "543.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Character-level Compositionality with Visual Features",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\nPrevious work has modeled the compositionality of words by creating characterlevel models of meaning, reducing problems of sparsity for rare words. However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry categorical content which resulting in embeddings that are coherent in visual space."
    }, {
      "heading" : "1 Introduction",
      "text" : "Compositionality—the fact that the meaning of larger linguistic units is created by combining the meaning of smaller units—is a hallmark of every natural language (Szabó, 2010). Recently, neural models have provided a powerful tool for learning how to compose words together into a meaning representation of whole sentences for many downstream tasks. This is done using models of various levels of sophistication, from simpler bag-ofwords (Iyyer et al., 2015) and linear recurrent neural network (RNN) models (Sutskever et al., 2014; Kiros et al., 2015), to more sophisticated models\nKalb Kälbera\nDo Do'(polite)\nCalf CalvesLaurel\nWhale\nSalmon\nSalmon\ngui jing\ngui gui\n(a)\n(b)\n(c)\n(d)\nhan'da ham''ni'''da\nusing tree-structured (Socher et al., 2013) or convolutional networks (Kalchbrenner et al., 2014). In fact, a growing body of evidence shows that it is essential to look below the word-level and consider compositionality within words themselves. For example, several works have proposed models that represent words by composing together the characters into a representation of the word itself (Ling et al., 2015; Zhang et al., 2015; Dhingra et al., 2016). Additionally, for languages with productive word formation (such as agglutination and compounding), models calculating morphologysensitive word representations have been found effective (Luong et al., 2013; Botha and Blunsom, 2014). Thesemodels help to learnmore robust representations for rare words by exploiting morphological patterns, as opposed to models that operate purely on the lexical level as the atomic units. For many languages, compositionality stops at the character-level: characters are atomic units of meaning or pronunciation in the language, and no further decomposition can be done.1 However, for other languages, character-level compositionality, where a character’s meaning or pronunciation\n1In English, for example, this is largely the case.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nLang Geography Sports Arts Military Economics Transportation Chinese 32.4k 49.8k 50.4k 3.6k 82.5k 40.4k Japanese 18.6k 82.7k 84.1k 81.6k 80.9k 91.8k Korean 6k 580 5.74k 840 5.78k 1.68k Lang Medical Education Food Religion Agriculture Electronics Chinese 30.3k 66.2k 554 66.9k 89.5k 80.5k Japanese 66.5k 86.7k 20.2k 98.1k 97.4k 1.08k Korean 16.1k 4.71k 33 2.60k 1.51k 1.03k\nTable 1: By-category statistics for the Wikipedia dataset. Note that Food is the abbreviation for “Food and Culture” and Religion is the abbreviation for “Religion and Belief”.\ncan be derived from the sum of its parts, is very much a reality. Perhaps the most compelling example of compositionality of sub-character units can be found in logographic writing systems such as the Han and Kanji characters used in Chinese and Japanese, respectively.2 As shown on the left side of Fig. 1, each part of a Chinese character (called a “radical”) potentially contributes to the meaning (i.e., Fig. 1(a)) or pronunciation (i.e., Fig. 1(b)) of the overall character. This is similar to how English characters combine into the meaning or pronunciation of an English word. Even in languages with phonemic orthographies, where each character corresponds to a pronunciation instead of a meaning, there are cases where composition occurs. Fig. 1(c) and (d) show the examples of Korean and German, respectively, where morphological inflection can cause single characters to make changes where some but not all of the component parts are shared. In this paper, we investigate the feasibility of modeling the compositionality of characters in a way similar to how humans do: by visually observing the character and using the features of its shape to learn a representation encoding its meaning. Our method is relatively simple, and generalizable to a wide variety of languages: we first transform each character from its Unicode representation to a rendering of its shape as an image, then calculate a representation of the image using Convolutional Neural Networks (CNNs) (Cun et al., 1990). These features then serve as inputs to a down-stream processing task and trained in an end-to-end manner, which first calculates a loss function, then back-propagates the loss back to the CNN.\n2Other prominent examples are largely for extinct languages: Egyptian hieroglyphics, Mayan glyphs, and Sumerian cuneiform scripts (Daniels and Bright, 1996).\nAs demonstrated by our motivating examples in Fig. 1, in logographic languages, character-level semantic or phonetic similarity is often indicated by visual cues; we conjecture that CNNs can appropriately model these visual patterns. Consequently, characters with similar visual appearances will be biased to have similar embeddings, allowing our model to handle rare characters effectively, just as character-level models have been effective for rare words. To evaluate our model’s ability to learn representations, particularly for rare characters, we perform experiments on a downstream task of classifying Wikipedia titles for three Asian languages: Chinese, Japanese, and Korean. We show that our proposed framework outperforms baseline model that use standard character embeddings for instances containing rare characters. A qualitative analysis of the characteristics of the learned embeddings of our model demonstrates that visually similar characters share similar embeddings. We also show that the learned representations are particularly effective under low-resource scenarios and complementary with standard character embeddings; combining the two representations through three different fusion methods (Snoek et al., 2005; Karpathy et al., 2014) leads to consistent improvements over the strongest baseline without visual features."
    }, {
      "heading" : "2 Dataset",
      "text" : "Before delving into the details of our model, we first describe a dataset we constructed to examine the ability of our model to capture the compositional characteristics of characters. Specifically, the dataset must satisfy two desiderata: (1) it must be necessary to fully utilize each character in the input in order to achieve high accuracy, and (2) there must be enough regularity and com-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n100 101 102 103 104\n106\n100\n101 102\n103\n104 105\nRank\nFr eq\nue nc\ny ⎯⎯ Chinese ⎯⎯\"Japanese ⎯⎯ Korean\nRank < 20% Freq. > 80%\nFigure 2: The character rank-frequency distribution of the corpora we considered in this paper. All three languages have a long-tail distribution.\npositionality in the characters of the language. To satisfy these desiderata, we create a text classification dataset where the input is a Wikipedia article title in Chinese, Japanese, or Korean, and the output is the category to which the article belongs. 3 This satisfies (1), because Wikipedia titles are short and thus each character in the title will be important to our decision about its category. It also satisfies (2), because Chinese, Japanese, and Korean have writing systems with large numbers of characters that decompose regularly as shown in Fig. 1. While this task in itself is novel, it is similar to previous work in named entity type inference using Wikipedia (Toral and Munoz, 2006; Kazama and Torisawa, 2007; Ratinov and Roth, 2009), which has proven useful for downstream named entity recognition systems."
    }, {
      "heading" : "2.1 Dataset Collection",
      "text" : "As the labels we would like to predict, we use 12 different main categories from the Wikipedia web page: Geography, Sports, Arts, Military, Economics, Transportation, Health Science, Education, Food Culture, Religion and Belief, Agriculture and Electronics. Wikipedia has a hierarchical structure, where each of these main categories has a number of subcategories, and each subcategory has its own subcategories, etc. We traverse this hierarchical structure, adding each main category tag to all of its descendants in this subcategory tree structure. In the case that a particular article is the descendant of multiple main categories, we favor the main category that minimizes the depth of the\n3This dataset and the crawling scripts are attached as supplementary material, and will be released publicly upon acceptance of the paper.\nGeography Sports Arts Military Economics Transportation Health Science Education Food Culture Religion and Belief Agriculture Electronics Visual model (Image as input) Lookup model (Symbol as input) CNN CNN CNN Softmax GRU 36 36\nFigure 3: An illustration of two models, our proposed Visual model at the top and the baseline Lookup model at the bottom using the same RNN architecture. A string of characters (e.g. “温病 学”), each converted into a 36x36 image, serves as input of our Visual model. dc is the dimension of the character embedding for the Lookup model.\narticle in the tree (e.g., if an article is two steps away from Sports and three steps away from Arts, it will receive the “Sports” label). We also perform some rudimentary filtering, removing pages that match the regular expression “.*:.*”, which catches special pages such as “title:agriculture”."
    }, {
      "heading" : "2.2 Statistics",
      "text" : "For Chinese, Japanese, and Korean, respectively, the number of articles is 593k/810k/46.6k, and the average length and standard deviation of the title is 6.25±3.96/8.60±5.58/6.10±3.71. As shown in Fig. 2, the character rank-frequency distributions of all three languages follows the 80/20 rule (Newman, 2005) (i.e., top 20% ranked characters that appear more than 80% of total frequencies), demonstrating that the characters in these languages belong to a long tail distribution. We further split the dataset into training, validation, and testing sets with a 6:2:2 ratio. The category distribution for each language can be seen in Tab. 1. Chinese has two varieties of characters, traditional and simplified, and the dataset is a mix of the two. Hence, we transform this dataset into two separate sets, one completely simplified and the other completely traditional using the Chinese text converter provided with Mac OS."
    }, {
      "heading" : "3 Model",
      "text" : "Our overall model for the classification task follows the encoder model by Sutskever et al. (2014). We calculate character representations, use a RNN\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nLayer# 3-layer CNN Configuration 1 Spatial Convolution (3, 3) → 32 2 ReLu 3 MaxPool (2, 2) 4 Spatial Convolution (3, 3) → 32 5 ReLu 6 MaxPool (2, 2) 7 Spatial Convolution (3, 3) → 32 8 ReLu 9 Linear (800, 128) 10 ReLu 11 Linear (128, 128) 12 ReLu\nTable 2: Architecture of the CNN used in the experiments. All the convolutional layers have 32 3×3 filters.\nto combine the character representations into a sentence representation, and then add a softmax layer after that to predict the probability for each class. As shown in Fig. 3, the baseline model, which we call it the Lookup model, calculates the representation for each character by looking it up in a character embedding matrix. Our proposed model, the Visual model instead learns the representation of each character from its visual appearance via CNN.\nLookupmodel Given a character vocabularyC, for the Lookup model as in the bottom part of Fig. 3, the input to the network is a stream of characters c1, c2, ...cN , where cn ∈ C. Each character is represented by a 1-of-|C| (one-hot) encoding. This one-hot vector is thenmultiplied by the lookupmatrix TC ∈ R|C|×dc , where dc is the dimension of the character embedding.\nVisual model The proposed method aims to learn a representation that includes image information, allowing for better parameter sharing among characters, particularly characters that are less common. Different from the Lookup model, each character is first transformed into a 36-by-36 image based on its Unicode encoding as shown in the upper part of Fig 3. We then pass the image through a CNN to get the embedding for the image. The parameters for the CNN are learned through backpropagation from the classification loss. Because we are training embeddings based on this classification loss, we expect that the CNN will focus on parts of the image that contain semantic information useful for category classification, a hypothe-\nsis that we examine in the experiments (see Section 5.5). In more detail, the specific structure of the CNN that we utilize consists of three convolution layers where each convolution layer is followed by the max pooling and ReLU nonlinear activation layers. The configurations of each layer are listed in Tab. 2. The output vector for the image embeddings also has size dc which is the same as the Lookupmodel.\nEncoder and Classifier For both the Lookup and the Visual models, we adopt an RNN encoder usingGated Recurrent Units (GRUs) (Chung et al., 2014). Each of the GRU units processes the character embeddings sequentially. At the end of the sequence, the incremental GRU computation results in a hidden state e embedding the sentence. The encoded sentence embedding is passed through a linear layer whose output is the same size as the number of classes. We use a softmax layer to compute the posterior class probabilities:\nP (y = j|e) = exp(wTj e+ bj)∑L i=1 exp(wTi e+ bj)\n(1)\nTo train themodel, we use cross-entropy loss between predicted and true targets:\nJ = 1\nB B∑ i=1 L∑ j=1 −ti,j log(pi,j) (2)\nwhere ti,j ∈ {0, 1} represents the ground truth label of the j-th class in the i-thWikipedia page title. B is the batch size and L is the number of categories."
    }, {
      "heading" : "4 Fusion-based Models",
      "text" : "One thing to note is that the Lookup and the Visual models have their own advantages. The Lookup model learns embedding that captures the semantics of each character symbol without sharing information with each other. On the contrary, the proposed Visual model directly learns embedding from visual information, which naturally shares information between visually similar characters. This characteristic gives the Visual model the ability to generalize better to rare characters, but also has the potential disadvantage of introducing noise for characters with similar appearances but different meanings. With the complementary nature of these two models in mind, we further combine the two embeddings to achieve better performances. We adopt\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nLookup/Visual 100% 50% 12.5% zh_trad 0.55/0.54 0.53/0.50 0.48/0.47 zh_simp 0.55/0.54 0.53/0.52 0.48/0.46 ja 0.42/0.39 0.47/0.45 0.44/0.41 ko 0.47/0.42 0.44/0.39 0.37/0.36\nTable 3: The classification results of the Lookup / Visual models for different percentages of full training size.\nthree fusion schemes, early fusion, late fusion (described by Snoek et al. (2005) and Karpathy et al. (2014)), and fallback fusion, a method specific to this paper.\nEarly Fusion Early fusion works by concatenating the two varieties of embeddings before feeding them into the RNN. In order to ensure that the dimensions of the RNN are the same after concatenation, the concatenated vector is fed through a hidden layer to reduce the size from 2× dc to dc. The whole model is then fine-tuned with training data.\nLate Fusion Instead of learning a joint representation like early fusion, late fusion averages the model predictions. Specifically, it takes the output of the softmax layers from both models and averages the probabilities to create a final distribution used to make the prediction.\nFallback Fusion Our final fallback fusion method hypothesizes that our Visual model does better with instances which contain more rare characters. First, in order to quantify the overall rareness of an instance consisting of multiple characters, we calculate the average training set frequency of the characters therein. The fallback fusion method uses the Visual model to predict testing instances with average character frequency below or equal to a threshold (here we use 0.0 frequency as cutoff, which means all characters in the instance do not appear in the training set), and uses the Lookup model to predict the rest of the instances."
    }, {
      "heading" : "5 Experiments and Results",
      "text" : "In this section, we compare our proposed Visual model with the baseline Lookup model through three different sets of experiments. First, we examine whether our model is capable of classifying text and achieving similar performance as the baseline model. Next, we examine the hypothesis that our model will outperform the baseline model\nLookup/Visual 100 1000 10000 zh_trad 0.22/0.49 0.35/0.35 0.40/0.39 zh_simp 0.25/0.53 0.39/0.37 0.41/0.40 ja 0.30/0.35 0.45/0.41 0.44/0.41 ko 0.44/0.33 0.44/0.33 0.48/0.42\nTable 4: Classification results for the Lookup / Visual of the k lowest frequency instances across four datasets.\nwhen dealing with low frequency characters. Finally, we examine the fusion methods described in Section 4."
    }, {
      "heading" : "5.1 Experimental Configurations",
      "text" : "The dimension of the embeddings and batch size for both models are set to dc = 128 and B = 400, respectively. We build our proposed model using Torch (Collobert et al., 2002), and use Adam (Kingma and Ba, 2014) with a learning rate η = 0.001 for stochastic optimization. The length of each instance is cut off or padded to 10 characters for batch training."
    }, {
      "heading" : "5.2 Comparison with the baseline model",
      "text" : "In this experiment, we examine whether our Visual model achieves similar performance with the baseline Lookup model in classification accuracy. The results in Tab. 3 show that the baseline model performs 1-2% better across four datasets; this is due to the fact that the Lookupmodel can directly learn character embeddings that capture the semantics of each character symbol for frequent characters. On the contrary, the Visual model learns embeddings from visual information, which constraints characters that has similar appearance to have similar embeddings. This is an advantage for rare characters, but a disadvantage for high frequency characters because being similar in appearance does not always lead to similar semantics. To demonstrate that this is in fact the case, besides looking at the overall classification accuracy, we also examine the performance on classifying low frequency instances which are sorted according to the average training set frequency of the characters therein. Tab. 4 and Fig. 4 (blue lines) both show that our model performs better in the 100 lowest frequency instances (the intersection point of the two models); this confirms that the Visual model can share visual information among characters and help to classify low frequency instances.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n101 102 103 100\n101\n102\n103\n102 103 100\n101\n102\n103 102 103\n100\n101\n102\n103\n102 103 100\n101\n102\n103\nA cc\num ul\nat ed\nN um\nbe r o f C or re ct ly P re di ct ed In st an\nce s\nRank\n(a) (b)\n(c) (d)\n⎯⎯!Visual,(TP(=(100% ⎯⎯!Visual,(TP(=(50% ⎯⎯!Visual,(TP(=(12.5% ⎯!⎯!Lookup,(TP(=(100% ⎯!⎯!Lookup,(TP(=(50% ⎯!⎯!Lookup,(TP(=(12.5%\n⎯⎯!Visual,(TP(=(100% ⎯⎯!Visual,(TP(=(50% ⎯⎯!Visual,(TP(=(12.5% ⎯!⎯!Lookup,(TP(=(100% ⎯!⎯!Lookup,(TP(=(50% ⎯!⎯!Lookup,(TP(=(12.5%\n⎯⎯!Visual,(TP(=(100% ⎯⎯!Visual,(TP(=(50% ⎯⎯!Visual,(TP(=(12.5% ⎯!⎯!Lookup,(TP(=(100% ⎯!⎯!Lookup,(TP(=(50% ⎯!⎯!Lookup,(TP(=(12.5% ⎯⎯!Visual,(TP(=(100% ⎯⎯!Visual,(TP(=(50% ⎯⎯!Visual,(TP(=(12.5% ⎯!⎯!Lookup,(TP(=(100% ⎯!⎯!Lookup,(TP(=(50% ⎯!⎯!Lookup,(TP(=(12.5%\nFigure 4: Experiments on different training sizes for four different datasets. More specifically, we consider three different training data size percentages (TPs) (100%, 50%, and 12.5%) and four datasets: (a) traditional Chinese, (b) simplified Chinese, (c) Japanese, and (d) Korean. We calculate the accumulated number of correctly predicted instances for the Visualmodel (solid lines) and the Lookup model (dashed lines)."
    }, {
      "heading" : "5.3 Experiments on different training data sizes",
      "text" : "In our second experiment, we consider two smaller training sizes (i.e., 50% and 12.5% of the full training size) indicated by green and red lines in Fig. 4. We performed this experiment under the hypothesis that because the proposed method was more robust to infrequent characters, the proposed model may perform better in low-resourced scenarios. If this is the case, the intersection point of the two models will shift right because of the increase of the number of instances with low average character frequency. As we can see in Fig. 4, the intersection point for 100% training data lies between the intersection point for 50% training data and 12.5%. This disagrees with our hypothesis; this is likely because while the number of low-frequency characters increases, smaller amounts of data also adversely impact the ability of CNN to learn useful visual features, and thus there is not a clear gain nor loss when using the proposed method. As a more extreme test of the ability of our proposed framework to deal with the unseen characters in the test set, we use traditional Chinese as our training data and simplified Chinese as our testing\nzh_trad zh_simp ja ko Lookup 0.5503 0.5543 0.4914 0.4765 Visual 0.5434 0.5403 0.4775 0.4207 early 0.5520 0.5546 0.4896 0.4796 late 0.5658 0.5685 0.5029 0.4869 fall 0.5507 0.5547 0.4914 0.4766\nTable 5: Experiment results for three different fusion methods across 4 datasets.\ndata. The model was able to achieve around 40% classification accuracy when we use the full training set, compared to 55%, which is achieved by the model trained on simplified Chinese. This result demonstrates that the model is able to transfer between similar scripts, similarly to howmost Chinese speakers can guess the meaning of the text, even if it is written in the other script."
    }, {
      "heading" : "5.4 Experiment on Different Fusion Methods",
      "text" : "Results of different fusion methods can be found in Tab. 5. The results show that late fusion gives the best performance among all the fusion schemes combining the Lookup model and the proposed Visual model. Early fusion achieves small improvements for all languages except Japanese, where it displays a slight drop. Unsurprisingly,\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nIron Bronze Salmon Serranidae\nSilk Coil Rhyme Pleased\nWave Put on Cypress Pillar\nCuckoo Eagle Mosquito Ant\nFigure 5: Examples of how much each part of the character contributes to its embedding (the darker the more). Two characters are shown per radical to emphasize that characters with same radical have similar patterns.\nfallback fusion performs better than the Lookup model and the Visual model alone, since it directly targets the weakness of the Lookup model (e.g., rare characters) and replaces the results with the Visual model. These results show that simple integration, no matter which schemes we use, is beneficial, demonstrating that both methods are capturing complementary information."
    }, {
      "heading" : "5.5 Visualization of Character Embeddings",
      "text" : "Finally, we qualitatively examine what is learned by our proposed model in two ways. First, we visualize which parts of the image are most important to the Visual model’s embedding calculation. Second, we show the 6-nearest neighbor results for characters using both the Lookup and the Visual embeddings.\nEmphasis of the Visual Model In order to delve deeper into what the Visual model has learned, we measure a modified version of the occlusion sensitivity proposed by Zeiler and Fergus (2014) by masking the original character image in four ways, and examine the importance of each part of the character to the model’s calculated representations. Specifically, we leave only the upper half, bottom half, left half, or right half of the image, and mask the remainder with white pixels since Chinese characters are usually formed by combining two radicals vertically or horizontally. We run these four images forward through the CNN part of the model and calculate the L2 distance between\n! !\n! ! !\n!\nVisual'model Lookup'model Visual'model Lookup'model\nFigure 6: Visualization of the Chinese traditional characters by finding the 6-nearest neighbors of the query (i.e., center) characters. The highlighted red indicates the radical along with the meaning of the characters.\nthe masked image embeddings with the full image embedding. The larger the distance, the more the masked part of the character contributes to the original embedding. The contribution of each part (e.g. the L2 distance) is represented as a heat map, and then it is normalized to adjust the opacity of the character strokes for better visualization. The value of each corner of the heatmap is calculated by adding the two L2 distances that contribute to this corner. The visualization is shown in Fig. 5. The meaning of each Chinese character in English is shown below the Chinese character. The opacity of the character strokes represent how much the corresponding parts contribute to the original embedding (the darker the more). In general, the darker part of the character is related to its semantics. For example, “金” means gold in Chinese, which is highlighted in both “鐵” (Iron) and “銅” (Bronze). We can also find similar results for other examples shown in Fig. 5.\nK-nearest neighbors Finally, to illustrate the difference of the learned embeddings between the two models, we display 6-nearest neighbors (L2 distance) for selected characters in Fig. 6. As can be seen, the Visual embedding for characters with similar appearances are close to each other. In addition, similarity in the radical part indicates semantic similarity between the characters. For example, the characters with radical “鳥” all refer to different type of birds. The Lookup embedding do not show such feature, as it learns the embedding individually for each symbol and relies heavily on the training set and the task. In fact, the characters shown in Fig. 6\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nfor the Lookupmodel do not exhibit semantic similarity either. There are two potential explanations for this: First, the category classification task that we utilized do not rely heavily on the finegrained semantics of each character, and thus the Lookup model was able to perform well without exactly capturing the semantics of each character precisely. Second, the Wikipedia dataset contains a large number of names and location and the characters therein might not have the same semantic meaning used in daily vocabulary."
    }, {
      "heading" : "6 Related Work",
      "text" : "Methods that utilize neural networks to learn distributed representations of words or characters have been widely developed. However, word2vec (Mikolov et al., 2013), for example, requires storing an extremely large table of vectors for all word types. For example, due to the size of word types in twitter tweets, work has been done to generate vector representations of tweets at character-level (Dhingra et al., 2016).\nThere is also work done in understanding mathematical expressions with a convolutional network for text and layout recognition by using an attention-based neural machine translation system (Deng et al., 2016). They tested on realworld rendered mathematical expressions paired with LaTeX markup and show the system is effective at generating accurate markup. Other than that, there are several works that combine visual information with text in improving machine translation (Sutskever et al., 2014), visual question answering, caption generation (Xu et al., 2015), etc. These works extract image representations from a pre-trained CNN (Zhu et al., 2016; Wang et al., 2016). Unrelated to images, CNNs have also been used for text classification (Kim, 2014; Zhang et al., 2015). These models look at the sequential dependencies at the word or character-level and achieve the state-of-the-art results. These works inspire us to use CNN to extract features from image and serve as the input to the RNN. Our model is able to directly back-propagate the gradient all the way through the CNN, which generates visual embeddings, in a way such that the embedding can contain both semantic and visual information. Several techniques for reducing the rare words effects have been introduced in the literature, including spelling expansion (Habash, 2008), dictio-\nnary term expansion (Habash, 2008), proper name transliteration (Daumé and Jagarlamudi, 2011), treating words as a sequence of characters (Luong and Manning, 2016), subword units (Sennrich et al., 2015), and reading text as bytes (Gillick et al., 2015). However, most of these techniques still have no mechanism for handling low frequency characters, which are the target of this work. Finally, there is one work by Shi et al. (2015) on “radical embedding”, which explicitly splits Chinese characters into radicals based on a dictionary of what radicals are included in which characters. The motivation of this method is similar to ours, but is only applicable to Chinese, in contrast to the method in this paper, which works on any language for which we can render text."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "In this paper, we proposed a new framework that utilizes appearance of characters, convolutional neural networks, recurrent neural networks to learn embeddings that are compositional in the component parts of the characters. More specifically, we collected a Wikipedia dataset, which consists of short titles of three different languages and satisfies the compositionality in the characters of the language. Next, we proposed an end-to-end model that learns visual embeddings for characters using CNN and showed that the features extracted from the CNN include both visual and semantic information. Furthermore, we showed that our Visual model outperforms the Lookup baseline model in low frequency instances. Additionally, by examining the character embeddings visually, we found that our Visual model is able to learn visually related embeddings. In summary, we tackled the problem of rare characters by using embeddings learned from images. In the future, we hope to further generalize this method to other tasks such as pronunciation estimation, which can take advantage of the fact that pronunciation information is encoded in parts of the characters as demonstrated in Fig. 1, or machine translation, which could benefit from a wholistic view that considers both semantics and pronunciation. We also hope to apply the model to other languageswith complicated compositional writing systems, potentially including historical texts such as hieroglyphics or cuneiform.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Compositional morphology for word representations and language modelling",
      "author" : [ "Jan A Botha", "Phil Blunsom." ],
      "venue" : "ICML. pages 1899–1907.",
      "citeRegEx" : "Botha and Blunsom.,? 2014",
      "shortCiteRegEx" : "Botha and Blunsom.",
      "year" : 2014
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1412.3555 .",
      "citeRegEx" : "Chung et al\\.,? 2014",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Torch: A modular machine learning software",
      "author" : [ "Ronan Collobert", "Samy Bengio", "Johnny Marithoz" ],
      "venue" : null,
      "citeRegEx" : "Collobert et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2002
    }, {
      "title" : "Advances in neural information processing systems",
      "author" : [ "Y. Le Cun", "B. Boser", "J.S. Denker", "R.E. Howard", "W. Habbard", "L.D. Jackel", "D. Henderson" ],
      "venue" : null,
      "citeRegEx" : "Cun et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Cun et al\\.",
      "year" : 1990
    }, {
      "title" : "The world’s writing systems",
      "author" : [ "Peter T Daniels", "William Bright." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Daniels and Bright.,? 1996",
      "shortCiteRegEx" : "Daniels and Bright.",
      "year" : 1996
    }, {
      "title" : "Domain adaptation for machine translation by mining unseen words",
      "author" : [ "Hal Daumé", "Jagadeesh Jagarlamudi." ],
      "venue" : "ACL-HLT . pages 407–412.",
      "citeRegEx" : "Daumé and Jagarlamudi.,? 2011",
      "shortCiteRegEx" : "Daumé and Jagarlamudi.",
      "year" : 2011
    }, {
      "title" : "What you get is what you see: A visual markup decompiler",
      "author" : [ "Yuntian Deng", "Anssi Kanervisto", "Alexander M. Rush." ],
      "venue" : "arXiv preprint arXiv:1609.04938 .",
      "citeRegEx" : "Deng et al\\.,? 2016",
      "shortCiteRegEx" : "Deng et al\\.",
      "year" : 2016
    }, {
      "title" : "Tweet2vec: Character-based distributed representations for social media",
      "author" : [ "Bhuwan Dhingra", "Zhong Zhou", "Dylan Fitzpatrick", "Michael Muehl", "William W Cohen." ],
      "venue" : "ACL .",
      "citeRegEx" : "Dhingra et al\\.,? 2016",
      "shortCiteRegEx" : "Dhingra et al\\.",
      "year" : 2016
    }, {
      "title" : "Multilingual language processing from bytes",
      "author" : [ "Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya." ],
      "venue" : "arXiv preprint arXiv:1512.00103 .",
      "citeRegEx" : "Gillick et al\\.,? 2015",
      "shortCiteRegEx" : "Gillick et al\\.",
      "year" : 2015
    }, {
      "title" : "Four techniques for online handling of out-of-vocabulary words in Arabic-English statistical machine translation",
      "author" : [ "Nizar Habash." ],
      "venue" : "HLT-Short. pages 57–60.",
      "citeRegEx" : "Habash.,? 2008",
      "shortCiteRegEx" : "Habash.",
      "year" : 2008
    }, {
      "title" : "Deep unordered composition rivals syntactic methods for text classification",
      "author" : [ "Mohit Iyyer", "Varun Manjunatha", "Jordan L BoydGraber." ],
      "venue" : "ACL.",
      "citeRegEx" : "Iyyer et al\\.,? 2015",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2015
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom." ],
      "venue" : "ACL pages 655–665.",
      "citeRegEx" : "Kalchbrenner et al\\.,? 2014",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Large-scale video classification with convolutional neural networks",
      "author" : [ "Andrej Karpathy", "George Toderici", "Sanketh Shetty", "Thomas Leung", "Rahul Sukthankar", "Li Fei-Fei." ],
      "venue" : "CVPR. pages 1725–1732.",
      "citeRegEx" : "Karpathy et al\\.,? 2014",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2014
    }, {
      "title" : "Exploiting Wikipedia as external knowledge for named entity recognition",
      "author" : [ "Jun’ichi Kazama", "Kentaro Torisawa" ],
      "venue" : "In EMNLP-CoNLL",
      "citeRegEx" : "Kazama and Torisawa.,? \\Q2007\\E",
      "shortCiteRegEx" : "Kazama and Torisawa.",
      "year" : 2007
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "EMNLP. pages 1746– 1751.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980 .",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "NIPS. pages 3294–3302.",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Finding function in form: Compositional character models for open vocabulary word representation",
      "author" : [ "Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis." ],
      "venue" : "EMNLP. pages 1520–1530.",
      "citeRegEx" : "Ling et al\\.,? 2015",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2015
    }, {
      "title" : "Achieving open vocabulary neural machine translation with hybrid word-character models",
      "author" : [ "Minh-Thang Luong", "Christopher D Manning." ],
      "venue" : "ACL pages 1054–1063.",
      "citeRegEx" : "Luong and Manning.,? 2016",
      "shortCiteRegEx" : "Luong and Manning.",
      "year" : 2016
    }, {
      "title" : "Better word representations with recursive neural networks for morphology",
      "author" : [ "Thang Luong", "Richard Socher", "Christopher Manning." ],
      "venue" : "CoNLL. pages 104–113.",
      "citeRegEx" : "Luong et al\\.,? 2013",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "NIPS. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Power laws, Pareto distributions and Zipf’s law",
      "author" : [ "Mej Newman." ],
      "venue" : "CONTEMP PHYS pages 323–351.",
      "citeRegEx" : "Newman.,? 2005",
      "shortCiteRegEx" : "Newman.",
      "year" : 2005
    }, {
      "title" : "Design challenges and misconceptions in named entity recognition",
      "author" : [ "Lev Ratinov", "Dan Roth." ],
      "venue" : "CoNLL. pages 147–155.",
      "citeRegEx" : "Ratinov and Roth.,? 2009",
      "shortCiteRegEx" : "Ratinov and Roth.",
      "year" : 2009
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "ACL pages 1715–1725.",
      "citeRegEx" : "Sennrich et al\\.,? 2015",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Radical embedding: Delving deeper to chinese radicals",
      "author" : [ "Xinlei Shi", "Junjie Zhai", "Xudong Yang", "Zehua Xie", "Chao Liu." ],
      "venue" : "ACL. pages 594–598.",
      "citeRegEx" : "Shi et al\\.,? 2015",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2015
    }, {
      "title" : "Early versus late fusion in semantic video analysis",
      "author" : [ "Cees GM Snoek", "Marcel Worring", "Arnold WM Smeulders." ],
      "venue" : "ACM MM. pages 399–402.",
      "citeRegEx" : "Snoek et al\\.,? 2005",
      "shortCiteRegEx" : "Snoek et al\\.",
      "year" : 2005
    }, {
      "title" : "Recursive deep models for semantic compositionality over a sentiment treebank",
      "author" : [ "Richard Socher", "Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "EMNLP. pages 1631–1642.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "NIPS. pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Compositionality",
      "author" : [ "Zoltán Gendler Szabó." ],
      "venue" : "Stanford encyclopedia of philosophy .",
      "citeRegEx" : "Szabó.,? 2010",
      "shortCiteRegEx" : "Szabó.",
      "year" : 2010
    }, {
      "title" : "A proposal to automatically build and maintain gazetteers for named entity recognition by using wikipedia",
      "author" : [ "Antonio Toral", "Rafael Munoz." ],
      "venue" : "EACL. pages 56–61.",
      "citeRegEx" : "Toral and Munoz.,? 2006",
      "shortCiteRegEx" : "Toral and Munoz.",
      "year" : 2006
    }, {
      "title" : "Cnn-rnn: A unified framework for multi-label image classification",
      "author" : [ "Jiang Wang", "Yi Yang", "Junhua Mao", "Zhiheng Huang", "Chang Huang", "Wei Xu." ],
      "venue" : "CVPR. pages 2285–2294.",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron C Courville", "Ruslan Salakhutdinov", "Richard S Zemel", "Yoshua Bengio." ],
      "venue" : "ICML.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Visualizing and understanding convolutional networks",
      "author" : [ "Matthew D Zeiler", "Rob Fergus." ],
      "venue" : "ECCV . Springer, pages 818–833.",
      "citeRegEx" : "Zeiler and Fergus.,? 2014",
      "shortCiteRegEx" : "Zeiler and Fergus.",
      "year" : 2014
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "NIPS. pages 649–657.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    }, {
      "title" : "Visual7w: Grounded question answering in images",
      "author" : [ "Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li FeiFei." ],
      "venue" : "CVPR. pages 4995–5004.",
      "citeRegEx" : "Zhu et al\\.,? 2016",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Compositionality—the fact that the meaning of larger linguistic units is created by combining the meaning of smaller units—is a hallmark of every natural language (Szabó, 2010).",
      "startOffset" : 163,
      "endOffset" : 176
    }, {
      "referenceID" : 10,
      "context" : "This is done using models of various levels of sophistication, from simpler bag-ofwords (Iyyer et al., 2015) and linear recurrent neural network (RNN) models (Sutskever et al.",
      "startOffset" : 88,
      "endOffset" : 108
    }, {
      "referenceID" : 27,
      "context" : ", 2015) and linear recurrent neural network (RNN) models (Sutskever et al., 2014; Kiros et al., 2015), to more sophisticated models Kalb Kälber a Do Do'(polite)",
      "startOffset" : 57,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : ", 2015) and linear recurrent neural network (RNN) models (Sutskever et al., 2014; Kiros et al., 2015), to more sophisticated models Kalb Kälber a Do Do'(polite)",
      "startOffset" : 57,
      "endOffset" : 101
    }, {
      "referenceID" : 26,
      "context" : "using tree-structured (Socher et al., 2013) or convolutional networks (Kalchbrenner et al.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : ", 2013) or convolutional networks (Kalchbrenner et al., 2014).",
      "startOffset" : 34,
      "endOffset" : 61
    }, {
      "referenceID" : 17,
      "context" : "For example, several works have proposed models that represent words by composing together the characters into a representation of the word itself (Ling et al., 2015; Zhang et al., 2015; Dhingra et al., 2016).",
      "startOffset" : 147,
      "endOffset" : 208
    }, {
      "referenceID" : 33,
      "context" : "For example, several works have proposed models that represent words by composing together the characters into a representation of the word itself (Ling et al., 2015; Zhang et al., 2015; Dhingra et al., 2016).",
      "startOffset" : 147,
      "endOffset" : 208
    }, {
      "referenceID" : 7,
      "context" : "For example, several works have proposed models that represent words by composing together the characters into a representation of the word itself (Ling et al., 2015; Zhang et al., 2015; Dhingra et al., 2016).",
      "startOffset" : 147,
      "endOffset" : 208
    }, {
      "referenceID" : 19,
      "context" : "Additionally, for languages with productive word formation (such as agglutination and compounding), models calculating morphologysensitive word representations have been found effective (Luong et al., 2013; Botha and Blunsom, 2014).",
      "startOffset" : 186,
      "endOffset" : 231
    }, {
      "referenceID" : 0,
      "context" : "Additionally, for languages with productive word formation (such as agglutination and compounding), models calculating morphologysensitive word representations have been found effective (Luong et al., 2013; Botha and Blunsom, 2014).",
      "startOffset" : 186,
      "endOffset" : 231
    }, {
      "referenceID" : 3,
      "context" : "Our method is relatively simple, and generalizable to a wide variety of languages: we first transform each character from its Unicode representation to a rendering of its shape as an image, then calculate a representation of the image using Convolutional Neural Networks (CNNs) (Cun et al., 1990).",
      "startOffset" : 278,
      "endOffset" : 296
    }, {
      "referenceID" : 4,
      "context" : "2Other prominent examples are largely for extinct languages: Egyptian hieroglyphics, Mayan glyphs, and Sumerian cuneiform scripts (Daniels and Bright, 1996).",
      "startOffset" : 130,
      "endOffset" : 156
    }, {
      "referenceID" : 25,
      "context" : "We also show that the learned representations are particularly effective under low-resource scenarios and complementary with standard character embeddings; combining the two representations through three different fusion methods (Snoek et al., 2005; Karpathy et al., 2014) leads to consistent improvements over the strongest baseline without visual features.",
      "startOffset" : 229,
      "endOffset" : 272
    }, {
      "referenceID" : 12,
      "context" : "We also show that the learned representations are particularly effective under low-resource scenarios and complementary with standard character embeddings; combining the two representations through three different fusion methods (Snoek et al., 2005; Karpathy et al., 2014) leads to consistent improvements over the strongest baseline without visual features.",
      "startOffset" : 229,
      "endOffset" : 272
    }, {
      "referenceID" : 29,
      "context" : "While this task in itself is novel, it is similar to previous work in named entity type inference using Wikipedia (Toral and Munoz, 2006; Kazama and Torisawa, 2007; Ratinov and Roth, 2009), which has proven useful for downstream named entity recognition systems.",
      "startOffset" : 114,
      "endOffset" : 188
    }, {
      "referenceID" : 13,
      "context" : "While this task in itself is novel, it is similar to previous work in named entity type inference using Wikipedia (Toral and Munoz, 2006; Kazama and Torisawa, 2007; Ratinov and Roth, 2009), which has proven useful for downstream named entity recognition systems.",
      "startOffset" : 114,
      "endOffset" : 188
    }, {
      "referenceID" : 22,
      "context" : "While this task in itself is novel, it is similar to previous work in named entity type inference using Wikipedia (Toral and Munoz, 2006; Kazama and Torisawa, 2007; Ratinov and Roth, 2009), which has proven useful for downstream named entity recognition systems.",
      "startOffset" : 114,
      "endOffset" : 188
    }, {
      "referenceID" : 21,
      "context" : "2, the character rank-frequency distributions of all three languages follows the 80/20 rule (Newman, 2005) (i.",
      "startOffset" : 92,
      "endOffset" : 106
    }, {
      "referenceID" : 27,
      "context" : "Our overall model for the classification task follows the encoder model by Sutskever et al. (2014). We calculate character representations, use a RNN",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 1,
      "context" : "Encoder and Classifier For both the Lookup and the Visual models, we adopt an RNN encoder usingGated Recurrent Units (GRUs) (Chung et al., 2014).",
      "startOffset" : 124,
      "endOffset" : 144
    }, {
      "referenceID" : 24,
      "context" : "three fusion schemes, early fusion, late fusion (described by Snoek et al. (2005) and Karpathy et al.",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 12,
      "context" : "(2005) and Karpathy et al. (2014)), and fallback fusion, a method specific to this paper.",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "We build our proposed model using Torch (Collobert et al., 2002), and use Adam (Kingma and Ba, 2014) with a learning rate η = 0.",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 15,
      "context" : ", 2002), and use Adam (Kingma and Ba, 2014) with a learning rate η = 0.",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 32,
      "context" : "Emphasis of the Visual Model In order to delve deeper into what the Visual model has learned, we measure a modified version of the occlusion sensitivity proposed by Zeiler and Fergus (2014) by masking the original character image in four ways, and examine the importance of each part of the character to the model’s calculated representations.",
      "startOffset" : 165,
      "endOffset" : 190
    }, {
      "referenceID" : 20,
      "context" : "However, word2vec (Mikolov et al., 2013), for example, requires storing an extremely large table of vectors for all word types.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : "For example, due to the size of word types in twitter tweets, work has been done to generate vector representations of tweets at character-level (Dhingra et al., 2016).",
      "startOffset" : 145,
      "endOffset" : 167
    }, {
      "referenceID" : 6,
      "context" : "There is also work done in understanding mathematical expressions with a convolutional network for text and layout recognition by using an attention-based neural machine translation system (Deng et al., 2016).",
      "startOffset" : 189,
      "endOffset" : 208
    }, {
      "referenceID" : 27,
      "context" : "Other than that, there are several works that combine visual information with text in improving machine translation (Sutskever et al., 2014), visual question answering, caption generation (Xu et al.",
      "startOffset" : 116,
      "endOffset" : 140
    }, {
      "referenceID" : 31,
      "context" : ", 2014), visual question answering, caption generation (Xu et al., 2015), etc.",
      "startOffset" : 55,
      "endOffset" : 72
    }, {
      "referenceID" : 34,
      "context" : "These works extract image representations from a pre-trained CNN (Zhu et al., 2016; Wang et al., 2016).",
      "startOffset" : 65,
      "endOffset" : 102
    }, {
      "referenceID" : 30,
      "context" : "These works extract image representations from a pre-trained CNN (Zhu et al., 2016; Wang et al., 2016).",
      "startOffset" : 65,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "Unrelated to images, CNNs have also been used for text classification (Kim, 2014; Zhang et al., 2015).",
      "startOffset" : 70,
      "endOffset" : 101
    }, {
      "referenceID" : 33,
      "context" : "Unrelated to images, CNNs have also been used for text classification (Kim, 2014; Zhang et al., 2015).",
      "startOffset" : 70,
      "endOffset" : 101
    }, {
      "referenceID" : 9,
      "context" : "Several techniques for reducing the rare words effects have been introduced in the literature, including spelling expansion (Habash, 2008), dictionary term expansion (Habash, 2008), proper name transliteration (Daumé and Jagarlamudi, 2011), treating words as a sequence of characters (Luong and Manning, 2016), subword units (Sennrich et al.",
      "startOffset" : 124,
      "endOffset" : 138
    }, {
      "referenceID" : 9,
      "context" : "Several techniques for reducing the rare words effects have been introduced in the literature, including spelling expansion (Habash, 2008), dictionary term expansion (Habash, 2008), proper name transliteration (Daumé and Jagarlamudi, 2011), treating words as a sequence of characters (Luong and Manning, 2016), subword units (Sennrich et al.",
      "startOffset" : 166,
      "endOffset" : 180
    }, {
      "referenceID" : 5,
      "context" : "Several techniques for reducing the rare words effects have been introduced in the literature, including spelling expansion (Habash, 2008), dictionary term expansion (Habash, 2008), proper name transliteration (Daumé and Jagarlamudi, 2011), treating words as a sequence of characters (Luong and Manning, 2016), subword units (Sennrich et al.",
      "startOffset" : 210,
      "endOffset" : 239
    }, {
      "referenceID" : 18,
      "context" : "Several techniques for reducing the rare words effects have been introduced in the literature, including spelling expansion (Habash, 2008), dictionary term expansion (Habash, 2008), proper name transliteration (Daumé and Jagarlamudi, 2011), treating words as a sequence of characters (Luong and Manning, 2016), subword units (Sennrich et al.",
      "startOffset" : 284,
      "endOffset" : 309
    }, {
      "referenceID" : 23,
      "context" : "Several techniques for reducing the rare words effects have been introduced in the literature, including spelling expansion (Habash, 2008), dictionary term expansion (Habash, 2008), proper name transliteration (Daumé and Jagarlamudi, 2011), treating words as a sequence of characters (Luong and Manning, 2016), subword units (Sennrich et al., 2015), and reading text as bytes (Gillick et al.",
      "startOffset" : 325,
      "endOffset" : 348
    }, {
      "referenceID" : 8,
      "context" : ", 2015), and reading text as bytes (Gillick et al., 2015).",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "Several techniques for reducing the rare words effects have been introduced in the literature, including spelling expansion (Habash, 2008), dictionary term expansion (Habash, 2008), proper name transliteration (Daumé and Jagarlamudi, 2011), treating words as a sequence of characters (Luong and Manning, 2016), subword units (Sennrich et al., 2015), and reading text as bytes (Gillick et al., 2015). However, most of these techniques still have no mechanism for handling low frequency characters, which are the target of this work. Finally, there is one work by Shi et al. (2015) on “radical embedding”, which explicitly splits Chinese characters into radicals based on a dictionary of what radicals are included in which characters.",
      "startOffset" : 211,
      "endOffset" : 580
    } ],
    "year" : 2017,
    "abstractText" : "Previous work has modeled the compositionality of words by creating characterlevel models of meaning, reducing problems of sparsity for rare words. However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry categorical content which resulting in embeddings that are coherent in visual space.",
    "creator" : "LaTeX with hyperref package"
  }
}