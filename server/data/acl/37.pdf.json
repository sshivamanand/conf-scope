{
  "name" : "37.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Conversational agents include task-oriented dialog systems and non-task-oriented chatbots. Dialog systems focus on helping people complete specific tasks in vertical domains (Young et al., 2010), while chatbots aim to naturally and meaningfully converse with humans on open domain topics (Ritter et al., 2011). Existing work on building chatbots includes generation based methods and retrieval based methods. Retrieval based chatbots enjoy the advantage of informative and fluent responses, because they select a proper response for the current conversation from a repository with re-\nsponse selection algorithms. While most existing work on retrieval based chatbots studies response selection for single-turn conversation (Wang et al., 2013) which only considers the last input message, we consider the problem in a multi-turn scenario. In a chatbot, multi-turn response selection takes a message and utterances in its previous turns as input and selects a response that is natural and relevant to the whole context.\nThe key to response selection lies in inputresponse matching. Different from single-turn conversation, multi-turn conversation requires matching between a response and a conversation context in which one needs to consider not only the matching between the response and the input message but also the matching between the response and the utterances in previous turns. The challenges of the task include (1) how to identify important information (words, phrases, and sentences) in the context that is crucial to selecting a proper response and how to leverage the information in matching; and (2) how to model relationships among the utterances in the context. Table 1 illustrates the challenges with an example. First, “hold a drum class” and “drum” in the context are very important. Without them, one may find responses relevant to the message (i.e., the last turn of the context) but nonsense in the context (e.g., “what lessons do you want?”). Second,\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nthe message highly depends on the second turn in the context, and the order of the utterances matters in response selection: exchanging the third turn and the last turn may lead to different responses. Existing work, however, either ignores relationships among utterances when concatenating them together (Lowe et al., 2015), or loses important information in context in the process of converting the whole context to a vector without enough supervision from responses (e.g., by a hierarchical RNN (Zhou et al., 2016)).\nWe propose a sequential matching network (SMN), a new context based matching model that can tackle both challenges in an end-to-end way. The reason that existing models lose important information in the context is that they first represent the whole context as a vector and then match the context vector with a response vector. Thus responses in these models cannot meet the context until the final step in matching. To avoid information loss, SMN matches a response with each utterance in the context at the beginning and encodes important information in each pair into a matching vector. The matching vectors are then accumulated in the temporal order of the utterances to model their relationships. The final matching degree is computed with the accumulation of the matching vectors. Specifically, for each utteranceresponse pair, the model constructs a word-word similarity matrix and a sequence-sequence similarity matrix by the embedding of words and the hidden states of a recurrent neural network with gated recurrent unites (GRU) (Chung et al., 2014) respectively. The two matrices capture important matching information in the pair on a word level and a segment level respectively, and the information is distilled and fused as a matching vector through an alternation of convolution and pooling operations on the matrices. By this means, important information from multiple levels of granularity in the context is recognized under sufficient supervision from the response and carried into matching with minimal loss. The matching vectors are then uploaded to another GRU to form a matching score for the context and the response. The GRU accumulates the pair matching in its hidden states in the chronological order of the utterances in the context. It models relationships and dependencies among the utterances in a matching fashion and has the utterance order supervise the accumulation of pair matching. The match-\ning degree of the context and the response is computed by a logit model with the hidden states of the GRU. SMN extends the powerful “2D” matching paradigm in text pair matching for single-turn conversation to context based matching for multi-turn conversation, and enjoys the advantage that both important information in utterance-response pairs and relationships among utterances are sufficiently preserved and leveraged in matching.\nWe test our model on the Ubuntu dialogue corpus (Lowe et al., 2015) which is a large scale public English data set for research in multi-turn conversation. The results show that our model can significantly outperform state-of-the-art methods, and improvement to the best baseline model on R10@1 is over 6%. In addition to the Ubuntu corpus, we create a human labeled Chinese data set, namely Douban Conversation Corpus, and test our model on it. Different from the Ubuntu corpus in which data is collected from a specific domain and negative candidates are randomly sampled, conversations in this data come from open domain, and response candidates in this data set are collected from a retrieval engine and labeled by three human judges. On this data, our model improves the best baseline model over 3% on R10@1 and 4% on P@1. As far as we know, Douban Conversation Corpus is the first human labeled data set for multi-turn response selection and could be a good complement to the Ubuntu corpus. We release Douban Conversation Corups and our source code at an anonymous url for blind review. We have uploaded code and data with this paper.\nOur contributions in this paper are three-folds: (1) proposal of a new context based matching model for multi-turn response selection in retrieval based chatbots; (2) publication of a large human labeled data set to research communities. (3) empirical verification of the effectiveness of the model on public data sets;"
    }, {
      "heading" : "2 Related Work",
      "text" : "Recently, building a chatbot with data driven approaches (Ritter et al., 2011; Higashinaka et al., 2014) has drawn a lot of attention. Existing work along this line includes retrieval based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation based methods (Shang et al., 2015; Serban et al., 2015; Vinyals and Le, 2015; Li et al., 2015, 2016; Xing\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n.... .... ....\nScore\n1 2,M M Convolution Pooling\n( )L\n.... .... ....\n1u\n1nu \nnu\nr\nWord Embedding GRU1\nGRU2\n....\n1v\n1nv \nnv\n1'nh \nUtterance-Response Matching Matching Accumulation\nSegment PairsWord Pairs\nMatching Prediction\n1'h\n'nh\nFigure 1: Architecture of SMN\net al., 2016; Serban et al., 2016a). Our work belongs to retrieval based methods, and we study context based response selection.\nEarly studies of retrieval based chatbots focus on response selection for single-turn conversation (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b). Recently, researchers begin to pay attention to multi-turn conversation. For example, Lowe et al. (2015) match a response with the literal concatenation of context utterances. Yan et al. (2016) concatenate context utterances with the input message as reformulated queries and perform matching with a deep neural network architecture. Zhou et al. (2016) improve multi-turn response selection with a multi-view model including an utterance view and a word view. Our model is different in that it matches a response with each utterance at first and accumulates matching information instead of sentences by a GRU, thus useful information for matching can be sufficiently retained."
    }, {
      "heading" : "3 Sequential Matching Network",
      "text" : ""
    }, {
      "heading" : "3.1 Problem Formalization",
      "text" : "Suppose that we have a data set D = {(yi, si, ri)}Ni=1, where si = {ui,1, . . . , ui,ni} represents a conversation context with {ui,k}nik=1 as utterances. ri is a response candidate and yi ∈ {0, 1} denotes a label. yi = 1 means ri is a proper response for si, otherwise yi = 0. Our goal is to learn a matching model g(·, ·) with D. For any context-response pair (s, r), g(s, r) measures the matching degree between s and r."
    }, {
      "heading" : "3.2 Model Overview",
      "text" : "We propose a sequential matching network (SMN) to model g(·, ·). Figure 1 gives the architecture.\nSMN first decomposes context-response matching into several utterance-response pair matching and then all pair matching is accumulated as a context based matching through a recurrent neural network. SMN consists of three layers. The first layer matches a response candidate with each utterance in the context on a word level and a segment level, and important matching information from the two levels is distilled by convolution and pooling and encoded in a matching vector. The matching vectors are then fed into the second layer where they are accumulated in the hidden states of a recurrent neural network with GRU following the chronological order of the utterances in the context. The third layer calculates the final matching score with the hidden states of the second layer.\nSMN enjoys several advantages over the existing models. First, a response candidate can meet each utterance in the context at the very beginning, thus matching information in every utteranceresponse pair can be sufficiently extracted and carried to the final matching score with minimal loss. Second, information extraction from each utterance is conducted on different levels of granularity and under sufficient supervision from the response, thus semantic structures that are useful to response selection in each utterance can be well identified and extracted. Third, matching and utterance relationships are coupled rather than separately modeled, thus utterance relationships (e.g., order), as a kind of knowledge, can supervise the formation of the matching score.\nBy taking utterance relationships into account, SMN extends the “2D” matching that has proven effective in text pair matching for single-turn response selection to sequential “2D” matching for context based matching in response selection for multi-turn conversation. In the following sections,\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nwe will describe details of the three layers."
    }, {
      "heading" : "3.3 Utterance-Response Matching",
      "text" : "Given an utterance u in a context s and a response candidate r, the model looks up an embedding table and represents u and r as U = [eu,1, . . . , eu,nu ] and R = [er,1, . . . , er,nr ] respectively, where eu,i, er,i ∈ Rd are the embeddings of the i-th word of u and r respectively. U ∈ Rd×nu and R ∈ Rd×nr are then used to construct a word-word similarity matrix M1 ∈ Rnu×nr and a sequence-sequence similarity matrix M2 ∈ Rnu×nr which are two input channels of a convolutional neural network (CNN). The CNN distills important matching information from the matrices and encodes the information into a matching vector v.\nSpecifically, ∀i, j, the (i, j)-th element of M1 is defined by\ne1,i,j = e > u,i · er,j . (1)\nM1 models the matching between u and r on a word level.\nTo construct M2, we first employ a GRU to transform U and R to hidden vectors. Suppose that Hu = [hu,1, . . . , hu,nu ] are the hidden vectors of U, then ∀i, hu,i ∈ Rm is defined by\nzi = σ(Wzeu,i +Uzhu,i−1) ri = σ(Wreu,i +Urhu,i−1)\nh̃u,i = tanh(Wheu,i +Uh(ri hu,i−1)) hu,i = zi h̃u,i + (1− zi) hu,i−1, (2)\nwhere hu,0 = 0, zi and ri are an update gate and a reset gate respectively, σ(·) is a sigmoid function, and Wz, Wh, Wr, Uz, Ur,Uh are parameters. Similarly, we have Hr = [hr,1, . . . , hr,nr ] as the hidden vectors of R. Then, ∀i, j, the (i, j)-th element of M2 is defined by\ne2,i,j = h > u,iAhr,j , (3)\nwhere A ∈ Rm×m is a linear transformation. ∀i, GRU models the sequential relationship and the dependency among words up to position i and encodes the text segment until the i-th word to a hidden vector. Therefore, M2 models the matching between u and r on a segment level. M1 and M2 are then processed by a CNN to form v. ∀f = 1, 2, CNN regards Mf as an input channel, and alternates convolution and max-pooling operations. Suppose that z(l,f) =\n[ z (l,f) i,j ] I(l,f)×J(l,f) denotes the output of feature maps of type-f on layer-l, where z(0,f) = Mf , ∀f = 1, 2. On the convolution layer, we employ a 2D convolution operation with a window size r (l,f) w × r(l,f)h , and define z (l,f) i,j as\nz (l,f) i,j = σ( Fl−1∑ f ′=0 r (l,f) w∑ s=0 r (l,f) h∑ t=0 W (l,f) s,t · z (l−1,f ′) i+s,j+t + b l,k), (4)\nwhere σ(·) is a ReLU, W(l,f) ∈ Rr (l,f) w ×r (l,f) h and bl,k are parameters, and Fl−1 is the number of feature maps on the (l − 1)-th layer. A max pooling operation follows a convolution operation and can be formulated as\nz (l,f) i,j = max\np (l,f) w >s≥0 max p (l,f) h >t≥0 zi+s,j+t, (5)\nwhere p(l,f)w and p (l,f) h are the width and the height of the 2D pooling respectively. The output of the final feature maps are concatenated and mapped to a low dimensional space with a linear transformation as the matching vector v ∈ Rq.\nFrom Equation (1), (3), (4), and (5), we can see that by learning word embedding and parameters of GRU from training data, words or segments in an utterance that are useful to recognize the appropriateness of a response may have high similarity with some words or segments in the response and result in high value areas in the similarity matrices. These areas will be transformed and selected by convolution and pooling operations and carry the important information in the utterance to the matching vector. This is how our model identifies important information in context and leverage it in matching under the supervision of the response. We consider multiple channels because we want to capture important matching information on multiple levels of granularity of text."
    }, {
      "heading" : "3.4 Matching Accumulation",
      "text" : "Suppose that [v1, . . . , vn] is the output of the first layer (corresponding to n pairs), at the second layer, a GRU takes [v1, . . . , vn] as an input and encodes the matching sequence into its hidden states Hm = [h ′ 1, . . . , h ′ n] ∈ Rq×n with a detailed parameterization similar to Equation (2). This layer has two functions: (1) it models the dependency and the temporal relationship of utterances in the context; (2) it leverages the temporal relationship to supervise the accumulation of the pair matching as a context based matching. Moreover, from\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nEquation (2), we can see that the reset gate (i.e., ri) and the update gate (i.e., zi) control how much information from the previous hidden state and the current input flows to the current hidden state, thus important matching vectors (corresponding to important utterances) can be accumulated while noise in the vectors can be filtered out."
    }, {
      "heading" : "3.5 Matching Prediction and Learning",
      "text" : "With [h′1, . . . , h ′ n], we define g(s, r) as\ng(s, r) = softmax(W2L[h ′ 1, . . . , h ′ n] + b2), (6)\nwhere W2 and b2 are parameters. We consider three parameterizations for L[h′1, . . . , h ′ n]: (1) only the last hidden state is used. Then L[h′1, . . . , h ′ n] = h ′ n. (2) the hidden states\nare linearly combined. Then, L[h′1, . . . , h ′ n] =∑n\ni=1wih ′ i, where wi ∈ R. (3) we follow (Yang et al., 2016) and employ an attention mechanism to combine the hidden states. Then, L[h′1, . . . , h ′ n] is defined as\nti = tanh(W1,1hui,nu +W1,2h ′ i + b1), αi = exp(t>i ts)∑ i(exp(t > i ts)) ,\nL[h′1, . . . , h ′ n] = n∑ i=1 αih ′ i, (7)\nwhere W1,1 ∈ Rq×m,W1,2 ∈ Rq×q and b1 ∈ Rq are parameters. h′i and hui,nu are the i-th matching vector and the final hidden state of the i-th utterance respectively. ts ∈ Rq is a virtual context vector which is randomly initialized and jointly learned in training.\nBoth (2) and (3) aim to learn weights for {h′1, . . . , h′n} from training data and highlight the effect of important matching vectors in the final matching. The difference is that weights in (2) are static, because the weights are totally determined by the positions of utterances, while weights in (3) are dynamically computed by the matching vectors and utterance vectors. We denote our model with the three parameterizations of L[h′1, . . . , h ′ n] as SMNlast, SMNstatic, and SMNdynamic, and empirically compare them in experiments.\nWe learn g(·, ·) by minimizing cross entropy withD. Let Θ denote the parameters of SMN, then the objective function L(D,Θ) of learning can be formulated as\n− N∑ i=1 [yilog(g(si, ri)) + (1− yi)log(1− g(si, ri))] . (8)"
    }, {
      "heading" : "4 Response Candidate Retrieval",
      "text" : "In practice of a retrieval based chatbot, to apply the matching approach to response selection, one needs to retrieve a bunch of response candidates from an index beforehand. While candidate retrieval is not the focus of the paper, it is an important step in a real system. In this work, we exploit a heuristic method to obtain response candidates from the index. Given a message un with {u1, . . . , un−1} utterances in its previous turns, we extract top 5 keywords from {u1, . . . , un−1} based on their tf-idf scores1 and expand un with the keywords. Then we send the expanded message to the index and retrieve response candidates using the inline retrieval algorithm of the index. Finally, we use g(s, r) to re-rank the candidates and return the top one as a response to the context."
    }, {
      "heading" : "5 Experiments",
      "text" : "We tested our model on a public English data set and a Chinese data set we publish with this paper."
    }, {
      "heading" : "5.1 Ubuntu Corpus",
      "text" : "The English data set is the Ubuntu Corpus (Lowe et al., 2015) which contains multi-turn dialogues collected from chat logs of Ubuntu Forum. The data set consists of 1 million context-response pairs for training, 0.5 million pairs for validation, and 0.5 million pairs for test. Positive responses are true responses from human, and negative ones are randomly sampled. The ratio of the positive and the negative is 1:1 in training, and 1:9 in validation and test. We used the copy shared by Xu et al. (2016) 2 in which numbers, urls, and paths are replaced by special placeholders. We followed (Lowe et al., 2015) and employed recall at position k in n candidates (Rn@k) as evaluation metrics."
    }, {
      "heading" : "5.2 Douban Conversation Corpus",
      "text" : "Ubuntu Corpus is a domain specific data set, and response candidates are obtained from negative sampling without human judgment. To further verify the efficacy of our model, we created a new data set with open domain conversations, namely Douban Conversation Corpus. Response candidates in the test set of Douban Conversation Corpus are collected following the procedure of a re-\n1Tf is word frequency in the context, while idf is calculated using the entire index.\n2https://www.dropbox.com/s/ 2fdn26rj6h9bpvl/ubuntudata.zip?dl=0\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\ntrieval based chatbot and are labeled by human judges. Douban Conversation Corpus simulates the real scenario of a retrieval based chatbot, and we publish it to research communities to facilitate the research of multi-turn response selection.\nSpecifically, we crawled 1.1 million dyadic dialogues (conversation between two persons) longer than 2 turns from Douban group3 which is a popular social networking service in China. From the data, we randomly sampled 0.5 million dialogues for creating a training set, 25 thousand dialouges for creating a validation set, and 1, 000 dialogues for creating a test set, and made sure that there is no overlap among the three sets. For each dialogue in training and validation, we took the last turn as a positive response for the previous turns as a context and randomly sampled another response from the 1.1 million data as a negative response. There are 1 million context-response pairs in the training set and 50 thousand pairs in the validation set.\nTo create the test set, we first crawled 15 million post-reply pairs from Sina Weibo4 which is the largest microblogging service in China and indexed the pairs with Lucene5. We took the last turn of each Douban dyadic dialogue in the test set as a message, retrieved 10 response candidates from the index following the method in Section 4, and finally formed a test set with 10, 000 context-response pairs. We recruited three labelers to judge if a candidate is a proper response to the context. A proper response means the response can naturally reply to the message given the whole context. Each pair received three labels and the majority of the labels were taken as the final decision. Table 2 gives the statistics of the three sets. Note that the Fleiss’ kappa (Fleiss, 1971) of the labeling is 0.41, which indicates that the three labelers reached a relatively high agreement.\nBesides Rn@ks, we also followed the convention of information retrieval and employed mean average precision (MAP) (Baeza-Yates et al., 1999), mean reciprocal rank (MRR) (Voorhees et al., 1999), and precision at position 1 (P@1) as evaluation metrics. We did not calculate R2@1 because in Douban corpus one context could have more than one correct responses, and we have to randomly sample one for R2@1, which may bring bias to evaluation. When using the labeled set,\n3https://www.douban.com/group 4http://weibo.com/ 5https://lucenenet.apache.org/\nwe removed conversations with all negative responses or all positive responses, as models make no difference on them. There are 6, 670 contextresponse pairs left in the test set."
    }, {
      "heading" : "5.3 Baseline",
      "text" : "We considered the following baselines:\nBasic models: models in (Lowe et al., 2015) and (Kadlec et al., 2015) including TF-IDF, RNN, CNN, LSTM and BiLSTM.\nMulti-view: the model proposed by Zhou et al. (2016) that utilizes a hierarchical recurrent neural network to model utterance relationships.\nDeep learning to respond (DL2R): the model proposed by Yan et al. (2016) that reformulates the message with other utterances in the context.\nAdvanced single-turn matching models: since BiLSTM does not represent the state-ofthe-art matching model, we concatenated the utterances in a context and matched the long text with a response candidate using more powerful models including MV-LSTM (Wan et al., 2016) (2D matching), Match-LSTM (Wang and Jiang, 2015), Attentive-LSTM (Tan et al., 2015) (two attention based models), and Multi-Channel which is described in Section 3.3. Multi-Channel is a simple version of our model without considering utterance relationships. We also appended the top 5 tf-idf words in context to the input message, and computed the score between the expanded message and a response with Multi-Channel, denoted as Multi-Channelexp."
    }, {
      "heading" : "5.4 Parameter Tuning",
      "text" : "For baseline models, if their results are available in the existing literatures (e.g., those on the Ubuntu corpus), we just copied the numbers, otherwise we implemented the models following the settings in the literatures. All models were implemented using Theano (Theano Development Team, 2016). Word embeddings were initialized by the results of word2vec (Mikolov et al., 2013) which ran on the training data, and the dimensionality of word vectors is 200. For Multi-Channel and layer one of\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nUbuntu Corpus Douban Conversation Corpus R2@1 R10@1 R10@2 R10@5 MAP MRR P@1 R10@1 R10@2 R10@5\nTF-IDF 0.659 0.410 0.545 0.708 0.331 0.359 0.179 0.095 0.172 0.405 RNN 0.768 0.403 0.547 0.819 0.390 0.422 0.208 0.011 0.223 0.589 CNN 0.848 0.549 0.684 0.896 0.417 0.440 0.226 0.012 0.252 0.647 LSTM 0.901 0.638 0.784 0.949 0.485 0.527 0.320 0.187 0.343 0.720 BiLSTM 0.895 0.630 0.780 0.944 0.479 0.514 0.313 0.184 0.330 0.716 Multi-View 0.908 0.662 0.801 0.951 0.505 0.543 0.342 0.202 0.350 0.729 DL2R 0.899 0.626 0.783 0.944 0.488 0.527 0.330 0.193 0.342 0.705 MV-LSTM 0.906 0.653 0.804 0.946 0.498 0.538 0.348 0.202 0.351 0.710 Match-LSTM 0.904 0.653 0.799 0.944 0.500 0.537 0.345 0.202 0.348 0.720 Attentive-LSTM 0.903 0.633 0.789 0.943 0.495 0.523 0.331 0.192 0.328 0.718 Multi-Channel 0.904 0.656 0.809 0.942 0.506 0.543 0.349 0.203 0.351 0.709 Multi-Channelexp 0.714 0.368 0.497 0.745 0.476 0.515 0.317 0.179 0.335 0.691 SMNlast 0.923 0.723 0.842 0.956 0.526 0.571 0.392 0.236 0.387 0.729 SMNstatic 0.927 0.725 0.838 0.962 0.523 0.572 0.387 0.228 0.387 0.734 SMNdynamic 0.926 0.726 0.847 0.961 0.529 0.569 0.395 0.233 0.396 0.724\nTable 3: Evaluation results on the two data sets. Numbers in bold mean that the improvement is statistically significant compared with the best baseline.\nour model, we set the dimensionality of the hidden states of GRU as 200. We tuned the window size of convolution and pooling in {(2, 2), (3, 3)(4, 4)} and chose (3, 3) finally. The number of feature maps is 8. In layer two, we set the dimensionality of matching vectors and the hidden states of GRU as 50. The parameters were updated by stochastic gradient descent with Adam algorithm (Kingma and Ba, 2014) on a single Tesla K80 GPU. The initial learning rate is 0.001, and the parameters of Adam, β1 and β2 are 0.9 and 0.999 respectively. We employed early-stopping as a regularization strategy. Models were trained in minibatches with a batch size 200, and maximum utterance length is 50. We set the maximum context length (i.e., number of utterances) as 10, because performance of models does not get improved on contexts longer than 10 (details are shown in the supplementary material). We padded zeros if the number of utterances in a context is less than 10, otherwise we kept the last 10 utterances."
    }, {
      "heading" : "5.5 Evaluation Results",
      "text" : "Table 3 shows the evaluation results on the two data sets. Our models outperform baselines greatly in terms of all metrics on both data sets, and the improvements are statistically significant (t-test with p-value ≤ 0.01, except R10@5 on Douban Corpus). Even the state-of-the-art singleturn matching models perform much worse than our models. The results demonstrate that one cannot neglects utterance relationships and simply perform multi-turn response selection by concatenating utterances together. Our models achieve significant improvements over Multi-View, which justified our “matching first” strategy. DL2R is\nworse than our models, indicating that utterance reformulation with heuristic rules is not a good method to utilize context information. Rn@ks are low on Douban corpus as there are multiple correct candidates for a context (e.g., if there are 3 correct responses, then the maximumR10@1 is 0.33). SMNdynamic is only slightly better than SMNstatic and SMNlast. The reason might be that GRU can select useful signals from the matching sequence and accumulate them in the final state with its gate mechanism, thus the efficacy of attention mechanism is not obvious for the task."
    }, {
      "heading" : "5.6 Further Analysis",
      "text" : "Visualization: we visualize the similarity matrices and the gates of GRU in layer two using an example from the Ubuntu corpus to further clarify how our model identifies important information in the context and how it selects important matching vectors with the gate mechanism of GRU as described in Section 3.3 and Section 3.4. The example is {u1: how can unzip many rar ( number for example ) files at once; u2: sure you can do that in bash; u3: okay how? u4: are the files all in the same directory? u5: yes they all are; r: then the command glebihan should extract them all from/to that directory}. It is from the test set and our model successfully ranked the correct response to the top position. Due to space limitation, we only visualized M1, M2 and the update gate (i.e. z) in Figure 2. Other pieces of our model are shown in the supplementary material. We can see that in u1 important words including “unzip”, “rar”, “files” are recognized and carried to matching by “command”, “extract”, and “directory” in r, while u3 is almost useless and thus little infor-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nUbuntu Corpus Douban Conversation Corpus R2@1 R10@1 R10@2 R10@5 MAP MRR P@1 R10@1 R10@2 R10@5\nReplaceM 0.905 0.661 0.799 0.950 0.503 0.541 0.343 0.201 0.364 0.729 ReplaceA 0.918 0.716 0.832 0.954 0.522 0.565 0.376 0.220 0.385 0.727 Only M1 0.919 0.704 0.832 0.955 0.518 0.562 0.370 0.228 0.371 0.737 Only M2 0.921 0.715 0.836 0.956 0.521 0.565 0.382 0.232 0.380 0.734 SMNlast 0.923 0.723 0.842 0.956 0.526 0.571 0.392 0.236 0.387 0.729\nTable 4: Evaluation results of model ablation.\nth en th e\nco m\nm an\nd gl eb ih an sh ou ld ex tra ct th em a ll fro m /toth at di re ct or y\nhow can\nunzip many\nrar ( _number_ for\nexample )\nfiles at once\n0.00\n0.15\n0.30\n0.45\n0.60\n0.75\n0.90\n1.05\n1.20\n1.35\n1.50\nv a lu\ne\n(a) M1 of u1 and r\nth en th e\nco m\nm an\nd\ngl eb\nih an\nsh ou\nld ex tr ac\nt th em a ll\nfro m\n/toth at\ndi re\nct or\ny\nokay how 0.00 0.15 0.30 0.45 0.60 0.75 0.90 1.05 1.20 1.35 1.50 v a lu e\n(b) M1 of u3 and r\n0 10 20 30 40\nu_1\nu_2\nu_3\nu_4\nu_5\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nv a lu\ne\n(c) Update gate\nFigure 2: Model visualization. Darker areas mean larger value.\nmation is extracted from it. u1 is crucial to response selection and nearly all information from u1 and r flows to the hidden state of GRU, while other utterances are less informative and the corresponding gates are almost “closed” to keep the information from u1 and r until the final state.\nModel ablation: we investigate the effect of different parts of SMN by removing them one by one from SMNlast, shown in Table 4. First, replacing the multi-channel “2D” matching with a neural tensor network (NTN) (Socher et al., 2013) (denoted as ReplaceM ) makes the performance drop dramatically. This is because NTN only matches a pair by an utterance vector and a response vector and loses important information in the pair. Together with the visualization, we can conclude that “2D” matching plays a key role in the “matching first” strategy as it captures the important matching information in each pair with minimal loss. Second, the performance slightly drops when replacing the GRU for matching accumulation with a multi-layer perceptron (denoted as ReplaceA). This indicates that utterance relationships are useful. Finally, we left only one channel in matching and found that M2 is a little more powerful than M1 and we achieve the best results with both of them (except on R10@5 on Douban Corpus).\nContext length: we study how our model (SMNlast) performs across the length of contexts. Figure 3 shows the comparison on MAP in different length intervals on the Douban corpus. Our model consistently performs better than the baselines, and when contexts become longer, the gap becomes larger. The results demonstrate that our model can well capture the dependencies, espe-\ncially long dependencies, among utterances in contexts. We give the comparisons on other metrics in our supplementary material.\n(2,5] (5,10] (10,) context length\n40\n45\n50\n55 60 M A P\nLSTM MV-LSTM Multi-View SMN\nFigure 3: Comparison across context length Retrieval v.s. Generation: we compared SMN with a state-of-the-art response generation model VHERD (Serban et al., 2016b) which was trained using D on the Douban corpus. We conducted a side-by-side human comparison on the top one responses of the two models for each context in the test set. The result is that SMN wins on 238 examples, loses on 207 examples, and is comparable with VHRED on the remaining 555 examples. This indicates that a retrieval based chatbot with SMN can provide a better experience than the state-of-the-art generation model in practice."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "We present a new context based model for multiturn response selection in retrieval-based chatbots. Experiment results on public data sets show that the model can significantly outperform the stateof-the-art methods. Besides, we publish the first human labeled multi-turn response selection data set to research communities. In the future, we are going to study how to model logical consistency of responses and improve candidate retrieval (see supplementary material).\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Modern information retrieval, volume 463",
      "author" : [ "Ricardo Baeza-Yates", "Berthier Ribeiro-Neto" ],
      "venue" : null,
      "citeRegEx" : "Baeza.Yates and Ribeiro.Neto,? \\Q1999\\E",
      "shortCiteRegEx" : "Baeza.Yates and Ribeiro.Neto",
      "year" : 1999
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1412.3555 .",
      "citeRegEx" : "Chung et al\\.,? 2014",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Measuring nominal scale agreement among many raters",
      "author" : [ "Joseph L Fleiss." ],
      "venue" : "Psychological bulletin 76(5):378.",
      "citeRegEx" : "Fleiss.,? 1971",
      "shortCiteRegEx" : "Fleiss.",
      "year" : 1971
    }, {
      "title" : "Towards an open-domain conversational system fully based on natural language",
      "author" : [ "Ryuichiro Higashinaka", "Kenji Imamura", "Toyomi Meguro", "Chiaki Miyazaki", "Nozomi Kobayashi", "Hiroaki Sugiyama", "Toru Hirano", "Toshiro Makino", "Yoshihiro Matsuo" ],
      "venue" : null,
      "citeRegEx" : "Higashinaka et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Higashinaka et al\\.",
      "year" : 2014
    }, {
      "title" : "Convolutional neural network architectures for matching natural language sentences",
      "author" : [ "Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 2042–2050.",
      "citeRegEx" : "Hu et al\\.,? 2014",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2014
    }, {
      "title" : "An information retrieval approach to short text conversation",
      "author" : [ "Zongcheng Ji", "Zhengdong Lu", "Hang Li." ],
      "venue" : "arXiv preprint arXiv:1408.6988 .",
      "citeRegEx" : "Ji et al\\.,? 2014",
      "shortCiteRegEx" : "Ji et al\\.",
      "year" : 2014
    }, {
      "title" : "Improved deep learning baselines for ubuntu corpus dialogs",
      "author" : [ "Rudolf Kadlec", "Martin Schmid", "Jan Kleindienst." ],
      "venue" : "arXiv preprint arXiv:1510.03753 .",
      "citeRegEx" : "Kadlec et al\\.,? 2015",
      "shortCiteRegEx" : "Kadlec et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980 .",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "A diversity-promoting objective function for neural conversation models",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "arXiv preprint arXiv:1510.03055 .",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "A persona-based neural conversation model",
      "author" : [ "Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan." ],
      "venue" : "arXiv preprint arXiv:1603.06155 .",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
      "author" : [ "Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau." ],
      "venue" : "arXiv preprint arXiv:1506.08909 .",
      "citeRegEx" : "Lowe et al\\.,? 2015",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Data-driven response generation in social media",
      "author" : [ "Alan Ritter", "Colin Cherry", "William B Dolan." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 583–593.",
      "citeRegEx" : "Ritter et al\\.,? 2011",
      "shortCiteRegEx" : "Ritter et al\\.",
      "year" : 2011
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "arXiv preprint arXiv:1507.04808 .",
      "citeRegEx" : "Serban et al\\.,? 2015",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2015
    }, {
      "title" : "Multiresolution recurrent neural networks: An application to dialogue response generation",
      "author" : [ "Iulian Vlad Serban", "Tim Klinger", "Gerald Tesauro", "Kartik Talamadupula", "Bowen Zhou", "Yoshua Bengio", "Aaron Courville." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Serban et al\\.,? 2016a",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "author" : [ "Iulian Vlad Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1605.06069 .",
      "citeRegEx" : "Serban et al\\.,? 2016b",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural responding machine for short-text conversation",
      "author" : [ "Lifeng Shang", "Zhengdong Lu", "Hang Li." ],
      "venue" : "ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers. pages 1577–1586.",
      "citeRegEx" : "Shang et al\\.,? 2015",
      "shortCiteRegEx" : "Shang et al\\.",
      "year" : 2015
    }, {
      "title" : "Reasoning with neural tensor networks for knowledge base completion",
      "author" : [ "Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 926–934.",
      "citeRegEx" : "Socher et al\\.,? 2013",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2013
    }, {
      "title" : "Lstmbased deep learning models for non-factoid answer selection",
      "author" : [ "Ming Tan", "Bing Xiang", "Bowen Zhou." ],
      "venue" : "arXiv preprint arXiv:1511.04108 .",
      "citeRegEx" : "Tan et al\\.,? 2015",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2015
    }, {
      "title" : "Theano: A Python framework for fast computation of mathematical expressions",
      "author" : [ "Theano Development Team." ],
      "venue" : "arXiv e-prints abs/1605.02688. http://arxiv.org/abs/1605.02688.",
      "citeRegEx" : "Team.,? 2016",
      "shortCiteRegEx" : "Team.",
      "year" : 2016
    }, {
      "title" : "A neural conversational model",
      "author" : [ "Oriol Vinyals", "Quoc Le." ],
      "venue" : "arXiv preprint arXiv:1506.05869 .",
      "citeRegEx" : "Vinyals and Le.,? 2015",
      "shortCiteRegEx" : "Vinyals and Le.",
      "year" : 2015
    }, {
      "title" : "The trec-8 question answering track report",
      "author" : [ "Ellen M Voorhees" ],
      "venue" : "Trec. volume 99, pages 77–",
      "citeRegEx" : "Voorhees,? 1999",
      "shortCiteRegEx" : "Voorhees",
      "year" : 1999
    }, {
      "title" : "Match-srnn: Modeling the recursive matching structure with spatial rnn",
      "author" : [ "Shengxian Wan", "Yanyan Lan", "Jun Xu", "Jiafeng Guo", "Liang Pang", "Xueqi Cheng." ],
      "venue" : "arXiv preprint arXiv:1604.04378 .",
      "citeRegEx" : "Wan et al\\.,? 2016",
      "shortCiteRegEx" : "Wan et al\\.",
      "year" : 2016
    }, {
      "title" : "A dataset for research on short-text conversations",
      "author" : [ "Hao Wang", "Zhengdong Lu", "Hang Li", "Enhong Chen." ],
      "venue" : "EMNLP. pages 935–945.",
      "citeRegEx" : "Wang et al\\.,? 2013",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Syntax-based deep matching of short texts",
      "author" : [ "Mingxuan Wang", "Zhengdong Lu", "Hang Li", "Qun Liu." ],
      "venue" : "arXiv preprint arXiv:1503.02427 .",
      "citeRegEx" : "Wang et al\\.,? 2015",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning natural language inference with lstm",
      "author" : [ "Shuohang Wang", "Jing Jiang." ],
      "venue" : "arXiv preprint arXiv:1512.08849 .",
      "citeRegEx" : "Wang and Jiang.,? 2015",
      "shortCiteRegEx" : "Wang and Jiang.",
      "year" : 2015
    }, {
      "title" : "Ranking responses oriented to conversational relevance in chat-bots",
      "author" : [ "Bowen Wu", "Baoxun Wang", "Hui Xue." ],
      "venue" : "COLING16 .",
      "citeRegEx" : "Wu et al\\.,? 2016a",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "Topic augmented neural network for short text conversation",
      "author" : [ "Yu Wu", "Wei Wu", "Zhoujun Li", "Ming Zhou." ],
      "venue" : "CoRR abs/1605.00090. http://arxiv.org/abs/1605.00090.",
      "citeRegEx" : "Wu et al\\.,? 2016b",
      "shortCiteRegEx" : "Wu et al\\.",
      "year" : 2016
    }, {
      "title" : "Topic augmented neural response generation with a joint attention mechanism",
      "author" : [ "Chen Xing", "Wei Wu", "Yu Wu", "Jie Liu", "Yalou Huang", "Ming Zhou", "Wei-Ying Ma." ],
      "venue" : "arXiv preprint arXiv:1606.08340 .",
      "citeRegEx" : "Xing et al\\.,? 2016",
      "shortCiteRegEx" : "Xing et al\\.",
      "year" : 2016
    }, {
      "title" : "Incorporating loose-structured knowledge into lstm with recall gate for conversation modeling",
      "author" : [ "Zhen Xu", "Bingquan Liu", "Baoxun Wang", "Chengjie Sun", "Xiaolong Wang." ],
      "venue" : "arXiv preprint arXiv:1605.05110 .",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to respond with deep neural networks for retrieval-based human-computer conversation system",
      "author" : [ "Rui Yan", "Yiping Song", "Hua Wu." ],
      "venue" : "Proceedings of the 39th International ACM SIGIR conference on Research and De-",
      "citeRegEx" : "Yan et al\\.,? 2016",
      "shortCiteRegEx" : "Yan et al\\.",
      "year" : 2016
    }, {
      "title" : "Hierarchical attention networks for document classification",
      "author" : [ "Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa-",
      "citeRegEx" : "Yang et al\\.,? 2016",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2016
    }, {
      "title" : "The hidden information state model: A practical framework for pomdp-based spoken dialogue management",
      "author" : [ "Steve Young", "Milica Gašić", "Simon Keizer", "François Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu." ],
      "venue" : "Computer Speech & Language",
      "citeRegEx" : "Young et al\\.,? 2010",
      "shortCiteRegEx" : "Young et al\\.",
      "year" : 2010
    }, {
      "title" : "Multiview response selection for human-computer conversation",
      "author" : [ "Xiangyang Zhou", "Daxiang Dong", "Hua Wu", "Shiqi Zhao", "R Yan", "D Yu", "Xuan Liu", "H Tian." ],
      "venue" : "EMNLP16 .",
      "citeRegEx" : "Zhou et al\\.,? 2016",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 32,
      "context" : "Dialog systems focus on helping people complete specific tasks in vertical domains (Young et al., 2010), while chatbots aim to naturally and meaningfully converse with humans on open domain topics (Ritter et al.",
      "startOffset" : 83,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : ", 2010), while chatbots aim to naturally and meaningfully converse with humans on open domain topics (Ritter et al., 2011).",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 23,
      "context" : "While most existing work on retrieval based chatbots studies response selection for single-turn conversation (Wang et al., 2013) which only considers the last input message, we consider the problem in a multi-turn scenario.",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 10,
      "context" : "Existing work, however, either ignores relationships among utterances when concatenating them together (Lowe et al., 2015), or loses important information in context in the process of converting the whole context to a vector without enough supervision from responses (e.",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 33,
      "context" : ", by a hierarchical RNN (Zhou et al., 2016)).",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "Specifically, for each utteranceresponse pair, the model constructs a word-word similarity matrix and a sequence-sequence similarity matrix by the embedding of words and the hidden states of a recurrent neural network with gated recurrent unites (GRU) (Chung et al., 2014) respectively.",
      "startOffset" : 252,
      "endOffset" : 272
    }, {
      "referenceID" : 10,
      "context" : "We test our model on the Ubuntu dialogue corpus (Lowe et al., 2015) which is a large scale public English data set for research in multi-turn conversation.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 12,
      "context" : "Recently, building a chatbot with data driven approaches (Ritter et al., 2011; Higashinaka et al., 2014) has drawn a lot of attention.",
      "startOffset" : 57,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "Recently, building a chatbot with data driven approaches (Ritter et al., 2011; Higashinaka et al., 2014) has drawn a lot of attention.",
      "startOffset" : 57,
      "endOffset" : 104
    }, {
      "referenceID" : 4,
      "context" : "Existing work along this line includes retrieval based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation based methods (Shang et al.",
      "startOffset" : 63,
      "endOffset" : 189
    }, {
      "referenceID" : 5,
      "context" : "Existing work along this line includes retrieval based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation based methods (Shang et al.",
      "startOffset" : 63,
      "endOffset" : 189
    }, {
      "referenceID" : 24,
      "context" : "Existing work along this line includes retrieval based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation based methods (Shang et al.",
      "startOffset" : 63,
      "endOffset" : 189
    }, {
      "referenceID" : 30,
      "context" : "Existing work along this line includes retrieval based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation based methods (Shang et al.",
      "startOffset" : 63,
      "endOffset" : 189
    }, {
      "referenceID" : 27,
      "context" : "Existing work along this line includes retrieval based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation based methods (Shang et al.",
      "startOffset" : 63,
      "endOffset" : 189
    }, {
      "referenceID" : 33,
      "context" : "Existing work along this line includes retrieval based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation based methods (Shang et al.",
      "startOffset" : 63,
      "endOffset" : 189
    }, {
      "referenceID" : 26,
      "context" : "Existing work along this line includes retrieval based methods (Hu et al., 2014; Ji et al., 2014; Wang et al., 2015; Yan et al., 2016; Wu et al., 2016b; Zhou et al., 2016; Wu et al., 2016a) and generation based methods (Shang et al.",
      "startOffset" : 63,
      "endOffset" : 189
    }, {
      "referenceID" : 23,
      "context" : "Early studies of retrieval based chatbots focus on response selection for single-turn conversation (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b).",
      "startOffset" : 99,
      "endOffset" : 172
    }, {
      "referenceID" : 5,
      "context" : "Early studies of retrieval based chatbots focus on response selection for single-turn conversation (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b).",
      "startOffset" : 99,
      "endOffset" : 172
    }, {
      "referenceID" : 24,
      "context" : "Early studies of retrieval based chatbots focus on response selection for single-turn conversation (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b).",
      "startOffset" : 99,
      "endOffset" : 172
    }, {
      "referenceID" : 27,
      "context" : "Early studies of retrieval based chatbots focus on response selection for single-turn conversation (Wang et al., 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b).",
      "startOffset" : 99,
      "endOffset" : 172
    }, {
      "referenceID" : 5,
      "context" : ", 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b). Recently, researchers begin to pay attention to multi-turn conversation. For example, Lowe et al. (2015) match a response with the literal concatenation of context utterances.",
      "startOffset" : 8,
      "endOffset" : 168
    }, {
      "referenceID" : 5,
      "context" : ", 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b). Recently, researchers begin to pay attention to multi-turn conversation. For example, Lowe et al. (2015) match a response with the literal concatenation of context utterances. Yan et al. (2016) concatenate context utterances with the input message as reformulated queries and perform matching with a deep neural network architecture.",
      "startOffset" : 8,
      "endOffset" : 257
    }, {
      "referenceID" : 5,
      "context" : ", 2013; Ji et al., 2014; Wang et al., 2015; Wu et al., 2016b). Recently, researchers begin to pay attention to multi-turn conversation. For example, Lowe et al. (2015) match a response with the literal concatenation of context utterances. Yan et al. (2016) concatenate context utterances with the input message as reformulated queries and perform matching with a deep neural network architecture. Zhou et al. (2016) improve multi-turn response selection with a multi-view model including an utterance view and a word view.",
      "startOffset" : 8,
      "endOffset" : 416
    }, {
      "referenceID" : 31,
      "context" : "(3) we follow (Yang et al., 2016) and employ an attention mechanism to combine the hidden states.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "1 Ubuntu Corpus The English data set is the Ubuntu Corpus (Lowe et al., 2015) which contains multi-turn dialogues collected from chat logs of Ubuntu Forum.",
      "startOffset" : 58,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "We followed (Lowe et al., 2015) and employed recall at position k in n candidates (Rn@k) as evaluation metrics.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "1 Ubuntu Corpus The English data set is the Ubuntu Corpus (Lowe et al., 2015) which contains multi-turn dialogues collected from chat logs of Ubuntu Forum. The data set consists of 1 million context-response pairs for training, 0.5 million pairs for validation, and 0.5 million pairs for test. Positive responses are true responses from human, and negative ones are randomly sampled. The ratio of the positive and the negative is 1:1 in training, and 1:9 in validation and test. We used the copy shared by Xu et al. (2016) 2 in which numbers, urls, and paths are replaced by special placeholders.",
      "startOffset" : 59,
      "endOffset" : 523
    }, {
      "referenceID" : 2,
      "context" : "Note that the Fleiss’ kappa (Fleiss, 1971) of the labeling is 0.",
      "startOffset" : 28,
      "endOffset" : 42
    }, {
      "referenceID" : 10,
      "context" : "3 Baseline We considered the following baselines: Basic models: models in (Lowe et al., 2015) and (Kadlec et al.",
      "startOffset" : 74,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : ", 2015) and (Kadlec et al., 2015) including TF-IDF, RNN, CNN, LSTM and BiLSTM.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "Advanced single-turn matching models: since BiLSTM does not represent the state-ofthe-art matching model, we concatenated the utterances in a context and matched the long text with a response candidate using more powerful models including MV-LSTM (Wan et al., 2016) (2D matching), Match-LSTM (Wang and Jiang, 2015), Attentive-LSTM (Tan et al.",
      "startOffset" : 247,
      "endOffset" : 265
    }, {
      "referenceID" : 25,
      "context" : ", 2016) (2D matching), Match-LSTM (Wang and Jiang, 2015), Attentive-LSTM (Tan et al.",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 18,
      "context" : ", 2016) (2D matching), Match-LSTM (Wang and Jiang, 2015), Attentive-LSTM (Tan et al., 2015) (two attention based models), and Multi-Channel which is described in Section 3.",
      "startOffset" : 73,
      "endOffset" : 91
    }, {
      "referenceID" : 6,
      "context" : ", 2015) and (Kadlec et al., 2015) including TF-IDF, RNN, CNN, LSTM and BiLSTM. Multi-view: the model proposed by Zhou et al. (2016) that utilizes a hierarchical recurrent neural network to model utterance relationships.",
      "startOffset" : 13,
      "endOffset" : 132
    }, {
      "referenceID" : 6,
      "context" : ", 2015) and (Kadlec et al., 2015) including TF-IDF, RNN, CNN, LSTM and BiLSTM. Multi-view: the model proposed by Zhou et al. (2016) that utilizes a hierarchical recurrent neural network to model utterance relationships. Deep learning to respond (DL2R): the model proposed by Yan et al. (2016) that reformulates the message with other utterances in the context.",
      "startOffset" : 13,
      "endOffset" : 293
    }, {
      "referenceID" : 11,
      "context" : "Word embeddings were initialized by the results of word2vec (Mikolov et al., 2013) which ran on the training data, and the dimensionality of word vectors is 200.",
      "startOffset" : 60,
      "endOffset" : 82
    }, {
      "referenceID" : 7,
      "context" : "The parameters were updated by stochastic gradient descent with Adam algorithm (Kingma and Ba, 2014) on a single Tesla K80 GPU.",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "First, replacing the multi-channel “2D” matching with a neural tensor network (NTN) (Socher et al., 2013) (denoted as ReplaceM ) makes the performance drop dramatically.",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "Generation: we compared SMN with a state-of-the-art response generation model VHERD (Serban et al., 2016b) which was trained using D on the Douban corpus.",
      "startOffset" : 84,
      "endOffset" : 106
    } ],
    "year" : 2017,
    "abstractText" : "We study response selection for multi-turn conversation in retrieval based chatbots. Existing work either concatenates utterances in context or matches a response with a highly abstract context vector finally, which may lose relationships among the utterances or important information in the context. We propose a sequential matching network (SMN) to address both problems. SMN first matches a response with each utterance in the context on multiple levels of granularity, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models relationships among the utterances. The final matching score is calculated with the hidden states of the RNN. Empirical study on two public data sets shows that SMN can significantly outperform state-of-the-art methods for response selection in multi-turn conversation.",
    "creator" : "LaTeX with hyperref package"
  }
}