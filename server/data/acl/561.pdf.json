{
  "name" : "561.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Semi-supervised sequence tagging with bidirectional language models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Due to their simplicity and efficacy, pre-trained word embedding have become ubiquitous in NLP systems. Many prior studies have shown that they capture useful semantic and syntactic information (Mikolov et al., 2013; Pennington et al., 2014) and including them in NLP systems has been shown to be enormously helpful for a variety of downstream tasks (Collobert et al., 2011).\nHowever, in many NLP tasks it is essential to represent not just the meaning of a word, but also the word in context. For example, in the two phrases “A Central Bank spokesman” and “The Central African Republic”, the word ‘Central’ is used as part of both an Organization and Location. Accordingly, current state of the art sequence tagging models typically include a bidirectional re-\ncurrent neural network (RNN) that encodes token sequences into a context sensitive representation before making token specific predictions (Yang et al., 2017; Ma and Hovy, 2016; Lample et al., 2016; Hashimoto et al., 2016).\nAlthough the token representation is initialized with pre-trained embeddings, the parameters of the bidirectional RNN are typically learned only on labeled data. Previous work has explored methods for jointly learning the bidirectional RNN with supplemental labeled data from other tasks (e.g., Søgaard and Goldberg, 2016; Yang et al., 2017).\nIn this paper, we explore an alternate semisupervised approach which does not require additional labeled data. We use a neural language model (LM) pre-trained on a large, unlabeled corpus to compute an encoding of the context at each position in the sequence (hereafter an LM embedding) and use it in the supervised sequence tagging model. Since the LM embeddings are used to compute the probability of future words in a neural LM, they are likely to encode both the semantic and syntactic roles of words in context.\nOur main contribution is to show that the context sensitive representation captured in the LM embeddings is useful in the supervised sequence tagging setting. When we include the LM embeddings in our system overall performance increases from 90.87% to 91.93% F1 for the CoNLL 2003 NER task, a more then 1% absolute F1 increase, and a substantial improvement over the previous state of the art. We also establish a new state of the art result (96.37% F1) for the CoNLL 2000 Chunking task.\nAs a secondary contribution, we show that using both forward and backward LM embeddings boosts performance over a forward only LM. We also demonstrate that domain specific pre-training is not necessary by applying a LM trained in the news domain to scientific papers.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199"
    }, {
      "heading" : "2 Language model augmented sequence taggers (TagLM)",
      "text" : ""
    }, {
      "heading" : "2.1 Overview",
      "text" : "The main components in our language-modelaugmented sequence tagger (TagLM) are illustrated in Fig. 1. After pre-training word embeddings and a neural LM on large, unlabeled corpora (Step 1), we extract the word and LM embeddings for every token in a given input sequence (Step 2) and use them in the supervised sequence tagging model (Step 3)."
    }, {
      "heading" : "2.2 Baseline sequence tagging model",
      "text" : "Our baseline sequence tagging model is a hierarchical neural tagging model, closely following a number of recent studies (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2017; Chiu and Nichols, 2016) (left side of Figure 2).\nGiven a sentence of tokens (t1, t2, . . . , tN ) it first forms a representation, xk, for each token by concatenating a character based representation ck with a token embedding wk:\nck = C(tk; θc)\nwk = E(tk; θw)\nxk = [ck;wk] (1)\nThe character representation ck captures morphological information and is either a convolutional neural network (CNN) (Ma and Hovy, 2016; Chiu and Nichols, 2016) or RNN (Yang et al., 2017; Lample et al., 2016). It is parameterized by C(·, θc) with parameters θc. The token embeddings, wk, are obtained as a lookup E(·, θw), initialized using pre-trained word embeddings, and fine tuned during training (Collobert et al., 2011).\nTo learn a context sensitive representation, we employ multiple layers of bidirectional RNNs. For each token position, k, the hidden state hk,i of RNN layer i is formed by concatenating the hidden states from the forward ( −→ h k,i) and backward ( ←− h k,i) RNNs. As a result, the bidirectional RNN is able to use both past and future information to make a prediction at token k. More formally, for the first RNN layer that operates on xk to output hk,1:\n−→ h k,1 = −→ R 1(xk, −→ h k−1,1; θ−→R1 ) ←− h k,1 = ←− R 1(xk, ←− h k+1,1; θ←−R1 )\nhk,1 = [ −→ h k,1; ←− h k,1] (2)\nThe second RNN layer is similar and uses hk,1 to output hk,2. In this paper, we use L = 2 layers of RNNs in all experiments and parameterize Ri as either Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) depending on the task.\nFinally, the output of the final RNN layer hk,L is used to predict a score for each possible tag using a single dense layer. Due to the dependencies between successive tags in our sequence labeling tasks (e.g. using the BIOES labeling scheme, it is not possible for I-PER to follow B-LOC), it is beneficial to model and decode each sentence jointly instead of independently predicting the label for each token. Accordingly, we add another layer with parameters for each label bigram, computing the sentence conditional random field (CRF) loss (Lafferty et al., 2001) using the forward-backward algorithm at training time, and using the Viterbi algorithm to find the most likely tag sequence at test time, similar to Collobert et al. (2011).\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nNew York is located ...\nNeural net\nChar CNN/ RNN\nEmbedding\nToken embedding\nRNNDense\nE-LOCB-LOC CRF\nbi-RNN (R2)\nToken representation\nNew York is located ...\nForward LM\nBackward LM\nh1 LM\nConcat LM embedding\nSequence tagging\nPre-trained bi-LM\nbi-RNN (R1)\nSequence representation\nConcatenation\nToken representation\nNew York is located ...\nToken representation\nh1,1 h2 LMh2,1\nh1,2 h2,2\nFigure 2: Overview of TagLM, our language model augmented sequence tagging architecture. The top level embeddings from a pre-trained bidirectional LM are inserted in a stacked bidirectional RNN sequence tagging model. See text for details."
    }, {
      "heading" : "2.3 Bidirectional LM",
      "text" : "A language model computes the probability of a token sequence (t1, t2, . . . , tN )\np(t1, t2, . . . , tN ) = N∏ k=1 p(tk | t1, t2, . . . , tk−1).\nRecent state of the art neural language models (Józefowicz et al., 2016) use a similar architecture to our baseline sequence tagger where they pass a token representation (either from a CNN over characters or as token embeddings) through multiple layers of LSTMs to embed the history (t1, t2, . . . , tk) into a fixed dimensional vector−→ h LMk . This is the forward LM embedding of the token at position k and is the output of the top LSTM layer in the language model. Finally, the language model predicts the probability of token tk+1 using a softmax layer over words in the vocabulary.\nThe need to capture future context in the LM embeddings suggests it is beneficial to also consider a backward LM in additional to the traditional forward LM. A backward LM predicts the previous token given the future context. Given a sentence with N tokens, it computes p(t1, t2, . . . , tN ) = N∏ k=1 p(tk | tk+1, tk+2, . . . , tN ).\nA backward LM can be implemented in an analogous way to a forward LM and produces the backward LM embedding ←− h LMk , for the sequence (tk, tk+1, . . . , tN ), the output embeddings of the top layer LSTM.\nIn our final system, after pre-training the forward and backward LMs separately, we remove the top layer softmax and concatenate the forward and backward LM embeddings to form bidirectional LM embeddings, i.e., hLMk = [ −→ h LMk ; ←− h LMk ]. Note that in our formulation, the forward and backward LMs are independent, without any shared parameters."
    }, {
      "heading" : "2.4 Combining LM with sequence model",
      "text" : "Our combined system, TagLM, uses the LM embeddings as additional inputs to the sequence tagging model. In particular, we concatenate the LM embeddings hLM with the output from one of the bidirectional RNN layers in the sequence model. In our experiments, we found that introducing the LM embeddings at the output of the first layer performed the best. More formally, we simply replace (2) with\nhk,1 = [ −→ h k,1; ←− h k,1;h LM k ]. (3)\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399"
    }, {
      "heading" : "3 Experiments",
      "text" : "We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task (Sang and Meulder, 2003) and the CoNLL 2000 Chunking task (Sang and Buchholz, 2000). We report the official evaluation metric (micro-averaged F1). In both cases, we use the BIOES labeling scheme for the output tags, following previous work which showed it outperforms other options (e.g., Ratinov and Roth, 2009). Following Chiu and Nichols (2016), we use the Senna word embeddings (Collobert et al., 2011) and pre-processed the text by lowercasing all tokens and replacing all digits with 0.\n3.0.1 CoNLL 2003 NER\nThe CoNLL 2003 NER task consists of newswire from the Reuters RCV1 corpus tagged with four different entity types (PER, LOC, ORG, MISC). It includes standard train, development and test sets. Following previous work (Yang et al., 2017; Chiu and Nichols, 2016) we trained on both the train and development sets after tuning hyperparameters on the development set.\nThe hyperparameters for our baseline model are similar to Yang et al. (2017). We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character encoder. The sequence layer uses two bidirectional GRUs with 300 hidden units each. For regularization, we add 25% dropout to the input of each GRU, but not to the recurrent connections.\n3.0.2 CoNLL 2000 chunking\nThe CoNLL 2000 chunking task uses sections 15- 18 from the Wall Street Journal corpus for training and section 20 for testing. It defines 11 syntactic chunk types (e.g., NP, VP, ADJP) in addition to other. We randomly sampled 1000 sentences from the training set as a held-out development set.\nThe baseline sequence tagger uses 30 dimensional character embeddings and a CNN with 30 filters of width 3 characters followed by a tanh non-linearity for the token character encoder. The sequence layer uses two bidirectional LSTMs with 200 hidden units. Following Ma and Hovy (2016) we added 50% dropout to the character embeddings, the input to each LSTM layer (but not recurrent connections) and to the output of the final LSTM layer.\n3.0.3 Pre-trained language models The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark (Chelba et al., 2014), a publicly available benchmark for large-scale language modeling. The training split has approximately 800 million tokens, about a 4000X increase over the number training tokens in the CoNLL datasets. Józefowicz et al. (2016) explored several model architectures and released their best single model and training recipes. Following Sak et al. (2014), they used linear projection layers at the output of each LSTM layer to reduce the computation time but still maintain a large LSTM state. Their single best model took three weeks to train on 32 GPUs and achieved 30.0 test perplexity. It uses a character CNN with 4096 filters for input, followed by two stacked LSTMs, each with 8192 hidden units and a 1024 dimensional projection layer. We use CNN-BIG-LSTM to refer to this language model in our results.\nIn addition to CNN-BIG-LSTM from Józefowicz et al. (2016),1 we used the same corpus to train two additional language models with fewer parameters: forward LSTM-2048-512 and backward LSTM-2048-512. Both language models use token embeddings as input to a single layer LSTM with 2048 units and a 512 dimension projection layer. We closely followed the procedure outlined in Józefowicz et al. (2016), except we used synchronous parameter updates across four GPUs instead of asynchronous updates across 32 GPUs and ended training after 10 epochs. The test set perplexities for our forward and backward LSTM-2048-512 language models are 47.7 and 47.3, respectively.2\n3.0.4 Training All experiments use the Adam optimizer (Kingma and Ba, 2014) with gradient norms clipped at 5.0. In addition to explicit dropout regularization, we also use early stopping to prevent over-fitting and use the following process to determine when to stop training. We first train with a constant learning rate α = 0.001 on the training data and monitor the development set performance at each epoch. Then, at the epoch with the highest de-\n1https://github.com/tensorflow/models/ tree/master/lm_1b\n2Due to different implementations, the perplexity of the forward LM with similar configurations in Józefowicz et al. (2016) is different (45.0 vs. 47.7).\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nModel F1± std Chiu and Nichols (2016) 90.91± 0.20 Lample et al. (2016) 90.94 Ma and Hovy (2016) 91.37 Our baseline without LM 90.87± 0.13 TagLM 91.93± 0.19\nTable 1: Test set F1 comparison on CoNLL 2003 NER task, using only CoNLL 2003 data and unlabeled text.\nModel F1± std Yang et al. (2017) 94.66 Hashimoto et al. (2016) 95.02 Søgaard and Goldberg (2016) 95.28 Our baseline without LM 95.00± 0.08 TagLM 96.37± 0.05\nTable 2: Test set F1 comparison on CoNLL 2000 Chunking task using only CoNLL 2000 data and unlabeled text.\nvelopment performance, we start a simple learning rate annealing schedule: decrease α an order of magnitude (i.e., divide by ten), train for five epochs, decrease α an order of magnitude again, train for five more epochs and stop.\nFollowing Chiu and Nichols (2016), we train each final model configuration ten times with different random seeds and report the mean and standard deviation F1. It is important to estimate the variance of model performance since the test data sets are relatively small."
    }, {
      "heading" : "3.1 Overall system results",
      "text" : "Tables 1 and 2 compare results from TagLM with previously published state of the art results without additional labeled data or task specific gazetteers. Tables 3 and 4 compare results of TagLM to other systems that include additional labeled data or gazetteers. In both tasks, TagLM establishes a new state of the art using bidirectional LMs (the forward CNN-BIG-LSTM and the backward LSTM-2048-512).\nIn the CoNLL 2003 NER task, our model scores 91.93 mean F1, which is a statistically significant increase over the previous best result of 91.62 ±0.33 from Chiu and Nichols (2016) that used gazetteers (at 95%, two-sided Welch t-test, p = 0.021).\nIn the CoNLL 2000 Chunking task, TagLM\nachieves 96.37 mean F1, exceeding all previously published results without additional labeled data by more then 1% absolute F1. The improvement over the previous best result of 95.77 in Hashimoto et al. (2016) that jointly trains with Penn Treebank (PTB) POS tags is statistically significant at 95% (p < 0.001 assuming standard deviation of 0.1).\nImportantly, the LM embeddings amounts to an average absolute improvement of 1.06 and 1.37 F1 in the NER and Chunking tasks, respectively.\nAdding external resources. Although we do not use external labeled data or gazetteers, we found that TagLM outperforms previous state of the art results in both tasks when external resources (labeled data or task specific gazetteers) are available. Furthermore, Tables 3 and 4 show that, in most cases, the improvements we obtain by adding LM embeddings are larger then the improvements previously obtained by adding other forms of transfer or joint learning. For example, Yang et al. (2017) noted an improvement of only 0.06 F1 in the NER task when transfer learning from both CoNLL 2000 chunks and PTB POS tags and Chiu and Nichols (2016) reported an increase of 0.71 F1 when adding gazetteers to their baseline. In the Chunking task, previous work has reported from 0.28 to 0.75 improvement in F1 when including supervised labels from the PTB POS tags or CoNLL 2003 entities (Yang et al., 2017; Søgaard and Goldberg, 2016; Hashimoto et al., 2016)."
    }, {
      "heading" : "3.2 Analysis",
      "text" : "To elucidate the characteristics of our LM augmented sequence tagger, we ran a number of additional experiments on the CoNLL 2003 NER task.\nHow to use LM embeddings? In this experiment, we concatenate the LM embeddings at different locations in the baseline sequence tagger. In particular, we used the LM embeddings hLMk to:\n• augment the input of the first RNN layer; i.e., xk = [ck;wk;h LM k ],\n• augment the output of the first RNN layer; i.e., hk,1 = [ −→ h k,1; ←− h k,1;h LM k ], 3 and\n• augment the output of the second RNN layer; i.e., hk,2 = [ −→ h k,2; ←− h k,2;h LM k ].\n3This configuration the same as Eq. 3 in §2.4. It was reproduced here for convenience.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nF1 F1 Model External resources Without With ∆ Yang et al. (2017) transfer from CoNLL 2000/PTB-POS 91.2 91.26 +0.06 Chiu and Nichols (2016) with gazetteers 90.91 91.62 +0.71 Collobert et al. (2011) with gazetteers 88.67 89.59 +0.92 Luo et al. (2015) joint with entity linking 89.9 91.2 +1.3 Ours no LM vs TagLM unlabeled data only 90.87 91.93 +1.06\nF1 F1 Model External resources Without With ∆ Yang et al. (2017) transfer from CoNLL 2003/PTB-POS 94.66 95.41 +0.75 Hashimoto et al. (2016) jointly trained with PTB-POS 95.02 95.77 +0.75 Søgaard and Goldberg (2016) jointly trained with PTB-POS 95.28 95.56 +0.28 Ours no LM vs TagLM unlabeled data only 95.00 96.37 +1.37\nTable 4: Improvements in test set F1 in CoNLL 2000 Chunking when including additional labeled data (except the case of TagLM where we do not use additional labeled data).\nUse LM embeddings at F1± std input to the first RNN layer 91.55± 0.21 output of the first RNN layer 91.93± 0.19 output of the second RNN layer 91.72± 0.13\nTable 5: Comparison of CoNLL-2003 test set F1 when the LM embeddings are included at different layers in the baseline tagger.\nTable 5 shows that the second alternative performs best. We speculate that the second RNN layer in the sequence tagging model is able to capture interactions between task specific context as expressed in the first RNN layer and general context as expressed in the LM embeddings in a way that improves overall system performance. These results are consistent with Søgaard and Goldberg (2016) who found that chunking performance was sensitive to the level at which additional POS supervision was added.\nDoes it matter which language model to use? In this experiment, we compare six different configurations of the forward and backward language models (including the baseline model which does not use any language models). The results are reported in Table 6.\nWe find that adding backward LM embeddings consistently outperforms forward-only LM em-\nbeddings, with F1 improvements between 0.22 and 0.27%, even with the relatively small backward LSTM-2048-512 LM.\nLM size is important, and replacing the forward LSTM-2048-512 with CNN-BIG-LSTM (test perplexities of 47.7 to 30.0 on 1B Word Benchmark) improves F1 by 0.26 - 0.31%, about as much as adding backward LM. Accordingly, we hypothesize (but have not tested) that replacing the backward LSTM-2048-512with a backward LM analogous to the CNN-BIG-LSTM would further improve performance.\nTo highlight the importance of including language models trained on a large scale data, we also experimented with training a language model on just the CoNLL 2003 training and development data. Due to the much smaller size of this data set, we decreased the model size to 512 hidden units with a 256 dimension projection and normalized tokens in the same manner as input to the sequence tagging model (lower-cased, with all digits replaced with 0). The test set perplexities for the forward and backward models (measured on the CoNLL 2003 test data) were 106.9 and 104.2, respectively. Including embeddings from these language models decreased performance slightly compared to the baseline system without any LM. This result supports the hypothesis that adding language models help because they learn composi-\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nForward language model Backward language model LM perplexity F1± std Fwd Bwd — — N/A N/A 90.87± 0.13 LSTM-512-256∗ LSTM-512-256∗ 106.9 104.2 90.79± 0.15 LSTM-2048-512 — 47.7 N/A 91.40± 0.18 LSTM-2048-512 LSTM-2048-512 47.7 47.3 91.62± 0.23 CNN-BIG-LSTM — 30.0 N/A 91.66± 0.13 CNN-BIG-LSTM LSTM-2048-512 30.0 47.3 91.93± 0.19\nTable 6: Comparison of CoNLL-2003 test set F1 for different language model combinations. All language models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256∗ which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.\ntion functions (i.e., the RNN parameters in the language model) from much larger data compared to the composition functions in the baseline tagger, which are only learned from labeled data.\nImportance of task specific RNN. To understand the importance of including a task specific sequence RNN we ran an experiment that removed the task specific sequence RNN and used only the LM embeddings with a dense layer and CRF to predict output tags. In this setup, performance was very low, 88.17 F1, well below our baseline. This result confirms that the RNNs in the baseline tagger encode essential information which are not encoded in the LM embeddings. This is unsurprising since the RNNs in the baseline tagger are trained on labeled examples, unlike the RNN in the language model which is only trained on unlabeled examples.\nDataset size. A priori, we expect the addition of LM embeddings to be most beneficial in cases where the task specific annotated datasets are small. To test this hypothesis, we replicated the setup from Yang et al. (2017) that samples 1% of the CoNLL 2003 training set and compared the performance of TagLM to our baseline without LM. In this scenario, test F1 increased 3.35% (from 67.66 to 71.01%) compared to an increase of 1.06% F1 for a similar comparison with the full training dataset. The analogous increases in Yang et al. (2017) are 3.97% for cross-lingual transfer from CoNLL 2002 Spanish NER and 6.28% F1 for transfer from PTB POS tags. However, they found only a 0.06% F1 increase when using the full training data and transferring from both CoNLL 2000 chunks and PTB POS tags. Taken together, this suggests that for very small labeled training sets, transferring from other tasks yields\na large improvement, but this improvement almost disappears when the training data is large. On the other hand, our approach is less dependent on the training set size and significantly improves performance even with larger training sets.\nNumber of parameters. Our TagLM formulation increases the number of parameters in the second RNN layer R2 due to the increase in the input dimension h1 if all other hyperparameters are held constant. To confirm that this did not have a material impact on the results, we ran two additional experiments. In the first, we trained a system without a LM but increased the second RNN layer hidden dimension so that number of parameters was the same as in TagLM. In this case, performance decreased slightly (by 0.15% F1) compared to the baseline model, indicating that solely increasing parameters does not improve performance. In the second experiment, we decreased the hidden dimension of the second RNN layer in TagLM to give it the same number of parameters as the baseline no LM model. In this case, test F1 increased slightly to 92.00 ± 0.11 indicating that the additional parameters in TagLM are slightly hurting performance.4\nDoes the LM transfer across domains? One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles. To test the sensitivity to the LM training domain, we also applied TagLM with a LM trained on news articles to the SemEval 2017 Shared Task 10, ScienceIE.5 Scien-\n4A similar experiment for the Chunking task did not improve F1 so this conclusion is task dependent.\n5https://scienceie.github.io/\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nceIE requires end-to-end joint entity and relationship extraction from scientific publications across three diverse fields (computer science, material sciences, and physics) and defines three broad entity types (Task, Material and Process). For this task, TagLM increased F1 on the development set by 4.12% (from 49.93 to to 54.05%) for entity extraction over our baseline without LM embeddings.6 We conclude that LM embeddings can improve the performance of a sequence tagger even when the data comes from a different domain."
    }, {
      "heading" : "4 Related work",
      "text" : "Unlabeled data. TagLM was inspired by the widespread use of pre-trained word embeddings in supervised sequence tagging models. Besides pre-trained word embeddings, our method is most closely related to Li and McCallum (2005). Instead of using a LM, Li and McCallum (2005) uses a probabilistic generative model to infer contextsensitive latent variables for each token, which are then used as extra features in a supervised CRF tagger (Lafferty et al., 2001). Other semisupervised learning methods for structured prediction problems include co-training (Blum and Mitchell, 1998; Pierce and Cardie, 2001), expectation maximization (Nigam et al., 2000; Mohit and Hwa, 2005), structural learning (Ando and Zhang, 2005) and maximum discriminant functions (Suzuki et al., 2007; Suzuki and Isozaki, 2008). It is easy to combine TagLM with any of the above methods by including LM embeddings as additional features in the discriminative components of the model (except for expectation maximization). A detailed discussion of semisupervised learning methods in NLP can be found in (Søgaard, 2013).\nLM embeddings are related to a class of methods (e.g., Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016) for learning sentence and document encoders from unlabeled data, which can be used for text classification and textual entailment among other tasks.\nNeural language models. LMs have always been a critical component in statistical machine translation systems (Koehn, 2009). Recently, neural LMs (Bengio et al., 2003; Mikolov et al., 2010) have also been integrated in neural machine translation systems (e.g., Kalchbrenner and Blunsom,\n6Test data set performance withheld to preserve anonymity and will be updated before publication.\n2013; Devlin et al., 2014) to score candidate translations. In contrast, TagLM uses neural LMs to encode words in the input sequence.\nUnlike forward LMs, bidirectional LMs have received little prior attention. Most similar to our formulation, Peris and Casacuberta (2015) used a bidirectional neural LM in a statistical machine translation system for instance selection. They tied the input token embeddings and softmax weights in the forward and backward directions, unlike our approach which uses two distinct models without any shared parameters. Frinken et al. (2012) also used a bidirectional n-gram LM for handwriting recognition.\nInterpreting RNN states. Recently, there has been some interest in interpreting the activations of RNNs. Linzen et al. (2016) showed that single LSTM units can learn to predict singular-plural distinctions. Karpathy et al. (2015) visualized character level LSTM states and showed that individual cells capture long-range dependencies such as line lengths, quotes and brackets. Our work complements these studies by showing that LM states are useful for downstream tasks as a way of interpreting what they learn.\nOther sequence tagging models. Current state of the art results in sequence tagging problems are based on bidirectional RNN models. However, many other sequence tagging models have been proposed in the literature for this class of problems (e.g., Lafferty et al., 2001; Collins, 2002). LM embeddings could also be used as additional features in other models, although it is not clear whether the model complexity would be sufficient to effectively make use of them."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, we proposed a simple and general semi-supervised method using pre-trained neural language models to augment token representations in sequence tagging models. Our method significantly outperforms current state of the art models in two popular datasets for NER and Chunking. Our analysis shows that adding a backward LM in addition to traditional forward LMs consistently improves performance. The proposed method is robust even when the LM is trained on unlabeled data from a different domain, or when the baseline model is trained on a large number of labeled examples.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "A highperformance semi-supervised learning method for text chunking",
      "author" : [ "Rie Kubota Ando", "Tong Zhang." ],
      "venue" : "ACL.",
      "citeRegEx" : "Ando and Zhang.,? 2005",
      "shortCiteRegEx" : "Ando and Zhang.",
      "year" : 2005
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin." ],
      "venue" : "Journal of machine learning research 3(Feb):1137–1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Combining labeled and unlabeled data with co-training",
      "author" : [ "Avrim Blum", "Tom Mitchell." ],
      "venue" : "Proceedings of the eleventh annual conference on Computational learning theory. ACM, pages 92–100.",
      "citeRegEx" : "Blum and Mitchell.,? 1998",
      "shortCiteRegEx" : "Blum and Mitchell.",
      "year" : 1998
    }, {
      "title" : "One billion word benchmark for measuring progress in statistical language modeling",
      "author" : [ "Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn." ],
      "venue" : "CoRR abs/1312.3005.",
      "citeRegEx" : "Chelba et al\\.,? 2014",
      "shortCiteRegEx" : "Chelba et al\\.",
      "year" : 2014
    }, {
      "title" : "Named entity recognition with bidirectional lstmcnns",
      "author" : [ "Jason Chiu", "Eric Nichols." ],
      "venue" : "Transactions of the Association for Computational Linguistics 4:357–370. https://transacl.org/ojs/index.php/tacl/article/view/792.",
      "citeRegEx" : "Chiu and Nichols.,? 2016",
      "shortCiteRegEx" : "Chiu and Nichols.",
      "year" : 2016
    }, {
      "title" : "On the properties of neural machine translation: Encoder-decoder approaches",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio." ],
      "venue" : "SSST@EMNLP.",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms",
      "author" : [ "Michael Collins." ],
      "venue" : "Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10. Associa-",
      "citeRegEx" : "Collins.,? 2002",
      "shortCiteRegEx" : "Collins.",
      "year" : 2002
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa." ],
      "venue" : "CoRR abs/1103.0398.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Fast and robust neural network joint models for statistical machine translation",
      "author" : [ "Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard M Schwartz", "John Makhoul." ],
      "venue" : "ACL (1). Citeseer, pages 1370–1380.",
      "citeRegEx" : "Devlin et al\\.,? 2014",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2014
    }, {
      "title" : "Bidirectional language model for handwriting recognition",
      "author" : [ "Volkmar Frinken", "Alicia Fornés", "Josep Lladós", "Jean-Marc Ogier." ],
      "venue" : "Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syn-",
      "citeRegEx" : "Frinken et al\\.,? 2012",
      "shortCiteRegEx" : "Frinken et al\\.",
      "year" : 2012
    }, {
      "title" : "A joint many-task model: Growing a neural network for multiple nlp tasks",
      "author" : [ "Kazuma Hashimoto", "Caiming Xiong", "Yoshimasa Tsuruoka", "Richard Socher." ],
      "venue" : "CoRR abs/1611.01587.",
      "citeRegEx" : "Hashimoto et al\\.,? 2016",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning distributed representations of sentences from unlabelled data",
      "author" : [ "Felix Hill", "Kyunghyun Cho", "Anna Korhonen." ],
      "venue" : "HLT-NAACL.",
      "citeRegEx" : "Hill et al\\.,? 2016",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation 9:1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Exploring the limits of language modeling",
      "author" : [ "Rafal Józefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu." ],
      "venue" : "CoRR abs/1602.02410.",
      "citeRegEx" : "Józefowicz et al\\.,? 2016",
      "shortCiteRegEx" : "Józefowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Visualizing and understanding recurrent networks",
      "author" : [ "Andrej Karpathy", "Justin Johnson", "Li Fei-Fei." ],
      "venue" : "CoRR abs/1506.02078.",
      "citeRegEx" : "Karpathy et al\\.,? 2015",
      "shortCiteRegEx" : "Karpathy et al\\.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Jamie Ryan Kiros", "Yukun Zhu", "Ruslan Salakhutdinov", "Richard S. Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Statistical machine translation",
      "author" : [ "Philipp Koehn." ],
      "venue" : "Cambridge University Press.",
      "citeRegEx" : "Koehn.,? 2009",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2009
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando Pereira." ],
      "venue" : "ICML.",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Associa-",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Distributed representations of sentences and documents",
      "author" : [ "Quoc V. Le", "Tomas Mikolov." ],
      "venue" : "ICML.",
      "citeRegEx" : "Le and Mikolov.,? 2014",
      "shortCiteRegEx" : "Le and Mikolov.",
      "year" : 2014
    }, {
      "title" : "Semi-supervised sequence modeling with syntactic topic models",
      "author" : [ "Wei Li", "Andrew McCallum." ],
      "venue" : "AAAI.",
      "citeRegEx" : "Li and McCallum.,? 2005",
      "shortCiteRegEx" : "Li and McCallum.",
      "year" : 2005
    }, {
      "title" : "Assessing the ability of lstms to learn syntaxsensitive dependencies",
      "author" : [ "Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg." ],
      "venue" : "CoRR abs/1611.01368.",
      "citeRegEx" : "Linzen et al\\.,? 2016",
      "shortCiteRegEx" : "Linzen et al\\.",
      "year" : 2016
    }, {
      "title" : "Joint entity recognition and disambiguation",
      "author" : [ "Gang Luo", "Xiaojiang Huang", "Chin-Yew Lin", "Zaiqing Nie." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational",
      "citeRegEx" : "Luo et al\\.,? 2015",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2015
    }, {
      "title" : "End-to-end sequence labeling via bi-directional lstm-cnns-crf",
      "author" : [ "Xuezhe Ma", "Eduard H. Hovy." ],
      "venue" : "CoRR abs/1603.01354.",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur." ],
      "venue" : "Interspeech.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Syntax-based semi-supervised named entity tagging",
      "author" : [ "Behrang Mohit", "Rebecca Hwa." ],
      "venue" : "Proceedings of the ACL 2005 on Interactive poster and demonstration sessions. Association for Computational Linguistics, pages 57–60.",
      "citeRegEx" : "Mohit and Hwa.,? 2005",
      "shortCiteRegEx" : "Mohit and Hwa.",
      "year" : 2005
    }, {
      "title" : "Text classification from labeled and unlabeled documents using em",
      "author" : [ "Kamal Nigam", "Andrew Kachites McCallum", "Sebastian Thrun", "Tom Mitchell." ],
      "venue" : "Machine learning 39(2-3):103–134.",
      "citeRegEx" : "Nigam et al\\.,? 2000",
      "shortCiteRegEx" : "Nigam et al\\.",
      "year" : 2000
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "A bidirectional recurrent neural language model for machine translation",
      "author" : [ "Álvaro Peris", "Francisco Casacuberta." ],
      "venue" : "Procesamiento del Lenguaje Natural 55:109–116.",
      "citeRegEx" : "Peris and Casacuberta.,? 2015",
      "shortCiteRegEx" : "Peris and Casacuberta.",
      "year" : 2015
    }, {
      "title" : "Limitations of co-training for natural language learning from large datasets",
      "author" : [ "David Pierce", "Claire Cardie." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Pierce and Cardie.,? 2001",
      "shortCiteRegEx" : "Pierce and Cardie.",
      "year" : 2001
    }, {
      "title" : "Design challenges and misconceptions in named entity recognition",
      "author" : [ "Lev-Arie Ratinov", "Dan Roth." ],
      "venue" : "CoNLL.",
      "citeRegEx" : "Ratinov and Roth.,? 2009",
      "shortCiteRegEx" : "Ratinov and Roth.",
      "year" : 2009
    }, {
      "title" : "Long short-term memory recurrent neural network architectures for large scale acoustic modeling",
      "author" : [ "Hasim Sak", "Andrew W. Senior", "Franoise Beaufays." ],
      "venue" : "INTERSPEECH.",
      "citeRegEx" : "Sak et al\\.,? 2014",
      "shortCiteRegEx" : "Sak et al\\.",
      "year" : 2014
    }, {
      "title" : "Introduction to the conll-2000 shared task chunking",
      "author" : [ "Erik F. Tjong Kim Sang", "Sabine Buchholz." ],
      "venue" : "CoNLL/LLL.",
      "citeRegEx" : "Sang and Buchholz.,? 2000",
      "shortCiteRegEx" : "Sang and Buchholz.",
      "year" : 2000
    }, {
      "title" : "Introduction to the conll-2003 shared task: Language-independent named entity recognition",
      "author" : [ "Erik F. Tjong Kim Sang", "Fien De Meulder." ],
      "venue" : "CoNLL.",
      "citeRegEx" : "Sang and Meulder.,? 2003",
      "shortCiteRegEx" : "Sang and Meulder.",
      "year" : 2003
    }, {
      "title" : "Semi-supervised learning and domain adaptation in natural language processing",
      "author" : [ "Anders Søgaard." ],
      "venue" : "Synthesis Lectures on Human Language Technologies 6(2):1–103.",
      "citeRegEx" : "Søgaard.,? 2013",
      "shortCiteRegEx" : "Søgaard.",
      "year" : 2013
    }, {
      "title" : "Deep multi-task learning with low level tasks supervised at lower layers",
      "author" : [ "Anders Søgaard", "Yoav Goldberg." ],
      "venue" : "ACL.",
      "citeRegEx" : "Søgaard and Goldberg.,? 2016",
      "shortCiteRegEx" : "Søgaard and Goldberg.",
      "year" : 2016
    }, {
      "title" : "Semi-supervised structured output learning based on a hybrid generative and discriminative approach",
      "author" : [ "Jun Suzuki", "Akinori Fujino", "Hideki Isozaki." ],
      "venue" : "EMNLP-CoNLL. pages 791–800.",
      "citeRegEx" : "Suzuki et al\\.,? 2007",
      "shortCiteRegEx" : "Suzuki et al\\.",
      "year" : 2007
    }, {
      "title" : "Semi-supervised sequential labeling and segmentation using gigaword scale unlabeled data",
      "author" : [ "Jun Suzuki", "Hideki Isozaki." ],
      "venue" : "ACL.",
      "citeRegEx" : "Suzuki and Isozaki.,? 2008",
      "shortCiteRegEx" : "Suzuki and Isozaki.",
      "year" : 2008
    }, {
      "title" : "Transfer learning for sequence tagging with hierarchical recurrent networks",
      "author" : [ "Zhilin Yang", "Ruslan Salakhutdinov", "William W. Cohen." ],
      "venue" : "ICLR accepted.",
      "citeRegEx" : "Yang et al\\.,? 2017",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 27,
      "context" : "Many prior studies have shown that they capture useful semantic and syntactic information (Mikolov et al., 2013; Pennington et al., 2014) and including them in NLP systems has been shown to be enormously helpful for a variety of downstream tasks (Collobert et al.",
      "startOffset" : 90,
      "endOffset" : 137
    }, {
      "referenceID" : 30,
      "context" : "Many prior studies have shown that they capture useful semantic and syntactic information (Mikolov et al., 2013; Pennington et al., 2014) and including them in NLP systems has been shown to be enormously helpful for a variety of downstream tasks (Collobert et al.",
      "startOffset" : 90,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : ", 2014) and including them in NLP systems has been shown to be enormously helpful for a variety of downstream tasks (Collobert et al., 2011).",
      "startOffset" : 116,
      "endOffset" : 140
    }, {
      "referenceID" : 41,
      "context" : "Accordingly, current state of the art sequence tagging models typically include a bidirectional recurrent neural network (RNN) that encodes token sequences into a context sensitive representation before making token specific predictions (Yang et al., 2017; Ma and Hovy, 2016; Lample et al., 2016; Hashimoto et al., 2016).",
      "startOffset" : 237,
      "endOffset" : 320
    }, {
      "referenceID" : 25,
      "context" : "Accordingly, current state of the art sequence tagging models typically include a bidirectional recurrent neural network (RNN) that encodes token sequences into a context sensitive representation before making token specific predictions (Yang et al., 2017; Ma and Hovy, 2016; Lample et al., 2016; Hashimoto et al., 2016).",
      "startOffset" : 237,
      "endOffset" : 320
    }, {
      "referenceID" : 20,
      "context" : "Accordingly, current state of the art sequence tagging models typically include a bidirectional recurrent neural network (RNN) that encodes token sequences into a context sensitive representation before making token specific predictions (Yang et al., 2017; Ma and Hovy, 2016; Lample et al., 2016; Hashimoto et al., 2016).",
      "startOffset" : 237,
      "endOffset" : 320
    }, {
      "referenceID" : 10,
      "context" : "Accordingly, current state of the art sequence tagging models typically include a bidirectional recurrent neural network (RNN) that encodes token sequences into a context sensitive representation before making token specific predictions (Yang et al., 2017; Ma and Hovy, 2016; Lample et al., 2016; Hashimoto et al., 2016).",
      "startOffset" : 237,
      "endOffset" : 320
    }, {
      "referenceID" : 41,
      "context" : "Previous work has explored methods for jointly learning the bidirectional RNN with supplemental labeled data from other tasks (e.g., Søgaard and Goldberg, 2016; Yang et al., 2017).",
      "startOffset" : 126,
      "endOffset" : 179
    }, {
      "referenceID" : 25,
      "context" : "2 Baseline sequence tagging model Our baseline sequence tagging model is a hierarchical neural tagging model, closely following a number of recent studies (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2017; Chiu and Nichols, 2016) (left side of Figure 2).",
      "startOffset" : 155,
      "endOffset" : 238
    }, {
      "referenceID" : 20,
      "context" : "2 Baseline sequence tagging model Our baseline sequence tagging model is a hierarchical neural tagging model, closely following a number of recent studies (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2017; Chiu and Nichols, 2016) (left side of Figure 2).",
      "startOffset" : 155,
      "endOffset" : 238
    }, {
      "referenceID" : 41,
      "context" : "2 Baseline sequence tagging model Our baseline sequence tagging model is a hierarchical neural tagging model, closely following a number of recent studies (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2017; Chiu and Nichols, 2016) (left side of Figure 2).",
      "startOffset" : 155,
      "endOffset" : 238
    }, {
      "referenceID" : 4,
      "context" : "2 Baseline sequence tagging model Our baseline sequence tagging model is a hierarchical neural tagging model, closely following a number of recent studies (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2017; Chiu and Nichols, 2016) (left side of Figure 2).",
      "startOffset" : 155,
      "endOffset" : 238
    }, {
      "referenceID" : 25,
      "context" : "The character representation ck captures morphological information and is either a convolutional neural network (CNN) (Ma and Hovy, 2016; Chiu and Nichols, 2016) or RNN (Yang et al.",
      "startOffset" : 118,
      "endOffset" : 161
    }, {
      "referenceID" : 4,
      "context" : "The character representation ck captures morphological information and is either a convolutional neural network (CNN) (Ma and Hovy, 2016; Chiu and Nichols, 2016) or RNN (Yang et al.",
      "startOffset" : 118,
      "endOffset" : 161
    }, {
      "referenceID" : 41,
      "context" : "The character representation ck captures morphological information and is either a convolutional neural network (CNN) (Ma and Hovy, 2016; Chiu and Nichols, 2016) or RNN (Yang et al., 2017; Lample et al., 2016).",
      "startOffset" : 169,
      "endOffset" : 209
    }, {
      "referenceID" : 20,
      "context" : "The character representation ck captures morphological information and is either a convolutional neural network (CNN) (Ma and Hovy, 2016; Chiu and Nichols, 2016) or RNN (Yang et al., 2017; Lample et al., 2016).",
      "startOffset" : 169,
      "endOffset" : 209
    }, {
      "referenceID" : 7,
      "context" : "The token embeddings, wk, are obtained as a lookup E(·, θw), initialized using pre-trained word embeddings, and fine tuned during training (Collobert et al., 2011).",
      "startOffset" : 139,
      "endOffset" : 163
    }, {
      "referenceID" : 5,
      "context" : "In this paper, we use L = 2 layers of RNNs in all experiments and parameterize Ri as either Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) depending on the task.",
      "startOffset" : 120,
      "endOffset" : 138
    }, {
      "referenceID" : 12,
      "context" : ", 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) depending on the task.",
      "startOffset" : 47,
      "endOffset" : 81
    }, {
      "referenceID" : 19,
      "context" : "Accordingly, we add another layer with parameters for each label bigram, computing the sentence conditional random field (CRF) loss (Lafferty et al., 2001) using the forward-backward algorithm at training time, and using the Viterbi algorithm to find the most likely tag sequence at test time, similar to Collobert et al.",
      "startOffset" : 132,
      "endOffset" : 155
    }, {
      "referenceID" : 7,
      "context" : ", 2001) using the forward-backward algorithm at training time, and using the Viterbi algorithm to find the most likely tag sequence at test time, similar to Collobert et al. (2011).",
      "startOffset" : 157,
      "endOffset" : 181
    }, {
      "referenceID" : 13,
      "context" : "Recent state of the art neural language models (Józefowicz et al., 2016) use a similar architecture to our baseline sequence tagger where they pass a token representation (either from a CNN over characters or as token embeddings) through multiple layers of LSTMs to embed the history (t1, t2, .",
      "startOffset" : 47,
      "endOffset" : 72
    }, {
      "referenceID" : 36,
      "context" : "We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task (Sang and Meulder, 2003) and the CoNLL 2000 Chunking task (Sang and Buchholz, 2000).",
      "startOffset" : 97,
      "endOffset" : 121
    }, {
      "referenceID" : 35,
      "context" : "We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task (Sang and Meulder, 2003) and the CoNLL 2000 Chunking task (Sang and Buchholz, 2000).",
      "startOffset" : 155,
      "endOffset" : 180
    }, {
      "referenceID" : 7,
      "context" : "Following Chiu and Nichols (2016), we use the Senna word embeddings (Collobert et al., 2011) and pre-processed the text by lowercasing all tokens and replacing all digits with 0.",
      "startOffset" : 68,
      "endOffset" : 92
    }, {
      "referenceID" : 4,
      "context" : "Following Chiu and Nichols (2016), we use the Senna word embeddings (Collobert et al.",
      "startOffset" : 10,
      "endOffset" : 34
    }, {
      "referenceID" : 41,
      "context" : "Following previous work (Yang et al., 2017; Chiu and Nichols, 2016) we trained on both the train and development sets after tuning hyperparameters on the development set.",
      "startOffset" : 24,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : "Following previous work (Yang et al., 2017; Chiu and Nichols, 2016) we trained on both the train and development sets after tuning hyperparameters on the development set.",
      "startOffset" : 24,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : ", 2017; Chiu and Nichols, 2016) we trained on both the train and development sets after tuning hyperparameters on the development set. The hyperparameters for our baseline model are similar to Yang et al. (2017). We use two bidirectional GRUs with 80 hidden units and 25 dimensional character embeddings for the token character encoder.",
      "startOffset" : 8,
      "endOffset" : 212
    }, {
      "referenceID" : 3,
      "context" : "3 Pre-trained language models The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark (Chelba et al., 2014), a publicly available benchmark for large-scale language modeling.",
      "startOffset" : 120,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "Following Ma and Hovy (2016) we added 50% dropout to the character embeddings, the input to each LSTM layer (but not recurrent connections) and to the output of the final LSTM layer.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 3,
      "context" : "3 Pre-trained language models The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark (Chelba et al., 2014), a publicly available benchmark for large-scale language modeling. The training split has approximately 800 million tokens, about a 4000X increase over the number training tokens in the CoNLL datasets. Józefowicz et al. (2016) explored several model architectures and released their best single model and training recipes.",
      "startOffset" : 121,
      "endOffset" : 369
    }, {
      "referenceID" : 3,
      "context" : "3 Pre-trained language models The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark (Chelba et al., 2014), a publicly available benchmark for large-scale language modeling. The training split has approximately 800 million tokens, about a 4000X increase over the number training tokens in the CoNLL datasets. Józefowicz et al. (2016) explored several model architectures and released their best single model and training recipes. Following Sak et al. (2014), they used linear projection layers at the output of each LSTM layer to reduce the computation time but still maintain a large LSTM state.",
      "startOffset" : 121,
      "endOffset" : 493
    }, {
      "referenceID" : 3,
      "context" : "3 Pre-trained language models The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark (Chelba et al., 2014), a publicly available benchmark for large-scale language modeling. The training split has approximately 800 million tokens, about a 4000X increase over the number training tokens in the CoNLL datasets. Józefowicz et al. (2016) explored several model architectures and released their best single model and training recipes. Following Sak et al. (2014), they used linear projection layers at the output of each LSTM layer to reduce the computation time but still maintain a large LSTM state. Their single best model took three weeks to train on 32 GPUs and achieved 30.0 test perplexity. It uses a character CNN with 4096 filters for input, followed by two stacked LSTMs, each with 8192 hidden units and a 1024 dimensional projection layer. We use CNN-BIG-LSTM to refer to this language model in our results. In addition to CNN-BIG-LSTM from Józefowicz et al. (2016),1 we used the same corpus to train two additional language models with fewer parameters: forward LSTM-2048-512 and backward LSTM-2048-512.",
      "startOffset" : 121,
      "endOffset" : 1007
    }, {
      "referenceID" : 3,
      "context" : "3 Pre-trained language models The primary bidirectional LMs we used in this study were trained on the 1B Word Benchmark (Chelba et al., 2014), a publicly available benchmark for large-scale language modeling. The training split has approximately 800 million tokens, about a 4000X increase over the number training tokens in the CoNLL datasets. Józefowicz et al. (2016) explored several model architectures and released their best single model and training recipes. Following Sak et al. (2014), they used linear projection layers at the output of each LSTM layer to reduce the computation time but still maintain a large LSTM state. Their single best model took three weeks to train on 32 GPUs and achieved 30.0 test perplexity. It uses a character CNN with 4096 filters for input, followed by two stacked LSTMs, each with 8192 hidden units and a 1024 dimensional projection layer. We use CNN-BIG-LSTM to refer to this language model in our results. In addition to CNN-BIG-LSTM from Józefowicz et al. (2016),1 we used the same corpus to train two additional language models with fewer parameters: forward LSTM-2048-512 and backward LSTM-2048-512. Both language models use token embeddings as input to a single layer LSTM with 2048 units and a 512 dimension projection layer. We closely followed the procedure outlined in Józefowicz et al. (2016), except we used synchronous parameter updates across four GPUs instead of asynchronous updates across 32 GPUs and ended training after 10 epochs.",
      "startOffset" : 121,
      "endOffset" : 1345
    }, {
      "referenceID" : 16,
      "context" : "4 Training All experiments use the Adam optimizer (Kingma and Ba, 2014) with gradient norms clipped at 5.",
      "startOffset" : 50,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "com/tensorflow/models/ tree/master/lm_1b Due to different implementations, the perplexity of the forward LM with similar configurations in Józefowicz et al. (2016) is different (45.",
      "startOffset" : 139,
      "endOffset" : 164
    }, {
      "referenceID" : 4,
      "context" : "Model F1± std Chiu and Nichols (2016) 90.",
      "startOffset" : 14,
      "endOffset" : 38
    }, {
      "referenceID" : 4,
      "context" : "Model F1± std Chiu and Nichols (2016) 90.91± 0.20 Lample et al. (2016) 90.",
      "startOffset" : 14,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "Model F1± std Chiu and Nichols (2016) 90.91± 0.20 Lample et al. (2016) 90.94 Ma and Hovy (2016) 91.",
      "startOffset" : 14,
      "endOffset" : 96
    }, {
      "referenceID" : 38,
      "context" : "Model F1± std Yang et al. (2017) 94.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "66 Hashimoto et al. (2016) 95.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 10,
      "context" : "66 Hashimoto et al. (2016) 95.02 Søgaard and Goldberg (2016) 95.",
      "startOffset" : 3,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "Following Chiu and Nichols (2016), we train each final model configuration ten times with different random seeds and report the mean and standard deviation F1.",
      "startOffset" : 10,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "33 from Chiu and Nichols (2016) that used gazetteers (at 95%, two-sided Welch t-test, p = 0.",
      "startOffset" : 8,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "33 from Chiu and Nichols (2016) that used gazetteers (at 95%, two-sided Welch t-test, p = 0.021). In the CoNLL 2000 Chunking task, TagLM achieves 96.37 mean F1, exceeding all previously published results without additional labeled data by more then 1% absolute F1. The improvement over the previous best result of 95.77 in Hashimoto et al. (2016) that jointly trains with Penn Treebank (PTB) POS tags is statistically significant at 95% (p < 0.",
      "startOffset" : 8,
      "endOffset" : 347
    }, {
      "referenceID" : 41,
      "context" : "75 improvement in F1 when including supervised labels from the PTB POS tags or CoNLL 2003 entities (Yang et al., 2017; Søgaard and Goldberg, 2016; Hashimoto et al., 2016).",
      "startOffset" : 99,
      "endOffset" : 170
    }, {
      "referenceID" : 38,
      "context" : "75 improvement in F1 when including supervised labels from the PTB POS tags or CoNLL 2003 entities (Yang et al., 2017; Søgaard and Goldberg, 2016; Hashimoto et al., 2016).",
      "startOffset" : 99,
      "endOffset" : 170
    }, {
      "referenceID" : 10,
      "context" : "75 improvement in F1 when including supervised labels from the PTB POS tags or CoNLL 2003 entities (Yang et al., 2017; Søgaard and Goldberg, 2016; Hashimoto et al., 2016).",
      "startOffset" : 99,
      "endOffset" : 170
    }, {
      "referenceID" : 37,
      "context" : "For example, Yang et al. (2017) noted an improvement of only 0.",
      "startOffset" : 13,
      "endOffset" : 32
    }, {
      "referenceID" : 4,
      "context" : "06 F1 in the NER task when transfer learning from both CoNLL 2000 chunks and PTB POS tags and Chiu and Nichols (2016) reported an increase of 0.",
      "startOffset" : 94,
      "endOffset" : 118
    }, {
      "referenceID" : 38,
      "context" : "F1 F1 Model External resources Without With ∆ Yang et al. (2017) transfer from CoNLL 2000/PTB-POS 91.",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "06 Chiu and Nichols (2016) with gazetteers 90.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : "06 Chiu and Nichols (2016) with gazetteers 90.91 91.62 +0.71 Collobert et al. (2011) with gazetteers 88.",
      "startOffset" : 3,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "06 Chiu and Nichols (2016) with gazetteers 90.91 91.62 +0.71 Collobert et al. (2011) with gazetteers 88.67 89.59 +0.92 Luo et al. (2015) joint with entity linking 89.",
      "startOffset" : 3,
      "endOffset" : 137
    }, {
      "referenceID" : 38,
      "context" : "F1 F1 Model External resources Without With ∆ Yang et al. (2017) transfer from CoNLL 2003/PTB-POS 94.",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 10,
      "context" : "75 Hashimoto et al. (2016) jointly trained with PTB-POS 95.",
      "startOffset" : 3,
      "endOffset" : 27
    }, {
      "referenceID" : 10,
      "context" : "75 Hashimoto et al. (2016) jointly trained with PTB-POS 95.02 95.77 +0.75 Søgaard and Goldberg (2016) jointly trained with PTB-POS 95.",
      "startOffset" : 3,
      "endOffset" : 102
    }, {
      "referenceID" : 37,
      "context" : "These results are consistent with Søgaard and Goldberg (2016) who found that chunking performance was sensitive to the level at which additional POS supervision was added.",
      "startOffset" : 34,
      "endOffset" : 62
    }, {
      "referenceID" : 41,
      "context" : "To test this hypothesis, we replicated the setup from Yang et al. (2017) that samples 1% of the CoNLL 2003 training set and compared the performance of TagLM to our baseline without LM.",
      "startOffset" : 54,
      "endOffset" : 73
    }, {
      "referenceID" : 41,
      "context" : "To test this hypothesis, we replicated the setup from Yang et al. (2017) that samples 1% of the CoNLL 2003 training set and compared the performance of TagLM to our baseline without LM. In this scenario, test F1 increased 3.35% (from 67.66 to 71.01%) compared to an increase of 1.06% F1 for a similar comparison with the full training dataset. The analogous increases in Yang et al. (2017) are 3.",
      "startOffset" : 54,
      "endOffset" : 390
    }, {
      "referenceID" : 19,
      "context" : "Instead of using a LM, Li and McCallum (2005) uses a probabilistic generative model to infer contextsensitive latent variables for each token, which are then used as extra features in a supervised CRF tagger (Lafferty et al., 2001).",
      "startOffset" : 208,
      "endOffset" : 231
    }, {
      "referenceID" : 2,
      "context" : "Other semisupervised learning methods for structured prediction problems include co-training (Blum and Mitchell, 1998; Pierce and Cardie, 2001), expectation maximization (Nigam et al.",
      "startOffset" : 93,
      "endOffset" : 143
    }, {
      "referenceID" : 32,
      "context" : "Other semisupervised learning methods for structured prediction problems include co-training (Blum and Mitchell, 1998; Pierce and Cardie, 2001), expectation maximization (Nigam et al.",
      "startOffset" : 93,
      "endOffset" : 143
    }, {
      "referenceID" : 29,
      "context" : "Other semisupervised learning methods for structured prediction problems include co-training (Blum and Mitchell, 1998; Pierce and Cardie, 2001), expectation maximization (Nigam et al., 2000; Mohit and Hwa, 2005), structural learning (Ando and Zhang, 2005) and maximum discriminant functions (Suzuki et al.",
      "startOffset" : 170,
      "endOffset" : 211
    }, {
      "referenceID" : 28,
      "context" : "Other semisupervised learning methods for structured prediction problems include co-training (Blum and Mitchell, 1998; Pierce and Cardie, 2001), expectation maximization (Nigam et al., 2000; Mohit and Hwa, 2005), structural learning (Ando and Zhang, 2005) and maximum discriminant functions (Suzuki et al.",
      "startOffset" : 170,
      "endOffset" : 211
    }, {
      "referenceID" : 0,
      "context" : ", 2000; Mohit and Hwa, 2005), structural learning (Ando and Zhang, 2005) and maximum discriminant functions (Suzuki et al.",
      "startOffset" : 50,
      "endOffset" : 72
    }, {
      "referenceID" : 39,
      "context" : ", 2000; Mohit and Hwa, 2005), structural learning (Ando and Zhang, 2005) and maximum discriminant functions (Suzuki et al., 2007; Suzuki and Isozaki, 2008).",
      "startOffset" : 108,
      "endOffset" : 155
    }, {
      "referenceID" : 40,
      "context" : ", 2000; Mohit and Hwa, 2005), structural learning (Ando and Zhang, 2005) and maximum discriminant functions (Suzuki et al., 2007; Suzuki and Isozaki, 2008).",
      "startOffset" : 108,
      "endOffset" : 155
    }, {
      "referenceID" : 37,
      "context" : "A detailed discussion of semisupervised learning methods in NLP can be found in (Søgaard, 2013).",
      "startOffset" : 80,
      "endOffset" : 95
    }, {
      "referenceID" : 17,
      "context" : "LM embeddings are related to a class of methods (e.g., Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016) for learning sentence and document encoders from unlabeled data, which can be used for text classification and textual entailment among other tasks.",
      "startOffset" : 48,
      "endOffset" : 115
    }, {
      "referenceID" : 11,
      "context" : "LM embeddings are related to a class of methods (e.g., Le and Mikolov, 2014; Kiros et al., 2015; Hill et al., 2016) for learning sentence and document encoders from unlabeled data, which can be used for text classification and textual entailment among other tasks.",
      "startOffset" : 48,
      "endOffset" : 115
    }, {
      "referenceID" : 16,
      "context" : "Besides pre-trained word embeddings, our method is most closely related to Li and McCallum (2005). Instead of using a LM, Li and McCallum (2005) uses a probabilistic generative model to infer contextsensitive latent variables for each token, which are then used as extra features in a supervised CRF tagger (Lafferty et al.",
      "startOffset" : 75,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : "Besides pre-trained word embeddings, our method is most closely related to Li and McCallum (2005). Instead of using a LM, Li and McCallum (2005) uses a probabilistic generative model to infer contextsensitive latent variables for each token, which are then used as extra features in a supervised CRF tagger (Lafferty et al.",
      "startOffset" : 75,
      "endOffset" : 145
    }, {
      "referenceID" : 18,
      "context" : "LMs have always been a critical component in statistical machine translation systems (Koehn, 2009).",
      "startOffset" : 85,
      "endOffset" : 98
    }, {
      "referenceID" : 1,
      "context" : "Recently, neural LMs (Bengio et al., 2003; Mikolov et al., 2010) have also been integrated in neural machine translation systems (e.",
      "startOffset" : 21,
      "endOffset" : 64
    }, {
      "referenceID" : 26,
      "context" : "Recently, neural LMs (Bengio et al., 2003; Mikolov et al., 2010) have also been integrated in neural machine translation systems (e.",
      "startOffset" : 21,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : ", 2010) have also been integrated in neural machine translation systems (e.g., Kalchbrenner and Blunsom, Test data set performance withheld to preserve anonymity and will be updated before publication. 2013; Devlin et al., 2014) to score candidate translations.",
      "startOffset" : 72,
      "endOffset" : 228
    }, {
      "referenceID" : 1,
      "context" : "Recently, neural LMs (Bengio et al., 2003; Mikolov et al., 2010) have also been integrated in neural machine translation systems (e.g., Kalchbrenner and Blunsom, Test data set performance withheld to preserve anonymity and will be updated before publication. 2013; Devlin et al., 2014) to score candidate translations. In contrast, TagLM uses neural LMs to encode words in the input sequence. Unlike forward LMs, bidirectional LMs have received little prior attention. Most similar to our formulation, Peris and Casacuberta (2015) used a bidirectional neural LM in a statistical machine translation system for instance selection.",
      "startOffset" : 22,
      "endOffset" : 531
    }, {
      "referenceID" : 1,
      "context" : "Recently, neural LMs (Bengio et al., 2003; Mikolov et al., 2010) have also been integrated in neural machine translation systems (e.g., Kalchbrenner and Blunsom, Test data set performance withheld to preserve anonymity and will be updated before publication. 2013; Devlin et al., 2014) to score candidate translations. In contrast, TagLM uses neural LMs to encode words in the input sequence. Unlike forward LMs, bidirectional LMs have received little prior attention. Most similar to our formulation, Peris and Casacuberta (2015) used a bidirectional neural LM in a statistical machine translation system for instance selection. They tied the input token embeddings and softmax weights in the forward and backward directions, unlike our approach which uses two distinct models without any shared parameters. Frinken et al. (2012) also used a bidirectional n-gram LM for handwriting recognition.",
      "startOffset" : 22,
      "endOffset" : 831
    }, {
      "referenceID" : 22,
      "context" : "Linzen et al. (2016) showed that single LSTM units can learn to predict singular-plural distinctions.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 15,
      "context" : "Karpathy et al. (2015) visualized character level LSTM states and showed that individual cells capture long-range dependencies such as line lengths, quotes and brackets.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "However, many other sequence tagging models have been proposed in the literature for this class of problems (e.g., Lafferty et al., 2001; Collins, 2002).",
      "startOffset" : 108,
      "endOffset" : 152
    } ],
    "year" : 2017,
    "abstractText" : "Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.",
    "creator" : "TeX"
  }
}