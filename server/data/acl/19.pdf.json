{
  "name" : "19.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generating and Exploiting Large-scale Pseudo Training Data for Zero Pronoun Resolution",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013). However, a major obstacle for training the supervised learning models for ZP resolution is the lack of annotated data. An important step is to organize the shared task on anaphora and coreference resolution, such as the ACE evaluations, SemEval-2010 shared task on Coreference Resolution in Multiple Languages (Marta Recasens, 2010) and CoNLL2012 shared task on Modeling Multilingual Unre-\nstricted Coreference in OntoNotes (Sameer Pradhan, 2012). Following these shared tasks, the annotated evaluation data can be released for the following researches. Despite the success and contributions of these shared tasks, it still faces the challenge of spending manpower on labeling the extended data for better training performance and domain adaptation.\nTo address the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Inspired by data generation on cloze-style reading comprehension, we can treat the zero pronoun resolution task as a special case of reading comprehension problem. So we can adopt similar data generation methods of reading comprehension to the zero pronoun resolution task. For the noun or pronoun in the document, which has the frequency equal to or greater than 2, we randomly choose one position where the noun or pronoun is located on, and replace it with a specific symbol 〈blank〉. Let query Q and answer A denote the sentence that contains a 〈blank〉, and the noun or pronoun which is replaced by the 〈blank〉, respectively. Thus, a pseudo training sample can be represented as a triple:\n〈D,Q,A〉 (1)\nFor the zero pronoun resolution task, a 〈blank〉 represents a zero pronoun (ZP) in query Q, and A indicates the corresponding antecedent of the ZP. In this way, tremendous pseudo training samples can be generated from the various documents, such as news corpus.\nTowards the shortcomings of the previous approaches that are based on feature engineering, we propose a neural network architecture, which is an attention-based neural network model, for zero pronoun resolution. Also we propose a two-step\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ntraining method, which benefit from both largescale pseudo training data and task-specific data, showing promising performance.\nTo sum up, the contributions of this paper are listed as follows.\n• To our knowledge, this is the first time that utilizing reading comprehension neural network model into zero pronoun resolution task.\n• We propose a two-step training approach, namely pre-training-then-adaptation, which benefits from both the large-scale automatically generated pseudo training data and taskspecific data.\n• Towards the shortcomings of the feature engineering approaches, we first propose an attention-based neural network model for zero pronoun resolution."
    }, {
      "heading" : "2 The Proposed Approach",
      "text" : "In this section, we will describe our approach in detail. First, we will describe our method of generating large-scale pseudo training data for zero pronoun resolution. Then we will introduce twostep training approach to alleviate the gaps between pseudo and real training data. Finally, the attention-based neural network model as well as associated unknown words processing techniques will be described."
    }, {
      "heading" : "2.1 Generating Pseudo Training Data",
      "text" : "In order to get large quantities of training data for neural network model, we propose an approach, which is inspired by (Hermann et al., 2015), to automatically generate large-scale pseudo training data for zero pronoun resolution. However, our approach is much more simple and general than that of (Hermann et al., 2015). We will introduce the details of generating the pseudo training data for zero pronoun resolution as follows.\nFirst, we collect a large number of documents that are relevant (or homogenous in some sense) to the released OntoNote 5.0 data for zero pronoun resolution task in terms of its domain. In our experiments, we used large-scale news data for training.\nGiven a certain document D, which is composed by a set of sentences D = {s1, s2, ..., sn},\nwe randomly choose an answer wordA in the document. Note that, we restrictA to be either a noun or pronoun, where the part-of-speech is identified using LTP Toolkit (Che et al., 2010), as well as the answer word should appear at least twice in the document. Second, after the answer word A is chosen, the sentence that contains A is defined as a queryQ, in which the answer wordA is replaced by a specific symbol 〈blank〉. In this way, given the queryQ and documentD, the target of the prediction is to recover the answer A. That is quite similar to the zero pronoun resolution task. Therefore, the automatically generated training samples is called pseudo training data. Figure 1 shows an example of a pseudo training sample.\nIn this way, we can generate tremendous triples of 〈D,Q,A〉 for training neural network, without making any assumptions on the nature of the original corpus."
    }, {
      "heading" : "2.2 Two-step Training",
      "text" : "It should be noted that, though we have generated large-scale pseudo training data for neural network training, there is still a gap between pseudo training data and the real zero pronoun resolution task in terms of the query style. So we should do some adaptations to our model to deal with the zero pronoun resolution problems ideally.\nIn this paper, we used an effective approach to deal with the mismatch between pseudo training data and zero pronoun resolution task-specific data. Generally speaking, in the first stage, we use a large amount of the pseudo training data to train a fundamental model, and choose the best model according to the validation accuracy. Then we continue to train from the previous best model using the zero pronoun resolution task-specific training data, which is exactly the same domain and query type as the standard zero pronoun resolution task data.\nThe using of the combination of proposed pseudo training data and task-specific data, i.e. zero pronoun resolution task data, is far more effective than using either of them alone. Though there is a gap between these two data, they share many similar characteristics to each other as illustrated in the previous part, so it is promising to utilize these two types of data together, which will compensate to each other.\nThe two-step training procedure can be concluded as,\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nDocument: 1 ||| welcome both of you to the studio to participate in our program , 欢迎两位呢来演播室参与我们的节目， 2 ||| it happened that i was going to have lunch with a friend at noon . 正好因为我也和朋友这个，这个中午一起吃饭。 3 ||| after that , i received an sms from 1860 . 然后我就收到 1860 的短信。 4 ||| uh-huh , it was by sms . 嗯，是通过短信 的方式 ， 5 ||| uh-huh , that means , er , you knew about the accident through the source of radio station . 嗯，就是说呃你是通过台里面的一 个信息的渠道知道这儿出了这样的事故。 6 ||| although we live in the west instead of the east part , and it did not affect us that much , 虽然我们生活在西部不是在东部，对我们影响不是 很大 ， 7 ||| but i think it is very useful to inform people using sms . 但是呢，我觉得 有这样 一个短信告诉大家呢 是非常有用的啊。 Query: 8 ||| some car owners said that <blank> was very good。 有车主表示，说这 <blank> 非常的好。 Answer: sms 短信\nFigure 1: Example of pseudo training sample for zero pronoun resolution system. (The original data is in Chinese, we translate this sample into English for clarity)\n• Pre-training stage: by using large-scale training data to train the neural network model, we can learn richer word embeddings, as well as relatively reasonable weights in neural networks than just training with a small amount of zero pronoun resolution task training data;\n• Adaptation stage: after getting the best model that is produced in the previous step, we continue to train the model with task-specific data, which can force the previous model to adapt to the new data, without losing much knowledge that has learned in the previous stage (such as word embeddings).\nAs we will see in the experiment section that the proposed two-step training approach is effective and brings significant improvements."
    }, {
      "heading" : "2.3 Attention-based Neural Network Model",
      "text" : "Our model is primarily an attention-based neural network model, which is similar to Attentive Reader proposed by (Hermann et al., 2015). Formally, when given a set of training triple 〈D,Q,A〉, we will construct our network in the following way.\nFirstly, we project one-hot representation of document D and query Q into a continuous space with the shared embedding matrix We. Then we input these embeddings into different bidirectional RNN to get their contextual representations respectively. In our model, we used the bidirectional Gated Recurrent Unit (GRU) as RNN implementation (Cho et al., 2014).\ne(x) =We · x, where x ∈ D,Q (2)\n−→ hs = −−−→ GRU(e(x)); ←− hs = ←−−− GRU(e(x)) (3)\nhs = [ −→ hs; ←− hs] (4)\nFor the query representation, instead of concatenating the final forward and backward states as its representations, we directly get an averaged representations on all bi-directional RNN slices, which can be illustrated as\nhquery = 1\nn n∑ t=1 hquery(t) (5)\nFor the document, we place a soft attention over all words in document (Bahdanau et al., 2014), which indicate the degree to which part of document is attended when filling the blank in the query sentence. Then we calculate a weighted sum of all document tokens to get the attended representation of document.\nm(t) = tanh(W · hdoc(t) + U · hquery) (6)\nα(t) = exp(Ws ·m(t)) n∑ j=1 exp(Ws ·m(j)) (7)\nhdoc att = hdoc · α (8)\nwhere variable α(t) is the normalized attention weight at tth word in document, hdoc is a matrix that concatenate all hdoc(t) in sequence.\nhdoc = concat[hdoc(1), hdoc(2), ..., hdoc(t)] (9)\nThen we use attended document representation and query representation to estimate the final answer, which can be illustrated as follows, where V\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nBi-GRU Encoder\nΣ\nd1 d2 d3 d4 q1 q2 q3\nQuery\nSoftmax Layer\nConcat Layer\nAttentionLayer\nAnswer\nDocument\nEmbedding Layer\nFigure 2: Architecture of attention-based neural network model for zero pronoun resolution task.\nis the vocabulary,\nr = concat[hdoc att, hquery] (10)\nP (A|D,Q) ∝ softmax(Wr · r) , s.t. A ∈ V (11)\nFigure 2 shows the proposed neural network architecture.\nNote that, for zero pronoun resolution task, antecedents of zero pronouns are always noun phrases (NPs), while our model generates only one word (a noun or a pronoun) as the result. To better adapt our model to zero pronoun resolution task, we further process the output result in the following procedure. First, for a given zero pronoun, we extract a set of NPs as its candidates utilizing the same strategy as (Chen and Ng, 2015). Then, we use our model to generate an answer (one word) for the zero pronoun. After that, we go through all the candidates from the nearest to the far-most. For an NP candidate, if the produced answer is its head word, we then regard this NP as the antecedent of the given zero pronoun. By doing so, for a given zero pronoun, we generate an NP as the prediction of its antecedent."
    }, {
      "heading" : "2.4 Unknown Words Processing",
      "text" : "Because of the restriction on both memory occupation and training time, it is usually suggested to use a shortlist of vocabulary in neural network training. However, we often replace the out-ofvocabularies to a unique special token, such as 〈unk〉. But this may place an obstacle in real\nworld test. When the model predicts the answer as 〈unk〉, we do not know what is the exact word it represents in the document, as there may have many 〈unk〉s in the document.\nIn this paper, we propose to use a simple but effective way to handle unknown words issue. The idea is straightforward, which can be illustrated as follows.\n• Identify all unknown words inside of each 〈D,Q,A〉;\n• Instead of replacing all these unknown words into one unique token 〈unk〉, we make a hash table to project these unique unknown words to numbered tokens, such as 〈unk1〉, 〈unk2〉, ..., 〈unkN〉 in terms of its occurrence order in the document. Note that, the same words are projected to the same unknown word tokens, and all these projections are only valid inside of current sample. For example, 〈unk1〉 indicate the first unknown word, say “apple”, in the current sample, but in another sample the 〈unk1〉 may indicate the unknown word “orange”. That is, the unknown word labels are indicating position features rather than the exact word;\n• Insert these unknown marks in the vocabulary. These marks may only take up dozens of slots, which is negligible to the size of shortlists (usually 30K ∼ 100K).\n(a) The weather today is not as pleasant as the weather of yesterday.\n(b) The <unk> today is not as <unk> as the <unk> of yesterday.\n(c) The <unk1> today is not as <unk2> as the <unk1> of yesterday.\nFigure 3: Example of unknown words processing. a) original sentence; b) original unknown words processing method; c) our method\nWe take one sentence “The weather of today is not as pleasant as the weather of yesterday.” as an example to show our unknown word processing method, which is shown in Figure 3.\nIf we do not discriminate the unknown words and assign different unknown words with the same token 〈unk〉, it would be impossible for us to know what is the exact word that 〈unk〉 represents for in the real test. However, when using our proposed unknown word processing method, if the model predicts a 〈unkX〉 as the answer,\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nwe can simply scan through the original document and identify its position according to its unknown word number X and replace the 〈unkX〉 with the real word. For example, in Figure 3, if we adopt original unknown words processing method, we could not know whether the 〈unk〉 is the word “weather” or “pleasant”. However, when using our approach, if the model predicts an answer as 〈unk1〉, from the original text, we can know that 〈unk1〉 represents the word “weather”."
    }, {
      "heading" : "3 Experiments",
      "text" : ""
    }, {
      "heading" : "3.1 Data",
      "text" : "In our experiments, we choose a selection of public news data to generate large-scale pseudo training data for pre-training our neural network model (pre-training step)1. In the adaptation step, we used the official dataset OntoNotes Release 5.02 which is provided by CoNLL-2012 shared task, to carry out our experiments. The CoNLL2012 shared task dataset consists of three parts: a training set, a development set and a test set. The datasets are made up of 6 different domains, namely Broadcast News (BN), Newswires (NW), Broadcast Conversations (BC), Telephone Conversations (TC), Web Blogs (WB), and Magazines (MZ). We closely follow the experimental settings as (Kong and Zhou, 2010; Chen and Ng, 2014, 2015, 2016), where we treat the training set for training and the development set for testing, because only the training and development set are annotated with ZPs. The statistics of training and testing data is shown in Table 1 and 2 respectively.\nSentences # Query #\nGeneral Train 18.47M 1.81M Domain Train 122.8K 9.4K Validation 11,191 2,667\nTable 1: Statistics of training data, including pseudo training data and OntoNotes 5.0 training data."
    }, {
      "heading" : "3.2 Neural Network Setups",
      "text" : "Training details of our neural network models are listed as follows.\n1The news data is available at http://www.sogou. com/labs/dl/cs.html\n2http://catalog.ldc.upenn.edu/ LDC2013T19\n• Embedding: We use randomly initialized embedding matrix with uniformed distribution in the interval [-0.1,0.1], and set units number as 256. No pre-trained word embeddings are used.\n• Hidden Layer: We use GRU with 256 units, and initialize the internal matrix by random orthogonal matrices (Saxe et al., 2013). As GRU still suffers from the gradient exploding problem, we set gradient clipping threshold to 10.\n• Vocabulary: As the whole vocabulary is very large (over 800K), we set a shortlist of 100K according to the word frequency and unknown words are mapped to 20 〈unkX〉 using the proposed method.\n• Optimization: We used ADAM update rule (Kingma and Ba, 2014) with an initial learning rate of 0.001, and used negative loglikelihood as the training objective. The batch size is set to 32.\nAll models are trained on Tesla K40 GPU. Our model is implemented with Theano (Theano Development Team, 2016) and Keras (Chollet, 2015)."
    }, {
      "heading" : "3.3 Experimental results",
      "text" : "Same to the previous researches that are related to zero pronoun resolution, we evaluate our system performance in terms of F-score (F). We focus on AZP resolution process, where we assume that gold AZPs and gold parse trees are given3. The same experimental setting is utilized in (Chen and Ng, 2014, 2015, 2016). The overall results are shown in Table 3, where the performances of each domain are listed in detail and overall performance is also shown in the last column.\n• Overall Performance We employ four Chinese ZP resolution baseline systems on OntoNotes 5.0 dataset. As we can\n3All gold information are provided by the CoNLL-2012 shared task dataset\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nNW (84) MZ (162) WB (284) BN (390) BC (510) TC (283) Overall\nKong and Zhou (2010) 34.5 32.7 45.4 51.0 43.5 48.4 44.9 Chen and Ng (2014) 38.1 31.0 50.4 45.9 53.8 54.9 48.7 Chen and Ng (2015) 46.4 39.0 51.8 53.8 49.4 52.7 52.2 Chen and Ng (2016) 48.8 41.5 56.3 55.4 50.8 53.1 52.2\nOur Approach† 59.2 51.3 60.5 53.9 55.5 52.9 55.3\nTable 3: Experimental result (F-score) on the OntoNotes 5.0 test data. The best results are marked with bold face. † indicates that our approach is statistical significant over the baselines (using t-test, with p < 0.05). The number in the brackets indicate the number of AZPs.\nsee that our model significantly outperforms the previous state-of-the-art system (Chen and Ng, 2016) by 3.1% in overall F-score, and substantially outperform the other systems by a large margin. When observing the performances of different domains, our approach also gives relatively consistent improvements among various domains, except for BN and TC with a slight drop. All these results approve that our proposed approach is effective and achieves significant improvements in AZP resolution.\nIn our quantitative analysis, we investigated the reasons of the declines in the BN and TC domain. A primary observation is that the word distributions in these domains are fairly different from others. The average document length of BN and TC are quite longer than other domains, which suggest that there is a bigger chance to have unknown words than other domains, and add difficulties to the model training. Also, we have found that in the BN and TC domains, the texts are often in oral form, which means that there are many irregular expressions in the context. Such expressions add noise to the model, and it is difficult for the model to extract useful information in these contexts. These phenomena indicate that further improvements can be obtained by filtering stop words in contexts, or increasing the size of task-specific data, while we leave this in the future work.\n• Effect of UNK processing As we have mentioned in the previous section, traditional unknown word replacing methods are vulnerable to the real word test. To alleviate this issue, we proposed the UNK processing mechanism to recover the UNK tokens to the real words. In Table 4, we compared the performance that with and without the proposed UNK processing,\nto show whether the proposed UNK processing method is effective. As we can see that, by applying our UNK processing mechanism, the model do learned the positional features in these lowfrequency words, and brings over 3% improvements in F-score, which indicated the effectiveness of our UNK processing approach.\n• Effect of Domain Adaptation We also tested out whether our domain adaptation method is effective. In this experiments, we used three different types of training data: only pseudo training data, only task-specific data, and our adaptation method, i.e. using pseudo training data in the pre-training step and task-specific data for domain adaptation step. The results are given in Table 5. As we can see that, using either pseudo training data or task-specific data alone can not bring inspiring result. By adopting our domain adaptation method, the model could give significant improvements over the other models, which demonstrate the effectiveness of our proposed two-step training approach. An intuition behind this phenomenon is that though pseudo training data is fairly big enough to train a reliable model parameters, there is still a gap to the real zero pronoun resolution tasks. On the contrary, though task-specific training data is exactly the same type as the real test, the quantity is not enough to train a reasonable model (such as word embedding). So it is better to make use of both to\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\ntake the full advantage. However, as the original task-specific data is fairly small compared to pseudo training data, we also wondered if the large-scale pseudo training data is only providing rich word embedding information. So we use the large-scale pseudo training data for embedding training using GloVe toolkit (Pennington et al., 2014), and initialize the word embeddings in the “only task-specific data” system. From the result we can see that the pseudo training data provide more information than word embeddings, because though we used GloVe embeddings in “only task-specific data”, it still can not outperform the system that uses domain adaptation which supports our claim.\nF-score\nOnly Pseudo Training Data 41.1 Only Task-Specific Data 44.2 Only Task-Specific Data + GloVe 50.9 Domain Adaptation 55.3\nTable 5: Performance comparison of using different training data."
    }, {
      "heading" : "4 Error Analysis",
      "text" : "To better evaluate our proposed approach, we performed a qualitative analysis of errors, where two major errors are revealed by our analysis, as discussed below."
    }, {
      "heading" : "4.1 Effect of Unknown Words",
      "text" : "Our approach does not do well when there are lots of 〈unk〉s in the context of ZPs, especially when the 〈unk〉s appears near the ZP. An example is given below, where words with # are regarded as 〈unk〉s in our model.\nφ 登上# 太平山# 顶 , 将香港岛# 和维多 利亚港# 的美景尽收眼底。 φ Successfully climbed# the peak of [Taiping Mountain]#, to have a panoramic view of the beauty of [Hong Kong Island]# and [Victoria Harbour]#.\nIn this case, the words “登上/climbed” and “太 平山/Taiping Mountain” that appears immediately after the ZP “φ” are all regarded as 〈unk〉s in our model. As we model the sequence of words by RNN, the 〈unk〉s make the model more difficult to capture the semantic information of the sentence, which in turn influence the overall performance. Especially for the words that are near\nthe ZP, which play important roles when modeling context information for the ZP. By looking at the word “顶/peak”, it is hard to comprehend the context information, due to the several surrounding 〈unk〉s. Though our proposed unknown words processing method is effective in empirical evaluation, we think that more advanced method for unknown words processing would be of a great help in improving comprehension of the context."
    }, {
      "heading" : "4.2 Long Distance Antecedents",
      "text" : "Also, our model makes incorrect decisions when the correct antecedents of ZPs are in long distance. As our model chooses answer from words in the context, if there are lots of words between the ZP and its antecedent, more noise information are introduced, and adds more difficulty in choosing the right answer. For example:\n我帮不了那个人 ... ...那天结束后 φ 回到 家中。 I can’t help that guy ... ... After that day, φ return home.\nIn this case, the correct antecedent of ZP “φ” is the NP candidate “我/I”. By seeing the contexts, we observe that there are over 30 words between the ZP and its antecedent. Although our model does not intend to fill the ZP gap only with the words near the ZP, as most of the antecedents appear just a few words before the ZPs, our model prefers the nearer words as correct antecedents. Hence, once there are lots of words between ZP and its nearest antecedent, our model can sometimes make wrong decisions. To correctly handle such cases, our model should learn how to filter the useless words and enhance the learning of longterm dependency."
    }, {
      "heading" : "5 Related Work",
      "text" : ""
    }, {
      "heading" : "5.1 Zero pronoun resolution",
      "text" : "For Chinese zero pronoun (ZP) resolution, early studies employed heuristic rules to Chinese ZP resolution. Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents. Then, supervised approaches to this task have been vastly explored. Zhao and Ng (2007) first present a supervised machine learning approach to the identification and resolution of Chinese ZPs. Kong and Zhou (2010) develop a tree-kernel based approach for Chinese ZP resolution. More recently, unsupervised approaches\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nhave been proposed. Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information. Also, there have been many works on ZP resolution for other languages. These studies can be divided into rule-based and supervised machine learning approaches. Ferrández and Peral (2000) proposed a set of hand-crafted rules for Spanish ZP resolution. Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011). Iida and Poesio (2011) developed a cross-lingual approach for Japanese and Italian ZPs where an ILPbased model was employed to zero anaphora detection and resolution.\nIn sum, most recent researches on ZP resolution are supervised approaches, which means that their performance highly relies on large-scale annotated data. Even for the unsupervised approach (Chen and Ng, 2014), they also utilize a supervised pronoun resolver to resolve ZPs. Therefore, the advantage of our proposed approach is obvious. We are able to generate large-scale pseudo training data for ZP resolution, and also we can benefit from the task-specific data for fine-tuning via the proposed two-step training approach."
    }, {
      "heading" : "5.2 Cloze-style Reading Comprehension",
      "text" : "Our neural network model is mainly motivated by the recent researches on cloze-style reading comprehension tasks, which aims to predict one-word answer given the document and query. These models can be seen as a general model of mining the relations between the document and query, so it is promising to combine these models to the specific domain.\nA representative work of cloze-style reading comprehension is done by Hermann et al. (2015). They proposed a methodology for obtaining large quantities of 〈D,Q,A〉 triples. By using this method, a large number of training data can be obtained without much human intervention, and make it possible to train a reliable neural network. They used attention-based neural networks for this task. Evaluation on CNN/DailyMail datasets showed that their approach is much effective than\ntraditional baseline systems. While our work is similar to Hermann et al. (2015), there are several differences which can be illustrated as follows. Firstly, though we both utilize the large-scale corpus, they require that the document should accompany with a brief summary of it, while this is not always available in most of the document, and it may place an obstacle in generating limitless training data. In our work, we do not assume any prerequisite of the training data, and directly extract queries from the document, which makes it easy to generate large-scale training data. Secondly, their work mainly focuses on reading comprehension in the general domain. We are able to exploit large-scale training data for solving problems in the specific domain, and we proposed two-step training method which can be easily adapted to other domains as well."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this study, we propose an effective way to generate and exploit large-scale pseudo training data for zero pronoun resolution task. The main idea behind our approach is to automatically generate large-scale pseudo training data and then utilize an attention-based neural network model to resolve zero pronouns. For training purpose, two-step training approach is employed, i.e. a pre-training and adaptation step, and this can be also easily applied to other tasks as well. The experimental results on OntoNotes 5.0 corpus are encouraging, showing that the proposed model and accompanying approaches significantly outperforms the stateof-the-art systems.\nThe future work will be carried out on two main aspects: First, as experimental results show that the unknown words processing is a critical part in comprehending context, we will explore more effective way to handle the UNK issue. Second, we will develop other neural network architecture to make it more appropriate for zero pronoun resolution task."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473 .",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Ltp: A chinese language technology platform",
      "author" : [ "Wanxiang Che", "Zhenghua Li", "Ting Liu." ],
      "venue" : "Proceedings of the 23rd International Conference on",
      "citeRegEx" : "Che et al\\.,? 2010",
      "shortCiteRegEx" : "Che et al\\.",
      "year" : 2010
    }, {
      "title" : "Chinese zero pronoun resolution: Some recent advances",
      "author" : [ "Chen Chen", "Vincent Ng." ],
      "venue" : "EMNLP. pages 1360–1365.",
      "citeRegEx" : "Chen and Ng.,? 2013",
      "shortCiteRegEx" : "Chen and Ng.",
      "year" : 2013
    }, {
      "title" : "Chinese zero pronoun resolution: An unsupervised approach combining ranking and integer linear programming",
      "author" : [ "Chen Chen", "Vincent Ng." ],
      "venue" : "Twenty-Eighth AAAI Conference on Artificial Intelligence.",
      "citeRegEx" : "Chen and Ng.,? 2014",
      "shortCiteRegEx" : "Chen and Ng.",
      "year" : 2014
    }, {
      "title" : "Chinese zero pronoun resolution: A joint unsupervised discourseaware model rivaling state-of-the-art resolvers",
      "author" : [ "Chen Chen", "Vincent Ng." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natu-",
      "citeRegEx" : "Chen and Ng.,? 2015",
      "shortCiteRegEx" : "Chen and Ng.",
      "year" : 2015
    }, {
      "title" : "Chinese zero pronoun resolution with deep neural networks",
      "author" : [ "Chen Chen", "Vincent Ng." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics,",
      "citeRegEx" : "Chen and Ng.,? 2016",
      "shortCiteRegEx" : "Chen and Ng.",
      "year" : 2016
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Keras",
      "author" : [ "François Chollet." ],
      "venue" : "https://github. com/fchollet/keras.",
      "citeRegEx" : "Chollet.,? 2015",
      "shortCiteRegEx" : "Chollet.",
      "year" : 2015
    }, {
      "title" : "Pronominal anaphora resolution in chinese",
      "author" : [ "Susan P Converse" ],
      "venue" : null,
      "citeRegEx" : "Converse.,? \\Q2006\\E",
      "shortCiteRegEx" : "Converse.",
      "year" : 2006
    }, {
      "title" : "A computational approach to zero-pronouns in spanish",
      "author" : [ "Antonio Ferrández", "Jesús Peral." ],
      "venue" : "Proceedings of the 38th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 166–172.",
      "citeRegEx" : "Ferrández and Peral.,? 2000",
      "shortCiteRegEx" : "Ferrández and Peral.",
      "year" : 2000
    }, {
      "title" : "Korean zero pronouns: analysis and resolution",
      "author" : [ "Na-Rae Han." ],
      "venue" : "Ph.D. thesis, Citeseer.",
      "citeRegEx" : "Han.,? 2006",
      "shortCiteRegEx" : "Han.",
      "year" : 2006
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 1684–",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Resolving pronoun references",
      "author" : [ "Jerry R Hobbs." ],
      "venue" : "Lingua 44(4):311–338.",
      "citeRegEx" : "Hobbs.,? 1978",
      "shortCiteRegEx" : "Hobbs.",
      "year" : 1978
    }, {
      "title" : "Exploiting syntactic patterns as clues in zero-anaphora resolution",
      "author" : [ "Ryu Iida", "Kentaro Inui", "Yuji Matsumoto." ],
      "venue" : "Proceedings of the 21st International Conference on Computational Linguistics and the",
      "citeRegEx" : "Iida et al\\.,? 2006",
      "shortCiteRegEx" : "Iida et al\\.",
      "year" : 2006
    }, {
      "title" : "Zero-anaphora resolution by learning rich syntactic pattern features",
      "author" : [ "Ryu Iida", "Kentaro Inui", "Yuji Matsumoto." ],
      "venue" : "ACM Transactions on Asian Language Information Processing (TALIP) 6(4):1.",
      "citeRegEx" : "Iida et al\\.,? 2007",
      "shortCiteRegEx" : "Iida et al\\.",
      "year" : 2007
    }, {
      "title" : "A cross-lingual ilp solution to zero anaphora resolution",
      "author" : [ "Ryu Iida", "Massimo Poesio." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computa-",
      "citeRegEx" : "Iida and Poesio.,? 2011",
      "shortCiteRegEx" : "Iida and Poesio.",
      "year" : 2011
    }, {
      "title" : "Japanese zero pronoun resolution based on ranking rules and machine learning",
      "author" : [ "Hideki Isozaki", "Tsutomu Hirao." ],
      "venue" : "Proceedings of the 2003 conference on Empirical methods in natural language processing. Association for Computational Linguis-",
      "citeRegEx" : "Isozaki and Hirao.,? 2003",
      "shortCiteRegEx" : "Isozaki and Hirao.",
      "year" : 2003
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980 .",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "A tree kernelbased unified framework for chinese zero anaphora resolution",
      "author" : [ "Fang Kong", "Guodong Zhou." ],
      "venue" : "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
      "citeRegEx" : "Kong and Zhou.,? 2010",
      "shortCiteRegEx" : "Kong and Zhou.",
      "year" : 2010
    }, {
      "title" : "Semeval-2010 task 1: Coreference resolution in multiple languages",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "Recasens.,? \\Q2010\\E",
      "shortCiteRegEx" : "Recasens.",
      "year" : 2010
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Conll-2012 shared task: Modeling multilingual unrestricted coreference in ontonotes",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "Pradhan.,? \\Q2012\\E",
      "shortCiteRegEx" : "Pradhan.",
      "year" : 2012
    }, {
      "title" : "A discriminative approach to japanese zero anaphora resolution with large-scale lexicalized case frames",
      "author" : [ "Ryohei Sasano", "Sadao Kurohashi." ],
      "venue" : "IJCNLP. pages 758–766.",
      "citeRegEx" : "Sasano and Kurohashi.,? 2011",
      "shortCiteRegEx" : "Sasano and Kurohashi.",
      "year" : 2011
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "Andrew M Saxe", "James L McClelland", "Surya Ganguli." ],
      "venue" : "arXiv preprint arXiv:1312.6120 .",
      "citeRegEx" : "Saxe et al\\.,? 2013",
      "shortCiteRegEx" : "Saxe et al\\.",
      "year" : 2013
    }, {
      "title" : "Theano: A Python framework for fast computation of mathematical expressions",
      "author" : [ "Theano Development Team." ],
      "venue" : "arXiv e-prints abs/1605.02688. http://arxiv.org/abs/1605.02688.",
      "citeRegEx" : "Team.,? 2016",
      "shortCiteRegEx" : "Team.",
      "year" : 2016
    }, {
      "title" : "Identification and resolution of chinese zero pronouns: A machine learning approach",
      "author" : [ "Shanheng Zhao", "Hwee Tou Ng." ],
      "venue" : "EMNLP-CoNLL. volume 2007, pages 541–550.",
      "citeRegEx" : "Zhao and Ng.,? 2007",
      "shortCiteRegEx" : "Zhao and Ng.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).",
      "startOffset" : 100,
      "endOffset" : 212
    }, {
      "referenceID" : 25,
      "context" : "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).",
      "startOffset" : 100,
      "endOffset" : 212
    }, {
      "referenceID" : 14,
      "context" : "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).",
      "startOffset" : 100,
      "endOffset" : 212
    }, {
      "referenceID" : 18,
      "context" : "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).",
      "startOffset" : 100,
      "endOffset" : 212
    }, {
      "referenceID" : 15,
      "context" : "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).",
      "startOffset" : 100,
      "endOffset" : 212
    }, {
      "referenceID" : 2,
      "context" : "Previous works on zero pronoun (ZP) resolution mainly focused on the supervised learning approaches (Han, 2006; Zhao and Ng, 2007; Iida et al., 2007; Kong and Zhou, 2010; Iida and Poesio, 2011; Chen and Ng, 2013).",
      "startOffset" : 100,
      "endOffset" : 212
    }, {
      "referenceID" : 11,
      "context" : "1 Generating Pseudo Training Data In order to get large quantities of training data for neural network model, we propose an approach, which is inspired by (Hermann et al., 2015), to automatically generate large-scale pseudo training data for zero pronoun resolution.",
      "startOffset" : 155,
      "endOffset" : 177
    }, {
      "referenceID" : 11,
      "context" : "However, our approach is much more simple and general than that of (Hermann et al., 2015).",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "Note that, we restrictA to be either a noun or pronoun, where the part-of-speech is identified using LTP Toolkit (Che et al., 2010), as well as the answer word should appear at least twice in the document.",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 11,
      "context" : "3 Attention-based Neural Network Model Our model is primarily an attention-based neural network model, which is similar to Attentive Reader proposed by (Hermann et al., 2015).",
      "startOffset" : 152,
      "endOffset" : 174
    }, {
      "referenceID" : 6,
      "context" : "In our model, we used the bidirectional Gated Recurrent Unit (GRU) as RNN implementation (Cho et al., 2014).",
      "startOffset" : 89,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "For the document, we place a soft attention over all words in document (Bahdanau et al., 2014), which indicate the degree to which part of document is attended when filling the blank in the query sentence.",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "First, for a given zero pronoun, we extract a set of NPs as its candidates utilizing the same strategy as (Chen and Ng, 2015).",
      "startOffset" : 106,
      "endOffset" : 125
    }, {
      "referenceID" : 18,
      "context" : "We closely follow the experimental settings as (Kong and Zhou, 2010; Chen and Ng, 2014, 2015, 2016), where we treat the training set for training and the development set for testing, because only the training and development set are annotated with ZPs.",
      "startOffset" : 47,
      "endOffset" : 99
    }, {
      "referenceID" : 23,
      "context" : "• Hidden Layer: We use GRU with 256 units, and initialize the internal matrix by random orthogonal matrices (Saxe et al., 2013).",
      "startOffset" : 108,
      "endOffset" : 127
    }, {
      "referenceID" : 17,
      "context" : "• Optimization: We used ADAM update rule (Kingma and Ba, 2014) with an initial learning rate of 0.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "Our model is implemented with Theano (Theano Development Team, 2016) and Keras (Chollet, 2015).",
      "startOffset" : 79,
      "endOffset" : 94
    }, {
      "referenceID" : 2,
      "context" : "9 Chen and Ng (2014) 38.",
      "startOffset" : 2,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "9 Chen and Ng (2014) 38.1 31.0 50.4 45.9 53.8 54.9 48.7 Chen and Ng (2015) 46.",
      "startOffset" : 2,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "9 Chen and Ng (2014) 38.1 31.0 50.4 45.9 53.8 54.9 48.7 Chen and Ng (2015) 46.4 39.0 51.8 53.8 49.4 52.7 52.2 Chen and Ng (2016) 48.",
      "startOffset" : 2,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : "see that our model significantly outperforms the previous state-of-the-art system (Chen and Ng, 2016) by 3.",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 20,
      "context" : "So we use the large-scale pseudo training data for embedding training using GloVe toolkit (Pennington et al., 2014), and initialize the word embeddings in the “only task-specific data” system.",
      "startOffset" : 90,
      "endOffset" : 115
    }, {
      "referenceID" : 12,
      "context" : "Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents.",
      "startOffset" : 104,
      "endOffset" : 117
    }, {
      "referenceID" : 8,
      "context" : "Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents.",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 8,
      "context" : "Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents. Then, supervised approaches to this task have been vastly explored. Zhao and Ng (2007) first present a supervised machine learning approach to the identification and resolution of Chinese ZPs.",
      "startOffset" : 0,
      "endOffset" : 227
    }, {
      "referenceID" : 8,
      "context" : "Converse (2006) proposes a rule-based method to resolve the zero pronouns, by utilizing Hobbs algorithm (Hobbs, 1978) in the CTB documents. Then, supervised approaches to this task have been vastly explored. Zhao and Ng (2007) first present a supervised machine learning approach to the identification and resolution of Chinese ZPs. Kong and Zhou (2010) develop a tree-kernel based approach for Chinese ZP resolution.",
      "startOffset" : 0,
      "endOffset" : 354
    }, {
      "referenceID" : 10,
      "context" : "Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al.",
      "startOffset" : 80,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011).",
      "startOffset" : 105,
      "endOffset" : 183
    }, {
      "referenceID" : 22,
      "context" : "Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011).",
      "startOffset" : 105,
      "endOffset" : 183
    }, {
      "referenceID" : 3,
      "context" : "Even for the unsupervised approach (Chen and Ng, 2014), they also utilize a supervised pronoun resolver to resolve ZPs.",
      "startOffset" : 35,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : "Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information.",
      "startOffset" : 0,
      "endOffset" : 163
    }, {
      "referenceID" : 2,
      "context" : "Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information. Also, there have been many works on ZP resolution for other languages. These studies can be divided into rule-based and supervised machine learning approaches. Ferrández and Peral (2000) proposed a set of hand-crafted rules for Spanish ZP resolution.",
      "startOffset" : 0,
      "endOffset" : 489
    }, {
      "referenceID" : 2,
      "context" : "Chen and Ng (2014) develop an unsupervised language-independent approach, utilizing the integer linear programming to using ten overt pronouns. Chen and Ng (2015) propose an end-to-end unsupervised probabilistic model for Chinese ZP resolution, using a salience model to capture discourse information. Also, there have been many works on ZP resolution for other languages. These studies can be divided into rule-based and supervised machine learning approaches. Ferrández and Peral (2000) proposed a set of hand-crafted rules for Spanish ZP resolution. Recently, supervised approaches have been exploited for ZP resolution in Korean (Han, 2006) and Japanese (Isozaki and Hirao, 2003; Iida et al., 2006, 2007; Sasano and Kurohashi, 2011). Iida and Poesio (2011) developed a cross-lingual approach for Japanese and Italian ZPs where an ILPbased model was employed to zero anaphora detection and resolution.",
      "startOffset" : 0,
      "endOffset" : 761
    }, {
      "referenceID" : 10,
      "context" : "A representative work of cloze-style reading comprehension is done by Hermann et al. (2015). They proposed a methodology for obtaining large quantities of 〈D,Q,A〉 triples.",
      "startOffset" : 70,
      "endOffset" : 92
    }, {
      "referenceID" : 10,
      "context" : "Evaluation on CNN/DailyMail datasets showed that their approach is much effective than traditional baseline systems. While our work is similar to Hermann et al. (2015), there are several differences which can be illustrated as follows.",
      "startOffset" : 83,
      "endOffset" : 168
    } ],
    "year" : 2017,
    "abstractText" : "Most existing approaches for zero pronoun resolution are heavily relying on annotated data, which is often released by shared task organizers. Therefore, the lack of annotated data becomes a major obstacle in the progress of zero pronoun resolution task. Also, it is expensive to spend manpower on labeling the data for better performance. To alleviate the problem above, in this paper, we propose a simple but novel approach to automatically generate large-scale pseudo training data for zero pronoun resolution. Furthermore, we successfully transfer the cloze-style reading comprehension neural network model into zero pronoun resolution task and propose a two-step training mechanism to overcome the gap between the pseudo training data and the real one. Experimental results show that the proposed approach significantly outperforms the state-of-the-art systems with an absolute improvements of 3.1% F-score on OntoNotes 5.0 data.",
    "creator" : "LaTeX with hyperref package"
  }
}