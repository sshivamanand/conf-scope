{
  "name" : "145.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "To model language, we must represent words. We can imagine representing every word with a binary one-hot vector corresponding to a dictionary position. But such a representation contains no valuable semantic information: distances between word vectors represent only differences in alphabetic ordering. Modern approaches, by contrast, learn to map words with similar meanings to nearby points in a vector space (Mikolov et al., 2013a), from large datasets such as Wikipedia. These learned word embeddings have become ubiquitous in predictive tasks.\nVilnis and McCallum (2014) recently proposed an alternative view, where words are represented by a whole probability distribution instead of a deterministic point vector. Specifically, they model each word by a Gaussian distribution, and learn its mean and covariance matrix from data. This approach generalizes any deterministic point embedding, which can be fully captured by the mean vector of the Gaussian distribution. Moreover, the full distribution provides much richer information\nthan point estimates for characterizing words, representing probability mass and uncertainty across a set of semantics. However, since a Gaussian distribution can have only one mode, the learned uncertainty in this representation can be overly diffuse for words with multiple distinct meanings (polysemys), in order for the model to assign some density to any plausible semantics (Vilnis and McCallum, 2014). Moreover, the mean of the Gaussian can be pulled in many opposing directions, leading to a biased distribution that centers its mass mostly around certain meaning while leaving the others not well represented. In this paper, we propose to represent each word with an expressive multimodal distribution, for multiple distinct meanings, entailment, heavy tailed uncertainty, and enhanced interpretability. For example, one mode of the word ‘bank’ could overlap with distributions for words such as ‘finance’ and ‘money’, and another mode could overlap with the distributions for ‘river’ and ‘creek’. It is our contention that such flexibility is critical for both qualitatively learning about the meanings of words, and for optimal performance on many predictive tasks. In particular, we model each word with a mixture of Gaussians (Section 3.1). We learn all the parameters of this mixture model using a maximum margin energy-based ranking objective (Joachims, 2002; Vilnis and McCallum, 2014) (Section 3.3), where the energy function describes the affinity between a pair of words. For analytic tractability with Gaussian mixtures, we use the inner product between probability distributions in a Hilbert space, known as the expected likelihood kernel (Jebara et al., 2004), as our energy function (Section 3.4). Additionally, we propose transformations for numerical stability and initialization, resulting in a robust, straightforward, and scal-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nable learning procedure, capable of training on a corpus with billions of words in days. We show that the model is able to automatically discover multiple meanings for words (Section 4.2), and significantly outperform other alternative methods across several tasks such as word similarity and entailment (Section 4.3, 4.4, 4.6)."
    }, {
      "heading" : "2 Related Work",
      "text" : "In the past decade, there has been an explosion of interest in word vector representations. word2vec, arguably the most popular word embedding, uses continuous bag of words and skipgrammodels, as well as negative sampling for efficient conditional probability estimation (Mikolov et al., 2013a,b). Other popular approaches use feedforward (Bengio et al., 2003) and recurrent neural network language models (Mikolov et al., 2010, 2011; Collobert and Weston, 2008) to predict missing words in sentences, producing hidden layers that can act as word embeddings that encode semantic information. A different approach to learning word embeddings is through factorization of word co-occurrence matrices such as GloVe embeddings (Pennington et al., 2014). The matrix factorization approach has been shown to have an implicit connection with skip-gram and negative sampling Levy and Goldberg (2014). Bayesian matrix factorization where row and columns are modeled as Gaussians has been explored in Salakhutdinov and Mnih (2008) and provides a different probabilistic perspective of word embeddings.\nIn exciting recent work, Vilnis and McCallum (2014) propose a Gaussian distribution to model each word. Their approach is significantly more expressive than typical point embeddings, with the ability to represent concepts such as entailment, by having the distribution for one word (e.g. ‘music’) encompass the distributions for sets of related words (‘jazz’ and ‘pop’). However, with a unimodal distribution, their approach cannot capture multiple distinct meanings, much like most deterministic approaches. A small body of work develops multiple deterministic embeddings that can capture polysemys, for example through a cluster centroid of context vectors (Huang et al., 2012), or an adapted skip-gram model with an EM algorithm to learn multiple latent representations per word Tian et al. (2014). Another related work by Nalisnick and Ravi (2015) models embeddings in\ninfinite-dimensional space where each embedding can gradually represent incremental word sense if complex meanings are observed. Probabilistic word embeddings have only recently begun to be explored, and have so far shown great promise. In this paper, we propose, to the best of our knowledge, the first probabilistic word embedding that can capture multiple meanings. We use a Gaussian mixture model which allows for a highly expressive distributions over words. At the same time, we retain scalability and analytic tractability with an expected likelihood kernel energy function for training. The model and training procedure harmonize to learn descriptive representations of words, with superior performance on several benchmarks."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section, we introduce our Gaussian mixture (GM) model for word representations, and present a training method to learn the parameters of the Gaussian mixture. This method uses an energy-based maximum margin objective, where we wish to maximize the similarity of distributions of nearby words in sentences. We propose an energy function that compliments the GM model by retaining analytic tractability. We also provide critical practical details for numerical stability and initialization."
    }, {
      "heading" : "3.1 Word Representation",
      "text" : "We represent each word w in a dictionary as a Gaussian mixture with K components. Specifically, the distribution of w, fw, is given by the density\nfw(x⃗) = K∑ i=1 pw,i N [x⃗; µ⃗w,i,Σw,i] (1)\n= K∑ i=1 pw,i√ 2π|Σw,i| e− 1 2 (x⃗−µ⃗w,i)⊤Σ−1w,i(x⃗−µ⃗w,i) ,\nwhere ∑K\ni=1 pw,i = 1. The mean vectors µ⃗w,i represent the location of the ith component of word w, and are akin to the point embeddings provided by popular approaches like word2vec. pw,i represents the component probability (mixture weight), and Σw,i is the component covariance matrix, containing uncertainty information. Our goal is to learn all of the model parameters µ⃗w,i, pw,i,Σw,i from a corpus of natural sentences to extract semantic information of words. Each\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nGaussian component’s mean vector of word w should be able to represent one of the word’s distinct meanings. For instance, one component of a polysemous word such as rock should represent the meaning related to stone or pebbles, whereas another component should represent the meaning related to music such as jazz, pop. Figure 1 illustrates our word embedding model, and the difference between multimodal and unimodal representations, for words with multiple meanings.\nmusic\nrock\njazz\nbasalt\npop\nstone\nrock\nstone\njazz\npop\nmusic\nbasalt\nmusic\njazz\nrock\nbasalt\npop\nstone\nrock\nmusic\nrock\njazz\nbasalt stone\npop\nrock\nbasalt\nstone\nmusic\njazz\npop\nFigure 1: Top: Each word is represented by a Gaussian mixture, where each mixture component corresponds to a distinct meaning. Each Gaussian component is represented by an ellipsoid, whose center is specified by the mean vector and contour surface specified by the covariance matrix, reflecting subtleties in meaning and uncertainty. On the left, we show examples of Gaussian mixture distributions of words where Gaussian components are randomly initialized. After training, we see on the right that one component of the word rock is closer to stone and basalt, whereas the other component is closer to jazz and pop. We also demonstrate the entailment concept where the distribution of more general word music encapsulates words such as jazz, rock, pop. Bottom: An embedding model where each word is represented by a Gaussian distribution (Vilnis and McCallum, 2014). For words with multiple meanings, such as ‘rock’, the variance of the learned representation becomes unnecessarily large in order to assign some probability to both meanings. Moreover, the mean vector for such words can be pulled between two clusters, centering the mass of the distribution on a region which is far from certain meanings."
    }, {
      "heading" : "3.2 Skip-Gram",
      "text" : "The training objective for learning θ = {µ⃗w,i, pw,i,Σw,i} draws inspiration from the continuous skip-gram model (Mikolov et al., 2013a), where word embeddings are trained to maximize the probability of observing a word given another nearby word. This procedure follows the distributional hypothesis that words occurring in natural contexts tend to be semantically related. For instance, the words jazz and music tend to occur near one another more often than jazz and cat; hence, jazz and music are more likely to be related. The learned word representation contains useful semantic information and can be used to perform a variety of NLP tasks such as analogy, word similarity analysis, sentiment classification, or as a preprocessed input for complex system such as statistical machine translation."
    }, {
      "heading" : "3.3 Energy-based Max-Margin Objective",
      "text" : "Each sample in the objective consists of two pairs of words, (w, c) and (w, c′). w is sampled from a sentence in a corpus and c is a nearby word within a context window of length ℓ. For instance, a word w = jazz which occurs in the sentence I listen to jazz music has context words I, listen, to , music. c′ is a negative context word (e.g. airplane) obtained from random sampling across the whole document from distribution Pn(c′) (details in supplementary material, Section A.1). The objective is to maximize the energy between words that occur near each other, w and c, and minimize the energy between w and its negative context c′. This approach is similar to negative sampling (Mikolov et al., 2013a,b), which contrasts the dot product between positive context pairs with negative context pairs. The energy function is a measure of similarity between distributions and will be presented and discussed in Section 3.4. We use a max-margin ranking objective (Joachims, 2002), used for Gaussian embeddings in Vilnis and McCallum (2014), which pushes the similarity of a word and its positive context higher than that of its negative context by a margin m:\nLθ(w, c, c ′) = max(0,\nm− logEθ(w, c) + logEθ(w, c′))\nThis objective can be minimized by mini-batch stochastic gradient descent with respect to the pa-\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nrameters θ = {µ⃗w,i, pw,i,Σw,i} – the mean vectors, covariance matrices, and mixture weights – of our multimodal embedding in Eq. (1)."
    }, {
      "heading" : "3.4 Energy Function",
      "text" : "For vector representation of words, a usual choice for similarity measure (energy function) is a dot product between two vectors. Our word representations are distributions instead of point vectors and therefore need a measure that reflects not only the point similarity, but also the uncertainty. In contrast to Vilnis and McCallum (2014), who use a KL energy function, we propose to use the expected likelihood kernel, which is a generalization of an inner product between vectors to an inner product between distributions (Jebara et al., 2004). That is,\nE(f, g) = ∫ f(x)g(x) dx = ⟨f, g⟩L2\nwhere ⟨·, ·⟩L2 denotes the inner product in Hilbert space L2. We choose this form of energy since it can be evaluated in a closed form given our choice of probabilistic embedding in Eq. (1).\nFor Gaussian mixtures f, g representing the words wf , wg, f(x) = ∑K i=1 piN (x; µ⃗f,i,Σf,i)\nand g(x) = ∑K i=1 qiN (x; µ⃗g,i,Σg,i), ∑K i=1 pi =\n1, and ∑K\ni=1 qi = 1, we find (See Section A.1) the log energy is\nlogEθ(f, g) = log K∑ j=1 K∑ i=1 piqje ξi,j (2)\nwhere\nξi,j ≡ logN (0; µ⃗f,i − µ⃗g,j ,Σf,i +Σg,j)\n= −1 2 log det(Σf,i +Σg,j)− D 2 log(2π)\n−1 2 (µ⃗f,i − µ⃗g,j)⊤(Σf,i +Σg,j)−1(µ⃗f,i − µ⃗g,j)\n(3)\nWe call the term ξi,j partial (log) energy. Observe that this term captures the similarity between the ith meaning of word wf and the jth meaning of word wg. The total energy in Equation 2 is the sum of possible pairs of partial energies, weighted accordingly by the mixture probabilities pi and qj .\nThe term−(µ⃗f,i−µ⃗g,j)⊤(Σf,i+Σg,j)−1(µ⃗f,i− µ⃗g,j) in ξi,j explains the difference in mean vectors of semantic pair (wf , i) and (wg, j). If the semantic uncertainty (covariance) for both pairs are low,\nthis term has more importance relative to other terms due to the inverse covariance scaling. We observe that the loss function Lθ in Section 3.3 attains a low value when Eθ(w, c) is relatively high. This could be achieved by µ⃗f,i being close to µ⃗g,j , which brings the term close to zero. It could also be achieved by high values ofΣf,i andΣg,j , which washes out the importance of the mean vector difference. The term − log det(Σf,i + Σg,j) serves as a regularizer that prevents the covariances from being pushed too high at the expense of learning a good mean embedding. At the beginning of training, ξi,j roughly are on the same scale among all pairs (i, j)’s. During this time, all components learn the signals from the word occurrences equally. As training progresses and the semantic representation of each mixture becomes more clear, there can be one term of ξi,j’s that is predominantly higher than other terms, giving rise to a semantic pair that is most related. The negative KL divergence is another sensible choice of energy function, providing an asymmetric metric between word distributions. However, unlike the expected likelihood kernel, KL divergence does not have a closed form if the two distributions are Gaussian mixtures."
    }, {
      "heading" : "4 Experiments",
      "text" : "We have introduced a model for multi-prototype embeddings, which expressively captures word meanings with whole probability distributions. We show that our combination of energy and objective functions, proposed in section 3, enables one to learn interpretable multimodal distributions through unsupervised training, for describing words with multiple distinct meanings. Our model also reduces the unnecessarily large variance of a Gaussian embedding model, and has improved results on word entailment tasks. To learn the parameters of the proposed mixture model, we train on a concatenation of two datasets: UKWAC (2.5 billion tokens) and Wackypedia (1 billion tokens) (Baroni et al., 2009). We discard words that occur fewer than 100 times in the corpus, which results in a vocabulary size of 314, 129 words. Unless stated otherwise, our Gaussian mixture model has two Gaussian components (see Section 4.2 for discussion on higher number of components). Our word sampling scheme is similar to that of word2vec with one negative context word for each positive\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nWord Co. Nearest Neighbors\nrock 0 basalt:1, boulder:1, boulders:0, stalagmites:0, stalactites:0, rocks:1, sand:0, quartzite:1, bedrock:0 rock 1 rock/:1, ska:0, funk:1, pop-rock:1, punk:1, indie-rock:0, band:0, indie:0, pop:1 bank 0 banks:1, mouth:1, river:1, River:0, confluence:0, waterway:1, downstream:1, upstream:0, dammed:0 bank 1 banks:0, banking:1, banker:0, Banks:1, bankas:1, Citibank:1, Interbank:1, Bankers:0, transactions:1 Apple 0 Strawberry:0, Tomato:1, Raspberry:1, Blackberry:1, Apples:0, Pineapple:1, Grape:1, Lemon:0 Apple 1 Macintosh:1, Mac:1, OS:1, Amiga:0, Compaq:0, Atari:1, PC:1, Windows:0, iMac:0 star 0 stars:0, Quaid:0, starlet:0, Dafoe:0, Stallone:0, Geena:0, Niro:0, Zeta-Jones:1, superstar:0 star 1 stars:1, brightest:0, Milky:0, constellation:1, stellar:0, nebula:1, galactic:1, supernova:1, Ophiuchus:1 cell 0 cellular:0, Nextel:0, 2-line:0, Sprint:0, phones.:1, pda:1, handset:0, handsets:1, pushbuttons:0 cell 1 cytoplasm:0, vesicle:0, cytoplasmic:1, macrophages:0, secreted:1, membrane:0, mitotic:0, endocytosis:1 left 0 After:1, back:0, finally:1, eventually:0, broke:0, joined:1, returned:1, after:1, soon:0 left 1 right-hand:0, hand:0, right:0, left-hand:0, lefthand:0, arrow:0, turn:0, righthand:0, Left:0\nWord Nearest Neighbors\nrock band, bands, Rock, indie, Stones, breakbeat, punk, electronica, funk bank banks, banking, trader, trading, Bank, capital, Banco, bankers, cash Apple Macintosh, Microsoft, Windows, Macs, Lite, Intel, Desktop, WordPerfect, Mac star stars, stellar, brightest, Stars, Galaxy, Stardust, eclipsing, stars., Star cell cells, DNA, cellular, cytoplasm, membrane, peptide, macrophages, suppressor, vesicles left leaving, turned, back, then, After, after, immediately, broke, end\nTable 1: Nearest neighbors based on cosine similarity between the mean vectors of Gaussian components for Gaussian mixture embedding (top) and Gaussian embedding (bottom). The notation w:i denotes the ith mixture component of the word w.\ncontext word. For reproducibility, we describe hyperparameter settings and implementation details in the supplementary material. We performed experiments using 12 cores of Intel i7 3.5GHz CPU. Code will be made publicly available upon publication.\nAfter training, we obtain learned parameters {µ⃗w,i,Σw,i, pi}Ki=1 for each word w. We treat the mean vector µ⃗w,i as the embedding of the ith mixture component with the covariance matrix Σw,i representing its subtlety and uncertainty. We perform qualitative evaluation to show that our embeddings learn meaningful multi-prototype representations and compare to existing models using a quantitative evaluation on word similarity datasets and word entailment.\nWe name our model as Word to Gaussian Mixture (w2gm) in constrast to Word to Gaussian (w2g) (Vilnis and McCallum, 2014). Unless stated otherwise, w2g refers to our implementation of w2gm model with one mixture component."
    }, {
      "heading" : "4.1 Similarity Measures",
      "text" : "Since our word embeddings contain multiple vectors and uncertainty parameters per word, we use the following measures that generalizes similarity scores. These measures pick out the component pair with maximum similarity and therefore determine the meanings that are most relevant."
    }, {
      "heading" : "4.1.1 Expected Likelihood Kernel",
      "text" : "The most natural choice for a similarity score is the expected likelihood kernel, an inner product between distributions, which we discussed in section 3.4. This metric incorporates the uncertainty from the covariance matrices in addition to the similarity between the mean vectors."
    }, {
      "heading" : "4.1.2 Maximum Cosine Similarity",
      "text" : "This metric measures the maximum similarity of mean vectors among all pairs of mixture components between distributions f and g. That is,\nd(f, g) = max i,j=1,...,K ⟨µf,i,µg,j⟩ ||µf,i|| · ||µg,j || , which corresponds to matching the meanings of f and g that are the most similar. For a Gaussian embedding, maximum similarity reduces to the usual cosine similarity."
    }, {
      "heading" : "4.1.3 Minimum Euclidean Distance",
      "text" : "Cosine similarity is popular for evaluating embeddings. However, our training objective directly involves the Euclidean distance in Eq. (3), as opposed to dot product of vectors such as in word2vec. Therefore, we also consider the Euclidean metric: d(f, g) = min\ni,j=1,...,K [||µf,i−µg,j ||]."
    }, {
      "heading" : "4.2 Qualitative Evaluation",
      "text" : "In Table 1, we show examples of polysemous words and their nearest neighbors in the embed-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nding space to demonstrate that our trained embeddings capture multiple word senses. For instance, a word such as rock that could mean either stone or rock music should have each of its meanings represented by a distinct Gaussian component. Our results for a mixture of two Gaussians model confirm this hypothesis, where we observe that the 0th component of rock being related to basalt, boulders and the 1st component being related to indie, funk, hip-hop. Similarly, the word bank has the 0th component representing the river bank and the 1st component representing the financial bank. By contrast, in Table 1 (bottom), see that for Gaussian embeddings with one mixture component, nearest neighbors of polysemous words are predominantly related to a single meaning. For instance, rockmostly has neighbors related to rock music and bank mostly related to the financial bank. The alternative meanings of these polysemous words are not well represented in the embeddings. As a numerical example, the cosine similarity between rock and stone for the Gaussian representation of Vilnis and McCallum (2014) is only 0.029, much lower than the cosine similarity 0.586 between the 0th component of rock and stone in our multimodal representation.\nEmbedding Visualization We provide an anonymous interactive visualization website (http://35.161.153.223: 6002) for our embeddings (w2gm) that allows real-time queries of words’ nearest neighbors (in the embeddings tab). We use a notation similar to that of Table 1, where a token w: i represents the component i of a word w. For instance, if we search for bank:0, we obtain the nearest neighbors such as river:1, confluence:0, waterway:1, which indicates that the 0th component of ‘bank’ has the meaning ‘river bank’. On the other hand, searching for bank:1 yields nearby words such as banking:1, banker:0, ATM:0, indicating that this component is close to the ‘financial bank’. We also have a visualization of a unimodal Gaussian embedding (w2g) for comparison.\nIn addition, we include results for our Gaussian mixture model where K = 3, which can learn three distinct meanings. For instance, each of the three components of ‘cell’ is close to (‘keypad’, ‘digits’), (‘incarcerated’, ‘inmate’) or (‘tissue’, ‘antibody’), indicating that the distribution\ncaptures the concept of ‘cellphone’, ‘jail cell’, or ‘biological cell’, respectively. We note that due to the limited number of words with more than 2 meanings, our model with K = 3 does not generally offer performance differences to our model with K = 2; hence, we do not further display K = 3 results for compactness."
    }, {
      "heading" : "4.3 Word Similarity",
      "text" : "We evaluate our embeddings on several standard word similarity datasets, namely, SimLex (Hill et al., 2014), WS or WordSim-353, WS-S (similarity), WS-R (relatedness) (Finkelstein et al., 2002), MEN (Bruni et al., 2014), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), YP (Yang and Powers, 2006), MTurk(287,-771) (Radinsky et al., 2011; Halawi et al., 2012), and RW (Luong et al.). Each dataset contains a list of word pairs with a human score of how related or similar the two words are. We calculate the Spearman correlation (Spearman, 1904) between the labels and our scores generated by the embeddings. The Spearman correlation is a rank-based correlation measure that assesses how well the scores describe the true labels. The correlation results are shown in Table 2 using the scores generated from the expected likelihood kernel, maximum cosine similarity, and maximum Euclidean distance. We show the results of our Gaussian mixture model and compare the performance with that of word2vec and the original Gaussian embedding by Vilnis and McCallum (2014). We note that our model of a unimodal Gaussian embedding w2g also outperforms the original model, which differs in model hyperparameters and initialization, for most datasets. Our multi-prototype model w2gm also performs better than skip-gram or Gaussian embedding methods on many datasets, namely, WS, WS-R, MEN, MC, RG, YP, MT-287, RW. The maximum cosine similarity yields the best performance on most datasets; however, the minimum Euclidean distance is a better metric for the datasets MC and RW. These results are consistent for both the single-prototype and the multi-prototype models. We also compare out results on WordSim-353 with the multi-prototype embedding method by Huang et al. (2012), shown in Table 3. We observe that our single-prototype model w2g is competitive compared to models by Huang et al.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nDataset sg* w2g* w2g/mc w2g/el w2g/me w2gm/mc w2gm/el w2gm/me\nSL 29.39 32.23 29.35 25.44 25.43 29.31 26.02 27.59 WS 59.89 65.49 71.53 61.51 64.04 73.47 62.85 66.39 WS-S 69.86 76.15 76.70 70.57 72.3 76.73 70.08 73.3 WS-R 53.03 58.96 68.34 54.4 55.43 71.75 57.98 60.13 MEN 70.27 71.31 72.58 67.81 65.53 73.55 68.5 67.7 MC 63.96 70.41 76.48 72.70 80.66 79.08 76.75 80.33 RG 70.01 71 73.30 72.29 72.12 74.51 71.55 73.52 YP 39.34 41.5 41.96 38.38 36.41 45.07 39.18 38.58 MT-287 - - 64.79 57.5 58.31 66.60 57.24 60.61 MT-771 - - 60.86 55.89 54.12 60.82 57.26 56.43 RW - - 28.78 32.34 33.16 28.62 31.64 35.27\nTable 2: Spearman correlation for word similarity datasets. The models sg, w2g, w2gm denote word2vec skip-gram, Gaussian embedding, and Gaussian mixture embedding (K=2). The measures mc, el, me denote maximum cosine similarity, expected likelihood kernel, and minimum Euclidean distance. For each of w2g and w2gm, we underline the similarity metric with the best score. For each dataset, we boldface the score with the best performance across all models. We note that the correlation scores for sg*, w2g* are taken from Vilnis and McCallum (2014) and the scores for w2g and w2mg are trained with window size 10.\nMODEL ρ× 100 HUANG 64.2 HUANG* 71.3 W2G 70.9 W2GM 73.5\nTable 3: Spearman’s correlation (ρ) on WordSim353 datasets for our Word to Mixture of Gaussians (W2MG) embeddings, as well as the multiprototype embedding by Huang et al. (2012). Huang* is trained using data with all stop words removed. W2MG-(S,M) refers to our model with 1 or 2 mixture components.\n(2012), even without using a corpus with stop words removed. This could be due to the autocalibration of importance via the covariance learning which decrease the importance of very frequent words such as the, to, a, etc. Moreover, our multi-prototype model substantially outperforms the model of Huang et al. (2012) on the WordSim-353 dataset."
    }, {
      "heading" : "4.4 Word Similarity for Polysemous and Homonymous Words",
      "text" : "We use the dataset SCWS introduced by Huang et al. (2012), where word pairs are chosen to have variations in meanings of polysemous and homonymous words.\nWe compare our method with multiprototype models by Huang (Huang et al., 2012), Tian (Tian et al., 2014), and Chen (Chen et al., 2014).\nWe note that Chen model uses an external lexical source WordNet that gives it an extra advantage.\nWe use many metrics to calculate the scores for the Spearman correlation. MaxSim refers to the maximum cosine similarity. AveSim is the average of cosine similarities with respect to the component probabilities.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nMODEL SCORE BEST AP BEST F1 W2G (5) COS 73.1 76.4 W2G (5) KL 73.7 76.0 W2GM (5) COS 73.6 76.3 W2GM (5) KL 75.7 77.9\nW2G (10) COS 73.0 76.1 W2G (10) KL 74.2 76.1 W2GM (10) COS 72.9 75.6 W2GM (10) KL 74.7 76.3\nTable 5: Entailment results for models w2g and w2gm with window size 5 and 10. The metrics used are the maximum cosine similarity, or the maximum negative KL divergence. We calculate the best average precision as well as the best F1 score. w2gm consistently outperforms w2g for describing entailment."
    }, {
      "heading" : "4.5 Reduction in Variance of Polysemous Words",
      "text" : "One motivation for our Gaussian mixture embedding is to model word uncertainty more accurately than Gaussian embeddings, which can have overly large variances for polysemous words. We see that our Gaussian mixture model does indeed reduce the variances of each component for such words. For instance, we observe that the word rock in w2g also has much higher variance per dimension (e−1.8 ≈ 1.65) compared to that of Gaussian components of rock in w2gm (which has variance of roughly e−2.5 ≈ 0.82). We also see, in the next section, that the Gaussian mixture model has desirable qualitative behavior for word entailment."
    }, {
      "heading" : "4.6 Word Entailment",
      "text" : "We evaluate our embeddings on the word entailment dataset from Baroni et al. (2012). The lexical entailment between words is denoted by w1 |= w2 which means that all instances of w1 are w2. The entailment dataset contains positive pairs such as aircraft |= vehicle and negative pairs such as aircraft ̸|= insect.\nWe generate entailment scores of word pairs and find the best threshold, measured by Average Precision (AP) or F1 score, which identifies negative versus positive entailment. We use the maximum cosine similarity and the minimum KL divergence, d(f, g) = min\ni,j=1,...,K KL(f ||g), for en-\ntailment scores. The minimum KL divergence is similar to the maximum cosine similarity, but also incorporates the embedding uncertainty. And KL divergence is an asymmetric measure, which is\nmore suitable for certain tasks such as word entailment. For instance, w1 |= w2 does not imply w2 |= w1 such as aircraft |= vehicle and car |= vehicle. The difference betweenKL(w1||w2) versus KL(w2||w1) distinguishes which word distribution encompasses another distribution, a concept demonstrated in Figure 1. Table 5 shows the results of our w2gm model versus the Gaussian embedding model w2g. We observe a trend for both models with window size 5 and 10 that the KL metric yields improvement (both AP and F1) over cosine similarity. In addition, w2gm has a better performance compared to w2g. The multi-prototype model estimates the meaning uncertainty better since it is no longer constrained to be unimodal, leading to better characterizations of entailment. On the other hand, the Gaussian embedding model suffers from large variance problem for polysemous words, which results in less informative word distribution and inferior entailment scores."
    }, {
      "heading" : "5 Discussion",
      "text" : "We introduced a model that represents words with expressive multimodal distributions formed from Gaussian mixtures. To learn the properties of each mixture, we proposed an analytic energy function for combination with a maximum margin objective. The resulting embeddings capture different semantics of polysemous words, uncertainty, and entailment, and also perform favorably on word similarity benchmarks. Elsewhere, latent probabilistic representations are proving to be exceptionally valuable, able to capture nuances such as face angles with variational autoencoders (Kingma and Welling, 2013) or subtleties in painting strokes with the InfoGAN (Chen et al., 2016). Similarly, probabilistic word embeddings can capture a range of subtle meanings, and advance the state of the art in predictive tasks. In the future, such representations could also open the doors to a new suite of applications in language modelling, where word distributions are used as inputs to probabilistic LSTMs, or in decision functions where uncertainty matters.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    }, {
      "heading" : "A Supplementary Material",
      "text" : "A.1 Derivation of Expected Likelihood Kernel\nWe derive the form of expected likelihood kernel for Gaussian mixtures. Let f, g be Gaussian mixture distributions representing the words wf , wg . That is, f(x) =\n∑K i=1 piN (x;µf,i,Σf,i) and g(x) =∑K\ni=1 qiN (x;µg,i,Σg,i), ∑K i=1 pi = 1, and ∑K\ni=1 qi = 1. The expected likelihood kernel is given by\nEθ(f, g) = ∫ ( K∑ i=1 piN (x;µf,i,Σf,i) ) ·(\nK∑ j=1 qjN (x;µg,j ,Σg,j)\n) dx\n= K∑ i=1 K∑ j=1 piqj ∫ N (x;µf,i,Σf,i) · N (x;µg,j ,Σg,j) dx\n= K∑ i=1 K∑ j=1 piqjN (0;µf,i − µg,j ,Σf,i +Σg,j)\n= K∑ i=1 K∑ j=1 piqje ξi,j\nwhere we note that ∫ N (x;µi,Σi)N (x;µj ,Σj) dx = N (0, µi − µj ,Σi + Σj) (Vilnis and McCallum, 2014) and ξi,j is the log partial energy, given by equation 3.\nA.2 Implementation In this section we discuss practical details for training the proposed model. Our code is implemented with Tensorflow (Abadi et al., 2015) and will be made publicly available upon publication.\nWord Sampling We use a word sampling scheme similar to the implementation in word2vec (Mikolov et al., 2013a,b). We use window size 10, unless stated otherwise, to generate the the context words for a given center word.\nFrequent words such as the, a, to are not as meaningful as relatively more infrequent words such as dog, love, rock and we are often more interested in learning the semantics of the less frequently observed words.\nWe use the subsampling method to balance the importance of frequent words and rare words, which has been shown to improve the performance of learning word vectors (Mikolov et al., 2013b). This technique discards word wi with probability P (wi) = 1 − √ t/f(wi), where f(wi) is the frequency of word wi in the training corpus and t is a frequency threshold. We use t = 10−5 in our experiments, which is the recommended value for word2vec skip-gram on large datasets.\nTo generate negative context words, each word type wi is sampled according to a distribution Pn(wi) ∝ U(wi)3/4 which is a distorted unigram distribution U(wi)3/4 that also serves to diminish the relative importance of frequent words. Both subsampling and the negative distribution choice are proven effective in word2vec training (Mikolov et al., 2013b).\nReduction to Diagonal Covariance We use a diagonal Σ, in which case inverting the covariance matrix is trivial and computations are particularly efficient.\n1000\n1001\n1002\n1003\n1004\n1005\n1006\n1007\n1008\n1009\n1010\n1011\n1012\n1013\n1014\n1015\n1016\n1017\n1018\n1019\n1020\n1021\n1022\n1023\n1024\n1025\n1026\n1027\n1028\n1029\n1030\n1031\n1032\n1033\n1034\n1035\n1036\n1037\n1038\n1039\n1040\n1041\n1042\n1043\n1044\n1045\n1046\n1047\n1048\n1049\n1050\n1051\n1052\n1053\n1054\n1055\n1056\n1057\n1058\n1059\n1060\n1061\n1062\n1063\n1064\n1065\n1066\n1067\n1068\n1069\n1070\n1071\n1072\n1073\n1074\n1075\n1076\n1077\n1078\n1079\n1080\n1081\n1082\n1083\n1084\n1085\n1086\n1087\n1088\n1089\n1090\n1091\n1092\n1093\n1094\n1095\n1096\n1097\n1098\n1099\nLet df ,dg denote the diagonal vectors of Σf ,Σg The expression for ξi,j reduces to\nξi,j = − 1\n2 D∑ r=1 log(dpr + d q r)\n−1 2\n∑[ (µp,i − µq,j) ◦\n1\ndp + dq ◦ (µp,i − µq,j) ] where ◦ denotes element-wise multiplication. The spherical case which we use in all our experiments is similar since we simply replace a vector d with a single value.\nOptimization Constraint and Stability We optimize logd since each component of diagonal vector d is constrained to be positive. Similarly, we constrain the probability pi to be in [0, 1] and sum to 1 by optimizing over unconstrained scores si ∈ (−∞,∞) and using a softmax function to convert the scores to probability pi = e\nsi∑K j=1 e sj .\nThe loss computation can be numerically unstable if elements of the diagonal covariances are very small, due to the term log(dfr + dgr) and 1dq+dp . Therefore, we add a small constant ϵ = 10−4 so that dfr + dgr and dq + dp becomes dfr + d g r + ϵ and dq + dp + ϵ.\nIn addition, we observe that ξi,j can be very small which would result in eξi,j ≈ 0 up to machine precision. In order to stabilize the computation in eq. 2, we compute its equivalent form\nlogE(f, g) = ξi′,j′ + log K∑ j=1 K∑ i=1 piqje ξi,j−ξi′,j′\nwhere ξi′,j′ = maxi,j ξi,j .\nModel Parameters and Training Details In the loss function Lθ , we use a margin m = 1. We initialize the word embeddings with D dimension with a uni-\nform distribution over [− √ 3 D , √ 3 D ] so that the expectation of variance is 1 and the mean is zero. (LeCun et al., 1998) In our experiments, we use the embedding size D = 50, unless stated otherwise. We initialize each dimension of the diagonal matrix (or a single value for spherical case) with a constant value v = 0.05, which yields a faster convergence given the scale of the norm used above. We also initialize the mixture scores si to be 0 so that the initial probabilities are equal among all K components. We use a batch size of 128.\nWe also use a separate output embeddings in addition to input embeddings, similar to word2vec implementation (Mikolov et al., 2013a,b). That is, each word has two sets of distributions qI and qO , each of which is a Gaussian mixture. For a given pair of word and context (w, c), we use the input distribution qI for w (input word) and the output distribution qO for context c (output word). We optimize the parameters of both qI and qO and use the trained input distributions qI as our final word representations.\nWe use mini-batch asynchronous gradient descent with Adagrad (Duchi et al., 2011) which performs adaptive learning rate for each parameter. We also experiment with Adam (Kingma and Ba, 2014) which corrects the bias in adaptive gradient update of Adagrad and is proven very popular for most recent neural network models. However, we found that it is much slower than Adagrad (≈ 10 times). This is because the gradient computation of the model is relatively fast, so a complex gradient update algorithm such as Adam becomes the bottleneck in the optimization. Therefore, we choose to use Adagrad which allows us to better scale to large datasets. We use a linearly decreasing learning rate from 0.05 to 0.00001."
    } ],
    "references" : [ {
      "title" : "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "author" : [ "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng." ],
      "venue" : "Software available from tensorflow.org.",
      "citeRegEx" : "Warden et al\\.,? 2015",
      "shortCiteRegEx" : "Warden et al\\.",
      "year" : 2015
    }, {
      "title" : "Entailment above the word level in distributional semantics",
      "author" : [ "Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan." ],
      "venue" : "EACL 2012, 13th Conference of the European Chapter of the Association for Computational Linguistics, Avignon, France, April",
      "citeRegEx" : "Baroni et al\\.,? 2012",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2012
    }, {
      "title" : "The wacky wide web: a collection of very large linguistically processed web-crawled corpora",
      "author" : [ "Marco Baroni", "Silvia Bernardini", "Adriano Ferraresi", "Eros Zanchetta." ],
      "venue" : "Language Resources and Evaluation 43(3):209– 226. https://doi.org/10.1007/s10579-009-9081-4.",
      "citeRegEx" : "Baroni et al\\.,? 2009",
      "shortCiteRegEx" : "Baroni et al\\.",
      "year" : 2009
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Janvin." ],
      "venue" : "Journal of Machine Learning Research 3:1137– 1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Multimodal distributional semantics",
      "author" : [ "Elia Bruni", "Nam Khanh Tran", "Marco Baroni." ],
      "venue" : "J. Artif. Int. Res. 49(1):1–47.",
      "citeRegEx" : "Bruni et al\\.,? 2014",
      "shortCiteRegEx" : "Bruni et al\\.",
      "year" : 2014
    }, {
      "title" : "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "author" : [ "Xi Chen", "Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel." ],
      "venue" : "Advances in Neural Information Processing Systems 29: Annual",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "A unified model for word sense representation and disambiguation",
      "author" : [ "Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of",
      "citeRegEx" : "Chen et al\\.,? 2014",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "A unified architecture for natural language processing: deep neural networks with multitask learning",
      "author" : [ "Ronan Collobert", "JasonWeston." ],
      "venue" : "Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008. pages 160–167.",
      "citeRegEx" : "Collobert and JasonWeston.,? 2008",
      "shortCiteRegEx" : "Collobert and JasonWeston.",
      "year" : 2008
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John C. Duchi", "Elad Hazan", "Yoram Singer." ],
      "venue" : "Journal of Machine Learning Research 12:2121–2159.",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Placing search in context: the concept revisited",
      "author" : [ "Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin." ],
      "venue" : "ACM Trans. Inf. Syst. 20(1):116–131.",
      "citeRegEx" : "Finkelstein et al\\.,? 2002",
      "shortCiteRegEx" : "Finkelstein et al\\.",
      "year" : 2002
    }, {
      "title" : "Large-scale learning of word relatedness with constraints",
      "author" : [ "Guy Halawi", "Gideon Dror", "Evgeniy Gabrilovich", "Yehuda Koren." ],
      "venue" : "The 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’12, Beijing, China, August 12-16, 2012. pages",
      "citeRegEx" : "Halawi et al\\.,? 2012",
      "shortCiteRegEx" : "Halawi et al\\.",
      "year" : 2012
    }, {
      "title" : "Simlex999: Evaluating semantic models with (genuine) similarity estimation",
      "author" : [ "Felix Hill", "Roi Reichart", "Anna Korhonen." ],
      "venue" : "CoRR abs/1408.3456.",
      "citeRegEx" : "Hill et al\\.,? 2014",
      "shortCiteRegEx" : "Hill et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving word representations via global context and multiple word prototypes",
      "author" : [ "Eric H. Huang", "Richard Socher", "Christopher D. Manning", "Andrew Y. Ng." ],
      "venue" : "The 50th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, July 8-14, 2012,",
      "citeRegEx" : "Huang et al\\.,? 2012",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2012
    }, {
      "title" : "Probability product kernels",
      "author" : [ "Tony Jebara", "Risi Kondor", "Andrew Howard." ],
      "venue" : "Journal of Machine Learning Research 5:819–844.",
      "citeRegEx" : "Jebara et al\\.,? 2004",
      "shortCiteRegEx" : "Jebara et al\\.",
      "year" : 2004
    }, {
      "title" : "Optimizing search engines using clickthrough data",
      "author" : [ "Thorsten Joachims." ],
      "venue" : "Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, July 23-26, 2002, Edmonton, Alberta, Canada. pages 133–142.",
      "citeRegEx" : "Joachims.,? 2002",
      "shortCiteRegEx" : "Joachims.",
      "year" : 2002
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba." ],
      "venue" : "CoRR abs/1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Autoencoding variational bayes",
      "author" : [ "Diederik P. Kingma", "Max Welling." ],
      "venue" : "CoRR abs/1312.6114. http://arxiv.org/abs/1312.6114.",
      "citeRegEx" : "Kingma and Welling.,? 2013",
      "shortCiteRegEx" : "Kingma and Welling.",
      "year" : 2013
    }, {
      "title" : "Efficient backprop",
      "author" : [ "Y. LeCun", "L. Bottou", "G. Orr", "K. Muller." ],
      "venue" : "G. Orr and Muller K., editors, Neural Networks: Tricks of the trade. Springer.",
      "citeRegEx" : "LeCun et al\\.,? 1998",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Neural word embedding as implicit matrix factorization",
      "author" : [ "Omer Levy", "Yoav Goldberg." ],
      "venue" : "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada. pages",
      "citeRegEx" : "Levy and Goldberg.,? 2014",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "CoRR abs/1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukás Burget", "Jan Cernocký", "Sanjeev Khudanpur." ],
      "venue" : "INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, Makuhari, Chiba, Japan,",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Extensions of recurrent neural network language model",
      "author" : [ "Tomas Mikolov", "Stefan Kombrink", "Lukás Burget", "Jan Cernocký", "Sanjeev Khudanpur." ],
      "venue" : "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing,",
      "citeRegEx" : "Mikolov et al\\.,? 2011",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2011
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Process-",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Contextual Correlates of Semantic Similarity",
      "author" : [ "George A. Miller", "Walter G. Charles." ],
      "venue" : "Language & Cognitive Processes 6(1):1–28. https://doi.org/10.1080/01690969108406936.",
      "citeRegEx" : "Miller and Charles.,? 1991",
      "shortCiteRegEx" : "Miller and Charles.",
      "year" : 1991
    }, {
      "title" : "Infinite dimensional word embeddings",
      "author" : [ "Eric T. Nalisnick", "Sachin Ravi." ],
      "venue" : "CoRR abs/1511.05392.",
      "citeRegEx" : "Nalisnick and Ravi.,? 2015",
      "shortCiteRegEx" : "Nalisnick and Ravi.",
      "year" : 2015
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "A word at a time: Computing word relatedness using temporal semantic analysis",
      "author" : [ "Kira Radinsky", "Eugene Agichtein", "Evgeniy Gabrilovich", "Shaul Markovitch." ],
      "venue" : "Proceedings of the 20th International Conference on World Wide Web. WWW ’11, pages 337–346.",
      "citeRegEx" : "Radinsky et al\\.,? 2011",
      "shortCiteRegEx" : "Radinsky et al\\.",
      "year" : 2011
    }, {
      "title" : "Contextual correlates of synonymy",
      "author" : [ "Herbert Rubenstein", "John B. Goodenough." ],
      "venue" : "Commun. ACM 8(10):627– 633.",
      "citeRegEx" : "Rubenstein and Goodenough.,? 1965",
      "shortCiteRegEx" : "Rubenstein and Goodenough.",
      "year" : 1965
    }, {
      "title" : "Bayesian probabilistic matrix factorization using markov chain monte carlo",
      "author" : [ "Ruslan Salakhutdinov", "Andriy Mnih." ],
      "venue" : "Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008. pages 880–887.",
      "citeRegEx" : "Salakhutdinov and Mnih.,? 2008",
      "shortCiteRegEx" : "Salakhutdinov and Mnih.",
      "year" : 2008
    }, {
      "title" : "The proof and measurement of association between two things",
      "author" : [ "C. Spearman." ],
      "venue" : "American Journal of Psychology 15:88–103.",
      "citeRegEx" : "Spearman.,? 1904",
      "shortCiteRegEx" : "Spearman.",
      "year" : 1904
    }, {
      "title" : "A probabilistic model for learning multi-prototype word embeddings",
      "author" : [ "Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu." ],
      "venue" : "COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference:",
      "citeRegEx" : "Tian et al\\.,? 2014",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2014
    }, {
      "title" : "Word representations via gaussian embedding",
      "author" : [ "Luke Vilnis", "Andrew McCallum." ],
      "venue" : "CoRR abs/1412.6623.",
      "citeRegEx" : "Vilnis and McCallum.,? 2014",
      "shortCiteRegEx" : "Vilnis and McCallum.",
      "year" : 2014
    }, {
      "title" : "Verb similarity on the taxonomy of wordnet",
      "author" : [ "Dongqiang Yang", "David M.W. Powers." ],
      "venue" : "In the 3rd International WordNet Conference (GWC-06), Jeju Island, Korea.",
      "citeRegEx" : "Yang and Powers.,? 2006",
      "shortCiteRegEx" : "Yang and Powers.",
      "year" : 2006
    }, {
      "title" : "Understanding deep learning requires rethinking generalization",
      "author" : [ "Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals." ],
      "venue" : "CoRR abs/1611.03530. http://arxiv.org/abs/1611.03530.",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "x;μi,Σi)N (x;μj ,Σj) dx = N (0, μi − μj ,Σi + Σj) (Vilnis and McCallum, 2014) and ξi,j is the log partial energy, given by equation",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "∫,? \\Q2014\\E",
      "shortCiteRegEx" : "∫",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Modern approaches, by contrast, learn to map words with similar meanings to nearby points in a vector space (Mikolov et al., 2013a), from large datasets such as Wikipedia.",
      "startOffset" : 108,
      "endOffset" : 131
    }, {
      "referenceID" : 31,
      "context" : "However, since a Gaussian distribution can have only one mode, the learned uncertainty in this representation can be overly diffuse for words with multiple distinct meanings (polysemys), in order for the model to assign some density to any plausible semantics (Vilnis and McCallum, 2014).",
      "startOffset" : 260,
      "endOffset" : 287
    }, {
      "referenceID" : 14,
      "context" : "We learn all the parameters of this mixture model using a maximum margin energy-based ranking objective (Joachims, 2002; Vilnis and McCallum, 2014) (Section 3.",
      "startOffset" : 104,
      "endOffset" : 147
    }, {
      "referenceID" : 31,
      "context" : "We learn all the parameters of this mixture model using a maximum margin energy-based ranking objective (Joachims, 2002; Vilnis and McCallum, 2014) (Section 3.",
      "startOffset" : 104,
      "endOffset" : 147
    }, {
      "referenceID" : 13,
      "context" : "For analytic tractability with Gaussian mixtures, we use the inner product between probability distributions in a Hilbert space, known as the expected likelihood kernel (Jebara et al., 2004), as our energy function (Section 3.",
      "startOffset" : 169,
      "endOffset" : 190
    }, {
      "referenceID" : 17,
      "context" : "Modern approaches, by contrast, learn to map words with similar meanings to nearby points in a vector space (Mikolov et al., 2013a), from large datasets such as Wikipedia. These learned word embeddings have become ubiquitous in predictive tasks. Vilnis and McCallum (2014) recently proposed an alternative view, where words are represented by a whole probability distribution instead of a deterministic point vector.",
      "startOffset" : 109,
      "endOffset" : 273
    }, {
      "referenceID" : 3,
      "context" : "Other popular approaches use feedforward (Bengio et al., 2003) and recurrent neural network language models (Mikolov et al.",
      "startOffset" : 41,
      "endOffset" : 62
    }, {
      "referenceID" : 25,
      "context" : "A different approach to learning word embeddings is through factorization of word co-occurrence matrices such as GloVe embeddings (Pennington et al., 2014).",
      "startOffset" : 130,
      "endOffset" : 155
    }, {
      "referenceID" : 12,
      "context" : "A small body of work develops multiple deterministic embeddings that can capture polysemys, for example through a cluster centroid of context vectors (Huang et al., 2012), or an adapted skip-gram model with an EM algorithm to learn multiple latent representations per word Tian et al.",
      "startOffset" : 150,
      "endOffset" : 170
    }, {
      "referenceID" : 3,
      "context" : "Other popular approaches use feedforward (Bengio et al., 2003) and recurrent neural network language models (Mikolov et al., 2010, 2011; Collobert and Weston, 2008) to predict missing words in sentences, producing hidden layers that can act as word embeddings that encode semantic information. A different approach to learning word embeddings is through factorization of word co-occurrence matrices such as GloVe embeddings (Pennington et al., 2014). The matrix factorization approach has been shown to have an implicit connection with skip-gram and negative sampling Levy and Goldberg (2014). Bayesian matrix factorization where row and columns are modeled as Gaussians has been explored in Salakhutdinov and Mnih (2008) and provides a different probabilistic perspective of word embeddings.",
      "startOffset" : 42,
      "endOffset" : 593
    }, {
      "referenceID" : 3,
      "context" : "Other popular approaches use feedforward (Bengio et al., 2003) and recurrent neural network language models (Mikolov et al., 2010, 2011; Collobert and Weston, 2008) to predict missing words in sentences, producing hidden layers that can act as word embeddings that encode semantic information. A different approach to learning word embeddings is through factorization of word co-occurrence matrices such as GloVe embeddings (Pennington et al., 2014). The matrix factorization approach has been shown to have an implicit connection with skip-gram and negative sampling Levy and Goldberg (2014). Bayesian matrix factorization where row and columns are modeled as Gaussians has been explored in Salakhutdinov and Mnih (2008) and provides a different probabilistic perspective of word embeddings.",
      "startOffset" : 42,
      "endOffset" : 722
    }, {
      "referenceID" : 3,
      "context" : "Other popular approaches use feedforward (Bengio et al., 2003) and recurrent neural network language models (Mikolov et al., 2010, 2011; Collobert and Weston, 2008) to predict missing words in sentences, producing hidden layers that can act as word embeddings that encode semantic information. A different approach to learning word embeddings is through factorization of word co-occurrence matrices such as GloVe embeddings (Pennington et al., 2014). The matrix factorization approach has been shown to have an implicit connection with skip-gram and negative sampling Levy and Goldberg (2014). Bayesian matrix factorization where row and columns are modeled as Gaussians has been explored in Salakhutdinov and Mnih (2008) and provides a different probabilistic perspective of word embeddings. In exciting recent work, Vilnis and McCallum (2014) propose a Gaussian distribution to model each word.",
      "startOffset" : 42,
      "endOffset" : 845
    }, {
      "referenceID" : 3,
      "context" : "Other popular approaches use feedforward (Bengio et al., 2003) and recurrent neural network language models (Mikolov et al., 2010, 2011; Collobert and Weston, 2008) to predict missing words in sentences, producing hidden layers that can act as word embeddings that encode semantic information. A different approach to learning word embeddings is through factorization of word co-occurrence matrices such as GloVe embeddings (Pennington et al., 2014). The matrix factorization approach has been shown to have an implicit connection with skip-gram and negative sampling Levy and Goldberg (2014). Bayesian matrix factorization where row and columns are modeled as Gaussians has been explored in Salakhutdinov and Mnih (2008) and provides a different probabilistic perspective of word embeddings. In exciting recent work, Vilnis and McCallum (2014) propose a Gaussian distribution to model each word. Their approach is significantly more expressive than typical point embeddings, with the ability to represent concepts such as entailment, by having the distribution for one word (e.g. ‘music’) encompass the distributions for sets of related words (‘jazz’ and ‘pop’). However, with a unimodal distribution, their approach cannot capture multiple distinct meanings, much like most deterministic approaches. A small body of work develops multiple deterministic embeddings that can capture polysemys, for example through a cluster centroid of context vectors (Huang et al., 2012), or an adapted skip-gram model with an EM algorithm to learn multiple latent representations per word Tian et al. (2014). Another related work by Nalisnick and Ravi (2015) models embeddings in infinite-dimensional space where each embedding can gradually represent incremental word sense if complex meanings are observed.",
      "startOffset" : 42,
      "endOffset" : 1594
    }, {
      "referenceID" : 3,
      "context" : "Other popular approaches use feedforward (Bengio et al., 2003) and recurrent neural network language models (Mikolov et al., 2010, 2011; Collobert and Weston, 2008) to predict missing words in sentences, producing hidden layers that can act as word embeddings that encode semantic information. A different approach to learning word embeddings is through factorization of word co-occurrence matrices such as GloVe embeddings (Pennington et al., 2014). The matrix factorization approach has been shown to have an implicit connection with skip-gram and negative sampling Levy and Goldberg (2014). Bayesian matrix factorization where row and columns are modeled as Gaussians has been explored in Salakhutdinov and Mnih (2008) and provides a different probabilistic perspective of word embeddings. In exciting recent work, Vilnis and McCallum (2014) propose a Gaussian distribution to model each word. Their approach is significantly more expressive than typical point embeddings, with the ability to represent concepts such as entailment, by having the distribution for one word (e.g. ‘music’) encompass the distributions for sets of related words (‘jazz’ and ‘pop’). However, with a unimodal distribution, their approach cannot capture multiple distinct meanings, much like most deterministic approaches. A small body of work develops multiple deterministic embeddings that can capture polysemys, for example through a cluster centroid of context vectors (Huang et al., 2012), or an adapted skip-gram model with an EM algorithm to learn multiple latent representations per word Tian et al. (2014). Another related work by Nalisnick and Ravi (2015) models embeddings in infinite-dimensional space where each embedding can gradually represent incremental word sense if complex meanings are observed.",
      "startOffset" : 42,
      "endOffset" : 1645
    }, {
      "referenceID" : 31,
      "context" : "Bottom: An embedding model where each word is represented by a Gaussian distribution (Vilnis and McCallum, 2014).",
      "startOffset" : 85,
      "endOffset" : 112
    }, {
      "referenceID" : 19,
      "context" : "2 Skip-Gram The training objective for learning θ = {μ⃗w,i, pw,i,Σw,i} draws inspiration from the continuous skip-gram model (Mikolov et al., 2013a), where word embeddings are trained to maximize the probability of observing a word given another nearby word.",
      "startOffset" : 125,
      "endOffset" : 148
    }, {
      "referenceID" : 14,
      "context" : "We use a max-margin ranking objective (Joachims, 2002), used for Gaussian embeddings in Vilnis and McCallum (2014), which pushes the similarity of a word and its positive context higher than that of its negative context by a margin m:",
      "startOffset" : 38,
      "endOffset" : 54
    }, {
      "referenceID" : 14,
      "context" : "We use a max-margin ranking objective (Joachims, 2002), used for Gaussian embeddings in Vilnis and McCallum (2014), which pushes the similarity of a word and its positive context higher than that of its negative context by a margin m:",
      "startOffset" : 39,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : "In contrast to Vilnis and McCallum (2014), who use a KL energy function, we propose to use the expected likelihood kernel, which is a generalization of an inner product between vectors to an inner product between distributions (Jebara et al., 2004).",
      "startOffset" : 227,
      "endOffset" : 248
    }, {
      "referenceID" : 30,
      "context" : "In contrast to Vilnis and McCallum (2014), who use a KL energy function, we propose to use the expected likelihood kernel, which is a generalization of an inner product between vectors to an inner product between distributions (Jebara et al.",
      "startOffset" : 15,
      "endOffset" : 42
    }, {
      "referenceID" : 2,
      "context" : "5 billion tokens) and Wackypedia (1 billion tokens) (Baroni et al., 2009).",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 31,
      "context" : "We name our model as Word to Gaussian Mixture (w2gm) in constrast to Word to Gaussian (w2g) (Vilnis and McCallum, 2014).",
      "startOffset" : 92,
      "endOffset" : 119
    }, {
      "referenceID" : 31,
      "context" : "As a numerical example, the cosine similarity between rock and stone for the Gaussian representation of Vilnis and McCallum (2014) is only 0.",
      "startOffset" : 104,
      "endOffset" : 131
    }, {
      "referenceID" : 11,
      "context" : "We evaluate our embeddings on several standard word similarity datasets, namely, SimLex (Hill et al., 2014), WS or WordSim-353, WS-S (similarity), WS-R (relatedness) (Finkelstein et al.",
      "startOffset" : 88,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : ", 2014), WS or WordSim-353, WS-S (similarity), WS-R (relatedness) (Finkelstein et al., 2002), MEN (Bruni et al.",
      "startOffset" : 66,
      "endOffset" : 92
    }, {
      "referenceID" : 4,
      "context" : ", 2002), MEN (Bruni et al., 2014), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), YP (Yang and Powers, 2006), MTurk(287,-771) (Radinsky et al.",
      "startOffset" : 13,
      "endOffset" : 33
    }, {
      "referenceID" : 23,
      "context" : ", 2014), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), YP (Yang and Powers, 2006), MTurk(287,-771) (Radinsky et al.",
      "startOffset" : 12,
      "endOffset" : 38
    }, {
      "referenceID" : 27,
      "context" : ", 2014), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), YP (Yang and Powers, 2006), MTurk(287,-771) (Radinsky et al.",
      "startOffset" : 43,
      "endOffset" : 76
    }, {
      "referenceID" : 32,
      "context" : ", 2014), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), YP (Yang and Powers, 2006), MTurk(287,-771) (Radinsky et al.",
      "startOffset" : 81,
      "endOffset" : 104
    }, {
      "referenceID" : 26,
      "context" : ", 2014), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), YP (Yang and Powers, 2006), MTurk(287,-771) (Radinsky et al., 2011; Halawi et al., 2012), and RW (Luong et al.",
      "startOffset" : 122,
      "endOffset" : 166
    }, {
      "referenceID" : 10,
      "context" : ", 2014), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), YP (Yang and Powers, 2006), MTurk(287,-771) (Radinsky et al., 2011; Halawi et al., 2012), and RW (Luong et al.",
      "startOffset" : 122,
      "endOffset" : 166
    }, {
      "referenceID" : 29,
      "context" : "We calculate the Spearman correlation (Spearman, 1904) between the labels and our scores generated by the embeddings.",
      "startOffset" : 38,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : ", 2002), MEN (Bruni et al., 2014), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), YP (Yang and Powers, 2006), MTurk(287,-771) (Radinsky et al., 2011; Halawi et al., 2012), and RW (Luong et al.). Each dataset contains a list of word pairs with a human score of how related or similar the two words are. We calculate the Spearman correlation (Spearman, 1904) between the labels and our scores generated by the embeddings. The Spearman correlation is a rank-based correlation measure that assesses how well the scores describe the true labels. The correlation results are shown in Table 2 using the scores generated from the expected likelihood kernel, maximum cosine similarity, and maximum Euclidean distance. We show the results of our Gaussian mixture model and compare the performance with that of word2vec and the original Gaussian embedding by Vilnis and McCallum (2014). We note that our model of a unimodal Gaussian embedding w2g also outperforms the original model, which differs in model hyperparameters and initialization, for most datasets.",
      "startOffset" : 14,
      "endOffset" : 897
    }, {
      "referenceID" : 4,
      "context" : ", 2002), MEN (Bruni et al., 2014), MC (Miller and Charles, 1991), RG (Rubenstein and Goodenough, 1965), YP (Yang and Powers, 2006), MTurk(287,-771) (Radinsky et al., 2011; Halawi et al., 2012), and RW (Luong et al.). Each dataset contains a list of word pairs with a human score of how related or similar the two words are. We calculate the Spearman correlation (Spearman, 1904) between the labels and our scores generated by the embeddings. The Spearman correlation is a rank-based correlation measure that assesses how well the scores describe the true labels. The correlation results are shown in Table 2 using the scores generated from the expected likelihood kernel, maximum cosine similarity, and maximum Euclidean distance. We show the results of our Gaussian mixture model and compare the performance with that of word2vec and the original Gaussian embedding by Vilnis and McCallum (2014). We note that our model of a unimodal Gaussian embedding w2g also outperforms the original model, which differs in model hyperparameters and initialization, for most datasets. Our multi-prototype model w2gm also performs better than skip-gram or Gaussian embedding methods on many datasets, namely, WS, WS-R, MEN, MC, RG, YP, MT-287, RW. The maximum cosine similarity yields the best performance on most datasets; however, the minimum Euclidean distance is a better metric for the datasets MC and RW. These results are consistent for both the single-prototype and the multi-prototype models. We also compare out results on WordSim-353 with the multi-prototype embedding method by Huang et al. (2012), shown in Table 3.",
      "startOffset" : 14,
      "endOffset" : 1597
    }, {
      "referenceID" : 29,
      "context" : "Table 2: Spearman correlation for word similarity datasets. The models sg, w2g, w2gm denote word2vec skip-gram, Gaussian embedding, and Gaussian mixture embedding (K=2). The measures mc, el, me denote maximum cosine similarity, expected likelihood kernel, and minimum Euclidean distance. For each of w2g and w2gm, we underline the similarity metric with the best score. For each dataset, we boldface the score with the best performance across all models. We note that the correlation scores for sg*, w2g* are taken from Vilnis and McCallum (2014) and the scores for w2g and w2mg are trained with window size 10.",
      "startOffset" : 9,
      "endOffset" : 547
    }, {
      "referenceID" : 12,
      "context" : "Table 3: Spearman’s correlation (ρ) on WordSim353 datasets for our Word to Mixture of Gaussians (W2MG) embeddings, as well as the multiprototype embedding by Huang et al. (2012). Huang* is trained using data with all stop words removed.",
      "startOffset" : 158,
      "endOffset" : 178
    }, {
      "referenceID" : 12,
      "context" : "Moreover, our multi-prototype model substantially outperforms the model of Huang et al. (2012) on the WordSim-353 dataset.",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 12,
      "context" : "We compare our method with multiprototype models by Huang (Huang et al., 2012), Tian (Tian et al.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 30,
      "context" : ", 2012), Tian (Tian et al., 2014), and Chen (Chen et al.",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 6,
      "context" : ", 2014), and Chen (Chen et al., 2014).",
      "startOffset" : 18,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "We use the dataset SCWS introduced by Huang et al. (2012), where word pairs are chosen to have variations in meanings of polysemous and homonymous words.",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "6 Word Entailment We evaluate our embeddings on the word entailment dataset from Baroni et al. (2012). The lexical entailment between words is denoted by w1 |= w2 which means that all instances of w1 are w2.",
      "startOffset" : 81,
      "endOffset" : 102
    }, {
      "referenceID" : 16,
      "context" : "Elsewhere, latent probabilistic representations are proving to be exceptionally valuable, able to capture nuances such as face angles with variational autoencoders (Kingma and Welling, 2013) or subtleties in painting strokes with the InfoGAN (Chen et al.",
      "startOffset" : 164,
      "endOffset" : 190
    }, {
      "referenceID" : 5,
      "context" : "Elsewhere, latent probabilistic representations are proving to be exceptionally valuable, able to capture nuances such as face angles with variational autoencoders (Kingma and Welling, 2013) or subtleties in painting strokes with the InfoGAN (Chen et al., 2016).",
      "startOffset" : 242,
      "endOffset" : 261
    } ],
    "year" : 2017,
    "abstractText" : "Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.",
    "creator" : "LaTeX with hyperref package"
  }
}