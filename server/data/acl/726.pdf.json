{
  "name" : "726.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning a Neural Semantic Parser from User Feedback",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Existing semantic parsing approaches for building natural language interfaces to databases (NLIDBs) either use special-purpose intermediate meaning representations that lack the full expressivity of database query languages or require extensive feature engineering, making it difficult to deploy them in new domains. We present a robust approach to quickly and easily learn and deploy semantic parsers from scratch, whose performance improves over time based on user feedback and requires minimal intervention.\nTo learn these semantic parsers, we (1) adapt\nneural sequence models to map utterances directly to SQL thereby bypassing intermediate representations and taking full advantage of SQL’s querying capabilities, (2) immediately deploy the model online to solicit questions and user feedback on results to reduce SQL annotation efforts, and (3) use crowd workers from skilled markets to provide SQL annotations that can directly be used for model improvement, in addition to being easier and cheaper to obtain than logical meaning representations. We demonstrate the effectiveness of the complete approach by successfully learning a semantic parser for an academic domain by simply deploying it online for three days.\nThis type of interactive learning is related to a number of recent ideas in semantic parsing, including batch learning of models that directly produce programs (e.g. regular expressions (Locascio et al., 2016)), learning from paraphrases (often gathered through crowdsourcing (Wang et al.,\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n2015)), data augmentation (e.g. based on manually engineered semantic grammars (Jia and Liang, 2016)) and learning through direct interaction with users (e.g. where a single user teaches the model new concepts (Wang et al., 2016)). However, there are unique advantages to our approach, including showing (1) that non-linguists can write SQL to encode complex, compositional computations (e.g. see Fig 1),1 (2) that external paraphrase resources and the structure of facts from the target database itself can be used for effective data augmentation, and (3) that actual database users can effectively drive the overall learning by simply providing feedback about what the model is currently getting correct.\nOur experiments measure the performance of these learning advances, both in batch on existing datasets and through a simple online experiment for the full interactive setting. For the batch evaluation, we use sentences from the benchmark GeoQuery and ATIS domains, converted to contain SQL meaning representations. Our neural learning with data augmentation achieves near state-ofthe-art accuracies, despite the extra complexities of mapping directly to SQL. We also perform simulated interactive learning on this data, showing that with perfect user feedback our full approach could learn high quality parsers with only 55% of the data. Finally, we do a small scale online experiment for a new domain, academic paper metadata search, demonstrating that actual users can provide useful feedback and our full approach is an effective method for learning a high quality parser that continues to improve over time as it is used."
    }, {
      "heading" : "2 Related Work",
      "text" : "Although diverse meaning representation languages have been used with semantic parsers – such as regular expressions (Kushman and Barzilay, 2013; Locascio et al., 2016) and systems of equations (Kushman et al., 2014; Roy et al., 2016) – parsers for querying databases have typically used either logic programs (Zelle and Mooney, 1996), lambda calculus (Zettlemoyer and Collins,\n1Parsers can also be learned directly from questionanswer pairs (Liang et al., 2011). However, recent work has shown that in many domains, especially large databases, it is easier to write a query than to manually sort through the data and list the answers (Yih et al., 2016). Learning parsers directly from SQL queries has the added benefit that we can potentially hire programmers on skilled-labor crowd markets, such as UpWork, to further improve scalability, an exploration we leave to future work.\n2005), or λ-DCS (Liang et al., 2011) as the meaning representation language. All three of these languages are modeled after natural language to simplify parsing. However, none of them is used to query databases outside of the semantic parsing literature; therefore, they are understood by few people and not supported by standard database implementations. In contrast, we parse directly to SQL, which is a popular database query language with wide usage and support.\nA few systems have been developed to directly generate SQL queries from natural language (Popescu et al., 2003; Giordani and Moschitti, 2012; Poon, 2013). However, all of these systems make strong assumptions on the structure of queries: they use manually engineered rules that can only generate a subset of SQL, require lexical matches between question tokens and table/column names, or require questions to have a certain syntactic structure. In contrast, our approach can generate arbitrary SQL queries, only uses lexical matching for entity names, and does not depend on syntactic parsing.\nWe use a neural sequence-to-sequence model to directly generate SQL queries from natural language questions. This approach builds on recent work demonstrating that such models are effective for tasks such as machine translation (Bahdanau et al., 2014) and natural language generation (Kiddon et al., 2016). Recently, neural models have been successfully applied to semantic parsing with simpler meaning representation languages (Dong and Lapata, 2016; Jia and Liang, 2016) and short regular expressions (Locascio et al., 2016). Our work extends these results to the task of SQL generation. Finally, Ling et al. (2016) generate Java/Python code for trading cards given a natural language description; however, this system suffers from low overall accuracy.\nA final direction of related work studies methods for reducing the annotation effort required to train a semantic parser. Semantic parsers have been trained from various kinds of annotations, including labeled queries (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2005), question/answer pairs (Liang et al., 2011; Berant et al., 2013), distant supervision (Krishnamurthy and Mitchell, 2012; Choi et al., 2015), and binary correct/incorrect feedback signals (Clarke et al., 2010). Each of these schemes presents a particular trade-off between annotation\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\neffort and parser accuracy; however, recent work has suggested that labeled queries are the most effective (Yih et al., 2016). Our approach trains on fully labeled SQL queries to maximize accuracy, but uses binary feedback from users to reduce the number of queries that need to be labeled. Annotation effort can also be reduced by using crowd workers to paraphrase automatically generated questions (Wang et al., 2015); however, this approach may not generate the questions that users actually want to ask the database – an experiment in this paper demonstrated that 48% of users’ questions in a calendar domain could not be generated."
    }, {
      "heading" : "3 Feedback-based Learning",
      "text" : "Our feedback-based learning approach can be used to quickly deploy semantic parsers to create NLIDBs for any new domain. Is is a simple interactive learning algorithm that deploys a preliminary semantic parser, then iteratively improves this parser using user feedback and selective query annotation. A key requirement of this algorithm is the ability to cheaply and efficiently annotate queries for chosen user utterances. We address this requirement by developing a model that directly outputs SQL queries (Section 4), which can also be produced by crowd workers.\nOur algorithm alternates between stages of training the model and making predictions to gather user feedback, with the goal of improving performance in each successive stage. The procedure is described in Algorithm 1. Our neural model N is initially trained on synthetic data T generated by domain-independent schema templates (see Section 4), and is then ready to answer new user questions, n. The results R of executing the predicted SQL query q are presented to the user who provides a binary correct/incorrect feedback signal. If the user marks the result correct, the pair (n, q) is added to the training set. If the user marks the result incorrect, the algorithm asks a crowd worker to annotate the utterance with the correct query, q̂, and adds (n, q̂) to the training set. This procedure can be repeated indefinitely, ideally increasing parser accuracy and requesting fewer annotations in each successive stage."
    }, {
      "heading" : "4 Semantic Parsing to SQL",
      "text" : "We use a neural sequence-to-sequence model for mapping natural language questions directly\nAlgorithm 1 Feedback-based learning 1: procedure LEARN(schema) 2: T ← initial data(schema) 3: while true do 4: T ← T∪ paraphrase(T ) 5: N ← train model(T ) 6: for each n ∈ new utterances do 7: q ← predict(N , n) 8: R ← execute(q) 9: f ← feedback(R) 10: if f = correct then 11: T ← T ∪ (n, q) 12: else if f = wrong then 13: q̂ ← annotate(n) 14: T ← T ∪ (n, q̂)\nto SQL queries and this allows us to scale our feedback-based learning approach, by easily crowdsourcing labels when necessary. We further present two data augmentation techniques which use content from the database schema and external paraphrase resources."
    }, {
      "heading" : "4.1 Model",
      "text" : "We use an encoder-decoder model with global attention, similar to Luong et al. (2015), where the anonymized utterance is encoded using a bidirectional LSTM network, then decoded to directly predict SQL query tokens. Fixed pre-trained word embeddings from word2vec (Mikolov et al., 2013) are concatenated to the embeddings that are learned for source tokens from the training data. The decoder predicts a conditional probability distribution over possible values for the next SQL token given the previous tokens using a combination of the previous SQL token embedding, attention over the hidden states of the encoder network, and an attention signal from the previous time step.\nFormally, if qi represents an embedding for the ith SQL token qi, the decoder distribution is\np(qi|q1, . . . , qi−1) ∝ exp (W tanh(Ŵ[hi : ci])) where hi represents the hidden state output of the decoder LSTM at the ith timestep, ci represents the context vector generated using an attention weighted sum of encoder hidden states based on hi, and, W and Ŵ are linear transformations. If sj is the hidden representation generated by the encoder for the jth word in the utterance (k words\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nlong), then the context vectors are defined to be\nci = k∑\nj=1\nαi,j · sj\nThe attention weights αi,j are computed using an inner product between the decoder hidden state for the current timestep hi, and the hidden representation of the jth source token sj\nαi,j = exp(hiTFsj)∑k j=1 exp(hi TFsj)\nwhere F is a linear transformation. The decoder LSTM cell f computes the next hidden state hi, and cell state, mi, based on the previous hidden and cell states, hi−1,mi−1, the embeddings of the previous SQL token qi−1 and the context vector of the previous timestep, ci−1\nhi,mi = f(hi−1,mi−1,qi−1, ci−1)\nWe apply dropout on non-recurrent connections for regularization, as suggested by Zaremba et al. (2014). Beam search is used for decoding the SQL queries after learning."
    }, {
      "heading" : "4.2 Entity Anonymization",
      "text" : "We handle entities in the utterances and SQL by replacing them with their types, using incremental numbering to model multiple entities of the same type (e.g., CITY NAME 1). During training, when the SQL is available, we infer the type from the associated column name; for example, Boston is a city in city.city name = ’Boston’. To recognize entities in the utterances at test time, we build a search engine on all entities from the target database. For every span of words (starting with a high span size and progressively reducing it), we query the search engine using a TF-IDF scheme to retrieve the entity that most closely matches the span, then replace the span with the entity’s type. We store these mappings and apply them to the generated SQL to fill in the entity names. TF-IDF matching allows some flexibility in matching entity names in utterances, for example, a user could say Donald Knuth instead of Donald E. Knuth."
    }, {
      "heading" : "4.3 Data Augmentation",
      "text" : "We present two data augmentation strategies that either (1) provide the initial training data to start the interactive learning, before more labeled examples become available, or (2) use external paraphrase resources to improve generalization.\nGet all <ENT1>.<NAME> having <ENT2>.<COL1>.<NAME> as <ENT2>.<COL1>.<TYPE>\nSELECT <ENT1>.<DEF> FROM JOIN_FROM(<ENT1>, <ENT2>) WHERE JOIN_WHERE(<ENT1>, <ENT2>) AND <ENT2>.<COL1> = <ENT2>.<COL1>.<TYPE>\n(a) Schema template\nSELECT author.authorId FROM author , writes , paper , paperDataset , dataset WHERE author.authorId = writes.authorId AND writes.paperId = paper.paperId AND paper.paperId = paperDataset.paperId AND paperDataset.datasetId = dataset.datasetId AND dataset.datasetName = DATASET_TYPE\nGet all author having dataset as DATASET_TYPE\n(b) Generated utterance-SQL pair\nFigure 2: (a) Example schema template consisting of a question and SQL query with slots to be filled with database entities, columns, and values; (b) Entity-anonymized training example generated by applying the template to an academic database.\nSchema Templates To bootstrap the model to answer simple questions initially, we defined 22 language/SQL templates that are schema-agnostic, so they can be applied to any database. These templates contain slots whose values are populated given a database schema. An example template is shown in Figure 2a. The <ENT> types represent tables in the database schema, <ENT>.<COL> represents a column in the particular table and <ENT>.<COL>.<TYPE> represents the type associated with the particular column. A template is instantiated by first choosing the entities and attributes. Next, join conditions, i.e., JOIN FROM and JOIN WHERE clauses, are generated from the tables on the shortest path between the chosen tables in the database schema graph, which connects tables (graph nodes) using foreign key constraints. Figure 2b shows an instantiation of a template using the path author - writes - paper - paperdataset - dataset. SQL queries generated in this manner are guaranteed to be executable on the target database. On the language side, an English name of each entity is plugged into the template to generate an utterance for the query.\nParaphrasing The second data augmentation strategy uses the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) to automatically generate paraphrases of training utterances. Such methods have been recently used to improve performance for parsing to logical forms (Chen et al., 2016). PPDB contains over 220 million paraphrase pairs divided into 6 sets (small to XXXL)\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nbased on precision of the paraphrases. We use the one-one and one-many paraphrases from the large version of PPDB. To paraphrase a training utterance, we pick a random word in the utterance that is not a stop word and replace it with a random paraphrase. We perform paraphrase expansion on all examples labeled during learning, as well as the initial seed examples from schema templates."
    }, {
      "heading" : "5 Benchmark Experiments",
      "text" : "Our first set of experiments demonstrates that our semantic parsing model has comparable accuracy to previous work, despite the increased difficulty of directly producing SQL. We demonstrate this result by running our model on two benchmark datasets for semantic parsing, GEO880 and ATIS."
    }, {
      "heading" : "5.1 Data sets",
      "text" : "GEO880 is a collection of 880 utterances issued to a database of US geographical facts (Geobase), originally in Prolog format. Popescu et al. (2003) created a relational database schema for Geobase together with SQL queries for a subset of 700 utterances. To compare against prior work on the full corpus, we annotated the remaining utterances and used the standard 600/280 training/test split (Zettlemoyer and Collins, 2005).\nATIS is a collection of 5,418 utterances to a flight booking system, accompanied by a relational database and SQL queries to answer the questions. We use 4,473 utterances for training, 497 for development and 448 for test, following Kwiatkowski et al. (2011). The original SQL queries were very inefficient to execute due to the use of IN clauses, so we converted them to joins (Ramakrishnan and Gehrke, 2003) while verifying that the output of the queries was unchanged.\nTable 1 shows characteristics of both data sets. GEO880 has shorter queries but is more compositional: almost 40% of the SQL queries have at least one nested subquery. ATIS has the longest utterances and queries, with an average utterance length of 11 words and an average SQL query length of 67 tokens. They also operate on approximately 6 tables per query on average. We will release our processed versions of both datasets."
    }, {
      "heading" : "5.2 Experimental Methodology",
      "text" : "We follow a standard train/dev/test methodology for our experiments. The training set is augmented using schema templates and 3 paraphrases per\ntraining example, as described in Section 4. Utterances were anonymized by replacing them with their corresponding types and all words that occur only once were replaced by UNK symbols. The development set is used for hyperparameter tuning and early stopping. For GEO880, we use cross validation on the training set to tune hyperparameters. We used a minibatch size of 100 and used Adam (Kingma and Ba, 2014) with a learning rate of 0.001 for 70 epochs for all our experiments. We used a beam size of 5 for decoding. We report test set accuracy of our SQL query predictions by executing them on the target database and comparing the result with the true result."
    }, {
      "heading" : "5.3 Results",
      "text" : "Tables 2 and 3 show test accuracies based on denotations for our model on GEO880 and ATIS respectively, compared with previous work.2 To our knowledge, this is the first result on directly parsing to SQL to achieve comparable performance to prior work without using any database-specific feature engineering. Popescu et al. (2003) and Giordani and Moschitti (2012) also directly produce SQL queries but on a subset of 700 examples from GEO880. The former only works on semantically tractable utterances where words can be unambiguously mapped to schema elements, while the latter uses a reranking approach that also limits the complexity of SQL queries that can be handled. GUSP (Poon, 2013) creates an intermediate representation that is then deterministically converted to SQL to obtain an accuracy of 74.8% on ATIS, which is boosted to 83.5% using manually introduced disambiguation rules. However, it requires a lot of SQL specific engineering (for example, special nodes for argmax) and is hard to extend to more complex SQL queries.\nOn both datasets, our SQL model has slightly\n2Note that 2.8% of GEO880 and 5% ATIS gold test set SQL queries (before any processing) produced empty results.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n559\n560\n561\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nSystem Acc.\nOurs (SQL) 82.5\nPopescu et al. (2003) (SQL) 77.5∗ Giordani and Moschitti (2012) (SQL) 87.2∗\nDong and Lapata (2016) 84.6 † Jia and Liang (2016) 89.3 Liang et al. (2011) 91.1\nTable 2: Accuracy of SQL query results on the Geo880 corpus; ∗ use Geo700; convert to logical forms instead of SQL; † measure accuracy in terms of obtaining the correct logical form, other systems use denotations\nlower accuracy than the best non-SQL results. Most relevant to this work are the neural sequence based approaches of Dong and Lapata (2016) and Jia and Liang (2016). We note that Jia and Liang (2016) use a data recombination technique that boosts accuracy from 85.0 on GEO880 and 76.3 on ATIS; this technique is also compatible with our model. Our results demonstrate that these models are powerful enough to directly produce SQL queries. Thus, our methods enable us to utilize the full expressivity of the SQL language without any extensions that certain logical representations require to answer more complex queries. More importantly, it can be immediately deployed for users in new domains, with a large programming community available for annotation, and thus, fits effectively into a framework for interactive learning.\nWe perform ablation studies on the development sets (see Table 4) and find that paraphrasing using PPDB consistently helps boost performance. However, unlike in the interactive experiments (Section 6), data augmentation using schema templates does not improve performance in the fully supervised setting."
    }, {
      "heading" : "6 Interactive Learning Experiments",
      "text" : "In this section, we learn a semantic parser for an academic domain from scratch by deploying an online system using our interactive learning algorithm (Section 3). After three train-deploy cycles, the system correctly answered 63.51% of user’s questions. To our knowledge, this is the first effort to learn a semantic parser using a live system, and is enabled by our models that can directly parse language to SQL without manual intervention."
    }, {
      "heading" : "6.1 User Interface",
      "text" : "We developed a web interface for accepting natural language questions to an academic database from users, using our model to generate a SQL query, and displaying the results after execution. Several example utterances are also displayed to help users understand the domain. Together with the results of the generated SQL query, users are prompted to provide feedback which is used for interactive learning. Screenshots of our interface are included in our Supplementary Materials.\nCollecting accurate user feedback on predicted queries is a key challenge in the interactive learning setting for two reasons. First, the system’s results can be incorrect due to poor entity identification or incompleteness in the database, neither of which are under the semantic parser’s control. Second, it can be difficult for users to determine if the presented results are in fact correct. This determination is especially challenging if the system responds with the correct type of result, for example, if the user requests “papers at ACL 2016” and the system responds with all ACL papers.\nWe address this challenge by providing users with two assists for understanding the system’s behavior, and allowing users to provide more granular feedback than simply correct/incorrect.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nThe first assist is type highlighting, which highlights entities identified in the utterance, for example, “paper by Michael I. Jordan (AUTHOR) in ICRA (VENUE) in 2016 (YEAR).” This assist is especially helpful because the academic database contains noisy keyword and dataset tables that were automatically extracted from the papers. The second assist is utterance paraphrasing, which shows the user another utterance that maps to the same SQL query. For example, for the above query, the system may show “what papers does Michael I. Jordan (AUTHOR) have in ICRA (VENUE) in 2016 (YEAR).” This assist only appears if a matching query (after entity anonymization) exists in the model’s training set.\nUsing these assists and the predicted results, users are asked to select from five feedback options: Correct, Wrong Types, Incomplete Result, Wrong Result and Can’t Tell. The Correct and Wrong Result options represent scenarios when the user is satisfied with the result, or the result is identifiably wrong, respectively. Wrong Types indicates incorrect entity identification, which can be determined from type highlighting. Incomplete Result indicates that the query is correct but the result is not; this outcome can occur because the database is incomplete. Can’t Tell indicates that the user is unsure about the feedback to provide."
    }, {
      "heading" : "6.2 Three-Stage Online Experiment",
      "text" : "In this experiment, using our developed user interface, we use Algorithm 1 to learn a semantic parser from scratch. The experiment had three stages; in each stage, we recruited 10 new users and asked them to issue at least 10 utterances to the system and to provide feedback on the results. We considered results marked as either Correct or Incomplete Result as correct queries for learning. The remaining incorrect utterances were sent to a crowd worker for annotation and were used to retrain the system for the next stage.\nTable 5 shows the percent of utterances judged by users as either Correct or Incomplete Result in each stage. In the first stage, we do not have any labeled examples, and the model is trained using only synthetically generated data from schema templates and paraphrases (see Section 4.3). Despite the lack of real examples, the system correctly answers 25% of questions. The system’s accuracy increases and annotation effort decreases in each successive stage as additional utterances are\nFeedback Error Rate (%)\nCorrect SQL 6.1 Incorrect SQL 6.3\nTable 6: Error rates of user feedback when the SQL is correct and incorrect. The Correct and Incomplete results options are erroneous if the SQL query is correct, and vice versa for incorrect queries.\ncontributed and incorrect utterances are labeled. This result demonstrates that we can successfully build semantic parsers for new domains by using neural models to generate SQL with crowdsourced annotations driven by user feedback.\nWe analyzed the feedback signals provided by the users in the final stage of the experiment to measure the quality of feedback. We found that 22.3% of the generated queries did not execute (and hence were incorrect). 6.1% of correctly generated queries were marked wrong by users (see Table 6). This erroneous feedback results in redundant annotation of already correct examples. The main cause of this erroneous feedback was incomplete data for aggregation queries, where users chose Wrong instead of Incomplete. 6.3% of incorrect queries were erroneously deemed correct by users. It is important that this fraction be low, as these queries become incorrectly-labeled examples in the training set that may contribute to the deterioration of model accuracy over time. This quality of feedback is already sufficient for our neural models to improve with usage, and creating better interfaces to make feedback more accurate is an important task for future work."
    }, {
      "heading" : "6.3 SCHOLAR dataset",
      "text" : "We release a new semantic parsing dataset for academic database search using the utterances gathered in the user study. We augment these labeled utterances with additional utterances labeled by crowd workers. (Note that these additional utterances were not used in the online experiment). The final dataset comprises 816 natural language utterances labeled with SQL, divided\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n0 2 4 6 8 10 12 Stages\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFr a ct io n C o rr e ct\nSimulated Interactive Learning on Geo880\nOurs Without templates Without paraphrasing\n1 2 3 4 5 6 7 8 9 10 Stages\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFr a ct io n C o rr e ct\nSimulated Interactive Learning on ATIS\nOurs Without templates Without paraphrasing\nFigure 3: Accuracy as a function of batch number in simulated interactive learning experiments on Geo880 (top) and ATIS (bottom).\ninto a 600/216 train/test split. We also provide a database on which to execute these queries containing academic papers with their authors, citations, journals, keywords and datasets used. Table 1 shows statistics of this data set. Our parser achieves an accuracy of 67% on this train/test split in the fully supervised setting."
    }, {
      "heading" : "6.4 Simulated Interactive Experiments",
      "text" : "We conducted additional simulated interactive learning experiments using GEO880 and ATIS to better understand the behavior of our train-deploy feedback loop, the effects of our data augmentation approaches, and the annotation effort required. We randomly divide each training set into K batches and present these batches sequentially to our interactive learning algorithm. Correctness feedback is provided by comparing the result of the predicted query to the gold query, i.e., we assume that users are able to perfectly distinguish correct results from incorrect ones.\nFigure 3 shows accuracies on GEO880 and ATIS respectively of each batch when the model is trained on all previous batches. As in the live experiment, accuracy improves with successive batches. Data augmentation using templates helps\nBatch Size 150 100 50\n% Wrong 70.2 60.4 54.3\nTable 7: Percentage of examples that required annotation (i.e., where the model initially made an incorrect prediction) on GEO880 vs. batch size.\nin the initial stages of GEO880, but its advantage is reduced as more labeled data is obtained. Templates did not improve accuracy on ATIS, possibly because most ATIS queries involve two entities, i.e., a source city and a destination city, whereas our templates only generate questions with a single entity type. Nevertheless, templates are important in a live system to motivate users to interact with it in early stages. As observed before, paraphrasing improves performance at all stages.\nTable 7 shows the percent of examples that require annotation using various batch sizes for GEO880. Smaller batch sizes reduce annotation effort, with a batch size of 50 requiring only 54.3% of the examples to be annotated. This result demonstrates that more frequent deployments of improved models leads to fewer mistakes."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We describe an approach to rapidly train a semantic parser as a NLIDB that iteratively improves parser accuracy over time while requiring minimal intervention. Our approach uses an attentionbased neural sequence-to-sequence model, with data augmentation from the target database and paraphrasing, to parse utterances to SQL. This model is deployed in an online system, where user feedback on its predictions is used to select utterances to send for crowd worker annotation.\nWe find that the semantic parsing model is comparable in performance to previous systems that either map from utterances to logical forms, or generate SQL, on two benchmark datasets, GEO880 and ATIS. We further demonstrate the effectiveness of our online system by learning a semantic parser from scratch for an academic domain. A key advantage of our approach is that it is not language-specific, and can easily be ported to other commonly- used query languages, such as SPARQL or ElasticSearch. Finally, we also release a new dataset of utterances and SQL queries for an academic domain.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473 .",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Semantic parsing on freebase from question-answer pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang." ],
      "venue" : "EMNLP. volume 2, page 6.",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "Sentence rewriting for semantic parsing",
      "author" : [ "Bo Chen", "Le Sun", "Xianpei Han", "Bo An." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computa-",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Scalable semantic parsing with partial ontologies",
      "author" : [ "Eunsol Choi", "Tom Kwiatkowski", "Luke Zettlemoyer." ],
      "venue" : "Proceedings of the 2015 Association for Computational Linguistics.",
      "citeRegEx" : "Choi et al\\.,? 2015",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2015
    }, {
      "title" : "Driving semantic parsing from the world’s response",
      "author" : [ "James Clarke", "Dan Goldwasser", "Ming-Wei Chang", "Dan Roth." ],
      "venue" : "Proceedings of the fourteenth conference on computational natural language learning. Association for Computational Lin-",
      "citeRegEx" : "Clarke et al\\.,? 2010",
      "shortCiteRegEx" : "Clarke et al\\.",
      "year" : 2010
    }, {
      "title" : "Language to logical form with neural attention",
      "author" : [ "Li Dong", "Mirella Lapata." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).",
      "citeRegEx" : "Dong and Lapata.,? 2016",
      "shortCiteRegEx" : "Dong and Lapata.",
      "year" : 2016
    }, {
      "title" : "PPDB: The paraphrase database",
      "author" : [ "Juri Ganitkevitch", "Benjamin Van Durme", "Chris Callison-Burch." ],
      "venue" : "Proceedings of NAACL-HLT . Association for Computational Linguistics, Atlanta, Georgia, pages 758–764. http://cs.jhu.edu/ ccb/publica-",
      "citeRegEx" : "Ganitkevitch et al\\.,? 2013",
      "shortCiteRegEx" : "Ganitkevitch et al\\.",
      "year" : 2013
    }, {
      "title" : "Translating questions to SQL queries with generative parsers discriminatively reranked",
      "author" : [ "Alessandra Giordani", "Alessandro Moschitti." ],
      "venue" : "Proceedings of COLING 2012: Posters. pages 401–410.",
      "citeRegEx" : "Giordani and Moschitti.,? 2012",
      "shortCiteRegEx" : "Giordani and Moschitti.",
      "year" : 2012
    }, {
      "title" : "Data recombination for neural semantic parsing",
      "author" : [ "Robin Jia", "Percy Liang." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).",
      "citeRegEx" : "Jia and Liang.,? 2016",
      "shortCiteRegEx" : "Jia and Liang.",
      "year" : 2016
    }, {
      "title" : "Globally coherent text generation with neural checklist models",
      "author" : [ "Chloé Kiddon", "Luke Zettlemoyer", "Yejin Choi." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Compu-",
      "citeRegEx" : "Kiddon et al\\.,? 2016",
      "shortCiteRegEx" : "Kiddon et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980 .",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Weakly supervised training of semantic parsers",
      "author" : [ "Jayant Krishnamurthy", "Tom M. Mitchell." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning.",
      "citeRegEx" : "Krishnamurthy and Mitchell.,? 2012",
      "shortCiteRegEx" : "Krishnamurthy and Mitchell.",
      "year" : 2012
    }, {
      "title" : "Learning to automatically solve algebra word problems",
      "author" : [ "Nate Kushman", "Yoav Artzi", "Luke Zettlemoyer", "Regina Barzilay." ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume",
      "citeRegEx" : "Kushman et al\\.,? 2014",
      "shortCiteRegEx" : "Kushman et al\\.",
      "year" : 2014
    }, {
      "title" : "Using semantic unification to generate regular expressions from natural language",
      "author" : [ "Nate Kushman", "Regina Barzilay." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human",
      "citeRegEx" : "Kushman and Barzilay.,? 2013",
      "shortCiteRegEx" : "Kushman and Barzilay.",
      "year" : 2013
    }, {
      "title" : "Lexical generalization in ccg grammar induction for semantic parsing",
      "author" : [ "Tom Kwiatkowski", "Luke Zettlemoyer", "Sharon Goldwater", "Mark Steedman." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Asso-",
      "citeRegEx" : "Kwiatkowski et al\\.,? 2011",
      "shortCiteRegEx" : "Kwiatkowski et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning dependency-based compositional semantics",
      "author" : [ "Percy Liang", "Michael I Jordan", "Dan Klein." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association",
      "citeRegEx" : "Liang et al\\.,? 2011",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2011
    }, {
      "title" : "Latent predictor networks for code generation",
      "author" : [ "Wang Ling", "Phil Blunsom", "Edward Grefenstette", "Karl Moritz Hermann", "Tomáš Kočiský", "Fumin Wang", "Andrew Senior." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Compu-",
      "citeRegEx" : "Ling et al\\.,? 2016",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural generation of regular expressions from natural language with minimal domain knowledge",
      "author" : [ "Nicholas Locascio", "Karthik Narasimhan", "Eduardo De Leon", "Nate Kushman", "Regina Barzilay." ],
      "venue" : "Proceedings of the 2016",
      "citeRegEx" : "Locascio et al\\.,? 2016",
      "shortCiteRegEx" : "Locascio et al\\.",
      "year" : 2016
    }, {
      "title" : "Effective approaches to attention-based neural machine translation",
      "author" : [ "Thang Luong", "Hieu Pham", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. pages 1412–1421.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Advances in neural information processing systems. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Grounded unsupervised semantic parsing",
      "author" : [ "Hoifung Poon." ],
      "venue" : "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).",
      "citeRegEx" : "Poon.,? 2013",
      "shortCiteRegEx" : "Poon.",
      "year" : 2013
    }, {
      "title" : "Towards a theory of natural language interfaces to databases",
      "author" : [ "Ana-Maria Popescu", "Oren Etzioni", "Henry Kautz." ],
      "venue" : "Proceedings of the 8th international conference on Intelligent user interfaces. ACM, pages 149–157.",
      "citeRegEx" : "Popescu et al\\.,? 2003",
      "shortCiteRegEx" : "Popescu et al\\.",
      "year" : 2003
    }, {
      "title" : "Database Management Systems",
      "author" : [ "Raghu Ramakrishnan", "Johannes Gehrke." ],
      "venue" : "McGraw-Hill, Inc., New York, NY, USA, 3 edition.",
      "citeRegEx" : "Ramakrishnan and Gehrke.,? 2003",
      "shortCiteRegEx" : "Ramakrishnan and Gehrke.",
      "year" : 2003
    }, {
      "title" : "Equation parsing : Mapping sentences to grounded equations",
      "author" : [ "Subhro Roy", "Shyam Upadhyay", "Dan Roth." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Roy et al\\.,? 2016",
      "shortCiteRegEx" : "Roy et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning language games through interaction",
      "author" : [ "S.I. Wang", "P. Liang", "C. Manning." ],
      "venue" : "Association for Computational Linguistics (ACL).",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Building a semantic parser overnight",
      "author" : [ "Yushi Wang", "Jonathan Berant", "Percy Liang." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Pro-",
      "citeRegEx" : "Wang et al\\.,? 2015",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Generation by inverting a semantic parser that uses statistical machine translation",
      "author" : [ "Yuk Wah Wong", "Raymond J Mooney." ],
      "venue" : "In Proceedings of the 2007 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
      "citeRegEx" : "Wong and Mooney.,? 2007",
      "shortCiteRegEx" : "Wong and Mooney.",
      "year" : 2007
    }, {
      "title" : "The value of semantic parse labeling for knowledge base question answering",
      "author" : [ "Wen-tau Yih", "Matthew Richardson", "Chris Meek", "MingWei Chang", "Jina Suh." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Yih et al\\.,? 2016",
      "shortCiteRegEx" : "Yih et al\\.",
      "year" : 2016
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1409.2329 .",
      "citeRegEx" : "Zaremba et al\\.,? 2014",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning to parse database queries using inductive logic programming",
      "author" : [ "John M. Zelle", "Raymond J. Mooney." ],
      "venue" : "Proceedings of the Thirteenth National Conference on Artificial Intelligence.",
      "citeRegEx" : "Zelle and Mooney.,? 1996",
      "shortCiteRegEx" : "Zelle and Mooney.",
      "year" : 1996
    }, {
      "title" : "Learning to map sentences to logical form: structured classification with probabilistic categorial grammars",
      "author" : [ "Luke S. Zettlemoyer", "Michael Collins." ],
      "venue" : "UAI ’05, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence.",
      "citeRegEx" : "Zettlemoyer and Collins.,? 2005",
      "shortCiteRegEx" : "Zettlemoyer and Collins.",
      "year" : 2005
    }, {
      "title" : "Online learning of relaxed ccg grammars for parsing to logical form",
      "author" : [ "Luke S Zettlemoyer", "Michael Collins." ],
      "venue" : "EMNLP-CoNLL. pages 678–687.",
      "citeRegEx" : "Zettlemoyer and Collins.,? 2007",
      "shortCiteRegEx" : "Zettlemoyer and Collins.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 17,
      "context" : "regular expressions (Locascio et al., 2016)), learning from paraphrases (often gathered through crowdsourcing (Wang et al.",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "based on manually engineered semantic grammars (Jia and Liang, 2016)) and learning through direct interaction with users (e.",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 24,
      "context" : "where a single user teaches the model new concepts (Wang et al., 2016)).",
      "startOffset" : 51,
      "endOffset" : 70
    }, {
      "referenceID" : 13,
      "context" : "Although diverse meaning representation languages have been used with semantic parsers – such as regular expressions (Kushman and Barzilay, 2013; Locascio et al., 2016) and systems of equations (Kushman et al.",
      "startOffset" : 117,
      "endOffset" : 168
    }, {
      "referenceID" : 17,
      "context" : "Although diverse meaning representation languages have been used with semantic parsers – such as regular expressions (Kushman and Barzilay, 2013; Locascio et al., 2016) and systems of equations (Kushman et al.",
      "startOffset" : 117,
      "endOffset" : 168
    }, {
      "referenceID" : 12,
      "context" : ", 2016) and systems of equations (Kushman et al., 2014; Roy et al., 2016) – parsers for querying databases have typically used either logic programs (Zelle and Mooney, 1996), lambda calculus (Zettlemoyer and Collins,",
      "startOffset" : 33,
      "endOffset" : 73
    }, {
      "referenceID" : 23,
      "context" : ", 2016) and systems of equations (Kushman et al., 2014; Roy et al., 2016) – parsers for querying databases have typically used either logic programs (Zelle and Mooney, 1996), lambda calculus (Zettlemoyer and Collins,",
      "startOffset" : 33,
      "endOffset" : 73
    }, {
      "referenceID" : 29,
      "context" : ", 2016) – parsers for querying databases have typically used either logic programs (Zelle and Mooney, 1996), lambda calculus (Zettlemoyer and Collins,",
      "startOffset" : 83,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "Parsers can also be learned directly from questionanswer pairs (Liang et al., 2011).",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 27,
      "context" : "However, recent work has shown that in many domains, especially large databases, it is easier to write a query than to manually sort through the data and list the answers (Yih et al., 2016).",
      "startOffset" : 171,
      "endOffset" : 189
    }, {
      "referenceID" : 15,
      "context" : "2005), or λ-DCS (Liang et al., 2011) as the meaning representation language.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 21,
      "context" : "A few systems have been developed to directly generate SQL queries from natural language (Popescu et al., 2003; Giordani and Moschitti, 2012; Poon, 2013).",
      "startOffset" : 89,
      "endOffset" : 153
    }, {
      "referenceID" : 7,
      "context" : "A few systems have been developed to directly generate SQL queries from natural language (Popescu et al., 2003; Giordani and Moschitti, 2012; Poon, 2013).",
      "startOffset" : 89,
      "endOffset" : 153
    }, {
      "referenceID" : 20,
      "context" : "A few systems have been developed to directly generate SQL queries from natural language (Popescu et al., 2003; Giordani and Moschitti, 2012; Poon, 2013).",
      "startOffset" : 89,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : "This approach builds on recent work demonstrating that such models are effective for tasks such as machine translation (Bahdanau et al., 2014) and natural language generation (Kiddon et al.",
      "startOffset" : 119,
      "endOffset" : 142
    }, {
      "referenceID" : 9,
      "context" : ", 2014) and natural language generation (Kiddon et al., 2016).",
      "startOffset" : 40,
      "endOffset" : 61
    }, {
      "referenceID" : 5,
      "context" : "Recently, neural models have been successfully applied to semantic parsing with simpler meaning representation languages (Dong and Lapata, 2016; Jia and Liang, 2016) and short regular expressions (Locascio et al.",
      "startOffset" : 121,
      "endOffset" : 165
    }, {
      "referenceID" : 8,
      "context" : "Recently, neural models have been successfully applied to semantic parsing with simpler meaning representation languages (Dong and Lapata, 2016; Jia and Liang, 2016) and short regular expressions (Locascio et al.",
      "startOffset" : 121,
      "endOffset" : 165
    }, {
      "referenceID" : 17,
      "context" : "Recently, neural models have been successfully applied to semantic parsing with simpler meaning representation languages (Dong and Lapata, 2016; Jia and Liang, 2016) and short regular expressions (Locascio et al., 2016).",
      "startOffset" : 196,
      "endOffset" : 219
    }, {
      "referenceID" : 29,
      "context" : "Semantic parsers have been trained from various kinds of annotations, including labeled queries (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2005), question/answer pairs (Liang et al.",
      "startOffset" : 96,
      "endOffset" : 174
    }, {
      "referenceID" : 26,
      "context" : "Semantic parsers have been trained from various kinds of annotations, including labeled queries (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2005), question/answer pairs (Liang et al.",
      "startOffset" : 96,
      "endOffset" : 174
    }, {
      "referenceID" : 30,
      "context" : "Semantic parsers have been trained from various kinds of annotations, including labeled queries (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2005), question/answer pairs (Liang et al.",
      "startOffset" : 96,
      "endOffset" : 174
    }, {
      "referenceID" : 15,
      "context" : "Semantic parsers have been trained from various kinds of annotations, including labeled queries (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2005), question/answer pairs (Liang et al., 2011; Berant et al., 2013), distant supervision (Krishnamurthy and Mitchell, 2012; Choi et al.",
      "startOffset" : 198,
      "endOffset" : 239
    }, {
      "referenceID" : 1,
      "context" : "Semantic parsers have been trained from various kinds of annotations, including labeled queries (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2005), question/answer pairs (Liang et al., 2011; Berant et al., 2013), distant supervision (Krishnamurthy and Mitchell, 2012; Choi et al.",
      "startOffset" : 198,
      "endOffset" : 239
    }, {
      "referenceID" : 11,
      "context" : ", 2013), distant supervision (Krishnamurthy and Mitchell, 2012; Choi et al., 2015), and binary correct/incorrect feedback signals (Clarke et al.",
      "startOffset" : 29,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : ", 2013), distant supervision (Krishnamurthy and Mitchell, 2012; Choi et al., 2015), and binary correct/incorrect feedback signals (Clarke et al.",
      "startOffset" : 29,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : ", 2015), and binary correct/incorrect feedback signals (Clarke et al., 2010).",
      "startOffset" : 55,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "This approach builds on recent work demonstrating that such models are effective for tasks such as machine translation (Bahdanau et al., 2014) and natural language generation (Kiddon et al., 2016). Recently, neural models have been successfully applied to semantic parsing with simpler meaning representation languages (Dong and Lapata, 2016; Jia and Liang, 2016) and short regular expressions (Locascio et al., 2016). Our work extends these results to the task of SQL generation. Finally, Ling et al. (2016) generate Java/Python code for trading cards given a natural language description; however, this system suffers from low overall accuracy.",
      "startOffset" : 120,
      "endOffset" : 509
    }, {
      "referenceID" : 27,
      "context" : "effort and parser accuracy; however, recent work has suggested that labeled queries are the most effective (Yih et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 125
    }, {
      "referenceID" : 25,
      "context" : "Annotation effort can also be reduced by using crowd workers to paraphrase automatically generated questions (Wang et al., 2015); however, this approach may not generate the questions that users actually want to ask the database – an experiment in this paper demonstrated that 48% of users’ questions in a calendar domain could not be generated.",
      "startOffset" : 109,
      "endOffset" : 128
    }, {
      "referenceID" : 19,
      "context" : "Fixed pre-trained word embeddings from word2vec (Mikolov et al., 2013) are concatenated to the embeddings that are learned for source tokens from the training data.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 18,
      "context" : "We use an encoder-decoder model with global attention, similar to Luong et al. (2015), where the anonymized utterance is encoded using a bidirectional LSTM network, then decoded to directly predict SQL query tokens.",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 28,
      "context" : "The decoder LSTM cell f computes the next hidden state hi, and cell state, mi, based on the previous hidden and cell states, hi−1,mi−1, the embeddings of the previous SQL token qi−1 and the context vector of the previous timestep, ci−1 hi,mi = f(hi−1,mi−1,qi−1, ci−1) We apply dropout on non-recurrent connections for regularization, as suggested by Zaremba et al. (2014). Beam search is used for decoding the SQL queries after learning.",
      "startOffset" : 350,
      "endOffset" : 372
    }, {
      "referenceID" : 6,
      "context" : "Paraphrasing The second data augmentation strategy uses the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) to automatically generate paraphrases of training utterances.",
      "startOffset" : 87,
      "endOffset" : 114
    }, {
      "referenceID" : 2,
      "context" : "Such methods have been recently used to improve performance for parsing to logical forms (Chen et al., 2016).",
      "startOffset" : 89,
      "endOffset" : 108
    }, {
      "referenceID" : 30,
      "context" : "To compare against prior work on the full corpus, we annotated the remaining utterances and used the standard 600/280 training/test split (Zettlemoyer and Collins, 2005).",
      "startOffset" : 138,
      "endOffset" : 169
    }, {
      "referenceID" : 22,
      "context" : "The original SQL queries were very inefficient to execute due to the use of IN clauses, so we converted them to joins (Ramakrishnan and Gehrke, 2003) while verifying that the output of the queries was unchanged.",
      "startOffset" : 118,
      "endOffset" : 149
    }, {
      "referenceID" : 20,
      "context" : "Popescu et al. (2003) created a relational database schema for Geobase together with SQL queries for a subset of 700 utterances.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 14,
      "context" : "We use 4,473 utterances for training, 497 for development and 448 for test, following Kwiatkowski et al. (2011). The original SQL queries were very inefficient to execute due to the use of IN clauses, so we converted them to joins (Ramakrishnan and Gehrke, 2003) while verifying that the output of the queries was unchanged.",
      "startOffset" : 86,
      "endOffset" : 112
    }, {
      "referenceID" : 10,
      "context" : "We used a minibatch size of 100 and used Adam (Kingma and Ba, 2014) with a learning rate of 0.",
      "startOffset" : 46,
      "endOffset" : 67
    }, {
      "referenceID" : 20,
      "context" : "GUSP (Poon, 2013) creates an intermediate representation that is then deterministically converted to SQL to obtain an accuracy of 74.",
      "startOffset" : 5,
      "endOffset" : 17
    }, {
      "referenceID" : 19,
      "context" : "Popescu et al. (2003) and Giordani and Moschitti (2012) also directly produce SQL queries but on a subset of 700 examples from GEO880.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "(2003) and Giordani and Moschitti (2012) also directly produce SQL queries but on a subset of 700 examples from GEO880.",
      "startOffset" : 11,
      "endOffset" : 41
    }, {
      "referenceID" : 7,
      "context" : "5∗ Giordani and Moschitti (2012) (SQL) 87.",
      "startOffset" : 3,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "Most relevant to this work are the neural sequence based approaches of Dong and Lapata (2016) and Jia and Liang (2016).",
      "startOffset" : 71,
      "endOffset" : 94
    }, {
      "referenceID" : 5,
      "context" : "Most relevant to this work are the neural sequence based approaches of Dong and Lapata (2016) and Jia and Liang (2016). We note that Jia and Liang (2016) use a data recombination technique that boosts accuracy from 85.",
      "startOffset" : 71,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "Most relevant to this work are the neural sequence based approaches of Dong and Lapata (2016) and Jia and Liang (2016). We note that Jia and Liang (2016) use a data recombination technique that boosts accuracy from 85.",
      "startOffset" : 71,
      "endOffset" : 154
    }, {
      "referenceID" : 20,
      "context" : "GUSP (Poon, 2013) (SQL) 74.",
      "startOffset" : 5,
      "endOffset" : 17
    }, {
      "referenceID" : 20,
      "context" : "8 GUSP++ (Poon, 2013) (SQL) 83.",
      "startOffset" : 9,
      "endOffset" : 21
    }, {
      "referenceID" : 18,
      "context" : "GUSP (Poon, 2013) (SQL) 74.8 GUSP++ (Poon, 2013) (SQL) 83.5 Zettlemoyer and Collins (2007) 84.",
      "startOffset" : 6,
      "endOffset" : 91
    }, {
      "referenceID" : 5,
      "context" : "6 † Dong and Lapata (2016) 84.",
      "startOffset" : 4,
      "endOffset" : 27
    }, {
      "referenceID" : 5,
      "context" : "6 † Dong and Lapata (2016) 84.2 † Jia and Liang (2016) 83.",
      "startOffset" : 4,
      "endOffset" : 55
    } ],
    "year" : 2017,
    "abstractText" : "We present an approach to rapidly and easily build natural language interfaces to databases for new domains, whose performance improves over time based on user feedback, and requires minimal intervention. To achieve this, we adapt neural sequence models to map utterances directly to SQL with its full expressivity, bypassing any intermediate meaning representations. These models are immediately deployed online to solicit feedback from real users to flag incorrect queries. Finally, the popularity of SQL facilitates gathering annotations for incorrect predictions using the crowd, which is directly used to improve our models. This complete feedback loop, without intermediate representations or database specific engineering, opens up new ways of building high quality semantic parsers. Experiments suggest that this approach can be deployed quickly for any new target domain, as we show by learning a semantic parser for an online academic database from scratch.",
    "creator" : "TeX"
  }
}