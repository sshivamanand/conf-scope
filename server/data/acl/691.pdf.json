{
  "name" : "691.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Using Ontology-Grounded Token Embeddings To Predict Prepositional Phrase Attachments",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Type-level word embeddings map a word type (i.e., a surface form) to a dense vector of real numbers such that similar word types have similar embeddings. When pre-trained on a large corpus of unlabeled text, they provide an effective mechanism for generalizing statistical models to words which do not appear in the labeled training data for a downstream task.\nIn this paper, we make the following distinction between types and tokens: By word types, we mean the surface form of the word, whereas by tokens we mean the instantiation of the surface form in a context. For example, the same word type ‘pool’ occurs as two different tokens in the sentences “He sat by the pool,” and “He played a game of pool.”\nMost word embedding models define a single vector for each word type. However, a fundamen-\ntal flaw in this design is their inability to distinguish between different meanings and abstractions of the same word. In the two sentences shown above, the word ‘pool’ has different meanings, but the same representation is typically used for both of them. Similarly, the fact that ‘pool’ and ‘lake’ are both kinds of water bodies is not explicitly incorporated in most type-level embeddings. Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014; Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space.\nPrevious work partially addresses these problems by estimating concept embeddings in WordNet (e.g., Rothe and Schütze, 2015), or improving word representations using information from knowledge graphs (e.g., Faruqui et al., 2015). However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings.\nIn this paper, we represent a word token in a given context by estimating a context-sensitive probability distribution over relevant concepts in WordNet (Miller, 1995) and use the expected value (i.e., weighted sum) of the concept embeddings as the token representation (see §2). In addition to providing context-sensitive token embeddings, the proposed method implicitly regularizes the embeddings of related words by forcing related words to share similar concept embeddings. As a result, the representation of a rare word which does not appear in the training data for a downstream task benefits from all the updates to related words which share one or more concept embeddings.\nOur approach to context-sensitive embeddings assumes the availability of a lexical ontology such\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nFigure 1: An example grounding for the word ‘pool’. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word ‘pool’ regardless of its context. Other WordNet senses for ‘pool’ were removed from the figure for simplicity.\nas WordNet, but it does not require a tagger for word senses. We use the proposed embeddings to predict prepositional phrase (PP) attachments (see §3), a challenging problem which emphasizes the selectional preferences between words in the PP and each of the candidate head words. Our empirical results and detailed analysis (see §4) show that the proposed embeddings effectively use WordNet to improve the accuracy of PP attachment predictions."
    }, {
      "heading" : "2 WordNet-Grounded Context-Sensitive Token Embeddings",
      "text" : "In this section, we focus on defining our contextsensitive token embeddings. We first describe our grounding of word types using WordNet concepts. Then, we describe our model of contextsensitive token-level embeddings as a weighted sum of WordNet concept embeddings."
    }, {
      "heading" : "2.1 WordNet Grounding",
      "text" : "We use WordNet to map each word type to a set of synsets, including possible generalizations or abstractions. Among the labeled relations defined in WordNet between different synsets, we focus on the hypernymy relation to help model generaliza-\ntion and selectional preferences between words, which is especially important for predicting PP attachments (Resnik, 1993). To ground a word type, we identify the the set of (direct and indirect) hypernyms of the WordNet senses of that word. A simplified grounding of the word ‘pool’ is illustrated in Figure 1. This grounding is key to our model of token embeddings, to be described in the following subsections."
    }, {
      "heading" : "2.2 Context-Sensitive Token Embeddings",
      "text" : "Our goal is to define a context-sensitive model of token embeddings which can be used as a dropin replacement for traditional type-level word embeddings.\nNotation. Let Senses(w) be the list of synsets defined as possible word senses of a given word type w in WordNet, and Hypernyms(s) be the list of hypernyms for a synset s.1 For example, according to Figure 1:\nSenses(pool) = [pond.n.01, pool.n.09], and\nHypernyms(pond.n.01) = [pond.n.01, lake.n.01,\nbody of water.n.01, entity.n.01] (1)\nEach WordNet synset s is associated with a set of parameters vs ∈ Rn which represent its embedding. This parameterization is similar to that of Rothe and Schütze (2015).\nEmbedding model. Given a sequence of tokens t and their corresponding word types w, let ui ∈ Rn be the embedding of the word token ti at position i. Unlike most embedding models, the token embeddings ui are not parameters. Rather, ui is computed as the expected value of concept embeddings used to ground the word type wi corresponding to the token ti: ui = ∑\ns∈Senses(wi) ∑ s′∈Hypernyms(s) p(s, s′ | t,w, i) vs′\n(2)\nsuch that∑ s∈Senses(wi) ∑ s′∈Hypernyms(s) p(s′, s | t,w, i) = 1\n(3)\n1For notational convenience, we assume that s ∈ Hypernyms(s).\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nFigure 2: Steps for computing the contextsensitive token embedding for the word ‘pool’, as described in §2.2.\nThe distribution which governs the expectation over synset embeddings factorizes into two components:\np(s, s′ | t,w, i) ∝λwi exp−λwi rank(s,wi)× MLP([vs′ ; context(i, t)]) (4)\nThe first component, λwi exp −λwi rank(s,wi), is a sense prior which reflects the prominence of each word sense for a given word type. Note that this is defined for each word type (wi), and is shared across all tokens which have the same word type. The parameterization of the sense prior is similar to an exponential distribution since WordNet senses are organized in descending order of their frequency. The scalar parameter (λwi) controls the decay of the probability mass.\nThe second component, MLP([vs′ ; context(i, t)]) is what makes the token representations context-sensitive. It scores each concept in the WordNet grounding of wi by feeding the concatenation of the concept embedding and a dense vector that summarizes the textual context into a multilayer perceptron (MLP) with two tanh layers followed by a softmax layer. This component is inspired by the soft attention often used in neural machine translation (Bahdanau et al., 2014).2 The definition of the\n2Although soft attention mechanism is typically used to\ncontext function is dependent on the encoder used to encode the context. We describe a specific instantiation of this function in §3.\nTo summarize, Figure 2 illustrates how to compute the embedding of a word token ti = ‘pool’ in a given context:\n1. compute a summary of the context context(i, t),\n2. enumerate related concepts for ti,\n3. compute p(s, s′ | t,w, i) for each pair (s, s′), and\n4. compute ui = E[vs′ ].\nIn the following section, we describe our model for predicting PP attachments, including our definition for context."
    }, {
      "heading" : "3 PP Attachment",
      "text" : "Disambiguating PP attachments is an important and challenging NLP problem, and is a good fit for evaluating our WordNet-grounded contextsensitive embeddings since modeling hypernymy and selectional preferences is critical for successful prediction of PP attachments (Resnik, 1993).\nFigure 3, reproduced from Belinkov et al. (2014), illustrates an example of the PP attachment prediction problem. The accuracy of a\nexplicitly represent the importance of each item in a sequence, it can also be applied to non-sequential items.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\ncompetitive English dependency parser at predicting the head word of an ambiguous prepositional phrase is 88.5%, significantly lower than the overall unlabeled attachment accuracy of the same parser (94.2%).3\nThis section formally defines the problem of PP attachment disambiguation, describes our baseline model, then shows how to integrate the token-level embeddings in the model."
    }, {
      "heading" : "3.1 Problem definition",
      "text" : "We follow Belinkov et al. (2014)’s definition of the PP attachment problem. Given a preposition p and its direct dependent d in the prepositional phrase (PP), our goal is to predict the correct head word for the PP among an ordered list of candidate head words h. Each example in the train, validation, and test sets consists of an input tuple 〈h, p, d〉 and an output index k to identify the correct head among the candidates in h."
    }, {
      "heading" : "3.2 Model definition",
      "text" : "Both our proposed and baseline models for PP attachment use bidirectional RNN with LSTM cells (bi-LSTM) to encode the sequence t = 〈h1, . . . , hK , p, d〉.\nWe score each candidate head by feeding the concatenation of the output bi-LSTM vectors for the head hk, the preposition p and the direct dependent d through an MLP, with a fully connected tanh layer to obtain a non-linear projection of the concatenation, followed by a fully-connected softmax layer:\np(hkis head) = MLPattach([lstm out(hk);\nlstm out(p);\nlstm out(d)]) (5)\nTo train the model, we use cross-entropy loss at the output layer for each candidate head in the training set. At test time, we predict the candidate head with the highest probability according to the model in Eq. 5, i.e.,\nk̂ = argmax k p(hkis head = 1). (6)\nThis model is inspired by the Head-Prep-ChildTernary model of Belinkov et al. (2014). The main difference is that we replace the input features for each token with the output bi-RNN vectors.\n3See Table 2 in §4 for detailed results.\nWe now describe the difference between the proposed model and the baseline. Generally, let lstm in(ti) and lstm out(ti) represent the input and output vectors of the bi-LSTM for each token ti ∈ t in the sequence.\nBaseline model. In the baseline model, we use type-level word embeddings to represent the input vector lstm in(ti) for a token ti in the sequence. The word embedding parameters are initialized with pre-trained vectors, then tuned along with the parameters of the bi-LSTM and MLPattach. We call this model LSTM-PP.\nProposed model. In the proposed model, we use token level word embedding as described in §2 as the input to the bi-LSTM, i.e., lstm in(ti) = ui. The context used for the attention component is simply the hidden state from the previous timestep. However, since we use a bi-LSTM, the model essentially has two RNNs, and accordingly we have two context vectors, and associated attentions. That is, contextf (i, t) = lstm in(ti−1) for the forward RNN and contextb(i, t) = lstm in(ti+1) for the backward RNN. The synset embedding parameters are initialized with pretrained vectors and tuned along with the sense decay (λw) and MLP parameters from Eq. 4, the parameters of the bi-LSTM and those of MLPattach. We call this model OntoLSTM-PP."
    }, {
      "heading" : "4 Experiments",
      "text" : "Dataset and evaluation. We used the English PP attachment dataset created and made available by Belinkov et al. (2014). The training and test splits contain 33,359 and 1951 labeled examples respectively. As explained in §3.1, the input for each example is 1) an ordered list of candidate head words, 2) the preposition, and 3) the direct dependent of the preposition. The head words are either nouns or verbs and the dependent is always a noun. All examples in this dataset have at least two candidate head words. As discussed in Belinkov et al. (2014), this dataset is a more realistic PP attachment task than the RRR dataset (Ratnaparkhi et al., 1994). The RRR dataset is a binary classification task with exactly two head word candidates in all examples. The context for each example in the RRR dataset is also limited which defeats the purpose of our context-sensitive embeddings.\nModel specifications and hyperparameters. For efficient implementation, we use mini-batch\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nupdates with the same number of senses and hypernyms for all examples, padding zeros and truncating senses and hypernyms as needed. For each word type, we use a maximum of S senses and H indirect hypernyms from WordNet. In our initial experiments on a held-out development set (10% of the training data), we found that values greater than S = 3 and H = 5 did not improve performance. We also used the development set to tune the number of layers in MLPattach separately for the OntoLSTM-PP and LSTM-PP, and the number of layers in the attention MLP in OntoLSTM-PP. When a synset has multiple hypernym paths, we use the shortest one. Finally, words types which do not appear in WordNet are assumed to have one unique sense per word type with no hypernyms. Since the POS tag for each word is included in the dataset, we exclude WordNet synsets which are incompatible with the POS tag. The synset embedding parameters are initilized using the synset vectors obtained by running AutoExtend (Rothe and Schütze, 2015) on 100-dimensional GloVe (Pennington et al., 2014) vectors for WordNet 3.1. Representation for the OOV word types in LSTMPP and OOV synset types in OntoLSTM-PP were randomly drawn from a uniform 100-d distribution. Initial sense prior parameters (λw) were also drawn from a uniform 1-d distribution.\nBaselines. In our experiments, we compare our proposed model, OntoLSTM-PP with three baselines – LSTM-PP initialized with GloVe embedding, LSTM-PP initialized with GloVe vectors retrofitted to WordNet using the approach of Faruqui et al. (2015), and finally the best performing standalone PP attachment system from Belinkov et al. (2014), referred to as “HPCD (full)” in the paper. “HPCD (full)” is a neural network model that learns to compose the vector representations of each of the candidate heads with those of the preposition and the dependent, and predict attachments. The input representations are enriched using syntactic context information, POS, WordNet and VerbNet (Kipper et al., 2008) information and the distance of the head word from the PP is explicitly encoded in composition architecture. In contrast, we do not use syntactic context, VerbNet and distance information, and do not explicitly encode POS information."
    }, {
      "heading" : "4.1 PP Attachment Results",
      "text" : "Table 1 shows that our proposed token level embedding scheme “OntoLSTM-PP” outperforms the better variant of our baseline “LSTM-PP” (with GloVe-retro intialization) by an absolute accuracy difference of 4.9%, or a relative error reduction of 32%. “OntoLSTM-PP’ also outperforms “HPCD (full)”, the previous best result on this dataset.\nInitializing the word embeddings with GloVeretro (which uses WordNet as described in Faruqui et al. (2015)) instead of “textitGloVe” amounts to a small improvement, compared to the improvements obtained using “OntoLSTM-PP”. This result illustrates that our approach of dynamically choosing a context sensitive distribution over synsets is a more effective way of making use of WordNet.\nEffect on dependency parsing. Following Belinkov et al. (2014), we used RBG parser (Lei et al., 2014), and modified it by adding a binary feature indicating the PP attachment predictions from our model.\nWe compare four ways to compute the additional binary features: 1) the predictions of the best standalone system “HPCD (full)” in Belinkov et al. (2014), 2) the predictions of our baseline model “LSTM-PP”, 3) the predictions of our improved model “OntoLSTM-PP”, and 4) the gold labels “Oracle PP”.\nTable 2 shows the effect of using the PP attachment predictions as features within a dependency parser. We note there is a relatively small difference in unlabeled attachment accuracy for all dependencies (not only PP attachments), even when gold PP attachments are used as additional features to the parser. However, when gold PP attachment are used, we note a large potential improvement of 10.46 points (between the PPA accuracy for “RBG” and “RBG + Oracle PP”), which confirms that adding PP predictions as features is\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nSystem Full UAS PPA Acc. RBG 94.17 88.51 RBG + HPCD (full) 94.19 89.59 RBG + LSTM-PP 94.14 86.35 RBG + OntoLSTM-PP 94.30 90.11 RBG + Oracle PP 94.60 98.97\nTable 2: Results from RBG dependency parser with features coming from various PP attachment predictors and oracle attachments.\nan effective approach. Our proposed model “RBG + OntoLSTM-PP” recovers 15% of this potential improvement, while “RBG + HPCD (full)” recovers 10%, which illustrates that PP attachment remains a difficult problem with plenty of room for improvements even when using a dedicated model to predict PP attachments and using its predictions in a dependency parser.\nWe also note that, although we use the same predictions of the “HPCD (full)” model in Belinkov et al. (2014),4 we report different results than Belinkov et al. (2014). For example, the unlabeled attachment score (UAS) of the baselines “RBG” and “RBG + HPCD (full)” are 94.17 and 94.19, respectively, in Table 2, compared to 93.96 and 94.05, respectively, in Belinkov et al. (2014). This is due to the use of different versions of the RBG parser.5"
    }, {
      "heading" : "4.2 Analysis",
      "text" : "In this subsection, we analyze different aspects of our model in order to develop a better understanding of its behavior.\nEffect of context sensitivity and sense priors. We now show some results that indicate the relative strengths of two components of our contextsensitive token embedding model. The second row in Table 3 shows the test accuracy of a system trained without sense priors (that is, making p(s|wi) from Eq. 2 a uniform distribution), and the third row shows the effect of making the token representations context-insensitive by giving a similar attention score to all related concepts, essentially making them type level representations,\n4The authors kindly provided their predictions for 1942 test examples (out of 1951 examples in the full test set). In Table 2, we use the same subset of 1942 test examples and will include a link to the subset in the final draft.\n5We use the latest commit (SHA: e07f74dd2ba47348fd548935155ded38eea20809) on the GitHub repository of the RGB parser.\nbut still grounded in WordNet. As it can be seen, removing context sensitivity has an adverse effect on the results. This illustrates the importance of the sense priors and the attention mechanism.\nIt is interesting that, even without sense priors and attention, the results with WordNet grounding is still higher than that of the two LSTM-PP systems in Table 1. This result illustrates the regularization behavior of sharing concept embeddings across multiple words, which is especially important for rare words.\nEffect of training data size. Since “OntoLSTM-PP” uses external information, the gap between the model and “LSTM-PP” is expected to be more pronounced when the training data sizes are smaller. To test this hypothesis, we trained the two models with different amounts of training data and measured their accuracies on the test set. The plot is shown in Figure 4. As expected, the gap tends to be larger at lower data sizes. Surprisingly, even with 2000 sentences in the training data set, OntoLSTM-PP outperforms LSTM-PP trained with the full data set. When both the models are trained with the fill dataset, LSTM-PP reaches a training accuracy of 95.3%, whereas OntoLSTM-PP reaches 93.5%. The fact that LSTM-PP is overfitting the training data more, indicates the regularization capability of OntoLSTM-PP.\nQualitative analysis. To better understand the effect of WordNet grounding, we took a sample of sentences from the test set whose PP attachments were correctly predicted by OntoLSTM-PP but not by LSTM-PP. A common pattern observed was\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nFigure 5: Two examples from the test set where OntoLSTM-PP predicts the head correctly and LSTM-PP does not, along with weights by OntoLSTM-PP to synsets that contribute to token representations of infrequent word types. The prepositions are shown in bold, LSTM-PP’s predictions in red and OntoLSTMPP’s predictions in green. Words that are not candidate heads or dependents are shown in brackets.\nModel PPA Acc. full 89.7 - sense priors 88.4 - attention 87.5\nTable 3: Effect of removing sense priors and context sensitivity (attention) from the model\nthat those sentences contained words not seen frequently in the training data. Figure 5 shows two such cases. In both cases, the weights assigned by OntoLSTM-PP to infrequent words are also shown. The word types soapsuds and buoyancy do not occur in the training data, but OntoLSTMPP was able to leverage the parameters learned for the synsets that contributed to their token representations. Another important observation is that the word type buoyancy has four senses in WordNet (we consider the first three), none of which is the metaphorical sense that is applicable to markets as shown in the example here. Selecting a combination of relevant hypernyms from various senses may have helped OntoLSTM-PP make the right prediction. This shows the value of using hypernymy information from WordNet. Moreover, this indicates the strength of the hybrid nature of the model, that lets it augment ontological information with distributional information.\nParameter space We note that the vocabulary sizes in OntoLSTM-PP and LSTM-PP are comparable as the synset types are shared across word types. In our experiments with the full PP attachment dataset, we learned embeddings for 18k synset types with OntoLSTM-PP and 11k word types with LSTM-PP. Since the biggest contribution to the parameter space comes from the embedding layer, the complexities of both the models are comparable.\nImplementation and code availability. The models are implemented using Keras (Chollet, 2015), and the functionality is available in the form of Keras layers to make it easier for other researchers to use the proposed embeddingn model.\nFuture work. This approach may be extended to other NLP tasks that can benefit from using encoders that can access WordNet information. WordNet also has some drawbacks, and may not always have sufficient coverage given the task at hand. As we have shown in §4.2, our model can deal with missing WordNet information by augmenting it with distributional information. Moreover, the methods described in this paper can be extended to other kinds of structured knowledge sources like Freebase which may be more suitable for tasks like question answering.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799"
    }, {
      "heading" : "5 Related Work",
      "text" : "This work is related to various lines of research within the NLP community: dealing with synonymy and homonymy in word representations both in the context of distributed embeddings and more traditional vector spaces; hybrid models of distributional and knowledge based semantics; and selectional preferences and their relation with syntactic and semantic relations.\nThe need for going beyond a single vector per word-type has been well established for a while, and many efforts were focused on building multi-prototype vector space models of meaning (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Jauhar et al., 2015; Neelakantan et al., 2015, etc.). However, the target of all these approaches is obtaining multi-sense word vector spaces, either by incorporating sense tagged information or other kinds of external context. The number of vectors learned is still fixed, based on the preset number of senses. In contrast, our focus is on learning a context dependent distribution over those concept representations. Other work not necessarily related to multi-sense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2014)’s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning. These approaches do not make use of lexical ontologies and is not amenable for joint training with a downstream NLP task.\nRelated to the idea of concept embeddings is Rothe and Schütze (2015) who estimated WordNet synset representations, given pre-trained typelevel word embeddings. In contrast, our work focuses on estimating token-level word embeddings as context sensitive distributions of concept embeddings.\nThere is a large body of work that tried to improve word embeddings using external resources. Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon. Jauhar et al. (2015) extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology.\nFaruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology. In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest.\nResnik (1993) showed the applicability of semantic classes and selectional preferences to resolving syntactic ambiguity. Zapirain et al. (2013) applied models of selectional preferences automatically learned from WordNet and distributional information, to the problem of semantic role labeling. Resnik (1993); Brill and Resnik (1994); Agirre (2008) and others have used WordNet information towards improving prepositional phrase attachment predictions."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we proposed a grounding of lexical items which acknowledges the semantic ambiguity of word types using WordNet and a method to learn a context-sensitive distribution over their representations. We also showed how to integrate the proposed representation with recurrent neural networks for disambiguating prepositional phrase attachments, showing that the proposed WordNetgrounded context-sensitive token embeddings outperforms standard type-level embeddings for predicting PP attachments. We provided a detailed qualitative and quantitative analysis of the proposed model."
    } ],
    "references" : [ {
      "title" : "Improving parsing and pp attachment performance with sense information",
      "author" : [ "Eneko Agirre." ],
      "venue" : "ACL. Citeseer.",
      "citeRegEx" : "Agirre.,? 2008",
      "shortCiteRegEx" : "Agirre.",
      "year" : 2008
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "CoRR abs/1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "A linear dynamical system model for text",
      "author" : [ "David Belanger", "Sham M. Kakade." ],
      "venue" : "ICML.",
      "citeRegEx" : "Belanger and Kakade.,? 2015",
      "shortCiteRegEx" : "Belanger and Kakade.",
      "year" : 2015
    }, {
      "title" : "Exploring compositional architectures and word vector representations for prepositional phrase attachment",
      "author" : [ "Yonatan Belinkov", "Tao Lei", "Regina Barzilay", "Amir Globerson." ],
      "venue" : "Transactions of the Association for Computational Linguistics 2:561–572.",
      "citeRegEx" : "Belinkov et al\\.,? 2014",
      "shortCiteRegEx" : "Belinkov et al\\.",
      "year" : 2014
    }, {
      "title" : "A rule-based approach to prepositional phrase attachment disambiguation",
      "author" : [ "Eric Brill", "Philip Resnik." ],
      "venue" : "Proceedings of the 15th conference on Computational linguistics-Volume 2. Association for Computational Linguistics, pages 1198–1204.",
      "citeRegEx" : "Brill and Resnik.,? 1994",
      "shortCiteRegEx" : "Brill and Resnik.",
      "year" : 1994
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "Danqi Chen", "Christopher D Manning." ],
      "venue" : "EMNLP. pages 740–750.",
      "citeRegEx" : "Chen and Manning.,? 2014",
      "shortCiteRegEx" : "Chen and Manning.",
      "year" : 2014
    }, {
      "title" : "A unified model for word sense representation and disambiguation",
      "author" : [ "Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "EMNLP. pages 1025–1035.",
      "citeRegEx" : "Chen et al\\.,? 2014",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Keras",
      "author" : [ "François Chollet." ],
      "venue" : "https://github. com/fchollet/keras.",
      "citeRegEx" : "Chollet.,? 2015",
      "shortCiteRegEx" : "Chollet.",
      "year" : 2015
    }, {
      "title" : "Retrofitting word vectors to semantic lexicons",
      "author" : [ "Manaal Faruqui", "Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard H. Hovy", "Noah A. Smith." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Faruqui et al\\.,? 2015",
      "shortCiteRegEx" : "Faruqui et al\\.",
      "year" : 2015
    }, {
      "title" : "Improving word representations via global context and multiple word prototypes",
      "author" : [ "Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Huang et al\\.,? 2012",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2012
    }, {
      "title" : "Ontologically grounded multi-sense representation learning for semantic vector space models",
      "author" : [ "Sujay Kumar Jauhar", "Chris Dyer", "Eduard H. Hovy." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Jauhar et al\\.,? 2015",
      "shortCiteRegEx" : "Jauhar et al\\.",
      "year" : 2015
    }, {
      "title" : "A large-scale classification of english verbs",
      "author" : [ "Karin Kipper", "Anna Korhonen", "Neville Ryant", "Martha Palmer." ],
      "venue" : "Language Resources and Evaluation 42(1):21–40.",
      "citeRegEx" : "Kipper et al\\.,? 2008",
      "shortCiteRegEx" : "Kipper et al\\.",
      "year" : 2008
    }, {
      "title" : "Neural architectures for named entity recognition",
      "author" : [ "Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Lample et al\\.,? 2016",
      "shortCiteRegEx" : "Lample et al\\.",
      "year" : 2016
    }, {
      "title" : "Low-rank tensors for scoring dependency structures",
      "author" : [ "Tao Lei", "Yuan Zhang", "Regina Barzilay", "Tommi Jaakkola." ],
      "venue" : "ACL. Association for Computational Linguistics.",
      "citeRegEx" : "Lei et al\\.,? 2014",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2014
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean." ],
      "venue" : "CoRR abs/1310.4546.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A Miller." ],
      "venue" : "Communications of the ACM 38(11):39–",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Efficient non-parametric estimation of multiple embeddings per word in vector space",
      "author" : [ "Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum." ],
      "venue" : "arXiv preprint arXiv:1504.06654 .",
      "citeRegEx" : "Neelakantan et al\\.,? 2015",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2015
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "A maximum entropy model for prepositional phrase attachment",
      "author" : [ "Adwait Ratnaparkhi", "Jeff Reynar", "Salim Roukos." ],
      "venue" : "Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics, pages 250–255.",
      "citeRegEx" : "Ratnaparkhi et al\\.,? 1994",
      "shortCiteRegEx" : "Ratnaparkhi et al\\.",
      "year" : 1994
    }, {
      "title" : "Multi-prototype vector-space models of word meaning",
      "author" : [ "Joseph Reisinger", "Raymond J Mooney." ],
      "venue" : "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.",
      "citeRegEx" : "Reisinger and Mooney.,? 2010",
      "shortCiteRegEx" : "Reisinger and Mooney.",
      "year" : 2010
    }, {
      "title" : "Semantic classes and syntactic ambiguity",
      "author" : [ "Philip Resnik." ],
      "venue" : "Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics.",
      "citeRegEx" : "Resnik.,? 1993",
      "shortCiteRegEx" : "Resnik.",
      "year" : 1993
    }, {
      "title" : "Autoextend: Extending word embeddings to embeddings for synsets and lexemes",
      "author" : [ "Sascha Rothe", "Hinrich Schütze." ],
      "venue" : "ACL.",
      "citeRegEx" : "Rothe and Schütze.,? 2015",
      "shortCiteRegEx" : "Rothe and Schütze.",
      "year" : 2015
    }, {
      "title" : "Word representations via gaussian embedding",
      "author" : [ "Luke Vilnis", "Andrew McCallum." ],
      "venue" : "CoRR abs/1412.6623.",
      "citeRegEx" : "Vilnis and McCallum.,? 2014",
      "shortCiteRegEx" : "Vilnis and McCallum.",
      "year" : 2014
    }, {
      "title" : "Improving lexical embeddings with semantic knowledge",
      "author" : [ "Mo Yu", "Mark Dredze." ],
      "venue" : "ACL.",
      "citeRegEx" : "Yu and Dredze.,? 2014",
      "shortCiteRegEx" : "Yu and Dredze.",
      "year" : 2014
    }, {
      "title" : "Selectional preferences for semantic role classification",
      "author" : [ "Benat Zapirain", "Eneko Agirre", "Lluis Marquez", "Mihai Surdeanu." ],
      "venue" : "Computational Linguistics 39(3):631–663.",
      "citeRegEx" : "Zapirain et al\\.,? 2013",
      "shortCiteRegEx" : "Zapirain et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014; Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space.",
      "startOffset" : 135,
      "endOffset" : 186
    }, {
      "referenceID" : 15,
      "context" : "In this paper, we represent a word token in a given context by estimating a context-sensitive probability distribution over relevant concepts in WordNet (Miller, 1995) and use the expected value (i.",
      "startOffset" : 153,
      "endOffset" : 167
    }, {
      "referenceID" : 20,
      "context" : "Among the labeled relations defined in WordNet between different synsets, we focus on the hypernymy relation to help model generalization and selectional preferences between words, which is especially important for predicting PP attachments (Resnik, 1993).",
      "startOffset" : 241,
      "endOffset" : 255
    }, {
      "referenceID" : 21,
      "context" : "This parameterization is similar to that of Rothe and Schütze (2015).",
      "startOffset" : 44,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "This component is inspired by the soft attention often used in neural machine translation (Bahdanau et al., 2014).",
      "startOffset" : 90,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "This component is inspired by the soft attention often used in neural machine translation (Bahdanau et al., 2014).2 The definition of the Although soft attention mechanism is typically used to Figure 3: Two sentences illustrating the importance of lexicalization in PP attachment decisions. In the top sentence, the PP ‘with butter’ attaches to the noun ‘spaghetti’. In the bottom sentence, the PP ‘with chopsticks’ attaches to the verb ‘ate’. Note: This figure and caption have been reproduced from Belinkov et al. (2014).",
      "startOffset" : 91,
      "endOffset" : 523
    }, {
      "referenceID" : 20,
      "context" : "Disambiguating PP attachments is an important and challenging NLP problem, and is a good fit for evaluating our WordNet-grounded contextsensitive embeddings since modeling hypernymy and selectional preferences is critical for successful prediction of PP attachments (Resnik, 1993).",
      "startOffset" : 266,
      "endOffset" : 280
    }, {
      "referenceID" : 3,
      "context" : "Figure 3, reproduced from Belinkov et al. (2014), illustrates an example of the PP attachment prediction problem.",
      "startOffset" : 26,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "1 Problem definition We follow Belinkov et al. (2014)’s definition of the PP attachment problem.",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "This model is inspired by the Head-Prep-ChildTernary model of Belinkov et al. (2014). The main difference is that we replace the input features for each token with the output bi-RNN vectors.",
      "startOffset" : 62,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : "(2014), this dataset is a more realistic PP attachment task than the RRR dataset (Ratnaparkhi et al., 1994).",
      "startOffset" : 81,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "We used the English PP attachment dataset created and made available by Belinkov et al. (2014). The training and test splits contain 33,359 and 1951 labeled examples respectively.",
      "startOffset" : 72,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "We used the English PP attachment dataset created and made available by Belinkov et al. (2014). The training and test splits contain 33,359 and 1951 labeled examples respectively. As explained in §3.1, the input for each example is 1) an ordered list of candidate head words, 2) the preposition, and 3) the direct dependent of the preposition. The head words are either nouns or verbs and the dependent is always a noun. All examples in this dataset have at least two candidate head words. As discussed in Belinkov et al. (2014), this dataset is a more realistic PP attachment task than the RRR dataset (Ratnaparkhi et al.",
      "startOffset" : 72,
      "endOffset" : 529
    }, {
      "referenceID" : 21,
      "context" : "The synset embedding parameters are initilized using the synset vectors obtained by running AutoExtend (Rothe and Schütze, 2015) on 100-dimensional GloVe (Pennington et al.",
      "startOffset" : 103,
      "endOffset" : 128
    }, {
      "referenceID" : 17,
      "context" : "The synset embedding parameters are initilized using the synset vectors obtained by running AutoExtend (Rothe and Schütze, 2015) on 100-dimensional GloVe (Pennington et al., 2014) vectors for WordNet 3.",
      "startOffset" : 154,
      "endOffset" : 179
    }, {
      "referenceID" : 11,
      "context" : "The input representations are enriched using syntactic context information, POS, WordNet and VerbNet (Kipper et al., 2008) information and the distance of the head word from the PP is explicitly encoded in composition architecture.",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 7,
      "context" : "In our experiments, we compare our proposed model, OntoLSTM-PP with three baselines – LSTM-PP initialized with GloVe embedding, LSTM-PP initialized with GloVe vectors retrofitted to WordNet using the approach of Faruqui et al. (2015), and finally the best performing standalone PP attachment system from Belinkov et al.",
      "startOffset" : 212,
      "endOffset" : 234
    }, {
      "referenceID" : 3,
      "context" : "(2015), and finally the best performing standalone PP attachment system from Belinkov et al. (2014), referred to as “HPCD (full)” in the paper.",
      "startOffset" : 77,
      "endOffset" : 100
    }, {
      "referenceID" : 3,
      "context" : "Table 1: Results on Belinkov et al. (2014)’s PPA test set.",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 8,
      "context" : "Initializing the word embeddings with GloVeretro (which uses WordNet as described in Faruqui et al. (2015)) instead of “textitGloVe” amounts to a small improvement, compared to the improvements obtained using “OntoLSTM-PP”.",
      "startOffset" : 85,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : "(2014), we used RBG parser (Lei et al., 2014), and modified it by adding a binary feature indicating the PP attachment predictions from our model.",
      "startOffset" : 27,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "Following Belinkov et al. (2014), we used RBG parser (Lei et al.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 3,
      "context" : "Following Belinkov et al. (2014), we used RBG parser (Lei et al., 2014), and modified it by adding a binary feature indicating the PP attachment predictions from our model. We compare four ways to compute the additional binary features: 1) the predictions of the best standalone system “HPCD (full)” in Belinkov et al. (2014), 2) the predictions of our baseline model “LSTM-PP”, 3) the predictions of our improved model “OntoLSTM-PP”, and 4) the gold labels “Oracle PP”.",
      "startOffset" : 10,
      "endOffset" : 326
    }, {
      "referenceID" : 3,
      "context" : "We also note that, although we use the same predictions of the “HPCD (full)” model in Belinkov et al. (2014),4 we report different results than Belinkov et al.",
      "startOffset" : 86,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "We also note that, although we use the same predictions of the “HPCD (full)” model in Belinkov et al. (2014),4 we report different results than Belinkov et al. (2014). For example, the unlabeled attachment score (UAS) of the baselines “RBG” and “RBG + HPCD (full)” are 94.",
      "startOffset" : 86,
      "endOffset" : 167
    }, {
      "referenceID" : 3,
      "context" : "We also note that, although we use the same predictions of the “HPCD (full)” model in Belinkov et al. (2014),4 we report different results than Belinkov et al. (2014). For example, the unlabeled attachment score (UAS) of the baselines “RBG” and “RBG + HPCD (full)” are 94.17 and 94.19, respectively, in Table 2, compared to 93.96 and 94.05, respectively, in Belinkov et al. (2014). This is due to the use of different versions of the RBG parser.",
      "startOffset" : 86,
      "endOffset" : 381
    }, {
      "referenceID" : 7,
      "context" : "The models are implemented using Keras (Chollet, 2015), and the functionality is available in the form of Keras layers to make it easier for other researchers to use the proposed embeddingn model.",
      "startOffset" : 39,
      "endOffset" : 54
    }, {
      "referenceID" : 14,
      "context" : "Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 14,
      "context" : "(2015) extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology.",
      "startOffset" : 35,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "Other work not necessarily related to multi-sense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2014)’s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning.",
      "startOffset" : 98,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "Other work not necessarily related to multi-sense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2014)’s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning.",
      "startOffset" : 98,
      "endOffset" : 257
    }, {
      "referenceID" : 1,
      "context" : "Other work not necessarily related to multi-sense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2014)’s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning. These approaches do not make use of lexical ontologies and is not amenable for joint training with a downstream NLP task. Related to the idea of concept embeddings is Rothe and Schütze (2015) who estimated WordNet synset representations, given pre-trained typelevel word embeddings.",
      "startOffset" : 98,
      "endOffset" : 576
    }, {
      "referenceID" : 1,
      "context" : "Other work not necessarily related to multi-sense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2014)’s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning. These approaches do not make use of lexical ontologies and is not amenable for joint training with a downstream NLP task. Related to the idea of concept embeddings is Rothe and Schütze (2015) who estimated WordNet synset representations, given pre-trained typelevel word embeddings. In contrast, our work focuses on estimating token-level word embeddings as context sensitive distributions of concept embeddings. There is a large body of work that tried to improve word embeddings using external resources. Yu and Dredze (2014) extended the CBOW model (Mikolov et al.",
      "startOffset" : 98,
      "endOffset" : 912
    }, {
      "referenceID" : 1,
      "context" : "Other work not necessarily related to multi-sense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2014)’s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning. These approaches do not make use of lexical ontologies and is not amenable for joint training with a downstream NLP task. Related to the idea of concept embeddings is Rothe and Schütze (2015) who estimated WordNet synset representations, given pre-trained typelevel word embeddings. In contrast, our work focuses on estimating token-level word embeddings as context sensitive distributions of concept embeddings. There is a large body of work that tried to improve word embeddings using external resources. Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon. Jauhar et al. (2015) extended the skipgram model (Mikolov et al.",
      "startOffset" : 98,
      "endOffset" : 1104
    }, {
      "referenceID" : 1,
      "context" : "Other work not necessarily related to multi-sense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2014)’s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning. These approaches do not make use of lexical ontologies and is not amenable for joint training with a downstream NLP task. Related to the idea of concept embeddings is Rothe and Schütze (2015) who estimated WordNet synset representations, given pre-trained typelevel word embeddings. In contrast, our work focuses on estimating token-level word embeddings as context sensitive distributions of concept embeddings. There is a large body of work that tried to improve word embeddings using external resources. Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon. Jauhar et al. (2015) extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology. Faruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology.",
      "startOffset" : 98,
      "endOffset" : 1303
    }, {
      "referenceID" : 1,
      "context" : "Other work not necessarily related to multi-sense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2014)’s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning. These approaches do not make use of lexical ontologies and is not amenable for joint training with a downstream NLP task. Related to the idea of concept embeddings is Rothe and Schütze (2015) who estimated WordNet synset representations, given pre-trained typelevel word embeddings. In contrast, our work focuses on estimating token-level word embeddings as context sensitive distributions of concept embeddings. There is a large body of work that tried to improve word embeddings using external resources. Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon. Jauhar et al. (2015) extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology. Faruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology. In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest. Resnik (1993) showed the applicability of semantic classes and selectional preferences to resolving syntactic ambiguity.",
      "startOffset" : 98,
      "endOffset" : 1694
    }, {
      "referenceID" : 1,
      "context" : "Other work not necessarily related to multi-sense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2014)’s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning. These approaches do not make use of lexical ontologies and is not amenable for joint training with a downstream NLP task. Related to the idea of concept embeddings is Rothe and Schütze (2015) who estimated WordNet synset representations, given pre-trained typelevel word embeddings. In contrast, our work focuses on estimating token-level word embeddings as context sensitive distributions of concept embeddings. There is a large body of work that tried to improve word embeddings using external resources. Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon. Jauhar et al. (2015) extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology. Faruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology. In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest. Resnik (1993) showed the applicability of semantic classes and selectional preferences to resolving syntactic ambiguity. Zapirain et al. (2013) applied models of selectional preferences automatically learned from WordNet and distributional information, to the problem of semantic role labeling.",
      "startOffset" : 98,
      "endOffset" : 1824
    }, {
      "referenceID" : 1,
      "context" : "Other work not necessarily related to multi-sense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2014)’s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning. These approaches do not make use of lexical ontologies and is not amenable for joint training with a downstream NLP task. Related to the idea of concept embeddings is Rothe and Schütze (2015) who estimated WordNet synset representations, given pre-trained typelevel word embeddings. In contrast, our work focuses on estimating token-level word embeddings as context sensitive distributions of concept embeddings. There is a large body of work that tried to improve word embeddings using external resources. Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon. Jauhar et al. (2015) extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology. Faruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology. In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest. Resnik (1993) showed the applicability of semantic classes and selectional preferences to resolving syntactic ambiguity. Zapirain et al. (2013) applied models of selectional preferences automatically learned from WordNet and distributional information, to the problem of semantic role labeling. Resnik (1993); Brill and Resnik (1994); Agirre (2008) and others have used WordNet information towards improving prepositional phrase attachment predictions.",
      "startOffset" : 98,
      "endOffset" : 1989
    }, {
      "referenceID" : 1,
      "context" : "Other work not necessarily related to multi-sense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system for estimating token-level word embeddings, and Vilnis and McCallum (2014)’s work which proposed mapping each word type to a density instead of a point in a space to account for uncertainty in meaning. These approaches do not make use of lexical ontologies and is not amenable for joint training with a downstream NLP task. Related to the idea of concept embeddings is Rothe and Schütze (2015) who estimated WordNet synset representations, given pre-trained typelevel word embeddings. In contrast, our work focuses on estimating token-level word embeddings as context sensitive distributions of concept embeddings. There is a large body of work that tried to improve word embeddings using external resources. Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon. Jauhar et al. (2015) extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology. Faruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology. In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest. Resnik (1993) showed the applicability of semantic classes and selectional preferences to resolving syntactic ambiguity. Zapirain et al. (2013) applied models of selectional preferences automatically learned from WordNet and distributional information, to the problem of semantic role labeling. Resnik (1993); Brill and Resnik (1994); Agirre (2008) and others have used WordNet information towards improving prepositional phrase attachment predictions.",
      "startOffset" : 98,
      "endOffset" : 2014
    }, {
      "referenceID" : 0,
      "context" : "Resnik (1993); Brill and Resnik (1994); Agirre (2008) and others have used WordNet information towards improving prepositional phrase attachment predictions.",
      "startOffset" : 40,
      "endOffset" : 54
    } ],
    "year" : 2017,
    "abstractText" : "Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.",
    "creator" : "LaTeX with hyperref package"
  }
}