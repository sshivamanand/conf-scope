{
  "name" : "520.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Deep learning methods have had a major impact on research in natural language processing and raised performance substantially in many of the standard evaluations. Moreover, multimodal tasks like image captioning (Karpathy and Li, 2015) or visual question answering (Antol et al., 2015) can now be tackled with great success. Such systems seem to solve the problems entirely on a sub-symbolic level, based only on raw image (and text) input, whereas previous approaches required a hand-crafted combination of various higher-level components.\nThere is, however, concern about how deep neural networks learn to solve such tasks. Investigations for image recognition (Szegedy et al., 2014; Nguyen et al., 2015; Zhang et al., 2017) have shown surprising behavior very different from what would be expected of their “surpassing\nhuman-level performance” (He et al., 2015). Deep networks for language tasks may exhibit similarly odd behavior (Sproat and Jaitly, 2016; Arthur et al., 2016). Such results cast doubt on whether deep learning systems reliably acquire appropriate generalizations. However, given the recursive nature of language and the potentially enormous problem space of image captioning and similar tasks, acquiring the ability for reliable generalization will eventually be essential.\nA more theoretical issue is the capability of network architectures to be able, in principle, to learn certain classes of structure. For instance, it has been shown that LSTMs possess the ability to handle long-range dependencies (Hochreiter and Schmidhuber, 1997; Gers and Schmidhuber, 2001). However, the formal experiments that have been done along such lines are limited, particularly in the multimodal domain of vision and language. While recent work indicates that the information encoded in image embeddings is rich enough for good captioning results, it is an open question whether current multimodal architectures are able, in principle, to combine visual information effectively to handle the full range of linguistic constructions.\nThis paper introduces a new test methodology for multimodal deep learning models. SHAPEWORLD is a framework for specifying datasets, which differ from standard evaluation datasets in two main ways. Firstly, a SHAPEWORLD dataset defines a process for generating artificial data, which is randomly sampled during training/testing according to constraints specified by the experimenter. Secondly, the evaluation focus of the methodology is on linguistic understanding capabilities of the type investigated by formal semantics. Hence the visual complexity and the openclass vocabulary size is reduced to a minimum, while allowing indefinitely complex syntactic con-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nstructions. Since it is possible to control data generation for both training and evaluation, we can introduce previously unseen configurations in the test data. This allows us to design tasks which require the system to recombine learned concepts to understand these novel instances, and is hence a form of zero-shot learning. We think of the SHAPEWORLD tasks as unit-testing multimodal systems for specific linguistic generalization capabilities, in a similar way to the bAbI task set of Weston et al. (2015) for text-only understanding.\nWe also present initial results on the performance of a generic multimodal architecture. The interest here is not obtaining high numbers, but rather analyzing the performance of a generic model with our evaluation methodology and demonstrating the potential of SHAPEWORLD in investigating multimodal deep neural networks. In fact, we have been surprised to see some poor performance even for simple tasks. We invite the community to explore the SHAPEWORLD tasks and use them to evaluate other successful image captioning or visual question answering systems. We want to emphasize, however, that the goal is not to achieve optimal performance by tuning a network architecture for one of these tasks. The central question is rather whether deep network architectures are able to successfully demonstrate the required understanding and generalization ability, and whether this will carry over to more complex tasks which can be defined using SHAPEWORLD."
    }, {
      "heading" : "2 Related work",
      "text" : "With the increasing popularity of deep learning approaches, artificial data of various kinds is again seen as a valuable tool in experimentation. Recently, the simulation paradigm has been argued to be a promising driver for artificial intelligence research (Kiela et al., 2016). Various platforms following this paradigm have already been released, amongst others, OpenAI Gym (Brockman et al., 2016), DeepMind Lab (Beattie et al., 2016), Project Malmo (Johnson et al., 2016), to name a few of the most popular. An important advantage of simulated data is its infinite availability, particularly in light of the need of many deep learning models for huge amounts of data. Automatically generating data greatly reduces the cost, time and human effort. Moreover, it allows researchers to focus on particular problems, isolated from noisy\nand complex environments. When focusing on language tasks, the simulation paradigm faces the problem that interesting language generation is a difficult task in its own right and that the difficulty increases with the complexity of the underlying world. The bAbI tasks (Weston et al., 2015) are generated by internally simulating a short scene and extracting a few simple sentences from it. A similar approach is taken by Narasimhan et al. (2015), but here the simulation is more complex, comprising a text-based role-playing game. The MazeBase game environment (Sukhbaatar et al., 2015) also uses language as a mean to represent the game world. However, the descriptions are in an abstract, formulaic format, and the focus of the simulation is much more on the planning than the language component. The long-term research proposal of Mikolov et al. (2015) also simulates a world where an agent learns to solve tasks by communication with a teacher module. At least for a start, this module is supposed to be scripted to automatically generate appropriate responses, given its internal knowledge of the world state.\nAutomatically generated data is common for tasks specifically focusing on the ability to efficiently process data of a certain formal structure. Here, data is deliberately stripped of any realworld connection to create an abstract capability check. Recent work in the context of deep learning has investigated sequence patterns (Joulin and Mikolov, 2015), combinatorial problems (Vinyals et al., 2015), or executing programming language code (Zaremba and Sutskever, 2014), amongst others. This kind of tasks is particularly common for neural network models (compare e.g., Bengio et al. (1994) more than twenty years ago). The reason for interest in abstract capability tests is that the learning process of deep neural networks is far more difficult to understand in a detailed way than shallower machine learning methods.\nThe multimodal tasks of image captioning and visual question answering are most closely related to our methodology, but usually consist of repurposed real-world photos and human-written descriptions1. However, there have been experiments in which parts of the data are artificial or generated automatically: for instance, automatic\n1Although we use the term “real-world”, in contrast to artificial simulations, it should be clear that it is very unlike the visual input which would be experienced by an entity situated in the real world\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nquestion generation (Ren et al., 2015) or modification of captions (Hodosh and Hockenmaier, 2016). Abstract Clipart scenes have been used for image captioning (Zitnick et al., 2016; Zitnick and Parikh, 2013). Most similar to our work are the experiments done by Bowman et al. (2015) and Sorodoc et al. (2016). Both generate artificial data fully automatically, based on abstract models, for a task which is targeted at a specific linguistic aspect, logical semantics and quantifiers, respectively.\nOur own work is based on automatically generated, fully artificial data. This data, however, is not designed to address only a single structural problem, but is able to cover a whole range of linguistic phenomena. In fact, our generation system closely resembles classical work in formal semantics, where a statement corresponds to a logical expression which can be evaluated against an abstract world model (Montague, 1970). We utilize broad-coverage realization driven by the English Resource Grammar (ERG) (Flickinger, 2000) processed with Packard’s Answer Constraint Engine (ACE, http://sweaglesw. org/linguistics/ace/). However, while SHAPEWORLD uses a logical representation internally, the external representation seen by the system under evaluation does not involve any abstract formalization of text or images. It nevertheless presents the intended problems clearly, without any uncontrolled noise, biases, or potentially hidden correlations, which can obfuscate results when using real-world images and text."
    }, {
      "heading" : "3 The SHAPEWORLD framework",
      "text" : "The SHAPEWORLD framework2 is based on the concept of microworlds – small and selfcontained artificial scenarios – which guide the data creation process. The SHAPEWORLD microworlds simply consist of colored shapes. This closed-world domain allows for exhaustive coverage of the space of possible microworlds and associated captions, reducing the vocabulary size to (for now) far less than 100 words with an emphasis\n2 The SHAPEWORLD code is written in Python 3 and is available on GitHub (https://git.io/vDWOX). The generated data is returned as NumPy arrays, so that it is possible to integrate it into Python-based deep learning projects based on common frameworks like TensorFlow, Theano, etc. In our experiments, we use Tensorflow and we provide example scripts as part of the package. The Python package pydmrs (Copestake et al., 2016) is required for the internal DMRS-based caption generation.\non closed-class words. In the following we explain the details of the data generation process inside the SHAPEWORLD framework. A schematic illustration of the process is shown in figure 1."
    }, {
      "heading" : "3.1 Image caption agreement task",
      "text" : "In this paper we focus on the task of image caption agreement. The system to be evaluated is presented with an image and a natural language caption, and has to decide whether they are consistent with each other. Compared to the classic image captioning task, it emphasizes understanding rather than the synthesis part of language use. We therefore avoid the problem of evaluating the appropriateness of a caption. The setup allows us to control the content of both modalities and consequently force a system to cope with difficult types of captions while obtaining a clear indicator of successful understanding.\nOne further motivation for the task is that human performance could be measured using the same setup. We would expect close-to-perfect human performance on the tasks described here, assuming time is not tightly constrained. Interesting comparisons are potentially possible where human performance depends on presentation: e.g., quantifiers such as most (Pietroski et al., 2009). However, we will not discuss this further in the current paper."
    }, {
      "heading" : "3.2 World and image generation",
      "text" : "At the core of each abstract microworld instance lies an abstract world model. In the SHAPEWORLD framework, the internal representation of a microworld is simply a list of entities given as records containing their primary attributes, such as position, shape, color, which are considered to be high-level semantic aspects reflected in captions. In addition, an entity has secondary attributes and methods which control, for instance, details of visual appearance, visual noise infusion, or the collision-free placement of entities.\nCurrently, the SHAPEWORLD framework provides eight shape types (square, rectangle, triangle, pentagon, cross, circle, semicircle and ellipse) and seven colors (red, green, blue, yellow, magenta, cyan and white). The location and the secondary attributes are sampled uniformly or according to a truncated normal distribution (object size: 10% to 15% of image size; rotation: random; shade: shifted up to 50% towards black or white; and general noise: up to 10% deviation\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nWorld Generator World Captioner\nWorld Model {s : tr, c : gr, x : 0.9, y : 0.1, . . . }, {s : ci, c : bl, x : 0.2, y : 0.3, . . . }, {s : cr, c : re, x : 0.7, y : 0.5, . . . }, {s : sq, c : re, x : 0.8, y : 0.8, . . . }, {s : ci, c : gr, x : 0.3, y : 0.9, . . . }\nModel for Caption {s : cr, c : bl, x : 0.9, y : 0.1, . . . }, {s : tr, c : re, x : 0.2, y : 0.3, . . . }, {s : cr, c : re, x : 0.7, y : 0.5, . . . }, {s : sq, c : gr, x : 0.8, y : 0.8, . . . }\nDMRS Graph Pattern\nCaption Object Image:\nAgreement: False ≈ 0 ∈ [0, 1]\nCaption: There is a red triangle.\n(for negative instance)\n(for positive\ninstance)\nFigure 1: The generation process for image caption agreement data. Depending on whether a positive instance, i.e., a true statement about the world, or a negative instance is to be generated, either the first world model is used for caption generation or a new microworld is generated.\nper pixel). Importantly, all these ways of infusing noise can be controlled, which is useful particularly since noise is often seen as important for successful training of deep models.\nThe generator module automatically generates a world model by randomly sampling all these attributes from a set of available values. Both the values and other aspects of the generation process can be specified and adjusted appropriately. The internal abstract representation is then used as a basis to extract a concrete microworld instance consisting of image and caption. An image (of size 64×64 in this work) is just a straightforward visualization of the world model. Note that the exact visual appearance of an entity with certain primary attributes varies from instance to instance. Caption generation is more complex, as discussed in the next section."
    }, {
      "heading" : "3.3 Caption generation",
      "text" : "The caption interface specifies the methods a language generation module has to provide to integrate into the data generation process. We currently provide an implementation using a grammar-based approach. More specifically, Dependency Minimal Recursion Semantics (DMRS) is an abstract semantic graph representation designed for use with high-precision grammars, such as those distributed by the DELPH-IN consortium. A semantic representation like DMRS is particularly suited for the SHAPEWORLD framework, since it essentially mirrors parts of the internal world model and hence acts like a languagespecific annotation.3 For instance, noun nodes\n3Although we currently use the English Resource Grammar (Flickinger, 2000), other DELPH-IN grammars use a\ncorrespond to entities, adjective nodes add attributes, and verb phrase nodes or sub-graphs specify relations between entities. Below an example of a DMRS semantic graph with its compositional components colored:\nThere is a blue circle.\nCompositionality of the semantic representation is a useful property and an important reason for our choice of using DMRS. Given compositionality, it is enough to specify the semantics of words – or, more precisely, of the linguistic atoms in the SHAPEWORLD context, which potentially are sub-graphs with multiple nodes and inner link structure – to be able to obtain the corresponding semantics of composed sub-graphs and so generate a wide range of combinatorially different captions. For instance, the semantics of words like square or red iteratively filter a subset of agreeing entities, transitive relations like to the left of act similarly on pairs of entity sets, and quantifiers compare the cardinality of two entity sets.\nFigure 2 shows an example of a more complex compositional caption, which contains the DMRS graph above as sub-graph. It also illustrates how various details are automatically inferred by the ERG, including number-agreement between subject and verb, and between quantifier and noun, and realization of an adjective as relative clause.\ncompatible approach, so SHAPEWORLD could be ported to other languages relatively easily.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nMost squares are green and there are some circles which are blue.\n≈ [ #{s1∈World : square(s1.shape)∧ green(s1.color)}\n#{s2∈World : square(s2.shape)} > 1 2\n] ∧ [ ∃s3 ∈ World : circle(s3.shape) ∧ blue(s3.color) ]\nFigure 2: An example of a DMRS graph corresponding to a more complex caption, with compositional components colored. The logical formula gives the formal semantic interpretation over a world model.\nThis greatly facilitates the generation of a combinatorially large amount of captions and makes the DMRS graph patterns reusable, as outlined in section 4. Figure 2 furthermore gives a formal semantic interpretation of the caption meaning as logical formula over a world model. In essentially the same way, the agreement of a caption with a microworld is computed in the SHAPEWORLD framework.\nSimilar to the generator module, the captioner module also randomly samples from a set of dataset-specific DMRS graph patterns, which are then applied to a world model to construct an agreeing caption object. Such a caption object can be turned into natural language. Its ability to check whether another world model would agree with the semantics of this caption is important for the generation of negative instances, i.e., captions that do not agree with the corresponding microworld they are supposed to describe. These captions are obtained by sampling a second, false world model, extracting a caption object from it, and ensuring that it does not accidentally also agree with the first, true microworld."
    }, {
      "heading" : "3.4 Training and testing on SHAPEWORLD datasets",
      "text" : "Since SHAPEWORLD datasets are actually data generation processes, training and evaluation work differently from classic datasets. Where usually one has a fixed set of test instances, here models are trained and tested on a fixed set of more abstract configuration constraints. In particular, in our experiments, the constraints for testing (and validation) differ from the training constraints, hence requiring true generalization abilities. This means that, for instance, a certain shapecolor-combination, a specific number of objects, a spatial location or a caption type can be held-out and never generated during training. The model is\npresented with instances of this configuration only when evaluated and is hence required to recombine concepts it acquired from the training data. It is thus possible for a system to achieve optimal performance during training, but completely fail the evaluation. Another parameter is the ratio of correct instances, which may differ between training and evaluation. This adds yet another level of control to allow us to analyze the behavior of a model."
    }, {
      "heading" : "4 Some SHAPEWORLD datasets",
      "text" : "In this paper we look at four datasets, each designed to investigate an aspect of the capability to understand language in a multimodal setup and to generalize to new instances not seen during training. Two example images of microworlds for each dataset are shown in Figure 3.\nOneShape The first and simplest dataset requires the evaluated system to learn to separate the concepts of shape and color, instead of treating each combination as an atomic object. For this, microworlds are generated which contain a single object, together with a simple existential statement of the form: There is a [color] [shape]. One color-shape-combination, red square, is reserved for evaluation, which leaves 55 combinations for training.\nMultiShape This dataset generates worlds with up to four objects, with the same type of existential statement as before, describing one of these objects. The system is evaluated on worlds containing five objects, requiring the ability to focus attention on one object without being distracted by other objects.\nSpatial Microworlds with two objects are generated in this dataset accompanied by a statement about their relative location: to the left of, to the\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nONESHAPE MULTISHAPE SPATIAL QUANTIFIER\nFigure 3: Two example microworlds generated by each of the four presented SHAPEWORLD datasets.\nright of, above or below. The overall caption is of the form: The [color] [shape] is [spatial] the [color] [shape]. As in the first dataset, a previously unseen attribute combination is presented for evaluation.\nQuantifier The quantifier dataset generates microworlds of three to six objects, with seven used for evaluation. The captions focus on the quantifiers no, a, the, some, two, most, all and every. Compatibility with the caption is analyzed according to the quantifier’s usual cardinality-based interpretation in formal semantics. For instance, most is considered to be true if the target attribute applies to more than half of the domain set of objects. There are a variety of possible caption patterns in this dataset with different degrees of underspecification in domain and target, as illustrated in the examples below:\n• Some shapes are green. • Most shapes are rectangles. • No shape is a red triangle. • All triangles are green. • Two blue shapes are pentagons.\nSince we first sample a microworld model and subsequently a caption, here we cannot easily control the sampling process to uniformly sample each possible caption, as is trivially the case for the other datasets. Consequently, we restrict the number of different shapes and colors possible in a world, so that situations for the quantifiers all, every and most become more likely. Moreover, we adapt the probabilities of the sampling process so that we observe a more uniform distribution over caption patterns when sampling a large number of instances.\nAn important property of the SHAPEWORLD datasets is their compositionality. Instead of having to define a dataset from scratch every time, we can specify atomic datasets like the ones described above, and then combine them in a mixer dataset, which tests for various different aspects\nof multimodal language understanding simultaneously. Reusability in fact applies even further down in the component hierarchy. For instance, we use the same generic world generator module for all four datasets. This is even more important for caption generation where, for instance, a logical combinator dataset can reuse different world captioner modules to generate simple statements which then are merged by logical connectives."
    }, {
      "heading" : "5 Experiments",
      "text" : "As discussed in the introduction, the aim of our experiments is to demonstrate that SHAPEWORLD allows detailed investigation of neural network architectures, rather than high performance as such, although naturally a certain minimum performance is required for the results to be interesting."
    }, {
      "heading" : "5.1 Network architecture",
      "text" : "We evaluate a generic multimodal deep neural network architecture shown in figure 4, which is inspired by recent work on visual question answering (Antol et al., 2015). The visual input is processed by a convolutional neural network (CNN) module (LeCun et al., 1989) with three layers of convolutions (filter sizes: 5, 3, 3; number of filters: 16, 32, 64) followed by a 2×2 max-pooling step, or average-pooling in the last layer, respectively. With an image size of 64×64, the obtained world embedding has 4096 dimensions. The textual input is transformed to 32-dimensional embeddings and passed on to a long short-term memory (LSTM) module (Hochreiter and Schmidhuber, 1997) with state size 64. The final state is used as the caption embedding, which is scaled and fused with the world embedding via pointwise multiplication. After a fully-connected hidden layer of 512 dimensions, yield the agreement score. The entire architecture is trained end-toend on the task. Both the CNN module and the word embeddings are included in the training, as opposed to using pre-trained, general-purpose versions.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nCNN CNN CNN\nworld embedding\nLSTM LSTM LSTM LSTM LSTM\nthere is a green circle\ncaption embedding\nword embeddings lookup\n?\nagreement embedding hidden layer agreement ∈ [0, 1]\nscale\nFigure 4: A schematic depiction of the multimodal deep neural network which we evaluate on the SHAPEWORLD datasets.\nONESHAPE\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000\ntraining evaluation\nONESHAPE (*)\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000\ntraining evaluation\nMULTISHAPE\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000\ntraining evaluation\nSPATIAL\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000\ntraining evaluation\nSPATIAL (*)\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000\ntraining evaluation\nQUANTIFIER\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000\ntraining evaluation\nFigure 5: Performance of the evaluated multimodal deep neural network on the four SHAPEWORLD datasets, measured as accuracy of correctly judged image caption agreements. ."
    }, {
      "heading" : "5.2 Training and evaluation setup",
      "text" : "In all experiments, we use a training batch size of 128 where the ratio of correct instances is 33%. To track the performance of the model throughout the learning process (as opposed to just at the end), we add an evaluation step every 100th training iteration, where we obtain the accuracy on both training and evaluation instances. These numbers are calculated on basis of a batch of 1024 instances each, with a correct instance ratio of 50%, and are used to visualize and investigate the learning performance in our experiments."
    }, {
      "heading" : "5.3 Results",
      "text" : "Figure 4 shows the performance results of our experiments. We observe a high accuracy on the ONESHAPE dataset on training instances, but an unexpectedly low and unstable performance on the\nheld-back instances. This indicates that the system does not naturally learn to distinguish shape and color as two separate concepts, but rather learns to classify each shape-colour-combination individually – after all, the ONESHAPE dataset essentially resembles a classification task. In a followup experiment, we reduced the embedding size to 8 and the LSTM state size to 16 dimensions (marked with (*) in figure 4). Being given less memory to learn simple non-generalizing classification, the model indeed achieves similarly high accuracy numbers in evaluation as during training.\nThe training performance on the MULTISHAPE dataset is relatively low and plateaus at around 80-85% after roughly 5000 iterations. The evaluation accuracy, in comparison, is not that much lower, indicating that the generalization to an unseen number of objects is learned, to some degree.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nOn the SPATIAL dataset, unlike on ONESHAPE, the system successfully learns the shape-colordistinction from the start. Note that each caption here involves two object descriptions, so instead of simply learning classification, it is necessary to disentangle the words of the caption. The fact that the evaluation accuracy is higher than the training accuracy in the beginning is because the held-back combination red square is (unsurprisingly) easier to recognize than some others. Again, we also ran an experiment with reduced embedding and LSTM size. Contrary to ONESHAPE, the accuracy curves indicate that this impedes the learning of an appropriate generalization ability for the more complex SPATIAL task.\nThe QUANTIFIER dataset confirms the lower accuracy of captions focusing on parts of a world which contains multiple shapes. The training accuracy is around 5% lower than for MULTISHAPE and shows no substantial difference to the validation accuracy, implying that quantifiers, expectedly, are more difficult to learn than existential statements, but definitely possible to some degree."
    }, {
      "heading" : "5.4 Discussion",
      "text" : "We want to emphasize the fact that the training accuracy here represents an interesting measure on its own. Contrary to the common evaluation methodology, where training performance is less expressive after multiple iterations over the training data, each instance is randomly generated anew in our setting. Consequently, even the training accuracy reflects a learning process, and overfitting happens on a conceptual/structural level instead of a numerical correlation level. Investigations of overfitting in this sense are not easily possible with non-artificial data.\nAnother type of investigation of what a model has learned involves manipulation of the evaluation data. For instance, we analyzed the fact that the network only achieves around 95% accuracy on the supposedly simple ONESHAPE dataset. Looking at a few misclassifications, we found that sometimes the negative instances are problematic where either only the shape or only the color attribute differed from what the caption stated. We thus created a modified version of the ONESHAPE dataset which generates only negative instances of this structure. Applying the trained model to this dataset indeed gave a training accuracy of around 80% and an evaluation accuracy of only 55-60%.\nChanging the network parameters altered the performance for the ONESHAPE and SPATIAL datasets in interestingly different ways. In general, when we changed parameters in our experiments, we often observed either no learning at all, less (or no) improvement in evaluation accuracy compared to training accuracy, or similar curves for both numbers. Given our aim to set up capability unit-tests, such discrete, to some degree invariant outcomes are what we ideally expect – a behavior we intend to investigate further. Another aspect, which might turn out interesting in more complex experiments is the shape of the accuracy curves."
    }, {
      "heading" : "6 Conclusion and future work",
      "text" : "We have presented a new test methodology and framework, SHAPEWORLD, for multimodal deep learning models with a focus on formal-semantic style generalization capabilities. In this framework, artificial data is automatically generated, and datasets specify parameters of this generation process to create appropriate data, which targets a specific multimodal understanding task. We evaluated a neural network architecture on four image caption agreement datasets, where the system has to decide whether a statement applies to an image. We show how the SHAPEWORLD framework can be used to investigate the learning process of deep neural networks, and that it clearly indicates whether certain abilities are acquired. There are clearly some deficiencies in the generalization ability of the evaluated network – one aim of this paper is to invite the research community to evaluate their architectures on our datasets.\nThe SHAPEWORLD framework is still under development. In particular, we plan to add new datasets addressing other aspects of language, as well as integrating options to enhance the language generation module, with the aim of providing more varied and natural image descriptions. One option would be to integrate a subsequent step applying paraphrase rules after caption generation – Copestake et al. (2016) describe how this can be implemented on the level of DMRS graphs. In addition, the framework can easily be extended to produce more complex worlds, e.g., based on Cliparts (Zitnick et al., 2016). However, we plan to stick to the simplicity and abstractness of our microworlds for now, since we think they offer several advantages and enough richness for various interesting investigations.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "VQA: Visual question answering",
      "author" : [ "Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "IEEE International Conference on Computer Vision. ICCV 2015.",
      "citeRegEx" : "Antol et al\\.,? 2015",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Incorporating discrete translation lexicons into neural machine translation",
      "author" : [ "Philip Arthur", "Graham Neubig", "Satoshi Nakamura." ],
      "venue" : "Conference on Empirical Methods in Natural Language Processing (EMNLP). Austin, Texas, USA.",
      "citeRegEx" : "Arthur et al\\.,? 2016",
      "shortCiteRegEx" : "Arthur et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Yoshua Bengio", "Patrice Simard", "Paolo Frasconi." ],
      "venue" : "Transactions on Neural Networks 5(2):157–166.",
      "citeRegEx" : "Bengio et al\\.,? 1994",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "Recursive neural networks can learn logical semantics",
      "author" : [ "Samuel R. Bowman", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality. Association for Compu-",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "OpenAI Gym",
      "author" : [ "Greg Brockman", "Vicki Cheung", "Ludwig Pettersson", "Jonas Schneider", "John Schulman", "Jie Tang", "Wojciech Zaremba" ],
      "venue" : null,
      "citeRegEx" : "Brockman et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Brockman et al\\.",
      "year" : 2016
    }, {
      "title" : "Resources for building applications with Dependency Minimal Recursion Semantics",
      "author" : [ "Ann Copestake", "Guy Emerson", "Michael W. Goodman", "Matic Horvat", "Alexander Kuhnle", "Ewa Muszyńska." ],
      "venue" : "Proceedings of the 10th International",
      "citeRegEx" : "Copestake et al\\.,? 2016",
      "shortCiteRegEx" : "Copestake et al\\.",
      "year" : 2016
    }, {
      "title" : "On building a more efficient grammar by exploiting types",
      "author" : [ "Dan Flickinger." ],
      "venue" : "Natural Language Engineering 6(1):15–28.",
      "citeRegEx" : "Flickinger.,? 2000",
      "shortCiteRegEx" : "Flickinger.",
      "year" : 2000
    }, {
      "title" : "LSTM recurrent networks learn simple context-free and context-sensitive languages",
      "author" : [ "Felix A. Gers", "Jürgen Schmidhuber." ],
      "venue" : "Transactions on Neural Networks 12(6):1333–1340.",
      "citeRegEx" : "Gers and Schmidhuber.,? 2001",
      "shortCiteRegEx" : "Gers and Schmidhuber.",
      "year" : 2001
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun." ],
      "venue" : "Proceedings of the IEEE International Conference on Computer Vision (ICCV). Santiago,",
      "citeRegEx" : "He et al\\.,? 2015",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural Computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Focused evaluation for image description with binary forcedchoice tasks",
      "author" : [ "Micah Hodosh", "Julia Hockenmaier." ],
      "venue" : "Proceedings of the 5th Workshop on Vision and Language. Berlin, Germany.",
      "citeRegEx" : "Hodosh and Hockenmaier.,? 2016",
      "shortCiteRegEx" : "Hodosh and Hockenmaier.",
      "year" : 2016
    }, {
      "title" : "The Malmo platform for artificial intelligence experimentation",
      "author" : [ "Matthew Johnson", "Katja Hofmann", "Tim Hutton", "David Bignell." ],
      "venue" : "Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI-16). AAAI Press, Palo Alto,",
      "citeRegEx" : "Johnson et al\\.,? 2016",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "Inferring algorithmic patterns with stack-augmented recurrent nets",
      "author" : [ "Armand Joulin", "Tomas Mikolov." ],
      "venue" : "Advances in Neural Information Processing Systems 28, Curran Associates, Inc., pages 190–198.",
      "citeRegEx" : "Joulin and Mikolov.,? 2015",
      "shortCiteRegEx" : "Joulin and Mikolov.",
      "year" : 2015
    }, {
      "title" : "Deep visualsemantic alignments for generating image descriptions",
      "author" : [ "Andrej Karpathy", "Fei-Fei Li." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2015, pages 3128– 3137.",
      "citeRegEx" : "Karpathy and Li.,? 2015",
      "shortCiteRegEx" : "Karpathy and Li.",
      "year" : 2015
    }, {
      "title" : "Virtual embodiment: A scalable long-term strategy for artificial intelligence research",
      "author" : [ "Douwe Kiela", "Luana Bulat", "Anita L. Vero", "Stephen Clark." ],
      "venue" : "CoRR abs/1610.07432.",
      "citeRegEx" : "Kiela et al\\.,? 2016",
      "shortCiteRegEx" : "Kiela et al\\.",
      "year" : 2016
    }, {
      "title" : "Backpropagation applied to handwritten zip code recognition",
      "author" : [ "Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel." ],
      "venue" : "Neural Compututation 1(4):541–551.",
      "citeRegEx" : "LeCun et al\\.,? 1989",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1989
    }, {
      "title" : "A roadmap towards machine intelligence",
      "author" : [ "Tomas Mikolov", "Armand Joulin", "Marco Baroni." ],
      "venue" : "CoRR abs/1511.08130.",
      "citeRegEx" : "Mikolov et al\\.,? 2015",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2015
    }, {
      "title" : "English as a formal language",
      "author" : [ "Richard Montague." ],
      "venue" : "Bruno Visentini, editor, Linguaggi nella societa e nella tecnica, Edizioni di Communita, pages 188–221.",
      "citeRegEx" : "Montague.,? 1970",
      "shortCiteRegEx" : "Montague.",
      "year" : 1970
    }, {
      "title" : "Language understanding for textbased games using deep reinforcement learning",
      "author" : [ "Karthik Narasimhan", "Tejas Kulkarni", "Regina Barzilay." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Associ-",
      "citeRegEx" : "Narasimhan et al\\.,? 2015",
      "shortCiteRegEx" : "Narasimhan et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "author" : [ "Anh Nguyen", "Jason Yosinski", "Jeff Clune." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2015, pages 427–436.",
      "citeRegEx" : "Nguyen et al\\.,? 2015",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2015
    }, {
      "title" : "The meaning of ’most’: Semantics, numerosity and psychology",
      "author" : [ "Paul Pietroski", "Jeffrey Lidz", "Tim Hunter", "Justin Halberda." ],
      "venue" : "Mind and Language 24(5):554–585.",
      "citeRegEx" : "Pietroski et al\\.,? 2009",
      "shortCiteRegEx" : "Pietroski et al\\.",
      "year" : 2009
    }, {
      "title" : "Image question answering: A visual semantic embedding model and a new dataset",
      "author" : [ "Mengye Ren", "Ryan Kiros", "Richard S. Zemel." ],
      "venue" : "CoRR abs/1505.02074.",
      "citeRegEx" : "Ren et al\\.,? 2015",
      "shortCiteRegEx" : "Ren et al\\.",
      "year" : 2015
    }, {
      "title" : "Look, some green circles!”: Learning to quantify from images",
      "author" : [ "Ionut Sorodoc", "Angeliki Lazaridou", "Gemma Boleda", "Aurélie Herbelot", "Sandro Pezzelle", "Raffaella Bernardi." ],
      "venue" : "Proceedings of the 5th Workshop on Vision and Language. Berlin,",
      "citeRegEx" : "Sorodoc et al\\.,? 2016",
      "shortCiteRegEx" : "Sorodoc et al\\.",
      "year" : 2016
    }, {
      "title" : "RNN Approaches to Text Normalization: A Challenge",
      "author" : [ "Richard Sproat", "Navdeep Jaitly." ],
      "venue" : "CoRR abs/1611.00068.",
      "citeRegEx" : "Sproat and Jaitly.,? 2016",
      "shortCiteRegEx" : "Sproat and Jaitly.",
      "year" : 2016
    }, {
      "title" : "MazeBase: A sandbox for learning from games",
      "author" : [ "Sainbayar Sukhbaatar", "Arthur Szlam", "Gabriel Synnaeve", "Soumith Chintala", "Rob Fergus." ],
      "venue" : "CoRR abs/1511.07401.",
      "citeRegEx" : "Sukhbaatar et al\\.,? 2015",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2015
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian J. Goodfellow", "Rob Fergus." ],
      "venue" : "CoRR abs/1312.6199.",
      "citeRegEx" : "Szegedy et al\\.,? 2014",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2014
    }, {
      "title" : "Pointer networks",
      "author" : [ "Oriol Vinyals", "Meire Fortunato", "Navdeep Jaitly." ],
      "venue" : "Proceedings of the 28th International Conference on Neural Information Processing Systems. MIT Press, Montreal, Canada, NIPS’15, pages 2692–2700.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Towards AI-complete question answering: A set of prerequisite toy tasks",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov." ],
      "venue" : "CoRR abs/1502.05698.",
      "citeRegEx" : "Weston et al\\.,? 2015",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to execute",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever." ],
      "venue" : "CoRR abs/1410.4615.",
      "citeRegEx" : "Zaremba and Sutskever.,? 2014",
      "shortCiteRegEx" : "Zaremba and Sutskever.",
      "year" : 2014
    }, {
      "title" : "Understanding deep learning requires rethinking generalization",
      "author" : [ "Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals." ],
      "venue" : "International Conference on Learning Representations (ICLR 2017) .",
      "citeRegEx" : "Zhang et al\\.,? 2017",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2017
    }, {
      "title" : "Bringing semantics into focus using visual abstraction",
      "author" : [ "C. Lawrence Zitnick", "Devi Parikh." ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2013, pages 3009–3016.",
      "citeRegEx" : "Zitnick and Parikh.,? 2013",
      "shortCiteRegEx" : "Zitnick and Parikh.",
      "year" : 2013
    }, {
      "title" : "Adopting abstract images for semantic scene understanding",
      "author" : [ "C. Lawrence Zitnick", "Ramakrishna Vedantam", "Devi Parikh." ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence 38(4):627–638.",
      "citeRegEx" : "Zitnick et al\\.,? 2016",
      "shortCiteRegEx" : "Zitnick et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Moreover, multimodal tasks like image captioning (Karpathy and Li, 2015) or visual question answering (Antol et al.",
      "startOffset" : 49,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "Moreover, multimodal tasks like image captioning (Karpathy and Li, 2015) or visual question answering (Antol et al., 2015) can now be tackled with great success.",
      "startOffset" : 102,
      "endOffset" : 122
    }, {
      "referenceID" : 25,
      "context" : "Investigations for image recognition (Szegedy et al., 2014; Nguyen et al., 2015; Zhang et al., 2017) have shown surprising behavior very different from what would be expected of their “surpassing human-level performance” (He et al.",
      "startOffset" : 37,
      "endOffset" : 100
    }, {
      "referenceID" : 19,
      "context" : "Investigations for image recognition (Szegedy et al., 2014; Nguyen et al., 2015; Zhang et al., 2017) have shown surprising behavior very different from what would be expected of their “surpassing human-level performance” (He et al.",
      "startOffset" : 37,
      "endOffset" : 100
    }, {
      "referenceID" : 29,
      "context" : "Investigations for image recognition (Szegedy et al., 2014; Nguyen et al., 2015; Zhang et al., 2017) have shown surprising behavior very different from what would be expected of their “surpassing human-level performance” (He et al.",
      "startOffset" : 37,
      "endOffset" : 100
    }, {
      "referenceID" : 8,
      "context" : ", 2017) have shown surprising behavior very different from what would be expected of their “surpassing human-level performance” (He et al., 2015).",
      "startOffset" : 128,
      "endOffset" : 145
    }, {
      "referenceID" : 23,
      "context" : "Deep networks for language tasks may exhibit similarly odd behavior (Sproat and Jaitly, 2016; Arthur et al., 2016).",
      "startOffset" : 68,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "Deep networks for language tasks may exhibit similarly odd behavior (Sproat and Jaitly, 2016; Arthur et al., 2016).",
      "startOffset" : 68,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "For instance, it has been shown that LSTMs possess the ability to handle long-range dependencies (Hochreiter and Schmidhuber, 1997; Gers and Schmidhuber, 2001).",
      "startOffset" : 97,
      "endOffset" : 159
    }, {
      "referenceID" : 7,
      "context" : "For instance, it has been shown that LSTMs possess the ability to handle long-range dependencies (Hochreiter and Schmidhuber, 1997; Gers and Schmidhuber, 2001).",
      "startOffset" : 97,
      "endOffset" : 159
    }, {
      "referenceID" : 27,
      "context" : "We think of the SHAPEWORLD tasks as unit-testing multimodal systems for specific linguistic generalization capabilities, in a similar way to the bAbI task set of Weston et al. (2015) for text-only understanding.",
      "startOffset" : 162,
      "endOffset" : 183
    }, {
      "referenceID" : 14,
      "context" : "Recently, the simulation paradigm has been argued to be a promising driver for artificial intelligence research (Kiela et al., 2016).",
      "startOffset" : 112,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "Various platforms following this paradigm have already been released, amongst others, OpenAI Gym (Brockman et al., 2016), DeepMind Lab (Beattie et al.",
      "startOffset" : 97,
      "endOffset" : 120
    }, {
      "referenceID" : 11,
      "context" : ", 2016), Project Malmo (Johnson et al., 2016), to name a few of the most popular.",
      "startOffset" : 23,
      "endOffset" : 45
    }, {
      "referenceID" : 27,
      "context" : "The bAbI tasks (Weston et al., 2015) are generated by internally simulating a short scene and extracting a few simple sentences from it.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 24,
      "context" : "The MazeBase game environment (Sukhbaatar et al., 2015) also uses language as a mean to represent the game world.",
      "startOffset" : 30,
      "endOffset" : 55
    }, {
      "referenceID" : 12,
      "context" : "Recent work in the context of deep learning has investigated sequence patterns (Joulin and Mikolov, 2015), combinatorial problems (Vinyals et al.",
      "startOffset" : 79,
      "endOffset" : 105
    }, {
      "referenceID" : 26,
      "context" : "Recent work in the context of deep learning has investigated sequence patterns (Joulin and Mikolov, 2015), combinatorial problems (Vinyals et al., 2015), or executing programming language code (Zaremba and Sutskever, 2014), amongst others.",
      "startOffset" : 130,
      "endOffset" : 152
    }, {
      "referenceID" : 28,
      "context" : ", 2015), or executing programming language code (Zaremba and Sutskever, 2014), amongst others.",
      "startOffset" : 48,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : "Various platforms following this paradigm have already been released, amongst others, OpenAI Gym (Brockman et al., 2016), DeepMind Lab (Beattie et al., 2016), Project Malmo (Johnson et al., 2016), to name a few of the most popular. An important advantage of simulated data is its infinite availability, particularly in light of the need of many deep learning models for huge amounts of data. Automatically generating data greatly reduces the cost, time and human effort. Moreover, it allows researchers to focus on particular problems, isolated from noisy and complex environments. When focusing on language tasks, the simulation paradigm faces the problem that interesting language generation is a difficult task in its own right and that the difficulty increases with the complexity of the underlying world. The bAbI tasks (Weston et al., 2015) are generated by internally simulating a short scene and extracting a few simple sentences from it. A similar approach is taken by Narasimhan et al. (2015), but here the simulation is more complex, comprising a text-based role-playing game.",
      "startOffset" : 98,
      "endOffset" : 1003
    }, {
      "referenceID" : 3,
      "context" : "Various platforms following this paradigm have already been released, amongst others, OpenAI Gym (Brockman et al., 2016), DeepMind Lab (Beattie et al., 2016), Project Malmo (Johnson et al., 2016), to name a few of the most popular. An important advantage of simulated data is its infinite availability, particularly in light of the need of many deep learning models for huge amounts of data. Automatically generating data greatly reduces the cost, time and human effort. Moreover, it allows researchers to focus on particular problems, isolated from noisy and complex environments. When focusing on language tasks, the simulation paradigm faces the problem that interesting language generation is a difficult task in its own right and that the difficulty increases with the complexity of the underlying world. The bAbI tasks (Weston et al., 2015) are generated by internally simulating a short scene and extracting a few simple sentences from it. A similar approach is taken by Narasimhan et al. (2015), but here the simulation is more complex, comprising a text-based role-playing game. The MazeBase game environment (Sukhbaatar et al., 2015) also uses language as a mean to represent the game world. However, the descriptions are in an abstract, formulaic format, and the focus of the simulation is much more on the planning than the language component. The long-term research proposal of Mikolov et al. (2015) also simulates a world where an agent learns to solve tasks by communication with a teacher module.",
      "startOffset" : 98,
      "endOffset" : 1413
    }, {
      "referenceID" : 2,
      "context" : ", Bengio et al. (1994) more than twenty years ago).",
      "startOffset" : 2,
      "endOffset" : 23
    }, {
      "referenceID" : 21,
      "context" : "question generation (Ren et al., 2015) or modification of captions (Hodosh and Hockenmaier, 2016).",
      "startOffset" : 20,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : ", 2015) or modification of captions (Hodosh and Hockenmaier, 2016).",
      "startOffset" : 36,
      "endOffset" : 66
    }, {
      "referenceID" : 31,
      "context" : "Abstract Clipart scenes have been used for image captioning (Zitnick et al., 2016; Zitnick and Parikh, 2013).",
      "startOffset" : 60,
      "endOffset" : 108
    }, {
      "referenceID" : 30,
      "context" : "Abstract Clipart scenes have been used for image captioning (Zitnick et al., 2016; Zitnick and Parikh, 2013).",
      "startOffset" : 60,
      "endOffset" : 108
    }, {
      "referenceID" : 17,
      "context" : "In fact, our generation system closely resembles classical work in formal semantics, where a statement corresponds to a logical expression which can be evaluated against an abstract world model (Montague, 1970).",
      "startOffset" : 194,
      "endOffset" : 210
    }, {
      "referenceID" : 6,
      "context" : "We utilize broad-coverage realization driven by the English Resource Grammar (ERG) (Flickinger, 2000) processed with Packard’s Answer Constraint Engine (ACE, http://sweaglesw.",
      "startOffset" : 83,
      "endOffset" : 101
    }, {
      "referenceID" : 3,
      "context" : "Most similar to our work are the experiments done by Bowman et al. (2015) and Sorodoc et al.",
      "startOffset" : 53,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "Most similar to our work are the experiments done by Bowman et al. (2015) and Sorodoc et al. (2016). Both generate artificial data fully automatically, based on abstract models, for a task which is targeted at a specific linguistic aspect, logical semantics and quantifiers, respectively.",
      "startOffset" : 53,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : "The Python package pydmrs (Copestake et al., 2016) is required for the internal DMRS-based caption generation.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 20,
      "context" : ", quantifiers such as most (Pietroski et al., 2009).",
      "startOffset" : 27,
      "endOffset" : 51
    }, {
      "referenceID" : 6,
      "context" : "3 For instance, noun nodes Although we currently use the English Resource Grammar (Flickinger, 2000), other DELPH-IN grammars use a correspond to entities, adjective nodes add attributes, and verb phrase nodes or sub-graphs specify relations between entities.",
      "startOffset" : 82,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "1 Network architecture We evaluate a generic multimodal deep neural network architecture shown in figure 4, which is inspired by recent work on visual question answering (Antol et al., 2015).",
      "startOffset" : 170,
      "endOffset" : 190
    }, {
      "referenceID" : 15,
      "context" : "The visual input is processed by a convolutional neural network (CNN) module (LeCun et al., 1989) with three layers of convolutions (filter sizes: 5, 3, 3; number of filters: 16, 32, 64) followed by a 2×2 max-pooling step, or average-pooling in the last layer, respectively.",
      "startOffset" : 77,
      "endOffset" : 97
    }, {
      "referenceID" : 9,
      "context" : "The textual input is transformed to 32-dimensional embeddings and passed on to a long short-term memory (LSTM) module (Hochreiter and Schmidhuber, 1997) with state size 64.",
      "startOffset" : 118,
      "endOffset" : 152
    }, {
      "referenceID" : 31,
      "context" : ", based on Cliparts (Zitnick et al., 2016).",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 5,
      "context" : "One option would be to integrate a subsequent step applying paraphrase rules after caption generation – Copestake et al. (2016) describe how this can be implemented on the level of DMRS graphs.",
      "startOffset" : 104,
      "endOffset" : 128
    } ],
    "year" : 2017,
    "abstractText" : "We introduce a novel framework for evaluating multimodal deep learning models with respect to their language understanding and generalization abilities. In this approach, artificial data is automatically generated according to the experimenter’s specifications. The content of the data, both during training and evaluation, can be controlled in detail, which enables tasks to be created that require generalization abilities, in particular the combination of previously introduced concepts in novel ways. We demonstrate the potential of our methodology by evaluating a multimodal architecture on four different tasks, and show that our framework gives us insights into the model’s capabilities and limitations.",
    "creator" : "LaTeX with hyperref package"
  }
}