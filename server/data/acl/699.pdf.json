{
  "name" : "699.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Keyphrase or keyword is a piece of short and summative content that expresses the main semantic meaning of a long text. A typical use of keyphrase or keyword is in scientific publications, to provide the core information of a paper. We use the term “keyphrase”, interchangeable as “keyword”, in the rest of this paper, as it implies that it may contain multiple words. High-quality keyphrases can facilitate the understanding, organizing and accessing of document content. As a result, many stud-\nies have devoted to studying the ways of automatic extracting keyphrases from textual content (Liu et al., 2009; Medelyan et al., 2009a; Witten et al., 1999). Due to the public accessibility, many scientific publication datasets are often used as the test beds for keyphrase extraction algorithms. Therefore, this study also focuses on extracting keyphrases from scientific publications.\nAutomatically extracting keyphrases from a document is called Keypharase Extraction, and it has been widely exploited in many applications, such as information retrieval (Jones and Staveley, 1999), text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), and opinion mining (Berend, 2011). Most of the existing keyphrase extraction algorithms addressed this problem through two steps (Liu et al., 2009; Tomokiyo and Hurst, 2003). The first step is to acquire a list of keyphrase candidates. Researchers have tried to use n-grams or noun phrases with certain part-of-speech patterns for identifying the potential candidates (Hulth, 2003; Le et al., 2016; Liu et al., 2010; Wang et al., 2016). The second step is to rank candidates regarding their importance to the document, either through supervised or unsupervised machine learning methods with a set of manuallydefined features (Frank et al., 1999; Liu et al., 2009, 2010; Kelleher and Luz, 2005; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Song et al., 2003; Witten et al., 1999).\nThere are two major drawbacks for the above keyphrase extraction approaches.\nFirstly, they can only extract the keyphrases that appear in the source text, whereas they fail at predicting the meaningful keyphrases with a slightly different sequential order or using synonym words. However, it is common in a scientific publication where authors assign keyphrases based on the semantic meaning instead of follow-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ning the written content in the publication. In this paper, we denote the phrases that do not match any contiguous subsequence of source text as absent keyphrases, and the ones that fully match a part of the text as present keyphrases. Table 1 shows the proportion of present and absent keyphrases from the document abstract in four commonlyused datasets, where we observe large portions of absent keyphrases in all the datasets. The absent keyphrases cannot be extracted through previous approaches, which further urges the development of a more powerful keyphrase prediction model.\nSecondly, when ranking phrase candidates, previous approaches often adopted the machine learning features such as TF-IDF and PageRank. However, these features only target to detect the importance of each word in the document based on the statistics of word occurrence and co-occurrence, whereas they can hardly reveal the semantics behind the document content.\nTable 1: Proportion of the present keyphrases and absent keyphrases in four public datasets\nDataset # Keyphrase % Present % Absent Inspec 19,275 55.69 44.31\nKrapivin 2,461 44.74 52.26 NUS 2,834 67.75 32.25\nSemEval 12,296 42.01 57.99\nTo overcome the limitations of previous studies, we re-examine the process of Keyphrase Prediction, about how real human annotators would assign keyphrases. Given a document, human annotators will firstly read the text to get a basic understanding of the content, then they try to digest its essential content and summarize into keyphrases. Their generation of keyphrases relies on the understanding of the content, which not necessarily to be the words that occurred in the source text. For example, when human annotators see “Latent Dirichlet Allocation” in the text, they could write down “topic modeling” and/or “text mining” as possible keyphrases. In addition to the semantic understanding, human annotators might also go back and picks up the most important parts based on syntactic features. For example, the phrases following “we propose/apply/use” are supposed to be important in the text. As a result, a better keyphrase prediction model should understand the semantic meaning of the content, as well as capture the contextual features.\nTo effectively capture semantic and syntactic features, we utilize recurrent neural networks (RNN) (Cho et al., 2014; Gers and Schmidhuber, 2001) to compress the semantic information in the given text into a dense vector (i.e., semantic understanding). Furthermore, we incorporate a copy mechanism (Gu et al., 2016) to equip our model with the capability of finding important parts based on language syntax (i.e., syntactic understanding). Thus, our model can generate keyphrases based on the understanding of the text, no matter whether the keyphrases are present in the text or not; meanwhile, it does not lose important in-text information.\nThe contribution of this paper is in three-fold: a) we propose to apply an RNN-based generative model to keyphrase prediction, and we also incorporate a copy mechanism in RNN, which enables the model to successfully predict rarely-occurred phrases; b) this is the first work concerning about the problem of absent keyphrase prediction for scientific publications, and our model recalls up to 20% of absent keyphrases; and c) we conduct a comprehensive comparison against six important baselines on a broad range of datasets, and the results show that our proposed model significantly outperforms existing supervised and unsupervised extraction methods.\nIn the remainder of this paper, we firstly review the related work in Section 2. Then we elaborate the proposed model in Section 3. After that, we present the experiment setting in Section 4 and results in Section 5, followed by our discussion in Section 6. Section 7 concludes the paper."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Automatic Keyphrase Extraction",
      "text" : "Keyphrase provides a succinct and accurate way of describing a subject or a subtopic in a document. A number of extraction algorithms have been proposed, and typically the process of extracting can be broken down into two steps.\nThe first step is to generate a list of phrase candidates with heuristic methods. As these candidates are prepared for further filtering, a considerable amount of candidates are produced in this step to increase the possibility that most of the correct keyphrases are kept. The primary ways of extracting candidates include retaining word sequences that match certain part-of-speech tag patterns (e.g., nouns, adjectives) (Liu et al., 2011;\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nWang et al., 2016; Le et al., 2016), and extracting important n-grams or noun phrases (Hulth, 2003; Medelyan et al., 2008).\nThe second step is to score each candidate phrase regarding its likelihood of being a keyphrase in the given document. The top-ranked candidates are returned as keyphrases. Both supervised and unsupervised machine learning methods are widely employed here. For supervised methods, this task is solved as a binary classification problem, and various of learning methods and features have been explored (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009b; Lopez and Romary, 2010; Gollapalli and Caragea, 2014). As for the unsupervised approaches, primary ideas include finding the central nodes in text graph (Mihalcea and Tarau, 2004; Grineva et al., 2009), detecting representative phrases from topical clusters (Liu et al., 2009, 2010) and so on.\nAside from the commonly adopted two steps process, another two previous studies realized the keyphrase extraction in entirely different ways. Tomokiyo and Hurst (2003) applied two language models to measure the phraseness and informativeness of phrases. Liu et al. (2011) share the most similar idea to our work. They used a word alignment model, which learns the translation from the documents to the keyphrases. This approach alleviates the problem of vocabulary gap between source and target to a certain degree. However, this translation model can hardly deal with semantic meaning. Additionally, they trained the model with the target of title/summary to enlarge the number of training samples, which may diverge away from the real objective of generating keyphrases."
    }, {
      "heading" : "2.2 Encoder-Decoder Model",
      "text" : "The RNN Encoder-Decoder model (also referred as Sequence-to-Sequence Learning) is an end-toend approach. It was firstly introduced by Cho et al. (2014) and Sutskever et al. (2014) to solve translation problems. As it provides a powerful tool for modeling variable-length sequence in an end-to-end fashion, it fits many Natural Language Processing tasks and soon achieves great successes (Rush et al., 2015; Vinyals et al., 2015; Serban et al., 2016).\nDifferent strategies have been explored to improve the performance of Encoder-Decoder model. The attention mechanism (Bahdanau et al.,\n2014) is a soft alignment approach that allows the model to automatically locate the relevant input component. In order to make use of the important information in the source text, some studies sought ways to copy certain parts of content from the source text and paste them into target text (Allamanis et al., 2016; Gu et al., 2016; Zeng et al., 2016). There exists a discrepancy between the optimizing objective during training and the metrics during evaluation. A few studies attempted to eliminate this discrepancy by incorporating new training algorithm (Marc’Aurelio Ranzato et al., 2016) or modifying optimizing objective(Shen et al., 2016)."
    }, {
      "heading" : "3 Methodology",
      "text" : "This section will introduce in detail our proposed deep keyphrase generation. Firstly, the task of keyphrase generation is defined, followed by the overview of how we apply the RNN EncoderDecoder model. Details of the framework as well as the copy mechanism will be introduced in Section 3.3 and 3.4."
    }, {
      "heading" : "3.1 Problem Definition",
      "text" : "Given a keyphrase dataset consisting of N data samples, the i-th data sample (x(i),p(i)) contains one source text x(i), and Mi target keyphrases p(i) = (p(i,1),p(i,2), . . . ,p(i,Mi)). Both the source text x(i) and keyphrase p(i,j) are sequences of words:\nx(i) = x (i) 1 , x (i) 2 , . . . , x (i) L xi\np(i,j) = y (i,j) 1 , y (i,j) 2 , . . . , y (i,j) L p(i,j)\nLx(i) and Lp(i,j)denotes the length of word sequence of x(i) and p(i,j) respectively.\nNow for each data sample, there are one source text sequence and multiple target phrase sequences. To apply the RNN Encoder-Decoder model, the data need to be converted into textkeyphrase pairs which contain only one source sequence and one target sequence. One simple way is to split the data sample (x(i),p(i)) intoMi pairs: (x(i),p(i,1)), (x(i),p(i,2)), . . . , (x(i),p(i,Mi)). Then the Encoder-Decoder model is ready to be applied to learn the mapping from source sequence to target sequence. For the purpose of simplicity, (x,y) is used to denote each data pair in the rest of this section, where x is the word sequence of a source text and y is the word sequence of its keyphrase.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399"
    }, {
      "heading" : "3.2 Encoder-Decoder Model",
      "text" : "The basic idea of our keyphrase generation model is to compress the content of source text into a hidden representation with encoder, and generate corresponding keyphrases by the decoder based on the representation . Both the encoder and decoder are implemented with recurrent neural networks (RNN).\nThe encoder RNN converts the variable-length input sequence x = (x1, x2, ..., xT ) into a set of hidden representation h = (h1, h2, . . . , hT ), by iterating the following equations along the time t:\nht = f (xt,ht−1) (1)\nwhere f is a non-linear function. And we get the context vector c, acting as the representation of the whole input x, through a non-linear function q.\nc = q(h1, h2, ..., hT ) (2)\nThe decoder is another RNN, decompresses the context vector and generates a variable-length sequence y = (y1, y2, ..., yT ′) word by word, through a conditional language model:\nst = g(yt−1, st−1, c) p(yt|y1,...,t−1,x) = g(yt−1, st, c) (3)\nwhere st is the hidden state of decoder RNN at time t. The non-linear function g is a softmax classifier which outputs the probabilities of all the words in the vocabulary. yt is the predicted word at time t, by taking the word with largest probability after g(·).\nThe encoder and decoder networks are trained jointly to maximize the conditional probability of the target sequence given a source sequence. After training, we use the beam search to generate phrases and a max heap is maintained to get the predictions with highest probabilities."
    }, {
      "heading" : "3.3 Details of Encoder and Decoder",
      "text" : "A bidirectional Gated Recurrent Unit (GRU) is applied as our encoder to replace the simple recurrent neural network. Previous studies (Bahdanau et al., 2014; Cho et al., 2014) indicate it can provide a general better performance language modeling than simple RNN and a simpler structure than Long Short-term Memory Networks(Hochreiter and Schmidhuber, 1997). So the above non-linear function f is replaced by the GRU function (see in (Cho et al., 2014)).\nAnother forward GRU is utilized as the decoder. In addition, an attention mechanism is adopted to improve the performance. The attention mechanism was firstly introduced by Bahdanau et al. (2014) to make the model focus on the important parts in input dynamically. The context vector c is computed as a weighted sum of hidden representation h = (h1, . . . , hT ):\nci = T∑ j=1 αijhj αij = exp(a(si−1, hj))∑T k=1 exp(a(si−1, hk))\n(4)\nwhere a(si−1, hj) is a soft alignment function that measures the similarity between si−1 and hj , namely to which degree the inputs around position j and the output at position i match."
    }, {
      "heading" : "3.4 Copy Mechanism",
      "text" : "To ensure the quality of learned representation and reduce the size of vocabulary, typically the RNN model only considers a certain number of frequent words (e.g. 30,000 words in (Cho et al., 2014)), but a large amount of long-tail words are simply ignored. Therefore the RNN is not able to predict any keyphrase which contains out-of-vocabulary words. Actually, the important phrases can also be identified by their syntactic and location features, even though we don’t know the meanings. The copy mechanism is one feasible solution that enables RNN to predict unknown words based on contextual features.\nBy incorporating the copy mechanism, the probability of predicting each new word yt would consist of two parts. The first term is the probability of generating the term (see Equation 3) and the second one is the probability of copying it from source text:\np(yt|y1,...,t−1,x) = pg(yt|y1,...,t−1,x) + pc(yt|y1,...,t−1,x) (5)\nSimilar to attention mechanism, the copy mechanism pinpoints the appearance of yt−1 in source text, and use its location information (ρtτ ) to compute the weighted hidden representation (ζ(yt−1)) of input x.\nζ(yt−1) = TS∑ τ=1 ρtτhτ (6)\nρtτ = { 1 K p(xτ , c|st−1) xτ = yt−1 0 Otherwise\n(7)\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nwhere K is the normalization term. Subsequently, we obtain the probabilities of copying pc(yt|y1,...,t−1) following the Equation 3, in which the target words are replaced by the words in source text, and the vector of yt−1 is replaced by [e(yt−1), ζ(yt−1)]\nT , where the e(yt−1) is the embedding of last predicted word."
    }, {
      "heading" : "4 Experiment Settings",
      "text" : "This section starts with discussing how we design our evaluation experiments, followed by the description of training and testing datasets. Then, we introduce evaluation metrics and baselines."
    }, {
      "heading" : "4.1 Training Dataset",
      "text" : "There are several publicly-available datasets for evaluating keyphrase generation. The largest one came from Krapivin et al. (2008), which contained 2,304 scientific publications. However, this amount of data is unable to train a robust recurrent neural network model. In fact, there are millions of scientific papers available online, each of which contains the keyphrases assigned by authors. Therefore, we collected a large amount of high-quality scientific metadata in Computer Science domain from various online digital libraries, including ACM Digital Library, ScienceDirect, Wiley, Web of Science, and so on. In total, we obtained 567,830 articles after removing duplicates and overlaps, which is 200 times larger than the one of Krapivin et al. (2008). Note that our model is only trained on 527,830 articles since 40,000 publications are held out for training baselines and for building a test dataset."
    }, {
      "heading" : "4.2 Testing Datasets",
      "text" : "For evaluating the proposed model more comprehensively, four widely-adopted scientific publication datasets are used. In addition, since these datasets only contain few hundreds or thousands of publications, we contribute a new testing dataset KP20k with a much larger number of scientific articles. We take the title and abstract as the source text. Each dataset is described in details in the below text.\n– Inspec (Hulth, 2003): This dataset provides 2,000 paper abstracts. We adopt the 500 testing papers and the corresponding uncontrolled keyphrases for evaluation, and the remaining 1,500 papers are used for training the supervised baseline models.\n– Krapivin (Krapivin et al., 2008): This dataset provides 2,304 papers with full-text and author-assigned keyphrases. However, the author did not mention how to split testing data, so we simply select the first 400 papers in alphabetical order as the testing data, and the remaining papers are used to train the supervised baselines.\n– NUS (Nguyen and Kan, 2007): We use the author-assigned keyphrases and treat all 211 papers as the testing data. Since the NUS dataset did not specifically mention the ways of splitting training and testing data, the results of the supervised baseline models are obtained through a five-fold cross validation.\n– SemEval-2010 (Kim et al., 2010): 288 articles are collected from ACM Digital Library. 100 articles are used for testing and the rest are for training supervised baselines.\n– KP20k: We build a new testing dataset that contains the title, abstract and keyphrases of 20,000 scientific articles in Computer Science. They are randomly selected from our obtained 567,830 articles. For the supervised baselines, another 20,000 articles are randomly selected for training. Therefore, our proposed model is trained on 527,830 that holds out these 40,000 articles."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "In total, there are 2,780,316 〈text, keyphrase〉 pairs for training, in which text refers to the concatenation of the title and abstract of a publication, and keyphrase indicates an author-assigned keyword. The text pre-processing steps including tokenization, lowercasing and replacing all digits with symbol 〈digit〉 are applied. Two encoder-decoder models are trained, one with only attention mechanism (RNN) and one with both attention and copy mechanism enabled (CopyRNN). For both models, we choose the top 50,000 frequently-occurred words as our vocabulary, the dimension of embedding is set to 150, and the dimension of hidden layers is set to 300. Models are optimized using Adam (Kingma and Ba, 2014) with initial learning rate = 10−4, gradient clipping = 0.1 and dropout rate = 0.5. The max depth of beam search is set to 6, and the beam size is set to 200. In the generation of keyphrases, we find that the model tends to assign higher probabilities for shorter keyphrases,\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nwhereas most of keyphrases contain more than two words. To resolve this problem, we apply a simple heuristic by preserving only the first singleword phrase (with the highest generating probability) and removing the rest."
    }, {
      "heading" : "4.4 Baseline Models",
      "text" : "Four unsupervised algorithms (Tf-Idf, TextRank (Mihalcea and Tarau, 2004), SingleRank (Wan and Xiao, 2008), ExpandRank (Wan and Xiao, 2008)) and two supervised algorithms (KEA (Witten et al., 1999) and Maui (Medelyan et al., 2009a)) are adopted as baselines. We set up the four unsupervised methods following the optimal settings in (Hasan and Ng, 2010) and the two supervised methods following the default setting as specified in their papers."
    }, {
      "heading" : "4.5 Evaluation Metric",
      "text" : "Three evaluation metrics, the macro-averaged precision, recall and F-measure (F1) are employed for measuring the algorithm performance. Follow the standard definition, precision is defined as the number of correctly-predicted keyphrases over the number of all predicted keyphrases, recall is computed by the number of correctly-predicted keyphrases over the total data records. Note that, when determining the match of two keyphrases, we use Porter Stemmer for pre-processing."
    }, {
      "heading" : "5 Results and Analysis",
      "text" : "We conduct an empirical study on three different tasks to evaluate our model."
    }, {
      "heading" : "5.1 Predicting Present Keyphrases",
      "text" : "This is the same as the keyphrase extraction task in prior studies, in which we would like to analyze how well our proposed model perform on the commonly-defined task. To make a fair comparison, we only consider the present keyphrases for evaluation in this task. Table 2 provides the performances of the six baseline models, as well as our proposed models (i.e., RNN and CopyRNN). For each method, the table lists its F-measure at top 5 and top 10 predictions on the five datasets. The best scores are highlighted in bold and the underlines indicate the second best performances.\nThe results show that the four unsupervised models (Tf-idf, TextTank, SingleRank and ExpandRank) perform robust across different datasets. The ExpandRank fails to return any result on the KP20k dataset due to its high time com-\nplexity. The measures on NUS and SemEval here are higher than the ones reported in (Hasan and Ng, 2010) and (Kim et al., 2010), probably because we utilized the paper abstract instead of the full-text for training, which may filter out some noisy information. The performances of the two supervised models (i.e., Maui and KEA) are unstable on some datasets, but Maui achieves the best performances on three datasets among all the baseline models.\nAs for our proposed keyphrase prediction approaches, the RNN model with the attention mechanism does not perform as well as we expected. It might be because the RNN model only concerns on finding the hidden semantics behind the text, which may tend to generate keyphrases or words that are too general and may not necessarily referring to the source text. In addition, it fails to recall keyphrases that contain out-of-vocabulary words (since the RNN model only takes the top 50,000 words in vocabulary). This indicates that a pure generative model may not fit the extraction task, and we need to further link back to the language usage in the source text. Indeed, the copyRNN model, by considering more contextual information, significantly outperforms not only the RNN model but also all baselines, exceeding the best baselines by more than 20% on average. This result demonstrates the importance of source text for extraction task. Besides, nearly 2% of all the correct predictions contain out-of-vocabulary words.\nThe example in Figure 1(a) shows the result of predicted present keyphrases by RNN and CopyRNN for an article about video search. We see that both models can generate phrases that related to the topic of information retrieval and video. However most of RNN predictions are high-level terminologies, which are too general to be selected as keyphrases. The CopyRNN, on the other hand, predicts more detailed phrases like “video metadata” and “integrated ranking”. An interesting bad case is, “rich content” is coordinate with a keyphrase “video metadata”, and the CopyRNN puts it into prediction mistakenly."
    }, {
      "heading" : "5.2 Predicting Absent Keyphrases",
      "text" : "As stated, one important motivation for this work is that we are interested in the proposed model’s capability for predicting absent keyphrases based on the “understanding” of content. It is worth noting that such prediction is a very challenging task, and, to the best of our knowledge, no ex-\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nMethod Inspec Krapivin NUS SemEval KP20k\nF1@5 F1@10 F1@5 F1@10 F1@5 F1@10 F1@5 F1@10 F1@5 F1@10\nTf-Idf 0.221 0.313 0.129 0.160 0.136 0.184 0.128 0.194 0.102 0.126 TextRank 0.223 0.281 0.189 0.162 0.195 0.196 0.176 0.187 0.175 0.147\nSingleRank 0.214 0.306 0.189 0.162 0.140 0.173 0.135 0.176 0.096 0.119 ExpandRank 0.210 0.304 0.081 0.126 0.132 0.164 0.139 0.170 N/A N/A\nMaui 0.040 0.042 0.249 0.216 0.249 0.268 0.044 0.039 0.270 0.230 KEA 0.098 0.126 0.110 0.152 0.069 0.084 0.025 0.026 0.171 0.154\nRNN 0.085 0.064 0.135 0.088 0.169 0.127 0.157 0.124 0.179 0.189 CopyRNN 0.278 0.342 0.311 0.266 0.334 0.326 0.293 0.304 0.333 0.262\nTable 2: The performance of predicting present keyphrase of various models on five benchmark datasets\nisting methods can handle this task. Therefore, we only provide the RNN and copyRNN performances in the discussion of the results of this task. Here, we evaluate the performance with the recall of top 10 and top 50 results, to see how many absent keyphrases can be correctly predicted. We use the absent keyphrases in the testing datasets for evaluation.\nDataset RNN CopyRNN R@10 R@50 R@10 R@50\nInspec 0.031 0.061 0.047 0.100 Krapivin 0.095 0.156 0.113 0.202\nNUS 0.050 0.089 0.058 0.116 SemEval 0.041 0.060 0.043 0.067 KP20k 0.083 0.144 0.125 0.211\nTable 3: Absent keyphrases prediction performance of RNN and CopyRNN on five datasets\nTable 3 present the recalls of the top 10/50 predicted keyphrases for our RNN and CopyRNN models, in which we observe that the CopyRNN can, on average, recall around 8% (15%) of keyphrases at top 10 (50) predictions. This indicates that, to some extent, both models can capture the hidden semantics behind the textual content and make reasonable predictions. In addition, with the advantage of features from the source text, the CopyRNN model also outperforms the RNN model in this condition, though not improve as much as the present keyphrase extraction task. An example is shown in Figure 1(b), in which we see that two absent keyphrases “video retrieval” and “video indexing” are correctly recalled by both models. The interesting thing is, the term “indexing” does not appear in the text, but the models\nmay detect the information “index videos” in the first sentence and paraphrase it to the target phrase. And the CopyRNN successfully predicts another two keyphrases by capturing the detailed information from the text (highlighted text segments)."
    }, {
      "heading" : "5.3 Transfer to News Articles",
      "text" : "The RNN and CopyRNN are supervised models, and they are trained on data in specific domain and writing style. However, with sufficient training on a large-scale dataset, we expect the models to be able to learn universal language features that are effective in other corpus as well. Thus in this task, we will test our model on another type of text, to see whether the model would work when being transferred to a different environment.\nWe utilize the popular news article dataset DUC-2001 (Wan and Xiao, 2008) for analysis. The dataset consists of 308 news articles and 2,488 manually annotated keyphrases. The result is shown in Table 4, from which we could see that the CopyRNN could extract a portion of correct keyphrases from a unfamiliar text. Compare to the results reported in (Hasan and Ng, 2010), the performance of CopyRNN is better than TextRank (Mihalcea and Tarau, 2004) and KeyCluster (Liu et al., 2009), but lags behind the other\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nthree baselines. As transfered to corpus in a complete strange type and domain, the model encounters more unknown words and has to rely more on the syntactic features in the text. In this experiment, the CopyRNN recalls 766 keyphrases. 14.3% of them contain out-of-vocabulary words and many names of persons and places are correctly predicted."
    }, {
      "heading" : "6 Discussion",
      "text" : "Our experimental results demonstrate that the CopyRNN model not only performs well on predicting present keyphrases but also has the ability of generating topical relevant keyphrases that are absent in the text. In a broader sense, this model attempts to map a long text (i.e., paper abstract) with representative short text chunks (i.e., keyphrases), which can potentially be applied to improve information retrieval performance by generating high-quality index terms, as well as assisting user browsing by summarizing long documents into short readable phrases.\nSo far we have examined our model on scientific publications and news articles, demonstrating that our model has the ability to capture universal language patterns and extract key information from unfamiliar texts. We believe that the models have a greater potential to be generalized to other domains and types, like books, online reviews etc., if it is trained on larger data corpus. Also, we directly apply our model, which is trained on publication dataset, into generating keyphrases for news articles without any adaptive training. We believe that with proper training on news data, the\nmodel would make further improvement. Additionally, this work mainly studies the problem of discovering core content from textual materials. Here, the encoder-decoder framework is applied to model language; however, such framework can also be extended to locate the core information on other data resources such as to summarize content from images and videos."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "In this paper, we propose an RNN-based generative model for predicting keyphrase in scientific text. To the best of our knowledge, this is the first application of the encoder-decoder model to keyphrase prediction task. Our model summarizes phrases based the deep semantic meaning of the text and it is able to handle rarely-occurred phrases by incorporating a copy mechanism. Comprehensive empirical studies demonstrate the effectiveness of our proposed model for generating both present and absent keyphrases for different types of text. Our future work may include the following two directions.\n– In this work, we only evaluated the performance of proposed model by conducting offline experiments. In the future, we are interested in comparing the model with human annotators and evaluating the quality of predicted phrases by human judges.\n– Our current model does not fully consider the correlation among target keyphrases. It would also be interesting to explore the multiple-output optimization on our model.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "A Convolutional Attention Network for Extreme Summarization of Source Code",
      "author" : [ "M. Allamanis", "H. Peng", "C. Sutton." ],
      "venue" : "ArXiv e-prints .",
      "citeRegEx" : "Allamanis et al\\.,? 2016",
      "shortCiteRegEx" : "Allamanis et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "arXiv preprint arXiv:1409.0473 .",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Opinion expression mining by exploiting keyphrase extraction",
      "author" : [ "Gábor Berend." ],
      "venue" : "IJCNLP. Citeseer, pages 1162–1170.",
      "citeRegEx" : "Berend.,? 2011",
      "shortCiteRegEx" : "Berend.",
      "year" : 2011
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Domain-specific keyphrase extraction",
      "author" : [ "Eibe Frank", "Gordon W Paynter", "Ian H Witten", "Carl Gutwin", "Craig G Nevill-Manning" ],
      "venue" : null,
      "citeRegEx" : "Frank et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Frank et al\\.",
      "year" : 1999
    }, {
      "title" : "Lstm recurrent networks learn simple context-free and contextsensitive languages",
      "author" : [ "Felix A Gers", "E Schmidhuber." ],
      "venue" : "IEEE Transactions on Neural Networks 12(6):1333–1340.",
      "citeRegEx" : "Gers and Schmidhuber.,? 2001",
      "shortCiteRegEx" : "Gers and Schmidhuber.",
      "year" : 2001
    }, {
      "title" : "Extracting keyphrases from research papers using citation networks",
      "author" : [ "Sujatha Das Gollapalli", "Cornelia Caragea." ],
      "venue" : "Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence. AAAI Press, AAAI’14, pages 1629–1635.",
      "citeRegEx" : "Gollapalli and Caragea.,? 2014",
      "shortCiteRegEx" : "Gollapalli and Caragea.",
      "year" : 2014
    }, {
      "title" : "Extracting key terms from noisy and multitheme documents",
      "author" : [ "Maria Grineva", "Maxim Grinev", "Dmitry Lizorkin." ],
      "venue" : "Proceedings of the 18th International Conference on World Wide Web. ACM, New York, NY, USA, WWW ’09, pages 661–670.",
      "citeRegEx" : "Grineva et al\\.,? 2009",
      "shortCiteRegEx" : "Grineva et al\\.",
      "year" : 2009
    }, {
      "title" : "Incorporating copying mechanism in sequence-to-sequence learning",
      "author" : [ "Jiatao Gu", "Zhengdong Lu", "Hang Li", "Victor OK Li." ],
      "venue" : "arXiv preprint arXiv:1603.06393 .",
      "citeRegEx" : "Gu et al\\.,? 2016",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2016
    }, {
      "title" : "Conundrums in unsupervised keyphrase extraction: making sense of the state-of-the-art",
      "author" : [ "Kazi Saidul Hasan", "Vincent Ng." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Lin-",
      "citeRegEx" : "Hasan and Ng.,? 2010",
      "shortCiteRegEx" : "Hasan and Ng.",
      "year" : 2010
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Improved automatic keyword extraction given more linguistic knowledge",
      "author" : [ "Anette Hulth." ],
      "venue" : "Proceedings of the 2003 conference on Empirical methods in natural language processing. Association for Computational Linguistics, pages 216–223.",
      "citeRegEx" : "Hulth.,? 2003",
      "shortCiteRegEx" : "Hulth.",
      "year" : 2003
    }, {
      "title" : "A study on automatically extracted keywords in text categorization",
      "author" : [ "Anette Hulth", "Beáta B Megyesi." ],
      "venue" : "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Compu-",
      "citeRegEx" : "Hulth and Megyesi.,? 2006",
      "shortCiteRegEx" : "Hulth and Megyesi.",
      "year" : 2006
    }, {
      "title" : "Phrasier: a system for interactive document retrieval using keyphrases",
      "author" : [ "Steve Jones", "Mark S Staveley." ],
      "venue" : "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. ACM, pages",
      "citeRegEx" : "Jones and Staveley.,? 1999",
      "shortCiteRegEx" : "Jones and Staveley.",
      "year" : 1999
    }, {
      "title" : "Automatic hypertext keyphrase detection",
      "author" : [ "Daniel Kelleher", "Saturnino Luz." ],
      "venue" : "Proceedings of the 19th International Joint Conference on Artificial Intelligence. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, IJCAI’05, pages 1608–1609.",
      "citeRegEx" : "Kelleher and Luz.,? 2005",
      "shortCiteRegEx" : "Kelleher and Luz.",
      "year" : 2005
    }, {
      "title" : "Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles",
      "author" : [ "Su Nam Kim", "Olena Medelyan", "Min-Yen Kan", "Timothy Baldwin." ],
      "venue" : "Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computa-",
      "citeRegEx" : "Kim et al\\.,? 2010",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2010
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980 .",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Large dataset for keyphrases extraction",
      "author" : [ "Mikalai Krapivin", "Aliaksandr Autayeu", "Maurizio Marchese." ],
      "venue" : "Technical Report DISI-09-055, DISI, Trento, Italy.",
      "citeRegEx" : "Krapivin et al\\.,? 2008",
      "shortCiteRegEx" : "Krapivin et al\\.",
      "year" : 2008
    }, {
      "title" : "Unsupervised Keyphrase Extraction: Introducing New Kinds of Words to Keyphrases",
      "author" : [ "Tho Thi Ngoc Le", "Minh Le Nguyen", "Akira Shimazu" ],
      "venue" : null,
      "citeRegEx" : "Le et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2016
    }, {
      "title" : "Automatic keyphrase extraction by bridging vocabulary gap",
      "author" : [ "Zhiyuan Liu", "Xinxiong Chen", "Yabin Zheng", "Maosong Sun." ],
      "venue" : "Proceedings of the Fifteenth Conference on Computational Natural Language Learning. Association for Computational",
      "citeRegEx" : "Liu et al\\.,? 2011",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2011
    }, {
      "title" : "Automatic keyphrase extraction via topic decomposition",
      "author" : [ "Zhiyuan Liu", "Wenyi Huang", "Yabin Zheng", "Maosong Sun." ],
      "venue" : "Proceedings of the 2010 conference on empirical methods in natural language processing. Association for Compu-",
      "citeRegEx" : "Liu et al\\.,? 2010",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2010
    }, {
      "title" : "Clustering to find exemplar terms for keyphrase extraction",
      "author" : [ "Zhiyuan Liu", "Peng Li", "Yabin Zheng", "Maosong Sun." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1. Association",
      "citeRegEx" : "Liu et al\\.,? 2009",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2009
    }, {
      "title" : "Humb: Automatic key term extraction from scientific articles in grobid",
      "author" : [ "Patrice Lopez", "Laurent Romary." ],
      "venue" : "Proceedings of the 5th International Workshop on Semantic Evaluation. Association for Computational Linguistics, Strouds-",
      "citeRegEx" : "Lopez and Romary.,? 2010",
      "shortCiteRegEx" : "Lopez and Romary.",
      "year" : 2010
    }, {
      "title" : "Sequence level training with recurrent neural networks",
      "author" : [ "Michael Auli", "Wojciech Zaremba" ],
      "venue" : null,
      "citeRegEx" : "Ranzato et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2016
    }, {
      "title" : "Keyword extraction from a single document using word co-occurrence statistical information",
      "author" : [ "Yutaka Matsuo", "Mitsuru Ishizuka." ],
      "venue" : "International Journal on Artificial Intelligence Tools 13(01):157– 169.",
      "citeRegEx" : "Matsuo and Ishizuka.,? 2004",
      "shortCiteRegEx" : "Matsuo and Ishizuka.",
      "year" : 2004
    }, {
      "title" : "Human-competitive tagging using automatic keyphrase extraction",
      "author" : [ "Olena Medelyan", "Eibe Frank", "Ian H Witten." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3-Volume 3. Association",
      "citeRegEx" : "Medelyan et al\\.,? 2009a",
      "shortCiteRegEx" : "Medelyan et al\\.",
      "year" : 2009
    }, {
      "title" : "Human-competitive tagging using automatic keyphrase extraction",
      "author" : [ "Olena Medelyan", "Eibe Frank", "Ian H. Witten." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3.",
      "citeRegEx" : "Medelyan et al\\.,? 2009b",
      "shortCiteRegEx" : "Medelyan et al\\.",
      "year" : 2009
    }, {
      "title" : "Topic indexing with wikipedia",
      "author" : [ "Olena Medelyan", "Ian H Witten", "David Milne." ],
      "venue" : "Proceedings of the AAAI WikiAI workshop. volume 1, pages 19–24.",
      "citeRegEx" : "Medelyan et al\\.,? 2008",
      "shortCiteRegEx" : "Medelyan et al\\.",
      "year" : 2008
    }, {
      "title" : "Textrank: Bringing order into texts",
      "author" : [ "Rada Mihalcea", "Paul Tarau." ],
      "venue" : "Association for Computational Linguistics.",
      "citeRegEx" : "Mihalcea and Tarau.,? 2004",
      "shortCiteRegEx" : "Mihalcea and Tarau.",
      "year" : 2004
    }, {
      "title" : "Keyphrase extraction in scientific publications",
      "author" : [ "Thuy Dung Nguyen", "Min-Yen Kan." ],
      "venue" : "International Conference on Asian Digital Libraries. Springer, pages 317–326.",
      "citeRegEx" : "Nguyen and Kan.,? 2007",
      "shortCiteRegEx" : "Nguyen and Kan.",
      "year" : 2007
    }, {
      "title" : "A neural attention model for abstractive sentence summarization",
      "author" : [ "Alexander M. Rush", "Sumit Chopra", "Jason Weston." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon,",
      "citeRegEx" : "Rush et al\\.,? 2015",
      "shortCiteRegEx" : "Rush et al\\.",
      "year" : 2015
    }, {
      "title" : "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "author" : [ "Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau." ],
      "venue" : "Proceedings of the 30th AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Serban et al\\.,? 2016",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2016
    }, {
      "title" : "Minimum risk training for neural machine translation",
      "author" : [ "Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu" ],
      "venue" : null,
      "citeRegEx" : "Shen et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2016
    }, {
      "title" : "Kpspotter: a flexible information gain-based keyphrase extraction system",
      "author" : [ "Min Song", "Il-Yeol Song", "Xiaohua Hu." ],
      "venue" : "Proceedings of the 5th ACM international workshop on Web information and data management. ACM, pages 50–53.",
      "citeRegEx" : "Song et al\\.,? 2003",
      "shortCiteRegEx" : "Song et al\\.",
      "year" : 2003
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems. pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "A language model approach to keyphrase extraction",
      "author" : [ "Takashi Tomokiyo", "Matthew Hurst." ],
      "venue" : "Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment - Volume 18.",
      "citeRegEx" : "Tomokiyo and Hurst.,? 2003",
      "shortCiteRegEx" : "Tomokiyo and Hurst.",
      "year" : 2003
    }, {
      "title" : "Grammar as a foreign language",
      "author" : [ "Oriol Vinyals", "Łukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 2773–2781.",
      "citeRegEx" : "Vinyals et al\\.,? 2015",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2015
    }, {
      "title" : "Single document keyphrase extraction using neighborhood knowledge",
      "author" : [ "Xiaojun Wan", "Jianguo Xiao" ],
      "venue" : null,
      "citeRegEx" : "Wan and Xiao.,? \\Q2008\\E",
      "shortCiteRegEx" : "Wan and Xiao.",
      "year" : 2008
    }, {
      "title" : "PTR: Phrase-Based Topical Ranking for Automatic Keyphrase Extraction in Scientific Publications",
      "author" : [ "Minmei Wang", "Bo Zhao", "Yihua Huang" ],
      "venue" : null,
      "citeRegEx" : "Wang et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Kea: Practical automatic keyphrase extraction",
      "author" : [ "Ian H Witten", "Gordon W Paynter", "Eibe Frank", "Carl Gutwin", "Craig G Nevill-Manning." ],
      "venue" : "Proceedings of the fourth ACM conference on Digital libraries. ACM, pages 254–255.",
      "citeRegEx" : "Witten et al\\.,? 1999",
      "shortCiteRegEx" : "Witten et al\\.",
      "year" : 1999
    }, {
      "title" : "Efficient summarization with read-again and copy mechanism",
      "author" : [ "Wenyuan Zeng", "Wenjie Luo", "Sanja Fidler", "Raquel Urtasun." ],
      "venue" : "arXiv preprint arXiv:1611.03382 .",
      "citeRegEx" : "Zeng et al\\.,? 2016",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2016
    }, {
      "title" : "World wide web site summarization",
      "author" : [ "Yongzheng Zhang", "Nur Zincir-Heywood", "Evangelos Milios." ],
      "venue" : "Web Intelligence and Agent Systems: An International Journal 2(1):39–53.",
      "citeRegEx" : "Zhang et al\\.,? 2004",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "As a result, many studies have devoted to studying the ways of automatic extracting keyphrases from textual content (Liu et al., 2009; Medelyan et al., 2009a; Witten et al., 1999).",
      "startOffset" : 116,
      "endOffset" : 179
    }, {
      "referenceID" : 25,
      "context" : "As a result, many studies have devoted to studying the ways of automatic extracting keyphrases from textual content (Liu et al., 2009; Medelyan et al., 2009a; Witten et al., 1999).",
      "startOffset" : 116,
      "endOffset" : 179
    }, {
      "referenceID" : 39,
      "context" : "As a result, many studies have devoted to studying the ways of automatic extracting keyphrases from textual content (Liu et al., 2009; Medelyan et al., 2009a; Witten et al., 1999).",
      "startOffset" : 116,
      "endOffset" : 179
    }, {
      "referenceID" : 13,
      "context" : "Automatically extracting keyphrases from a document is called Keypharase Extraction, and it has been widely exploited in many applications, such as information retrieval (Jones and Staveley, 1999), text summarization (Zhang et al.",
      "startOffset" : 170,
      "endOffset" : 196
    }, {
      "referenceID" : 41,
      "context" : "Automatically extracting keyphrases from a document is called Keypharase Extraction, and it has been widely exploited in many applications, such as information retrieval (Jones and Staveley, 1999), text summarization (Zhang et al., 2004), text categorization (Hulth and Megyesi, 2006), and opinion mining (Berend, 2011).",
      "startOffset" : 217,
      "endOffset" : 237
    }, {
      "referenceID" : 12,
      "context" : ", 2004), text categorization (Hulth and Megyesi, 2006), and opinion mining (Berend, 2011).",
      "startOffset" : 29,
      "endOffset" : 54
    }, {
      "referenceID" : 2,
      "context" : ", 2004), text categorization (Hulth and Megyesi, 2006), and opinion mining (Berend, 2011).",
      "startOffset" : 75,
      "endOffset" : 89
    }, {
      "referenceID" : 21,
      "context" : "Most of the existing keyphrase extraction algorithms addressed this problem through two steps (Liu et al., 2009; Tomokiyo and Hurst, 2003).",
      "startOffset" : 94,
      "endOffset" : 138
    }, {
      "referenceID" : 35,
      "context" : "Most of the existing keyphrase extraction algorithms addressed this problem through two steps (Liu et al., 2009; Tomokiyo and Hurst, 2003).",
      "startOffset" : 94,
      "endOffset" : 138
    }, {
      "referenceID" : 11,
      "context" : "Researchers have tried to use n-grams or noun phrases with certain part-of-speech patterns for identifying the potential candidates (Hulth, 2003; Le et al., 2016; Liu et al., 2010; Wang et al., 2016).",
      "startOffset" : 132,
      "endOffset" : 199
    }, {
      "referenceID" : 18,
      "context" : "Researchers have tried to use n-grams or noun phrases with certain part-of-speech patterns for identifying the potential candidates (Hulth, 2003; Le et al., 2016; Liu et al., 2010; Wang et al., 2016).",
      "startOffset" : 132,
      "endOffset" : 199
    }, {
      "referenceID" : 20,
      "context" : "Researchers have tried to use n-grams or noun phrases with certain part-of-speech patterns for identifying the potential candidates (Hulth, 2003; Le et al., 2016; Liu et al., 2010; Wang et al., 2016).",
      "startOffset" : 132,
      "endOffset" : 199
    }, {
      "referenceID" : 38,
      "context" : "Researchers have tried to use n-grams or noun phrases with certain part-of-speech patterns for identifying the potential candidates (Hulth, 2003; Le et al., 2016; Liu et al., 2010; Wang et al., 2016).",
      "startOffset" : 132,
      "endOffset" : 199
    }, {
      "referenceID" : 4,
      "context" : "The second step is to rank candidates regarding their importance to the document, either through supervised or unsupervised machine learning methods with a set of manuallydefined features (Frank et al., 1999; Liu et al., 2009, 2010; Kelleher and Luz, 2005; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Song et al., 2003; Witten et al., 1999).",
      "startOffset" : 188,
      "endOffset" : 349
    }, {
      "referenceID" : 14,
      "context" : "The second step is to rank candidates regarding their importance to the document, either through supervised or unsupervised machine learning methods with a set of manuallydefined features (Frank et al., 1999; Liu et al., 2009, 2010; Kelleher and Luz, 2005; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Song et al., 2003; Witten et al., 1999).",
      "startOffset" : 188,
      "endOffset" : 349
    }, {
      "referenceID" : 24,
      "context" : "The second step is to rank candidates regarding their importance to the document, either through supervised or unsupervised machine learning methods with a set of manuallydefined features (Frank et al., 1999; Liu et al., 2009, 2010; Kelleher and Luz, 2005; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Song et al., 2003; Witten et al., 1999).",
      "startOffset" : 188,
      "endOffset" : 349
    }, {
      "referenceID" : 28,
      "context" : "The second step is to rank candidates regarding their importance to the document, either through supervised or unsupervised machine learning methods with a set of manuallydefined features (Frank et al., 1999; Liu et al., 2009, 2010; Kelleher and Luz, 2005; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Song et al., 2003; Witten et al., 1999).",
      "startOffset" : 188,
      "endOffset" : 349
    }, {
      "referenceID" : 33,
      "context" : "The second step is to rank candidates regarding their importance to the document, either through supervised or unsupervised machine learning methods with a set of manuallydefined features (Frank et al., 1999; Liu et al., 2009, 2010; Kelleher and Luz, 2005; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Song et al., 2003; Witten et al., 1999).",
      "startOffset" : 188,
      "endOffset" : 349
    }, {
      "referenceID" : 39,
      "context" : "The second step is to rank candidates regarding their importance to the document, either through supervised or unsupervised machine learning methods with a set of manuallydefined features (Frank et al., 1999; Liu et al., 2009, 2010; Kelleher and Luz, 2005; Matsuo and Ishizuka, 2004; Mihalcea and Tarau, 2004; Song et al., 2003; Witten et al., 1999).",
      "startOffset" : 188,
      "endOffset" : 349
    }, {
      "referenceID" : 3,
      "context" : "To effectively capture semantic and syntactic features, we utilize recurrent neural networks (RNN) (Cho et al., 2014; Gers and Schmidhuber, 2001) to compress the semantic information in the given text into a dense vector (i.",
      "startOffset" : 99,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "To effectively capture semantic and syntactic features, we utilize recurrent neural networks (RNN) (Cho et al., 2014; Gers and Schmidhuber, 2001) to compress the semantic information in the given text into a dense vector (i.",
      "startOffset" : 99,
      "endOffset" : 145
    }, {
      "referenceID" : 8,
      "context" : "Furthermore, we incorporate a copy mechanism (Gu et al., 2016) to equip our model with the capability of finding important parts based on language syntax (i.",
      "startOffset" : 45,
      "endOffset" : 62
    }, {
      "referenceID" : 11,
      "context" : ", 2016), and extracting important n-grams or noun phrases (Hulth, 2003; Medelyan et al., 2008).",
      "startOffset" : 58,
      "endOffset" : 94
    }, {
      "referenceID" : 27,
      "context" : ", 2016), and extracting important n-grams or noun phrases (Hulth, 2003; Medelyan et al., 2008).",
      "startOffset" : 58,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "For supervised methods, this task is solved as a binary classification problem, and various of learning methods and features have been explored (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009b; Lopez and Romary, 2010; Gollapalli and Caragea, 2014).",
      "startOffset" : 144,
      "endOffset" : 276
    }, {
      "referenceID" : 39,
      "context" : "For supervised methods, this task is solved as a binary classification problem, and various of learning methods and features have been explored (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009b; Lopez and Romary, 2010; Gollapalli and Caragea, 2014).",
      "startOffset" : 144,
      "endOffset" : 276
    }, {
      "referenceID" : 11,
      "context" : "For supervised methods, this task is solved as a binary classification problem, and various of learning methods and features have been explored (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009b; Lopez and Romary, 2010; Gollapalli and Caragea, 2014).",
      "startOffset" : 144,
      "endOffset" : 276
    }, {
      "referenceID" : 26,
      "context" : "For supervised methods, this task is solved as a binary classification problem, and various of learning methods and features have been explored (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009b; Lopez and Romary, 2010; Gollapalli and Caragea, 2014).",
      "startOffset" : 144,
      "endOffset" : 276
    }, {
      "referenceID" : 22,
      "context" : "For supervised methods, this task is solved as a binary classification problem, and various of learning methods and features have been explored (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009b; Lopez and Romary, 2010; Gollapalli and Caragea, 2014).",
      "startOffset" : 144,
      "endOffset" : 276
    }, {
      "referenceID" : 6,
      "context" : "For supervised methods, this task is solved as a binary classification problem, and various of learning methods and features have been explored (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009b; Lopez and Romary, 2010; Gollapalli and Caragea, 2014).",
      "startOffset" : 144,
      "endOffset" : 276
    }, {
      "referenceID" : 28,
      "context" : "As for the unsupervised approaches, primary ideas include finding the central nodes in text graph (Mihalcea and Tarau, 2004; Grineva et al., 2009), detecting representative phrases from topical clusters (Liu et al.",
      "startOffset" : 98,
      "endOffset" : 146
    }, {
      "referenceID" : 7,
      "context" : "As for the unsupervised approaches, primary ideas include finding the central nodes in text graph (Mihalcea and Tarau, 2004; Grineva et al., 2009), detecting representative phrases from topical clusters (Liu et al.",
      "startOffset" : 98,
      "endOffset" : 146
    }, {
      "referenceID" : 4,
      "context" : "For supervised methods, this task is solved as a binary classification problem, and various of learning methods and features have been explored (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009b; Lopez and Romary, 2010; Gollapalli and Caragea, 2014). As for the unsupervised approaches, primary ideas include finding the central nodes in text graph (Mihalcea and Tarau, 2004; Grineva et al., 2009), detecting representative phrases from topical clusters (Liu et al., 2009, 2010) and so on. Aside from the commonly adopted two steps process, another two previous studies realized the keyphrase extraction in entirely different ways. Tomokiyo and Hurst (2003) applied two language models to measure the phraseness and informativeness of phrases.",
      "startOffset" : 145,
      "endOffset" : 685
    }, {
      "referenceID" : 4,
      "context" : "For supervised methods, this task is solved as a binary classification problem, and various of learning methods and features have been explored (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Medelyan et al., 2009b; Lopez and Romary, 2010; Gollapalli and Caragea, 2014). As for the unsupervised approaches, primary ideas include finding the central nodes in text graph (Mihalcea and Tarau, 2004; Grineva et al., 2009), detecting representative phrases from topical clusters (Liu et al., 2009, 2010) and so on. Aside from the commonly adopted two steps process, another two previous studies realized the keyphrase extraction in entirely different ways. Tomokiyo and Hurst (2003) applied two language models to measure the phraseness and informativeness of phrases. Liu et al. (2011) share the most similar idea to our work.",
      "startOffset" : 145,
      "endOffset" : 789
    }, {
      "referenceID" : 30,
      "context" : "As it provides a powerful tool for modeling variable-length sequence in an end-to-end fashion, it fits many Natural Language Processing tasks and soon achieves great successes (Rush et al., 2015; Vinyals et al., 2015; Serban et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 238
    }, {
      "referenceID" : 36,
      "context" : "As it provides a powerful tool for modeling variable-length sequence in an end-to-end fashion, it fits many Natural Language Processing tasks and soon achieves great successes (Rush et al., 2015; Vinyals et al., 2015; Serban et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 238
    }, {
      "referenceID" : 31,
      "context" : "As it provides a powerful tool for modeling variable-length sequence in an end-to-end fashion, it fits many Natural Language Processing tasks and soon achieves great successes (Rush et al., 2015; Vinyals et al., 2015; Serban et al., 2016).",
      "startOffset" : 176,
      "endOffset" : 238
    }, {
      "referenceID" : 1,
      "context" : "The attention mechanism (Bahdanau et al., 2014) is a soft alignment approach that allows the model to automatically locate the relevant input component.",
      "startOffset" : 24,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "In order to make use of the important information in the source text, some studies sought ways to copy certain parts of content from the source text and paste them into target text (Allamanis et al., 2016; Gu et al., 2016; Zeng et al., 2016).",
      "startOffset" : 181,
      "endOffset" : 241
    }, {
      "referenceID" : 8,
      "context" : "In order to make use of the important information in the source text, some studies sought ways to copy certain parts of content from the source text and paste them into target text (Allamanis et al., 2016; Gu et al., 2016; Zeng et al., 2016).",
      "startOffset" : 181,
      "endOffset" : 241
    }, {
      "referenceID" : 40,
      "context" : "In order to make use of the important information in the source text, some studies sought ways to copy certain parts of content from the source text and paste them into target text (Allamanis et al., 2016; Gu et al., 2016; Zeng et al., 2016).",
      "startOffset" : 181,
      "endOffset" : 241
    }, {
      "referenceID" : 32,
      "context" : ", 2016) or modifying optimizing objective(Shen et al., 2016).",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "It was firstly introduced by Cho et al. (2014) and Sutskever et al.",
      "startOffset" : 29,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "It was firstly introduced by Cho et al. (2014) and Sutskever et al. (2014) to solve translation problems.",
      "startOffset" : 29,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "Previous studies (Bahdanau et al., 2014; Cho et al., 2014) indicate it can provide a general better performance language modeling than simple RNN and a simpler structure than Long Short-term Memory Networks(Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 17,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "Previous studies (Bahdanau et al., 2014; Cho et al., 2014) indicate it can provide a general better performance language modeling than simple RNN and a simpler structure than Long Short-term Memory Networks(Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 17,
      "endOffset" : 58
    }, {
      "referenceID" : 10,
      "context" : ", 2014) indicate it can provide a general better performance language modeling than simple RNN and a simpler structure than Long Short-term Memory Networks(Hochreiter and Schmidhuber, 1997).",
      "startOffset" : 155,
      "endOffset" : 189
    }, {
      "referenceID" : 3,
      "context" : "So the above non-linear function f is replaced by the GRU function (see in (Cho et al., 2014)).",
      "startOffset" : 75,
      "endOffset" : 93
    }, {
      "referenceID" : 1,
      "context" : "Previous studies (Bahdanau et al., 2014; Cho et al., 2014) indicate it can provide a general better performance language modeling than simple RNN and a simpler structure than Long Short-term Memory Networks(Hochreiter and Schmidhuber, 1997). So the above non-linear function f is replaced by the GRU function (see in (Cho et al., 2014)). Another forward GRU is utilized as the decoder. In addition, an attention mechanism is adopted to improve the performance. The attention mechanism was firstly introduced by Bahdanau et al. (2014) to make the model focus on the important parts in input dynamically.",
      "startOffset" : 18,
      "endOffset" : 534
    }, {
      "referenceID" : 3,
      "context" : "30,000 words in (Cho et al., 2014)), but a large amount of long-tail words are simply ignored.",
      "startOffset" : 16,
      "endOffset" : 34
    }, {
      "referenceID" : 17,
      "context" : "The largest one came from Krapivin et al. (2008), which contained 2,304 scientific publications.",
      "startOffset" : 26,
      "endOffset" : 49
    }, {
      "referenceID" : 17,
      "context" : "The largest one came from Krapivin et al. (2008), which contained 2,304 scientific publications. However, this amount of data is unable to train a robust recurrent neural network model. In fact, there are millions of scientific papers available online, each of which contains the keyphrases assigned by authors. Therefore, we collected a large amount of high-quality scientific metadata in Computer Science domain from various online digital libraries, including ACM Digital Library, ScienceDirect, Wiley, Web of Science, and so on. In total, we obtained 567,830 articles after removing duplicates and overlaps, which is 200 times larger than the one of Krapivin et al. (2008). Note that our model is only trained on 527,830 articles since 40,000 publications are held out for training baselines and for building a test dataset.",
      "startOffset" : 26,
      "endOffset" : 677
    }, {
      "referenceID" : 11,
      "context" : "– Inspec (Hulth, 2003): This dataset provides 2,000 paper abstracts.",
      "startOffset" : 9,
      "endOffset" : 22
    }, {
      "referenceID" : 17,
      "context" : "– Krapivin (Krapivin et al., 2008): This dataset provides 2,304 papers with full-text and author-assigned keyphrases.",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 29,
      "context" : "– NUS (Nguyen and Kan, 2007): We use the author-assigned keyphrases and treat all 211 papers as the testing data.",
      "startOffset" : 6,
      "endOffset" : 28
    }, {
      "referenceID" : 15,
      "context" : "– SemEval-2010 (Kim et al., 2010): 288 articles are collected from ACM Digital Library.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 16,
      "context" : "Models are optimized using Adam (Kingma and Ba, 2014) with initial learning rate = 10−4, gradient clipping = 0.",
      "startOffset" : 32,
      "endOffset" : 53
    }, {
      "referenceID" : 28,
      "context" : "4 Baseline Models Four unsupervised algorithms (Tf-Idf, TextRank (Mihalcea and Tarau, 2004), SingleRank (Wan and Xiao, 2008), ExpandRank (Wan and Xiao, 2008)) and two supervised algorithms (KEA (Witten et al.",
      "startOffset" : 65,
      "endOffset" : 91
    }, {
      "referenceID" : 37,
      "context" : "4 Baseline Models Four unsupervised algorithms (Tf-Idf, TextRank (Mihalcea and Tarau, 2004), SingleRank (Wan and Xiao, 2008), ExpandRank (Wan and Xiao, 2008)) and two supervised algorithms (KEA (Witten et al.",
      "startOffset" : 104,
      "endOffset" : 124
    }, {
      "referenceID" : 37,
      "context" : "4 Baseline Models Four unsupervised algorithms (Tf-Idf, TextRank (Mihalcea and Tarau, 2004), SingleRank (Wan and Xiao, 2008), ExpandRank (Wan and Xiao, 2008)) and two supervised algorithms (KEA (Witten et al.",
      "startOffset" : 137,
      "endOffset" : 157
    }, {
      "referenceID" : 39,
      "context" : "4 Baseline Models Four unsupervised algorithms (Tf-Idf, TextRank (Mihalcea and Tarau, 2004), SingleRank (Wan and Xiao, 2008), ExpandRank (Wan and Xiao, 2008)) and two supervised algorithms (KEA (Witten et al., 1999) and Maui (Medelyan et al.",
      "startOffset" : 194,
      "endOffset" : 215
    }, {
      "referenceID" : 25,
      "context" : ", 1999) and Maui (Medelyan et al., 2009a)) are adopted as baselines.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 9,
      "context" : "We set up the four unsupervised methods following the optimal settings in (Hasan and Ng, 2010) and the two supervised methods following the default setting as specified in their papers.",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 9,
      "context" : "The measures on NUS and SemEval here are higher than the ones reported in (Hasan and Ng, 2010) and (Kim et al.",
      "startOffset" : 74,
      "endOffset" : 94
    }, {
      "referenceID" : 15,
      "context" : "The measures on NUS and SemEval here are higher than the ones reported in (Hasan and Ng, 2010) and (Kim et al., 2010), probably because we utilized the paper abstract instead of the full-text for training, which may filter out some noisy information.",
      "startOffset" : 99,
      "endOffset" : 117
    }, {
      "referenceID" : 37,
      "context" : "We utilize the popular news article dataset DUC-2001 (Wan and Xiao, 2008) for analysis.",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 9,
      "context" : "Compare to the results reported in (Hasan and Ng, 2010), the performance of CopyRNN is better than TextRank (Mihalcea and Tarau, 2004) and KeyCluster (Liu et al.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 28,
      "context" : "Compare to the results reported in (Hasan and Ng, 2010), the performance of CopyRNN is better than TextRank (Mihalcea and Tarau, 2004) and KeyCluster (Liu et al.",
      "startOffset" : 108,
      "endOffset" : 134
    }, {
      "referenceID" : 21,
      "context" : "Compare to the results reported in (Hasan and Ng, 2010), the performance of CopyRNN is better than TextRank (Mihalcea and Tarau, 2004) and KeyCluster (Liu et al., 2009), but lags behind the other",
      "startOffset" : 150,
      "endOffset" : 168
    } ],
    "year" : 2017,
    "abstractText" : "Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divide the to-besummarized content into multiple text chunks, then rank and select the most meaningful ones. These approaches can neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also generates absent keyphrases based on the semantic meaning of the text.",
    "creator" : "LaTeX with hyperref package"
  }
}