{
  "name" : "326.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Chinese word segmentation (CWS) is a preliminary and important task for Chinese natural language processing (NLP). Currently, the state-ofthe-art methods are based on statistical supervised learning algorithms (Xue, 2003; Zhao et al., 2006), and rely on a large-scale annotated corpus whose cost is extremely expensive. Although there have been great achievements in building CWS corpora, they are somewhat incompatible due to different segmentation criteria. As shown in Table 1, given a sentence “姚明进入总决赛 (YaoMing reaches the final)”, the two commonly-used corpora, PKU’s People’s Daily (PKU) (Yu et al., 2001) and Penn Chinese Treebank (CTB) (Xia, 2000), use different segmentation criteria. In a sense, it is a waste of resources if we fail to fully exploit these corpora. Recently, some efforts have been made to exploit heterogeneous annotation data for Chinese word segmentation or part-of-speech tagging\nCorpora Yao Ming reaches the final CTB 姚明 进入 总决赛 PKU 姚 明 进入 总 决赛\n(Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015). These methods adopted stacking or multi-task architectures and showed that heterogeneous corpora can help each other. However, most of these model adopt the shallow linear classifier with discrete features, which makes it difficult to design the shared feature spaces, usually resulting in a complex model. Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016). In this paper, we propose an adversarial multicriteria learning for CWS by integrating shared knowledge from multiple segmentation criteria. Specifically, we regard each segmentation criterion as a single task and propose three different shared-private models under the framework of multi-task learning (Caruana, 1997; Ben-David and Schuller, 2003), where a shared layer is used to extract the criteria-invariant features, and a private layer is used to extract the criteria-specific features. Inspired by the success of adversarial strategy on domain adaption (Ajakan et al., 2014; Ganin et al., 2016; Bousmalis et al., 2016), we further utilize adversarial strategy to make sure the shared layer can extract the common underlying and criteria-invariant features, which are suitable for all the criteria. Finally, we exploit the eight segmentation criteria on the five simplified Chinese and three traditional Chinese corpora. Experiments show that our models are effective to improve the performance for CWS. We also observe\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nthat traditional Chinese could benefit from incorporating knowledge from simplified Chinese. The contributions of this paper could be summarized as follows.\n• Multi-criteria learning is first introduced for CWS, in which we propose three sharedprivate models to integrate multiple segmentation criteria. • An adversarial strategy is used to force the shared layer to learn criteria-invariant features, in which an new objective function is also proposed instead of the original crossentropy loss. • We conduct extensive experiments on eight CWS corpora with different segmentation criteria, which is by far the largest number of datasets used simultaneously."
    }, {
      "heading" : "2 General Neural Model for Chinese Word Segmentation",
      "text" : "Chinese word segmentation task is usually regarded as a character based sequence labeling problem. Specifically, each character in a sentence is labeled as one of L = {B,M,E, S}, indicating the begin, middle, end of a word, or a word with single character. There are lots of prevalent methods to solve sequence labeling problem such as maximum entropy Markov model (MEMM), conditional random fields (CRF), etc. Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Xu and Sun, 2016; Yao and Huang, 2016; Cai and Zhao, 2016; Zhang et al., 2016). Specifically, given a sequence with n characters X = {x1, . . . , xn}, the aim of CWS task is to figure out the ground truth of labels Y ∗ = {y∗1, . . . , y∗n}:\nY ∗ = argmax Y ∈Ln p(Y |X), (1)\nwhere L = {B,M,E, S}. The general architecture of neural CWS could be characterized by three components: (1) a character embedding layer; (2) feature layers consisting of several classical neural networks and (3) a tag inference layer. The role of feature layers is to extract features, which could be either convolution neural network or recurrent neural network. In this paper, we employ the state-of-the-art architecture\n(Huang et al., 2015; Ma and Hovy, 2016) which adopts the bi-direction long short-term memory with CRF as tag inference layer. Figure 1 illustrates the general architecture of CWS."
    }, {
      "heading" : "2.1 Embedding layer",
      "text" : "In neural models, the first step usually is to map discrete language symbols to distributed embedding vectors. Formally, we lookup embedding vector from embedding matrix for each character xi as exi ∈ Rde , where de is a hyper-parameter indicating the size of character embedding."
    }, {
      "heading" : "2.2 Feature layers",
      "text" : "We adopt bi-directional long short-term memory (Bi-LSTM) as feature layers. While there are numerous LSTM variants, here we use the LSTM architecture used by (Jozefowicz et al., 2015), which is similar to the architecture of (Graves, 2013) but without peep-hole connections.\nLSTM LSTM introduces gate mechanism and memory cell to maintain long dependency information and avoid gradient vanishing. Formally, LSTM, with input gate i, output gate o, forget gate f and memory cell c, could be expressed as:\nii oi fi c̃i\n = \nσ σ σ ϕ\n(Wgᵀ [ exihi−1 ] + bg ) , (2)\nci = ci−1 ⊙ fi + c̃i ⊙ ii, (3) hi = oi ⊙ ϕ(ci), (4)\nwhere Wg ∈ R(de+dh)×4dh and bg ∈ R4dh are trainable parameters. dh is a hyper-parameter, in-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\ndicating the hidden state size. Function σ(·) and ϕ(·) are sigmoid and tanh functions respectively.\nBi-LSTM In order to incorporate information from both sides of sequence, we use bi-directional LSTM (Bi-LSTM) with forward and backward directions. The update of each Bi-LSTM unit can be written precisely as follows:\nhi = −→h i ⊕ ←−h i, (5)\n= Bi-LSTM(exi , −→h i−1, ←−h i+1, θ), (6)\nwhere −→h i and ←−h i are the hidden states at position i of the forward and backward LSTMs respectively; ⊕ is concatenation operation; θ denotes all parameters in Bi-LSTM model."
    }, {
      "heading" : "2.3 Inference Layer",
      "text" : "After extracting features, we employ conditional random fields (CRF) (Lafferty et al., 2001) layer to inference tags. In CRF layer, p(Y |X) in Eq (1) could be formalized as:\np(Y |X) = Ψ(Y |X)∑ Y ′∈Ln Ψ(Y ′|X) . (7)\nHere, Ψ(Y |X) is the potential function, and we only consider interactions between two successive labels (first order linear chain CRFs):\nΨ(Y |X) = n∏\ni=2\nψ(X, i, yi−1, yi), (8)\nψ(x, i, y′, y) = exp(s(X, i)y + by′y), (9)\nwhere by′y ∈ R is trainable parameters respective to label pair (y′, y). Score function s(X, i) ∈ R|L| assigns score for each label on tagging the i-th character:\ns(X, i) = W⊤s hi + bs, (10)\nwhere hi is the hidden state of Bi-LSTM at position i; Ws ∈ Rdh×|L| and bs ∈ R|L| are trainable parameters."
    }, {
      "heading" : "3 Multi-Criteria Learning for Chinese Word Segmentation",
      "text" : "Although neural models are widely used on CWS, most of them cannot deal with incompatible criteria with heterogonous segmentation criteria simultaneously. Inspired by the success of multi-task learning (Caruana, 1997; Ben-David and Schuller, 2003),\nwe regard the heterogenous criteria asmultiple “related” tasks, which could improve the performance of each other simultaneously with shared information. Formally, assume that there areM corpora with heterogeneous segmentation criteria. We referDm as corpusm with Nm samples:\nDm = {(X(m)i , Y (m) i )} Nm i=1, (11)\nwhere Xmi and Y mi denote the i-th sentence and the corresponding label in corpusm. To exploit the shared information between these different criteria, we propose three sharing models for CWS task as shown in Figure 2. The feature layers of these three models consist of a private (criterion-specific) layer and a shared (criterioninvariant) layer. The difference between three models is the information flow between the task layer and the shared layer. Besides, all of these three models also share the embedding layer."
    }, {
      "heading" : "3.1 Model-I: Parallel Shared-Private Model",
      "text" : "In the feature layer of Model-I, we regard the private layer and shared layer as two parallel layers. For corpusm, the hidden states of shared layer and private layer are:\nh(s)i =Bi-LSTM(exi , −→h (s)i−1, ←−h (s)i+1, θs), (12)\nh(m)i =Bi-LSTM(exi , −→h (m)i−1, ←−h (m)i+1, θm), (13)\nand the score function in the CRF layer is computed as:\ns(m)(X, i) = W(m)s ⊤ [ h(s)i h(m)i ] + b(m)s , (14)\nwhere W(m)s ∈ R2dh×|L| and b(m)s ∈ R|L| are criterion-specific parameters for corpusm."
    }, {
      "heading" : "3.2 Model-II: Stacked Shared-Private Model",
      "text" : "In the feature layer of Model-II, we arrange the shared layer and private layer in stacked manner. The private layer takes output of shared layer as input. For corpus m, the hidden states of shared layer and private layer are:\nh(s)i = Bi-LSTM(exi , −→h (s)i−1, ←−h (s)i+1, θs), (15)\nh(m)i = Bi-LSTM( [ exi h(s)i ] , −→h (m)i−1, ←−h (m)i+1 , θm) (16)\nand the score function in the CRF layer is computed as:\ns(X, i) = W(m)s ⊤ h(m)i + b (m) s , (17)\nwhere W(m)s ∈ R2dh×|L| and b(m)s ∈ R|L| are criterion-specific parameters for corpusm.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nCRF\nCRF\nTask A\nTask B\nX(A)\nX(B)\nY(B)\nY(A)\n(a) Model-I\nCRF\nCRF\nTask A\nTask B\nX(A)\nX(B)\nY(B)\nY(A)\n(b) Model-II\nCRF\nCRF\nTask A\nTask B\nX(A)\nX(B)\nY(B)\nY(A)\n(c) Model-III\nFigure 2: Three shared-private models for multi-criteria learning. The yellow blocks are the shared BiLSTM layer, while the gray block are the private Bi-LSTM layer. The yellow circles denote the shared embedding layer. The red information flow indicates the difference between three models."
    }, {
      "heading" : "3.3 Model-III: Skip-Layer Shared-Private Model",
      "text" : "In the feature layer of Model-III, the shared layer and private layer are in stacked manner as ModelII. Additionally, we send the outputs of shared layer to CRF layer directly. The Model III can be regarded as a combination ofModel-I andModel-II. For corpusm, the hidden states of shared layer and private layer are the same with Eq (15) and (16), and the score function in CRF layer is computed as the same as Eq (14)."
    }, {
      "heading" : "3.4 Objective function",
      "text" : "The parameters of the network are trained to maximize the log conditional likelihood of true labels on all the corpora. The objective functionJseg can be computed as:\nJseg(Θm,Θs) = M∑\nm=1 Nm∑ i=1 log p(Y (m)i |X (m) i ; Θ m,Θs),\n(18) whereΘm andΘs denote all the parameters in private and shared layers respectively."
    }, {
      "heading" : "4 Incorporating Adversarial Training for Shared Layer",
      "text" : "Although the shared-private model separates the feature space into shared and private spaces, there is no guarantee that sharable features do not exist in private feature space, or vice versa. Inspired by the work on domain adaptation (Ajakan et al., 2014; Ganin et al., 2016; Bousmalis et al., 2016), we hope that the features extracted by shared layer is invariant across the heterogonous segmentation criteria. Therefore, we jointly optimize the shared\nlayer via adversarial training (Goodfellow et al., 2014). Therefore, besides the task loss for CWS, we additionally introduce an adversarial loss to prevent criterion-specific feature from creeping into shared space as shown in Figure 3. We use a criterion discriminator which aims to recognizewhich criterion the sentence is annotated by using the shared features. Specifically, given a sentence X with length n, we refer to h(s)X as shared features for X in one of the sharing models. Here, we compute h(s)X by simply averaging the hidden states of shared layer h(s)X = 1 n ∑n i h (s) xi . The criterion discriminator computes the probability p(·|X) over all criteria as:\np(·|X; Θd,Θs) = softmax(W⊤d h (s) X + bd), (19)\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nwhereΘd indicates the parameters of criterion discriminator Wd ∈ Rdh×M and bd ∈ RM ; Θs denotes the parameters of shared layers."
    }, {
      "heading" : "4.1 Adversarial loss function",
      "text" : "The criterion discriminator maximizes the cross entropy of predicted criterion distribution p(·|X) and true criterion.\nmax Θd J 1adv(Θd) = M∑ m=1 Nm∑ i=1 log p(m|X(m)i ; Θ d,Θs). (20)\nAn adversarial loss aims to produce shared features, such that a criterion discriminator cannot reliably predict the criterion by using these shared features. Therefore, we maximize the entropy of predicted criterion distribution when training shared parameters.\nmax Θs J 2adv(Θs) = M∑ m=1 Nm∑ i=1 H ( p(m|X(m)i ; Θ d,Θs) ) ,\n(21) where H(p) = − ∑ i pi log pi is an entropy of distribution p. Unlike (Ganin et al., 2016), we use entropy term instead of negative cross-entropy."
    }, {
      "heading" : "5 Training",
      "text" : "Finally, we combine the task and adversarial objective functions.\nJ (Θ;D) = Jseg(Θm,Θs) + λJ 1adv(Θd) + λJ 2adv(Θs), (22) where λ is the weight that controls the interaction of the loss terms and D is the training corpora. The training procedure is to optimize two discriminative classifiers alternately as shown in Algorithm 1. We use AdaGrad (Duchi et al., 2011) with minibatchs to maximize the objectives. Notably, when using adversarial strategy, we firstly train 2400 epochs (each epoch only trains on eight batches from different corpora), then we only optimize Jseg(Θm,Θs) with Θs fixed until convergence (early stop strategy)."
    }, {
      "heading" : "6 Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Datasets",
      "text" : "To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIGHAN2005 (Emerson, 2005) and SIGHAN2008 (Jin and Chen, 2008). Table 2 gives the details of the eight datasets. Among\nAlgorithm 1 Adversarial multi-criteria learning for CWS task. 1: for i = 1; i <= n_epoch; i++ do 2: # Train tag predictor for CWS 3: form = 1;m <= M ;m++ do 4: # Randomly pick data from corpusm 5: B = {X,Y }bm1 ∈ Dm 6: Θs += α∇ΘsJ (Θ;B) 7: Θm += α∇ΘmJ (Θ;B) 8: end for 9: # Train criterion discriminator 10: form = 1;m <= M ;m++ do 11: B = {X,Y }bm1 ∈ Dm 12: Θd += α∇ΘdJ (Θ;B) 13: end for 14: end for\nDatasets Nw Nc |Dw| |Dc| Ns Si gh an 05 MSRA Train 2.4M 4.1M 88.1K 5.2K 86.9KTest 0.1M 0.2M 12.9K 2.8K 4.0K AS Train 5.4M 8.4M 141.3K 6.1K 709.0KTest 0.1M 0.2M 18.8K 3.7K 14.4K\nSi gh an 08\nPKU Train 1.1M 1.8M 55.2K 4.7K 47.3KTest 0.2M 0.3M 17.6K 3.4K 6.4K CTB Train 0.6M 1.1M 42.2K 4.2K 23.4KTest 0.1M 0.1M 9.8K 2.6K 2.1K CKIP Train 0.7M 1.1M 48.1K 4.7K 94.2KTest 0.1M 0.1M 15.3K 3.5K 10.9K CITYU Train 1.1M 1.8M 43.6K 4.4K 36.2KTest 0.2M 0.3M 17.8K 3.4K 6.7K\nNCC Train 0.5M 0.8M 45.2K 5.0K 18.9KTest 0.1M 0.2M 17.5K 3.6K 3.6K SXU Train 0.5M 0.9M 32.5K 4.2K 17.1KTest 0.1M 0.2M 12.4K 2.8K 3.7K\nTable 2: Details of eight datasets. Nw and Nc indicate numbers of tokens and characters respectively. Dw and Dc are the dictionaries of distinguished words and characters respectively. Ns indicates the number of sentences.\nthese datasets, AS, CITYU and CKIP are traditional Chinese, while the remains, MSRA, PKU, CTB, NCC and SXU, are simplified Chinese. We use 10% data of shuffled train set as development set for all datasets."
    }, {
      "heading" : "6.2 Experimental Configurations",
      "text" : "Table 3 gives the details of the hyper-parameter configurations. Since the scale of each datasets varies, we use different training batch sizes for datasets. Specifically, we set batch sizes of AS and MSR datasets as 512 and 256 respectively, and 128 for remains. We employ dropout strategy on embedding layer, keeping 80% inputs (20% dropout rate).\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nCharacter embedding size de = 50 Initial learning rate α = 0.2\nLoss weight coefficient λ = 0.05 LSTM dimensionality dh = 100 Dropout rate on input layer p = 20% Batch size 20\nTable 3: Configurations of Hyper-parameters.\nFor initialization, we random all parameters following uniform distribution at (−0.05, 0.05). We simply map traditional Chinese characters to simplified Chinese, and optimize on the same character embedding matrix across datasets, which is pre-trained on Chinese Wikipedia corpus, using word2vec toolkit (Mikolov et al., 2013). Following previous works (Chen et al., 2015; Pei et al., 2014), all experiments including baseline results are using pre-tarined character embedding with bigram feature."
    }, {
      "heading" : "6.3 Overall Results",
      "text" : "Table 4 shows the experiment results of the proposed models on test sets of eight CWS datasets, which has three blocks. (1) In the first block, we can see that the performance of Bi-LSTM cannot be improved by merely increasing the depth of networks. (2) In the second block, our proposed threemodels based on multi-criteria learning boost performance. Model-I gains 0.75% improvement on averaging F-measure score compared with Bi-LSTM result (94.14%). Only the performance on MSRA drops slightly. Compared to the baseline results (Bi-LSTM and stacked Bi-LSTM), the proposed models boost the performance with the help of exploiting information across these heterogeneous segmentation criteria. Although various criteria have different segmentation granularities, there are still some underlying information shared. For instance, MSRA and CTB treat family name and last name as one token “宁泽涛 (NingZeTao)”, whereas some other datasets, like PKU, regard them as two tokens, “宁 (Ning)” and “泽涛 (ZeTao)”. The partial boundaries (before “宁 (Ning)” or after “涛 (Tao)”) can be shared. (3) In the third block, we introduce adversarial training. By introducing adversarial training, the performances are further boosted, and Model-I is slightly better than Model-II and Model-III. The adversarial training tries to make shared layer keep\ncriteria-invariant features. For instance, as shown in Table 4, when we use shared information, the performance onMSRA drops (worse than baseline result). The reason may be that the shared parameters bias to other segmentation criteria and introduce noisy features into shared parameters. When we additionally incorporate the adversarial strategy, we observe that the performance on MSRA is improved and outperforms the baseline results. We could also observe the improvements on other datasets. However, the boost from the adversarial strategy is not significant. The main reason might be that the proposed three sharing models implicitly attempt to keep invariant features by shared parameters and learn discrepancies by the task layer."
    }, {
      "heading" : "6.4 Traditional & Simplified Chinese",
      "text" : "Traditional Chinese and simplified Chinese are two similar languages with slightly difference on character forms and usages on grammar. We investigate that if datasets in traditional Chinese and simplified Chinese could help each other. Table 5 gives the results of Model-I on 3 traditional Chinese datasets under the help of 5 simplified Chinese datasets. Specifically, we firstly train the model on simplified Chinese datasets, then we train traditional Chinese datasets independently with shared parameters fixed. As we can see, the average performance is boosted by 0.41% on F-measure score (from 93.78% to 94.19%), which indicates that shared features learned from simplified Chinese segmentation criteria can help to improve performance on traditional Chinese. Like MSRA, as AS dataset is relatively large (train set of 5.4M tokens), the features learned by shared parameters might bias to other datasets and thus hurt performance on such large dataset AS."
    }, {
      "heading" : "6.5 Speed",
      "text" : "To further explore the convergence speed, we plot the results on development sets through epochs. Figure 4 shows the learning curve ofModel-I without incorporating adversarial strategy. As shown in Figure 4, the proposed model makes progress gradually on all datasets. After about 1000 epochs, the performance becomes stable and convergent. We also test the decoding speed, and our models process 441.38 sentences per second averagely. As the proposed models and the baseline models (Bi-LSTM and stacked Bi-LSTM) are nearly in the same complexity, all models are nearly the\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nModels MSRA AS PKU CTB CKIP CITYU NCC SXU Avg.\nBi-LSTM\nP 95.70 93.64 93.67 95.19 92.44 94.00 91.86 95.11 93.95 R 95.99 94.77 92.93 95.42 93.69 94.15 92.47 95.23 94.33 F 95.84 94.20 93.30 95.30 93.06 94.07 92.17 95.17 94.14 OOV 66.28 70.07 66.09 76.47 72.12 65.79 59.11 71.27 68.40\nStacked Bi-LSTM\nP 95.69 93.89 94.10 95.20 92.40 94.13 91.81 94.99 94.03 R 95.81 94.54 92.66 95.40 93.39 93.99 92.62 95.37 94.22 F 95.75 94.22 93.37 95.30 92.89 94.06 92.21 95.18 94.12 OOV 65.55 71.50 67.92 75.44 70.50 66.35 57.39 69.69 68.04 Multi-Criteria Learning\nModel-I\nP 95.67 94.44 94.93 95.95 93.99 95.10 92.54 96.07 94.84 R 95.82 95.09 93.73 96.00 94.52 95.60 92.69 96.08 94.94 F 95.74 94.76 94.33 95.97 94.26 95.35 92.61 96.07 94.89 OOV 69.89 74.13 72.96 81.12 77.58 80.00 64.14 77.05 74.61\nModel-II\nP 95.74 94.60 94.82 95.90 93.51 95.30 92.26 96.17 94.79 R 95.74 95.20 93.76 95.94 94.56 95.50 92.84 95.95 94.94 F 95.74 94.90 94.28 95.92 94.03 95.40 92.55 96.06 94.86 OOV 69.67 74.87 72.28 79.94 76.67 81.05 61.51 77.96 74.24\nModel-III\nP 95.76 93.99 94.95 95.85 93.50 95.56 92.17 96.10 94.74 R 95.89 95.07 93.48 96.11 94.58 95.62 92.96 96.13 94.98 F 95.82 94.53 94.21 95.98 94.04 95.59 92.57 96.12 94.86 OOV 70.72 72.59 73.12 81.21 76.56 82.14 60.83 77.56 74.34 Adversarial Multi-Criteria Learning\nModel-I+ADV\nP 95.95 94.17 94.86 96.02 93.82 95.39 92.46 96.07 94.84 R 96.14 95.11 93.78 96.33 94.70 95.70 93.19 96.01 95.12 F 96.04 94.64 94.32 96.18 94.26 95.55 92.83 96.04 94.98 OOV 71.60 73.50 72.67 82.48 77.59 81.40 63.31 77.10 74.96\nModel-II+ADV\nP 96.02 94.52 94.65 96.09 93.80 95.37 92.42 95.85 94.84 R 95.86 94.98 93.61 95.90 94.69 95.63 93.20 96.07 94.99 F 95.94 94.75 94.13 96.00 94.24 95.50 92.81 95.96 94.92 OOV 72.76 75.37 73.13 82.19 77.71 81.05 62.16 76.88 75.16\nModel-III+ADV\nP 95.92 94.25 94.68 95.86 93.67 95.24 92.47 96.24 94.79 R 95.83 95.11 93.82 96.10 94.48 95.60 92.73 96.04 94.96 F 95.87 94.68 94.25 95.98 94.07 95.42 92.60 96.14 94.88 OOV 70.86 72.89 72.20 81.65 76.13 80.71 63.22 77.88 74.44\nTable 4: Results of proposed models on test sets of eight CWS datasets. There are three blocks. The first block consists of two baseline models: Bi-LSTM and stacked Bi-LSTM. The second block consists of our proposed three models without adversarial training. The third block consists of our proposed three models with adversarial training.\nAS CKIP CITYU Avg. Base line 94.20 93.06 94.07 93.78 This work 94.12 93.24 95.20 94.19\nTable 5: Performance on 3 traditional Chinese datasets with shared parameters fixed shared. The shared parameters are trained on 5 simplified Chinese datasets. Here, we conduct Model-I without incorporating adversarial training strategy.\nsame efficient. However, the time consumption of training process varies from model to model. For the models without adversarial training, it costs about 10 hours for training (the same for stacked Bi-LSTM to train eight datasets), whereas it takes about 16 hours for the models with adversarial training. All the experiments are conducted on the hardware with Intel(R) Xeon(R) CPU E5-2643 v3 @ 3.40GHz and NVIDIA GeForce GTX TITAN\nX."
    }, {
      "heading" : "6.6 Error Analysis",
      "text" : "We further investigate the benefits of the proposed models by comparing the error distributions between the single-criterion learning (baseline model Bi-LSTM) and multi-criteria learning (Model-I and Model-I with adversarial training) as shown in Figure 5. According to the results, we could observe that a large proportion of points lie above diagonal lines in Figure 5a and Figure 5b, which implies that performance benefit from integrating knowledge and complementary information from other corpora. As shown in Table 4, on the test set of CITYU, the performance of Model-I and its adversarial version (Model-I+ADV) boost from 92.17% to 95.59% and 95.42% respectively. In addition, we observe that adversarial strategy is effective to prevent criterion specific features\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n0 500 1,000 1,500 2,000 2,500\n90\n92\n94\n96\nepoches\nFva lu e( % )\nMSRA AS PKU CTB CKIP CITYU NCC SXU\nFigure 4: Convergence speed of Model-I without adversarial training on development sets of eight datasets.\n40\n50\n60\n70\n80\n90\n100\n40 50 60 70 80 90 100\nM u\nlt i-\nC ri\nte ri\na Le\nar n\nin g\nBase Line (a)\n40\n50\n60\n70\n80\n90\n100\n40 50 60 70 80 90 100\nM u\nlt i-\nC ri\nte ri\na Le\nar n\nin g\n+ A\nd ve\nrs ar\ny\nBase Line (b)\nFigure 5: F-measure scores on test set of CITYU dataset. Each point denotes a sentence, with the (x, y) values of each point denoting the F-measure scores of the two models, respectively. (a) is comparison between Bi-LSTM andModel-I. (b) is comparison between Bi-LSTM and Model-I with adversarial training.\nfrom creeping into shared space. For instance, the segmentation granularity of personal name is often different according to heterogenous criteria. With the help of adversarial strategy, our models could correct a large proportion of mistakes on personal name. Table 6 lists the examples from 2333-th and 89-th sentences in test sets of PKU andMSRA datasets respectively."
    }, {
      "heading" : "7 Related Works",
      "text" : "There are many works on exploiting heterogeneous annotation data to improve various NLP tasks. Jiang et al. (2009) proposed a stackingbased model which could train a model for one specific desired annotation criterion by utilizing knowledge from corpora with other heterogeneous annotations. Sun and Wan (2012) proposed a\nModels PKU-2333 MSRA-89\nGolds Lu Wu Xuan Mu Ling Ying 卢 武铉 穆玲英\nBase Line 卢武铉 穆 玲英 Model-I 卢武铉 穆 玲英\nModell-I+ADV 卢 武铉 穆玲英\nTable 6: Segmentation cases of personal name.\nstructure-based stacking model to reduce the approximation error, which makes use of structured features such as sub-words. These models are unidirectional aid and also suffer from error propagation problem. Qiu et al. (2013) used multi-tasks learning framework to improve the performance of POS tagging on two heterogeneous datasets. Li et al. (2015) proposed a coupled sequence labeling model which could directly learn and infer two heterogeneous annotations. Chao et al. (2015) also utilize multiple corpora using coupled sequence labeling model. These methods adopt the shallow classifiers, therefore suffering from the problem of defining shared features. Our proposed models use deep neural networks, which can easily share information with hidden shared layers. Chen et al. (2016) also adopted neural network models for exploiting heterogeneous annotations based on neural multi-view model, which can be regarded as a simplified version of our proposed models by removing private hidden layers. Unlike the above models, we design three sharing-private architectures and keep shared layer to extract criterion-invariance features by introducing adversarial training. Moreover, we fully exploit eight corpora with heterogeneous segmentation criteria to model the underlying shared information."
    }, {
      "heading" : "8 Conclusions & Future Works",
      "text" : "In this paper, we propose adversarial multi-criteria learning for CWS by fully exploiting the underlying shared knowledge across multiple heterogeneous criteria. Experiments show that our proposed three shared-private models are effective to extract the shared information, and achieve significant improvements over the single-criterion methods.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Domain-adversarial neural networks",
      "author" : [ "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "François Laviolette", "Mario Marchand." ],
      "venue" : "arXiv preprint arXiv:1412.4446 .",
      "citeRegEx" : "Ajakan et al\\.,? 2014",
      "shortCiteRegEx" : "Ajakan et al\\.",
      "year" : 2014
    }, {
      "title" : "Exploiting task relatedness for multiple task learning",
      "author" : [ "S. Ben-David", "R. Schuller." ],
      "venue" : "Learning Theory and Kernel Machines pages 567–580.",
      "citeRegEx" : "Ben.David and Schuller.,? 2003",
      "shortCiteRegEx" : "Ben.David and Schuller.",
      "year" : 2003
    }, {
      "title" : "Domain separation networks",
      "author" : [ "Konstantinos Bousmalis", "George Trigeorgis", "Nathan Silberman", "Dilip Krishnan", "Dumitru Erhan." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 343– 351.",
      "citeRegEx" : "Bousmalis et al\\.,? 2016",
      "shortCiteRegEx" : "Bousmalis et al\\.",
      "year" : 2016
    }, {
      "title" : "Neural word segmentation learning for chinese",
      "author" : [ "Deng Cai", "Hai Zhao." ],
      "venue" : "arXiv preprint arXiv:1606.04300 .",
      "citeRegEx" : "Cai and Zhao.,? 2016",
      "shortCiteRegEx" : "Cai and Zhao.",
      "year" : 2016
    }, {
      "title" : "Multitask learning",
      "author" : [ "Rich Caruana." ],
      "venue" : "Machine learning 28(1):41–75.",
      "citeRegEx" : "Caruana.,? 1997",
      "shortCiteRegEx" : "Caruana.",
      "year" : 1997
    }, {
      "title" : "Exploiting heterogeneous annotations for weibo word segmentation and pos tagging",
      "author" : [ "Jiayuan Chao", "Zhenghua Li", "Wenliang Chen", "Min Zhang." ],
      "venue" : "National CCF Conference on Natural Language Processing and Chinese Computing. Springer, pages",
      "citeRegEx" : "Chao et al\\.,? 2015",
      "shortCiteRegEx" : "Chao et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural network for heterogeneous annotations",
      "author" : [ "Hongshen Chen", "Yue Zhang", "Qun Liu." ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing .",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Long short-term memory neural networks for chinese word segmentation",
      "author" : [ "Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang." ],
      "venue" : "EMNLP. pages 1197–1206.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "Ronan Collobert", "Jason Weston." ],
      "venue" : "Proceedings of ICML.",
      "citeRegEx" : "Collobert and Weston.,? 2008",
      "shortCiteRegEx" : "Collobert and Weston.",
      "year" : 2008
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer." ],
      "venue" : "The Journal ofMachine Learning Research 12:2121–2159.",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "The second international chinese word segmentation bakeoff",
      "author" : [ "Thomas Emerson." ],
      "venue" : "Proceedings of the fourth SIGHAN workshop on Chinese language Processing. volume 133.",
      "citeRegEx" : "Emerson.,? 2005",
      "shortCiteRegEx" : "Emerson.",
      "year" : 2005
    }, {
      "title" : "Domain-adversarial training of neural networks",
      "author" : [ "Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "François Laviolette", "Mario Marchand", "Victor Lempitsky." ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Ganin et al\\.,? 2016",
      "shortCiteRegEx" : "Ganin et al\\.",
      "year" : 2016
    }, {
      "title" : "Generative adversarial nets",
      "author" : [ "Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "Advances in Neural Information Processing Systems. pages 2672–2680.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Generating sequences with recurrent neural networks",
      "author" : [ "Alex Graves." ],
      "venue" : "arXiv preprint arXiv:1308.0850 .",
      "citeRegEx" : "Graves.,? 2013",
      "shortCiteRegEx" : "Graves.",
      "year" : 2013
    }, {
      "title" : "Bidirectional lstm-crf models for sequence tagging",
      "author" : [ "Zhiheng Huang", "Wei Xu", "Kai Yu." ],
      "venue" : "arXiv preprint arXiv:1508.01991 .",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic adaptation of annotation standards: Chinese word segmentation and POS tagging: a case study",
      "author" : [ "W. Jiang", "L. Huang", "Q. Liu." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint",
      "citeRegEx" : "Jiang et al\\.,? 2009",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2009
    }, {
      "title" : "The fourth international chinese language processing bakeoff: Chinese word segmentation, named entity recognition and chinese pos tagging",
      "author" : [ "G. Jin", "X. Chen." ],
      "venue" : "Sixth SIGHANWorkshop on Chinese Language Processing. page 69.",
      "citeRegEx" : "Jin and Chen.,? 2008",
      "shortCiteRegEx" : "Jin and Chen.",
      "year" : 2008
    }, {
      "title" : "An empirical exploration of recurrent network architectures",
      "author" : [ "Rafal Jozefowicz", "Wojciech Zaremba", "Ilya Sutskever." ],
      "venue" : "Proceedings of The 32nd International Conference on Machine Learning.",
      "citeRegEx" : "Jozefowicz et al\\.,? 2015",
      "shortCiteRegEx" : "Jozefowicz et al\\.",
      "year" : 2015
    }, {
      "title" : "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "author" : [ "John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira." ],
      "venue" : "Proceedings of the Eighteenth International Conference on Machine Learning.",
      "citeRegEx" : "Lafferty et al\\.,? 2001",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2001
    }, {
      "title" : "Coupled sequence labeling on heterogeneous annotations: Pos tagging as a case study",
      "author" : [ "Zhenghua Li", "Jiayuan Chao", "Min Zhang", "Wenliang Chen." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th In-",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "Multi-task sequence to sequence learning",
      "author" : [ "Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser." ],
      "venue" : "arXiv preprint arXiv:1511.06114 .",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Accurate linear-time chinese word segmentation via embedding matching",
      "author" : [ "Jianqiang Ma", "Erhard W Hinrichs." ],
      "venue" : "ACL (1). pages 1733–1743.",
      "citeRegEx" : "Ma and Hinrichs.,? 2015",
      "shortCiteRegEx" : "Ma and Hinrichs.",
      "year" : 2015
    }, {
      "title" : "End-to-end sequence labeling via bi-directional lstm-cnns-crf",
      "author" : [ "Xuezhe Ma", "Eduard Hovy." ],
      "venue" : "arXiv preprint arXiv:1603.01354 .",
      "citeRegEx" : "Ma and Hovy.,? 2016",
      "shortCiteRegEx" : "Ma and Hovy.",
      "year" : 2016
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781 .",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Maxmargin tensor neural network for chinese word segmentation",
      "author" : [ "Wenzhe Pei", "Tao Ge", "Chang Baobao." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Pei et al\\.,? 2014",
      "shortCiteRegEx" : "Pei et al\\.",
      "year" : 2014
    }, {
      "title" : "Joint chinese word segmentation and pos tagging on heterogeneous annotated corpora with multiple task learning",
      "author" : [ "Xipeng Qiu", "Jiayi Zhao", "Xuanjing Huang." ],
      "venue" : "EMNLP. pages 658–668.",
      "citeRegEx" : "Qiu et al\\.,? 2013",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2013
    }, {
      "title" : "Reducing approximation and estimation errors for chinese lexical processing with heterogeneous annotations",
      "author" : [ "Weiwei Sun", "Xiaojun Wan." ],
      "venue" : "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-",
      "citeRegEx" : "Sun and Wan.,? 2012",
      "shortCiteRegEx" : "Sun and Wan.",
      "year" : 2012
    }, {
      "title" : "The part-of-speech tagging guidelines for the penn Chinese treebank (3.0)",
      "author" : [ "F. Xia" ],
      "venue" : null,
      "citeRegEx" : "Xia.,? \\Q2000\\E",
      "shortCiteRegEx" : "Xia.",
      "year" : 2000
    }, {
      "title" : "Dependency-based gated recursive neural network for chinese word segmentation",
      "author" : [ "Jingjing Xu", "Xu Sun." ],
      "venue" : "The 54th Annual Meeting of the Association for Computational Linguistics. page 567.",
      "citeRegEx" : "Xu and Sun.,? 2016",
      "shortCiteRegEx" : "Xu and Sun.",
      "year" : 2016
    }, {
      "title" : "Chinese word segmentation as character tagging",
      "author" : [ "N. Xue." ],
      "venue" : "Computational Linguistics and Chinese Language Processing 8(1):29–48.",
      "citeRegEx" : "Xue.,? 2003",
      "shortCiteRegEx" : "Xue.",
      "year" : 2003
    }, {
      "title" : "Bi-directional lstm recurrent neural network for chinese word segmentation",
      "author" : [ "Yushi Yao", "Zheng Huang." ],
      "venue" : "International Conference on Neural Information Processing. Springer, pages 345–353.",
      "citeRegEx" : "Yao and Huang.,? 2016",
      "shortCiteRegEx" : "Yao and Huang.",
      "year" : 2016
    }, {
      "title" : "Processing norms of modern Chinese corpus",
      "author" : [ "S. Yu", "J. Lu", "X. Zhu", "H. Duan", "S. Kang", "H. Sun", "H. Wang", "Q. Zhao", "W. Zhan." ],
      "venue" : "Technical report, Technical report.",
      "citeRegEx" : "Yu et al\\.,? 2001",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2001
    }, {
      "title" : "Transition-based neural word segmentation",
      "author" : [ "Meishan Zhang", "Yue Zhang", "Guohong Fu." ],
      "venue" : "Proceedings of the 54nd ACL .",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "An improved chinese word segmentation system with conditional random field",
      "author" : [ "H. Zhao", "C.N. Huang", "M. Li." ],
      "venue" : "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing. Sydney: July, volume 1082117.",
      "citeRegEx" : "Zhao et al\\.,? 2006",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2006
    }, {
      "title" : "Deep learning for chinese word segmentation and pos tagging",
      "author" : [ "Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu." ],
      "venue" : "EMNLP. pages 647–657.",
      "citeRegEx" : "Zheng et al\\.,? 2013",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "Currently, the state-ofthe-art methods are based on statistical supervised learning algorithms (Xue, 2003; Zhao et al., 2006), and rely on a large-scale annotated corpus whose cost is extremely expensive.",
      "startOffset" : 95,
      "endOffset" : 125
    }, {
      "referenceID" : 33,
      "context" : "Currently, the state-ofthe-art methods are based on statistical supervised learning algorithms (Xue, 2003; Zhao et al., 2006), and rely on a large-scale annotated corpus whose cost is extremely expensive.",
      "startOffset" : 95,
      "endOffset" : 125
    }, {
      "referenceID" : 31,
      "context" : "As shown in Table 1, given a sentence “姚明进入总决赛 (YaoMing reaches the final)”, the two commonly-used corpora, PKU’s People’s Daily (PKU) (Yu et al., 2001) and Penn Chinese Treebank (CTB) (Xia, 2000), use different segmentation criteria.",
      "startOffset" : 135,
      "endOffset" : 152
    }, {
      "referenceID" : 27,
      "context" : ", 2001) and Penn Chinese Treebank (CTB) (Xia, 2000), use different segmentation criteria.",
      "startOffset" : 40,
      "endOffset" : 51
    }, {
      "referenceID" : 15,
      "context" : "(Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015).",
      "startOffset" : 0,
      "endOffset" : 74
    }, {
      "referenceID" : 26,
      "context" : "(Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015).",
      "startOffset" : 0,
      "endOffset" : 74
    }, {
      "referenceID" : 25,
      "context" : "(Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015).",
      "startOffset" : 0,
      "endOffset" : 74
    }, {
      "referenceID" : 19,
      "context" : "(Jiang et al., 2009; Sun and Wan, 2012; Qiu et al., 2013; Li et al., 2015).",
      "startOffset" : 0,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016).",
      "startOffset" : 106,
      "endOffset" : 173
    }, {
      "referenceID" : 20,
      "context" : "Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016).",
      "startOffset" : 106,
      "endOffset" : 173
    }, {
      "referenceID" : 6,
      "context" : "Fortunately, recent deep neural models provide a convenient way to share information among multiple tasks (Collobert and Weston, 2008; Luong et al., 2015; Chen et al., 2016).",
      "startOffset" : 106,
      "endOffset" : 173
    }, {
      "referenceID" : 4,
      "context" : "Specifically, we regard each segmentation criterion as a single task and propose three different shared-private models under the framework of multi-task learning (Caruana, 1997; Ben-David and Schuller, 2003), where a shared layer is used to extract the criteria-invariant features, and a private layer is used to extract the criteria-specific features.",
      "startOffset" : 162,
      "endOffset" : 207
    }, {
      "referenceID" : 1,
      "context" : "Specifically, we regard each segmentation criterion as a single task and propose three different shared-private models under the framework of multi-task learning (Caruana, 1997; Ben-David and Schuller, 2003), where a shared layer is used to extract the criteria-invariant features, and a private layer is used to extract the criteria-specific features.",
      "startOffset" : 162,
      "endOffset" : 207
    }, {
      "referenceID" : 0,
      "context" : "Inspired by the success of adversarial strategy on domain adaption (Ajakan et al., 2014; Ganin et al., 2016; Bousmalis et al., 2016), we further utilize adversarial strategy to make sure the shared layer can extract the common underlying and criteria-invariant features, which are suitable for all the criteria.",
      "startOffset" : 67,
      "endOffset" : 132
    }, {
      "referenceID" : 11,
      "context" : "Inspired by the success of adversarial strategy on domain adaption (Ajakan et al., 2014; Ganin et al., 2016; Bousmalis et al., 2016), we further utilize adversarial strategy to make sure the shared layer can extract the common underlying and criteria-invariant features, which are suitable for all the criteria.",
      "startOffset" : 67,
      "endOffset" : 132
    }, {
      "referenceID" : 2,
      "context" : "Inspired by the success of adversarial strategy on domain adaption (Ajakan et al., 2014; Ganin et al., 2016; Bousmalis et al., 2016), we further utilize adversarial strategy to make sure the shared layer can extract the common underlying and criteria-invariant features, which are suitable for all the criteria.",
      "startOffset" : 67,
      "endOffset" : 132
    }, {
      "referenceID" : 34,
      "context" : "Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Xu and Sun, 2016; Yao and Huang, 2016; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 302
    }, {
      "referenceID" : 24,
      "context" : "Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Xu and Sun, 2016; Yao and Huang, 2016; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 302
    }, {
      "referenceID" : 7,
      "context" : "Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Xu and Sun, 2016; Yao and Huang, 2016; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 302
    }, {
      "referenceID" : 21,
      "context" : "Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Xu and Sun, 2016; Yao and Huang, 2016; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 302
    }, {
      "referenceID" : 28,
      "context" : "Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Xu and Sun, 2016; Yao and Huang, 2016; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 302
    }, {
      "referenceID" : 30,
      "context" : "Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Xu and Sun, 2016; Yao and Huang, 2016; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 302
    }, {
      "referenceID" : 3,
      "context" : "Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Xu and Sun, 2016; Yao and Huang, 2016; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 302
    }, {
      "referenceID" : 32,
      "context" : "Recently, neural networks are widely applied to Chinese word segmentation task for their ability to minimize the effort in feature engineering (Zheng et al., 2013; Pei et al., 2014; Chen et al., 2015; Ma and Hinrichs, 2015; Xu and Sun, 2016; Yao and Huang, 2016; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 143,
      "endOffset" : 302
    }, {
      "referenceID" : 14,
      "context" : "(Huang et al., 2015; Ma and Hovy, 2016) which adopts the bi-direction long short-term memory with CRF as tag inference layer.",
      "startOffset" : 0,
      "endOffset" : 39
    }, {
      "referenceID" : 22,
      "context" : "(Huang et al., 2015; Ma and Hovy, 2016) which adopts the bi-direction long short-term memory with CRF as tag inference layer.",
      "startOffset" : 0,
      "endOffset" : 39
    }, {
      "referenceID" : 17,
      "context" : "While there are numerous LSTM variants, here we use the LSTM architecture used by (Jozefowicz et al., 2015), which is similar to the architecture of (Graves, 2013) but without peep-hole connections.",
      "startOffset" : 82,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : ", 2015), which is similar to the architecture of (Graves, 2013) but without peep-hole connections.",
      "startOffset" : 49,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : "3 Inference Layer After extracting features, we employ conditional random fields (CRF) (Lafferty et al., 2001) layer to inference tags.",
      "startOffset" : 87,
      "endOffset" : 110
    }, {
      "referenceID" : 4,
      "context" : "Inspired by the success of multi-task learning (Caruana, 1997; Ben-David and Schuller, 2003), we regard the heterogenous criteria asmultiple “related” tasks, which could improve the performance of each other simultaneously with shared information.",
      "startOffset" : 47,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "Inspired by the success of multi-task learning (Caruana, 1997; Ben-David and Schuller, 2003), we regard the heterogenous criteria asmultiple “related” tasks, which could improve the performance of each other simultaneously with shared information.",
      "startOffset" : 47,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "Inspired by the work on domain adaptation (Ajakan et al., 2014; Ganin et al., 2016; Bousmalis et al., 2016), we hope that the features extracted by shared layer is invariant across the heterogonous segmentation criteria.",
      "startOffset" : 42,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "Inspired by the work on domain adaptation (Ajakan et al., 2014; Ganin et al., 2016; Bousmalis et al., 2016), we hope that the features extracted by shared layer is invariant across the heterogonous segmentation criteria.",
      "startOffset" : 42,
      "endOffset" : 107
    }, {
      "referenceID" : 2,
      "context" : "Inspired by the work on domain adaptation (Ajakan et al., 2014; Ganin et al., 2016; Bousmalis et al., 2016), we hope that the features extracted by shared layer is invariant across the heterogonous segmentation criteria.",
      "startOffset" : 42,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "layer via adversarial training (Goodfellow et al., 2014).",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 11,
      "context" : "Unlike (Ganin et al., 2016), we use entropy term instead of negative cross-entropy.",
      "startOffset" : 7,
      "endOffset" : 27
    }, {
      "referenceID" : 9,
      "context" : "We use AdaGrad (Duchi et al., 2011) with minibatchs to maximize the objectives.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "1 Datasets To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIGHAN2005 (Emerson, 2005) and SIGHAN2008 (Jin and Chen, 2008).",
      "startOffset" : 112,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : "1 Datasets To evaluate our proposed architecture, we experiment on eight prevalent CWS datasets from SIGHAN2005 (Emerson, 2005) and SIGHAN2008 (Jin and Chen, 2008).",
      "startOffset" : 143,
      "endOffset" : 163
    }, {
      "referenceID" : 23,
      "context" : "We simply map traditional Chinese characters to simplified Chinese, and optimize on the same character embedding matrix across datasets, which is pre-trained on Chinese Wikipedia corpus, using word2vec toolkit (Mikolov et al., 2013).",
      "startOffset" : 210,
      "endOffset" : 232
    }, {
      "referenceID" : 7,
      "context" : "Following previous works (Chen et al., 2015; Pei et al., 2014), all experiments including baseline results are using pre-tarined character embedding with bigram feature.",
      "startOffset" : 25,
      "endOffset" : 62
    }, {
      "referenceID" : 24,
      "context" : "Following previous works (Chen et al., 2015; Pei et al., 2014), all experiments including baseline results are using pre-tarined character embedding with bigram feature.",
      "startOffset" : 25,
      "endOffset" : 62
    }, {
      "referenceID" : 15,
      "context" : "Jiang et al. (2009) proposed a stackingbased model which could train a model for one specific desired annotation criterion by utilizing knowledge from corpora with other heterogeneous annotations.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 15,
      "context" : "Jiang et al. (2009) proposed a stackingbased model which could train a model for one specific desired annotation criterion by utilizing knowledge from corpora with other heterogeneous annotations. Sun and Wan (2012) proposed a Models PKU-2333 MSRA-89 Golds Lu Wu Xuan Mu Ling Ying 卢 武铉 穆玲英 Base Line 卢武铉 穆 玲英 Model-I 卢武铉 穆 玲英 Modell-I+ADV 卢 武铉 穆玲英",
      "startOffset" : 0,
      "endOffset" : 216
    }, {
      "referenceID" : 21,
      "context" : "Qiu et al. (2013) used multi-tasks learning framework to improve the performance of POS tagging on two heterogeneous datasets.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 16,
      "context" : "Li et al. (2015) proposed a coupled sequence labeling model which could directly learn and infer two heterogeneous annotations.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 5,
      "context" : "Chao et al. (2015) also utilize multiple corpora using coupled sequence labeling model.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "Chao et al. (2015) also utilize multiple corpora using coupled sequence labeling model. These methods adopt the shallow classifiers, therefore suffering from the problem of defining shared features. Our proposed models use deep neural networks, which can easily share information with hidden shared layers. Chen et al. (2016) also adopted neural network models for exploiting heterogeneous annotations based on neural multi-view model, which can be regarded as a simplified version of our proposed models by removing private hidden layers.",
      "startOffset" : 0,
      "endOffset" : 326
    } ],
    "year" : 2017,
    "abstractText" : "Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existingmethods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge frommultiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning.",
    "creator" : "LaTeX with hyperref package"
  }
}