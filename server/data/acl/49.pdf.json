{
  "name" : "49.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Chunk-based Decoder for Neural Machine Translation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural machine translation (NMT) performs an end-to-end translation based on a simple encoderdecoder model (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), and now has overwhelmed the classical, complex statistical machine translation (SMT) (Sennrich et al., 2016; Luong and Manning, 2016; Cromieres et al., 2016; Neubig, 2016). In NMT, an encoder first maps a source sequence into vector representations and a decoder then maps the vectors into a target sequence (§ 2). This simple framework allows researchers to incorporate the structure of the source\nsentence as in SMT by leveraging various architectures as the encoder (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b; Eriguchi et al., 2016b). Most of the NMT models, however, still rely on a sequential decoder based on recurrent neural network (RNN), due to the difficulty in capturing the structure of a target sentence that is unseen during translation.\nWith the sequential decoder, however, there are two problems to be solved. First, it is difficult to model long-distance dependencies (Bahdanau et al., 2015). A hidden state ht in an RNN is only conditioned by its previous output yt−1, previous hidden state ht−1 and current input xt. This makes it difficult to capture the dependencies between an older output yt−N if they are too far from the current output. This problem can become more serious when the target sequence become longer. For example in Figure 1, when one translates the English sentence into the Japanese one, after the decoder predicts the content word “噛ま (bite)”, it has to predict five function words “れ (passive)”, “た (past)”, “そう (hearsay)”, “だ (positive)”, and “けれど (but)” and a punctuation mark “、” before predicting the next content word “君 (you)”. In such a case, the decoder is required to capture the longer dependencies in a target sentence.\nAnother problem with the sequential decoder is that it is expected to cover possible word orders simply by memorizing the local word sequences in the limited training data. This problem can be more serious in free word-order languages such as Czech, German, Japanese, and Turkish. In the case of the example in Figure 1, the order of the subject phrase “だれかが (someone was)” and the modifier phrase “犬に (by a dog)” are flexible. This means that simply memorizing the word order in training data is not enough to train a model that can assign a high probability to a correct sentence regardless of its word order.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n!\" # $ % & '( \" ) *+ ! ,\"- . / 0 12 3 4 5 65 7 !\"#$\"%$ &\"' ()*$ +\", )%-,.$\n%,// %,//\n0%1 233 34$5.&3 33 *45* !\"#$\"%$3 3365! ()**$%3 33(+ 5 &\"'3 3373 336$.$%8* +\",333 )%-,.$&3 339\n:51\nFigure 1: Translation from English to Japanese. The aligned words are content words and the underlined words are function words.\nLooking back to the past, chunks (or phrases) are utilized to handle the above aforementioned problems in statistical machine translation (SMT) (Watanabe et al., 2003; Koehn et al., 2003) and in example-based machine translation (EBMT) (Kim et al., 2010). By using a chunk rather than a word as the basic translation unit, one can treat a sentence as a shorter sequence. This makes it easy to capture the longer dependencies in a target sentence. The order of words in a chunk is relatively fixed while that in a sentence is much more flexible. Thus, modeling intra-chunk (local) dependencies and inter-chunk (global) dependencies independently can help capture the difference of the flexibility between the word order and the chunk order in free word-order languages.\nIn this paper, we refine the original RNN decoder to consider chunk information in NMT. We propose three novel NMT models that capture and utilize the chunk structure in the target language (§ 3). Our focus is the hierarchical structure of a sentence: each sentence consists of chunks, and each chunk consists of words. To encourage an NMT model to capture the hierarchical structure, we start from a hierarchical RNN that consists of a chunk-level decoder and a word-level decoder (Model 1). Then, we improve the word-level decoder by introducing inter-chunk connections to capture the interaction between chunks (Model 2). Finally, we introduce a feedback mechanism to the chunk-level decoder to enhance the memory capacity of previous outputs (Model 3).\nWe evaluate the three models on the WAT ’16 English-to-Japanese translation task (§ 4). The experimental results show that our best model outperforms the best single NMT model reported in WAT ’16 (Eriguchi et al., 2016b).\nOur contributions are twofold: (1) chunk information is firstly introduced into NMT to improve translation performance, and (2) a novel hierarchical decoder is devised to model the properties of chunk structure in the encoder-decoder framework."
    }, {
      "heading" : "2 Preliminaries: Attention-based Neural Machine Translation",
      "text" : "In this section, we briefly introduce the architecture of the attention-based NMT model, which is the basis of our proposed models."
    }, {
      "heading" : "2.1 Neural Machine Translation",
      "text" : "An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps a source sentence into a fixed-length vector, the decoder maps the vector into a target sentence. The implementation of the encoder can be a convolutional neural network (CNN) (Kalchbrenner and Blunsom, 2013), a long short-term memory (LSTM) (Sutskever et al., 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al., 2014b; Bahdanau et al., 2015), or a Tree-LSTM (Eriguchi et al., 2016b). While various architectures are leveraged as an encoder to capture the structural information in the source language, most of the NMT models rely on a standard sequential network such as LSTM or GRU as its decoder.\nFollowing (Bahdanau et al., 2015), we use GRU as the recurrent unit in this paper. A GRU unit computes its hidden state vector hi given an input vector xi and the previous hidden state hi−1:\nhi = GRU(hi−1, xi). (1)\nThe function GRU(·) is calculated as:\nri = σ(Wrxi +Urhi−1 + br), (2)\nzi = σ(Wzxi +Uzhi−1 + bz), (3)\nh̃i = tanh(Wxi +U(ri hi−1 + b)), (4) hi = (1− zi) h̃i + zi hi−1, (5)\nwhere the vectors ri and zi are reset gate and update gate, respectively. While the former gate allows the model to forget the previous states, the latter gate decides how much the model updates its content. All the W s and Us, or the bs above\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n!\"\n!\"#$%&'( )*%%&\"( +,-,&+\n#\"\n#$\"\nだれ か が %\"\n&'\n.\n&( &).\n/ )&-'% *)\n.\n.#(\n#$(\n#'\n#$'\nFigure 2: Standard word-based decoder.\nare trainable matrices or vectors. σ(·) and denote the sigmoid function and element-wise multiplication operator, respectively.\nWe train a GRU that encodes a source sentence {x1, · · · , xI} into a single vector hI . At the same time, we jointly train another GRU that decodes hI to the target sentence {y1, · · · , yJ}. Here, the jth word in the target sentence yj can be predicted with this decoder GRU and a nonlinear function g(·) followed by a softmax layer, as\nc = hI , (6)\nsj = GRU(sj−1, [yj−1; c]), (7)\ns̃j = g(yj−1, sj , c), (8) P (yj |y<j ,x) = softmax(s̃j), (9)\nwhere c is a context vector of the encoded sentence and sj is a hidden state of the decoder GRU.\nFollowing Bahdanau et al. (2015), we use a mini-batch stochastic gradient descent (SGD) algorithm with ADADELTA (Zeiler, 2012) to train the above two GRUs (i.e., the encoder and the decoder) jointly. The objective is to minimize the cross-entropy loss of the training data D, as\nJ = ∑\n(x,y)∈D\n− logP (y|x). (10)"
    }, {
      "heading" : "2.2 Attention Mechanism for Neural Machine Translation",
      "text" : "To use all the hidden states of the encoder and improve the translation performance of long sentences, Bahdanau et al. (2015) proposed using an attention mechanism. In their model, the context vector is not simply the last encoder state hI but rather the weighted sum of all encoder states, as follows:\ncj = I∑\ni=1\nαjihi. (11)\nHere, the weight αji decides how much a source word xi contributes to the target word yj . αji is\ncomputed by a feedforward layer and a softmax layer as\neji = v · tanh(Wehi +Uesj + be), (12) αji = exp(eji)∑J\nj′=1 exp(ej′i) , (13)\nwhere We, Ue are trainable matrices and the be is a trainable vector. In a decoder using the attention mechanism, the obtained context vector cj in each timestep replaces cs in Eqs. (7) and (8). An illustration of the NMT model with the attention mechanism is shown in Figure 2.\nThe attention mechanism is expected to learn alignments between source and target words, and plays a similar role to the translation model in phrase-based SMT (Koehn et al., 2003)."
    }, {
      "heading" : "3 Chunk-based Neural Machine Translation",
      "text" : "Taking non-sequential information such as chunks (or phrases) structure into consideration is proved to be helpful for SMT (Watanabe et al., 2003; Koehn et al., 2003) and EBMT (Kim et al., 2010). We here focus on two important properties of chunks (Abney, 1991): (1) The word order in a chunk is almost always fixed; (2) A chunk consists of a few (typically one) content words surrounded by zero or more function words.\nTo fully utilize the above properties of a chunk, we propose to model the intra-chunk and the interchunk dependencies independently with a “chunkby-chunk” decoder (See Figure 3). In the standard word-by-word decoder described in § 2, a target word yj in the target sentence y is predicted by taking the previous outputs y<j and the source\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n!\"\n!\"#$%&'('& )'*\"$'#+ ,!-./.-0\n1'23'4567&+84*\"$'#+ ,!-././0\n#$\n%$ &'( %) &'(\n9:34;+/ 9:34;+;%/ 9:34;+;\n%\" &'(\n%*\" &'(\nだれ か が 犬 噛ま れ\n%*$ &'( %*) &'(\n+\"\n<\n%,-./ &'(\n+,-./\n9:34;%&'('& )'*\"$'#+ ,!-./.=0\n#) #0<\n> :'7#$ 10\n<\n<\n<\n!\"#$%& '(&)*+$,-./0*1& .\"**$2+3\"*& 4§56'7\n!\"#$%& 5(&8\",#-+\"-./0*1& 9$$#:;21& 4§5657\n%$ &2(\n%3 &2( %34$ &2(\n%\" &2(\nFigure 4: Proposed model: Chunk-based NMT. A chunk-level decoder generates a chunk representation for each chunk while a word-level decoder uses the representation to predict each word. The solid lines in the figure illustrate Model 1. The dashed blue arrows in the word-level decoder denote the connections added in Model 2. The dotted red arrows in the chunk-level decoder denote the feedback states added in Model 3; the connections in the thick black arrows are replaced with the dotted red arrows.\nsentence x as input:\nP (y|x) = J∏\nj=1\nP (yj |y<j ,x), (14)\nwhere J is the length of the target sentence. Not assuming any structural information of the target language, the sequential decoder has to memorize long dependencies in a sequence. To release the model from the strong pressure of memorizing the long dependencies over a sentence, we redefine this problem as the combination of a word prediction problem and a chunk generation problem: P (y|x) = K∏ k=1 P (ck|c<k,x) Jk∏ j=1 P (yj |y<j , ck,x)  , (15) whereK is the number of chunks in the target sentence and Jk is the length of the k-th chunk (see Figure 3). The first term represents the generation probability of a chunk and the second term indicates the probability of a word in the chunk . We model the former term as a chunk-level decoder and the latter term as a word-level decoder. As we will later confirm in § 4, both K and Jk are much shorter than the sentence length J , which is why our decoders do not have to memorize the long dependencies like the standard decoder does.\nIn the above formulation, we model the information of the words and their orders in a chunk.\nNo matter which language we target, a chunk usually consists of some content words and function words, and the word order in the chunk is almost always fixed (Abney, 1991). Although our idea can be used in several languages, the optimal network architecture could be adapted depending on the word order of the target language. In this work, we design models for languages in which content words are followed by function words, such as Japanese and Korean. The details of our models are described in the following sections."
    }, {
      "heading" : "3.1 Model 1: Standard Chunk-based NMT",
      "text" : "The model described in this section is the basis of our proposed models. It consists of three parts: a sequential encoder (§ 3.1.1), a chunk-level decoder (§ 3.1.2), and a word-level decoder (§ 3.1.3). The part drawn in black solid lines in Figure 4 illustrates the architecture of Model 1."
    }, {
      "heading" : "3.1.1 Sequential Encoder",
      "text" : "We adopt a standard single-layer bidirectional GRU (Cho et al., 2014b; Bahdanau et al., 2015) as our encoder (see the right part in Figure 4). By using a standard sequential encoder, we need not perform any additional preprocessing on test data such as syntactic parsing. This design prevents our model from being affected by any errors that may occur as a result of additional preprocessing during test time.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499"
    }, {
      "heading" : "3.1.2 Chunk-level Decoder",
      "text" : "Our chunk-level decoder (see Figure 3) outputs a chunk representation. The chunk representation contains the information about the words that should be predicted by the word-level decoder.\nTo generate the representation of the k-th chunk s̃ (c) k , the chunk-level decoder (see the bottom layer in Figure 4) takes the last states of the word-level decoder s(w)Jk−1 and updates its hidden state s (c) k as:\ns (c) k = GRU(s (c) k−1, s (w) Jk−1 ), (16) s̃ (c) k = Wcs (c) k + bc. (17)\nThe obtained chunk representation s̃(c)k continues to be fed into the word-level decoder until it outputs all the words in current chunk."
    }, {
      "heading" : "3.1.3 Word-level Decoder",
      "text" : "Our word-level decoder (see Figure 4) differs from the standard sequential decoder described in § 2 in that it takes the chunk representation s̃(c)k as input:\ns (w) j = GRU(s (w) j−1, [s̃ (c) k ; yj−1; c (w) j−1]), (18) s̃ (w) j = g(yj−1, s (w) j , c (w) j ), (19)\nP (yj |y<j ,x) = softmax(s̃(w)j ). (20)\nIn a standard sequential decoder, the hidden state iterates over the length of a target sentence. In other words, its hidden layers are required to memorize the long-term dependencies in the target language. In contrast, in our word-level decoder, the hidden state iterates only over the length of a chunk. Thus, our word-level decoder is released from the pressure of memorizing the long (interchunk) dependencies and can focus on learning the short (intra-chunk) dependencies."
    }, {
      "heading" : "3.2 Model 2: Inter-Chunk Connection",
      "text" : "The second term in Eq. (15) only iterates over a chunk (j = 1 to Jk). This means that the last state and the last output of a chunk are not being fed into the word-level decoder at the next timestep (see the black part in Figure 4). In other words, s (w) 1 in Eq. (18) is always initialized before generating the first word in a chunk. This may affect the word-level decoder because it cannot access any previous information at the first word of each chunk.\nTo address this problem, we add new connections to Model 1 between the first state in a chunk\nand the last state in the previous chunk, as\ns (w) 1 = GRU(s (w) Jk−1 , [s̃ (c) k ; yJk−1 ; c (w) Jk−1 ]). (21)\nThe dashed blue arrows in Figure 4 illustrate the added inter-chunk connections."
    }, {
      "heading" : "3.3 Model 3: Word-to-Chunk Feedback",
      "text" : "The chunk-level decoder in Eq. (16) is only conditioned by s(w)Jk−1 , the last word state in each chunk (see the black part in Figure 4). This may affect the chunk-level decoder because it cannot memorize what kind of information has already been generated by the word-level decoder. The information about the words in a chunk should not be included in the representation of the next chunk; otherwise, it may generate the same chunks for multiple times, or forget to translate some words in the source sentence.\nTo encourage the chunk-level decoder to remove the information about the previous outputs more carefully, we add feedback states to our chunk-level decoder in Model 2. The feedback state in the chunk-level decoder is updated at every timestep j as:\ns (c) j = GRU(s (c) j−1, s (w) j ). (22)\nThe red lines in Figure 4 illustrate the added feedback states and their connection. The connections in the thick black arrows are replaced with the dotted red arrows in Model 3."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Setup",
      "text" : "Data To clarify the effectiveness of our decoders, we choose Japanese, a free word-order language, as the target language. Japanese sentences are easy to be broken into well-defined chunks (called bunsetsus (Hashimoto, 1934) in Japanese), and the accuracy of bunsetsu-chunking is over 99% (Murata et al., 2000; Yoshinaga and Kitsuregawa, 2014). The effect of chunking errors in training the decoder can be suppressed so we can evaluate the potential of our method. We use the English-Japanese training corpus in the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016), which was provided in WAT ’16. To remove inaccurate translation pairs, we extracted the first 2 million data from the 3 million translation pairs following the setting that gave the best performances in WAT ’15 (Neubig et al., 2015).\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nCorpus # words # chunks # sentences Train 44,286,317 13,707,397 1,505,871 Dev. 54,287 - 1,790 Test 54,088 - 1,812\nTable 1: Statistics of the target language (Japanese) in extracted corpus after preprocessing.\nPreprocessings For Japanese sentences, we performed tokenization using KyTea 0.4.7.1 (Neubig et al., 2011) Then we performed bunsetsuchunking with CaboCha 0.69.2 For English sentences, we performed the same preprocessings described on the WAT ’16 Website.3 To suppress having possible chunking errors affect the translation quality, we removed extremely long chunks from the training data. Specifically, among the 2 million preprocessed translation pairs, we excluded the sentence pairs that matched any of following conditions: (1) The length of the source sentence or target sentence is larger than 64; (2) The maximum length of a chunk in the target sentence is larger than 8 (around 1% of whole data); (3) The maximum number of chunks in the target sentence is larger than 20 (around 2% of whole data). The amount of the excluded sentences by the condition (2) or (3) is negligible (less than 3% of whole data). Table 1 shows the details of the extracted data.\nPostprocessing To perform unknown word replacement (Luong et al., 2015), we built a bilingual English-Japanese dictionary from all of the 3 million translation pairs. The dictionary was extracted with the MGIZA++ 0.7.04 (Och and Ney, 2003; Gao and Vogel, 2008) word alignment tool by automatically extracting the alignments between English words and Japanese words.\nEvaluation Following the WAT ’16 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) to evaluate our models. The BLEU scores were calculated with multi-bleu.pl in Moses 2.1.15 (Koehn et al., 2007); RIBES scores were calculated with RIBES.py 1.03.1.6 (Isozaki et al., 2010)\n1http://www.phontron.com/kytea/ 2http://taku910.github.io/cabocha/ 3http://lotus.kuee.kyoto-u.ac.jp/WAT/\nbaseline/dataPreparationJE.html 4https://github.com/moses-smt/mgiza 5http://www.statmt.org/moses/ 6http://www.kecl.ntt.co.jp/icl/lirg/ ribes/index.html\nTraining Details We use a single layer bidirectional GRU for the encoder and standard single layer GRUs for the word-level decoder and the chunk-level decoder. The vocabulary sizes are set to 30k for both source and target languages. The conditional probability of each target word is computed with a deep-output (Pascanu et al., 2014) layer with maxout (Goodfellow et al., 2013) units. The maximum number of output chunks is set to 20 and the maximum length of a chunk is set to 9.\nThe models are optimized using ADADELTA following (Bahdanau et al., 2015). The hyperparameters of the training procedure are fixed to the values given in Table 2. Note that the learning rate is halved when the BLEU score on the development set does not increase for 30,000 batches. All the parameters are initialized randomly with Gaussian distribution. It takes about a week to train each model with an NVIDIA TITAN X (Pascal) GPU."
    }, {
      "heading" : "4.2 Results",
      "text" : "Following (Cho et al., 2014a), we perform beam search8 with length-normalized log-probability to decode target sentences. We saved the trained models that performed best on the development set during training, and use them to test the systems with the test set. Table 3 shows the results on the test set. Note that all the models listed in Table 3, including our three models, are single models without ensemble techniques. We set the word-based sequence-to-sequence model (Li et al., 2016) as our baseline, which is a standard implementation of the attention-based NMT described in § 2. We also compare our methods with the tree-to-sequence model (Eriguchi et al., 2016b) to compare the effectiveness of capturing the structure in the source language and that in the target language. Our improved models (Model 2 and Model 3) outperform all the single models reported in WAT ’16. The best model (Model 3)\n7http://lotus.kuee.kyoto-u.ac.jp/WAT/ evaluation\n8Beam size is set to 20.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nSystem RNN |Vsrc| |Vtrg| BLEU RIBES Word-based Seq-to-Seq (Li et al., 2016) GRU 40k 30k 33.47 78.75 Word-based Tree-to-Seq (Eriguchi et al., 2016b) LSTM 88k 66k 34.87 81.58 Character-based Tree-to-Seq (Eriguchi et al., 2016a) LSTM 88k 3k 31.52 79.39 Proposed chunk-based model (Model 1) GRU 30k 30k 33.56 79.92 + Inter-chunk connection (Model 2) GRU 30k 30k 35.44 80.95 + Word-to-chunk feedback (Model 3) GRU 30k 30k 36.20 82.06\nTable 3: The results and settings of the baseline systems and our systems. |Vsrc| and |Vtrg| denote the vocabulary size of the source language and the target language, respectively. Only single NMT models (w/o ensembling) reported in WAT ’16 are listed here. Full results are available on the WAT ’16 Website.7\n!\"#$%&' ()*+, -.-&$,,/&+%$*0&/, ()&,,1&2, ,.#()\"$,,3+,,()&\"$4,,\"1,,56.++, ($.1+*(*\"1,,-\"*1(,,7$\"8,,%$\"++6*19&/ $&+*1,,(\",,6*1&.$,,-\"648&$, :,\n;&7&$&1%&' 著者の 架橋 樹脂から 線型ポリマへの ガラス転移点 に関 する 新しい 理論に つい て 述べた 。\n<\"/&6,=' 架橋樹脂から 線状ポリマへ の ガラス転移点に 関する 著者の 理論につ いて 述べた。 .#()\"$3+ ()&,()&\"$4 /&+%$*0&/7$\"8,%$\"++6*19&/ $&+*1 (\",6*1&.$,-\"648&$ .0\"#(,56.++,($.1+*(*\"1,-\"*1(\n<\"/&6,>' 架橋樹脂から 線状ポリマーへの ガラス転移点に 関する 著者の 新しい 理論につ いて 述べた。 1&2.#()\"$3+ /&+%$*0&/()&,()&\"$47$\"8,%$\"++6*19&/ $&+*1 (\",6*1&.$,-\"648&$ .0\"#(,56.++,($.1+*(*\"1,-\"*1(\n<\"/&6,?' 架橋樹脂から 線状高 分子に ガラス転移点 に関 する 新しい 著者の 著者 の 理論に つい て 述べた 。 1&2 .#()\"$3+ .#()\"$3+ ()&,()&\"$4 /&+%$*0&/7$\"8,%$\"++6*19&/ $&+*1 (\",6*1&.$,-\"648&$ .0\"#(,56.++,($.1+*(*\"1,-\"*1(\nFigure 5: Translation examples of a long sentence. Each sequence of underlined words correspond to a chunk recognized by our decoder. Model 1 outputs a chunk “著者の (author’s)” twice by mistake. Model 2 does not output a chunk “新しい (new)” by mistake. In contrast, Model 3 outputs a correct translation although there is a minor word order difference from the reference.\noutperforms the tree-to-sequence model (Eriguchi et al., 2016b) by +1.33 BLEU score and +0.48 RIBES score. The results show that capturing the chunk structure in the target language is more effective than capturing the syntax structure in the source language. Compared with the characterbased NMT model, our Model 3 outperformed the model of (Eriguchi et al., 2016a) by +4.68 BLEU score and +2.67 RIBES score. The characterbased model has a great advantage in that it does not require a large vocabulary size. Although the character-based model is less time-consuming thanks to the small target vocabulary size (|Vtrg| = 3k), our chunk-based model significantly outperformed it in terms of translation quality. One possible reason for this is that using a character-based model rather than a word-based model makes it more difficult to capture long-distance dependencies because the length of a target sequence becomes much longer in the character-based model.\nTo understand the qualitative difference between our three models, we show translation examples in Figure 5. While Model 3 outputs a cor-\nrect translation, there are some errors in the outputs of Model 1 and Model 2. Only Model 1 outputs a chunk “著者の (author’s)” twice continuously. This error indicates that the inter-chunk connections added in Model 2 play important roles in memorizing previous word states more efficiently. On the other hand, Model 2 does not output a chunk “新しい (new)” by mistake, which is probably because it does not have a good ability to memorize the previous chunks. This phenomenon supports the importance of the feedback states that are added in Model 3."
    }, {
      "heading" : "5 Related Work",
      "text" : "There has been much work done on using chunk (or phrase) structure to improve machine translation quality. The most notable work was phrasebased SMT (Koehn et al., 2003), which has been the basis for a huge amount of work on SMT for more than ten years. Apart from this, Watanabe et al. (2003) proposed a chunk-based translation model that generates output sentences in a chunkby-chunk manner. The chunk structure is effective\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nnot only for SMT but also for example-based machine translation (EBMT). Kim et al. (2010) proposed a chunk-based EBMT and showed that using chunk structures can help with finding better word alignments. Our work is different from their works in that our models are based on NMT, but not SMT or EBMT. The decoders in the above works can model the chunk structure by storing chunk pairs in a large table. On the other hand, we do that by separately training a chunk generation model and a word prediction model with two RNNs.\nWhile most of the NMT models focus on the conversion between sequential data, some works have tried to incorporate non-sequential information into NMT (Eriguchi et al., 2016b; Su et al., 2017). Eriguchi et al. (2016b) use tree-based LSTM (Tai et al., 2015) to encode input sentence into context vectors. Given a syntactic tree of a source sentence, their tree-based encoder encodes words from the leaf nodes to the root nodes recursively. Su et al. (2017) proposed a lattice-based encoder that considers multiple tokenization results while encoding the input sentence. To prevent the tokenization errors from propagating to the whole NMT system, their lattice-based encoder can utilize multiple tokenization results. These works focus on the encoding process and propose better encoders that can exploit the structures of the source language. In contrast, our work focuses on the decoding process to capture the structure of the target language. The encoders described above and our proposed decoders are complementary so they can be combined into a single network.\nConsidering that our Model 1 described in § 3.1 can be seen as a hierarchical RNN, our work is also related to previous studies that utilize multilayer RNNs to capture hierarchical structures in data. Hierarchical RNNs are used not only in the field of machine translation (Luong and Manning, 2016) but also for various NLP tasks such as document modeling (Li et al., 2015; Lin et al., 2015), dialog generation (Serban et al., 2017), image captioning (Krause et al., 2016), and video captioning (Yu et al., 2016). In particular, Li et al. (2015) and Luong and Manning (2016) use hierarchical sequence-to-sequence models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the out-\nof-vocabulary problem. In contrast, we build a chunk-word level model to explicitly capture the syntactic structure based on chunk segmentation.\nIn addition, the architecture of Model 3 is also related to stacked RNN, which has shown to be effective to improve the translation quality (Luong et al., 2015; Sutskever et al., 2014). Although these architectures look similar to each other, there is a fundamental difference between the directions of the connection between two layers. A stacked RNN consists of multiple RNN layers that are connected from the input side to the output side at every timestep. In contrast, our Model 3 has a different connection at each timestep. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feedback the information from the word-level decoder to the chunk-level decoder. By switching the connections between two layers, our model can capture the chunk structure explicitly. This is the first work that proposes the decoders for NMT that can capture plausible linguistic structures like chunk."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose chunk-based decoders for NMT. As the attention mechanism in NMT plays a similar role to the translation model in phrase-based SMT, our chunk-based decoders are intended to capture the notion of chunk in chunkbased (or phrase-based) SMT. We utilize the chunk structure to efficiently capture long-distance dependencies and cope with the problem of free word-order languages like Japanese. We design three models that have hierarchical RNN-like architectures, each of which consists of a word-level decoder and a chunk-level decoder. We performed experiments on the WAT ’16 English-to-Japanese translation task and found that our best model outperforms all the single models that were reported in WAT ’16 by +4.68 to +1.33 BLEU scores and by +3.31 to +0.48 RIBES scores.\nIn future work, we will apply our method to other target languages and evaluate the effectiveness on different languages such as Czech, German or Turkish. In addition, we plan to combine our decoder with other encoders that capture language structure, such as a Tree-LSTM (Eriguchi et al., 2016b), or an order-free encoder, such as a CNN (Kalchbrenner and Blunsom, 2013).\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Parsing by chunks",
      "author" : [ "Steven P. Abney." ],
      "venue" : "Principle-based parsing, Springer, pages 257–278.",
      "citeRegEx" : "Abney.,? 1991",
      "shortCiteRegEx" : "Abney.",
      "year" : 1991
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Third International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "On the properties of neural machine translation: Encoder–decoder approaches",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Dzmitry Bahdanau", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Eighth Workshop on Syntax, Semantics and Structure in Statis-",
      "citeRegEx" : "Cho et al\\.,? 2014a",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings of",
      "citeRegEx" : "Cho et al\\.,? 2014b",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Kyoto university participation to WAT 2016",
      "author" : [ "Fabien Cromieres", "Chenhui Chu", "Toshiaki Nakazawa", "Sadao Kurohashi." ],
      "venue" : "Proceedings of the Third Workshop on Asian Translation (WAT). pages 166– 174.",
      "citeRegEx" : "Cromieres et al\\.,? 2016",
      "shortCiteRegEx" : "Cromieres et al\\.",
      "year" : 2016
    }, {
      "title" : "Character-based decoding in treeto-sequence attention-based neural machine translation",
      "author" : [ "Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka." ],
      "venue" : "Proceedings of the Third Workshop on Asian Translation (WAT). pages 175–183.",
      "citeRegEx" : "Eriguchi et al\\.,? 2016a",
      "shortCiteRegEx" : "Eriguchi et al\\.",
      "year" : 2016
    }, {
      "title" : "Tree-to-sequence attentional neural machine translation",
      "author" : [ "Akiko Eriguchi", "Kazuma Hashimoto", "Yoshimasa Tsuruoka." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL). pages 823–833.",
      "citeRegEx" : "Eriguchi et al\\.,? 2016b",
      "shortCiteRegEx" : "Eriguchi et al\\.",
      "year" : 2016
    }, {
      "title" : "Parallel implementations of word alignment tool",
      "author" : [ "Qin Gao", "Stephan Vogel." ],
      "venue" : "Software Engineering, Testing, and Quality Assurance for Natural Language Processing. pages 49–57.",
      "citeRegEx" : "Gao and Vogel.,? 2008",
      "shortCiteRegEx" : "Gao and Vogel.",
      "year" : 2008
    }, {
      "title" : "Maxout networks",
      "author" : [ "Ian J. Goodfellow", "David Warde-Farley", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning (ICML). pages 1319–1327.",
      "citeRegEx" : "Goodfellow et al\\.,? 2013",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2013
    }, {
      "title" : "Kokugoho Yosetsu",
      "author" : [ "Shinkichi Hashimoto." ],
      "venue" : "Meiji Shoin.",
      "citeRegEx" : "Hashimoto.,? 1934",
      "shortCiteRegEx" : "Hashimoto.",
      "year" : 1934
    }, {
      "title" : "Automatic evaluation of translation quality for distant language pairs",
      "author" : [ "Hideki Isozaki", "Tsutomu Hirao", "Kevin Duh", "Katsuhito Sudoh", "Hajime Tsukada." ],
      "venue" : "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Isozaki et al\\.,? 2010",
      "shortCiteRegEx" : "Isozaki et al\\.",
      "year" : 2010
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 1700– 1709.",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Chunk-based EBMT",
      "author" : [ "Jae Dong Kim", "Ralf D. Brown", "Jaime G. Carbonell." ],
      "venue" : "Proceedings of the 14th workshop of the European Association for Machine Translation (EAMT).",
      "citeRegEx" : "Kim et al\\.,? 2010",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2010
    }, {
      "title" : "Statistical phrase-based translation",
      "author" : [ "Philipp Koehn", "Franz J. Och", "Daniel Marcu." ],
      "venue" : "Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL).",
      "citeRegEx" : "Koehn et al\\.,? 2003",
      "shortCiteRegEx" : "Koehn et al\\.",
      "year" : 2003
    }, {
      "title" : "A hierarchical approach for generating descriptive image paragraphs",
      "author" : [ "Jonathan Krause", "Justin Johnson", "Ranjay Krishna", "Li Fei-Fei." ],
      "venue" : "arXiv:1611.06607 [cs.CV].",
      "citeRegEx" : "Krause et al\\.,? 2016",
      "shortCiteRegEx" : "Krause et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical neural autoencoder for paragraphs and documents",
      "author" : [ "Jiwei Li", "Minh-Thang Luong", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference",
      "citeRegEx" : "Li et al\\.,? 2015",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2015
    }, {
      "title" : "System description of bjtu nlp neural machine translation system",
      "author" : [ "Shaotong Li", "JinAn Xu", "Yufeng Chen", "Yujie Zhang." ],
      "venue" : "Proceedings of the Third Workshop on Asian Translation (WAT). pages 104–110.",
      "citeRegEx" : "Li et al\\.,? 2016",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Hierarchical recurrent neural network for document modeling",
      "author" : [ "Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 899–907.",
      "citeRegEx" : "Lin et al\\.,? 2015",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2015
    }, {
      "title" : "Achieving open vocabulary neural machine translation with hybrid word-character models",
      "author" : [ "Minh-Thang Luong", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL). pages",
      "citeRegEx" : "Luong and Manning.,? 2016",
      "shortCiteRegEx" : "Luong and Manning.",
      "year" : 2016
    }, {
      "title" : "Addressing the rare word problem in neural machine translation",
      "author" : [ "Minh-Thang Luong", "Ilya Sutskever", "Quoc Le", "Oriol Vinyals", "Wojciech Zaremba." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Bunsetsu identification using category-exclusive rules",
      "author" : [ "Masaki Murata", "Kiyotaka Uchimoto", "Qing Ma", "Hitoshi Isahara." ],
      "venue" : "Proceedings of the 18th International Conference on Computational Linguistics (COLING). pages 565–571.",
      "citeRegEx" : "Murata et al\\.,? 2000",
      "shortCiteRegEx" : "Murata et al\\.",
      "year" : 2000
    }, {
      "title" : "ASPEC: Asian scientific paper excerpt corpus",
      "author" : [ "Toshiaki Nakazawa", "Manabu Yaguchi", "Kiyotaka Uchimoto", "Masao Utiyama", "Eiichiro Sumita", "Sadao Kurohashi", "Hitoshi Isahara." ],
      "venue" : "Proceedings of the Ninth International Conference on Lan-",
      "citeRegEx" : "Nakazawa et al\\.,? 2016",
      "shortCiteRegEx" : "Nakazawa et al\\.",
      "year" : 2016
    }, {
      "title" : "Lexicons and minimum risk training for neural machine translation: NAISTCMU at WAT2016",
      "author" : [ "Graham Neubig." ],
      "venue" : "Proceedings of the Third Workshop on Asian Translation (WAT). pages 119– 125.",
      "citeRegEx" : "Neubig.,? 2016",
      "shortCiteRegEx" : "Neubig.",
      "year" : 2016
    }, {
      "title" : "Neural reranking improves subjective quality of machine translation: NAIST at WAT2015",
      "author" : [ "Graham Neubig", "Makoto Morishita", "Satoshi Nakamura." ],
      "venue" : "Proceedings of the Second Workshop on Asian Translation (WAT). pages 35–41.",
      "citeRegEx" : "Neubig et al\\.,? 2015",
      "shortCiteRegEx" : "Neubig et al\\.",
      "year" : 2015
    }, {
      "title" : "Pointwise prediction for robust, adaptable Japanese morphological analysis",
      "author" : [ "Graham Neubig", "Yosuke Nakata", "Shinsuke Mori." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technolo-",
      "citeRegEx" : "Neubig et al\\.,? 2011",
      "shortCiteRegEx" : "Neubig et al\\.",
      "year" : 2011
    }, {
      "title" : "A systematic comparison of various statistical alignment models",
      "author" : [ "Franz J. Och", "Hermann Ney." ],
      "venue" : "Computational Linguistics 29(1):19–51.",
      "citeRegEx" : "Och and Ney.,? 2003",
      "shortCiteRegEx" : "Och and Ney.",
      "year" : 2003
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "Proceedings of 40th Annual Meeting of the Association for Computational Linguistics (ACL). pages 311–318.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "How to construct deep recurrent neural networks",
      "author" : [ "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of the Second International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Pascanu et al\\.,? 2014",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2014
    }, {
      "title" : "Edinburgh neural machine translation systems for WMT 16",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "Proceedings of the First Conference on Machine Translation (WMT). pages 371– 376.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "A hierarchical latent variable encoder-decoder model for generating dialogues",
      "author" : [ "Iulian V. Serban", "Alessandro Sordoni", "Ryan Lowe", "Laurent Charlin", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 31st AAAI Conference on Artifi-",
      "citeRegEx" : "Serban et al\\.,? 2017",
      "shortCiteRegEx" : "Serban et al\\.",
      "year" : 2017
    }, {
      "title" : "Lattice-based recurrent neural network encoders for neural machine translation",
      "author" : [ "Jinsong Su", "Zhixing Tan", "Deyi Xiong", "Rongrong Ji", "Xiaodong Shi", "Yang Liu." ],
      "venue" : "Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI).",
      "citeRegEx" : "Su et al\\.,? 2017",
      "shortCiteRegEx" : "Su et al\\.",
      "year" : 2017
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "Advances in neural information processing systems (NIPS). pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks",
      "author" : [ "Kai Sheng Tai", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics",
      "citeRegEx" : "Tai et al\\.,? 2015",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "Chunk-based statistical translation",
      "author" : [ "Taro Watanabe", "Eiichiro Sumita", "Hiroshi G. Okuno." ],
      "venue" : "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL). pages 303–310.",
      "citeRegEx" : "Watanabe et al\\.,? 2003",
      "shortCiteRegEx" : "Watanabe et al\\.",
      "year" : 2003
    }, {
      "title" : "A self-adaptive classifier for efficient text-stream processing",
      "author" : [ "Naoki Yoshinaga", "Masaru Kitsuregawa." ],
      "venue" : "Proceedings of the 25th International Conference on Computational Linguistics (COLING). pages 1091–1102.",
      "citeRegEx" : "Yoshinaga and Kitsuregawa.,? 2014",
      "shortCiteRegEx" : "Yoshinaga and Kitsuregawa.",
      "year" : 2014
    }, {
      "title" : "Video paragraph captioning using hierarchical recurrent neural networks",
      "author" : [ "Haonan Yu", "Jiang Wang", "Zhiheng Huang", "Yi Yang", "Wei Xu." ],
      "venue" : "Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pages 4584–",
      "citeRegEx" : "Yu et al\\.,? 2016",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2016
    }, {
      "title" : "ADADELTA: An adaptive learning rate method",
      "author" : [ "Matthew D. Zeiler." ],
      "venue" : "arXiv:1212.5701 [cs.LG].",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Neural machine translation (NMT) performs an end-to-end translation based on a simple encoderdecoder model (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), and now has overwhelmed the classical, complex statistical machine translation (SMT) (Sennrich et al.",
      "startOffset" : 107,
      "endOffset" : 182
    }, {
      "referenceID" : 31,
      "context" : "Neural machine translation (NMT) performs an end-to-end translation based on a simple encoderdecoder model (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), and now has overwhelmed the classical, complex statistical machine translation (SMT) (Sennrich et al.",
      "startOffset" : 107,
      "endOffset" : 182
    }, {
      "referenceID" : 3,
      "context" : "Neural machine translation (NMT) performs an end-to-end translation based on a simple encoderdecoder model (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), and now has overwhelmed the classical, complex statistical machine translation (SMT) (Sennrich et al.",
      "startOffset" : 107,
      "endOffset" : 182
    }, {
      "referenceID" : 28,
      "context" : ", 2014b), and now has overwhelmed the classical, complex statistical machine translation (SMT) (Sennrich et al., 2016; Luong and Manning, 2016; Cromieres et al., 2016; Neubig, 2016).",
      "startOffset" : 95,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : ", 2014b), and now has overwhelmed the classical, complex statistical machine translation (SMT) (Sennrich et al., 2016; Luong and Manning, 2016; Cromieres et al., 2016; Neubig, 2016).",
      "startOffset" : 95,
      "endOffset" : 181
    }, {
      "referenceID" : 4,
      "context" : ", 2014b), and now has overwhelmed the classical, complex statistical machine translation (SMT) (Sennrich et al., 2016; Luong and Manning, 2016; Cromieres et al., 2016; Neubig, 2016).",
      "startOffset" : 95,
      "endOffset" : 181
    }, {
      "referenceID" : 22,
      "context" : ", 2014b), and now has overwhelmed the classical, complex statistical machine translation (SMT) (Sennrich et al., 2016; Luong and Manning, 2016; Cromieres et al., 2016; Neubig, 2016).",
      "startOffset" : 95,
      "endOffset" : 181
    }, {
      "referenceID" : 11,
      "context" : "This simple framework allows researchers to incorporate the structure of the source sentence as in SMT by leveraging various architectures as the encoder (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b; Eriguchi et al., 2016b).",
      "startOffset" : 154,
      "endOffset" : 253
    }, {
      "referenceID" : 31,
      "context" : "This simple framework allows researchers to incorporate the structure of the source sentence as in SMT by leveraging various architectures as the encoder (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b; Eriguchi et al., 2016b).",
      "startOffset" : 154,
      "endOffset" : 253
    }, {
      "referenceID" : 3,
      "context" : "This simple framework allows researchers to incorporate the structure of the source sentence as in SMT by leveraging various architectures as the encoder (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b; Eriguchi et al., 2016b).",
      "startOffset" : 154,
      "endOffset" : 253
    }, {
      "referenceID" : 6,
      "context" : "This simple framework allows researchers to incorporate the structure of the source sentence as in SMT by leveraging various architectures as the encoder (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b; Eriguchi et al., 2016b).",
      "startOffset" : 154,
      "endOffset" : 253
    }, {
      "referenceID" : 1,
      "context" : "First, it is difficult to model long-distance dependencies (Bahdanau et al., 2015).",
      "startOffset" : 59,
      "endOffset" : 82
    }, {
      "referenceID" : 33,
      "context" : "Looking back to the past, chunks (or phrases) are utilized to handle the above aforementioned problems in statistical machine translation (SMT) (Watanabe et al., 2003; Koehn et al., 2003) and in example-based machine translation (EBMT) (Kim et al.",
      "startOffset" : 144,
      "endOffset" : 187
    }, {
      "referenceID" : 13,
      "context" : "Looking back to the past, chunks (or phrases) are utilized to handle the above aforementioned problems in statistical machine translation (SMT) (Watanabe et al., 2003; Koehn et al., 2003) and in example-based machine translation (EBMT) (Kim et al.",
      "startOffset" : 144,
      "endOffset" : 187
    }, {
      "referenceID" : 12,
      "context" : ", 2003) and in example-based machine translation (EBMT) (Kim et al., 2010).",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 6,
      "context" : "The experimental results show that our best model outperforms the best single NMT model reported in WAT ’16 (Eriguchi et al., 2016b).",
      "startOffset" : 108,
      "endOffset" : 132
    }, {
      "referenceID" : 11,
      "context" : "The implementation of the encoder can be a convolutional neural network (CNN) (Kalchbrenner and Blunsom, 2013), a long short-term memory (LSTM) (Sutskever et al.",
      "startOffset" : 78,
      "endOffset" : 110
    }, {
      "referenceID" : 31,
      "context" : "The implementation of the encoder can be a convolutional neural network (CNN) (Kalchbrenner and Blunsom, 2013), a long short-term memory (LSTM) (Sutskever et al., 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al.",
      "startOffset" : 144,
      "endOffset" : 193
    }, {
      "referenceID" : 18,
      "context" : "The implementation of the encoder can be a convolutional neural network (CNN) (Kalchbrenner and Blunsom, 2013), a long short-term memory (LSTM) (Sutskever et al., 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al.",
      "startOffset" : 144,
      "endOffset" : 193
    }, {
      "referenceID" : 3,
      "context" : ", 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al., 2014b; Bahdanau et al., 2015), or a Tree-LSTM (Eriguchi et al.",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : ", 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al., 2014b; Bahdanau et al., 2015), or a Tree-LSTM (Eriguchi et al.",
      "startOffset" : 63,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : ", 2015), or a Tree-LSTM (Eriguchi et al., 2016b).",
      "startOffset" : 24,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "Following (Bahdanau et al., 2015), we use GRU as the recurrent unit in this paper.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 36,
      "context" : "(2015), we use a mini-batch stochastic gradient descent (SGD) algorithm with ADADELTA (Zeiler, 2012) to train the above two GRUs (i.",
      "startOffset" : 86,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : "Following Bahdanau et al. (2015), we use a mini-batch stochastic gradient descent (SGD) algorithm with ADADELTA (Zeiler, 2012) to train the above two GRUs (i.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "2 Attention Mechanism for Neural Machine Translation To use all the hidden states of the encoder and improve the translation performance of long sentences, Bahdanau et al. (2015) proposed using an attention mechanism.",
      "startOffset" : 156,
      "endOffset" : 179
    }, {
      "referenceID" : 13,
      "context" : "The attention mechanism is expected to learn alignments between source and target words, and plays a similar role to the translation model in phrase-based SMT (Koehn et al., 2003).",
      "startOffset" : 159,
      "endOffset" : 179
    }, {
      "referenceID" : 33,
      "context" : "Taking non-sequential information such as chunks (or phrases) structure into consideration is proved to be helpful for SMT (Watanabe et al., 2003; Koehn et al., 2003) and EBMT (Kim et al.",
      "startOffset" : 123,
      "endOffset" : 166
    }, {
      "referenceID" : 13,
      "context" : "Taking non-sequential information such as chunks (or phrases) structure into consideration is proved to be helpful for SMT (Watanabe et al., 2003; Koehn et al., 2003) and EBMT (Kim et al.",
      "startOffset" : 123,
      "endOffset" : 166
    }, {
      "referenceID" : 12,
      "context" : ", 2003) and EBMT (Kim et al., 2010).",
      "startOffset" : 17,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "We here focus on two important properties of chunks (Abney, 1991): (1) The word order in a chunk is almost always fixed; (2) A chunk consists of a few (typically one) content words surrounded by zero or more function words.",
      "startOffset" : 52,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "No matter which language we target, a chunk usually consists of some content words and function words, and the word order in the chunk is almost always fixed (Abney, 1991).",
      "startOffset" : 158,
      "endOffset" : 171
    }, {
      "referenceID" : 3,
      "context" : "1 Sequential Encoder We adopt a standard single-layer bidirectional GRU (Cho et al., 2014b; Bahdanau et al., 2015) as our encoder (see the right part in Figure 4).",
      "startOffset" : 72,
      "endOffset" : 114
    }, {
      "referenceID" : 1,
      "context" : "1 Sequential Encoder We adopt a standard single-layer bidirectional GRU (Cho et al., 2014b; Bahdanau et al., 2015) as our encoder (see the right part in Figure 4).",
      "startOffset" : 72,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "Japanese sentences are easy to be broken into well-defined chunks (called bunsetsus (Hashimoto, 1934) in Japanese), and the accuracy of bunsetsu-chunking is over 99% (Murata et al.",
      "startOffset" : 84,
      "endOffset" : 101
    }, {
      "referenceID" : 20,
      "context" : "Japanese sentences are easy to be broken into well-defined chunks (called bunsetsus (Hashimoto, 1934) in Japanese), and the accuracy of bunsetsu-chunking is over 99% (Murata et al., 2000; Yoshinaga and Kitsuregawa, 2014).",
      "startOffset" : 166,
      "endOffset" : 220
    }, {
      "referenceID" : 34,
      "context" : "Japanese sentences are easy to be broken into well-defined chunks (called bunsetsus (Hashimoto, 1934) in Japanese), and the accuracy of bunsetsu-chunking is over 99% (Murata et al., 2000; Yoshinaga and Kitsuregawa, 2014).",
      "startOffset" : 166,
      "endOffset" : 220
    }, {
      "referenceID" : 21,
      "context" : "We use the English-Japanese training corpus in the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016), which was provided in WAT ’16.",
      "startOffset" : 97,
      "endOffset" : 120
    }, {
      "referenceID" : 23,
      "context" : "To remove inaccurate translation pairs, we extracted the first 2 million data from the 3 million translation pairs following the setting that gave the best performances in WAT ’15 (Neubig et al., 2015).",
      "startOffset" : 180,
      "endOffset" : 201
    }, {
      "referenceID" : 24,
      "context" : "1 (Neubig et al., 2011) Then we performed bunsetsuchunking with CaboCha 0.",
      "startOffset" : 2,
      "endOffset" : 23
    }, {
      "referenceID" : 19,
      "context" : "Postprocessing To perform unknown word replacement (Luong et al., 2015), we built a bilingual English-Japanese dictionary from all of the 3 million translation pairs.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "04 (Och and Ney, 2003; Gao and Vogel, 2008) word alignment tool by automatically extracting the alignments between English words and Japanese words.",
      "startOffset" : 3,
      "endOffset" : 43
    }, {
      "referenceID" : 7,
      "context" : "04 (Och and Ney, 2003; Gao and Vogel, 2008) word alignment tool by automatically extracting the alignments between English words and Japanese words.",
      "startOffset" : 3,
      "endOffset" : 43
    }, {
      "referenceID" : 26,
      "context" : "Evaluation Following the WAT ’16 evaluation procedure, we used BLEU (Papineni et al., 2002) and RIBES (Isozaki et al.",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 10,
      "context" : ", 2002) and RIBES (Isozaki et al., 2010) to evaluate our models.",
      "startOffset" : 18,
      "endOffset" : 40
    }, {
      "referenceID" : 10,
      "context" : "6 (Isozaki et al., 2010)",
      "startOffset" : 2,
      "endOffset" : 24
    }, {
      "referenceID" : 27,
      "context" : "The conditional probability of each target word is computed with a deep-output (Pascanu et al., 2014) layer with maxout (Goodfellow et al.",
      "startOffset" : 79,
      "endOffset" : 101
    }, {
      "referenceID" : 8,
      "context" : ", 2014) layer with maxout (Goodfellow et al., 2013) units.",
      "startOffset" : 26,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "The models are optimized using ADADELTA following (Bahdanau et al., 2015).",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 2,
      "context" : "2 Results Following (Cho et al., 2014a), we perform beam search8 with length-normalized log-probability to decode target sentences.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 16,
      "context" : "We set the word-based sequence-to-sequence model (Li et al., 2016) as our baseline, which is a standard implementation of the attention-based NMT described in § 2.",
      "startOffset" : 49,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "We also compare our methods with the tree-to-sequence model (Eriguchi et al., 2016b) to compare the effectiveness of capturing the structure in the source language and that in the target language.",
      "startOffset" : 60,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "System RNN |Vsrc| |Vtrg| BLEU RIBES Word-based Seq-to-Seq (Li et al., 2016) GRU 40k 30k 33.",
      "startOffset" : 58,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "75 Word-based Tree-to-Seq (Eriguchi et al., 2016b) LSTM 88k 66k 34.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 5,
      "context" : "58 Character-based Tree-to-Seq (Eriguchi et al., 2016a) LSTM 88k 3k 31.",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "outperforms the tree-to-sequence model (Eriguchi et al., 2016b) by +1.",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 5,
      "context" : "Compared with the characterbased NMT model, our Model 3 outperformed the model of (Eriguchi et al., 2016a) by +4.",
      "startOffset" : 82,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "The most notable work was phrasebased SMT (Koehn et al., 2003), which has been the basis for a huge amount of work on SMT for more than ten years.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "The most notable work was phrasebased SMT (Koehn et al., 2003), which has been the basis for a huge amount of work on SMT for more than ten years. Apart from this, Watanabe et al. (2003) proposed a chunk-based translation model that generates output sentences in a chunkby-chunk manner.",
      "startOffset" : 43,
      "endOffset" : 187
    }, {
      "referenceID" : 12,
      "context" : "Kim et al. (2010) proposed a chunk-based EBMT and showed that using chunk structures can help with finding better word alignments.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 6,
      "context" : "While most of the NMT models focus on the conversion between sequential data, some works have tried to incorporate non-sequential information into NMT (Eriguchi et al., 2016b; Su et al., 2017).",
      "startOffset" : 151,
      "endOffset" : 192
    }, {
      "referenceID" : 30,
      "context" : "While most of the NMT models focus on the conversion between sequential data, some works have tried to incorporate non-sequential information into NMT (Eriguchi et al., 2016b; Su et al., 2017).",
      "startOffset" : 151,
      "endOffset" : 192
    }, {
      "referenceID" : 32,
      "context" : "(2016b) use tree-based LSTM (Tai et al., 2015) to encode input sentence into context vectors.",
      "startOffset" : 28,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "While most of the NMT models focus on the conversion between sequential data, some works have tried to incorporate non-sequential information into NMT (Eriguchi et al., 2016b; Su et al., 2017). Eriguchi et al. (2016b) use tree-based LSTM (Tai et al.",
      "startOffset" : 152,
      "endOffset" : 218
    }, {
      "referenceID" : 5,
      "context" : "While most of the NMT models focus on the conversion between sequential data, some works have tried to incorporate non-sequential information into NMT (Eriguchi et al., 2016b; Su et al., 2017). Eriguchi et al. (2016b) use tree-based LSTM (Tai et al., 2015) to encode input sentence into context vectors. Given a syntactic tree of a source sentence, their tree-based encoder encodes words from the leaf nodes to the root nodes recursively. Su et al. (2017) proposed a lattice-based encoder that considers multiple tokenization results while encoding the input sentence.",
      "startOffset" : 152,
      "endOffset" : 456
    }, {
      "referenceID" : 18,
      "context" : "Hierarchical RNNs are used not only in the field of machine translation (Luong and Manning, 2016) but also for various NLP tasks such as document modeling (Li et al.",
      "startOffset" : 72,
      "endOffset" : 97
    }, {
      "referenceID" : 15,
      "context" : "Hierarchical RNNs are used not only in the field of machine translation (Luong and Manning, 2016) but also for various NLP tasks such as document modeling (Li et al., 2015; Lin et al., 2015), dialog generation (Serban et al.",
      "startOffset" : 155,
      "endOffset" : 190
    }, {
      "referenceID" : 17,
      "context" : "Hierarchical RNNs are used not only in the field of machine translation (Luong and Manning, 2016) but also for various NLP tasks such as document modeling (Li et al., 2015; Lin et al., 2015), dialog generation (Serban et al.",
      "startOffset" : 155,
      "endOffset" : 190
    }, {
      "referenceID" : 29,
      "context" : ", 2015), dialog generation (Serban et al., 2017), image captioning (Krause et al.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 14,
      "context" : ", 2017), image captioning (Krause et al., 2016), and video captioning (Yu et al.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 35,
      "context" : ", 2016), and video captioning (Yu et al., 2016).",
      "startOffset" : 30,
      "endOffset" : 47
    }, {
      "referenceID" : 19,
      "context" : "In addition, the architecture of Model 3 is also related to stacked RNN, which has shown to be effective to improve the translation quality (Luong et al., 2015; Sutskever et al., 2014).",
      "startOffset" : 140,
      "endOffset" : 184
    }, {
      "referenceID" : 31,
      "context" : "In addition, the architecture of Model 3 is also related to stacked RNN, which has shown to be effective to improve the translation quality (Luong et al., 2015; Sutskever et al., 2014).",
      "startOffset" : 140,
      "endOffset" : 184
    }, {
      "referenceID" : 14,
      "context" : ", 2017), image captioning (Krause et al., 2016), and video captioning (Yu et al., 2016). In particular, Li et al. (2015) and Luong and Manning (2016) use hierarchical sequence-to-sequence models, but not for the purpose of learning syntactic structures of target sentences.",
      "startOffset" : 27,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : ", 2017), image captioning (Krause et al., 2016), and video captioning (Yu et al., 2016). In particular, Li et al. (2015) and Luong and Manning (2016) use hierarchical sequence-to-sequence models, but not for the purpose of learning syntactic structures of target sentences.",
      "startOffset" : 27,
      "endOffset" : 150
    }, {
      "referenceID" : 14,
      "context" : ", 2017), image captioning (Krause et al., 2016), and video captioning (Yu et al., 2016). In particular, Li et al. (2015) and Luong and Manning (2016) use hierarchical sequence-to-sequence models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations.",
      "startOffset" : 27,
      "endOffset" : 291
    }, {
      "referenceID" : 14,
      "context" : ", 2017), image captioning (Krause et al., 2016), and video captioning (Yu et al., 2016). In particular, Li et al. (2015) and Luong and Manning (2016) use hierarchical sequence-to-sequence models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the outof-vocabulary problem.",
      "startOffset" : 27,
      "endOffset" : 412
    }, {
      "referenceID" : 6,
      "context" : "In addition, we plan to combine our decoder with other encoders that capture language structure, such as a Tree-LSTM (Eriguchi et al., 2016b), or an order-free encoder, such as a CNN (Kalchbrenner and Blunsom, 2013).",
      "startOffset" : 117,
      "endOffset" : 141
    }, {
      "referenceID" : 11,
      "context" : ", 2016b), or an order-free encoder, such as a CNN (Kalchbrenner and Blunsom, 2013).",
      "startOffset" : 50,
      "endOffset" : 82
    } ],
    "year" : 2017,
    "abstractText" : "Chunks (or phrases) had once played a pivotal role in machine translation. By using a chunk rather than a word as the basic translation unit, local (intrachunk) and global (inter-chunk) word orders/dependencies can be easily modeled. The chunk structure, despite its importance, has not been considered in the decoders of neural machine translation (NMT). In this paper, we propose chunkbased decoders for NMT, each of which consists of a chunk-level decoder and a word-level decoder. The chunk-level decoder models global dependencies, while the word-level decoder decides the word orders in a chunk. To generate a target sentence, the chunk-level decoder generates a chunk representation containing global information, based on which, the wordlevel decoder predicts the words inside the chunk. Experimental results show that our method can significantly improve translation performance in a WAT ’16 Englishto-Japanese translation task.",
    "creator" : "TeX"
  }
}