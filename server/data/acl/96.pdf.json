{
  "name" : "96.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sarcasm SIGN: Interpreting Sarcasm with Sentiment Based Monolingual Machine Translation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Sarcasm is a sophisticated form of communication in which speakers convey their message in an indirect way. It is defined in the MerriamWebster dictionary (Merriam-Webster, 1983) as the use of words that mean the opposite of what one would really want to say in order to insult someone, to show irritation, or to be funny. Considering this definition, it is not surprising to find frequent use of sarcastic language in opinionated\n1Our dataset and code will be made publicly available.\nuser generated content, in environments such as Twitter, Facebook, Reddit and many more.\nIn textual communication, knowledge about the speaker’s intent is necessary in order to fully understand and interpret sarcasm. Consider, for example, the sentence ”what a wonderful day”. A literal analysis of this sentence demonstrates a positive experience, due to the use of the word wonderful. However, if we knew that the sentence was meant sarcastically, wonderful would turn into a word of a strong negative sentiment. In spoken language, sarcastic utterances are often accompanied by a certain tone of voice which points out the intent of the speaker, whereas in textual communication, sarcasm is inherently ambiguous, and its identification and interpretation may be challenging even for humans.\nIn this paper we present the novel task of interpretation of sarcastic utterances. We define the purpose of the interpretation task as the capability to generate a non-sarcastic utterance that captures the meaning behind the original sarcastic text.\nOur work currently targets the Twitter domain since it is a medium in which sarcasm is prevalent, and it allows us to focus on the interpretation of tweets marked with the content tag #sarcasm. And so, for example, given the tweet ”how I love Mondays. #sarcasm” we would like our system to generate interpretations such as ”how I hate Mondays” or ”I really hate Mondays”. In order to learn such interpretations, we constructed a parallel corpus of 3000 sarcastic tweets, each of which has five non-sarcastic interpretations (Section 3).\nOur task is complex since sarcasm can be expressed in many forms, it is ambiguous in nature and its understanding may require world knowledge. Following are several examples taken from our corpus:\n1. loving life so much right now. #sarcasm 2. Way to go California! #sarcasm\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n3. Great, a choice between two excellent candidates, Donald Trump or Hillary Clinton. #sarcasm\nIn example (1) it is quite straightforward to see the exaggerated positive sentiment used in order to convey strong negative feelings. Examples (2) and (3), however, do not contain any excessive sentiment. Instead, previous knowledge is required if one wishes to fully understand and interpret what went wrong with California, or who Hillary Clinton and Donald Trump are.\nSince sarcasm is a refined and indirect form of speech, its interpretation may be challenging for certain populations. For example, studies show that children with deafness, autism or Asperger’s Syndrome struggle with non literal communication such as sarcastic language (Peterson et al., 2012; Kimhi, 2014). Moreover, since sarcasm transforms the polarity of an apparently positive or negative expression into its opposite, it poses a challenge for automatic systems for opinion mining, sentiment analysis and extractive summarization (Popescu et al., 2005; Pang and Lee, 2008; Wiebe et al., 2004). Extracting the honest meaning behind the sarcasm may alleviate such issues.\nIn order to design an automatic sarcasm interpretation system, we first rely on previous work in established similar tasks (section 2), particularly machine translation (MT), borrowing algorithms as well as evaluation measures. In section 4 we discuss the automatic evaluation measures we apply in our work and present human based measures for: (a) the fluency of a generated nonsarcastic utterance, (b) its adequacy as interpretation of the original sarcastic tweet’s meaning, and (c) whether or not it captures the sentiment of the original tweet. Then, in section 5, we explore the performance of prominent phrase-based and neural MT systems on our task in development data experiments. We next present the Sarcasm SIGN (Sarcasm Sentimental Interpretation GeNerator, section 6), our novel MT based algorithm which puts a special emphasis on sentiment words. Lastly, in Section 7 we assess the performance of the various algorithms and show that while they perform similarly in terms of automatic MT evaluation, SIGN is superior according to the human measures. We conclude with a discussion on future research directions for our task, regarding both algorithms and evaluation."
    }, {
      "heading" : "2 Related Work",
      "text" : "The use of irony and sarcasm has been well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al., 2005; Peterson et al., 2012) literature. In computational work, the interest in sarcasm has dramatically increased over the past few years. This is probably due to factors such as the rapid growth in user generated content on the web, in which sarcasm is used excessively (Maynard et al., 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the challenge that sarcasm poses for opinion mining and sentiment analysis systems (Pang and Lee, 2008; Maynard and Greenwood, 2014). Despite this rising interest, and despite many works that deal with sarcasm identification (Tsur et al., 2010; Davidov et al., 2010; González-Ibánez et al., 2011; Riloff et al., 2013; Barbieri et al., 2014), to the best of our knowledge, generation of sarcasm interpretations has not been previously attempted.\nTherefore, the following sections are dedicated to previous work from neighboring NLP fields which are relevant to our work: sarcasm detection, MT, paraphrasing and text summarization.\nSarcasm Detection Recent computational work on sarcasm revolves mainly around detection. Due to the large volume of detection work, we survey only several representative examples.\nTsur et al. (2010) and Davidov et al. (2010) presented a semi-supervised approach for detecting irony and sarcasm in product-reviews and tweets, where features are based on ironic speech patterns extracted from a labeled dataset. González-Ibánez et al. (2011) used lexical and pragmatic features, e.g. emojis and whether the utterance is a comment to another person, in order to train a classifier that distinguishes sarcastic utterances from tweets of positive and negative sentiment.\nRiloff et al. (2013) observed that a certain type of sarcasm is characterized by a contrast between a positive sentiment and a negative situation. Consequently, they described a bootstrapping algorithm that learns distinctive phrases connected to negative situations along with a positive sentiment and used these phrases to train their classifier. Barbieri et al. (2014) avoided using word patterns and instead employed features such as the length and sentiment of the tweet, and the use of rare words.\nDespite the differences between detection and\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\ninterpretation, this line of work is highly relevant to ours in terms of feature design. Moreover, it presents fundamental notions, such as the sentiment polarity of the sarcastic utterance and of its interpretation, that we adopt. Finally, when utterances are not marked for sarcasm as in the Twitter domain, or when these labels are not reliable, detection is a necessary step before interpretation.\nMachine Translation We approach our task as one of monolingual MT, where we translate sarcastic English into non-sarcastic English. Therefore, our starting point is the application of MT techniques and evaluation measures. The three major approaches to MT are phrase based (Koehn et al., 2007), syntax based (Koehn et al., 2003) and the recent neural approach. For automatic MT evaluation, often an n-gram co-occurrence based scoring is performed in order to measure the lexical closeness between a candidate and a reference translations. Example measures are NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2011), and the widely used BLEU (Papineni et al., 2002), which represents precision: the fraction of n-grams from the machine generated translation that also appear in the human reference.\nHere we employ the phrase based Moses system (Koehn et al., 2007) and an RNN-encoder-decoder architecture, based on Cho et al. (2014). Later we will show that these algorithms can be further improved and will explore the quality of the MT evaluation measures in the context of our task.\nParaphrasing and Summarization Tasks such as paraphrasing and summarization are often addressed as monolingual MT, and so they are close in nature to our task. Quirk et al. (2004) proposed a model of paraphrasing based on monolingual MT, and utilized alignment models used in the Moses translation system (Koehn et al., 2007; Wubben et al., 2010; Bannard and Callison-Burch, 2005). Xu et al. (2015) presented the task of paraphrase generation while targeting a particular writing style, specifically paraphrasing modern English into Shakespearean English, and approached it with phrase based MT.\nWork on paraphrasing and summarization is often evaluated using MT evaluation measures such as BLEU. As BLEU is precision-oriented, complementary recall-oriented measures are often used as well. A prominent example is ROUGE (Lin, 2004), a family of measures used mostly for\nevaluation in automatic summarization: candidate summaries are scored according to the fraction of n-grams from the human references they contain.\nWe also utilize PINC (Chen and Dolan, 2011), a measure which rewards paraphrases for being different from their source, by introducing new n-grams. PINC is often combined with BLEU due to their complementary nature: while PINC rewards n-gram novelty, BLEU rewards similarity to the reference. The highest correlation with human judgments is achieved by the product of PINC with a sigmoid function of BLEU (Chen and Dolan, 2011)."
    }, {
      "heading" : "3 A Parallel Sarcastic Tweets Corpus",
      "text" : "To properly investigate our task, we collected a dataset, first of its kind, of sarcastic tweets and their non-sarcastic (honest) interpretations. This data, as well as the instructions provided for our human judges, will be made publicly available and will hopefully provide a basis for future work regarding sarcasm on Twitter. Despite the focus of the current work on the Twitter domain, we consider our task as a more general one, and hope that our discussion, observations and algorithms will be beneficial for other domains as well.\nUsing the Twitter API2, we collected tweets marked with the content tag #sarcasm, posted between Januray and June of 2016. Following Tsur et al. (2010), González-Ibánez et al. (2011) and Bamman and Smith (2015), we address the problem of noisy tweets with automatic filtering: we remove all tweets not written in English, discard retweets (tweets that have been forwarded or shared) and remove tweets containing URLs or images, so that the sarcasm in the tweet regards to the text only and not to an image or a link. This results in 3000 sarcastic tweets containing text only.\nIn order to obtain honest interpretations for our sarcastic tweets, we used Fiverr3 – a platform for selling and purchasing services from independent suppliers (also referred to as workers). We employed ten Fiverr workers, half of them from the field of comedy writing, and half from the field of literature paraphrasing. The chosen workers were made sure to have an active Twitter account, in order to ensure their acquaintance with social networks and with Twitter’s colorful language (hashtags, common acronyms such as LOL, etc.).\n2http://apiwiki.twitter.com 3https://www.fiverr.com\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nSarcastic Tweets Honest Interpretations\nWhat a great way to end my night. #sarcasm 1. Such a bad ending to my night 2. Oh what a great way to ruin my night 3. What a horrible way to end a night 4. Not a good way to end the night 5. Well that wasn’t the night I was hoping for\nStaying up till 2:30 was a brilliant idea, very productive #sarcasm 1. Bad idea staying up late, not very productive 2. It was not smart or productive for me to stay up so late 3. Staying up till 2:30 was not a brilliant idea, very non-productive 4. I need to go to bed on time 5. Staying up till 2:30 was completely useless\nTable 1: Examples from our parallel sarcastic tweet corpus.\nWe then randomly divided our tweet corpus to two batches of size 1500 each, and randomly assigned five workers to each batch. We instructed the workers to translate each sarcastic tweet into a non sarcastic utterance, while maintaining the original meaning. We encouraged the workers to use external knowledge sources (such as Google) if they came across a subject they were not familiar with, or if the sarcasm was unclear to them.\nTable 1 presents two examples from our corpus. The table demonstrates the tendency of the workers to generally agree on the core meaning of the sarcastic tweets. Yet, since sarcasm is inherently vague, it is not surprising that the interpretations differ from one worker to another. For example, some workers change only one or two words from the original sarcastic tweet, while others rephrase the entire utterance. We regard this as beneficial, since it brings a natural, human variance into the task. This variance makes the evaluation of automatic sarcasm interpretation algorithms challenging, as we further discuss in the next section."
    }, {
      "heading" : "4 Evaluation Measures",
      "text" : "As mentioned above, in certain cases world knowledge is mandatory in order to correctly evaluate sarcasm interpretations. For example, in the case of the second sarcastic tweet in table 1, we need to know that 2:30 is considered a late hour so that staying up till 2:30 and staying up late would be considered equivalent despite the lexical difference. Furthermore, we notice that transforming a sarcastic utterance into a non sarcastic one often requires to change a small number of words. For example, a single word change in the sarcastic tweet ”How I love Mondays. #sarcasm” leads to the non-sarcastic utterance How I hate Mondays.\nThis is not typical for MT, where usually the entire source sentence is translated to a new sentence\nin the target language and we would expect lexical similarity between the machine generated translation and the human reference it is compared to. This raises a doubt as to whether n-gram based MT evaluation measures such as the aforementioned are suitable for our task. We hence asses the quality of an interpretation using automatic evaluation measures from the tasks of MT, paraphrasing, and summarization (Section 2), and compare these measures to human-based measures. Automatic Measures We use BLEU and ROUGE as measures of n-gram precision and recall, respectively. We report scores of ROUGE-1, ROUGE-2 and ROUGE-L (recall based on unigrams, bigrams and longest common subsequence between candidate and reference, respectively). In order to asses the n-gram novelty of interpretations (i.e, difference from the source), we report PINC and PINC∗sigmoid(BLEU) (see Section 2). Human judgments We employed an additional group of five Fiverr workers and asked them to score each generated interpretations with two scores on a 1-7 scale, 7 being the best. The scores are: adequacy: the degree to which the interpretation captures the meaning of the original tweet; and fluency: how readable the interpretation is. In addition, reasoning that a high quality interpretation is one that captures the true intent of the sarcastic utterance by using words suitable to its sentiment, we ask the workers to assign the interpretation with a binary score indicating whether the sentiment presented in the interpretation agrees with the sentiment of the original sarcastic tweet.4\nThe human measures enjoy high agreement levels between the human judges. The averaged root mean squared error calculated on the test set\n4For example, we consider ”Best day ever #sarcasm” and its interpretation ”Worst day ever” to agree on the sentiment, despite the use of opposite sentiment words.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nSarcastic Tweet Moses Interpretation Neural Interpretation Boy , am I glad the rain’s here #sarcasm Boy, I’m so annoyed that the rain is here I’m not glad to go today Another night of work, Oh, the joy #sarcasm Another night of work, Ugh, unbearable Another night, I don’t like it Being stuck in an airport is fun #sarcasm Be stuck in an airport is not fun Yay, stuck at the office again You’re the best. #sarcasm You’re the best You’re my best friend\nTable 2: Sarcasm interpretations generated by Moses and by the RNN.\nEvaluation Measure Moses RNN Precision Oriented BLEU 62.91 41.05 Novelty Oriented PINC 51.81 76.45 PINC∗sigmoid(BLEU) 33.79 45.96\nRecall Oriented\nROUGE-1 66.44 42.20 ROUGE-2 41.03 29.97 ROUGE-l 65.31 40.87\nHuman Judgments Fluency 6.46 5.12 Adequacy 2.54 2.08 % correct sentiment 28.84 17.93\nTable 3: Development data results for MT models.\nacross all pairs of judges and across the various algorithms we experiment with are: 1.44 for fluency and 1.15 for adequacy. For sentiment scores the averaged agreement at the same setup is 93.2%."
    }, {
      "heading" : "5 Sarcasm Interpretations as MT",
      "text" : "As our task is about the generation of one English sentence given another, a natural starting point is treating it as monolingual MT. We hence begin with utilizing two widely used MT systems, representing two different approaches: Phrase Based MT vs. Neural MT. We then analyze the performance of these two systems, and based on our conclusions we design our SIGN model.\nPhrase Based MT We employ Moses5, using word alignments extracted by GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diagfinal strategy. We use phrases of up to 8 words to build our phrase table, and do not filter sentences according to length since tweets contain at most 140 characters. We employ the KenLM algorithm (Heafield, 2011) for language modeling, and train it on the non-sarcastic tweet interpretations (the target side of the parallel corpus).\nNeural Machine Translation We use GroundHog, a publicly available implementation of an RNN encoder-decoder, with LSTM hidden states.6 Our encoder and decoder contain 250 hidden units each. We use the minibatch stochastic gradient\n5http://www.statmt.org/moses 6https://github.com/lisa-groundhog/GroundHog\ndescent (SGD) algorithm together with Adadelta (Zeiler, 2012) to train each model, where each SGD update is computed using a minibatch of 16 utterances. Following Sutskever et al. (2014), we use beam search for test time decoding. Henceforth we refer to this system as RNN. Performance Analysis We divide our corpus into training, development and test sets of sizes 2400, 300 and 300 respectively. We train Moses and the RNN on the training set and tune their parameters on the development set. Table 3 presents development data results, as these are preliminary experiments that aim to asses the compatibility of MT algorithms to our task.\nMoses scores much higher in terms of BLEU and ROUGE, meaning that compared to the RNN its interpretations capture more n-grams appearing in the human references while maintaining high precision. The RNN outscores Moses in terms of PINC and PINC∗sigmoid(BLEU), meaning that its interpretations are more novel, in terms of ngrams. This alone might not be a negative trait; However, according to human judgments Moses performs better in terms of fluency, adequacy and sentiment, and so the novelty of the RNN’s interpretations does not necessarily contribute to their quality, and even possibly reduces it.\nTable 2 illustrates several examples of the interpretations generated by both Moses and the RNN. While the interpretations generated by the RNN are readable, they generally do not maintain the meaning of the original tweet. We believe that this is the result of the neural network overfitting the training set, despite regularization and dropout layers, probably due to the relatively small training set size. In light of these results when we experiment with the SIGN algorithm (Section 7), we employ Moses as its MT component.\nThe final example of Table 2 is representative of cases where both Moses and the RNN fail to capture the sarcastic sense of the tweet, incorrectly interpreting it or leaving it unchanged. In order to deal with such cases, we wish to utilize a property typical of sarcastic language. Sarcasm is mostly\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n“How I love Mondays # sarcasm\n“How I cluster-i Mondays # sarcasm MOSES\nlove like ...\ncluster-i\n“How I hate Mondays\n“How I cluster-j Mondays # sarcasm\nhate despise ...\ncluster-j de-clusteringclustering\nPositive Clusters merit, wonder, props, praise, congratulations..\npatience, dignity, truth, chivalry, rationality...\nNegative Clusters hideous, horrible, nasty, obnoxious, scary, pathetic... shame, sadness, sorrow, fear, disappointment, regret, danger...\nTable 4: Examples of two positive and two negative clusters created by the SIGN algorithm.\nused to convey a certain emotion by using strong sentiment words that express the exact opposite of their literal meaning. Hence, many sarcastic utterances can be correctly interpreted by keeping most of their words, replacing only sentiment words with expressions of the opposite sentiment. For example, the sarcasm in the utterance ”You’re the best. #sarcasm” is hidden in best, a word of a strong positive sentiment. If we transform this word into a word of the opposite sentiment, such as worst, then we get a non-sarcastic utterance with the correct sentiment.\nWe next present the Sarcasm SIGN (Sarcasm Sentimental Interpretation GeNerator), an algorithm which capitalizes on sentiment words in order to produce accurate interpretations."
    }, {
      "heading" : "6 The Sarcasm SIGN Algorithm",
      "text" : "SIGN (Figure 1) targets sentiment words in sarcastic utterances. First, it clusters sentiment words according to semantic relatedness. Then, each sentiment word is replaced with its cluster 7 and the transformed data is fed into an MT system (Moses in this work), at both its training and test phases. Consequently, at test time the MT system outputs non-sarcastic utterances with clusters replacing sentiment words. Finally, SIGN performs a declustering process on these MT outputs, replacing sentiment clusters with suitable words.\nIn order to detect the sentiment of words, we 7This means that we replace a word with cluster-j where j\nis the number of the cluster to which the word belongs.\nturn to SentiWordNet (Esuli and Sebastiani, 2006), a lexical resource based on WordNet (Miller et al., 1990; Kilgarriff and Fellbaum, 2000). Using SentiWordNet’s positivity and negativity scores, we collect from our training data a set of distinctly positive words (∼ 70) and a set of distinctly negative words (∼ 160).8 We then utilize the pre-trained dependency-based word embeddings of Levy and Goldberg (2014)9 and cluster each set using the k-means algorithm with L2 distance. We aim to have ten words on average in each cluster, and so the positive set is clustered into 7 clusters, and the negative set into 16 clusters. Table 4 presents examples from our clusters.\nUpon receiving a sarcastic tweet, at both training and test, SIGN searches it for sentiment words according to the positive and negative sets. If such a word is found, it is replaced with its cluster. For example, given the sentence ”How I love Mondays. #sarcasm”, love will be recognized as a positive sentiment word, and the sarcastic tweet will become: ”How I cluster-i Mondays. #sarcasm” where i is the cluster number of the word love.\nDuring training, this process is also applied to the non-sarcastic references. And so, if one such reference is ”I dislike Mondays.”, then dislike will be identified and the reference will become ”I cluster-j Mondays.”, where j is the cluster number of the word dislike. Moses is then trained on these new representations of the corpus, using the exact same setup as before. This training process produces a mapping between positive and negative clusters, and outputs sarcastic interpretations with clustered sentiment words (e.g, ”I cluster-j Mondays.”). At test time, after Moses generates an utterance containing clusters, a de-clustering pro-\n8The scores are in the [0,1] range. We set the threshold of 0.6 for both distinctly positive and distinctly negative words.\n9https://levyomer.wordpress.com/2014/ 04/25/dependency-based-word-embeddings/. We choose these embeddings since they are believed to better capture the relations between a word and its context, having been trained on dependency-parsed sentences.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nEvaluation Measure Moses SIGN-centroid SIGN-context SIGN-oracle Precision Oriented BLEU 65.24 63.52 66.96 67.49 Novelty Oriented PINC 45.92 47.11 46.65 46.10 PINC∗sigmoid(BLEU) 30.21 30.79 31.13 30.54 Recall Oriented ROUGE-1 70.26 68.43 69.67 70.34 ROUGE-2 42.18 40.34 40.96 42.81 ROUGE-l 69.82 68.24 69.98 70.01\nTable 5: Test data results with automatic evaluation measures.\ncess takes place: the clusters are replaced with the appropriate sentiment words.\nWe experiment with several de-clustering approaches: (1) SIGN-centroid: the chosen sentiment word will be the one closest to the centroid of cluster j. For example in the tweet ”I cluster-j Mondays.”, the sentiment word closest to the centroid of cluster j will be chosen; (2) SIGNcontext: the cluster is replaced with its word that has the highest average Pointwise Mutual Information (PMI) with the words in a symmetric context window of size 3 around the cluster’s location in the output. For example, for ”I cluster-j Mondays.”, the sentiment word from cluster j which has the highest average PMI with the words in {’I’,’Mondays’} will be chosen. The PMI values are computed on the training data; and (3) SIGNOracle: an upper bound where a person manually chooses the most suitable word from the cluster.\nWe expect this process to improve the quality of sarcasm interpretations in two aspects. First, as mentioned earlier, sarcastic tweets often differ from their non sarcastic interpretations in as little as one sentiment word. SIGN should help highlight the sentiment words most in need of interpretation. Second, under the pre-processing SIGN performs to the input examples of Moses, the latter is inclined to learn a mapping from positive to negative clusters, and vice versa. This is likely to encourage the Moses output to generate outputs of the same sentiment as the original sarcastic tweet, but with honest sentiment words. For example, if the sarcastic tweet expresses a negative sentiment with strong positive words, the non-sarcastic interpretation will express this negative sentiment with negative words, thus stripping away the sarcasm."
    }, {
      "heading" : "7 Experiments and Results",
      "text" : "We experiment with SIGN and the Moses and RNN baselines at the same setup of section 5. We report test set results for automatic and human measures, in Tables 5 and 6 respectively. As in\nFluency Adequacy % correct sentiment % changed\nMoses 6.67 2.55 25.7 42.3 SIGN-Centroid 6.38 3.23* 42.2* 67.4 SIGN-Context 6.66 3.61* 46.2* 68.5 SIGN-Oracle 6.69 3.67* 46.8* 68.8\nTable 6: Test set results with human measures. %changed provides the fraction of tweets that were changed during interpretation (i.e. the tweet and its interpretation are not identical). In cases where one of our models presents significant improvement over Moses, the results are decorated with a star. Statistical significance is tested with the paired t-test for fluency and adequacy, and with the McNemar paired test for labeling disagreements (Gillick and Cox, 1989) for % correct sentiment, in both cases with p < 0.05.\nthe development data experiments (Table 3), the RNN presents critically low adequacy scores of 2.11 across the entire test set and of 1.89 in cases where the interpretation and the tweet differ. This, along with its low fluency scores (5.74 and 5.43 respectively) and its very low BLEU and ROUGE scores make us deem this model immature for our task and dataset, hence we exclude it from this section’s tables and do not discuss it further.\nIn terms of automatic evaluation (Table 5), SIGN and Moses do not perform significantly different. When it comes to human evaluation (Table 6) however, SIGN-context presents substantial gains. While for fluency Moses and SIGN-context perform similarly, SIGN-context performs much better in terms of adequacy and the percentage of tweets with the correct sentiment. The differences are substantial as well as statistically significant: adequacy of 3.61 for SIGN-context compared to 2.55 of Moses, and correct sentiment for 46.2% of the SIGN interpretations, compared to only 25.7% of the Moses interpretations.\nTable 6 further provides an initial explanation to the improvement of SIGN over Moses: Moses tends to keep interpretations identical to the origi-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nnal sarcastic tweet, altering them in only 42.3% of the cases, 10 while SIGN-context’s interpretations differ from the original sarcastic tweet in 68.5% of the cases, which comes closer to the 73.8% in the gold standard human interpretations. If for each of the algorithms we only regard to interpretations that differ from the original sarcastic tweet, the differences between the models are less substantial. Nonetheless, SIGN-context still presents improvement by correctly changing sentiment in 67.5% of the cases compared to 60.8% for Moses.\nBoth tables consistently show that the contextbased selection strategy of SIGN outperforms the centroid alternative. This makes sense as, being context-ignorant, SIGN-centroid might produce non-fluent or inadequate interpretations for a given context. For example, the tweet ”Also gotta move a piano as well. joy #sarcasm” is changed to ”Also gotta move a piano as well. bummer” by SIGN-context, while SIGN-centroid changes it to the less appropriate ”Also gotta move a piano as well. boring”. Nonetheless, even this naive de-clustering approach substantially improves adequacy and sentiment accuracy over Moses.\nFinally, comparison to SIGN-oracle reveals that the context selection strategy is not far from human performance with respect to both automatic and human evaluation measures. Still, some gain can be achieved, especially for the human measures on tweets that were changed at interpretation. This indicates that SIGN can improve mostly through a better clustering of sentiment words, rather than through a better selection strategy."
    }, {
      "heading" : "8 Discussion and Future Work",
      "text" : "The performance gap between Moses and SIGN may stem from the difference in their optimization criteria. Moses aims to optimize the BLEU score and given the overall lexical similarity between the original tweets and their interpretations, it therefore tends to keep them identical. SIGN, in contrast, targets sentiment words and changes them frequently. Consequently, we do not observe substantial differences between the algorithms in the automatic measures that are mostly based on n-gram differences between the source and the interpretation. Likewise, the human fluency measure that accounts for the readability of the interpretation is not seriously affected by the translation process. When it comes to the human adequacy and\n10We elaborate on this in section 8.\nsentiment measures, which account for the understanding of the tweet’s meaning, SIGN reveals its power and demonstrates much better performance compared to Moses.\nTo further understand the relationship between the automatic and the human based measures we computed the Pearson correlations for each pair of (automatic, human) measures. We observe that all correlation values are low (up to 0.12 for fluency, 0.13-0.18 for sentiment and 0.19-0.24 for adequacy). Moreover, for fluency the correlation values are insignificant (using a correlation significance t-test with p = 0.05). We believe this indicates that these automatic measures do not provide appropriate evaluation for our task. Designing automatic measures is hence left for future research.\nA qualitative analysis reveals that as expected, SIGN-context performs well on sarcastic tweets with clear sentiment words, transforming expressions such as ”Audits are a blast to do #sarcasm” and ”Being stuck in an airport is fun #sarcasm” into ”Audits are a bummer to do” and ”Being stuck in an airport is boring”, respectively. Even when there are several sentiment words and not all of them require a change, e.g. in ”Constantly being irritated, anxious and depressed is a great feeling! #sarcasm”, SIGN-context produces the adequate interpretation: ”Constantly being irritated, anxious and depressed is a terrible feeling”.\nIn some cases, however, SIGN struggles with producing correct interpretations. For example, the tweet ”Can you imagine if Lebron had help? #sarcasm”, was left unchanged by all SIGN models (including the upper bound oracle). Notice that the sarcasm in this tweet is not expressed by specific sentiment words. Moreover, world knowledge (who Lebron is, what kind of help he requires) is crucial in order to comprehend and interpret this sarcastic utterance. In addition, phrases that express sentiment without explicit sentiment words, for example ”can’t wait” in the tweet ”Can’t wait until tomorrow #sarcasm”, diminish SIGN’s advantage. Further Improving SIGN so that it properly interprets such tweets is a major future direction."
    } ],
    "references" : [ {
      "title" : "Contextualized sarcasm detection on twitter",
      "author" : [ "David Bamman", "Noah A Smith." ],
      "venue" : "Ninth International AAAI Conference",
      "citeRegEx" : "Bamman and Smith.,? 2015",
      "shortCiteRegEx" : "Bamman and Smith.",
      "year" : 2015
    }, {
      "title" : "Paraphrasing with bilingual parallel corpora",
      "author" : [ "Colin Bannard", "Chris Callison-Burch." ],
      "venue" : "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, pages 597–604.",
      "citeRegEx" : "Bannard and Callison.Burch.,? 2005",
      "shortCiteRegEx" : "Bannard and Callison.Burch.",
      "year" : 2005
    }, {
      "title" : "Modelling sarcasm in twitter, a novel approach",
      "author" : [ "Francesco Barbieri", "Horacio Saggion", "Francesco Ronzano." ],
      "venue" : "ACL 2014 page 50. www.aclweb.org/anthology/W14-2609.",
      "citeRegEx" : "Barbieri et al\\.,? 2014",
      "shortCiteRegEx" : "Barbieri et al\\.",
      "year" : 2014
    }, {
      "title" : "Collecting highly parallel data for paraphrase evaluation",
      "author" : [ "David L Chen", "William B Dolan." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Associa-",
      "citeRegEx" : "Chen and Dolan.,? 2011",
      "shortCiteRegEx" : "Chen and Dolan.",
      "year" : 2011
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "arXiv preprint",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Semi-supervised recognition of sarcastic sentences in twitter and amazon",
      "author" : [ "Dmitry Davidov", "Oren Tsur", "Ari Rappoport." ],
      "venue" : "Proceedings of the fourteenth conference on computational natural language learning. Association",
      "citeRegEx" : "Davidov et al\\.,? 2010",
      "shortCiteRegEx" : "Davidov et al\\.",
      "year" : 2010
    }, {
      "title" : "Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems",
      "author" : [ "Michael Denkowski", "Alon Lavie" ],
      "venue" : "In Proceedings of the Sixth Workshop on Statistical Machine Translation",
      "citeRegEx" : "Denkowski and Lavie.,? \\Q2011\\E",
      "shortCiteRegEx" : "Denkowski and Lavie.",
      "year" : 2011
    }, {
      "title" : "Automatic evaluation of machine translation quality using ngram co-occurrence statistics",
      "author" : [ "George Doddington." ],
      "venue" : "Proceedings of the second international conference on Human Language Technology Research. Mor-",
      "citeRegEx" : "Doddington.,? 2002",
      "shortCiteRegEx" : "Doddington.",
      "year" : 2002
    }, {
      "title" : "Sentiwordnet: A publicly available lexical resource for opinion mining",
      "author" : [ "Andrea Esuli", "Fabrizio Sebastiani." ],
      "venue" : "Proceedings of LREC. Citeseer, volume 6, pages 417–422. https://www.aclweb.org/anthology/P/P14/P14-",
      "citeRegEx" : "Esuli and Sebastiani.,? 2006",
      "shortCiteRegEx" : "Esuli and Sebastiani.",
      "year" : 2006
    }, {
      "title" : "Irony in language and thought: A cognitive science reader",
      "author" : [ "Raymond W Gibbs", "Herbert L Colston." ],
      "venue" : "Psychology Press.",
      "citeRegEx" : "Gibbs and Colston.,? 2007",
      "shortCiteRegEx" : "Gibbs and Colston.",
      "year" : 2007
    }, {
      "title" : "Identifying sarcasm",
      "author" : [ "Nina Wacholder" ],
      "venue" : null,
      "citeRegEx" : "Wacholder.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wacholder.",
      "year" : 2011
    }, {
      "title" : "Moses: Open source",
      "author" : [ "Richard Zens" ],
      "venue" : null,
      "citeRegEx" : "Zens,? \\Q2007\\E",
      "shortCiteRegEx" : "Zens",
      "year" : 2007
    }, {
      "title" : "Rouge: A package",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "308",
      "shortCiteRegEx" : "308",
      "year" : 2004
    }, {
      "title" : "Challenges in developing opinion mining tools for social media",
      "author" : [ "Diana Maynard", "Kalina Bontcheva", "Dominic Rout." ],
      "venue" : "Proceedings of the@ NLP can u tag# usergeneratedcontent pages 15–22.",
      "citeRegEx" : "Maynard et al\\.,? 2012",
      "shortCiteRegEx" : "Maynard et al\\.",
      "year" : 2012
    }, {
      "title" : "Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis",
      "author" : [ "Diana Maynard", "Mark A Greenwood." ],
      "venue" : "LREC. pages 4238–4243. http://dblp.unitrier.de/rec/bib/conf/lrec/MaynardG14.",
      "citeRegEx" : "Maynard and Greenwood.,? 2014",
      "shortCiteRegEx" : "Maynard and Greenwood.",
      "year" : 2014
    }, {
      "title" : "Webster’s ninth new collegiate dictionary",
      "author" : [ "Inc Merriam-Webster." ],
      "venue" : "Merriam-Webster. https://doi.org/10.1353/dic.1984.0017.",
      "citeRegEx" : "Merriam.Webster.,? 1983",
      "shortCiteRegEx" : "Merriam.Webster.",
      "year" : 1983
    }, {
      "title" : "Introduction to wordnet: An on-line lexical database",
      "author" : [ "George A Miller", "Richard Beckwith", "Christiane Fellbaum", "Derek Gross", "Katherine J Miller." ],
      "venue" : "International journal of lexicography 3(4):235–244. https://doi.org/10.1093/ijl/3.4.235.",
      "citeRegEx" : "Miller et al\\.,? 1990",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 1990
    }, {
      "title" : "Irony and the ironic",
      "author" : [ "Douglas Colin Muecke" ],
      "venue" : null,
      "citeRegEx" : "Muecke.,? \\Q1982\\E",
      "shortCiteRegEx" : "Muecke.",
      "year" : 1982
    }, {
      "title" : "A systematic comparison of various statistical alignment models",
      "author" : [ "Franz Josef Och", "Hermann Ney." ],
      "venue" : "Computational linguistics 29(1):19–51.",
      "citeRegEx" : "Och and Ney.,? 2003",
      "shortCiteRegEx" : "Och and Ney.",
      "year" : 2003
    }, {
      "title" : "Opinion mining and sentiment analysis",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Foundations and trends in information retrieval 2(1-2):1–135. http://dblp.unitrier.de/rec/bib/journals/ftir/PangL07.",
      "citeRegEx" : "Pang and Lee.,? 2008",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2008
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu." ],
      "venue" : "Proceedings of the 40th annual meeting on association for computational linguistics. Associa-",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "The mind behind the message: Advancing theory-of-mind scales for typically developing children, and those with deafness, autism, or asperger syndrome",
      "author" : [ "Candida C Peterson", "Henry M Wellman", "Virginia Slaughter." ],
      "venue" : "Child development",
      "citeRegEx" : "Peterson et al\\.,? 2012",
      "shortCiteRegEx" : "Peterson et al\\.",
      "year" : 2012
    }, {
      "title" : "Opine: Extracting product features and opinions from reviews",
      "author" : [ "Ana-Maria Popescu", "Bao Nguyen", "Oren Etzioni." ],
      "venue" : "Proceedings of HLT/EMNLP on interactive demonstrations. Association for Computational Linguistics, pages 32–33.",
      "citeRegEx" : "Popescu et al\\.,? 2005",
      "shortCiteRegEx" : "Popescu et al\\.",
      "year" : 2005
    }, {
      "title" : "Monolingual machine translation for paraphrase generation",
      "author" : [ "Chris Quirk", "Chris Brockett", "William B Dolan" ],
      "venue" : null,
      "citeRegEx" : "Quirk et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Quirk et al\\.",
      "year" : 2004
    }, {
      "title" : "Sarcasm as contrast between a positive sentiment and negative situation",
      "author" : [ "Ellen Riloff", "Ashequl Qadir", "Prafulla Surve", "Lalindra De Silva", "Nathan Gilbert", "Ruihong Huang." ],
      "venue" : "EMNLP. volume 13, pages 704–714.",
      "citeRegEx" : "Riloff et al\\.,? 2013",
      "shortCiteRegEx" : "Riloff et al\\.",
      "year" : 2013
    }, {
      "title" : "The neuroanatomical basis of understanding sarcasm and its relationship to social cognition",
      "author" : [ "SG Shamay-Tsoory", "Rachel Tomer", "Judith Aharon-Peretz." ],
      "venue" : "Neuropsychology 19(3):288. https://doi.org/10.1037/0894-4105.19.3.288.",
      "citeRegEx" : "Shamay.Tsoory et al\\.,? 2005",
      "shortCiteRegEx" : "Shamay.Tsoory et al\\.",
      "year" : 2005
    }, {
      "title" : "The meaning of irony",
      "author" : [ "FJ Stingfellow." ],
      "venue" : "New York: State University of NY .",
      "citeRegEx" : "Stingfellow.,? 1994",
      "shortCiteRegEx" : "Stingfellow.",
      "year" : 1994
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Advances in neural information processing systems. pages 3104–3112. http://papers.nips.cc/paper/5346-sequence-to-",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Icwsm-a great catchy name: Semisupervised recognition of sarcastic sentences in online product reviews",
      "author" : [ "Oren Tsur", "Dmitry Davidov", "Ari Rappoport." ],
      "venue" : "ICWSM. http://dblp.unitrier.de/rec/bib/conf/icwsm/TsurDR10.",
      "citeRegEx" : "Tsur et al\\.,? 2010",
      "shortCiteRegEx" : "Tsur et al\\.",
      "year" : 2010
    }, {
      "title" : "or# sarcasma quantitative and qualitative study based on twitter https://aclweb.org/anthology/Y/Y13/Y13-1035.pdf",
      "author" : [ "Po-Ya Angela Wang" ],
      "venue" : null,
      "citeRegEx" : "Wang.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang.",
      "year" : 2013
    }, {
      "title" : "Learning subjective language",
      "author" : [ "Janyce Wiebe", "Theresa Wilson", "Rebecca Bruce", "Matthew Bell", "Melanie Martin." ],
      "venue" : "Computational linguistics 30(3):277–308. http://dblp.unitrier.de/rec/bib/journals/coling/WiebeWBBM04.",
      "citeRegEx" : "Wiebe et al\\.,? 2004",
      "shortCiteRegEx" : "Wiebe et al\\.",
      "year" : 2004
    }, {
      "title" : "Paraphrase generation as monolingual translation: Data and evaluation",
      "author" : [ "Sander Wubben", "Antal Van Den Bosch", "Emiel Krahmer." ],
      "venue" : "Proceedings of the 6th International Natural Language Generation Conference. Association for Computa-",
      "citeRegEx" : "Wubben et al\\.,? 2010",
      "shortCiteRegEx" : "Wubben et al\\.",
      "year" : 2010
    }, {
      "title" : "Semeval-2015 task 1: Paraphrase and semantic similarity in twitter (pit)",
      "author" : [ "Wei Xu", "Chris Callison-Burch", "William B Dolan." ],
      "venue" : "Proceedings of SemEval https://doi.org/10.18653/v1/s15-2001.",
      "citeRegEx" : "Xu et al\\.,? 2015",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Adadelta: an adaptive learning rate method",
      "author" : [ "Matthew D Zeiler." ],
      "venue" : "arXiv preprint arXiv:1212.5701 http://dblp.uni-trier.de/rec/bib/journals/corr/abs1212-5701.",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "It is defined in the MerriamWebster dictionary (Merriam-Webster, 1983) as the use of words that mean the opposite of what one would really want to say in order to insult someone, to show irritation, or to be funny.",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 21,
      "context" : "For example, studies show that children with deafness, autism or Asperger’s Syndrome struggle with non literal communication such as sarcastic language (Peterson et al., 2012; Kimhi, 2014).",
      "startOffset" : 152,
      "endOffset" : 188
    }, {
      "referenceID" : 22,
      "context" : "Moreover, since sarcasm transforms the polarity of an apparently positive or negative expression into its opposite, it poses a challenge for automatic systems for opinion mining, sentiment analysis and extractive summarization (Popescu et al., 2005; Pang and Lee, 2008; Wiebe et al., 2004).",
      "startOffset" : 227,
      "endOffset" : 289
    }, {
      "referenceID" : 19,
      "context" : "Moreover, since sarcasm transforms the polarity of an apparently positive or negative expression into its opposite, it poses a challenge for automatic systems for opinion mining, sentiment analysis and extractive summarization (Popescu et al., 2005; Pang and Lee, 2008; Wiebe et al., 2004).",
      "startOffset" : 227,
      "endOffset" : 289
    }, {
      "referenceID" : 30,
      "context" : "Moreover, since sarcasm transforms the polarity of an apparently positive or negative expression into its opposite, it poses a challenge for automatic systems for opinion mining, sentiment analysis and extractive summarization (Popescu et al., 2005; Pang and Lee, 2008; Wiebe et al., 2004).",
      "startOffset" : 227,
      "endOffset" : 289
    }, {
      "referenceID" : 17,
      "context" : "The use of irony and sarcasm has been well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al.",
      "startOffset" : 70,
      "endOffset" : 128
    }, {
      "referenceID" : 26,
      "context" : "The use of irony and sarcasm has been well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al.",
      "startOffset" : 70,
      "endOffset" : 128
    }, {
      "referenceID" : 9,
      "context" : "The use of irony and sarcasm has been well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al.",
      "startOffset" : 70,
      "endOffset" : 128
    }, {
      "referenceID" : 25,
      "context" : "The use of irony and sarcasm has been well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al., 2005; Peterson et al., 2012) literature.",
      "startOffset" : 148,
      "endOffset" : 199
    }, {
      "referenceID" : 21,
      "context" : "The use of irony and sarcasm has been well studied in the linguistics (Muecke, 1982; Stingfellow, 1994; Gibbs and Colston, 2007) and the psychology (Shamay-Tsoory et al., 2005; Peterson et al., 2012) literature.",
      "startOffset" : 148,
      "endOffset" : 199
    }, {
      "referenceID" : 13,
      "context" : "This is probably due to factors such as the rapid growth in user generated content on the web, in which sarcasm is used excessively (Maynard et al., 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the challenge that sarcasm poses for opinion mining and sentiment analysis systems (Pang and Lee, 2008; Maynard and Greenwood, 2014).",
      "startOffset" : 132,
      "endOffset" : 217
    }, {
      "referenceID" : 0,
      "context" : "This is probably due to factors such as the rapid growth in user generated content on the web, in which sarcasm is used excessively (Maynard et al., 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the challenge that sarcasm poses for opinion mining and sentiment analysis systems (Pang and Lee, 2008; Maynard and Greenwood, 2014).",
      "startOffset" : 132,
      "endOffset" : 217
    }, {
      "referenceID" : 29,
      "context" : "This is probably due to factors such as the rapid growth in user generated content on the web, in which sarcasm is used excessively (Maynard et al., 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the challenge that sarcasm poses for opinion mining and sentiment analysis systems (Pang and Lee, 2008; Maynard and Greenwood, 2014).",
      "startOffset" : 132,
      "endOffset" : 217
    }, {
      "referenceID" : 19,
      "context" : ", 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the challenge that sarcasm poses for opinion mining and sentiment analysis systems (Pang and Lee, 2008; Maynard and Greenwood, 2014).",
      "startOffset" : 158,
      "endOffset" : 207
    }, {
      "referenceID" : 14,
      "context" : ", 2012; Kaplan and Haenlein, 2011; Bamman and Smith, 2015; Wang, 2013) and the challenge that sarcasm poses for opinion mining and sentiment analysis systems (Pang and Lee, 2008; Maynard and Greenwood, 2014).",
      "startOffset" : 158,
      "endOffset" : 207
    }, {
      "referenceID" : 28,
      "context" : "Despite this rising interest, and despite many works that deal with sarcasm identification (Tsur et al., 2010; Davidov et al., 2010; González-Ibánez et al., 2011; Riloff et al., 2013; Barbieri et al., 2014), to the best of our knowledge, generation of sarcasm interpretations has not been previously attempted.",
      "startOffset" : 91,
      "endOffset" : 206
    }, {
      "referenceID" : 5,
      "context" : "Despite this rising interest, and despite many works that deal with sarcasm identification (Tsur et al., 2010; Davidov et al., 2010; González-Ibánez et al., 2011; Riloff et al., 2013; Barbieri et al., 2014), to the best of our knowledge, generation of sarcasm interpretations has not been previously attempted.",
      "startOffset" : 91,
      "endOffset" : 206
    }, {
      "referenceID" : 24,
      "context" : "Despite this rising interest, and despite many works that deal with sarcasm identification (Tsur et al., 2010; Davidov et al., 2010; González-Ibánez et al., 2011; Riloff et al., 2013; Barbieri et al., 2014), to the best of our knowledge, generation of sarcasm interpretations has not been previously attempted.",
      "startOffset" : 91,
      "endOffset" : 206
    }, {
      "referenceID" : 2,
      "context" : "Despite this rising interest, and despite many works that deal with sarcasm identification (Tsur et al., 2010; Davidov et al., 2010; González-Ibánez et al., 2011; Riloff et al., 2013; Barbieri et al., 2014), to the best of our knowledge, generation of sarcasm interpretations has not been previously attempted.",
      "startOffset" : 91,
      "endOffset" : 206
    }, {
      "referenceID" : 25,
      "context" : "Tsur et al. (2010) and Davidov et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 4,
      "context" : "(2010) and Davidov et al. (2010) presented a semi-supervised approach for detecting irony and sarcasm in product-reviews and tweets, where features are based on ironic speech patterns extracted from a labeled dataset.",
      "startOffset" : 11,
      "endOffset" : 33
    }, {
      "referenceID" : 4,
      "context" : "(2010) and Davidov et al. (2010) presented a semi-supervised approach for detecting irony and sarcasm in product-reviews and tweets, where features are based on ironic speech patterns extracted from a labeled dataset. González-Ibánez et al. (2011) used lexical and pragmatic features, e.",
      "startOffset" : 11,
      "endOffset" : 248
    }, {
      "referenceID" : 4,
      "context" : "(2010) and Davidov et al. (2010) presented a semi-supervised approach for detecting irony and sarcasm in product-reviews and tweets, where features are based on ironic speech patterns extracted from a labeled dataset. González-Ibánez et al. (2011) used lexical and pragmatic features, e.g. emojis and whether the utterance is a comment to another person, in order to train a classifier that distinguishes sarcastic utterances from tweets of positive and negative sentiment. Riloff et al. (2013) observed that a certain type of sarcasm is characterized by a contrast between a positive sentiment and a negative situation.",
      "startOffset" : 11,
      "endOffset" : 495
    }, {
      "referenceID" : 2,
      "context" : "Barbieri et al. (2014) avoided using word patterns and instead employed features such as the length and sentiment of the tweet, and the use of rare words.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 7,
      "context" : "Example measures are NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2011), and the widely used BLEU (Papineni et al.",
      "startOffset" : 26,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "Example measures are NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2011), and the widely used BLEU (Papineni et al.",
      "startOffset" : 53,
      "endOffset" : 80
    }, {
      "referenceID" : 20,
      "context" : "Example measures are NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2011), and the widely used BLEU (Papineni et al., 2002), which represents precision: the fraction of n-grams from the machine generated translation that also appear in the human reference.",
      "startOffset" : 107,
      "endOffset" : 130
    }, {
      "referenceID" : 4,
      "context" : ", 2007) and an RNN-encoder-decoder architecture, based on Cho et al. (2014). Later we will show that these algorithms can be further improved and will explore the quality of the MT evaluation measures in the context of our task.",
      "startOffset" : 58,
      "endOffset" : 76
    }, {
      "referenceID" : 31,
      "context" : "(2004) proposed a model of paraphrasing based on monolingual MT, and utilized alignment models used in the Moses translation system (Koehn et al., 2007; Wubben et al., 2010; Bannard and Callison-Burch, 2005).",
      "startOffset" : 132,
      "endOffset" : 207
    }, {
      "referenceID" : 1,
      "context" : "(2004) proposed a model of paraphrasing based on monolingual MT, and utilized alignment models used in the Moses translation system (Koehn et al., 2007; Wubben et al., 2010; Bannard and Callison-Burch, 2005).",
      "startOffset" : 132,
      "endOffset" : 207
    }, {
      "referenceID" : 3,
      "context" : "We also utilize PINC (Chen and Dolan, 2011), a measure which rewards paraphrases for being different from their source, by introducing new n-grams.",
      "startOffset" : 21,
      "endOffset" : 43
    }, {
      "referenceID" : 3,
      "context" : "The highest correlation with human judgments is achieved by the product of PINC with a sigmoid function of BLEU (Chen and Dolan, 2011).",
      "startOffset" : 112,
      "endOffset" : 134
    }, {
      "referenceID" : 21,
      "context" : "Quirk et al. (2004) proposed a model of paraphrasing based on monolingual MT, and utilized alignment models used in the Moses translation system (Koehn et al.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : ", 2010; Bannard and Callison-Burch, 2005). Xu et al. (2015) presented the task of paraphrase generation while targeting a particular writing style, specifically paraphrasing modern English into Shakespearean English, and approached it with phrase based MT.",
      "startOffset" : 8,
      "endOffset" : 60
    }, {
      "referenceID" : 27,
      "context" : "Following Tsur et al. (2010), González-Ibánez et al.",
      "startOffset" : 10,
      "endOffset" : 29
    }, {
      "referenceID" : 27,
      "context" : "Following Tsur et al. (2010), González-Ibánez et al. (2011) and Bamman and Smith (2015), we address the problem of noisy tweets with automatic filtering: we remove all tweets not written in English, discard retweets (tweets that have been forwarded or shared) and remove tweets containing URLs or images, so that the sarcasm in the tweet regards to the text only and not to an image or a link.",
      "startOffset" : 10,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : "(2011) and Bamman and Smith (2015), we address the problem of noisy tweets with automatic filtering: we remove all tweets not written in English, discard retweets (tweets that have been forwarded or shared) and remove tweets containing URLs or images, so that the sarcasm in the tweet regards to the text only and not to an image or a link.",
      "startOffset" : 11,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : "Phrase Based MT We employ Moses5, using word alignments extracted by GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diagfinal strategy.",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 33,
      "context" : "com/lisa-groundhog/GroundHog descent (SGD) algorithm together with Adadelta (Zeiler, 2012) to train each model, where each SGD update is computed using a minibatch of 16 utterances.",
      "startOffset" : 76,
      "endOffset" : 90
    }, {
      "referenceID" : 27,
      "context" : "Following Sutskever et al. (2014), we use beam search for test time decoding.",
      "startOffset" : 10,
      "endOffset" : 34
    }, {
      "referenceID" : 8,
      "context" : "turn to SentiWordNet (Esuli and Sebastiani, 2006), a lexical resource based on WordNet (Miller et al.",
      "startOffset" : 21,
      "endOffset" : 49
    }, {
      "referenceID" : 16,
      "context" : "turn to SentiWordNet (Esuli and Sebastiani, 2006), a lexical resource based on WordNet (Miller et al., 1990; Kilgarriff and Fellbaum, 2000).",
      "startOffset" : 87,
      "endOffset" : 139
    }, {
      "referenceID" : 8,
      "context" : "turn to SentiWordNet (Esuli and Sebastiani, 2006), a lexical resource based on WordNet (Miller et al., 1990; Kilgarriff and Fellbaum, 2000). Using SentiWordNet’s positivity and negativity scores, we collect from our training data a set of distinctly positive words (∼ 70) and a set of distinctly negative words (∼ 160).8 We then utilize the pre-trained dependency-based word embeddings of Levy and Goldberg (2014)9 and cluster each set using the k-means algorithm with L2 distance.",
      "startOffset" : 22,
      "endOffset" : 414
    } ],
    "year" : 2017,
    "abstractText" : "Sarcasm is a form of speech in which speakers say the opposite of what they truly mean in order to convey a strong sentiment. In other words, ”Sarcasm is the giant chasm between what I say, and the person who doesn’t get it.”. In this paper we present the novel task of sarcasm interpretation, defined as the generation of a non-sarcastic utterance conveying the same message as the original sarcastic one. We introduce a novel dataset of 3000 sarcastic tweets, each interpreted by five human judges. Addressing the task as monolingual machine translation (MT), we experiment with MT algorithms and evaluation measures. We then present SIGN: an MT based sarcasm interpretation algorithm that targets sentiment words, a defining element of textual sarcasm. We show that while the scores of n-gram based automatic measures are similar for all interpretation models, SIGN’s interpretations are scored higher by humans for adequacy and sentiment polarity. We conclude with a discussion on future research directions for our new task.1",
    "creator" : "LaTeX with hyperref package"
  }
}