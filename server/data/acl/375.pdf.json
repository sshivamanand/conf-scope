{
  "name" : "375.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CANE: Context-Aware Network Embedding for Relation Modeling",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Network embedding (NE), i.e., network representation learning (NRL), aims to map vertices of a network into a low-dimensional space according to their structural roles in the network. NE provides an efficient and effective way to represent and manage large-scale networks, alleviating the computation and sparsity issues of conventional symbol-based representations. Hence, NE is attracting many research interests in recent years (Perozzi et al., 2014; Tang et al., 2015; Grover and\nLeskovec, 2016), and achieves promising performance on many network analysis tasks including link prediction, vertex classification, and community detection.\nIn real-world social networks, it is intuitive that one vertex may demonstrate various aspects when interacting with different neighbor vertices. For example, a researcher usually collaborates with different partners on diverse research topics (as illustrated in Fig. 1), a social-media user contacts with various friends sharing distinct interests, and a web page links with multiple pages for different purposes. However, most existing NE methods only arrange one single embedding vector to each vertex, and give rise to the following two invertible issues: (1) These methods cannot cope with the aspect transition of a vertex flexibly when interacting with different neighbors. (2) In these models, a vertex tends to force the embeddings of its neighbors close to each other, which may be not case all the time. For example, the left user and right user in Fig. 1, share less common interests, but are learned to be close to each other since they both link to the middle person. This will accordingly\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nmake vertex embeddings indiscriminative. To address these issues, we aim to propose a Context-Aware Network Embedding (CANE) framework for modeling relationships between vertices precisely. More specifically, we present CANE on information networks, where each vertex also contains rich external information such as text, labels or other meta-data, and the significance of context is more critical for NE in this scenario. Without loss of generality, we implement CANE on text-based information networks in this paper, which can be easily extended to other types of information networks.\nIn conventional NE models, each vertex is represented as a static embedding vector, denoted as context-free embedding. On the contrary, CANE assigns dynamic embeddings to a vertex according to different neighbors it interacts with, named as context-aware embedding . Take a vertex u and its neighbor vertex v for example. The contextfree embedding of u remains unchanged when interacting with different neighbors. On the contrary, the context-aware embedding of u is dynamic when confronting different neighbors.\nWhen u interacting with v, their context embeddings with respect to each other are derived from their text information, Su and Sv respectively. For each vertex, we can easily use neural models, such as convolutional neural networks (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (Kiros et al., 2015; Tai et al., 2015), to build context-free text-based embedding. In order to realize context-aware textbased embeddings, we introduce the selective attention scheme and build mutual attention between u and v into these neural models. The mutual attention is expected to guide neural models to emphasize those words that are focused by its neighbor vertices and eventually obtain contextaware embeddings.\nBoth context-free embeddings and contextaware embeddings of each vertex can be efficiently learned together via concatenation using existing NE methods such as DeepWalk (Perozzi et al., 2014), LINE (Tang et al., 2015) and node2vec (Grover and Leskovec, 2016).\nWe conduct experiments on three real-world datasets of different areas. Experimental results on link prediction reveal the effectiveness of our framework as compared to other state-of-the-art methods. The results suggest that, context-aware\nembeddings are critical for network analysis, especially for those tasks concerning about complicated interactions between vertices such as link prediction. We also explore the performance of our framework via vertex classification and case studies, which again confirms the flexibility and superiority of our models."
    }, {
      "heading" : "2 Related Work",
      "text" : "With the rapid growth of large-scale social networks, network embedding, i.e. network representation learning has been proposed as a critical technique for network analysis tasks.\nIn recent years, there have been a large number of NE models proposed to learn efficient vertex embeddings (Tang and Liu, 2009; Cao et al., 2015; Wang et al., 2016). For example, DeepWalk (Perozzi et al., 2014) performs random walks over networks and introduces an efficient word representation learning model, Skip-Gram (Mikolov et al., 2013a), to learn network embeddings. LINE (Tang et al., 2015) optimizes the joint and conditional probabilities of edges in large-scale networks to learn vertex representations. Node2vec (Grover and Leskovec, 2016) modifies the random walk strategy in DeepWalk into biased random walks to explore the network structure more efficiently. Nevertheless, most of these NE models only encode the structural information into vertex embeddings, without considering heterogenous information accompanied with vertices in real-world social networks.\nTo address this issue, researchers make great efforts to incorporate heterogenous information into conventional NE models. For instance, Yang et al. (2015) present text-associated DeepWalk (TADW) to improve matrix factorization based DeepWalk with text information. Tu et al. (2016) propose max-margin DeepWalk (MMDW) to learn discriminative network representations by utilizing labeling information of vertices. Chen et al. (2016) propose group-enhanced network embedding (GENE) to integrate existing group information in NE. Sun et al. (2016) regard text content as a special kind of vertices, and propose contextenhanced network embedding (CENE) through leveraging both structural and textural information to learn network embeddings.\nTo the best of our knowledge, all existing NE models focus on learning context-free embeddings, but ignore the diverse roles when a vertex\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\ninteracts with others. In contrast, we assume that a vertex has different embeddings according to which vertex it interacts with, and propose CANE to learn context-aware vertex embeddings."
    }, {
      "heading" : "3 Problem Formulation",
      "text" : "We first give basic notations and definitions in this work. Suppose there is an information network G = (V,E, T ), where V is the set of vertices, E ⊆ V ×V are edges between vertices, and T denotes the text information of vertices. Each edge eu,v ∈ E represents the relationship between two vertices (u, v), with an associated weight wu,v. Here, the text information of a specific vertex v ∈ V is represented as a word sequence Sv = (w1, w2, . . . , wnv), where nv = |Sv|. NRL aims to learn a low-dimensional embedding v ∈ Rd for each vertex v ∈ V according to its network structure and associated information, e.g. text and labels. Note that, d |V | is the dimension of representation space.\nDefinition 1. Context-free Embeddings: Conventional NRL models learn context-free embedding for each vertex. It means the embedding of a vertex is fixed and won’t change with respect to its context information (i.e., another vertex it interacts with).\nDefinition 2. Context-aware Embeddings: Different from existing NRL models that learn context-free embeddings, CANE learns various embeddings for a vertex according to its different contexts. Specifically, for an edge eu,v, CANE learns context-aware embeddings v(u) and u(v)."
    }, {
      "heading" : "4 The Method",
      "text" : ""
    }, {
      "heading" : "4.1 Overall Framework",
      "text" : "To take full use of both network structure and associated text information, we propose two types of embeddings for a vertex v, i.e., structurebased embedding vs and text-based embedding vt. Structure-based embedding is able to capture the information in the network structure, while text-based embedding can capture the textual meanings lying in the associated text information. With these embeddings, we can simply concatenate them and obtain the vertex embeddings as v = vs⊕vt, where ⊕ indicates the concatenation operation. Note that, the text-based embedding vt can be either context-free or context-aware, which will be introduced detailedly in section 4.4 and 4.5\nrespectively. When vt is context-aware, the overall vertex embeddings v will be context-aware as well.\nWith above definitions, CANE aims to maximize the overall objective of edges as follows:\nL = ∑ e∈E L(e). (1)\nHere, the objective of each edge L(e) consists of two parts as follows:\nL(e) = Ls(e) + Lt(e), (2)\nwhere Ls(e) denotes the structure-based objective and Lt(e) represents the text-based objective.\nIn the following part, we give detailed introduction to the two objectives respectively."
    }, {
      "heading" : "4.2 Structure-based Objective",
      "text" : "Without loss of generality, we assume the network is directed, as an undirected edge can be considered as two directed edges with opposite directions and equal weights.\nThus, the structure-based objective aims to measure the log-likelihood of a directed edge using the structure-based embeddings as\nLs(e) = wu,v log p(v s|us). (3)\nFollowing LINE (Tang et al., 2015), we define the conditional probability of v generated by u in Eq. (3) as\np(vs|us) = exp(u s · vs)∑\nz∈V exp(u s · zs)\n. (4)"
    }, {
      "heading" : "4.3 Text-based Objective",
      "text" : "Vertices in real-world social networks usually accompany with associated text information. Therefore, we propose the text-based objective to take advantage of these text information, as well as learn text-based embeddings for vertices.\nThe text-based objective Lt(e) can be defined with various measurements. To be compatible with Ls(e), we define Lt(e) as follows:\nLt(e) = α ·Ltt(e) + β ·Lts(e) + γ ·Lst(e), (5)\nwhere α, β and γ control the weights of various parts, and\nLtt(e) = wu,v log p(v t|ut), Lts(e) = wu,v log p(v t|us), Lst(e) = wu,v log p(v s|ut).\n(6)\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nThe conditional probabilities in Eq. (6) map the two types of vertex embeddings into the same representation space, but do not enforce them to be identical for the consideration of their own characteristics. Similarly, we employ softmax function for calculating the probabilities, as in Eq. (4).\nThe structure-based embeddings are regarded as parameters, the same as in conventional NE models. But for text-based embeddings, we intend to obtain them from associated text information of vertices. Besides, the text-based embeddings can be obtained either in context-free ways or in context-aware ones. In the following sections, we will give detailed introduction respectively."
    }, {
      "heading" : "4.4 Context-Free Text Embedding",
      "text" : "There has been a variety of neural models to obtain text embeddings from a word sequence, such as convolutional neural networks (CNN) (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (RNN) (Kiros et al., 2015; Tai et al., 2015).\nIn this work, we investigate different neural networks for text modeling, including CNN, Bidirectional RNN (Schuster and Paliwal, 1997) and GRU (Cho et al., 2014), and employ the best performed CNN, which can capture the local semantic dependency among words.\nTaking the word sequence of a vertex as input, CNN obtains the text-based embedding through three layers, i.e. looking-up, convolution and pooling.\nLooking-up. Given a word sequence S = (w1, w2, . . . , wn), the looking-up layer transforms each word wi ∈ S into its corresponding word embedding wi ∈ Rd ′ and obtains embedding sequence as S = (w1,w2, . . . ,wn). Here, d′ indicates the dimension of word embeddings.\nConvolution. After looking-up, the convolution layer extracts local features of input embedding sequence S. To be specific, it performs convolution operation over a sliding window of length l using a convolution matrix C ∈ Rd×(l×d′) as follows:\nxi = C · Si:i+l−1 + b, (7)\nwhere Si:i+l−1 denotes the concatenation of word embeddings within the i-th window and b is the bias vector. Note that, we add zero padding vectors (Hu et al., 2014) at the edge of the sentence.\nMax-pooling. To obtain the text embedding vt, we operate max-pooling and non-linear transfor-\nmation over {xi0, . . . ,xin} as follows:\nri = tanh(max(x i 0, . . . ,x i n)), (8)\nAt last, we encode the text information of a vertex with CNN and obtain its text-based embedding vt = [r1, . . . , rd]\nT . As vt is irrelevant to the other vertices it interacts with, we name it as contextfree text embedding."
    }, {
      "heading" : "4.5 Context-Aware Text Embedding",
      "text" : "As stated before, we assume that a specific vertex plays different roles when interacting with others vertices. In other words, each vertex should have its own points of focus about a specific vertex, which leads to its context-aware text embeddings.\nTo achieve this, we employ mutual attention to obtain context-aware text embedding. It enables the pooling layer in CNN to be aware of the vertex pair in an edge, in a way that text information from a vertex can directly affect the text embedding of the other vertex, and vice versa.\nIn Fig. 2, we give an illustration of the generating process of context-aware text embedding. Given an edge eu,v with two corresponding text sequences Su and Sv, we can get the matrices P ∈ Rd×m and Q ∈ Rd×n through convolution layer. Here, m and n represent the lengths of Su\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nand Sv respectively. By introducing an attentive matrix A ∈ Rd×d, we compute the correlation matrix F ∈ Rm×n as follows:\nF = tanh(PTAQ). (9)\nNote that, each element Fi,j in F represents the pair-wise correlation score between two hidden vectors, i.e., Pi and Qj .\nAfter that, we conduct pooling operations along rows and columns of F to generate the importance vectors, named as row-pooling and column pooling respectively. According to our experiments, mean-pooling performs better than max-pooling. Thus, we employ mean-pooling operation as follows:\ngpi = mean(Fi,1, . . . ,Fi,n), gqi = mean(F1,i, . . . ,Fm,i). (10)\nThe importance vectors of P and Q are obtained as gp = [gp1 , . . . , g p m]T and gq = [gq1, . . . , g q n]T .\nNext, we employ softmax function to transform importance vectors gp and gq to attention vectors ap and aq. For instance, the i-th element of ap is formalized as follows:\napi = exp(gpi )∑\nj∈[1,m] exp(g p j ) . (11)\nAt last, the context-aware text embeddings of u and v are computed as\nut(v) = Pa p, vt(u) = Qa q.\n(12)\nNow, given an edge (u, v), we can obtain the context-aware embeddings of vertices with their structure embeddings and context-aware text embeddings as u(v) = us⊕ut(v) and v(u) = v s⊕vt(u)."
    }, {
      "heading" : "4.6 Optimization of CANE",
      "text" : "According to Eq. (3) and Eq. (6), CANE aims to maximize several conditional probabilities between u ∈ {us,ut(v)} and v ∈ {v\ns,vt(u)}. It is intuitive that optimizing the conditional probability using softmax function is computationally expensive. Thus, we employ negative sampling (Mikolov et al., 2013b) and transform the objective into the following form:\nlog σ(uT ·v)+ k∑\ni=1\nEz∼P (v)[log σ(−uT ·z)], (13)\nwhere k is the number of negative samples and σ represents the sigmoid function. P (v) ∝ dv3/4 denotes the distribution of vertices, where dv is the out-degree of v.\nAfterwards, we employ Adam (Kingma and Ba, 2015) to optimize the transformed objective."
    }, {
      "heading" : "5 Experiments",
      "text" : "In order to investigate the effectiveness of CANE on modeling relationships between vertices, we conduct experiments of link prediction on several real-world datasets. Besides, we also employ vertex classification to verify whether contextaware embeddings of a vertex can compose a highquality context-free embedding in return."
    }, {
      "heading" : "5.1 Datasets",
      "text" : "Datasets Cora HepTh Zhihu\n#Vertices 2, 277 1, 038 10, 000 #Edges 5, 214 1, 990 43, 894 #Labels 7 − −\nTable 1: Statistics of Datasets.\nWe select three real-world network datasets as follows:\nCora1 is a typical paper citation network constructed by (McCallum et al., 2000). After filtering out papers without text information, there are 2, 277 machine learning papers in this network, which are divided into 7 categories.\nHepTh2 (High Energy Physics Theory) is another citation network from arXiv3 released by (Leskovec et al., 2005). We filter out papers without abstract information and retain 1, 038 papers at last.\nZhihu4 is the largest online Q&A website in China. Users follow each other and answer questions in this site. We randomly crawl 10, 000 active users from Zhihu, and take the descriptions of their concerned topics as text information.\nThe detailed statistics are listed in Table 1."
    }, {
      "heading" : "5.2 Baselines",
      "text" : "We employ the following methods as baselines:\nStructure-only: DeepWalk (Perozzi et al., 2014) performs random walks over networks and employ Skip-Gram 1https://people.cs.umass.edu/∼mccallum/data.html 2https://snap.stanford.edu/data/cit-HepTh.html 3https://arxiv.org/ 4https://www.zhihu.com/\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n%Removed edges 15% 25% 35% 45% 55% 65% 75% 85% 95%\nDeepWalk 56.0 63.0 70.2 75.5 80.1 85.2 85.3 87.8 90.3 LINE 55.0 58.6 66.4 73.0 77.6 82.8 85.6 88.4 89.3 node2vec 55.9 62.4 66.1 75.0 78.7 81.6 85.9 87.3 88.2\nNaive Combination 72.7 82.0 84.9 87.0 88.7 91.9 92.4 93.9 94.0 TADW 86.6 88.2 90.2 90.8 90.0 93.0 91.0 93.4 92.7 CENE 72.1 86.5 84.6 88.1 89.4 89.2 93.9 95.0 95.9\nCANE (text only) 78.0 80.5 83.9 86.3 89.3 91.4 91.8 91.4 93.3 CANE (w/o attention) 85.8 90.5 91.6 93.2 93.9 94.6 95.4 95.1 95.5\nCANE 86.8 91.5 92.2 93.9 94.6 94.9 95.6 96.6 97.7\nTable 2: AUC values on Cora. (α = 1.0, β = 0.3, γ = 0.3)\nmodel (Mikolov et al., 2013a) to learn vertex embeddings.\nLINE (Tang et al., 2015) learns vertex embeddings in large-scale networks using first-order and second-order proximities.\nNode2vec (Grover and Leskovec, 2016) proposes a biased random walk algorithm based on DeepWalk to explore neighborhood architecture more efficiently.\nStructure and Text: Naive Combination: We simply concatenate the best-performed structure-based embeddings with CNN based embeddings to represent the vertices.\nTADW (Yang et al., 2015) employs matrix factorization to incorporate text features of vertices into network embeddings.\nCENE (Sun et al., 2016) leverages both structure and textural information by regarding text content as a special kind of vertices, and optimizes the probabilities of heterogenous links."
    }, {
      "heading" : "5.3 Evaluation Metrics and Experiment Settings",
      "text" : "For link prediction, we adopt a standard evaluation metric AUC (Hanley and McNeil, 1982), which represents the probability that vertices in a random unobserved link are more similar than those in a random nonexistent link.\nFor vertex classification, we employ L2regularized logistic regression (L2R-LR) (Fan et al., 2008) to train classifiers, and evaluate the classification accuracies of various methods.\nTo be fair, we set the embedding dimension to 200 for all methods. In LINE, we set the number of negative samples to 5; we learn the 100 dimensional first-order and second-order embeddings respectively, and concatenate them to form the 200 dimensional embeddings. In node2vec, we employ grid search and select the best per-\nformed hyper-parameters for training. We also employ grid search to set the hyper-parameters α, β and γ in CANE. Besides, we set the number of negative samples k to 1 in CANE to speed up the training process. To demonstrate the effectiveness of considering attention mechanism and two types of objectives in Eqs. (3) and (6), we design three versions of CANE for evaluation, i.e., CANE with text only, CANE without attention and CANE."
    }, {
      "heading" : "5.4 Link Prediction",
      "text" : "As shown in Table 2, Table 3 and Table 4, we evaluate the AUC values while removing different ratios of edges on Cora, HepTh and Zhihu respectively. Note that, when we only keep 5% edges for training, most vertices are isolated, which results in the poor and meaningless performance of all the methods. Thus, we omit the results under this training ratio. From these tables, we have the following observations:\n(1) Our proposed CANE consistently achieves significant improvement comparing to all the baselines on all different datasets and different training ratios. It indicates the effectiveness of CANE when applied to link prediction task, and verifies that CANE has the capability of modeling relationships between vertices precisely.\n(2) What calls for special attention is that, both CENE and TADW exhibit unstable performance under various training ratios. Specifically, CENE performs poorly under small training ratios, because it reserves much more parameters (e.g., convolution kernels and word embeddings) than TADW, which need more data for training. Different from CENE, TADW performs much better under small training ratios, because DeepWalk based methods can explore the sparse network structure well through random walks even with limited edges. However, it achieves poor performance\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n695\n696\n697\n698\n699\n%Removed edges 15% 25% 35% 45% 55% 65% 75% 85% 95%\nDeepWalk 55.2 66.0 70.0 75.7 81.3 83.3 87.6 88.9 88.0 LINE 53.7 60.4 66.5 73.9 78.5 83.8 87.5 87.7 87.6 node2vec 57.1 63.6 69.9 76.2 84.3 87.3 88.4 89.2 89.2\nNaive Combination 78.7 82.1 84.7 88.7 88.7 91.8 92.1 92.0 92.7 TADW 87.0 89.5 91.8 90.8 91.1 92.6 93.5 91.9 91.7 CENE 86.2 84.6 89.8 91.2 92.3 91.8 93.2 92.9 93.2\nCANE (text only) 83.8 85.2 87.3 88.9 91.1 91.2 91.8 93.1 93.5 CANE (w/o attention) 84.5 89.3 89.2 91.6 91.1 91.8 92.3 92.5 93.6\nCANE 90.0 91.2 92.0 93.0 94.2 94.6 95.4 95.7 96.3\nTable 3: AUC values on HepTh. (α = 0.7, β = 0.2, γ = 0.2)\n%Removed edges 15% 25% 35% 45% 55% 65% 75% 85% 95%\nDeepWalk 56.6 58.1 60.1 60.0 61.8 61.9 63.3 63.7 67.8 LINE 52.3 55.9 59.9 60.9 64.3 66.0 67.7 69.3 71.1 node2vec 54.2 57.1 57.3 58.3 58.7 62.5 66.2 67.6 68.5\nNaive Combination 55.1 56.7 58.9 62.6 64.4 68.7 68.9 69.0 71.5 TADW 52.3 54.2 55.6 57.3 60.8 62.4 65.2 63.8 69.0 CENE 56.2 57.4 60.3 63.0 66.3 66.0 70.2 69.8 73.8\nCANE (text only) 55.6 56.9 57.3 61.6 63.6 67.0 68.5 70.4 73.5 CANE (w/o attention) 56.7 59.1 60.9 64.0 66.1 68.9 69.8 71.0 74.3\nCANE 56.8 59.3 62.9 64.5 68.9 70.4 71.4 73.6 75.4\nTable 4: AUC values on Zhihu. (α = 1.0, β = 0.3, γ = 0.3)\nunder large ones, as its simplicity and the limitation of bag-of-words assumption. On the contrary, CANE has a stable performance on various situations. It demonstrates the flexibility and robustness of CANE.\n(3) By introducing attention mechanism, the learnt context-aware embeddings obtain considerable improvements than the ones without attention. It verifies our assumption that a specific vertex should play different roles when interacting with other vertices, and thus benefits the relevant link prediction task.\nTo summarize, all the above observations demonstrate that CANE is able to learn highquality context-aware embeddings, which are conducive to estimating the relationship between vertices precisely. Moreover, the experimental results on link prediction task state the effectiveness and robustness of CANE."
    }, {
      "heading" : "5.5 Vertex Classification",
      "text" : "In CANE, we obtain various embeddings of a vertex according to the vertex it connects to. It’s intuitive that the obtained context-aware embeddings are naturally applicable to link prediction task. However, network analysis tasks, such as vertex classification and clustering, require a global embedding, rather than several context-aware embed-\ndings for each vertex. To demonstrate the capability of CANE to solve these issues, we generate the global embedding of a vertex u by simply averaging all the contextaware embeddings as follows:\nu = 1\nN ∑ (u,v)|(v,u)∈E u(v),\nwhere N indicates the number of context-aware embeddings of u.\n50\n55 60 65 70 75 80 85 90 95\n100\nDe ep\nW alk LI NE\nno de\n2v ec NC TA DW CE NE\nCA NE\n(te xt\non ly)\nCA NE\n(w /0\nat ten\ntio n) CA NE\nA cc\nur ac\ny (×\n1 00\n)\nWith the generated global embeddings, we conduct experiments of vertex classification on Cora. As shown in Fig. 3, we observe that:\n(1) CANE achieves comparable performance with state-of-the-art model CENE. It states that the\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nlearnt context-aware embeddings can transform into high-quality context-free embeddings through simple average operation, which can be further employed to other network analysis tasks.\n(2) With the introduction of mutual attention mechanism, CANE has an encouraging improvement than the one without attention, which is in accordance with the results of link prediction. It denotes that CANE is flexible to various network analysis tasks."
    }, {
      "heading" : "5.6 Case Study",
      "text" : "To demonstrate the significance of mutual attention on selecting meaningful features from text information, we visualize the heat maps of two vertex pairs in Fig. 4. Note that, every word in this figure accompanies with various background colors. The stronger the background color is, the larger the weight of this word is. The weight of each word is calculated according to the attention weights as follows.\nFor each vertex pair, we can get the attention weight of each convolution window according to Eq. (11). To obtain the weights of words, we assign the attention weight to each word in this window, and add the attention weights of a word together as its final weight.\nWe select three connected vertices in Cora for example, denoted as A, B and C. From Fig. 4, we observe that, though there exists citation relations with identical paper A, paper B and C concern about different parts of A. The attention weights over A in edge #1 are assigned to “reinforcement learning”. On the contrary, the weights in edge #2 are assigned to “machine learning’”, “supervised learning algorithms” and “complex stochastic models”. Moreover, all these key elements in A can find corresponding words in B and C. It’s intuitive that these key elements give an exact explanation on the citation relations. The discovered significant correlations between vertex pairs reflects the effectiveness of mutual attention mechanism, as well as the capability of CANE for modeling relations precisely."
    }, {
      "heading" : "6 Conclusion and Future Work",
      "text" : "In this paper, we propose the concept of ContextAware Network Embedding (CANE) for the first time, which aims to learn various context-aware embeddings for a vertex according to the neighbors it interacts with. Specifically, we implement\nCANE on text-based information networks with proposed mutual attention mechanism, and conduct experiments on several real-world information networks. Experimental results on link prediction demonstrate that CANE is effective for modeling the relationship between vertices. Besides, the learnt context-aware embeddings can compose high-quality context-free embeddings.\nWe will explore the following directions in future:\n(1) We have investigated the effectiveness of CANE on text-based information networks. In future, we will strive to implement CANE on wider variety of information networks with multi-modal data, such as labels, images and so on.\n(2) CANE encodes latent relations between vertices into their context-aware embeddings. Furthermore, there usually exist explicit relations in social networks (e.g., families, friends and colleagues relations between social network users), which are expected to be critical to NE. Thus, we want to explore how to incorporate and predict these explicit relations between vertices in NE.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "Phil Blunsom", "Edward Grefenstette", "Nal Kalchbrenner." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Blunsom et al\\.,? 2014",
      "shortCiteRegEx" : "Blunsom et al\\.",
      "year" : 2014
    }, {
      "title" : "Grarep: Learning graph representations with global structural information",
      "author" : [ "Shaosheng Cao", "Wei Lu", "Qiongkai Xu." ],
      "venue" : "Proceedings of CIKM. pages 891–900.",
      "citeRegEx" : "Cao et al\\.,? 2015",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2015
    }, {
      "title" : "Incorporate group information to enhance network embedding",
      "author" : [ "Jifan Chen", "Qi Zhang", "Xuanjing Huang." ],
      "venue" : "Proceedings of CIKM.",
      "citeRegEx" : "Chen et al\\.,? 2016",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart Van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Liblinear: A library for large linear classification",
      "author" : [ "Rong-En Fan", "Kai-Wei Chang", "Cho-Jui Hsieh", "XiangRui Wang", "Chih-Jen Lin." ],
      "venue" : "JMLR 9:1871– 1874.",
      "citeRegEx" : "Fan et al\\.,? 2008",
      "shortCiteRegEx" : "Fan et al\\.",
      "year" : 2008
    }, {
      "title" : "Node2vec: Scalable feature learning for networks",
      "author" : [ "Aditya Grover", "Jure Leskovec." ],
      "venue" : "Proceedings of KDD.",
      "citeRegEx" : "Grover and Leskovec.,? 2016",
      "shortCiteRegEx" : "Grover and Leskovec.",
      "year" : 2016
    }, {
      "title" : "The meaning and use of the area under a receiver operating characteristic (roc) curve",
      "author" : [ "James A Hanley", "Barbara J McNeil." ],
      "venue" : "Radiology 143(1):29–",
      "citeRegEx" : "Hanley and McNeil.,? 1982",
      "shortCiteRegEx" : "Hanley and McNeil.",
      "year" : 1982
    }, {
      "title" : "Convolutional neural network architectures for matching natural language sentences",
      "author" : [ "Baotian Hu", "Zhengdong Lu", "Hang Li", "Qingcai Chen." ],
      "venue" : "Proceedings of NIPS. pages 2042–2050.",
      "citeRegEx" : "Hu et al\\.,? 2014",
      "shortCiteRegEx" : "Hu et al\\.",
      "year" : 2014
    }, {
      "title" : "Effective use of word order for text categorization with convolutional neural networks",
      "author" : [ "Rie Johnson", "Tong Zhang." ],
      "venue" : "arXiv preprint arXiv:1412.1058 .",
      "citeRegEx" : "Johnson and Zhang.,? 2014",
      "shortCiteRegEx" : "Johnson and Zhang.",
      "year" : 2014
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim" ],
      "venue" : null,
      "citeRegEx" : "Kim.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Kingma and Ba.,? 2015",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Proceedings of NIPS. pages 3294–3302.",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Graphs over time: densification laws, shrinking diameters and possible explanations",
      "author" : [ "Jure Leskovec", "Jon Kleinberg", "Christos Faloutsos." ],
      "venue" : "Proceedings of KDD. pages 177–187.",
      "citeRegEx" : "Leskovec et al\\.,? 2005",
      "shortCiteRegEx" : "Leskovec et al\\.",
      "year" : 2005
    }, {
      "title" : "Automating the construction of internet portals with machine learning",
      "author" : [ "Andrew McCallum", "Kamal Nigam", "Jason Rennie", "Kristie Seymore." ],
      "venue" : "Information Retrieval Journal 3:127–163.",
      "citeRegEx" : "McCallum et al\\.,? 2000",
      "shortCiteRegEx" : "McCallum et al\\.",
      "year" : 2000
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of ICIR.",
      "citeRegEx" : "Mikolov et al\\.,? 2013a",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean." ],
      "venue" : "Proceedings of NIPS. pages 3111–3119.",
      "citeRegEx" : "Mikolov et al\\.,? 2013b",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Deepwalk: Online learning of social representations",
      "author" : [ "Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena." ],
      "venue" : "Proceedings of KDD. pages 701–710.",
      "citeRegEx" : "Perozzi et al\\.,? 2014",
      "shortCiteRegEx" : "Perozzi et al\\.",
      "year" : 2014
    }, {
      "title" : "Bidirectional recurrent neural networks",
      "author" : [ "Mike Schuster", "Kuldip K Paliwal." ],
      "venue" : "IEEE Transactions on Signal Processing 45(11):2673–2681.",
      "citeRegEx" : "Schuster and Paliwal.,? 1997",
      "shortCiteRegEx" : "Schuster and Paliwal.",
      "year" : 1997
    }, {
      "title" : "A general framework for content-enhanced network representation learning",
      "author" : [ "Xiaofei Sun", "Jiang Guo", "Xiao Ding", "Ting Liu." ],
      "venue" : "arXiv preprint arXiv:1610.02906 .",
      "citeRegEx" : "Sun et al\\.,? 2016",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 2016
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks",
      "author" : [ "Kai Sheng Tai", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Tai et al\\.,? 2015",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "Line: Large-scale information network embedding",
      "author" : [ "Jian Tang", "Meng Qu", "Mingzhe Wang", "Ming Zhang", "Jun Yan", "Qiaozhu Mei." ],
      "venue" : "Proceedings of WWW. pages 1067–1077.",
      "citeRegEx" : "Tang et al\\.,? 2015",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2015
    }, {
      "title" : "Relational learning via latent social dimensions",
      "author" : [ "Lei Tang", "Huan Liu." ],
      "venue" : "Proceedings of SIGKDD. pages 817–826.",
      "citeRegEx" : "Tang and Liu.,? 2009",
      "shortCiteRegEx" : "Tang and Liu.",
      "year" : 2009
    }, {
      "title" : "Max-margin deepwalk: Discriminative learning of network representation",
      "author" : [ "Cunchao Tu", "Weicheng Zhang", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of IJCAI.",
      "citeRegEx" : "Tu et al\\.,? 2016",
      "shortCiteRegEx" : "Tu et al\\.",
      "year" : 2016
    }, {
      "title" : "Structural deep network embedding",
      "author" : [ "Daixin Wang", "Peng Cui", "Wenwu Zhu." ],
      "venue" : "Proceedings of KDD.",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Network representation learning with rich text information",
      "author" : [ "Cheng Yang", "Zhiyuan Liu", "Deli Zhao", "Maosong Sun", "Edward Y Chang." ],
      "venue" : "Proceedings of IJCAI.",
      "citeRegEx" : "Yang et al\\.,? 2015",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Hence, NE is attracting many research interests in recent years (Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016), and achieves promising performance on many network analysis tasks including link prediction, vertex classification, and community detection.",
      "startOffset" : 64,
      "endOffset" : 132
    }, {
      "referenceID" : 20,
      "context" : "Hence, NE is attracting many research interests in recent years (Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016), and achieves promising performance on many network analysis tasks including link prediction, vertex classification, and community detection.",
      "startOffset" : 64,
      "endOffset" : 132
    }, {
      "referenceID" : 5,
      "context" : "Hence, NE is attracting many research interests in recent years (Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016), and achieves promising performance on many network analysis tasks including link prediction, vertex classification, and community detection.",
      "startOffset" : 64,
      "endOffset" : 132
    }, {
      "referenceID" : 0,
      "context" : "For each vertex, we can easily use neural models, such as convolutional neural networks (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (Kiros et al.",
      "startOffset" : 88,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "For each vertex, we can easily use neural models, such as convolutional neural networks (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (Kiros et al.",
      "startOffset" : 88,
      "endOffset" : 146
    }, {
      "referenceID" : 9,
      "context" : "For each vertex, we can easily use neural models, such as convolutional neural networks (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (Kiros et al.",
      "startOffset" : 88,
      "endOffset" : 146
    }, {
      "referenceID" : 11,
      "context" : ", 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (Kiros et al., 2015; Tai et al., 2015), to build context-free text-based embedding.",
      "startOffset" : 74,
      "endOffset" : 112
    }, {
      "referenceID" : 19,
      "context" : ", 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (Kiros et al., 2015; Tai et al., 2015), to build context-free text-based embedding.",
      "startOffset" : 74,
      "endOffset" : 112
    }, {
      "referenceID" : 16,
      "context" : "Both context-free embeddings and contextaware embeddings of each vertex can be efficiently learned together via concatenation using existing NE methods such as DeepWalk (Perozzi et al., 2014), LINE (Tang et al.",
      "startOffset" : 169,
      "endOffset" : 191
    }, {
      "referenceID" : 20,
      "context" : ", 2014), LINE (Tang et al., 2015) and node2vec (Grover and Leskovec, 2016).",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : ", 2015) and node2vec (Grover and Leskovec, 2016).",
      "startOffset" : 21,
      "endOffset" : 48
    }, {
      "referenceID" : 21,
      "context" : "In recent years, there have been a large number of NE models proposed to learn efficient vertex embeddings (Tang and Liu, 2009; Cao et al., 2015; Wang et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 164
    }, {
      "referenceID" : 1,
      "context" : "In recent years, there have been a large number of NE models proposed to learn efficient vertex embeddings (Tang and Liu, 2009; Cao et al., 2015; Wang et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 164
    }, {
      "referenceID" : 23,
      "context" : "In recent years, there have been a large number of NE models proposed to learn efficient vertex embeddings (Tang and Liu, 2009; Cao et al., 2015; Wang et al., 2016).",
      "startOffset" : 107,
      "endOffset" : 164
    }, {
      "referenceID" : 16,
      "context" : "For example, DeepWalk (Perozzi et al., 2014) performs random walks over networks and introduces an efficient word representation learning model, Skip-Gram (Mikolov et al.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 14,
      "context" : ", 2014) performs random walks over networks and introduces an efficient word representation learning model, Skip-Gram (Mikolov et al., 2013a), to learn network embeddings.",
      "startOffset" : 118,
      "endOffset" : 141
    }, {
      "referenceID" : 20,
      "context" : "LINE (Tang et al., 2015) optimizes the joint and conditional probabilities of edges in large-scale networks to learn vertex representations.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "Node2vec (Grover and Leskovec, 2016) modifies the random walk strategy in DeepWalk into biased random walks to explore the network structure more efficiently.",
      "startOffset" : 9,
      "endOffset" : 36
    }, {
      "referenceID" : 1,
      "context" : "In recent years, there have been a large number of NE models proposed to learn efficient vertex embeddings (Tang and Liu, 2009; Cao et al., 2015; Wang et al., 2016). For example, DeepWalk (Perozzi et al., 2014) performs random walks over networks and introduces an efficient word representation learning model, Skip-Gram (Mikolov et al., 2013a), to learn network embeddings. LINE (Tang et al., 2015) optimizes the joint and conditional probabilities of edges in large-scale networks to learn vertex representations. Node2vec (Grover and Leskovec, 2016) modifies the random walk strategy in DeepWalk into biased random walks to explore the network structure more efficiently. Nevertheless, most of these NE models only encode the structural information into vertex embeddings, without considering heterogenous information accompanied with vertices in real-world social networks. To address this issue, researchers make great efforts to incorporate heterogenous information into conventional NE models. For instance, Yang et al. (2015) present text-associated DeepWalk (TADW) to improve matrix factorization based DeepWalk with text information.",
      "startOffset" : 128,
      "endOffset" : 1034
    }, {
      "referenceID" : 1,
      "context" : "In recent years, there have been a large number of NE models proposed to learn efficient vertex embeddings (Tang and Liu, 2009; Cao et al., 2015; Wang et al., 2016). For example, DeepWalk (Perozzi et al., 2014) performs random walks over networks and introduces an efficient word representation learning model, Skip-Gram (Mikolov et al., 2013a), to learn network embeddings. LINE (Tang et al., 2015) optimizes the joint and conditional probabilities of edges in large-scale networks to learn vertex representations. Node2vec (Grover and Leskovec, 2016) modifies the random walk strategy in DeepWalk into biased random walks to explore the network structure more efficiently. Nevertheless, most of these NE models only encode the structural information into vertex embeddings, without considering heterogenous information accompanied with vertices in real-world social networks. To address this issue, researchers make great efforts to incorporate heterogenous information into conventional NE models. For instance, Yang et al. (2015) present text-associated DeepWalk (TADW) to improve matrix factorization based DeepWalk with text information. Tu et al. (2016) propose max-margin DeepWalk (MMDW) to learn discriminative network representations by utilizing labeling information of vertices.",
      "startOffset" : 128,
      "endOffset" : 1161
    }, {
      "referenceID" : 1,
      "context" : "In recent years, there have been a large number of NE models proposed to learn efficient vertex embeddings (Tang and Liu, 2009; Cao et al., 2015; Wang et al., 2016). For example, DeepWalk (Perozzi et al., 2014) performs random walks over networks and introduces an efficient word representation learning model, Skip-Gram (Mikolov et al., 2013a), to learn network embeddings. LINE (Tang et al., 2015) optimizes the joint and conditional probabilities of edges in large-scale networks to learn vertex representations. Node2vec (Grover and Leskovec, 2016) modifies the random walk strategy in DeepWalk into biased random walks to explore the network structure more efficiently. Nevertheless, most of these NE models only encode the structural information into vertex embeddings, without considering heterogenous information accompanied with vertices in real-world social networks. To address this issue, researchers make great efforts to incorporate heterogenous information into conventional NE models. For instance, Yang et al. (2015) present text-associated DeepWalk (TADW) to improve matrix factorization based DeepWalk with text information. Tu et al. (2016) propose max-margin DeepWalk (MMDW) to learn discriminative network representations by utilizing labeling information of vertices. Chen et al. (2016) propose group-enhanced network embedding (GENE) to integrate existing group information in NE.",
      "startOffset" : 128,
      "endOffset" : 1310
    }, {
      "referenceID" : 1,
      "context" : "In recent years, there have been a large number of NE models proposed to learn efficient vertex embeddings (Tang and Liu, 2009; Cao et al., 2015; Wang et al., 2016). For example, DeepWalk (Perozzi et al., 2014) performs random walks over networks and introduces an efficient word representation learning model, Skip-Gram (Mikolov et al., 2013a), to learn network embeddings. LINE (Tang et al., 2015) optimizes the joint and conditional probabilities of edges in large-scale networks to learn vertex representations. Node2vec (Grover and Leskovec, 2016) modifies the random walk strategy in DeepWalk into biased random walks to explore the network structure more efficiently. Nevertheless, most of these NE models only encode the structural information into vertex embeddings, without considering heterogenous information accompanied with vertices in real-world social networks. To address this issue, researchers make great efforts to incorporate heterogenous information into conventional NE models. For instance, Yang et al. (2015) present text-associated DeepWalk (TADW) to improve matrix factorization based DeepWalk with text information. Tu et al. (2016) propose max-margin DeepWalk (MMDW) to learn discriminative network representations by utilizing labeling information of vertices. Chen et al. (2016) propose group-enhanced network embedding (GENE) to integrate existing group information in NE. Sun et al. (2016) regard text content as a special kind of vertices, and propose contextenhanced network embedding (CENE) through leveraging both structural and textural information to learn network embeddings.",
      "startOffset" : 128,
      "endOffset" : 1423
    }, {
      "referenceID" : 20,
      "context" : "Following LINE (Tang et al., 2015), we define the conditional probability of v generated by u in Eq.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "4 Context-Free Text Embedding There has been a variety of neural models to obtain text embeddings from a word sequence, such as convolutional neural networks (CNN) (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (RNN) (Kiros et al.",
      "startOffset" : 164,
      "endOffset" : 222
    }, {
      "referenceID" : 8,
      "context" : "4 Context-Free Text Embedding There has been a variety of neural models to obtain text embeddings from a word sequence, such as convolutional neural networks (CNN) (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (RNN) (Kiros et al.",
      "startOffset" : 164,
      "endOffset" : 222
    }, {
      "referenceID" : 9,
      "context" : "4 Context-Free Text Embedding There has been a variety of neural models to obtain text embeddings from a word sequence, such as convolutional neural networks (CNN) (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (RNN) (Kiros et al.",
      "startOffset" : 164,
      "endOffset" : 222
    }, {
      "referenceID" : 11,
      "context" : ", 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (RNN) (Kiros et al., 2015; Tai et al., 2015).",
      "startOffset" : 80,
      "endOffset" : 118
    }, {
      "referenceID" : 19,
      "context" : ", 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (RNN) (Kiros et al., 2015; Tai et al., 2015).",
      "startOffset" : 80,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : "In this work, we investigate different neural networks for text modeling, including CNN, Bidirectional RNN (Schuster and Paliwal, 1997) and GRU (Cho et al.",
      "startOffset" : 107,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "In this work, we investigate different neural networks for text modeling, including CNN, Bidirectional RNN (Schuster and Paliwal, 1997) and GRU (Cho et al., 2014), and employ the best performed CNN, which can capture the local semantic dependency among words.",
      "startOffset" : 144,
      "endOffset" : 162
    }, {
      "referenceID" : 7,
      "context" : "Note that, we add zero padding vectors (Hu et al., 2014) at the edge of the sentence.",
      "startOffset" : 39,
      "endOffset" : 56
    }, {
      "referenceID" : 15,
      "context" : "Thus, we employ negative sampling (Mikolov et al., 2013b) and transform the objective into the following form:",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : "Afterwards, we employ Adam (Kingma and Ba, 2015) to optimize the transformed objective.",
      "startOffset" : 27,
      "endOffset" : 48
    }, {
      "referenceID" : 13,
      "context" : "We select three real-world network datasets as follows: Cora1 is a typical paper citation network constructed by (McCallum et al., 2000).",
      "startOffset" : 113,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "HepTh2 (High Energy Physics Theory) is another citation network from arXiv3 released by (Leskovec et al., 2005).",
      "startOffset" : 88,
      "endOffset" : 111
    }, {
      "referenceID" : 16,
      "context" : "2 Baselines We employ the following methods as baselines: Structure-only: DeepWalk (Perozzi et al., 2014) performs random walks over networks and employ Skip-Gram https://people.",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : "model (Mikolov et al., 2013a) to learn vertex embeddings.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 20,
      "context" : "LINE (Tang et al., 2015) learns vertex embeddings in large-scale networks using first-order and second-order proximities.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 5,
      "context" : "Node2vec (Grover and Leskovec, 2016) proposes a biased random walk algorithm based on DeepWalk to explore neighborhood architecture more efficiently.",
      "startOffset" : 9,
      "endOffset" : 36
    }, {
      "referenceID" : 24,
      "context" : "TADW (Yang et al., 2015) employs matrix factorization to incorporate text features of vertices into network embeddings.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 18,
      "context" : "CENE (Sun et al., 2016) leverages both structure and textural information by regarding text content as a special kind of vertices, and optimizes the probabilities of heterogenous links.",
      "startOffset" : 5,
      "endOffset" : 23
    }, {
      "referenceID" : 6,
      "context" : "For link prediction, we adopt a standard evaluation metric AUC (Hanley and McNeil, 1982), which represents the probability that vertices in a random unobserved link are more similar than those in a random nonexistent link.",
      "startOffset" : 63,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "For vertex classification, we employ L2regularized logistic regression (L2R-LR) (Fan et al., 2008) to train classifiers, and evaluate the classification accuracies of various methods.",
      "startOffset" : 80,
      "endOffset" : 98
    } ],
    "year" : 2017,
    "abstractText" : "Network embedding (NE) is playing a critical role in network analysis, due to its ability to represent vertices with efficient low-dimensional embedding vectors. However, existing NE models aim to learn a fixed context-free embedding for each vertex, and neglect the diverse roles when interacting with other vertices. In this paper, we assume that one vertex usually shows different aspects when interacting with different neighbor vertices, and should own different embeddings respectively. Therefore, we present ContextAware Network Embedding (CANE), a novel NE model to address this issue. CANE learns context-aware embeddings for vertices with mutual attention mechanism and is expected to model the semantic relationships between vertices more precisely. In experiments, we compare our model with existing NE models on three real-world datasets. Experimental results shows that CANE achieves significant improvement than state-of-the-art methods on link prediction, and comparable performance on vertex classification.",
    "creator" : "LaTeX with hyperref package"
  }
}