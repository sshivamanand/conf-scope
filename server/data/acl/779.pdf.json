{
  "name" : "779.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Teacher-Student Framework for Zero-Resource Neural Machine Translation",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community. Although NMT has achieved state-of-theart translation performance on resource-rich language pairs such as English-French and GermanEnglish (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016), it still suffers from the unavailability of large-scale parallel corpora for translating low resource languages. Due to the large parameter space, neural models usually learn poorly from low-count events, resulting in a poor choice for low resource language pairs. Zoph et al. (2016) indicate that NMT obtains much worse translation quality than a statistical machine translation\n(SMT) system on low-resource languages. As a result, a number of authors have endeavored to explore methods for translating language pairs without parallel corpora available. These methods can be roughly divided into two broad categories: multilingual and pivot-based. Firat et al. (2016b) present a multi-way, multilingual model with shared attention to achieve zeroresource translation. They fine-tune the attention part using pseudo bilingual sentences for the zeroresource language pair. Another direction is to develop a universal NMT model in multilingual scenarios (Johnson et al., 2016; Ha et al., 2016). They use parallel corpora of multiple languages to train one single model, which is then able to translate a language pair without parallel corpora available. Although these approaches prove to be effective, the combination of multiple languages in modeling and training leads to significantly increased complexity.\nAnother direction is to achieve source-to-target NMT without parallel data via a pivot, which is either text (Cheng et al., 2016) or image (Nakayama and Nishida, 2016b). Cheng et al. (2016) propose a pivot-based method to zero-resource NMT: it first translates the source language to a pivot language, which is then translated to the target language. Nakayama and Nishida (2016b) show that using multimedia information as pivot also significantly benefit zero-resource translation. However, pivot-based approaches usually need to divide the decoding process into two steps, which is not only more computationally expensive, but also potentially suffers from the error propagation problem (Zhu et al., 2013).\nIn this paper, we propose a new method for zero-resource neural machine translation. Our method assumes that parallel sentences should have close probabilities of generating a sentence in a third language. To train a source-to-target NMT\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n(a) (b)\nX YZ Z\nX\nY\nP (z|x;✓x!z) P (y|z;✓z!y)\nP (y|z;✓z!y)\nP (y|x;✓x!y)\nFigure 1: (a) The pivot-based approach and (b) the teacher-student approach to zero-resource neural machine translation. X, Y, and Z denote source, target, and pivot languages, respectively. We use a dashed line to denote that there is a parallel corpus available for the connected language pair. Solid lines with arrows represent translation directions. The pivot-based approach leverages a pivot to achieve indirect source-to-target translation: it first translates x into z, which is then translated into y. Our approach directly trains the intended source-to-target model (“student”) on a source-pivot parallel corpus, with the guidance of an existing pivot-to-target model (“teacher”). Our training algorithm is based on the translation equivalence assumption: if x is a translation of z, then P (y|x;θx→y) should be close to P (y|z;θz→y).\nmodel without parallel corpora (“student”), we leverage an existing pivot-to-target NMT model (“teacher”) to guide the learning process of the student model on a source-pivot parallel corpus. As compared with the pivot-based approaches (Cheng et al., 2016; Nakayama and Nishida, 2016b), our method allows direct modeling of the intended NMT model, without the need to divide training or decoding into two steps. This strategy not only improves efficiency but also avoids error propagation in decoding. Experiments on the Europarl and WMT datasets show that our approach achieves significant improvements in terms of both translation quality and decoding efficiency over a baseline pivot-based approach to zeroresource NMT on Spanish-French and GermanFrench translation tasks."
    }, {
      "heading" : "2 Background",
      "text" : "Neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2014) advocates the use of neural networks to model the translation process in an end-to-end manner. As a data-driven approach, NMT treats parallel corpora as the major source for acquiring translation knowledge.\nLet x be a source-language sentence and y be a target-language sentence. We use P (y|x;θx→y) to denote a source-to-target neural translation model, where θx→y is a set of model parame-\nters. Given a source-target parallel corpus Dx,y, which is a set of parallel source-target sentences, the model parameters can be learned by maximizing the log-likelihood of the parallel corpus:\nθ̂x→y = argmax θx→y { ∑ 〈x,y〉∈Dx,y logP (y|x;θx→y) }\nGiven learned model parameters θ̂x→y, the decision rule for finding the translation with the highest probability for a source sentence x is given by\nŷ = argmax y\n{ P (y|x; θ̂x→y) } (1)\nAs a data-driven approach, NMT heavily relies on the availability of large-scale parallel corpora to deliver state-of-the-art translation performance (Wu et al., 2016). Zoph et al. (2016) report that NMT obtains much lower BLEU scores than SMT if only small-scale parallel corpora are available. The heavy dependence on the quantity of training data poses a severe challenge for NMT to translation zero-resource language pairs.\nSimple and easy-to-implement, pivot-based methods have been widely used in SMT for translating zero-resource language pairs (De Gispert and Marino, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007;\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nBertoldi et al., 2008; Wu and Wang, 2009; Zahabi et al., 2013; Kholy et al., 2013). As pivotbased methods are agnostic to model structures, they have been adapted to NMT recently (Cheng et al., 2016; Nakayama and Nishida, 2016b).\nFigure 1 illustrates the basic idea of pivot-based approaches to zero-resource NMT (Cheng et al., 2016; Nakayama and Nishida, 2016b). Let X, Y, and Z to denote source, target, and pivot languages. We use dashed lines to denote language pairs with parallel corpora available and solid lines with arrows to denote translation directions.\nIntuitively, the source-to-target translation can be indirectly modeled by bridging two NMT models via a pivot:\nP (y|x;θx→z,θz→y) = ∑ z P (z|x;θx→z)P (y|z;θz→y) (2)\nAs shown in Figure 1, the pivot-based approaches assume that the source-to-target parallel corpus Dx,z and the pivot-to-target parallel corpus Dz,y are available. As it is impractical to enumerate all possible pivot sentences, the two NMT models are trained separately in practice:\nθ̂x→z = argmax θx→z { ∑ 〈x,z〉∈Dx,z logP (z|x;θx→z) }\nθ̂z→y = argmax θz→y { ∑ 〈z,y〉∈Dz,y logP (y|z;θz→y) }\nDue to the exponential search space of pivot sentences, the decoding process of translating an unseen source sentence x has to be divided into two steps:\nẑ = argmax z\n{ P (z|x; θ̂x→z) } (3)\nŷ = argmax y\n{ P (y|ẑ; θ̂z→y) } (4)\nDespite its simplicity, the above two-step decoding process potentially suffers from the error propagation problem (Zhu et al., 2013): the translation errors made in the first step (i.e., source-topivot translation) will affect the second step (i.e., pivot-to-target translation).\nTherefore, it is necessary to explore methods for direct modeling of source-to-target translation without parallel corpora available."
    }, {
      "heading" : "3 Approach",
      "text" : ""
    }, {
      "heading" : "3.1 Assumptions",
      "text" : "In this work, we propose to directly model the intended source-to-target neural translation based on a teacher-student framework. The basic idea is to use a pre-trained pivot-to-target model (“teacher”) to guide the learning process of a source-to-target model without training data available (“student”) on a source-pivot parallel corpus.\nAs shown in Figure 1(b), we still assume that a source-to-pivot parallel corpus Dx,z and a pivot-to-source parallel corpus Dz,y are available. Unlike pivot-based approaches, we first use the pivot-to-target parallel corpus Dz,y to obtain a teacher model P (y|z; θ̂z→y), where θ̂z→y is a set of learned model parameters. Then, the teacher model “teaches” the student model P (y|x;θx→y) based on the following assumption. Assumption 1 If a source sentence x is a translation of a pivot sentence z, then the probability of generating a target sentence y from x should be close to that from its counterpart z.\nWe can move a further step to introduce a wordlevel assumption:\nAssumption 2 If a source word x is a translation of a pivot word z, then the probability of generating a target sentence y from x should be close to that from its counterpart z.\nThe two assumptions are empirically verified in our experiments (see Table 2). In the following subsections, we will introduce two approaches to zero-resource neural machine translation based on the two assumptions."
    }, {
      "heading" : "3.2 Sentence-Level Teaching",
      "text" : "Given a source-pivot parallel corpus Dx,z , our training objective based on Assumption 1 is defined as follows:\nJSENT(θx→y) = ∑ 〈x,z〉∈Dx,z KL ( P (y|z; θ̂z→y) ∣∣∣∣∣∣P (y|x;θx→y)) (5) where the KL divergence sums over all possible target sentences:\nKL ( P (y|z; θ̂z→y) ∣∣∣∣∣∣P (y|x;θx→y)) =\n∑ y∈Y P (y|z; θ̂z→y) log P (y|z; θ̂z→y) P (y|x;θx→y) (6)\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nAs the teacher model parameters are fixed, the training objective can be equivalently written as\nJSENT(θx→y) = − ∑ 〈x,z〉∈Dx,z Ey|z;θ̂z→y [ logP (y|x;θx→y) ] (7)\nIn training, our goal is to find a set of source-totarget model parameters that maximizes the training objective:\nθ̂x→y = argmax θx→y\n{ JSENT(θx→y) } (8)\nHowever, a major difficulty is the intractability in calculating the gradients because of the exponential search space of target sentences. To address this problem, it is possible to construct a subspace by either sampling (Shen et al., 2016), generating a k-best list (Yong et al., 2016) or mode approximation (Kim and Rush, 2016). Then, standard stochastic gradient descent algorithms can be used to optimize model parameters."
    }, {
      "heading" : "3.3 Word-Level Teaching",
      "text" : "Instead of minimizing the KL divergence between the teacher and student models at the sentence level, we further define a training objective based on Assumption 2 at the word level:\nJWORD(θx→y) = ∑ 〈x,z〉∈Dx,z Ey|z;θ̂z→y [ J(x,y, z, θ̂z→y,θx→y) ] (9)\nwhere\nJ(x,y, z, θ̂z→y,θx→y)\n= |y|∑ j=1 KL ( P (y|z,y<j ; θ̂z→y) ∣∣∣∣∣∣ P (y|x,y<j ;θx→y) ) (10)\nEquation (9) suggests that the teacher model P (y|z,y<j ; θ̂z→y) “teaches” the student model P (y|x,y<j ;θx→y) in a word-by-word way. Note that the KL-divergence between two models is defined at the word level:\nKL ( P (y|z,y<j ; θ̂z→y) ∣∣∣∣∣∣P (y|x,y<j ;θx→y)) = ∑ y∈Vy P (y|z,y<j ; θ̂z→y) log P (y|z,y<j ; θ̂z→y) P (y|x,y<j ;θx→y)\nCorpus Direction Train Dev. Test\nEuroparl Es→ En 850K 2,000 2,000 De→ En 840K 2,000 2,000 En→ Fr 900K 2,000 2,000\nWMT Es→ En 6.78M 3,003 3,003 En→ Fr 9.29M 3,003 3,003\nTable 1: Data statistics. For the Europarl corpus, we evaluate our approach on Spanish-French (EsFr) and Germany-French (De-Fr) translation tasks. For the WMT corpus, we our approach on the Spanish-French (Es-Fr) translation task. English is used a pivot language in all experiments.\nTherefore, our goal is to find a set of source-totarget model parameters that maximizes the training objective:\nθ̂x→y = argmax θx→y\n{ JWORD(θx→y) } (11)\nWe use similar approach for approximating the full search space with sentence-level teaching."
    }, {
      "heading" : "4 Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Setup",
      "text" : "We evaluate our approach on the Europarl and WMT corpora. To compare with the pivot-based methods, we use the same dataset as in the paper (Cheng et al., 2016). The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.perl script. All the experiments treat English as the pivot language and French as the target language.\nFor the Europarl corpus (Koehn, 2005), we evaluate our proposed model on Spanish-French (Es-Fr) and Germany-French (De-Fr) translation tasks in a zero-resource scenario. To build nonoverlapping source-to-pivot and pivot-to-target datasets, we split pivot sentences shared by the original source-to-pivot and pivot-to-target corpora into two equal parts and merge them separately to the remaining source-to-pivot and pivotto-target corpora. Both the development and test datasets are from shared task 2006. All the sentences are tokenized by the tokenize.perl script. To deal with out-of-vocabulary words, we also adopt byte pair encoding (BPE) (Sennrich et al., 2016) to split words into sub-words. The size of sub-words is set to 30K for each language.\nFor the WMT corpus, we evaluate our approach on a Spanish-French (Es-Fr) translation task with\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nApprox. P (y|x;θx→y) .\nrandom init. trained\nJSENT greedy 313.0 53.5 beam 323.5 51.8 JWORD greedy 274.0 37.1 beam 288.7 36.5 sampling 268.6 40.5\nTable 2: Approximate KL divergence from source-to-target to pivot-to-target model. We use a random initialized source-to-target model for comparison.\na zero-resource setting. We combine the following corpora to form the Es-En and En-Fr parallel corpora: Common Crawl, News Commentary, Europarl v7 and UN. All the sentences are tokenized by the tokenize.perl script. Newstest2011 serves as the development set and newstest2012 and newstest2013 serve as test sets. We use case-sensitive BLEU to evaluate translation results. BPE is also used to reduce the vocabulary size. The size of sub-words is set to 43K, 33K, 43K for Spanish, English and French, respectively. See Table 1 for detailed statistics for both Europarl and WMT corpora.\nWe leverage a tutorial NMT open-source code implemented by Theano for all the experiments. 1 and compare our approach with state-of-the-art multilingual method (Firat et al., 2016b) and pivotbased method (Cheng et al., 2016) with the following variations:\n1. Sentence-Level Teaching: for simplicity, we use mode as suggested in (Kim and Rush, 2016) to approximate the target sentence space, which is the result of running beam search on the pivot sentences and taking the highest-scoring target sentence with the teacher model. Beam size with K = 1 (greedy decoding) and K = 5 are investigated in our paper, denoted as sent-greedy and sent-beam, respectively.\n2. Word-Level Teaching: we use the same mode approximation approach as in sentence-level teaching, denoted as word-greedy (beam search with K=1) and word-beam (beam search with K=5) respectively. Besides, Monte Carlo estimation by sampling from the\n1dl4mt-tutorial: https://github.com/nyu-dl\nteacher model is also investigated since it introduces more diverse data, denoted as wordsampling."
    }, {
      "heading" : "4.2 Assumptions Verification",
      "text" : "To verify the assumptions in Section 3.1, we train a source-to-target translation model P (y|x;θx→y) and a pivot-to-target translation model P (y|z;θz→y) using trilingual corpus. Then we use these two models to calculate JSENT and JWORD, which reveals how close the sentencelevel and word-level distributions are, respectively. Since it is difficult to measure JSENT and JWORD directly , we instead calculate their approximations as illustrated in Section 4.1. We report average KL divergence with different approximations evaluated on the aligned trilingual development set Exzy with 2000 sentences from shared task 2006 as shown in Table 1. We also report KL Divergence from a randomly initialized NMT model Prandom to pivot-to-target P (y|z,θz→y) for comparison.\nThe results agree with our assumptions since significantly smaller KL divergence is observed for KL ( P (y|z;θz→y)||P (y|x;θx→y) ) . For sentence-level distributions, smaller KL divergence for beam search approximation is observed compared with greedy approximation, indicating that mode produced by the beam search results of P (y|z;θz→y) have higher probabilities in P (y|x;θx→y) compared with the greedy results. Thus, we suspect that the sent-beam method will surpass the sent-greedy method. For word-level results, KL divergence of sampling approximation is the highest, indicating poorer performance than the other two methods. However, sampling methods introduce more data diversity at the target side. Thus it is harder to decide which factor dominates the training process. We leave further discussion to Section 4.3."
    }, {
      "heading" : "4.3 Results on the Europarl Corpus",
      "text" : ""
    }, {
      "heading" : "4.3.1 Comparison with pivot-based methods",
      "text" : "Table 3 gives BLEU scores on the Europarl corpus of our best performing sentence-level method (sent-beam) and word-level method (wordsampling) compared with pivot-based methods (Cheng et al., 2016). The same data preprocessing are applied in (Cheng et al., 2016) as in our experiments. We find that our sentence-level method and word-level method outperform all of their zero-resource approaches across language\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nMethod BLEU\nEs→ Fr De→ Fr\nCheng.# pivot 29.79 23.70 hard 29.93 23.88 soft 30.57 23.79 likelihood† 32.59 25.93 Ours sent-beam 31.64 24.39 word-sampling 33.86 27.03\nTable 3: Comparison with previous work on Spanish-French and German-French translation tasks from Europarl corpus. English is treated as the pivot language. † denotes using parallel source-to-target corpus. # denotes methods in Cheng et al. (2016).\npairs. Our word-sampling method improves their best zero-resource results on Spanish-French translation by 3.29 BLEU and Germany-French translation by 3.15 BLEU. Besides that, our word-sampling method surprisingly obtains improvement over the likelihood method, which leverages source-to-target parallel corpus and also takes much longer to train. The significant improvements can be explained by the error propagation problem of pivot-based methods that translation error of the source-pivot translation system will be transferred to the pivot-target translation."
    }, {
      "heading" : "4.3.2 Comparison of Sentence-Level and Word-Level Methods",
      "text" : "Table 4 shows BLEU scores on the Europarl corpus of our proposed five methods.\nFor sentence-level approaches, sent-beam outperforms sent-greedy by 0.59 BLEU over es→ fr translation and 2.51 BLEU over de→ fr translation. The results are in line with our observations in Section 4.2 that sentence-level KLD by beam approximation is smaller than that by greedy approximation. However, as the time complexity grows linearly in the number of beams k, the better performance is achieved at the expense of beam search time.\nFor word-level experiments, we observe that word-sampling performs much better than the other two methods: 1.94 BLEU on es→ fr translation and 2.65 BLEU on de→ fr translation; 1.88 BLEU on es→ fr translation and 2.84 BLEU on de→ fr translation. Althrough word-level KLD calculated by Monte Carlo estimation is larger\nMethods #sents\nBLEU de-en en-fr de-fr\nMLE − − 840K 26.06 - - 1.5M 29.02 Ours 840K 900K - 27.03\nTable 5: Comparison on German-French translation tasks from Europarl corpus with stardard NMT.\nthan that by mode approximation, word-sampling introduces more data diversity for training, which dominates the effect of KLD difference."
    }, {
      "heading" : "4.3.3 BLEU Score over Iterations",
      "text" : "Figure 3 shows valid loss 2 and BLEU score over iterations. We observe that word-level models tend to have lower valid loss compared with sentencelevel methods. Generally, models with lower valid loss are inclined to have higher BLEU. Our results indicate that this is not necessarily the case: sentbeam converges to +0.82 the BLEU on validation set with -13 valid loss compared with the wordbeam method. Kim and Rush (2016) claims a similar observation in data distillation for NMT and provides an explanation that student distributions are more peaked for sentence-level models. This is indeed the case in our result: on Germany-French translation task the argmax for the sent-beam student model (on average) accounts approximately for 3.49% of the total probability mass, while the corresponding number is 1.25% for the wordbeam student model and 2.60% for the teacher model. However, argmax for the sentence-level model in (Kim and Rush, 2016) comprises around 15% output probability, while ours only accounts for 3.49%. It possibly reveals that it is more diffi-\n2Valid loss: the average NLL of sentence pairs on the validation set.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nMethod # Training Sents\n# Para. BLEU\nEs→ En En→ Fr Newstest2012 Newstest2013 Existing zero-resource NMT systems\nCheng et al. (2016)† pivot 6.78M 9.29M - 19.81 - Cheng et al. (2016)† likelihood 6.78M 9.29M 100K 21.48 - Firat et al. (2016b) one-to-one 34.71M 65.77M - 17.59 17.61 Firat et al. (2016b)† many-to-one 34.71M 65.77M - 21.33 21.19\nOur zero-resource NMT systems word-sampling 6.78M 9.29M - 25.26 25.06\nTable 6: Comparison with previous work on Spanish-French translation in zero resource scenario over WMT corpus. The BLEU scores are case sensitive. †: the pivot path involved in the translation path during testing.\ncult to model around the teacher’s mode for crosstask teaching than in data distillation."
    }, {
      "heading" : "4.3.4 Comparison with Standard NMT",
      "text" : "Table 5 gives the comparison on German-French zero-resource translation between word-sampling approach and standard NMT model trained by MLE. Our proposed method even slightly outperforms standard NMT system trained with parallel source-to-target corpus when this corpus contains similar number of sentences as source-topivot corpus, demonstrating that the effectiveness of our methods to learn from teacher model. This phenomenon can be explained by that the total size of corpus involved in our method is twice that of standard MLE. The translation knowledge contained in other language pairs benefit source-totarget translation, as observed in multilingual scenario (Johnson et al., 2016; Ha et al., 2016)."
    }, {
      "heading" : "4.3.5 BLEU Score with Low-Resource Source-to-Pivot Corpus",
      "text" : "Our proposed method can also be applied to zeroresource NMT with low source-to-pivot corpus. Specifically, the size of source-pivot corpus is magnitude smaller than that of pivot-target corpus. This setting makes sense in applications. For example, there are significantly fewer Urdu-English corpus available than English-French corpus.\nTo fulfill this task, we combine our best performing word-sampling method with the initialization and parameter freezing strategy proposed in (Zoph et al., 2016). We set the size of DeEn corpus to be 100K and use the same teacher model trained with 900K corpus. Table 7 gives the BLEU score of our method on De-Fr translation compared with three other methods. Noting that our task is much harder than (Zoph et al.,\n2016) (transfer) since they are given parallel DeFr corpus directly. Surprisingly, our method outperforms all other methods. We significantly improve the baseline pivot method by 5.63 BLEU and the state-of-the-art low resource method transfer by 0.56 BLEU. We leave further evaluation of this method for future work. 3"
    }, {
      "heading" : "4.4 Results on the WMT Corpus",
      "text" : "Word-sampling method obtains the best performance in our five proposed approaches according to experiments on the Europarl corpus. To further investigate this approach, we conduct experiment on large scale WMT corpus for es→ fr translation. Table 6 shows the results of our word-sampling method in comparison with other state-of-the-art baselines. Cheng et al.(2016) use the same training&develpment sets and the same\n3We test initialization trick in our original settings and observe that with initialization, the BLEU starts to increase very fast (21.25 Bleu with 10000 iterations while without initialization, the number is 2.5). However the model converges to similar point as without initialization.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n0 3 6 9 1 2 1 53 0\n6 0\n9 0\n1 2 0\n1 5 0\n1 8 0\n2 1 0\n0 3 6 9 1 2 1 50\n5\n1 0\n1 5\n2 0\n2 5 3 0 V a lid L o ss\nI t e r a t i o n s\ns e n t  g r e e d y s e n t  b e a m w o r d  g r e e d y w o r d  b e a m w o r d  s a m p l i n g\n×1 04 ×1 04\nB L\nE U\nI t e r a t i o n s\ns e n t  g r e e d y s e n t  b e a m w o r d  g r e e d y w o r d  b e a m w o r d  s a m p l i n g\nFigure 2: Validation loss and BLEU across iterations of our proposed methods.\npreprocessing as ours. Firat et al.(2016b) uses a much larger training sets. Our method obtains significant improvement over the pivot baseline by 5.45 BLEU on newstest2012 and over many-toone 3.87 BLEU on newstest2013, not to mention that both method are depending on source-pivottarget decoding path."
    }, {
      "heading" : "5 Related Work",
      "text" : "Training NMT system in zero-resource scenario by leveraging some other languages has attracted intensive attention in last year. Firat et al. (2016b) propose an approach which genuinely delivers the multi-way, multilingual NMT model proposed by (Firat et al., 2016a) to zero-resource translation. They use the multi-way NMT model trained by other language pairs to generate pseudo parallel corpus and fine-tune the attention mechanism of the multi-way NMT model to enable zero-resource translation. Several authors propose a universal encoder-decoder network in multilingual scenarios to perform zero-shot learning. This universal model extracts translation knowledge from multiple different languages, making zero-resource translation feasible without directly training (Johnson et al., 2016; Ha et al., 2016).\nBesides multilingual NMT, another import line of research is pivot-based methods. Inspired by the widely used pivot idea in SMT (De Gispert and Marino, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; Kholy et al., 2013), Cheng et al.(2016) propose pivotbased NMT by simultaneously improving source\nto pivot and pivot to target translation quality in order to improve source to target translation quality. Nakayama and Nishida (2016a) achieve zeroresource machine translation by utilizing image as pivot.\nIn (Kim and Rush, 2016), the author first introduce knowledge distilling in neural machine translation. They suggest to generate pseudo corpus to train the student network. Our work is highly related to theirs. However, we focus on zeroresource learning instead of model compression."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we propose a novel framework to transfer the knowledge of the teacher model trained with rich-resource language pair into the student model with zero-resource with the help of source-to-pivot corpus. We introduce sentencelevel and word-level teaching to guide the learning process of the student model. Our experiments on Europarl Corpus and WMT corpus across languages show that our proposed word-level sampling method can significantly outperforms the state-of-the-art pivot-based methods and multilingual methods in terms of both translation quality and decoding efficiency.\nWe also analyze zero-resource translation with low source-to-pivot corpus, and combine our word-level sampling method with initialization and parameter freezing suggested by (Zoph et al., 2016). The experiments on Europarl Corpus show that our approach obtains an significant improvement over pivot method significantly.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "CoRR abs/1409.0473.",
      "citeRegEx" : "Bahdanau et al\\.,? 2014",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Phrase-based statistical machine translation with pivot languages",
      "author" : [ "Nicola Bertoldi", "Madalina Barbaiani", "Marcello Federico", "Roldano Cattoni." ],
      "venue" : "IWSLT .",
      "citeRegEx" : "Bertoldi et al\\.,? 2008",
      "shortCiteRegEx" : "Bertoldi et al\\.",
      "year" : 2008
    }, {
      "title" : "Neural machine translation with pivot languages",
      "author" : [ "Yong Cheng", "Yang Liu", "Qian Yang", "Maosong Sun", "Wei Xu." ],
      "venue" : "CoRR abs/1611.04928.",
      "citeRegEx" : "Cheng et al\\.,? 2016",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2016
    }, {
      "title" : "Machine translation by triangulation: Making effective use of multi-parallel corpora",
      "author" : [ "Trevor Cohn", "Mirella Lapata." ],
      "venue" : "ACL.",
      "citeRegEx" : "Cohn and Lapata.,? 2007",
      "shortCiteRegEx" : "Cohn and Lapata.",
      "year" : 2007
    }, {
      "title" : "Catalanenglish statistical machine translation without parallel corpus: bridging through spanish",
      "author" : [ "Adrià De Gispert", "Jose B Marino." ],
      "venue" : "Proceedings of 5th International Conference on Language Resources and Evaluation (LREC). Citeseer, pages",
      "citeRegEx" : "Gispert and Marino.,? 2006",
      "shortCiteRegEx" : "Gispert and Marino.",
      "year" : 2006
    }, {
      "title" : "Multi-way, multilingual neural machine translation with a shared attention mechanism",
      "author" : [ "Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "HLT-NAACL.",
      "citeRegEx" : "Firat et al\\.,? 2016a",
      "shortCiteRegEx" : "Firat et al\\.",
      "year" : 2016
    }, {
      "title" : "Zero-resource translation with multi-lingual neural machine translation",
      "author" : [ "Orhan Firat", "Baskaran Sankaran", "Yaser Al-Onaizan", "Fatos T. Yarman-Vural", "Kyunghyun Cho." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Firat et al\\.,? 2016b",
      "shortCiteRegEx" : "Firat et al\\.",
      "year" : 2016
    }, {
      "title" : "Toward multilingual neural machine translation with universal encoder and decoder",
      "author" : [ "Thanh-Le Ha", "Jan Niehues", "Alexander H. Waibel." ],
      "venue" : "CoRR abs/1611.04798.",
      "citeRegEx" : "Ha et al\\.,? 2016",
      "shortCiteRegEx" : "Ha et al\\.",
      "year" : 2016
    }, {
      "title" : "On using very large target vocabulary for neural machine translation",
      "author" : [ "Sébastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio." ],
      "venue" : "ACL.",
      "citeRegEx" : "Jean et al\\.,? 2015",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Language independent connectivity strength features for phrase pivot statistical machine translation",
      "author" : [ "Ahmed El Kholy", "Nizar Habash", "Gregor Leusch", "Evgeny Matusov", "Hassan Sawaf" ],
      "venue" : null,
      "citeRegEx" : "Kholy et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kholy et al\\.",
      "year" : 2013
    }, {
      "title" : "Sequencelevel knowledge distillation",
      "author" : [ "Yoon Kim", "Alexander M. Rush." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kim and Rush.,? 2016",
      "shortCiteRegEx" : "Kim and Rush.",
      "year" : 2016
    }, {
      "title" : "Europarl: a parallel corpus for statistical machine translation",
      "author" : [ "Philipp Koehn" ],
      "venue" : null,
      "citeRegEx" : "Koehn.,? \\Q2005\\E",
      "shortCiteRegEx" : "Koehn.",
      "year" : 2005
    }, {
      "title" : "Addressing the rare word problem in neural machine translation",
      "author" : [ "Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba." ],
      "venue" : "ACL.",
      "citeRegEx" : "Luong et al\\.,? 2015",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Zeroresource machine translation by multimodal encoder-decoder network with multimedia pivot",
      "author" : [ "Hideki Nakayama", "Noriki Nishida." ],
      "venue" : "CoRR abs/1611.04503.",
      "citeRegEx" : "Nakayama and Nishida.,? 2016a",
      "shortCiteRegEx" : "Nakayama and Nishida.",
      "year" : 2016
    }, {
      "title" : "Zero-resource machine translation by multimodel encoder-decoder network with multimedia pivot",
      "author" : [ "Hideki Nakayama", "Noriki Nishida." ],
      "venue" : "CoRR abs/1611.04503v1.",
      "citeRegEx" : "Nakayama and Nishida.,? 2016b",
      "shortCiteRegEx" : "Nakayama and Nishida.",
      "year" : 2016
    }, {
      "title" : "Bleu: a method for automatic evaluation of machine translation",
      "author" : [ "Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Papineni et al\\.,? 2002",
      "shortCiteRegEx" : "Papineni et al\\.",
      "year" : 2002
    }, {
      "title" : "Neural machine translation of rare words with subword units",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "CoRR abs/1508.07909.",
      "citeRegEx" : "Sennrich et al\\.,? 2016",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2016
    }, {
      "title" : "Minimum risk training for neural machine translation",
      "author" : [ "Shiqi Shen", "Yong Cheng", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu." ],
      "venue" : "CoRR abs/1512.02433.",
      "citeRegEx" : "Shen et al\\.,? 2016",
      "shortCiteRegEx" : "Shen et al\\.",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "CoRR abs/1409.3215.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "A comparison of pivot methods for phrase-based statistical machine translation",
      "author" : [ "Masao Utiyama", "Hitoshi Isahara." ],
      "venue" : "HLT-NAACL.",
      "citeRegEx" : "Utiyama and Isahara.,? 2007",
      "shortCiteRegEx" : "Utiyama and Isahara.",
      "year" : 2007
    }, {
      "title" : "Pivot language approach for phrase-based statistical machine translation",
      "author" : [ "Hua Wu", "Haifeng Wang." ],
      "venue" : "Machine Translation 21:165–181.",
      "citeRegEx" : "Wu and Wang.,? 2007",
      "shortCiteRegEx" : "Wu and Wang.",
      "year" : 2007
    }, {
      "title" : "Revisiting pivot language approach for machine translation",
      "author" : [ "Hua Wu", "Haifeng Wang." ],
      "venue" : "ACL/IJCNLP.",
      "citeRegEx" : "Wu and Wang.,? 2009",
      "shortCiteRegEx" : "Wu and Wang.",
      "year" : 2009
    }, {
      "title" : "Semisupervised learning for neural machine translation",
      "author" : [ "Cheng Yong", "Wei Xu", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu." ],
      "venue" : "ACL.",
      "citeRegEx" : "Yong et al\\.,? 2016",
      "shortCiteRegEx" : "Yong et al\\.",
      "year" : 2016
    }, {
      "title" : "Using context vectors in improving a machine translation system with bridge language",
      "author" : [ "Samira Tofighi Zahabi", "Somayeh Bakhshaei", "Shahram Khadivi." ],
      "venue" : "ACL.",
      "citeRegEx" : "Zahabi et al\\.,? 2013",
      "shortCiteRegEx" : "Zahabi et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving pivot-based statistical machine translation using random walk",
      "author" : [ "Xiaoning Zhu", "Zhongjun He", "Hua Wu", "Haifeng Wang", "Conghui Zhu", "Tiejun Zhao." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Zhu et al\\.,? 2013",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2013
    }, {
      "title" : "Transfer learning for low-resource neural machine translation",
      "author" : [ "Barret Zoph", "Deniz Yuret", "Jonathan May", "Kevin Knight." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Zoph et al\\.,? 2016",
      "shortCiteRegEx" : "Zoph et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community.",
      "startOffset" : 33,
      "endOffset" : 112
    }, {
      "referenceID" : 19,
      "context" : "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community.",
      "startOffset" : 33,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : "Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community.",
      "startOffset" : 33,
      "endOffset" : 112
    }, {
      "referenceID" : 13,
      "context" : "Although NMT has achieved state-of-theart translation performance on resource-rich language pairs such as English-French and GermanEnglish (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016), it still suffers from the unavailability of large-scale parallel corpora for translating low resource languages.",
      "startOffset" : 139,
      "endOffset" : 195
    }, {
      "referenceID" : 8,
      "context" : "Although NMT has achieved state-of-theart translation performance on resource-rich language pairs such as English-French and GermanEnglish (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016), it still suffers from the unavailability of large-scale parallel corpora for translating low resource languages.",
      "startOffset" : 139,
      "endOffset" : 195
    }, {
      "referenceID" : 7,
      "context" : "Another direction is to develop a universal NMT model in multilingual scenarios (Johnson et al., 2016; Ha et al., 2016).",
      "startOffset" : 80,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "Another direction is to achieve source-to-target NMT without parallel data via a pivot, which is either text (Cheng et al., 2016) or image (Nakayama and Nishida, 2016b).",
      "startOffset" : 109,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : ", 2016) or image (Nakayama and Nishida, 2016b).",
      "startOffset" : 17,
      "endOffset" : 46
    }, {
      "referenceID" : 25,
      "context" : "However, pivot-based approaches usually need to divide the decoding process into two steps, which is not only more computationally expensive, but also potentially suffers from the error propagation problem (Zhu et al., 2013).",
      "startOffset" : 206,
      "endOffset" : 224
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Bahdanau et al., 2014), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community. Although NMT has achieved state-of-theart translation performance on resource-rich language pairs such as English-French and GermanEnglish (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016), it still suffers from the unavailability of large-scale parallel corpora for translating low resource languages. Due to the large parameter space, neural models usually learn poorly from low-count events, resulting in a poor choice for low resource language pairs. Zoph et al. (2016) indicate that NMT obtains much worse translation quality than a statistical machine translation (SMT) system on low-resource languages.",
      "startOffset" : 8,
      "endOffset" : 635
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Bahdanau et al., 2014), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community. Although NMT has achieved state-of-theart translation performance on resource-rich language pairs such as English-French and GermanEnglish (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016), it still suffers from the unavailability of large-scale parallel corpora for translating low resource languages. Due to the large parameter space, neural models usually learn poorly from low-count events, resulting in a poor choice for low resource language pairs. Zoph et al. (2016) indicate that NMT obtains much worse translation quality than a statistical machine translation (SMT) system on low-resource languages. As a result, a number of authors have endeavored to explore methods for translating language pairs without parallel corpora available. These methods can be roughly divided into two broad categories: multilingual and pivot-based. Firat et al. (2016b) present a multi-way, multilingual model with shared attention to achieve zeroresource translation.",
      "startOffset" : 8,
      "endOffset" : 1021
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Bahdanau et al., 2014), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community. Although NMT has achieved state-of-theart translation performance on resource-rich language pairs such as English-French and GermanEnglish (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016), it still suffers from the unavailability of large-scale parallel corpora for translating low resource languages. Due to the large parameter space, neural models usually learn poorly from low-count events, resulting in a poor choice for low resource language pairs. Zoph et al. (2016) indicate that NMT obtains much worse translation quality than a statistical machine translation (SMT) system on low-resource languages. As a result, a number of authors have endeavored to explore methods for translating language pairs without parallel corpora available. These methods can be roughly divided into two broad categories: multilingual and pivot-based. Firat et al. (2016b) present a multi-way, multilingual model with shared attention to achieve zeroresource translation. They fine-tune the attention part using pseudo bilingual sentences for the zeroresource language pair. Another direction is to develop a universal NMT model in multilingual scenarios (Johnson et al., 2016; Ha et al., 2016). They use parallel corpora of multiple languages to train one single model, which is then able to translate a language pair without parallel corpora available. Although these approaches prove to be effective, the combination of multiple languages in modeling and training leads to significantly increased complexity. Another direction is to achieve source-to-target NMT without parallel data via a pivot, which is either text (Cheng et al., 2016) or image (Nakayama and Nishida, 2016b). Cheng et al. (2016) propose a pivot-based method to zero-resource NMT: it first translates the source language to a pivot language, which is then translated to the target language.",
      "startOffset" : 8,
      "endOffset" : 1850
    }, {
      "referenceID" : 0,
      "context" : ", 2014; Bahdanau et al., 2014), which directly models the translation process in an end-to-end way, has attracted intensive attention from the community. Although NMT has achieved state-of-theart translation performance on resource-rich language pairs such as English-French and GermanEnglish (Luong et al., 2015; Jean et al., 2015; Wu et al., 2016), it still suffers from the unavailability of large-scale parallel corpora for translating low resource languages. Due to the large parameter space, neural models usually learn poorly from low-count events, resulting in a poor choice for low resource language pairs. Zoph et al. (2016) indicate that NMT obtains much worse translation quality than a statistical machine translation (SMT) system on low-resource languages. As a result, a number of authors have endeavored to explore methods for translating language pairs without parallel corpora available. These methods can be roughly divided into two broad categories: multilingual and pivot-based. Firat et al. (2016b) present a multi-way, multilingual model with shared attention to achieve zeroresource translation. They fine-tune the attention part using pseudo bilingual sentences for the zeroresource language pair. Another direction is to develop a universal NMT model in multilingual scenarios (Johnson et al., 2016; Ha et al., 2016). They use parallel corpora of multiple languages to train one single model, which is then able to translate a language pair without parallel corpora available. Although these approaches prove to be effective, the combination of multiple languages in modeling and training leads to significantly increased complexity. Another direction is to achieve source-to-target NMT without parallel data via a pivot, which is either text (Cheng et al., 2016) or image (Nakayama and Nishida, 2016b). Cheng et al. (2016) propose a pivot-based method to zero-resource NMT: it first translates the source language to a pivot language, which is then translated to the target language. Nakayama and Nishida (2016b) show that using multimedia information as pivot also significantly benefit zero-resource translation.",
      "startOffset" : 8,
      "endOffset" : 2040
    }, {
      "referenceID" : 2,
      "context" : "As compared with the pivot-based approaches (Cheng et al., 2016; Nakayama and Nishida, 2016b), our method allows direct modeling of the intended NMT model, without the need to divide training or decoding into two steps.",
      "startOffset" : 44,
      "endOffset" : 93
    }, {
      "referenceID" : 15,
      "context" : "As compared with the pivot-based approaches (Cheng et al., 2016; Nakayama and Nishida, 2016b), our method allows direct modeling of the intended NMT model, without the need to divide training or decoding into two steps.",
      "startOffset" : 44,
      "endOffset" : 93
    }, {
      "referenceID" : 19,
      "context" : "Neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2014) advocates the use of neural networks to model the translation process in an end-to-end manner.",
      "startOffset" : 27,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "Neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2014) advocates the use of neural networks to model the translation process in an end-to-end manner.",
      "startOffset" : 27,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : "Zoph et al. (2016) report that NMT obtains much lower BLEU scores than SMT if only small-scale parallel corpora are available.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "As pivotbased methods are agnostic to model structures, they have been adapted to NMT recently (Cheng et al., 2016; Nakayama and Nishida, 2016b).",
      "startOffset" : 95,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : "As pivotbased methods are agnostic to model structures, they have been adapted to NMT recently (Cheng et al., 2016; Nakayama and Nishida, 2016b).",
      "startOffset" : 95,
      "endOffset" : 144
    }, {
      "referenceID" : 2,
      "context" : "Figure 1 illustrates the basic idea of pivot-based approaches to zero-resource NMT (Cheng et al., 2016; Nakayama and Nishida, 2016b).",
      "startOffset" : 83,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "Figure 1 illustrates the basic idea of pivot-based approaches to zero-resource NMT (Cheng et al., 2016; Nakayama and Nishida, 2016b).",
      "startOffset" : 83,
      "endOffset" : 132
    }, {
      "referenceID" : 25,
      "context" : "Despite its simplicity, the above two-step decoding process potentially suffers from the error propagation problem (Zhu et al., 2013): the translation errors made in the first step (i.",
      "startOffset" : 115,
      "endOffset" : 133
    }, {
      "referenceID" : 18,
      "context" : "To address this problem, it is possible to construct a subspace by either sampling (Shen et al., 2016), generating a k-best list (Yong et al.",
      "startOffset" : 83,
      "endOffset" : 102
    }, {
      "referenceID" : 23,
      "context" : ", 2016), generating a k-best list (Yong et al., 2016) or mode approximation (Kim and Rush, 2016).",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 11,
      "context" : ", 2016) or mode approximation (Kim and Rush, 2016).",
      "startOffset" : 30,
      "endOffset" : 50
    }, {
      "referenceID" : 2,
      "context" : "To compare with the pivot-based methods, we use the same dataset as in the paper (Cheng et al., 2016).",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : "The evaluation metric is case-insensitive BLEU (Papineni et al., 2002) as calculated by the multi-bleu.",
      "startOffset" : 47,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "For the Europarl corpus (Koehn, 2005), we evaluate our proposed model on Spanish-French (Es-Fr) and Germany-French (De-Fr) translation tasks in a zero-resource scenario.",
      "startOffset" : 24,
      "endOffset" : 37
    }, {
      "referenceID" : 17,
      "context" : "To deal with out-of-vocabulary words, we also adopt byte pair encoding (BPE) (Sennrich et al., 2016) to split words into sub-words.",
      "startOffset" : 77,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "1 and compare our approach with state-of-the-art multilingual method (Firat et al., 2016b) and pivotbased method (Cheng et al.",
      "startOffset" : 69,
      "endOffset" : 90
    }, {
      "referenceID" : 2,
      "context" : ", 2016b) and pivotbased method (Cheng et al., 2016) with the following variations:",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 11,
      "context" : "Sentence-Level Teaching: for simplicity, we use mode as suggested in (Kim and Rush, 2016) to approximate the target sentence space, which is the result of running beam search on the pivot sentences and taking the highest-scoring target sentence with the teacher model.",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : "1 Comparison with pivot-based methods Table 3 gives BLEU scores on the Europarl corpus of our best performing sentence-level method (sent-beam) and word-level method (wordsampling) compared with pivot-based methods (Cheng et al., 2016).",
      "startOffset" : 215,
      "endOffset" : 235
    }, {
      "referenceID" : 2,
      "context" : "The same data preprocessing are applied in (Cheng et al., 2016) as in our experiments.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 2,
      "context" : "# denotes methods in Cheng et al. (2016).",
      "startOffset" : 21,
      "endOffset" : 41
    }, {
      "referenceID" : 11,
      "context" : "However, argmax for the sentence-level model in (Kim and Rush, 2016) comprises around 15% output probability, while ours only accounts for 3.",
      "startOffset" : 48,
      "endOffset" : 68
    }, {
      "referenceID" : 11,
      "context" : "Kim and Rush (2016) claims a similar observation in data distillation for NMT and provides an explanation that student distributions are more peaked for sentence-level models.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "BLEU Es→ En En→ Fr Newstest2012 Newstest2013 Existing zero-resource NMT systems Cheng et al. (2016)† pivot 6.",
      "startOffset" : 80,
      "endOffset" : 100
    }, {
      "referenceID" : 2,
      "context" : "BLEU Es→ En En→ Fr Newstest2012 Newstest2013 Existing zero-resource NMT systems Cheng et al. (2016)† pivot 6.78M 9.29M - 19.81 Cheng et al. (2016)† likelihood 6.",
      "startOffset" : 80,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "BLEU Es→ En En→ Fr Newstest2012 Newstest2013 Existing zero-resource NMT systems Cheng et al. (2016)† pivot 6.78M 9.29M - 19.81 Cheng et al. (2016)† likelihood 6.78M 9.29M 100K 21.48 Firat et al. (2016b) one-to-one 34.",
      "startOffset" : 80,
      "endOffset" : 203
    }, {
      "referenceID" : 2,
      "context" : "BLEU Es→ En En→ Fr Newstest2012 Newstest2013 Existing zero-resource NMT systems Cheng et al. (2016)† pivot 6.78M 9.29M - 19.81 Cheng et al. (2016)† likelihood 6.78M 9.29M 100K 21.48 Firat et al. (2016b) one-to-one 34.71M 65.77M - 17.59 17.61 Firat et al. (2016b)† many-to-one 34.",
      "startOffset" : 80,
      "endOffset" : 263
    }, {
      "referenceID" : 7,
      "context" : "The translation knowledge contained in other language pairs benefit source-totarget translation, as observed in multilingual scenario (Johnson et al., 2016; Ha et al., 2016).",
      "startOffset" : 134,
      "endOffset" : 173
    }, {
      "referenceID" : 26,
      "context" : "To fulfill this task, we combine our best performing word-sampling method with the initialization and parameter freezing strategy proposed in (Zoph et al., 2016).",
      "startOffset" : 142,
      "endOffset" : 161
    }, {
      "referenceID" : 26,
      "context" : "† denotes the transfer learning method in (Zoph et al., 2016).",
      "startOffset" : 42,
      "endOffset" : 61
    }, {
      "referenceID" : 2,
      "context" : "Cheng et al.(2016) use the same training&develpment sets and the same",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "Firat et al.(2016b) uses a much larger training sets.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 5,
      "context" : "(2016b) propose an approach which genuinely delivers the multi-way, multilingual NMT model proposed by (Firat et al., 2016a) to zero-resource translation.",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 7,
      "context" : "This universal model extracts translation knowledge from multiple different languages, making zero-resource translation feasible without directly training (Johnson et al., 2016; Ha et al., 2016).",
      "startOffset" : 155,
      "endOffset" : 194
    }, {
      "referenceID" : 3,
      "context" : "Inspired by the widely used pivot idea in SMT (De Gispert and Marino, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; Kholy et al., 2013), Cheng et al.",
      "startOffset" : 46,
      "endOffset" : 208
    }, {
      "referenceID" : 20,
      "context" : "Inspired by the widely used pivot idea in SMT (De Gispert and Marino, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; Kholy et al., 2013), Cheng et al.",
      "startOffset" : 46,
      "endOffset" : 208
    }, {
      "referenceID" : 21,
      "context" : "Inspired by the widely used pivot idea in SMT (De Gispert and Marino, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; Kholy et al., 2013), Cheng et al.",
      "startOffset" : 46,
      "endOffset" : 208
    }, {
      "referenceID" : 1,
      "context" : "Inspired by the widely used pivot idea in SMT (De Gispert and Marino, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; Kholy et al., 2013), Cheng et al.",
      "startOffset" : 46,
      "endOffset" : 208
    }, {
      "referenceID" : 24,
      "context" : "Inspired by the widely used pivot idea in SMT (De Gispert and Marino, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; Kholy et al., 2013), Cheng et al.",
      "startOffset" : 46,
      "endOffset" : 208
    }, {
      "referenceID" : 10,
      "context" : "Inspired by the widely used pivot idea in SMT (De Gispert and Marino, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; Kholy et al., 2013), Cheng et al.",
      "startOffset" : 46,
      "endOffset" : 208
    }, {
      "referenceID" : 11,
      "context" : "In (Kim and Rush, 2016), the author first introduce knowledge distilling in neural machine translation.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "Firat et al. (2016b) propose an approach which genuinely delivers the multi-way, multilingual NMT model proposed by (Firat et al.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 1,
      "context" : "Inspired by the widely used pivot idea in SMT (De Gispert and Marino, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; Kholy et al., 2013), Cheng et al.(2016) propose pivotbased NMT by simultaneously improving source to pivot and pivot to target translation quality in order to improve source to target translation quality.",
      "startOffset" : 145,
      "endOffset" : 229
    }, {
      "referenceID" : 1,
      "context" : "Inspired by the widely used pivot idea in SMT (De Gispert and Marino, 2006; Cohn and Lapata, 2007; Utiyama and Isahara, 2007; Wu and Wang, 2007; Bertoldi et al., 2008; Zahabi et al., 2013; Kholy et al., 2013), Cheng et al.(2016) propose pivotbased NMT by simultaneously improving source to pivot and pivot to target translation quality in order to improve source to target translation quality. Nakayama and Nishida (2016a) achieve zeroresource machine translation by utilizing image as pivot.",
      "startOffset" : 145,
      "endOffset" : 423
    }, {
      "referenceID" : 26,
      "context" : "We also analyze zero-resource translation with low source-to-pivot corpus, and combine our word-level sampling method with initialization and parameter freezing suggested by (Zoph et al., 2016).",
      "startOffset" : 174,
      "endOffset" : 193
    } ],
    "year" : 2017,
    "abstractText" : "While end-to-end neural machine translation (NMT) has made remarkable progress recently, it still suffers from the data scarcity problem for low resource language pairs and domains. In this paper, we propose a method for zero-resource NMT by assuming that parallel sentences have close probabilities of generating a sentence in a third language. Based on the assumption, we are able to train a source-to-target NMT model without parallel corpora available (“student”) guided by an existing pivot-to-target NMT model (“teacher”) on a source-pivot parallel corpus. Experimental results show that the proposed method significantly improves over a baseline pivot-based model by +3.0 BLEU points across various language pairs.",
    "creator" : "LaTeX with hyperref package"
  }
}