{
  "name" : "318.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Improved Word Representation Learning with Sememes",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Sememes are defined as minimum semantic units of word meanings, and there exists a limited close set of sememes to compose the semantic meanings of an open set of concepts (i.e. word sense). However, sememes are not explicit for each word. Hence, people manually annotate word sememes and build linguistic common-sense\nknowledge bases. HowNet (Dong and Dong, 2003) is one of such knowledge bases, which annotates each concepts in Chinese with one or more relevant sememes. Different from WordNet (Miller, 1995), the philosophy of HowNet emphasizes the significance of part and attribute represented by sememes. HowNet has been widely utilized in word similarity computation (Liu and Li, 2002) and sentiment analysis (Xianghua et al., 2013), and in section 3.2 we will give detailed introduction on sememes, senses and words in HowNet.\nIn this paper, we aim to incorporate word sememes into word representation learning (WRL) and learn improved word embeddings in a lowdimensional semantic space. WRL is a fundamental and critical step in many NLP tasks such as language modeling (Bengio et al., 2003) and neural machine translation (Sutskever et al., 2014).\nThere have been a lot of researches for learning word representations, among which word2vec (Mikolov et al., 2013) achieves a nice balance between effectiveness and efficiency. In word2vec, each word corresponds to one single embedding, ignoring the polysemy of most words. To address this issue, (Huang et al., 2012) introduces multiprototype model for WRL, conducting unsupervised word sense induction and embeddings according to context clusters. (Chen et al., 2014) further utilizes the synset information in WordNet to instruct word sense representation learning.\nFrom these previous studies we conclude that, word sense disambiguation are critical for WRL, and we believe that the sememe annotation of word senses in HowNet can provide essential semantic regularization for the both tasks. To explore its feasibility, we propose a novel Sememe-Encoded Word Representation Learning (SE-WRL) model, which detects word senses and learns representations simultaneously. More\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nspecifically, this framework regards each word sense as a combination of its sememes, and iteratively performs word sense disambiguation according to their contexts and learn representations of sememes, senses and words by extending Skipgram in word2vec (Mikolov et al., 2013). In this framework, an attention-based method is proposed to automatically select appropriate word senses according to contexts. To take full advantages of sememes, we propose three different learning and attention strategies for SE-WRL.\nIn experiments, we evaluate our framework on two tasks including word similarity and word analogy, and further conduct case studies on sememe, sense and word representations. The evaluation results show that our models outperform other baselines significantly, especially on word analogy. This indicates that our models can automatically detect appropriate word senses according to contexts, and both word sense disambiguation and representation learning can benefit from the sememe annotation in HowNet.\nThe key contributions of this work are concluded as follows: (1) To the best of our knowledge, this is the first work to utilize sememes in HowNet to improve word representation learning. (2) We successfully apply the attention scheme to detect word senses and learn representations according to contexts with the favor of the sememe annotation in HowNet. (3) We conduct extensive experiments and verify the effectiveness of incorporating word sememes for improved WRL."
    }, {
      "heading" : "2 Related Work",
      "text" : ""
    }, {
      "heading" : "2.1 Word Representation",
      "text" : "Recent years have witnessed the great thrive in word representation learning. It is simple and straightforward to represent words using one-hot representations, but it usually struggles with the data sparsity issue and the neglect of semantic relations between words.\nTo address these issues, (Rumelhart et al., 1988) proposes the idea of distributed representation which projects all words into a continuous low-dimensional semantic space, considering each word as a vector. Distributed word representations are powerful and have been widely utilized in many NLP tasks, including neural language models (Bengio et al., 2003; Mikolov et al., 2010), machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), parsing (Chen and Manning, 2014)\nand text classification (Zhang et al., 2015). Word distributed representations are capable of encoding semantic meanings in vector space, serving as the fundamental and essential inputs of many NLP tasks.\nThere are large amounts of efforts devoted to learning better word representations. As the exponential growth of text corpora, model efficiency becomes an important issue. (Mikolov et al., 2013) proposes two models, CBOW and Skipgram, achieving a good balance between effectiveness and efficiency. These models assume that the meanings of words can be well reflected by their contexts, and learn word representations by maximizing the predictive probabilities between words and their contexts. (Pennington et al., 2014) further utilizes matrix factorization on word affinity matrix to learn word representations. However, these models merely arrange only one vector for each word, regardless of the fact that many words have multiple senses. (Huang et al., 2012; Tian et al., 2014) utilize multi-prototype vector models to learn word representations, and build distinct vectors for each word sense. (Neelakantan et al., 2015) presents an extension to Skip-gram model for learning non-parametric multiple embeddings per word. (Rothe and Schütze, 2015) also utilizes an Autoencoder to jointly learn word, sense and synset representations in the same semantic space.\nThis paper, for the first time, jointly learns representations of sememes, senses and words. The sememe annotation in HowNet provides useful semantic regularization for WRL. Moreover, the unified representations incorporated with sememes also provide us more explicit explanations of both word and sense embeddings."
    }, {
      "heading" : "2.2 Word Sense Disambiguation and Representation Learning",
      "text" : "Word sense disambiguation (WSD) aims to computationally identify word senses or meanings in a certain context. There are mainly two approaches for WSD, namely the supervised methods and the knowledge-based methods. Supervised methods usually take the surrounding words or senses as features and use classifiers like SVM for word sense disambiguation (Lee et al., 2004), which are intensively limited to the time-consuming human annotation of training data.\nOn contrary, knowledge-based methods utilize large external knowledge resources such as knowl-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nedge bases or dictionaries to suggest possible senses for a word. (Banerjee and Pedersen, 2002) exploits the rich hierarchy of semantic relations in WordNet (Miller, 1995) for an adapted dictionarybased WSD algorithm. (Bordes et al., 2011) introduces synset information in WordNet to WRL. (Chen et al., 2014) considers synsets in WordNet as different word senses, and jointly conducts word sense disambiguation and word / sense representation learning. (Guo et al., 2014) considers bilingual datasets to learn sense-specific word representations. Moreover, (Jauhar et al., 2015) proposes two approaches to learn sense-specific word representations that are grounded to ontologies. (Pilehvar and Collier, 2016) utilizes personalized PageRank to learn de-conflated semantic representations of words.\nIn this paper, we follow the knowledge-based approach and automatically detect word senses according to the contexts with the favor of sememe information in HowNet. To the best of our knowledge, this is the first attempt to apply attentionbased models to encode sememe information for word representation learning."
    }, {
      "heading" : "3 Methodology",
      "text" : "In this section we present our framework SememeEncoded WRL (SE-WRL) that considers sememe information for word sense disambiguation and representation learning. Specifically, we learn our models on a large-scale text corpus with the semantic regularization of the sememe annotation in HowNet, and obtain sememe, sense and word embeddings for evaluation tasks.\nIn the following sections, we first introduce HowNet and the structures of sememes, senses and words. Then we discuss the conventional WRL model Skip-gram that we utilize for the the sememe-encoded framework. Finally, we propose three sememe-encoded models in details."
    }, {
      "heading" : "3.1 Sememes, Senses and Words in HowNet",
      "text" : "In this section, we first introduce the arrangement of sememes, senses and words in HowNet. HowNet annotates precise senses to each word, and for each sense HowNet annotates the significance of parts and attributes represented by sememes.\nFig. 1 gives an example of sememes, senses and words in HowNet. The first layer represents the word “apple”. The word “apple” actually has two\nmain senses shown on the second layer: one is a sort of juicy fruit (apple), and another is a famous computer brand (Apple brand). The third and following layers are those sememes explaining each sense. For instance, the first sense Apple brand indicates a computer brand, and thus has sememes computer, bring and SpeBrand.\nFrom Fig. 1 we can find that, sememes of many senses in HowNet are annotated with various relations, such as define and modifier, and form complicated hierarchical structures. In this paper, for simplicity we only consider all annotated sememes of each sense as a sememe set without considering their internal structure. HowNet assumes the limited annotated sememes can well represent senses and words in real-world scenario, and thus sememes are expected to be useful for both WSD and WRL.\nWe introduce the notions utilized in the following sections as follows. We define the overall sememe, sense and word sets used in training as X , S and W respectively. For each w ∈ W , there are possible multiple senses s(w)i ∈ S(w) where S(w) represents the sense set ofw. Each sense s(w)i consists of several sememes x(si)j ∈ X (w) i . For each target word w in a sequential plain text, C(w) represents its context word set."
    }, {
      "heading" : "3.2 Conventional Skip-gram Model",
      "text" : "We directly utilize the widely-used model Skipgram to implement our SE-WRL model, because Skip-gram has well balanced effectiveness as well as efficiency (Mikolov et al., 2013). The standard skip-gram model assumes that word embeddings should relate to their context words. It aims at maximizing the predictive probability of context words conditioned on the target word w. For-\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nmally, we utilize a sliding window to select the context word set C(w). For a word sequence H = {w1, · · · , wn}, Skip-gram model intends to maximize:\nL(H) = n−K∑ i=K log Pr(wi−K , · · · , wi+K |wi), (1)\nwhere K is the size of sliding window. Pr(wi−K , · · · , wi+K |wi) represents the predictive probability of context words conditioned on the target word wi, formalized by the following softmax function:\nPr(wi−K , · · · , wi+K |wi) = ∏\nwc∈C(wi)\nPr(wc|wi)\n= ∏\nwc∈C(wi)\nexp(w>c ·wi)∑ w′i∈W exp(w>c ·w′i) ,\n(2)\nin which wc and wi stand for embeddings of context word wc ∈ C(wi) and target word wi respectively. We can also follow the strategies of hierarchical softmax and negative sampling proposed in (Mikolov et al., 2013) to accelerate the calculation of softmax."
    }, {
      "heading" : "3.3 SE-WRL Model",
      "text" : "In this section, we introduce the SE-WRL models with three different strategies to utilize sememe information, including Simple Sememe Aggregation Model (SSA), Sememe Attention over Context Model (SAC) and Sememe Attention over Target Model (SAT)."
    }, {
      "heading" : "3.3.1 Simple Sememe Aggregation Model",
      "text" : "The Simple Sememe Aggregation Model (SSA) is a straightforward idea based on Skip-gram model. For each word, SSA considers all sememes in all senses of the word together, and represents the target word using the average of all its sememe embeddings. Formally, we have:\nw = 1\nm ∑ s (w) i ∈S(w) ∑ x (si) j ∈X (w) i x (si) j , (3)\nwhich means the word embedding of w is composed by the average of all its sememe embeddings. Here, m stands for the overall number of sememes belonging to w.\nThis model simply follows the assumption that, the semantic meaning of a word is composed by\nthe semantic units, i.e., sememes. As compared to the conventional Skip-gram model, since sememes are shared by multiple words, this model can utilize sememe information to encode latent semantic correlations between words. In this case, similar words that share the same sememes may finally obtain similar representations."
    }, {
      "heading" : "3.3.2 Sememe Attention over Context Model",
      "text" : "The SSA Model replaces the target word embedding with the aggregated sememe embeddings to encode sememe information into word representation learning. However, each word in SSA model still has only one single representation in different contexts, which cannot deal with polysemy of most words. It is intuitive that we should construct distinct embeddings for a target word according to specific contexts, with the favor of word sense annotation in HowNet.\nTo address this issue, we come up with the Sememe Attention over Context Model (SAC). SAC utilizes the attention scheme to automatically select appropriate senses for context words according to the target word. That is, SAC conducts word sense disambiguation for context words to learn better representations of target words. The structure of the SAC model is shown in Fig. 2.\nMore specifically, we utilize the original word embedding for target word w, but use sememe embeddings to represent context word wc instead of original context word embeddings. Suppose a word typically demonstrates some specific senses in one sentence. Here we employ the target word embedding as an attention to select the most appropriate senses to make up context word embeddings. We formalize the context word embedding wc as follows:\nwc = |S(wc)|∑ j=1 att(s (wc) j ) · s (wc) j , (4)\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nwhere s(wc)j stands for the j-th sense embedding ofwc, and att(s (wc) j ) represents the attention score of the j-th sense with respect to the target word w, defined as follows:\natt(s (wc) j ) = exp(w · ŝ(wc)j )∑|S(wc)| k=1 exp(w · ŝ (wc) k ) . (5)\nNote that, when calculating attention, we use the average of sememe embeddings to represent each sense s(wc)j :\nŝ (wc) j =\n1\n|X(wc)j | |X(wc)j |∑ k=1 x (sj) k .\n(6)\nThe attention strategy assumes that, the more relevant a context word sense embedding is to the target word w, the more this sense should be considered when building context word embeddings. With the favor of attention scheme, we can represent each context word as a certain distribution over its sense. This can be regarded as soft WSD. As shown in experiments, it will help learn better word representations."
    }, {
      "heading" : "3.3.3 Sememe Attention over Target Model",
      "text" : "The Sememe Attention over Context Model can flexibly select appropriate senses and sememes for context words according to the target word. The process can be also applied to select appropriate senses for the target word, by taking context words as attention. Hence, we propose the Sememe Attention over Target Model (SAT) as shown in Fig. 3.\nWt\nWt-2 Wt-1 Wt+1 Wt+2\ncontextual embedding\natt1 att2 att3\nS1 S2 S3\ncontext\nword\nsense\nsememe\nFigure 3: Sememe Attention over Target Model.\nDifferent from SAC model, SAT learns the original word embeddings for context words, but sememe embeddings for target words. We apply context words as attention over multiple senses of\nthe target word w to build the embedding of w, formalized as follows:\nw = |S(w)|∑ j=1 att(s (w) j ) · s (w) j , (7)\nwhere s(w)j stands for the j-th sense embedding of w, and the context-based attention is defined as follows:\natt(s (w) j ) = exp(w′c · ŝ (w) j )∑|S(w)|\nk=1 exp(w ′ c · ŝ (w) k )\n, (8)\nwhere, similar to Eq. (6), we also use the average of sememe embeddings to represent each sense s (w) j . Here, w ′ c is the context embedding, consisting of a constrained window of word embeddings in C(wi). We have:\nw′c = 1\n2K ′ k=i+K′∑ k=i−K′ wk, k 6= i. (9)\nNote that, since in experiment we find the sense selection of the target word only relies on more limited context words for calculating attention, hence we select a smaller K ′ as compared to K.\nRecall that, SAC only uses one target word as attention to select senses of context words, but SAT use several context words together as attention to select appropriate senses of target words. Hence SAT is expected to conduct more reliable WSD and result in more accurate word representations, which will be explored in experiments."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we evaluate the effectiveness of our SE-WRL models on two tasks including word similarity and word analogy, which are two classical evaluation tasks mainly focusing on evaluating the quality of learned word representations. We also explore the potential of our models in word sense disambiguation with case study, showing the power of our attention-based models."
    }, {
      "heading" : "4.1 Dataset",
      "text" : "We use the webpages in Sogou-T1 as the text corpus to learn WRL models. Sogou-T is provided by a Chinese commercial search engine, which contains 2.7 billion words in total.\n1https://www.sogou.com/labs/resource/ t.php\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nWe also utilize the sememe annotation in HowNet. The number of distinct sememes used in this paper is 1, 889. The average senses for each word is about 2.4, while the average sememes for each sense is about 1.6. Throughout the Sogou-T corpus, we find that 42.2% of words have multiple senses. This indicates the significance of WSD.\nFor evaluation, we choose wordsim-240 and wordsim-2972 to evaluate the performance of word similarity computation. The two datasets both contain frequently-used Chinese word pairs with similarity scores annotated manually. We choose the Chinese Word Analogy dataset proposed by (Chen et al., 2015) to evaluate the performance of word analogy inference, that is, w(“king”) − w(“man”) ' w(“queen”) − w(“woman”)."
    }, {
      "heading" : "4.2 Experimental Settings",
      "text" : "We evaluate three SE-WRL models including SSA, SAC and SAT on all tasks. As for baselines, we consider three conventional WRL models including Skip-gram, CBOW and GloVe. For Skipgram and CBOW, we directly use the code released by Google (Mikolov et al., 2013). GloVe is proposed by (Pennington et al., 2014), which seeks the advantages of the WRL models based on statistics and those based on prediction. Moreover, we implement another baseline Maximum Selection over Target Model (MST) inspired by (Chen et al., 2014). It represents the current word embeddings with only the most probable sense according to the contexts, instead of viewing a word as a certain distribution over all its senses similar to that of SAT.\nFor a fair comparison, we train these models with the same experimental settings and with their best parameters. As for the parameter settings, we set the context window size K = 8 as the upper bound, and during training the window size is dynamically selected ranging from 1 to 8 randomly. We set the dimensions of word, sense and sememe embeddings to be the same 200. For learning rate α, its initial value is 0.025 and will descend through iterations. We set the number of negative samples to be 25. We also set a lower bound of word frequency as 50, and in the training set those words less frequent than this bound will be filtered out. For SAT, we set K ′ = 2.\n2https://github.com/Leonard-Xu/CWE/ tree/master/data"
    }, {
      "heading" : "4.3 Word Similarity",
      "text" : "The task of word similarity aims to evaluate the quality of word representations by comparing the similarity ranks of word pairs computed by WRL models with the ranks given by dataset. WRL models typically compute word similarities according to their distances in the semantic space."
    }, {
      "heading" : "4.3.1 Evaluation Protocol",
      "text" : "In experiments, we choose the cosine similarity between two word embeddings to rank word pairs. For evaluation, we compute the Spearman correlation between the ranks of models and the ranks of human judgements."
    }, {
      "heading" : "4.3.2 Experiment Results",
      "text" : "Table 1 shows the results of these models for word similarity computation. From the results we can observe that:\n(1) Our SAT model outperforms other models, including all baselines, on both two test sets. This indicates that, by utilizing sememe annotation properly, our model can better capture the semantic relations of words, and learn more accurate word embeddings.\n(2) The SSA model represents a word with the average of its sememe embeddings. In general, SSA model performs slightly better than baselines, which tentatively proves that sememe information is helpful. The reason is that words which share common sememe embeddings will benefit from each other. Especially, those words with lower frequency, which cannot be learned sufficiently using conventional WRL models, in contrast can obtain better word embeddings from SSA simply because their sememe embeddings can be trained sufficiently through other words.\n(3) The SAT model performs much better than SSA and SAC. This indicates that SAT can obtain more precise sense distribution of a word. The reason has been mentioned above that, different from\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nModel Accuracy Mean RankCapital City Relationship All Capital City Relationship All\nCBOW 49.8 85.7 86.0 64.2 36.98 1.23 62.64 37.62 GloVe 57.3 74.3 81.6 65.8 19.09 1.71 3.58 12.63 Skip-gram 66.8 93.7 76.8 73.4 137.19 1.07 2.95 83.51 MST 65.7 95.4 82.7 74.5 50.29 1.05 2.48 31.05\nSSA 62.3 93.7 81.6 71.9 45.74 1.06 3.33 28.52 SAC 61.6 95.4 77.9 70.8 19.08 1.02 2.18 12.18 SAT 83.2 98.9 82.4 85.3 14.42 1.01 2.63 9.48\nTable 2: Evaluation results of word analogy inference.\nSAC using only one target word as attention for WSD, SAT adopts richer contextual information as attention for WSD.\n(4) SAT works better than MST, and we can conclude that, a soft disambiguation over senses prevents inevitable errors when selecting only one most-probable sense. The result makes sense because for many words, their various senses are not always completely different from each other, but share some common elements. In some contexts, a single sense may not convey the exact meaning of this word."
    }, {
      "heading" : "4.4 Word Analogy",
      "text" : "Word analogy inference is another widely-used task to evaluate the quality of WRL models (Mikolov et al., 2013)."
    }, {
      "heading" : "4.4.1 Evaluation Protocol",
      "text" : "The dataset proposed by (Chen et al., 2015) consists of 1, 124 analogies, which contains three analogy types: (1) capitals of countries (Capital), 677 groups; (2) states/provinces of cities (City), 175 groups; (3) family words (Relationship), 272 groups. Given an analogy group of words (w1, w2, w3, w4), WRL models usually get w2−w1+w3 equal to w4. Hence for word analogy inference, we suppose w4 is missing, and WRL models will rank all candidate words according to their scores as follows:\nR(w) = cos(w2 −w1 +w3,w), (10)\nand select the top-ranked word as the answer. For word analogy inference, we consider two evaluation metrics: (1) Accuracy. For each analogy group, a WRL model selects the top-ranked word w = argmaxw R(w), which is judged as positive if w = w4. The percentage of positive samples is regarded as the accuracy score for this WRL model. (2) Mean Rank. For each analogy group, a WRL model will assign a rank for\nthe gold standard word w4 according to the scores computed by Eq. (10). We use the mean rank of all gold standard words as the evaluation metric."
    }, {
      "heading" : "4.4.2 Experiment Results",
      "text" : "Table 2 shows the evaluation results of these models for word analogy inference. From the table, we can observe that:\n(1) The SAT model performs best among all models, and the superiority is more significant than that on word similarity computation. This indicates that SAT will enhance the modeling of implicit relations between word embeddings in the semantic space. The reason is that sememes annotated to word senses have encoded these word relations. For example, capital and Cuba are two sememes of the word “Havana”, which provide explicit semantic relations between the words “Cuba” and “Havana”.\n(2) The SAT model does well on both classes of Capital and City, because some words in these classes have low frequencies, while their sememes occur so many times that sememe embeddings can be learned sufficiently. With these sememe embeddings, these low-frequent words can be learned more efficiently by SAT.\n(3) It seems that CBOW works better than SAT on Relationship class. Whereas for the mean rank, CBOW gets the worst results, which indicates the performance of CBOW is unstable. On the contrary, although the accuracy of SAT is a bit lower than that of CBOW, SAT seldom gives outrageous prediction. In most wrong cases, SAT predicts the word “grandfather” instead of “grandmother”, which is not completely non-sense, because in HowNet the words “grandmother”, “grandfather”, “grandma” and some other similar words share four common sememes while only one sememe of them are different. These similar sememes make the attention process less discriminative with each other. But for the wrong cases\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nWord: °J(“Apple brand/apple”) sense1: Apple brand (computer, PatternValue, able, bring, SpeBrand) sense2: duct (fruit)\n°J kJ¥ {¡£Apple is always famous as the king of fruits¤ Apple brand: 0.28 apple: 0.72 °J>MÃ{ ~éÄ£The Apple brand computer can not startup\nnormally¤ Apple brand: 0.87 apple: 0.13\nWord: *Ñ(“proliferate/metastasize”) sense1: proliferate (disperse) sense2: metastasize (disperse, disease)\n¼ *Ñ£Prevent epidemic from metastasizing¤ proliferate: 0.06 metastasize: 0.94 Ø*ÑØÉì^ £Treaty on the Non-Proliferation of Nuclear\nWeapons¤ proliferate: 0.68 metastasize: 0.32\nWord: èÎ(“contingent/troops”) sense1: contingent (community) sense2: troops (army)\nl|èÎ?\\1 ãìNm£Eight contingents enter the second stage of team competition¤\ncontingent: 0.90 troops: 0.10\núSÄ èÎ| ï £Construct the organization of public security’s troops in grass-roots unit¤\ncontingent: 0.15 troops: 0.85\nTable 3: Examples of sememes, senses and words in context with attention.\nof CBOW, we find that many mistakes are about words with low frequencies, such as “stepdaughter” which occurs merely for 358 times. With the help of sememes, these low-frequency words arise no issue for SAT."
    }, {
      "heading" : "4.5 Case study",
      "text" : "The above experiments verify the effectiveness of our models for WRL. Here we show some examples of sememes, senses and words for case study."
    }, {
      "heading" : "4.5.1 Word Sense Disambiguation",
      "text" : "In order to demonstrate the validity of Sememe Attention, we select three attention results in training set, as shown in Table 3. In this table, the first rows of three examples are word-sense-sememe structures of each word. For instance, in the third example, the word has two senses, contingent and troops; contingent has one sememe community, while troops has one sememe army. The three examples all indicate that our models can estimate appropriate distributions of senses for a word given a context."
    }, {
      "heading" : "4.5.2 Effect of Context Words for Attention",
      "text" : "We demonstrate the effect of context words for attention in Table. 4. The word “Havana” consists of four sememes, among which two sememes capital and Cuba describe distinct attributes of the word from different aspects.\nHere, we list three different context words “Cuba”, “Russia” and “cigar”. Given the context word “Cuba”, both sememes get high weights, indicating their contributions to the meaning of “Havana” in this context. The context word “Russia” is more relevant to the sememe capital. When the context word is “cigar”, the sememe Cuba has more\ninfluence, because cigar is a famous specialty of Cuba. From these examples, we can conclude that our Sememe Attention can accurately capture the word meanings in complicated contexts."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "In this paper, we propose a novel method to model sememe information for learning better word representations. Specifically, we utilize sememe information to represent various senses of each word, and propose Sememe Attention to automatically select appropriate senses in contexts. We evaluate our models on word similarity and word analogy, and results show the advantages of our Sememe-Encoded WRL models. We also analyze several cases in WSD and WRL, which confirms our models are capable of selecting appropriate word senses with the favor of sememe attention.\nWe will explore the following research directions in future: (1) The sememe information in HowNet is annotated with hierarchical structure and relations, which have not been considered in our framework. We will explore to utilize these annotations for better WRL. (2) We believe the idea of sememes is universal and could be wellfunctioned beyond languages. We will explore the effectiveness of sememe information for WRL in other languages.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "An adapted lesk algorithm for word sense disambiguation using wordnet",
      "author" : [ "Satanjeev Banerjee", "Ted Pedersen." ],
      "venue" : "Proceedings of CICLing. pages 136–145.",
      "citeRegEx" : "Banerjee and Pedersen.,? 2002",
      "shortCiteRegEx" : "Banerjee and Pedersen.",
      "year" : 2002
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Yoshua Bengio", "Réjean Ducharme", "Pascal Vincent", "Christian Jauvin." ],
      "venue" : "JMLR 3:1137–1155.",
      "citeRegEx" : "Bengio et al\\.,? 2003",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning structured embeddings of knowledge bases",
      "author" : [ "Antoine Bordes", "Jason Weston", "Ronan Collobert", "Yoshua Bengio." ],
      "venue" : "Conference on Artificial Intelligence.",
      "citeRegEx" : "Bordes et al\\.,? 2011",
      "shortCiteRegEx" : "Bordes et al\\.",
      "year" : 2011
    }, {
      "title" : "A fast and accurate dependency parser using neural networks",
      "author" : [ "Danqi Chen", "Christopher D Manning." ],
      "venue" : "Proceedings of EMNLP. pages 740–750.",
      "citeRegEx" : "Chen and Manning.,? 2014",
      "shortCiteRegEx" : "Chen and Manning.",
      "year" : 2014
    }, {
      "title" : "A unified model for word sense representation and disambiguation",
      "author" : [ "Xinxiong Chen", "Zhiyuan Liu", "Maosong Sun." ],
      "venue" : "Proceedings of EMNLP. pages 1025–1035.",
      "citeRegEx" : "Chen et al\\.,? 2014",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2014
    }, {
      "title" : "Joint learning of character and word embeddings",
      "author" : [ "Xinxiong Chen", "Lei Xu", "Zhiyuan Liu", "Maosong Sun", "Huan-Bo Luan." ],
      "venue" : "Proceedings of IJCAI. pages 1236–1242.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Hownet-a hybrid language and knowledge resource",
      "author" : [ "Zhendong Dong", "Qiang Dong." ],
      "venue" : "Proceedings of NLP-KE. IEEE, pages 820–824.",
      "citeRegEx" : "Dong and Dong.,? 2003",
      "shortCiteRegEx" : "Dong and Dong.",
      "year" : 2003
    }, {
      "title" : "Learning sense-specific word embeddings by exploiting bilingual resources",
      "author" : [ "Jiang Guo", "Wanxiang Che", "Haifeng Wang", "Ting Liu." ],
      "venue" : "Proceedings of COLING. pages 497–507.",
      "citeRegEx" : "Guo et al\\.,? 2014",
      "shortCiteRegEx" : "Guo et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving word representations via global context and multiple word prototypes",
      "author" : [ "Eric H Huang", "Richard Socher", "Christopher D Manning", "Andrew Y Ng." ],
      "venue" : "Proceedings of ACL. pages 873–882.",
      "citeRegEx" : "Huang et al\\.,? 2012",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2012
    }, {
      "title" : "Ontologically grounded multi-sense representation learning for semantic vector space models",
      "author" : [ "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy." ],
      "venue" : "Proceedings of NAACL. volume 1.",
      "citeRegEx" : "Jauhar et al\\.,? 2015",
      "shortCiteRegEx" : "Jauhar et al\\.",
      "year" : 2015
    }, {
      "title" : "Supervised word sense disambiguation with support vector machines and multiple knowledge sources",
      "author" : [ "Yoong Keok Lee", "Hwee Tou Ng", "Tee Kiah Chia." ],
      "venue" : "Proceedings of SENSEVAL-3. pages 137–140.",
      "citeRegEx" : "Lee et al\\.,? 2004",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2004
    }, {
      "title" : "Word similarity computing based on how-net",
      "author" : [ "Qun Liu", "Sujian Li." ],
      "venue" : "CLCLP 7(2):59–76.",
      "citeRegEx" : "Liu and Li.,? 2002",
      "shortCiteRegEx" : "Liu and Li.",
      "year" : 2002
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "Proceedings of ICLR.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "Tomas Mikolov", "Martin Karafiát", "Lukas Burget", "Jan Cernockỳ", "Sanjeev Khudanpur." ],
      "venue" : "Interspeech. volume 2, page 3.",
      "citeRegEx" : "Mikolov et al\\.,? 2010",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2010
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "George A Miller." ],
      "venue" : "Communications of the ACM 38(11):39–",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Efficient nonparametric estimation of multiple embeddings per word in vector space",
      "author" : [ "Arvind Neelakantan", "Jeevan Shankar", "Alexandre Passos", "Andrew McCallum." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Neelakantan et al\\.,? 2015",
      "shortCiteRegEx" : "Neelakantan et al\\.",
      "year" : 2015
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D Manning." ],
      "venue" : "Proceedings of EMNLP. volume 14, pages 1532–43.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "De-conflated semantic representations",
      "author" : [ "Mohammad Taher Pilehvar", "Nigel Collier." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Pilehvar and Collier.,? 2016",
      "shortCiteRegEx" : "Pilehvar and Collier.",
      "year" : 2016
    }, {
      "title" : "Autoextend: Extending word embeddings to embeddings for synsets and lexemes",
      "author" : [ "Sascha Rothe", "Hinrich Schütze." ],
      "venue" : "Proceedings of ACL .",
      "citeRegEx" : "Rothe and Schütze.,? 2015",
      "shortCiteRegEx" : "Rothe and Schütze.",
      "year" : 2015
    }, {
      "title" : "Learning representations by backpropagating errors",
      "author" : [ "David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams." ],
      "venue" : "Cognitive modeling 5(3):1.",
      "citeRegEx" : "Rumelhart et al\\.,? 1988",
      "shortCiteRegEx" : "Rumelhart et al\\.",
      "year" : 1988
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V Le." ],
      "venue" : "Proceedings of NIPS. pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "A probabilistic model for learning multi-prototype word embeddings",
      "author" : [ "Fei Tian", "Hanjun Dai", "Jiang Bian", "Bin Gao", "Rui Zhang", "Enhong Chen", "Tie-Yan Liu." ],
      "venue" : "Proceedings of COLING. pages 151–160.",
      "citeRegEx" : "Tian et al\\.,? 2014",
      "shortCiteRegEx" : "Tian et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-aspect sentiment analysis for chinese online social reviews based on topic modeling and hownet lexicon",
      "author" : [ "Fu Xianghua", "Liu Guo", "Guo Yanyan", "Wang Zhiqiang." ],
      "venue" : "Knowledge-Based Systems 37:186–195.",
      "citeRegEx" : "Xianghua et al\\.,? 2013",
      "shortCiteRegEx" : "Xianghua et al\\.",
      "year" : 2013
    }, {
      "title" : "Character-level convolutional networks for text classification",
      "author" : [ "Xiang Zhang", "Junbo Zhao", "Yann LeCun." ],
      "venue" : "Proceedings of NIPS. pages 649–657.",
      "citeRegEx" : "Zhang et al\\.,? 2015",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "HowNet (Dong and Dong, 2003) is one of such knowledge bases, which annotates each concepts in Chinese with one or more relevant sememes.",
      "startOffset" : 7,
      "endOffset" : 28
    }, {
      "referenceID" : 15,
      "context" : "Different from WordNet (Miller, 1995), the philosophy of HowNet emphasizes the significance of part and attribute represented by sememes.",
      "startOffset" : 23,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "HowNet has been widely utilized in word similarity computation (Liu and Li, 2002) and sentiment analysis (Xianghua et al.",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 23,
      "context" : "HowNet has been widely utilized in word similarity computation (Liu and Li, 2002) and sentiment analysis (Xianghua et al., 2013), and in section 3.",
      "startOffset" : 105,
      "endOffset" : 128
    }, {
      "referenceID" : 2,
      "context" : "WRL is a fundamental and critical step in many NLP tasks such as language modeling (Bengio et al., 2003) and neural machine translation (Sutskever et al.",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 21,
      "context" : ", 2003) and neural machine translation (Sutskever et al., 2014).",
      "startOffset" : 39,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "There have been a lot of researches for learning word representations, among which word2vec (Mikolov et al., 2013) achieves a nice balance between effectiveness and efficiency.",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "To address this issue, (Huang et al., 2012) introduces multiprototype model for WRL, conducting unsupervised word sense induction and embeddings according to context clusters.",
      "startOffset" : 23,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "(Chen et al., 2014) further utilizes the synset information in WordNet to instruct word sense representation learning.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : "specifically, this framework regards each word sense as a combination of its sememes, and iteratively performs word sense disambiguation according to their contexts and learn representations of sememes, senses and words by extending Skipgram in word2vec (Mikolov et al., 2013).",
      "startOffset" : 254,
      "endOffset" : 276
    }, {
      "referenceID" : 20,
      "context" : "To address these issues, (Rumelhart et al., 1988) proposes the idea of distributed representation which projects all words into a continuous low-dimensional semantic space, considering each word as a vector.",
      "startOffset" : 25,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "Distributed word representations are powerful and have been widely utilized in many NLP tasks, including neural language models (Bengio et al., 2003; Mikolov et al., 2010), machine translation (Sutskever et al.",
      "startOffset" : 128,
      "endOffset" : 171
    }, {
      "referenceID" : 14,
      "context" : "Distributed word representations are powerful and have been widely utilized in many NLP tasks, including neural language models (Bengio et al., 2003; Mikolov et al., 2010), machine translation (Sutskever et al.",
      "startOffset" : 128,
      "endOffset" : 171
    }, {
      "referenceID" : 21,
      "context" : ", 2010), machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), parsing (Chen and Manning, 2014) and text classification (Zhang et al.",
      "startOffset" : 29,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : ", 2010), machine translation (Sutskever et al., 2014; Bahdanau et al., 2015), parsing (Chen and Manning, 2014) and text classification (Zhang et al.",
      "startOffset" : 29,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : ", 2015), parsing (Chen and Manning, 2014) and text classification (Zhang et al.",
      "startOffset" : 17,
      "endOffset" : 41
    }, {
      "referenceID" : 24,
      "context" : ", 2015), parsing (Chen and Manning, 2014) and text classification (Zhang et al., 2015).",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 13,
      "context" : "(Mikolov et al., 2013) proposes two models, CBOW and Skipgram, achieving a good balance between effectiveness and efficiency.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 17,
      "context" : "(Pennington et al., 2014) further utilizes matrix factorization on word affinity matrix to learn word representations.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 9,
      "context" : "(Huang et al., 2012; Tian et al., 2014) utilize multi-prototype vector models to learn word representations, and build distinct vectors for each word sense.",
      "startOffset" : 0,
      "endOffset" : 39
    }, {
      "referenceID" : 22,
      "context" : "(Huang et al., 2012; Tian et al., 2014) utilize multi-prototype vector models to learn word representations, and build distinct vectors for each word sense.",
      "startOffset" : 0,
      "endOffset" : 39
    }, {
      "referenceID" : 16,
      "context" : "(Neelakantan et al., 2015) presents an extension to Skip-gram model for learning non-parametric multiple embeddings per word.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 19,
      "context" : "(Rothe and Schütze, 2015) also utilizes an Autoencoder to jointly learn word, sense and synset representations in the same semantic space.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 11,
      "context" : "Supervised methods usually take the surrounding words or senses as features and use classifiers like SVM for word sense disambiguation (Lee et al., 2004), which are intensively limited to the time-consuming human annotation of training data.",
      "startOffset" : 135,
      "endOffset" : 153
    }, {
      "referenceID" : 1,
      "context" : "(Banerjee and Pedersen, 2002) exploits the rich hierarchy of semantic relations in WordNet (Miller, 1995) for an adapted dictionarybased WSD algorithm.",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : "(Banerjee and Pedersen, 2002) exploits the rich hierarchy of semantic relations in WordNet (Miller, 1995) for an adapted dictionarybased WSD algorithm.",
      "startOffset" : 91,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : "(Bordes et al., 2011) introduces synset information in WordNet to WRL.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 5,
      "context" : "(Chen et al., 2014) considers synsets in WordNet as different word senses, and jointly conducts word sense disambiguation and word / sense representation learning.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "(Guo et al., 2014) considers bilingual datasets to learn sense-specific word representations.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 10,
      "context" : "Moreover, (Jauhar et al., 2015) proposes two approaches to learn sense-specific word representations that are grounded to ontologies.",
      "startOffset" : 10,
      "endOffset" : 31
    }, {
      "referenceID" : 18,
      "context" : "(Pilehvar and Collier, 2016) utilizes personalized PageRank to learn de-conflated semantic representations of words.",
      "startOffset" : 0,
      "endOffset" : 28
    }, {
      "referenceID" : 13,
      "context" : "2 Conventional Skip-gram Model We directly utilize the widely-used model Skipgram to implement our SE-WRL model, because Skip-gram has well balanced effectiveness as well as efficiency (Mikolov et al., 2013).",
      "startOffset" : 185,
      "endOffset" : 207
    }, {
      "referenceID" : 13,
      "context" : "We can also follow the strategies of hierarchical softmax and negative sampling proposed in (Mikolov et al., 2013) to accelerate the calculation of softmax.",
      "startOffset" : 92,
      "endOffset" : 114
    }, {
      "referenceID" : 6,
      "context" : "We choose the Chinese Word Analogy dataset proposed by (Chen et al., 2015) to evaluate the performance of word analogy inference, that is, w(“king”) − w(“man”) ' w(“queen”) − w(“woman”).",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "For Skipgram and CBOW, we directly use the code released by Google (Mikolov et al., 2013).",
      "startOffset" : 67,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : "GloVe is proposed by (Pennington et al., 2014), which seeks the advantages of the WRL models based on statistics and those based on prediction.",
      "startOffset" : 21,
      "endOffset" : 46
    }, {
      "referenceID" : 5,
      "context" : "Moreover, we implement another baseline Maximum Selection over Target Model (MST) inspired by (Chen et al., 2014).",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : "4 Word Analogy Word analogy inference is another widely-used task to evaluate the quality of WRL models (Mikolov et al., 2013).",
      "startOffset" : 104,
      "endOffset" : 126
    }, {
      "referenceID" : 6,
      "context" : "1 Evaluation Protocol The dataset proposed by (Chen et al., 2015) consists of 1, 124 analogies, which contains three analogy types: (1) capitals of countries (Capital), 677 groups; (2) states/provinces of cities (City), 175 groups; (3) family words (Relationship), 272 groups.",
      "startOffset" : 46,
      "endOffset" : 65
    } ],
    "year" : 2017,
    "abstractText" : "Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed by several sememes. Since sememes are not explicit for each word, people manually annotate word sememes and form linguistic common-sense knowledge bases. In this paper, we present that, word sememe information can improve word representation learning (WRL), which maps words into a low-dimensional semantic space and serves as a fundamental step for many NLP tasks. The key idea is to utilize word sememes to accurately capture exact meanings of a word within specific contexts. More specifically, we follow the framework of Skip-gram and present three sememe-encoded models to learn representations of sememes, senses and words, where we apply the attention scheme to detect word senses in various contexts. We conduct experiments on two tasks including word similarity and word analogy, and our models significantly outperform baselines. The results indicate that WRL can benefit from sememes via the attention scheme, and also confirm our models being capable of properly modeling sememe information.",
    "creator" : "LaTeX with hyperref package"
  }
}