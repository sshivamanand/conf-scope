{
  "name" : "97.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Educational advisory body to the Japanese government has decided that writing tests will be introduced into the new national center test for university entrance examinations, as announced in a final report (MEXT, 2016) at the high school and university articulation meeting by the Ministry of Education, Culture, Sports, Science and Technology. The use of AI-based computers was proposed to stabilize the test scores efficiently. The required\ntype of writing test is a short-answer test, where a correct answer is expected to exist. Therefore, the test is scored by judging agreement of the meaning with the correct answer. Another type of writing test is essay writing, where a correct answer does not exist. The written answers are evaluated based on the rhetoric, the connection expressions, and the content.\nBecause short-answer scoring involves technical difficulty, the number of characters is restricted to 80 characters at most from dozens of characters. Two characters in Japanese are generally equivalent to one word in English. A short-answer test is widely considered to be more authentic and reliable for measuring ability compared with a multiple-choice test. If technical problems related to the short-answer test are solved, the potential demand for its use, as well as that for the national center test, will be enormous. Many systems for evaluating essays have been developed and offered in the United States (Shermis and Burstein, 2013). The authors’ group also developed the first and most well-known Japanese automated essay scoring system named Jess (Ishioka and Kameda, 2006), and it is in practical use now.\nWhile a short-answer scoring system has been developed because of its importance, various technical problems remain unsolved. New York University (NYU) and the Educational Testing Service (ETS) developed the first automated scoring tools in this field; they evaluated the NYU online program (Vigilante, 1999). Leacock and Chodorow (2003) reported the latest specifications of the c-rater developed by ETS. Pulman and Sukkarieh (2005) tried to generate several sentences having the same meaning as the correct answer sentence using the natural language technique of information extraction. However, the concordance rate with human examiners was found to be small and impractical.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nThe key technology to solve the short answer scoring seems to be recognizing textual entailment. The National Institute of Informatics (NII) in Japan has promoted the “Todai robot project” to solve multiple choice tests of the Japanese national center test for university entrance examinations (NCTUEE) using this technology with knowledge resources from many textbooks and Wikipedia. They found that perfect recognition of textural entailment or correct understanding of the meaning is difficult at present (NII, 2013). To combine several methods and patch them ad hoc is the most that can be done if people feigned to answer. We have also tried to acquire the basic technology and to produce an automated written short-answer system based on our scientific research conducted in 2014–2016, but we reached the limit of the system.\nTherefore, we thought of a support system for short written tests where a human rater can correct the automated score by referring to the original scores. When the human rater agrees with the result of the automated score, he/she can just approve the score indicated by default and can produce the corresponding mark. We chose to leave room for human raters to overwrite it without making it a perfect automated scoring system.\nThe new examination, which was created by the NCTUEE, will utilize the written test for Japanese literature, whose scoring seems to be more difficult than the examinations of science and social studies, which are prepared basically using the facts written in their respective textbooks. The written test for Japanese literature needs reading skills rather than skills of information processing and pattern matching. In this test in particular, we have to detect the semantic difference in the compared written answers with the model sentences, though almost no difference exists in the vocabulary used. Therefore, we tried to tackle the scoring of short sentences in social studies, where precise judgments are less needed.\nIn section 2, we indicate the test items and the model answers used in a trial examination for university entrance examinations. In section 3, we show the specifications of our proposed system. In section 4, we present our evaluation of the performance on eight tests of social studies. Section 5 concludes with a summary."
    }, {
      "heading" : "2 Test items used in a trial examination",
      "text" : "We assigned a theme in three subjects of world history, Japanese history, and geography of the “Gakken nation-wide trial examination” in fiscal year 2015. The world history test set includes four written test items and two test items each for geography and Japanese history; the total is 8 test items.\nTable 1 shows the “content” asked and the “correct answer,” which are given to test examinees in a distributed booklet of “test answers and explanations.”\nHerein, I comment a little about the correct answer of World history B2 #3. “Jizya” was the capitation imposed on non-Muslims as compensation required for their beliefs in the Islamic world; when indigenous people in a place of conquest converted to Islam, they were not taxed originally. However, “kharaj” was a land tax, and it was imposed on those who possessed land, regardless of whether or not they were Muslim.\nHowever, even if indigenous people converted to Islam in the time of Umayyad, “Jizya” was still imposed, and even if land was possessed, they were exempted from “kharaj.” The Abbasid dynasty returned it to the original state (Table 2). This test item asks about the content, and the recognizing correct meaning is necessary for getting scores.\nTable 2: Tax system changes from Umayyad to Abbasid dynasty\nMuslims non-Muslims Jizya tax → tax exempt tax kharaj tax tax exempt → tax"
    }, {
      "heading" : "3 Specifications of the scoring support system",
      "text" : ""
    }, {
      "heading" : "3.1 Outline",
      "text" : "Our system is for automated scoring and for supporting human raters. The approach functions as follows.\n1. A system automatically judges each answer posed on whether or not its prepared key phrases agree with those of the model answer using the “scoring criteria” from a surfacelike point of view.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nTable 1: The content and the correct answer examples of written test items\nSubject Test Item #\n(Allotment) Content and the correct answer\nWorld history B2\n#1 (3 pt.) [content] Ancient Greece: Solon’s politics of assets [correct answer] A citizen’s right to vote was set according to oneEfs political classes, which depended on his or her ownership of properties. (17 characters in Japanese)\n#3 (5 pt.) [content] Islam: Tax system in the Abbasid dynasty [correct answer] When indigenous people in a place of conquest were Muslim, they were exempted from “Jizya.” When Arabs had land in a place of conquest, “kharaj” was imposed. (60 chars.)\nJapanese history B1\n#2 (3 pt.) [content] Peace treaty of the Russo-Japanese War; Change in the territory [correct answer] The south in Sakhalin was ceded to Japan from Russia. (22 chars) #4 (5 pt.) [content] Dissolution of financial giants after World War II [correct answer] A holding company, a cartel, and a trust were prohibited by the Antimonopoly Act, and a huge monopoly was divided by the excessive economic power decentralization law. (59 chars)\nJapanese history B2\n#1 (3pt.) [content] The Emperor Genmei: Tax burden in the ordinance system [correct answer] A certain amount of cloth is offered instead of labor for capital[m9]. (20 chars) #3 (6 pt.) [content] Kamakura era second half: Commercial activities in the city [correct answer] The carrier, called a ”Toi,” and the usury person, called a ”Kariage,” appeared, and an exchange settled in the bill was started instead of sending money. (59 chars)\nGeometry B #1 (3 pt.) [content] The genesis of the Namib desert [correct answer] The cold current that flows through an offing[m10] and a medium latitude high-pressure area (19 chars)\n#4 (6 pt.) [content] The population of the world: A comparison in India and China [correct answer] The birthrate in India is higher than that of China, and the population growth is remarkable because India failed in its attempts at strict birth control, whereas China conducted a one-child policy. (59 chars)\nc⃝Gakken Holdings\n2. The system gives not only a temporary score based on the criterion-based judgment but also a prediction score offered by machine learning based on the understanding of other human raters or supervised data. A certain degree of semantic meaning is also used.\n3. A human rater can certify the prediction score by which a system presents this information as reference. He or she can correct this and overwrite based on his/her judgment.\nTo reduce the time and effort, the system precision should possess a certain degree of fitness with human ratings; more than 80% of the precision is desirable for tentative targets.\nThe flowchart of our system is as shown in figure 1.\n(a) Before scoring, we collected a lot of score data from various human raters and performed a machine learning of “Random Forests” (Breiman, 2001). The degree of fitness with the scoring guideline is also necessary. On the basis of these learning results, we set up a scoring engine to return the scores for new answers.\n(b) The system generates a scoring screen written in the Hyper Text Markup Language.\n(c) A user or human rater opens a scoring screen of (b) using a web browser on his/her terminal machine. Then, a CGI program is activated. The recommended value as a result of the scoring engine of (a) is indicated here. The scoring result is stocked in a file or a database. The user repeats this mark operation."
    }, {
      "heading" : "3.2 Scoring Screen",
      "text" : "Figure 2 shows a screen shot of our prototype system. “The answer sentence that should be scored” (in red ink) is located in the upper part of the system; the middle part has some scoring criteria such as “synonyms and permitted different transcriptions,” “model or correct answers that warrant a full mark,” “partial phrases that warrant partial scores,” and “mandatory phrases.” For the “model answer” and “partial correct phrases,” the system judges the degree of fitness with the answer sentence to be scored; the system also judges whether or not the answer sentence includes “mandatory phrases,” whether or not it is meaningfully composed, and whether or not it exceeds the character limit; if the answer should be written as a noun or noun phrase, the system judges whether or not it matches the specified “type” format. These judg-\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nWritten answers\n(c)\nTerminal\nScoring\nResults Display the Score\nRandomForests Machine Learning\nScoring Criteria\nScoring Screen （HTML）\nScoring Engine\nWeb\nCGI\n(a)\n(b)\nFigure 1: Flowchart of the system\nments are either yes or no, and toggle buttons are used. A human rater reviews these judgments and revises them if necessary.\nTentative scores located in the lower part are based on the aforementioned alternative judgment. The right-hand window is to determine the final score. The initial mark is settled by which predictive probability based on the past learned results gives the maximum. The probability values are also indicated.\nWhen no learning data exist, that is to say, when no pre-scored data about the relevant test item exist, the message to that effect is shown in the top windows: no probability and no initial mark are naturally determined."
    }, {
      "heading" : "3.3 Difference between the tentative score and the mechanical prediction score",
      "text" : "The example indicated by figure 2 is a case of World history B2 #3. The answer indicated here is as follows: Indigenous people in a place of conquest originally had both “Jizya” and “kharaj” imposed on them, but only “kharaj,” like it was with Arabs, later came to be imposed on them. Compared with the model answer of “Even when indigenous people in a place of conquest were Muslim, they were exempted from Jizya. When an Arab had land in a place of conquest, kharaj was imposed,” we found that the appearance of written words was similar, but the apparent meaning of the sentences was quite different. Therefore,\nthe system gives a score of 4 points of the tentative score (5 points of allotment) determined by buttons checked based on agreement of surfacelike words and phrases that appeared; but the mechanical prediction score is 0 points, and it takes account of other elements besides the surface-like side. The prediction probability of the recommended score is 0.79. It shows that the effect of the machine learning is functioning appropriately."
    }, {
      "heading" : "3.4 Automatic screen creation from a scoring criterion file",
      "text" : "Our system is a Web application. Thus, the screen indicated by figure 2 is generated by HyperText Markup Language. We built the mechanism to make this HTML file automatically from a plain scoring criterion file that a computer beginner can handle.\nFigure 3 is a plain original file that makes a screen like the one in figure 2. Two or three elements are set for criteria. In order, the label, allotment of points, and correspondence are located. The tab is the delimiter.\nSynonyms and different transcriptions are recorded in “syno,” which appeared in “gold” as a model answer and in “part” as a partially correct phrase. “Syno” is not always limited to a definite lexical meaning. When it has semantically the same meaning, it is also permitted. “Part” includes two types; one is possible to add to a partial point, and the other is for which a maximum is taken. If there are multiple same labels (for example, part1), we use the maximum of the points; different labels (for example, part 1 and part 2) can add the allotted points. “Lack” is a mandatory phrase; if no phrases exist, the point is deducted. A comma can be used for the meaning of “both.” “Vol” shows the number of characters available. “None” shows a nonsense sentence, and “goji” shows a wrong word such as kanji that does not exist. Minus points indicate points to be deducted.\nWe use “fitness” as the degree of the relationship between the written answer and “model answer” designated in “gold” or “partial correct phrases” in “part.” We define this as the harmonic mean of two kinds of relationships: one is the degree of the reference during the sentence keywords from the viewpoint of a written answer; the other is that from a model answer. These relationships are just like precision and recall often used in in-\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nFigure 2: Short-answer scoring and support system screen (In case of world history B2 #3)\nsyno jizya jizyah syno Muslim Islam syno \"Indigenous people\" \"Different ethnic groups\" gold 5 \"Even when indigenous people in a place of conquest were Muslim, they were exempted from Jizya. Even when an Arab had land in a place of conquest, kharaj was imposed.\" part1 2 \"Even when indigenous people in a place of conquest were Muslim, they were exempted from Jizya.\" part2 2 \"Even when an Arab had land in a place of conquest, kharaj was imposed.\" lack1 -1 \"had land\" lack2 -2 jizya lack2 -2 kharaj lack2 -5 jizya,kharaj vol -5 60-40 nons -5 goji -1\nFigure 3: Scoring criterion file (labels, allotment of points, and correspondences are tab delimited.)\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nformation retrieval, e.g., a Google search. This harmonic mean or “fitness” is called an F-measure taking a float number from 0 to 1. Our system rounds this to either 0 or 1 as a toggle button occurrence, and it shows a non-rounded value as a reference for the user."
    }, {
      "heading" : "4 Performance Evaluation",
      "text" : ""
    }, {
      "heading" : "4.1 Evaluation of the classification",
      "text" : "We built a prediction model using machine learning of random forests (RFs) V4.1 (Liaw and Wiener, 2015; Breiman and Cutler, 2004). The predictors include not only the degree of fitness indicated in Figure 3 but also semantic (cosine) similarity between the answer, model answer, and test item sentences. The reasons for using the RFs in the methods of many machine learning techniques are as follows:\n1. When using many predictor variables, the classification often functions effectively.\n2. The degree of contributions can be estimated to determine effective predictor variables quantitatively in the classification.\n3. RFs are suitable for this because test scoring requires multiple classifications with values of 0–3 or 0–6.\nFor eight test items indicated in table 1, we compared human ratings with the estimate based prediction model. The best rate that that a prediction and a correct answer were identical is 78% in the cases of Geometry B #4. The worst rate is 43% in the cases of Japanese History B1 #2, which was surprisingly low. This is because about 80% of examinees got zero scores; thus, machine learning does not work well. We omitted cross matrices between human ratings and the estimate because of space limitations.\nTable 3 shows the probability in which the differences between the scores were within one point. These values consist of 71–95% removing the case of world history B2 #3, which is necessary for correct understanding of the meaning. It shows the performance of the classification was in the level available.\nRFs do not need cross validation to calculate the error rate. That is, a separated test set is not necessary to determine it (Breiman, 2001). The error rate can be estimated internally during a run. In RFs, each tree is constructed using a different\nTable 3: Probability in which the estimates differed from the human ratings within one point\nItem # Prob. Item # Prob. World History B2 #1 0.75 Jpn History B2 #1 0.86 World History B2 #3 0.48 Jpn History B2 #3 0.71 Jpn History B1 #2 0.76 Geo B #1 0.91 Jpn History B1 #4 0.88 Geo B #4 0.95\nbootstrap sample from two-thirds of the original data. The remaining one-third of cases is used for the test data.\nThe default procedure runs 500 times and forms the final classification tree using the most votes, when using the RandomForest package implemented in R(Liaw and Wiener, 2015). The error rate for the test set can be obtained at the same time. The numerical values shown in Table 3 were obtained using this procedure. The sample sizes ranged in 70–120, depending on the subjects.\nIf we compare the original data with the estimates using the final RF model, the concordance rate will be near 100% because too many predictive variables were compared with the sample size."
    }, {
      "heading" : "4.2 Variables that contribute to the classification",
      "text" : "RFs evaluate the importance of variables in distinction using an index of the Gini coefficient. The bigger the coefficient, the more the classification is affected. Figure 4 shows variables arranged in descending order of the Gini coefficient; the horizontal axis shows their values. Due to space limitations, we show three cases of world history B2 #3, Japanese history B1 #2, and geometry B #4.\nThe three dominant contributions in case of world history B2 #3 are as follows:\nQA sim: The cosine similarity between test item sentences and the answer.\nSA sim 1: The cosine similarity between model answer #1 and the answer.\nsa jpkwrel Fvl gold std01: The F-measure in keywords agreement between model answer #1 and the answer.\nA variable name that starts with a capital letter implies linguistic semantic meaning built by the vocabulary used in Japanese Wikipedia. The typical examples are cosine similarity, precision, recall, and F-measure in the semantic space. Variations that multiply the allotment are also included.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nThe variable names that start with a capital letter are an index showing surface-like lexical relationships. The total number of used variables is from 40 to 70, depending on the size of the scoring criteria.\nWhen the indexes indicated in figure 4 are compared by each test item, the variables that contribute to distinction are not fixed. However, the Fmeasure and cosine similarity between the model (correct) answer and the answer in the semantic space often appear in a higher position. Generally speaking, semantic variables are dominant compared with surficial lexical variables."
    }, {
      "heading" : "4.3 Inspection using agreement of case elements",
      "text" : "The case grammar proposed by Charles J. Fillmore (Fillmore, 1968) is part of a theory that tries to understand sentence meanings by putting a verb (the word with declined or conjugated endings) at the center of the understanding and that analyzes using a combination with deep cases such as the Agent and Object. Deep cases are often not determined easily, so we use surface cases instead of them. We tried a check on the agreement with case elements of a “model answer” and those of a “written answer.” The mechanism for the inspection had been arranged.\nHowever, almost no agreement of case elements happened; the degree of agreement does not become one of the effective predictors of the classification. The ways in which sentences can be filled out to indicate the same meaning are numerous. Therefore, we did not add case grammar variables on the agreement of the case elements in this experiment."
    }, {
      "heading" : "5 Conclusion",
      "text" : "Recognizing textual entailment between a model (correct) answer and a written answer is still difficult technically because complicated collation needs to be determined under contractual and semantic levels. Our technique is based on the collation between the keywords in two answers, and it uses both predictors considering superficial and semantic aspects. Therefore, it can be judged as sufficiently realistic for an approach of the first step. A form that entrusts the last judgment to the human is most suitable.\nIn this study, we investigated cases of scoring in social studies (geography and world/Japanese\nsa_jpkwrel_prcxpt_part1_std04 sa_jpkwrel_Fvlxpt_part2_std05 SA_f1_part3 sa_jpkwrel_prcxpt_part2_std05 sa_jpkwrel_prc_part1_std03 sa_jpkwrel_prc_part2_std05 sa_jpkwrel_Fvlxpt_part1_std02 sa_jpkwrel_Fvl_part2_std05 sa_jpkwrel_prcxpt_gold__std01 sa_jpkwrel_Fvl_part1_std02 SA_pr_1 sa_jpkwrel_prcxpt_part1_std03 sa_jpkwrel_rcl_gold__std01 sa_jpkwrel_prc_lack2_std08 sa_jpkwrel_mcc_gold__std01 sa_jpkwrel_rclxpt_gold__std01 sa_jpkwrel_prc_gold__std01 sa_jpkwrel_Fvlxpt_part1_std03 SA_prXpt_1 sa_jpkwrel_Fvl_part1_std04 sa_jpwc_morps sa_jpkwrel_Fvl_part1_std03 sa_jpkwrel_Fvlxpt_part1_std04 SA_f1_1 sa_jpkwrel_Fvlxpt_gold__std01 sa_jpwc_chars sa_jpwc_words sa_jpkwrel_Fvl_gold__std01 SA_sim_1 QA_sim\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n0.0 0.5 1.0 1.5\nWrdHistoryB2_3\nMeanDecreaseGini\nsa_jpkwrel_prcxpt_lack__std05 sa_jpkwrel_rclxpt_gold__std02 sa_jpkwrel_prc_lack__std05 SA_prXpt_2 sa_jpkwrel_rclxpt_lack__std05 sa_jpkwrel_prcxpt_gold__std01 sa_jpkwrel_Fvl_gold__std01 sa_jpkwrel_mcc_gold__std02 SA_pr_2 sa_jpkwrel_prcxpt_gold__std02 sa_jpkwrel_Fvl_part__std03 sa_jpkwrel_mcc_gold__std01 SA_f1_2 sa_jpkwrel_Fvlxpt_lack__std05 sa_jpkwrel_Fvlxpt_gold__std01 sa_jpkwrel_Fvlxpt_gold__std02 SA_prXpt_1 sa_jpkwrel_prc_part__std03 sa_jpkwrel_prcxpt_part__std03 sa_jpwc_chars sa_jpkwrel_prc_gold__std02 sa_jpkwrel_Fvlxpt_part__std03 sa_jpkwrel_Fvl_lack__std05 QA_sim SA_f1_1 sa_jpkwrel_Fvl_gold__std02 sa_jpwc_words sa_jpwc_morps SA_sim_2 SA_sim_1\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nJpnHistoryB1_2\nMeanDecreaseGini\nsa_jpkwrel_prc_part2_std08 SA_f1_part6 sa_jpkwrel_Fvl_part2_std07 sa_jpkwrel_rcl_gold__std01 sa_jpkwrel_prcxpt_part2_std04 sa_jpkwrel_Fvl_part2_std04 sa_jpkwrel_rclxpt_part1_std02 sa_jpkwrel_Fvlxpt_gold__std01 sa_jpkwrel_prc_part2_std04 sa_jpwc_chars sa_jpkwrel_Fvlxpt_part2_std07 SA_f1_part1 sa_jpkwrel_Fvl_part2_std08 sa_jpkwrel_rcl_part1_std02 SA_f1_part7 SA_prXpt_part1 sa_jpkwrel_Fvlxpt_part2_std08 sa_jpkwrel_Fvlxpt_part2_std04 sa_jpwc_morps sa_jpkwrel_Fvl_gold__std01 sa_jpkwrel_prcxpt_gold__std01 SA_pr_part1 sa_jpwc_bnsts QA_sim sa_jpkwrel_prc_gold__std01 SA_sim_1 sa_jpkwrel_prcxpt_part1_std02 sa_jpkwrel_Fvl_part1_std02 sa_jpkwrel_Fvlxpt_part1_std02 sa_jpkwrel_prc_part1_std02\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n0.0 0.2 0.4 0.6 0.8 1.0 1.2\nGeoB_4\nMeanDecreaseGini\nFigure 4: Variables that contribute to distinction (In the case of world history B2 #3, Japanese history B1 #2, and geometry B #4)\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nhistory). Our procedure will be applied to other subjects, such as Japanese literature, when many different transcriptions of a correct answer are prepared. Many different expressions in the same sense can be allowed, especially in Japanese. Our system has a mechanism to choose the biggest score among the same labels, so this can be prepared using the specifications of the current state.\nMoreover, when there is a content-like important word or phrase such as “father’s feeling,” our system can register this type of words as “mandatory” in scoring criteria and can decrease the score by a suitable allotment when it is lacking. This function has already been implemented and will be helpful to understand the semantics.\nHowever, a sufficiently large number of human scores cannot be provided for supervised learning. Indeed, actual written answer scores are often zero because they are illogical or are off-topic. Obtaining suitable and well-balanced score data will be necessary to ensure proper estimation.\nBecause a short written test has been proposed as a new common test for entering Japanese universities, our new scoring and support system is now being considered. We hope this system will provide researchers who study this field and practical businesspeople with useful information and suggestions."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank everybody in the Gakken group for offering the test sets, scoring criteria, and human scores. In particular, Mr. Tsugunao Matsuoka and Ms. Naoko Saigo gave us valuable comments. This project was supported by JSPS KAKENHI Grant Number JP:26350357."
    } ],
    "references" : [ {
      "title" : "Random forests",
      "author" : [ "Leo Breiman." ],
      "venue" : "Machine Learning 45(1):5–32.",
      "citeRegEx" : "Breiman.,? 2001",
      "shortCiteRegEx" : "Breiman.",
      "year" : 2001
    }, {
      "title" : "Random forests",
      "author" : [ "Leo Breiman", "Adele Cutler." ],
      "venue" : "http://www.stat.berkeley.edu/ ̃breiman/ RandomForests/.",
      "citeRegEx" : "Breiman and Cutler.,? 2004",
      "shortCiteRegEx" : "Breiman and Cutler.",
      "year" : 2004
    }, {
      "title" : "The Case for Case, Universals in Linguistic Theory, New York: Holt, Rinehart, and Winston, pages 1–88",
      "author" : [ "Charles J. Fillmore" ],
      "venue" : null,
      "citeRegEx" : "Fillmore.,? \\Q1968\\E",
      "shortCiteRegEx" : "Fillmore.",
      "year" : 1968
    }, {
      "title" : "Automated japanese essay scoring system based on articles written by experts",
      "author" : [ "Tsunenori Ishioka", "Msayuki Kameda." ],
      "venue" : "Proceeding ACL44 Proceedings of the 21st International Conference on Computational Linguistics and the 44th",
      "citeRegEx" : "Ishioka and Kameda.,? 2006",
      "shortCiteRegEx" : "Ishioka and Kameda.",
      "year" : 2006
    }, {
      "title" : "Crater: Automated scoring of short-answer questions",
      "author" : [ "Claudia Leacock", "Martin Chodorow." ],
      "venue" : "Computers and the Humanities 37(4):389– 405. https://doi.org/10.1023/A:1025779619903.",
      "citeRegEx" : "Leacock and Chodorow.,? 2003",
      "shortCiteRegEx" : "Leacock and Chodorow.",
      "year" : 2003
    }, {
      "title" : "Package randomforest",
      "author" : [ "Andy Liaw", "Matthew Wiener." ],
      "venue" : "Breiman and Cutler’s Random Forests for Classification and Regression 4.6-12.",
      "citeRegEx" : "Liaw and Wiener.,? 2015",
      "shortCiteRegEx" : "Liaw and Wiener.",
      "year" : 2015
    }, {
      "title" : "Publishing the final report of high school and university articulation meeting",
      "author" : [ "MEXT." ],
      "venue" : "Ministry of Education, Culture, Sports, Science and Technology in Japan. http://www.mext.go.jp/b menu/ shingi/chousa/shougai/033/toushin/1369233.htm.",
      "citeRegEx" : "MEXT.,? 2016",
      "shortCiteRegEx" : "MEXT.",
      "year" : 2016
    }, {
      "title" : "Itefs ability which understands the meaning to be asked about",
      "author" : [ "NII." ],
      "venue" : "NII Today (60):8–9.",
      "citeRegEx" : "NII.,? 2013",
      "shortCiteRegEx" : "NII.",
      "year" : 2013
    }, {
      "title" : "Automatic short answer marking",
      "author" : [ "Stephen G. Pulman", "Jana Z. Sukkarieh." ],
      "venue" : "EdAppsNLP 05 Proceedings of the second workshop on Building Educational Applications Using NLP. Association for Computational Linguistics, pages 9–16.",
      "citeRegEx" : "Pulman and Sukkarieh.,? 2005",
      "shortCiteRegEx" : "Pulman and Sukkarieh.",
      "year" : 2005
    }, {
      "title" : "Handbook of Automated Essay Evaluation",
      "author" : [ "Mark D. Shermis", "Jill Burstein." ],
      "venue" : "Routledge.",
      "citeRegEx" : "Shermis and Burstein.,? 2013",
      "shortCiteRegEx" : "Shermis and Burstein.",
      "year" : 2013
    }, {
      "title" : "Online computer scoring of constructed-response questions",
      "author" : [ "Richard Vigilante." ],
      "venue" : "Journal of Information Technology 1(2):57–62.",
      "citeRegEx" : "Vigilante.,? 1999",
      "shortCiteRegEx" : "Vigilante.",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Educational advisory body to the Japanese government has decided that writing tests will be introduced into the new national center test for university entrance examinations, as announced in a final report (MEXT, 2016) at the high school and university articulation meeting by the Ministry of Education, Culture, Sports, Science and Technology.",
      "startOffset" : 206,
      "endOffset" : 218
    }, {
      "referenceID" : 9,
      "context" : "Many systems for evaluating essays have been developed and offered in the United States (Shermis and Burstein, 2013).",
      "startOffset" : 88,
      "endOffset" : 116
    }, {
      "referenceID" : 3,
      "context" : "The authors’ group also developed the first and most well-known Japanese automated essay scoring system named Jess (Ishioka and Kameda, 2006), and it is in practical use now.",
      "startOffset" : 115,
      "endOffset" : 141
    }, {
      "referenceID" : 10,
      "context" : "New York University (NYU) and the Educational Testing Service (ETS) developed the first automated scoring tools in this field; they evaluated the NYU online program (Vigilante, 1999).",
      "startOffset" : 165,
      "endOffset" : 182
    }, {
      "referenceID" : 3,
      "context" : "The authors’ group also developed the first and most well-known Japanese automated essay scoring system named Jess (Ishioka and Kameda, 2006), and it is in practical use now. While a short-answer scoring system has been developed because of its importance, various technical problems remain unsolved. New York University (NYU) and the Educational Testing Service (ETS) developed the first automated scoring tools in this field; they evaluated the NYU online program (Vigilante, 1999). Leacock and Chodorow (2003) reported the latest specifications of the c-rater developed by ETS.",
      "startOffset" : 116,
      "endOffset" : 513
    }, {
      "referenceID" : 3,
      "context" : "The authors’ group also developed the first and most well-known Japanese automated essay scoring system named Jess (Ishioka and Kameda, 2006), and it is in practical use now. While a short-answer scoring system has been developed because of its importance, various technical problems remain unsolved. New York University (NYU) and the Educational Testing Service (ETS) developed the first automated scoring tools in this field; they evaluated the NYU online program (Vigilante, 1999). Leacock and Chodorow (2003) reported the latest specifications of the c-rater developed by ETS. Pulman and Sukkarieh (2005) tried to generate several sentences having the same meaning as the correct answer sentence using the natural language technique of information extraction.",
      "startOffset" : 116,
      "endOffset" : 609
    }, {
      "referenceID" : 7,
      "context" : "They found that perfect recognition of textural entailment or correct understanding of the meaning is difficult at present (NII, 2013).",
      "startOffset" : 123,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : "(a) Before scoring, we collected a lot of score data from various human raters and performed a machine learning of “Random Forests” (Breiman, 2001).",
      "startOffset" : 132,
      "endOffset" : 147
    }, {
      "referenceID" : 5,
      "context" : "1 (Liaw and Wiener, 2015; Breiman and Cutler, 2004).",
      "startOffset" : 2,
      "endOffset" : 51
    }, {
      "referenceID" : 1,
      "context" : "1 (Liaw and Wiener, 2015; Breiman and Cutler, 2004).",
      "startOffset" : 2,
      "endOffset" : 51
    }, {
      "referenceID" : 0,
      "context" : "That is, a separated test set is not necessary to determine it (Breiman, 2001).",
      "startOffset" : 63,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "The default procedure runs 500 times and forms the final classification tree using the most votes, when using the RandomForest package implemented in R(Liaw and Wiener, 2015).",
      "startOffset" : 151,
      "endOffset" : 174
    }, {
      "referenceID" : 2,
      "context" : "Fillmore (Fillmore, 1968) is part of a theory that tries to understand sentence meanings by putting a verb (the word with declined or conjugated endings) at the center of the understanding and that analyzes using a combination with deep cases such as the Agent and Object.",
      "startOffset" : 9,
      "endOffset" : 25
    } ],
    "year" : 2017,
    "abstractText" : "We have developed an automated Japanese short-answer scoring and support machine for new National Center written test exams. Our approach is based on the fact that recognizing textual entailment and/or synonymy has been almost impossible for several years. The system generates automated scores on the basis of evaluation criteria or rubrics, and human raters revise them. The system determines semantic similarity between the model answers and the actual written answers as well as a certain degree of semantic identity and implication. Owing to the need for the scoring results to be classified at multiple levels, we use random forests to utilize many predictors effectively rather than use support vector machines. An experimental prototype operates as a web system on a Linux computer. We compared human scores with the automated scores for a case in which 3–6 allotment points were placed in 8 categories of a social studies test as a trial examination. The differences between the scores were within one point for 70–90 percent of the data when high semantic judgment was not needed.",
    "creator" : "LaTeX with hyperref package"
  }
}