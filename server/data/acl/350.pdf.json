{
  "name" : "350.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Event Extraction (EE), a challenging task in Information Extraction, aims at detecting and typing events (Event Detection), and extracting arguments with different roles (Argument Identification) from natural-language texts. For example, in the sentence shown in Figure 1, an EE system is expected to identify an Attack event triggered by threw and extract the corresponding five augments with different roles: Yesterday (Role=Time), demonstrators (Role=Attacker), stones (Role=Instrument), soldiers (Role=Target), and Israeli (Role=Place).\nTo this end, so far most methods (Nguyen et al.,\n2016; Chen et al., 2015; Li et al., 2014; Hong et al., 2011; Ji and Grishman, 2008) usually adopted supervised learning paradigm which relies on elaborate human-annotated data, such as ACE 20051, to train extractors. Although this paradigm was widely studied, existing approaches still suffer from high costs for manually labeling training data and low coverage of predefined event types. In ACE 2005, all 33 event types are manually predefined and the corresponding event information (including triggers, event types, arguments and their roles) are manually annotated only in 599 English documents since the annotation process is extremely expensive. As Figure 2 shown, nearly 60% of event types in ACE 2005 have less than 100 labeled samples and there are even three event types which have less than ten labeled samples. Moreover, those predefined 33 event types are in low coverage for Natural Language Processing (NLP) applications on large-scale data.\nTherefore, for extracting large scale events, especially in open domain scenarios, how to automatically and efficiently generate sufficient training data is an important problem. This paper aims to automatically generate training data for EE, which involves labeling triggers, event types, arguments and their roles. Figure 1 shows an example of labeled sentence. Recent improvements of Distant Supervision (DS) have been proven to be effective to label training data for Relation Extraction (RE), which aims to predict semantic relations between pairs of entities, formulated as (entity1, relation, entity2). And DS for RE as-\n1http://projects.ldc.upenn.edu/ace/\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nACL 2017 Submission 350. Confidential Review Copy. DO NOT DISTRIBUTE.\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nFigure 2: Statistics of ACE 2005 English Data.\nsumes that if two entities have a relationship in a known knowledge base, then all sentences that mention these two entities will express that relationship in some way (Mintz et al., 2009). However, when we use DS for RE to EE, we meet following challenges:\nTriggers are not given out in existing knowledge bases. EE aims to detect an event instance of a specific type and extract their arguments and roles, formulated as (event instance, event type; role1, argument1; role2, argument2; ...; rolen, argumentn), which can be regarded as a kind of multiple or complicated relational data. In Figure 3, the right part shows an example of spouse of relation between Barack Obama and Michelle Obama, where two rectangles represent two entities and the edge connecting them represents their relation. DS for RE uses two entities to automatically label training data; In comparison, the left part in Figure 3 shows a marriage event of Barack Obama and Michelle Obama, where the dash circle represents the marriage event instance of Barack Obama and Michelle Obama, rectangles represent arguments of the event instance, and each edge connecting an argument and the event instance expresses the role of the argument. For example, Barack Obama plays a Spouse role in this marriage event instance. It seems that we could use an event instance and an argument to automatically generate training data for argument identification just like DS for RE. However, an event instance is a virtual node in existing knowledge bases and mentioned implicitly in texts. For example, in Freebase, the aforementioned marriage event instance is represented as m.02nqglv (see details in Section 2). Thus we cannot directly use an event instance and an argument, like m.02nqglv and Barack Obama, to label back in sentences. In ACE event extraction program, an event instance is represented as a trigger word, which is the main word that most clearly represents an event occurrence in sentences, like threw\nMarriageMichelle Obama 10/03/1992 Trinity United Church of Christ Null Spouse Spouse location of ceremony time_from time_to Barack Obama Michelle ObamaSpouse_ofVS An example of marriage event An example of spouse_of relation MarriageMichelle Obama 10/03/1992 Trinity United Church of Christ Null Spouse Spouse location of ceremony time_from time_to Barack Obama Michelle ObamaSpouse_of An example of marriage event An example of spouse_of relation\nMarriageMichelle Obama\n10/03/1992\nTrinity United Church of Christ\nNull\nSpouse Spouse\nlocation of ceremony\ntime_from time_to\nBarack Obama Michelle Obama Spouse_of\nAn example of marriage event An example of spouse_of relation\nMichelle Obama\nFigure 3: A comparison of events and relations.\nin Figure 1. Following ACE, we can use trigger words to represent event instance, like married for people.marriage event instance. Unfortunately, triggers are not given out in existing knowledge bases.\nTo resolve the trigger missing problem mentioned above, we need to discover trigger words before employing distant supervision to automatically label event arguments. Following DS in RE, we could naturally assume that a sentence contains all arguments of an event in the knowledge base tend to express that event, and the verbs occur in these sentences tend to evoke this type of events. However, arguments for a specific event instance are usually mentioned in multiple sentences. Simply employing all arguments in the knowledge base to label back in sentences will generate few sentences as training samples. As shown in Table 1, only 0.02% of instances can find all argument mentions in one sentence.\nEvent Type EI# A# S# education.education 530,538 8 0 film.film crew gig 252,948 3 8 people.marriage 152,276 5 0\n... ... ... ... military.military service 27,933 6 0\nolympics.olympic medal honor 20,790 5 4 sum of the selected 21 events 3,870,492 100 798\nTable 1: Statistics of events in Freebase. EI# denotes number of event instances in Freebase. A# denotes number of arguments for each event types, and S# indicates number of sentences contain all arguments of each event type in Wikipedia.\nTo solve above problems, we propose an approach to automatically generate labeled data for large scale EE by jointly using world knowledge (Freebase) and linguistic knowledge (FrameNet). At first, we put forward an approach to prioritize arguments and select key or representative arguments (see details in Section 3.1) for each event type by using Freebase; Secondly, we merely use key arguments to label events and figure out trigger words; Thirdly, an external linguistic knowledge resource, FrameNet, is employed to filter noisy trigger words and expand more triggers; Af-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nACL 2017 Submission 350. Confidential Review Copy. DO NOT DISTRIBUTE.\nter that, we propose a Soft Distant Supervision (SDS) for EE to automatically label training data, which assumes that any sentence containing all key arguments in Freebase and a corresponding trigger word is likely to express that event in some way, and arguments occurring in that sentence are likely to play the corresponding roles in that event. Finally, we evaluate the quality of the automatically labeled training data by both manual and automatic evaluations. In addition, we employ a CNNbased EE approach with multi-instance learning for the automatically labeled data as a baseline for further research on this data. In summary, the contributions of this paper are as follows:\n• To our knowledge, it is the first work to automatically label data for large scale EE via world knowledge and linguistic knowledge. All the labeled data in this paper have been released and can be downloaded freely2. • We propose an approach to figure out key ar-\nguments of an event by using Freebase, and use them to automatically detect events and corresponding trigger words. Moreover, we employ FrameNet to filter noisy triggers and expand more triggers. • The experimental results show that the qual-\nity of our large scale automatically labeled data is competitive with elaborately humanannotated data. Also, our automatically labeled data can augment traditional humanannotated data, which could significantly improve the extraction performance."
    }, {
      "heading" : "2 Background",
      "text" : "In this paper, we respectively use Freebase as our world knowledge containing event instance and FrameNet as the linguistic knowledge containing trigger information. The articles in Wikipedia are used as unstructured texts to be labeled. To understand our method easily, we first introduce them as follows:\nFreebase is a semantic knowledge base (Bollacker et al., 2008), which makes use of mediators (also called compound value types, CVTs) to merge multiple values into a single value. As shown in Figure 3, people.marriage is one type of CVTs. There are many instances of people.marriage and the marriage of Barack Obama and Michelle Obama is numbered as m.02nqglv. Spouse, from, to and location of ceremony are\n2https://github.com/acl2017submission/event-data\nroles of the people.marriage CVTs. Barack Obama, Michelle Obama, 10/3/1992 and Trinity United Church of Christ are the values of the instances. In this paper, we regard these CVTs as events, type of CVTs as event type, CVT instances as event instances, values in CVTs as arguments in events and roles of CVTs as the roles of arguments play in the event, respectively. According to the statistics of the Freebase released on 23th April, 2015, there are around 1885 CVTs and around 14 million CVTs instances. After filtering out useless and meaningless CVTs, such as CVTs about user profiles and website information, we select 21 types of CVTs with around 3.8 million instances for experiments, which mainly involves events about education, military, sports and so on.\nFrameNet3 is a linguistic resource storing information about lexical and predicate argument semantics (Baker et al., 1998). FrameNet contains more than 1, 000 frames and 10, 000 Lexical Units (LUs). Each frame of FrameNet can be taken as a semantic frame of a type of events (Liu et al., 2016). Each frame has a set of lemmas with part of speech tags that can evoke the frame, which are called LUs. For example, appoint.v is a LU of Appointing frame in FrameNet, which can be mapped to people.appointment events in Freebase. And a LUs of the frame plays a similar role as the trigger of an event. Thus we use FrameNet to detect triggers in our automatically data labeling process.\nWikipedia4 that we used was released on January, 2016. All 6.3 million articles in it are used in our experiments. We use Wikipedia because it is relatively up-to-date, and much of the information in Freebase is derived from Wikipedia."
    }, {
      "heading" : "3 Method of Generating Training Data",
      "text" : "Figure 4 describes the architecture of automatically labeling data, which primarily involves the following four components: (i) Key argument detection, which prioritizes arguments of each event type and selects key arguments for each type of event; (ii) Trigger word detection, which uses key arguments to label sentences that may express events preliminarily, and then detect triggers; (iii) Trigger word filtering and expansion, which uses FrameNet to filter noisy triggers and expand triggers; (iv) Automatically labeled data generation, which uses a SDS to label events in sentences.\n3http://framenet.icsi.berkeley.edu 4https://www.wikipedia.org/\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nACL 2017 Submission 350. Confidential Review Copy. DO NOT DISTRIBUTE.\nWikipedia\nTrainingDS\nFigure 4: The architecture of automatically labeling training data for large scale event extraction."
    }, {
      "heading" : "3.1 Key Argument Detection",
      "text" : "This section illustrates how to detect key arguments for each event type via Freebase. Intuitively, arguments of a type of event play different roles. Some arguments play indispensable roles in an event, and serve as vital clues when distinguishing different events. For example, compared with arguments like time, location and so on, spouses are key arguments in a marriage event. We call these arguments as key arguments. We propose to use Key Rate (KR) to estimate the importance of an argument to a type of event, which is decided by two factors: Role Saliency and Event Relevance.\nRole Saliency (RS) reflects the saliency of an argument to represent a specific event instance of a given event type. If we tend to use an argument to distinguish one event instance form other instances of a given event type, this argument will play a salient role in the given event type. We define RS as follows:\nRSij = Count(Ai, ETj)\nCount(ETj) (1)\nwhere RSij is the role saliency of i-th argument to j-th event type, Count(Ai , ETj) is the number of Arguemnti occurring in all instances of eventTypej in Freebase and Count(ETj) is the number of instances of eventTypej in Freebase.\nEvent Relevance (ER) reflects the ability in which an argument can be used to discriminate different event types. If an argument occurs in every event type, the argument will have a low event relevance. We propose to compute ER as follows:\nERi = log Sum(ET )\n1 + Count(ETCi) (2)\nwhere ERi is the event relevance of i-th argument, Sum (ET ) is the number of all event types in knowledge base and Count(ETCi) is the number of event types containing i-th argument. Finally, KR is computed as follows:\nKRij = RSij ∗ ERi (3) We compute KR for all arguments of each event type, and sort them according to KR. Then we choose top K arguments as key arguments."
    }, {
      "heading" : "3.2 Trigger Word Detection",
      "text" : "After detecting key arguments for every event types, we use these key arguments to label sentences that may express events in Wikipedia. At first, we use Standford CoreNLP tool5 to converts the raw Wikipedia texts into a sequence of sentences, attaches NLP annotations (POS tag, NER tag). Finally, we select sentences contains all key arguments of an event instance in Freebase as sentences expressing corresponding events. Then we use these labeled sentences to detect triggers.\nIn a sentence, a verb tend to express an occurrence of an event. For example, in ACE 2005 English data, there are 60% of events triggered by verbs. As shown in Figure 1, threw is a trigger of Attack event. Intuitively, if a verb occurs more times than other verbs in the labeled sentences of one event type , the verb tends to trigger this type of event; and if a verb occurs in sentences of every event types, like is, the verb will have a low probability to trigger events. Thus we propose Trigger Candidate Frequency (TCF) and Trigger Event Type Frequency (TETF) to evaluate above two aspects. Finally we employ Trigger Rate (TR), which is the product of TCF and TETF to estimate the probability of a verb to be a trigger, which is formulated as follows:\nTRij = TCFij ∗ TETFi (4)\nTCFij = Count(Vi, ETSj)\nCount(ETSj) (5)\nTETFi = log Sum(ET )\n1 + Count(ETIi) (6)\nwhere TRij is the trigger rate of i-th verb to jth event type, Count(Vi, ETSj) is the number of sentences, which express j-th type of event and contain i-th verb, Count(ETSj) is the number of sentences expressing j-th event type, Count(ETIi) is the number of event types, which have the labeled sentences containing i-th verb. Finally, we choose verbs with high TR values as the trigger words for each event type."
    }, {
      "heading" : "3.3 Trigger Word Filtering and Expansion",
      "text" : "We can obtain an initial verbal trigger lexicon by above trigger word detection. However, this initial trigger lexicon is noisy and merely contains verbal triggers. The nominal triggers like marriage are missing. Because the number of nouns in one sentence is usually larger than that of verbs, it is hard to use TR to find nominal triggers. Thus, we propose to use linguistic resource FrameNet to filter\n5http://stanfordnlp.github.io/CoreNLP/\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nACL 2017 Submission 350. Confidential Review Copy. DO NOT DISTRIBUTE.\nnoisy verbal triggers and expand nominal triggers. As the success of word embedding in capturing semantics of words (Turian et al., 2010), we employ word embedding to map the events in Freebase to frames in FrameNet. Specifically, we use the average word embedding of all words in i-th Freebase event type name ei and word embedding of k-th lexical units of j-th frame ej,k to compute the semantic similarity. Finally, we select the frame contains max similarity of ei and ej,k as the mapped frame, which can be formulated as follows:\nframe(i) = argmax j (similarity(ei, ej,k)) (7) Then, we filter the verb, which is in initial verbal trigger word lexicon and not in the mapping frame. And we use all nouns in the mapped frame to expand trigger lexicon. 3.4 Automatically labeled data generation Finally, we propose a Soft Distant Supervision and use it to automatically generate training data, which assumes that any sentence containing all key arguments in Freebase and a corresponding trigger word is likely to express that event in some way, and arguments occurring in that sentence are likely to play the corresponding roles in that event."
    }, {
      "heading" : "4 Method of Event Extraction",
      "text" : "In this paper, event extraction is formulated as a two-stage, multi-class classification task. The first stage is called Event Classification, which aims to predict whether the key argument candidates participate in a Freebase event. If the key arguments participate a Freebase event, the second stage is conducted, which aims to assign arguments to the event and identify their corresponding roles. We call this stage as argument classification. We employ two similar Dynamic Multi-pooling Convolutional Neural Networks with Multi-instance Learning (DMCNNs-MIL) for above two stages. The Dynamic Multi-pooling Convolutional Neural Networks (DMCNNs) is the best reported CNN-based model for event extraction (Chen et al., 2015) by using human-annotated training data. However, our automatically labeled data face a noise problem, which is a intrinsic problem of using DS to construct training data (Hoffmann et al., 2011; Surdeanu et al., 2012). In order to alleviate the wrong label problem, we use Multi-instance Learning (MIL) for two DMCNNs. Because the second stage is more complicated and limited in space, we take the MIL used in arguments classification as an example and describes as follows:\nWe define all of the parameters for the stage of argument classification to be trained in DMCNNs as θ. Suppose that there are T bags {M1,M2, ...,MT } and that the i-th bag contains qi instances (sentences) Mi = { m1i ,m 2 i , ...,m qi i } , the objective of multi-instance learning is to predict the labels of the unseen bags. In stage of argument classification, we take sentences containing the same argument candidate and triggers with a same event type as a bag and all instances in a bag are considered independently. Given an input instance mji , the network with the parameter θ outputs a vector O, where the r-th component Or corresponds to the score associated with argument role r. To obtain the conditional probability p(r|mji , θ), we apply a softmax operation over all argument role types:\np(r|mji , θ) = eor n∑ k=1 eok (8)\nwhere, n is the number of roles. And the objective of multi-instance learning is to discriminate bags rather than instances. Thus, we define the objective function on the bags. Given all (T ) training bags (Mi, yi), we can define the objective function using cross-entropy at the bag level as follows:\nJ (θ) = T∑ i=1 log p(yi|mji , θ) (9)\nwhere j is constrained as follows:\nj∗ = argmax j p(r|mji , θ) 1 ≤ j ≤ qi (10)\nTo compute the network parameter θ, we maximize the log likelihood J (θ) through stochastic gradient descent over mini-batches with the Adadelta (Zeiler, 2012) update rule."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we first manually evaluate our automatically labeled data. Then, we conduct automatic evaluations for our labeled data based on ACE corpus and analyze effects of different approaches to automatically label training data. Finally, we shows the performance of DMCNNs-MIL on our automatically labeled data."
    }, {
      "heading" : "5.1 Our Automatically Labeled Data",
      "text" : "By using the proposed methods, a large set of labeled data could be generated automatically. Table 2 shows the statistics of the five largest automatically labeled events among selected 21 Freebase events. Two hyper parameters, the number of key arguments and the value of TR in our automatically data labeling, are set as 2 and 0.8, by\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nACL 2017 Submission 350. Confidential Review Copy. DO NOT DISTRIBUTE.\nEvent Type FreebaseSize Sentences (KA) Sentences (KA+T) Examples of argument roles sorted by KR Examples of triggers\npeople.marriage 152,276 56,837 26,349 spouse, spouse, from, to, location marriage, marry, wed, wedding, couple,..., wife music.group membership 239,813 90,617 20,742 group, member, start, role, end musician, singer, sing, sang, sung, concert,..., play\neducation.education 530,538 26,966 11,849 student, institution, degree,..., minor educate, education, graduate, learn, study,..., student organization.leadership 43,610 5,429 3,416 organization, person, title,..., to CEO, charge, administer, govern, rule, boss,..., chair\nolympics.olympic medal honor 20,790 4,056 2,605 medalist, olympics, event,..., country win, winner, tie, victor, gold, silver,..., bronze ... ... ... ... ... ...\nsum of 21 selected events 3,870,492 421,602 72,611 argument1, argument2 ,..., argumentN trigger1, trigger2, trigger3, ... , triggerN\nTable 2: The statistics of five largest automatically labeled events in selected 21 Freebase events, with their size of instances in Freebase, sentences labeled with key argument (KA) and KA + Triggers(T), examples of arguments roles sorted by KR and examples of triggers.\ngrid search respectively. When we merely use two key arguments to label data, we will obtain 421, 602 labeled sentences. However, these sentences miss labeling triggers. Thus, we leverage these rough labeled data and FrameNet to find triggers and use SDS to generate labeled data. Finally, 72, 611 labeled sentences are generated automatically. Compared with nearly 6, 000 human annotated labeled sentence in ACE, our method can automatically generate large scale labeled training data."
    }, {
      "heading" : "5.2 Manual Evaluations of Labeled Data",
      "text" : "##001 He is the uncle of [Amal Clooney], [wife] of the actor [George Clooney]. Trigger: wife Event Type: Marriage MannalAnotate[Y/N]: Argument: Amal Clooney Role:Spouse MannalAnotate[Y/N]: Argument: George Clooney Role:Spouse MannalAnotate[Y/N]:\n##002 She was [married] to the cinematographer [Theo Nischwitz] and was sometimes credited as [Gertrud Hinz-Nischwitz]. Trigger: married Event Type: Marriage MannalAnotate[Y/N]: Argument: Theo Nischwitz Role:Spouse MannalAnotate[Y/N]: Argument: Gertrud Hinz-Nischwitz Role:Spouse MannalAnotate[Y/N]:\nFigure 5: Examples of manual evaluations.\nWe firstly manually evaluate the precision of our automatically generated labeled data. We randomly select 500 samples from our automatically labeled data. Each selected sample is a sentence with a highlighted trigger, labeled arguments and corresponding event type and argument roles. Figure 5 gives some samples. Annotators are asked to assign one of two labels to each sample. “Y”: the word highlighted in the given sentence indeed triggers an event of the corresponding type or the word indeed plays the corresponding role in that event. Otherwise “N” is labeled. It is very easy to annotate a sample for annotators, thus the annotated results are expected to be of high quality. Each sample is independently annotated by three annotators6 (including one of the authors and two of our colleagues who are familiar with event extraction task) and the final decision is made by voting.\nWe repeat above evaluation process on the final 72, 611 labeled data three times and the average\n6The inter-agreement rate is 87.5%\nStage Average Precision Trigger Labeling 88.9\nArgument Labeling 85.4\nTable 3: Manual Evaluation Results\nprecision is shown in Table 3. Our automatically generated data can achieve a precision of 88.9 and 85.4 for trigger labeling and argument labeling respectively, which demonstrates that our automatically labeled data is of high quality."
    }, {
      "heading" : "5.3 Automatic Evaluations of Labeled Data",
      "text" : "To prove the effectiveness of the proposed approach automatically, we add automatically generated labeled data into ACE dataset to expand the training sets and see whether the performance of the event extractor trained on such expanded training sets is improved. In our automatically labeled data, there are some event types that can correspond to those in ACE dataset. For example, our people.marriage events can be mapped to life.marry events in ACE2005 dataset. We mapped these types of events manually and we add them into ACE training corpus in two ways. (1) we delete the human annotated ACE data for these mapped event types in ACE dataset and add our automatically labeled data to remainder ACE training data. We call this Expanded Data (ED) as ED Only. (2) We directly add our automatically labeled data of mapped event types to ACE training data and we call this training data as ACE+ED. Then we use such data to train the same event extraction model (DMCNN) and evaluate them on the ACE testing data set. Following (Nguyen et al., 2016; Chen et al., 2015; Li et al., 2013), we used the same test set with 40 newswire articles and the same development set with 30 documents and the rest 529 documents are used for ACE training set. And we use the same evaluation metric P, R, F as ACE task defined. We select three baselines trained with ACE data. (1) Li’s structure, which is the best reported structured-based system (Li et al., 2013). (2) Chen’s DMCNN, which is the best reported CNN-based system (Chen et al., 2015). (3) Nguyen’s JRNN, which is the state-of-\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nACL 2017 Submission 350. Confidential Review Copy. DO NOT DISTRIBUTE.\nMethods TriggerIdentification(%) Trigger Identification + Classification(%) Argument Identification(%)\nArgument Role(%)\nP R F P R F P R F P R F Li’s structure trained with ACE 76.9 65.0 70.4 73.7 62.3 67.5 69.8 47.9 56.8 64.7 44.4 52.7 Chen’s DMCNN trained with ACE 80.4 67.7 73.5 75.6 63.6 69.1 68.8 51.9 59.1 62.2 46.9 53.5 Nguyen’s JRNN trained with ACE 68.5 75.7 71.9 66.0 73.0 69.3 61.4 64.2 62.8 54.2 56.7 55.4 DMCNN trained with ED Only 77.6 67.7 72.3 72.9 63.7 68.0 64.9 51.7 57.6 58.7 46.7 52.0 DMCNN trained with ACE+ED 79.7 69.6 74.3 75.7 66.0 70.5 71.4 56.9 63.3 62.8 50.1 55.7\nTable 4: Overall performance on ACE blind test data\nthe-arts system (Nguyen et al., 2016). The results are shown in Table 4. Compared with all models, DMCNN trained with ACE+ED achieves the highest performance. This demonstrates that our automatically generated labeled data could expand human annotated training data effectively. Moreover, compared with Chen’s DMCNN trained with ACE, DMCNN trained with ED Only achieves a competitive performance. This demonstrates that our large scale automatically labeled data is competitive with elaborately humanannotated data."
    }, {
      "heading" : "5.4 Discussion Impact of Key Rate",
      "text" : "In this section, we prove the effectiveness of KR to find key arguments and explore the impact of different numbers of key arguments to automatically generate data. We specifically select two methods as baselines for comparison with our KR method: ER and RS, which use the event relevance and role salience to sort arguments of each type of events respectively. Then we choose the same number of key arguments in all methods and use these key arguments to label data. After that we evaluate these methods by using above automatic evaluations based on ACE data. Results are shown in Table 5. ACE+KR achieve the best performance in both stages. This demonstrates the effectiveness of our KR methods.\nFeature Trigger Argument F1 F1\nACE 69.1 53.5 ACE + RS 70.1 55.3 ACE + ER 69.5 54.2 ACE + KR 70.5 55.7\nTable 5: Effects of ER, RS and KR\nTo explore the impact of different numbers of key arguments, we sort all arguments of each type of events according to KR value and select top k arguments as the key arguments. Examples are shown in Table 2. Then we automatically evaluate the performance by using automatic evaluations proposed above. Figure 6 shows the results, when we set k = 2, the method achieves a best\nImpact of Trigger Rate and FrameNet In this section, we prove the effectiveness of TR and FrameNet to find triggers. We specifically select two methods as baselines: TCF and TETF. TCF, TETF and TR respectively use the trigger candidate frequency, trigger event type frequency and trigger rate to sort trigger candidates of each type of events. Then we generate initial trigger lexicon by using all trigger candidates with high TCF value, TETF value or TR value. We set these hyper parameters as 0.8, 0.9 and 0.8, respectively, which are determined by grid search from (0.5, 0.6, 0.7, 0.8, 0.9, 1.0). FrameNet was used to filter noisy verbal triggers and expand nominal triggers. Trigger examples generated by TR+Framenet are shown in Table 2. Then we evaluate the performance of these methods by using above automatic evaluations. Results are shown in Table 6, Compared with ACE+TCF and ACE+TETF, ACE+TR gains a higher performance in both stages. It demonstrates the effectiveness of our TR methods. When we use FrameNet to generate triggers, compared with ACE+TR, we get a 1.0 improvement on trigger classification and a 1.7 improvement on argument classification. Such improvements are higher than improvements gained by other methods (TCF, IEF, TR), which demon-\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nACL 2017 Submission 350. Confidential Review Copy. DO NOT DISTRIBUTE.\nstrates the effectiveness of the usage of FrameNet.\nFeature Trigger Argument F1 F1\nACE 69.1 53.5 ACE + TCF 69.3 53.8\nACE + TETF 69.2 53.7 ACE + TR 69.5 54.0 ACE + TR + FrameNet 70.5 55.7\nTable 6: Effects of TCF, TETF,TR and FrameNet"
    }, {
      "heading" : "5.5 Performance of DMCNN-MIL",
      "text" : "Following previous work (Mintz et al., 2009) in distant supervised RE, we evaluate our method in two ways: held-out and manual evaluation.\nHeld-out Evaluation In the held-out evaluation, we hold out part of the Freebase event data during training, and compare newly discovered event instances against this heldout data. We use the following criteria to judge the correctness of each predicted event automatically: (1) An event is correct if its key arguments and event type match those of an event instance in Freebase; (2) An argument is correctly classified if its event type and argument role match those of any of the argument instance in the corresponding Freebase event. Figure 7 and Figure 8 show the precision-recall (P-R) curves for each method in the two stages of event extraction respectively. We can see that multi-instance learning is effective to alleviate the noise problem in our distant supervised event extraction.\nFigure 7: P-R curves for event classification.\nHuman Evaluation Because the incomplete nature of Freebase, heldout evaluation suffers from false negatives problem. We also perform a manual evaluation to eliminate these problems. In the manual evaluation, we manually check the newly discovered event instances that are not in Freebase. Because the number of these event instances in the test data is unknown, we cannot calculate the recall in this case. Instead, we calculate the precision of the top n extracted event instances. The human evaluation results are presented in Table 7. We can see that DMCNNs-MIL achieves the best performance."
    }, {
      "heading" : "6 Related Work",
      "text" : "Most of previous event extraction work focused on supervised learning paradigm and trained event extractors on human-annotated data which yield relatively high performance. (Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013, 2014; Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016). However, these supervised methods depend on the quality of the training data and labeled training data is expensive to produce. Unsupervised methods can extract large numbers of events without using labeled data (Chambers and Jurafsky, 2011; Cheung et al., 2013; Huang et al., 2016). But extracted events may not be easy to be mapped to events for a particular knowledge base.\nDistant supervision have been used in relation extraction for automatically labeling training data (Mintz et al., 2009; Hinton et al., 2012; Krause et al., 2012; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Surdeanu et al., 2012; Zeng et al., 2015). But DS for RE cannot directly use for EE. For the reasons that an event is more complicated than a relation and the task of EE is more difficult than RE. The best reported supervised RE and EE system got a F1-score of 88.0% (Wang et al., 2016) and 55.4% (Nguyen et al., 2016) respectively. Reschke et al. (2014) extended the distant supervision approach to fill slots in plane crash. However, the method can only extract arguments of one plane crash type and need flight number strings as input. In other words, the approach cannot extract whole event with different types automatically."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "In this paper, we present an approach to automatically label training data for EE. The experimental results show the quality of our large scale automatically labeled data is competitive with elaborately human-annotated data. Also, we provide a DMCNN-MIL model for this data as a baseline for further research. In the future, we will use the proposed automatically data labeling method to more event types and explore more models to extract events by using automatically labeled data.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\nACL 2017 Submission 350. Confidential Review Copy. DO NOT DISTRIBUTE."
    } ],
    "references" : [ {
      "title" : "The stages of event extraction",
      "author" : [ "David Ahn." ],
      "venue" : "Proceedings of the Workshop on Annotating and Reasoning About Time and Events. pages 1–8. http://dl.acm.org/citation.cfm?id=1629235.1629236.",
      "citeRegEx" : "Ahn.,? 2006",
      "shortCiteRegEx" : "Ahn.",
      "year" : 2006
    }, {
      "title" : "The berkeley framenet project",
      "author" : [ "Collin F Baker", "Charles J Fillmore", "John B Lowe." ],
      "venue" : "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics. Asso-",
      "citeRegEx" : "Baker et al\\.,? 1998",
      "shortCiteRegEx" : "Baker et al\\.",
      "year" : 1998
    }, {
      "title" : "Semantic parsing on freebase from question-answer pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang." ],
      "venue" : "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Association",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "Freebase: a collaboratively created graph database for structuring human knowledge",
      "author" : [ "Kurt Bollacker", "Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor." ],
      "venue" : "Proceedings of the 2008 ACM SIGMOD international confer-",
      "citeRegEx" : "Bollacker et al\\.,? 2008",
      "shortCiteRegEx" : "Bollacker et al\\.",
      "year" : 2008
    }, {
      "title" : "Template-based information extraction without the templates",
      "author" : [ "Nathanael Chambers", "Dan Jurafsky." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. As-",
      "citeRegEx" : "Chambers and Jurafsky.,? 2011",
      "shortCiteRegEx" : "Chambers and Jurafsky.",
      "year" : 2011
    }, {
      "title" : "Event extraction via dynamic multi-pooling convolutional neural networks",
      "author" : [ "Yubo Chen", "Liheng Xu", "Kang Liu", "Daojian Zeng", "Jun Zhao." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Probabilistic frame induction",
      "author" : [ "Kit Jackie Chi Cheung", "Hoifung Poon", "Lucy Vanderwende." ],
      "venue" : "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.",
      "citeRegEx" : "Cheung et al\\.,? 2013",
      "shortCiteRegEx" : "Cheung et al\\.",
      "year" : 2013
    }, {
      "title" : "Improving neural networks by preventing coadaptation of feature detectors",
      "author" : [ "Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov." ],
      "venue" : "arXiv preprint arXiv:1207.0580 https://arxiv.org/pdf/1207.0580.",
      "citeRegEx" : "Hinton et al\\.,? 2012",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "Knowledge-based weak supervision for information",
      "author" : [ "Raphael Hoffmann", "Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "S. Daniel Weld" ],
      "venue" : null,
      "citeRegEx" : "Hoffmann et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hoffmann et al\\.",
      "year" : 2011
    }, {
      "title" : "Large-scale learning of relation",
      "author" : [ "Feiyu Xu" ],
      "venue" : null,
      "citeRegEx" : "Xu.,? \\Q2012\\E",
      "shortCiteRegEx" : "Xu.",
      "year" : 2012
    }, {
      "title" : "Leveraging framenet to im",
      "author" : [ "Jun Zhao" ],
      "venue" : null,
      "citeRegEx" : "Zhao.,? \\Q2016\\E",
      "shortCiteRegEx" : "Zhao.",
      "year" : 2016
    }, {
      "title" : "Event extraction as dependency parsing",
      "author" : [ "David McClosky", "Mihai Surdeanu", "Christopher Manning." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association",
      "citeRegEx" : "McClosky et al\\.,? 2011",
      "shortCiteRegEx" : "McClosky et al\\.",
      "year" : 2011
    }, {
      "title" : "Distant supervision for relation extraction without labeled data",
      "author" : [ "Mike Mintz", "Steven Bills", "Rion Snow", "Daniel Jurafsky." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on",
      "citeRegEx" : "Mintz et al\\.,? 2009",
      "shortCiteRegEx" : "Mintz et al\\.",
      "year" : 2009
    }, {
      "title" : "Event detection and domain adaptation with convolutional neural networks",
      "author" : [ "Huu Thien Nguyen", "Ralph Grishman." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Con-",
      "citeRegEx" : "Nguyen and Grishman.,? 2015",
      "shortCiteRegEx" : "Nguyen and Grishman.",
      "year" : 2015
    }, {
      "title" : "Joint event extraction via recurrent neural networks",
      "author" : [ "Thien Huu Nguyen", "Kyunghyun Cho", "Ralph Grishman." ],
      "venue" : "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguis-",
      "citeRegEx" : "Nguyen et al\\.,? 2016",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2016
    }, {
      "title" : "Event extraction using distant supervision",
      "author" : [ "Kevin Reschke", "Martin Jankowiak", "Mihai Surdeanu", "Christopher D Manning", "Daniel Jurafsky." ],
      "venue" : "Proceedings of the Ninth International Conference on Language Resources and E-",
      "citeRegEx" : "Reschke et al\\.,? 2014",
      "shortCiteRegEx" : "Reschke et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-instance multi-label learning for relation extraction",
      "author" : [ "Mihai Surdeanu", "Julie Tibshirani", "Ramesh Nallapati", "D. Christopher Manning." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and",
      "citeRegEx" : "Surdeanu et al\\.,? 2012",
      "shortCiteRegEx" : "Surdeanu et al\\.",
      "year" : 2012
    }, {
      "title" : "Word representations: A simple and general method for semi-supervised learning",
      "author" : [ "Joseph Turian", "Lev-Arie Ratinov", "Yoshua Bengio." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Associa-",
      "citeRegEx" : "Turian et al\\.,? 2010",
      "shortCiteRegEx" : "Turian et al\\.",
      "year" : 2010
    }, {
      "title" : "Relation classification via multi-level attention cnns",
      "author" : [ "Linlin Wang", "Zhu Cao", "Gerard de Melo", "Zhiyuan Liu." ],
      "venue" : "Proceedings",
      "citeRegEx" : "Wang et al\\.,? 2016",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2016
    }, {
      "title" : "Adadelta: An adaptive learning rate method",
      "author" : [ "Matthew D Zeiler." ],
      "venue" : "arXiv preprint arXiv:1212.5701 https://arxiv.org/pdf/1212.5701.",
      "citeRegEx" : "Zeiler.,? 2012",
      "shortCiteRegEx" : "Zeiler.",
      "year" : 2012
    }, {
      "title" : "Distant supervision for relation extraction via piecewise convolutional neural networks",
      "author" : [ "Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association",
      "citeRegEx" : "Zeng et al\\.,? 2015",
      "shortCiteRegEx" : "Zeng et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "sumes that if two entities have a relationship in a known knowledge base, then all sentences that mention these two entities will express that relationship in some way (Mintz et al., 2009).",
      "startOffset" : 168,
      "endOffset" : 188
    }, {
      "referenceID" : 3,
      "context" : "To understand our method easily, we first introduce them as follows: Freebase is a semantic knowledge base (Bollacker et al., 2008), which makes use of mediators (also called compound value types, CVTs) to merge multiple values into a single value.",
      "startOffset" : 107,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "FrameNet3 is a linguistic resource storing information about lexical and predicate argument semantics (Baker et al., 1998).",
      "startOffset" : 102,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "As the success of word embedding in capturing semantics of words (Turian et al., 2010), we employ word embedding to map the events in Freebase to frames in FrameNet.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 5,
      "context" : "The Dynamic Multi-pooling Convolutional Neural Networks (DMCNNs) is the best reported CNN-based model for event extraction (Chen et al., 2015) by using human-annotated training data.",
      "startOffset" : 123,
      "endOffset" : 142
    }, {
      "referenceID" : 8,
      "context" : "However, our automatically labeled data face a noise problem, which is a intrinsic problem of using DS to construct training data (Hoffmann et al., 2011; Surdeanu et al., 2012).",
      "startOffset" : 130,
      "endOffset" : 176
    }, {
      "referenceID" : 16,
      "context" : "However, our automatically labeled data face a noise problem, which is a intrinsic problem of using DS to construct training data (Hoffmann et al., 2011; Surdeanu et al., 2012).",
      "startOffset" : 130,
      "endOffset" : 176
    }, {
      "referenceID" : 19,
      "context" : "where j is constrained as follows: j = argmax j p(r|mji , θ) 1 ≤ j ≤ qi (10) To compute the network parameter θ, we maximize the log likelihood J (θ) through stochastic gradient descent over mini-batches with the Adadelta (Zeiler, 2012) update rule.",
      "startOffset" : 222,
      "endOffset" : 236
    }, {
      "referenceID" : 14,
      "context" : "Following (Nguyen et al., 2016; Chen et al., 2015; Li et al., 2013), we used the same test set with 40 newswire articles and the same development set with 30 documents and the rest 529 documents are used for ACE training set.",
      "startOffset" : 10,
      "endOffset" : 67
    }, {
      "referenceID" : 5,
      "context" : "Following (Nguyen et al., 2016; Chen et al., 2015; Li et al., 2013), we used the same test set with 40 newswire articles and the same development set with 30 documents and the rest 529 documents are used for ACE training set.",
      "startOffset" : 10,
      "endOffset" : 67
    }, {
      "referenceID" : 5,
      "context" : "(2) Chen’s DMCNN, which is the best reported CNN-based system (Chen et al., 2015).",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 14,
      "context" : "7 Table 4: Overall performance on ACE blind test data the-arts system (Nguyen et al., 2016).",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "5 Performance of DMCNN-MIL Following previous work (Mintz et al., 2009) in distant supervised RE, we evaluate our method in two ways: held-out and manual evaluation.",
      "startOffset" : 51,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "(Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013, 2014; Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016).",
      "startOffset" : 0,
      "endOffset" : 166
    }, {
      "referenceID" : 11,
      "context" : "(Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013, 2014; Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016).",
      "startOffset" : 0,
      "endOffset" : 166
    }, {
      "referenceID" : 5,
      "context" : "(Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013, 2014; Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016).",
      "startOffset" : 0,
      "endOffset" : 166
    }, {
      "referenceID" : 13,
      "context" : "(Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013, 2014; Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016).",
      "startOffset" : 0,
      "endOffset" : 166
    }, {
      "referenceID" : 14,
      "context" : "(Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013, 2014; Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016).",
      "startOffset" : 0,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : "Unsupervised methods can extract large numbers of events without using labeled data (Chambers and Jurafsky, 2011; Cheung et al., 2013; Huang et al., 2016).",
      "startOffset" : 84,
      "endOffset" : 154
    }, {
      "referenceID" : 6,
      "context" : "Unsupervised methods can extract large numbers of events without using labeled data (Chambers and Jurafsky, 2011; Cheung et al., 2013; Huang et al., 2016).",
      "startOffset" : 84,
      "endOffset" : 154
    }, {
      "referenceID" : 12,
      "context" : "Distant supervision have been used in relation extraction for automatically labeling training data (Mintz et al., 2009; Hinton et al., 2012; Krause et al., 2012; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Surdeanu et al., 2012; Zeng et al., 2015).",
      "startOffset" : 99,
      "endOffset" : 258
    }, {
      "referenceID" : 7,
      "context" : "Distant supervision have been used in relation extraction for automatically labeling training data (Mintz et al., 2009; Hinton et al., 2012; Krause et al., 2012; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Surdeanu et al., 2012; Zeng et al., 2015).",
      "startOffset" : 99,
      "endOffset" : 258
    }, {
      "referenceID" : 2,
      "context" : "Distant supervision have been used in relation extraction for automatically labeling training data (Mintz et al., 2009; Hinton et al., 2012; Krause et al., 2012; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Surdeanu et al., 2012; Zeng et al., 2015).",
      "startOffset" : 99,
      "endOffset" : 258
    }, {
      "referenceID" : 16,
      "context" : "Distant supervision have been used in relation extraction for automatically labeling training data (Mintz et al., 2009; Hinton et al., 2012; Krause et al., 2012; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Surdeanu et al., 2012; Zeng et al., 2015).",
      "startOffset" : 99,
      "endOffset" : 258
    }, {
      "referenceID" : 20,
      "context" : "Distant supervision have been used in relation extraction for automatically labeling training data (Mintz et al., 2009; Hinton et al., 2012; Krause et al., 2012; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Surdeanu et al., 2012; Zeng et al., 2015).",
      "startOffset" : 99,
      "endOffset" : 258
    }, {
      "referenceID" : 18,
      "context" : "0% (Wang et al., 2016) and 55.",
      "startOffset" : 3,
      "endOffset" : 22
    }, {
      "referenceID" : 14,
      "context" : "4% (Nguyen et al., 2016) respectively.",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "(Ahn, 2006; Ji and Grishman, 2008; Hong et al., 2011; McClosky et al., 2011; Li et al., 2013, 2014; Chen et al., 2015; Nguyen and Grishman, 2015; Nguyen et al., 2016). However, these supervised methods depend on the quality of the training data and labeled training data is expensive to produce. Unsupervised methods can extract large numbers of events without using labeled data (Chambers and Jurafsky, 2011; Cheung et al., 2013; Huang et al., 2016). But extracted events may not be easy to be mapped to events for a particular knowledge base. Distant supervision have been used in relation extraction for automatically labeling training data (Mintz et al., 2009; Hinton et al., 2012; Krause et al., 2012; Krishnamurthy and Mitchell, 2012; Berant et al., 2013; Surdeanu et al., 2012; Zeng et al., 2015). But DS for RE cannot directly use for EE. For the reasons that an event is more complicated than a relation and the task of EE is more difficult than RE. The best reported supervised RE and EE system got a F1-score of 88.0% (Wang et al., 2016) and 55.4% (Nguyen et al., 2016) respectively. Reschke et al. (2014) extended the distant supervision approach to fill slots in plane crash.",
      "startOffset" : 1,
      "endOffset" : 1117
    } ],
    "year" : 2017,
    "abstractText" : "Modern models of event extraction for tasks like ACE are based on supervised learning of events from small hand-labeled data. However, hand-labeled training data is expensive to produce, in low coverage of event types, and limited in size, which makes supervised methods hard to extract large scale of events for knowledge base population. To solve the data labeling problem, we propose to automatically label training data for event extraction via world knowledge and linguistic knowledge, which can detect key arguments and trigger words for each event type and employ them to label events in texts automatically. The experimental results show that the quality of our large scale automatically labeled data is competitive with elaborately human-labeled data. And our automatically labeled data can incorporate with human-labeled data, then improve the performance of models learned from these data.",
    "creator" : "LaTeX with hyperref package"
  }
}