{
  "name" : "387.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n010\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n060\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Detection of sentiment and sarcasm in usergenerated short reviews is of primary importance for social media analysis, recommendation and dialog systems. Traditional sentiment analyzers and\nsarcasm detectors face challenges that arise at lexical, syntactic, semantic and pragmatic levels (Liu and Zhang, 2012; Mishra et al., 2016c). Featurebased systems (Akkaya et al., 2009; Sharma and Bhattacharyya, 2013; Poria et al., 2014) can aptly handle lexical and syntactic challenges (e.g. learning that the word deadly conveys a strong positive sentiment in opinions such as Shane Warne is a deadly bowler, as opposed to The high altitude Himalayan roads have deadly turns). It is, however, extremely difficult to tackle subtleties at semantic and pragmatic levels. For example, the sentence “I really love my job. I work 40 hours a week to be this poor.” requires an NLP system to be able to understand that the opinion holder has not expressed a positive sentiment towards her / his job. In the absence of explicit clues in the text, it is difficult for automatic systems to arrive at a correct classification decision, as they often lack external knowledge about various aspects of the text being classified.\nMishra et al. (2016b) and Mishra et al. (2016c) show that NLP systems based on cognitive data (or simply, Cognitive NLP systems) , that of leverage eye-movement information obtained from human readers, can tackle the semantic and pragmatic challenges better. The hypothesis here is that human gaze activities are related to the cognitive processes in the brain, that combines the “external knowledge” that the a reader possesses with textual clues that she / he perceives. While incorporating behavioral information obtained from gaze-data in NLP systems is intriguing and quite plausible, especially due to the availability of low cost eye-tracking machinery (Wood and Bulling, 2014; Yamamoto et al., 2013), few methods exist for text classification and they rely on handcrafted features extracted from gaze data (Mishra et al., 2016b,c). These systems have limited capabilities due to two reasons: (a) Manually designed gaze based features may not adequately capture all\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n162\n163\n164\n165\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nforms of textual subtleties (b) Eye-movement data is not as intuitive to analyze as text which makes the task of designing manual features more difficult. So, in this work, instead of handcrafting the gaze based and textual features, we try to learn feature representations from both gaze and textual data using Convolutional Neural Networks (CNNs). We test our technique on two publicly available datasets enriched with eye-movement information, used for binary classification tasks of sentiment polarity and sarcasm detection. Our experiments show that the automatically extracted features help to achieve significant classification performance improvement over (a) existing systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone.\nThe rest of the paper is organized as follows. Section 2 discusses the motivation behind using readers’ eye-movement data in a text classification setting. Section 3 discusses on why CNNs is preferred over other available alternatives for feature extraction. The CNN architecture is proposed and discussed in Section 4. Section 5 describes our experimental setup and results are discussed in Section 6. We provide a detailed analysis of the results along with some insightful observations in Section 7. Section 8 points to relevant literature followed by Section 9 that concludes the paper. Terminology: A fixation is a relatively long stay of gaze on a visual object (such as words in text) where as a sacccade corresponds to quick shifting of gaze between two positions of rest. Forward and backward saccades are called progressions and regressions respectively. A scanpath is a line graph that contains fixations as nodes and saccades as edges."
    }, {
      "heading" : "2 Eye-movement and Linguistic Subtleties",
      "text" : "Presence of linguistic subtleties often induces (a) surprisal (Kutas and Hillyard, 1980; Malsburg et al., 2015), due to the underlying disparity /context incongruity or (b) higher cognitive load (Rayner and Duffy, 1986), due to the presence of lexically and syntactically complex structures. While surprisal accounts for irregular saccades (Malsburg et al., 2015), higher cognitive load results in longer fixation duration (Kliegl et al., 2004).\nMishra et al. (2016b) find that presence of sarcasm in text triggers either irregular saccadic patterns or unusually high duration fixations than non-sarcastic texts (illustrated through example scanpath representations in Figure 1). For sentiment bearing texts, highly subtle eye-movement patterns are observed for semantically / pragmatically complex negative opinions (expressing irony, sarcasm, thwarted expectations etc.) than the simple ones (Mishra et al., 2016b). The association between linguistic subtleties and eye-movement patterns could be captured through sophisticated feature engineering that considers both gaze and text inputs. In our work, CNNs take the onus of feature engineering.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nT ex\nt C\nom p\non en\nt\nNon-static\nStatic\nSaccade\nFixation\nG az\ne C\nom p\non en t P1 P2 P3 P4 P5 P6 P7 P8\nN×K representation of sentences with static and non static channels\nP×G representation of sentences\nwith fixation and saccade channels\n1-D convolution operation with multiple filter width\nand feature maps\n2-D convolution operation with multiple filter row and\nColumn widths\nMax-pooling for each filter width\nMax-pooling over\nmultiple dimensions for\nmultiple filter widths\nFully connected with dropouts and softmax\noutput\nMerged pooled values\nFigure 3: Deep convolutional model for feature extraction from both text and gaze inputs"
    }, {
      "heading" : "3 Why Convolutional Neural Network?",
      "text" : "CNNs have been quite effective in learning filters for image processing tasks, filters being used to transform the input image into more informative feature space (Krizhevsky et al., 2012). Filters learned at various CNN layers are quite similar to handcrafted filters used for detection of edges, contours and removal of redundant backgrounds. We believe, a similar technique can also be applied to eye-movement data, where the learned filters will, hopefully, extract informative cognitive features. For instance, for sarcasm, we expect the network to learn filters that detect long distance saccades (refer to Figure 2 for an analogical illustration). With more number of convolution filters of different dimensions, the network may extract multiple features related to different gaze attributes (such as fixations, progressions, regressions and skips) and will be free from any form of human bias that manually extracted features are susceptible to."
    }, {
      "heading" : "4 Learning Feature Representations: The CNN architecture",
      "text" : "Figure 3 shows the CNN architecture with two components for processing and extracting features from text and gaze inputs. The components are explained below."
    }, {
      "heading" : "4.1 Text Component",
      "text" : "The text component is quite similar to the one proposed by Kim (2014) for sentence classification.\nWords (in the form of one-hot representation) in the input text are first replaced by their embeddings of dimension K (ith word in the sentence represented by an embedding vector xi ∈ RK). As per Kim (2014), a multi-channel variant of CNN (referred to as MULTICHANNELTEXT) can be implemented by using two channels of embeddingsone that remains static through out training (referred to as STATICTEXT), and the other one that gets updated during training (referred to as NONSTATICTEXT). We separately experiment with static, non-static and multi-channel variants.\nFor each possible input channel of the text component, a given text is transformed into a tensor of fixed length N (padded with zero-tensors wherever necessary to tackle length variations) by concatenating the word embeddings.\nx1:N = x1 ⊕ x2 ⊕ x3 ⊕ ...⊕ xN (1)\nwhere ⊕ is the concatenation operator. To extract local features1, convolution operation is applied. Convolution operation involves a filter, W ∈ RHK , which is convolved with a window of H embeddings to produce a local feature for the H words. A local feature, ci is generated from a window of embeddings xi:i+H−1 by applying a non linear function (such as a hyperbolic tangent) over the convoluted output. Mathematically,\nci = f(W.xi:i+H−1 + b) (2)\n1features specific to a region in case of images or window of words in case of text\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nwhere b ∈ R is the bias and f is the non-linear function. This operation is applied to each possible window of H words to produce a feature map (c) for the window size H .\nc = [c1, c2, c3, ..., cN−H+1] (3)\nA global feature is then obtained by applying max pooling operation2 (Collobert et al., 2011) over the feature map. The idea behind max-pooling is to capture the most important feature - one with the highest value - for each feature map.\nWe have described the process by which one feature is extracted from one filter (for illustration, red bordered portions in Figure 3 depict the case of H = 2). The model uses multiple filters (with varying window sizes) to obtain multiple features representing the text. In the MULTICHANNELTEXT variant, for a window of H words, convolution operation is separately applied on both the embedding channels. Local features learned from both the channels are concatenated before applying max-pooling."
    }, {
      "heading" : "4.2 Gaze Component",
      "text" : "The gaze component deals with scanpaths of multiple participants annotating the same text. Scanpaths can be pre-processed to extract two sequences of gaze data to form separate channels of input: (1) A sequence of normalized3 durations of fixations (in milliseconds) in the order in which they appear in the scanpath and (2) A sequence of position of fixations (in terms of word id) in the order in which they appear in the scanpath. These channels are related to two fundamental gaze attributes such as fixation and saccade respectively. With two channels, we thus have three possible configurations of the gaze component such as (i) FIXATION, where the input is normalized fixation duration sequence, (ii) SACCADE, where the input is fixation position sequence, and (iii) MULTICHANNELGAZE, where both the inputs channels are considered.\nFor each possible input channel, the input is in the form of a P × G matrix (with P → number of participants and G → length of the input sequence). Each element of the matrix gij ∈ R, with i ∈ P and j ∈ G, corresponds to the jth gaze attribute (either fixation duration or word id, depending on the channel) of the input sequence of\n2mean pooling does not perform well. 3scaled across participants using min-max normalization\nto reduce subjectivity\nthe ith participant. Now, unlike the text component, here we apply convolution operation across two dimensions i.e. choosing a two dimensional convolution filter W ∈ RJK (for simplicity, we have kept J = K, thus , making the dimension of W , J2). For the dimension size of J2, a local feature cij is computed from the window of gaze elements gij:(i+J−1)(j+J−1) by,\ncij = f(W.gij:(i+J−1)(j+J−1) + b) (4)\nwhere b ∈ R is the bias and f is a non-linear function. This operation is applied to each possible window of size J2 to produce a feature map (c),\nc =[c11, c12, c13, ..., c1(G−J+1),\nc21, c22, c23, ..., c2(G−J+1), ...,\nc(P−J+1)1, c(P−J+1)2, ..., c(P−J+1)(G−J+1)]\n(5)\nA global feature is then obtained by applying max pooling operation. Unlike the text component, max-pooling operator is applied to a 2D window of local features size M × N (for simplicity, we set M = N , denoted henceforth as M2). For the window of size M2, the pooling operation on c will result in as set of global features ĉJ = max{cij:(i+M−1)(j+M−1)} for each possible i, j.\nWe have described the process by which one feature is extracted from one filter (of 2D window size J2 and max-pooling window size of M2). In Figure 3, red and blue bordered portions illustrate the cases of J2 = [3, 3] and M2 = [2, 2] respectively. Like the text component, the gaze component uses multiple filters (also with varying window size) to obtain multiple features representing the gaze input. In the MULTICHANNELGAZE variant, for a 2D window of J2, convolution operation is separately applied on both fixation duration and saccade channels and local features learned from both the channels are concatenated before max-pooling is applied.\nOnce the global features are learned from both the text and gaze components, they are merged and passed to a fully connected feed forward layer (with number of units set to 150) followed by a SoftMax layer that outputs the the probabilistic distribution over the class labels.\nThe gaze component of our network is not invariant of the order in which the scanpath data is given as input- i.e., the P rows in the P × G can not be shuffled, even if each row is independent from others. The only way we can think of for\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\naddressing this issue is by applying convolution operations to all P × G matrices formed with all the permutations of P , capturing every possible ordering. Unfortunately, this makes the training process significantly less scalable, as the number of model parameters to be learned becomes huge. As of now, training and testing are carried out by keeping the order of the input constant."
    }, {
      "heading" : "5 Experiment Setup",
      "text" : "We now share several details regarding our experiments below. 1. Dataset: We experiment on sentiment and sarcasm tasks using two publicly available datasets enriched with eye-movement information. Dataset 1 has been released by Mishra et al. (2016a). It contains 994 text snippets with 383 positive and 611 negative examples. Out of the 994 snippets, 350 are sarcastic. Dataset 2 has been used by Joshi et al. (2014) and it consists of 843 snippets comprising movie reviews and normalized tweets out of which 443 are positive and 400 are negative. Eye-movement data of 7 and 5 readers is available for each snippet for dataset 1 and 2 respectively. 2. CNN Variants: With text component alone we have three variants such as STATICTEXT, NONSTATICTEXT and MULTICHANNELTEXT (refer to Section 4.1). Similarly, with gaze component we have variants such as FIXATION, SACCADE and MULTICHANNELGAZE (refer to Section 4.2). With both text and gaze components, 9 more variants could be experimented with. 3. Hyper-parameters: For text component, we experiment with filter widths (H) of [3, 4]. For the gaze component, 2D filters (J2) set to [3 × 3], [4× 4] respectively. The max pooling 2D window, M2, is set to [2 × 2]. In both gaze and text components, number of filters is set to 150, resulting in 150 feature maps for each window. These model hyper-parameters are fixed by trial and error and are possibly good enough to provide a first level insight into our system. Tuning of hyperparameters might help in improving the performance of our framework, which is on our future research agenda. 4. Regularization: For regularization dropout is employed on the penultimate layer with a constraint on l2-norms of the weight vectors (Hinton et al., 2012). Dropout prevents co-adaptation of hidden units by randomly dropping out - i.e., setting to zero - a proportion p of the hidden units\nduring forward propagation. We set p to 0.25. 5. Training: We use ADADELTA optimizer (Zeiler, 2012), with a learning rate of 0.1. The input batch size is set to 32 and number of training iterations (epochs) is set to 200. 10% of the training data is used for validation. 6. Use of pre-trained embeddings: Initializing the embedding layer with of pre-trained embeddings can be more effective than random initialization (Kim, 2014). In our experiments, we have used embeddings using word2vec facilitated by Mikolov et al. (2013) (best results obtained with embedding dimension of 50). We have also tried randomly initializing the embeddings but better results are obtained with pre-trained embeddings. 7. Comparison with existing work: For sentiment analysis, we compare our systems’s accuracy (for both datasets 1 and 2) with Mishra et al. (2016c)’s systems that rely on handcrafted text and gaze features. For sarcasm detection, we compare Mishra et al. (2016b)’s sarcasm classifier with ours using dataset 1 (with available gold standard labels for sarcasm). We follow the same 10-fold traintest configuration as these existing works for consistency."
    }, {
      "heading" : "6 Results",
      "text" : "In this section, we discuss the results for different model variants for sentiment polarity and sarcasm detection tasks."
    }, {
      "heading" : "6.1 Results for Sentiment Analysis Task",
      "text" : "Table 1 presents results for sentiment analysis task. For dataset 1, different variants of our CNN architecture outperform the best systems reported by Mishra et al. (2016c), with a maximum F-score improvement of 3.8%. This improvement is statistically significant of p < 0.05 as confirmed by McNemar test. Moreover, we observe an F-score improvement of around 5% for CNNs with both gaze and text components as compared to CNNs with only text components (similar to the system by Kim (2014)), which is also statistically significant (with p < 0.05).\nFor dataset 2, CNN based approaches do not perform better than manual feature based approaches. However, variants with both text and gaze components outperform the ones with only text component (Kim, 2014), with a maximum Fscore improvement of 2.9%. We observe that for dataset 2, training accuracy reaches 100 within\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nDataset1 Dataset2\nConfiguration P R F P R F\nTraditional systems based on\nNäive Bayes 63.0 59.4 61.14 50.7 50.1 50.39 Multi-layered Perceptron 69.0 69.2 69.2 66.8 66.8 66.8\ntextual features SVM (Linear Kernel) 72.8 73.2 72.6 70.3 70.3 70.3 Systems by Mishra et al. (2016c) Gaze based (Best) 61.8 58.4 60.05 53.6 54.0 53.3 Text + Gaze (Best) 73.3 73.6 73.5 71.9 71.8 71.8\nCNN with only text input (Kim, 2014) STATICTEXT 63.85 61.26 62.22 55.46 55.02 55.24 NONSTATICTEXT 72.78 71.93 72.35 60.51 59.79 60.14 MULTICHANNELTEXT 72.17 70.91 71.53 60.51 59.66 60.08\nCNN with only gaze Input\nFIXATION 60.79 58.34 59.54 53.95 50.29 52.06 SACCADE 64.19 60.56 62.32 51.6 50.65 51.12 MULTICHANNELGAZE 65.2 60.35 62.68 52.52 51.49 52\nCNN with both text and gaze Input\nSTATICTEXT + FIXATION 61.52 60.86 61.19 54.61 54.32 54.46 STATICTEXT + SACCADE 65.99 63.49 64.71 58.39 56.09 57.21 STATICTEXT + MULTICHANNELGAZE 65.79 62.89 64.31 58.19 55.39 56.75 NONSTATICTEXT + FIXATION 73.01 70.81 71.9 61.45 59.78 60.60 NONSTATICTEXT + SACCADE 77.56 73.34 75.4 65.13 61.08 63.04 NONSTATICTEXT + MULTICHANNELGAZE 79.89 74.86 77.3 63.93 60.13 62 MULTICHANNELTEXT + FIXATION 74.44 72.31 73.36 60.72 58.47 59.57 MULTICHANNELTEXT + SACCADE 78.75 73.94 76.26 63.7 60.47 62.04 MULTICHANNELTEXT + MULTICHANNELGAZE 78.38 74.23 76.24 64.29 61.08 62.64\nTable 1: Results for different traditional feature based systems and CNN model variants for the task of sentiment analysis. Abbreviations (P,R,F)→ Precision, Recall, F-score. SVM→Support Vector Machine\n25 epochs with validation accuracy stable around 50%, indicating the possibility of overfitting. Tuning the regularization parameters specific to dataset 2 may help here. Even though CNN might not be proving to be a choice as good as handcrafted features for dataset 2, the bottom line remains that incorporation of gaze data into CNN consistently improves the performance over onlytext-based CNN variants."
    }, {
      "heading" : "6.2 Results for Sarcasm Detection Task",
      "text" : "For sarcasm detection, our CNN model variants outperform traditional systems by a maximum margin of 11.27% (Table 2). However, the improvement by adding the gaze component to the CNN network is just 1.36%, which is statistically insignificant over CNN with text component. While inspecting the sarcasm dataset, we observe a clear difference between the vocabulary of sarcasm and non-sarcasm classes in our dataset. This, perhaps, was captured well by the text component, especially the variant with only non-static embeddings."
    }, {
      "heading" : "7 Discussion",
      "text" : "In this section, some important observations from our experiments are discussed. • Effect of embedding dimension variation: Embedding dimension has proven to have a deep impact on the performance of neural systems (dos\nSantos and Gatti, 2014; Collobert et al., 2011). We repeated our experiments by varying the embedding dimensions in the range of [50-300]4 and observed that reducing embedding dimension improves the F-scores by a little margin. Best results are obtained when the embedding dimension is as low as 50. Small embedding dimensions are probably reducing the chances of over-fitting when the data size is small. We also observe that for different embedding dimensions, performance of CNN with both gaze and text components is consistently better than that with only text component. • Effect of static / non static text channels: Nonstatic embedding channel has a major role in tuning embeddings for sentiment analysis by bringing adjectives expressing similar sentiment close to each other (e.g, good and nice), where as static channel seems to prevent over-tuning of embeddings (over-tuning often brings verbs like love closer to the pronoun I in embedding space, purely due to higher co-occurrence of these two words in sarcastic examples). • Effect of fixation / saccade channels: For sentiment detection, saccade channel seems to be handing text having semantic incongruity (due to the presence of irony / sarcasm) better. Fixation channel does not help much, may be because of higher variance in fixation duration. For sarcasm\n4a standard range (Liu et al., 2015; Melamud et al., 2016)\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nConfiguration P R F\nTraditional systems based on\nNäive Bayes 69.1 60.1 60.5 Multi-layered Perceptron 69.7 70.4 69.9\ntextual features SVM (Linear Kernel) 72.1 71.9 72 Systems by Riloff et al. (2013) Text based (Ordered) 49 46 47 Text + Gaze (Unordered) 46 41 42\nSystem by Joshi et al. (2015)\nText based (best) 70.7 69.8 64.2\nSystems by Mishra et al. (2016b)\nGaze based (Best) 73 73.8 73.1 Text based (Best) 72.1 71.9 72 Text + Gaze (Best) 76.5 75.3 75.7\nCNN with only text input (Kim, 2014) STATICTEXT 67.17 66.38 66.77 NONSTATICTEXT 84.19 87.03 85.59 MULTICHANNELTEXT 84.28 87.03 85.63\nCNN with only gaze input\nFIXATION 74.39 69.62 71.93 SACCADE 68.58 68.23 68.40 MULTICHANNELGAZE 67.93 67.72 67.82\nCNN with both text and gaze Input\nSTATICTEXT + FIXATION 72.38 71.93 72.15 STATICTEXT + SACCADE 73.12 72.14 72.63 STATICTEXT + MULTICHANNELGAZE 71.41 71.03 71.22 NONSTATICTEXT + FIXATION 87.42 85.2 86.30 NONSTATICTEXT + SACCADE 84.84 82.68 83.75 NONSTATICTEXT + MULTICHANNELGAZE 84.98 82.79 83.87 MULTICHANNELTEXT + FIXATION 87.03 86.92 86.97 MULTICHANNELTEXT + SACCADE 81.98 81.08 81.53 MULTICHANNELTEXT + MULTICHANNELGAZE 83.11 81.69 82.39\nTable 2: Results for different traditional feature based systems and CNN model variants for the task of sarcasm detection on dataset 1. Abbreviations (P,R,F)→ Precision, Recall, F-score\ndetection, fixation and saccade channels perform with similar accuracy when employed separately. Accuracy reduces with gaze multichannel, may be because of higher variation of both fixations and saccades across sarcastic and non-sarcastic classes, as opposed to sentiment classes.\n• Effectiveness of the CNN learned features To examine how good the features learned by the CNN are, we analyzed the features for a few example cases. Figure 4 presents some of the testexamples for the task of sarcasm detection. Example 1 contains sarcasm while examples 2, 3 and 4 are non-sarcastic. To see if there is any difference in the automatically learned features between text-only and combined text and gaze variants, we examine the feature vector (of dimension 150) for the examples obtained from different model variants. Output of the hidden layer after merge layer is considered as features learned by the network. We plot the features, in the form of color-bars, following Li et al. (2016) - denser colors representing higher feature values. In Figure 4, we show only two (representative) model variants viz., MULTICHANNELTEXT and MULTICHANNELTEXT+ MULTICHANNELGAZE. As one can see, addition of gaze information helps\nto generate features with more subtle differences (marked by blue rectangular boxes) for sarcastic and non-sarcastic texts. It is also interesting to note that in the marked region, features for the sarcastic texts exhibit more intensity than the nonsarcastic ones - perhaps capturing the notion that sarcasm typically conveys an intensified negative opinion. This difference is not clear in feature vectors learned by text only systems for instances like example 2, which has been incorrectly classified by MULTICHANNELTEXT. Example 4 is incorrectly classified by both the systems, perhaps due to lack of context. In cases like this, addition of gaze information does not help much in learning more distinctive features, as it becomes difficult for even humans to classify such texts."
    }, {
      "heading" : "8 Related Work",
      "text" : "Sentiment and sarcasm classification are two important problems in NLP and have been the focus of research for many communities for quite some time. Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n1. I would like to live in Manchester, England. The transition between Manchester and death would be unnoticeable. (Sarcastic, Negative Sentiment)\n2. We really did not like this camp. After a disappointing summer, we switched to another camp, and all of us much happier on all fronts! (Non Sarcastic, Negative Sentiment)\n3. Helped me a lot with my panics attack I take 6 mg a day for almost 20 years can't stop of course but make me feel very comfortable (Non Sarcastic, Positive Sentiment)\n4. Howard is the King and always will be, all others are weak clones. (Non Sarcastic, Positive Sentiment)\n(a) MultichannelText + MultichannelGaze (b) MultichannelText\nFigure 4: Visualization of representations learned by two variants of the network for sarcasm detection task. The output of the Merge layer (of dimension 150) are plotted in the form of colour-bars. Plots with thick red borders correspond to wrongly predicted examples.\n(Balamurali et al., 2011). For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (González-Ibánez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined architecture (Wang et al., 2016). Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Veale, 2016) that uses a combination of RNNs and CNNs.\nEye-tracking technology is a relatively new NLP, with a very few systems directly making use of gaze data in prediction frameworks. Klerke et al. (2016) present a novel multi-task learning approach for sentence compression using labeled data, while, Barrett and Søgaard (2015) discriminate between grammatical functions using gaze features. The closest works to ours is by Mishra et al. (2016b) and Mishra et al. (2016c) that introduce feature engineering based on both gaze\nand text data for sentiment and sarcasm detection tasks. These recent advancements motivate us to explore the cognitive NLP paradigm ."
    }, {
      "heading" : "9 Conclusion and Future Directions",
      "text" : "In this work, we proposed a multimodal ensemble of features, automatically learned using variants of CNNs from text and readers’ eye-movement data, for the tasks of sentiment and sarcasm classification. On multiple published datasets for which gaze information is available, our systems could achieve significant performance improvements over (a) systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone. An analysis of the learned features confirms that the combination of automatically learned features is indeed capable of representing deep linguistic subtleties in text that pose challenges to sentiment and sarcasm classifiers. Our future agenda includes: (a) optimizing the CNN framework hyper-parameters (e.g., filter width, dropout, embedding dimensions etc.) to obtain better results, (b) exploring the applicability of our technique for document-level sentiment analysis and (c) applying our framework on related problems, such as emotion analysis, text summarization and question-answering.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Cognitive NLP systemsi.e., NLP systems that make use of behavioral data augment traditional text based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc.. Such extraction of features is typically manual. We contend that manual extraction of features is not good enough to tackle text subtleties that characteristically prevail in complex classification tasks like sentiment analysis and sarcasm detection, and that even the extraction and choice of features should be delegated to the learning system. We introduce a framework to automatically extract cognitive features from the eye-movement / gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed framework is based on Convolutional Neural Network (CNN). The CNN learns features from both gaze and text and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.",
    "creator" : "LaTeX with hyperref package"
  }
}