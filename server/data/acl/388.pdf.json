{
  "name" : "388.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "The Universal Dependencies (UD) initiative seeks to develop cross-linguistically consistent annotation guidelines as well as a large number of uniformly annotated treebanks for many languages.1 Such resources could advance multilingual applications of parsing, improve comparability of evaluation results, enable cross-lingual learning, and more generally support natural language understanding.\nSeeking to exploit the benefits of UD for natural language understanding, we introduce UDEPLAMBDA, a semantic interface for UD that maps\n1http://www.universaldependencies.org/.\nnatural language to logical forms, representing underlying predicate-argument structures, in an almost language-independent manner. Our framework is based on DEPLAMBDA (Reddy et al., 2016) a recently developed method that converts English Stanford Dependencies (SD) to logical forms. The conversion process is illustrated in Figure 1 and discussed in more detail in Section 2. Whereas DEPLAMBDA works only for English, UDEPLAMBDA applies to any language for which UD annotations are available.2 In this paper, we describe the rationale behind UDEPLAMBDA and highlight important differences from DEPLAMBDA, some of which stem from the different treatment of various linguistic constructions in UD.\nOur experiments focus on semantic parsing as a testbed for evaluating the framework’s multilingual appeal. We address the task of learning to map natural language to machine interpretable formal meaning representations, specifically retrieving answers to questions from Freebase. To facilitate multilingual evaluation, we provide translations of the English WebQuestions (Berant et al., 2013) and GraphQuestions (Su et al., 2016) datasets to German and Spanish. We demonstrate that UDEPLAMBDA can be used to derive logical forms for these languages using a minimal amount of language-specific knowledge. Aside from developing the first multilingual semantic parsing tool for Freebase, we also experimentally show that UDEPLAMBDA outperforms strong baselines across languages and datasets. For English, it achieves the strongest result to date on GraphQuestions, with competitive results on WebQuestions. Beyond semantic parsing, we believe that the logical forms produced by our framework will be of use in various natural understanding tasks including entailment (Beltagy et al., 2016), text-based question\n2As of v1.3, UD annotations are available for 47 languages.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nanswering (Lewis and Steedman, 2013), sentence simplification (Narayan and Gardent, 2014), summarization (Liu et al., 2015), paraphrasing (Pavlick et al., 2015), and relation extraction (Rocktäschel et al., 2015). Our implementation and translated datasets will be made publicly available."
    }, {
      "heading" : "2 DEPLAMBDA",
      "text" : "Before describing UDEPLAMBDA, we provide an overview of DEPLAMBDA (Reddy et al., 2016) on which our approach is based. DEPLAMBDA converts a dependency tree to its logical form in three steps: binarization, substitution, and composition, each of which is briefly outlined below.\nBinarization A dependency tree is first mapped to a Lisp-style s-expression indicating the order of semantic composition. Figure 1(b) shows the s-expression for the sentence Disney won an Oscar for the movie Frozen, derived from the dependency tree in Figure 1(a). Here, the sub-expression (dobj won (det Oscar an)) indicates that the logical form of the phrase won an Oscar is derived by composing the logical form of the label dobj with the logical form of the word won and the logical form of the phrase an Oscar, derived analogously.\nAn obliqueness hierarchy is employed to impose a strict ordering on the modifiers to each head in the dependency tree. As an example, won has three modifiers in Figure 1(a), which according to the obliqueness hierarchy are composed in the order dobj> nmod> nsubj. In constructions like coordination, this ordering is crucial to arrive at the correct semantics (see Section 3.3).\nSubstitution Each symbol in the s-expressions is substituted for a lambda expression encoding its semantics. Words and dependency labels are assigned different types of expressions. In general, words have expressions of the following kind: ENTITY ⇒ λx.word(xa); e.g. Oscar⇒ λx.Oscar(xa) EVENT ⇒ λx.word(xe); e.g. won⇒ λx.won(xe) FUNCTIONAL⇒ λx.TRUE; e.g. an⇒ λx.TRUE\nHere, the subscripts ·a and ·e denote the types of individuals (Ind) and events (Event), respectively, whereas x denotes a paired variable (xa,xe) of type Ind×Event. Roughly speaking, proper nouns and adjectives invoke ENTITY expressions, verbs and adverbs invoke EVENT expressions, and common nouns invoke both ENTITY and EVENT expressions (see Section 3.3), while remaining words invoke FUNCTIONAL expressions. As in DEPLAMBDA,\nwe enforce the constraint that every s-expression is of the type η = Ind×Event → Bool, which simplifies the type system considerably.\nExpressions for dependency labels glue the semantics of heads and modifiers to articulate predicate-argument structure. These expressions in general take one of the following forms: COPY ⇒ λ f gx.∃y. f (x)∧g(y)∧ rel(x,y) e.g. nsubj, dobj, nmod, advmod INVERT ⇒ λ f gx.∃y. f (x)∧g(y)∧ reli(y,x) e.g. amod, acl MERGE ⇒ λ f gx. f (x)∧g(x) e.g. compound, appos, amod, acl HEAD ⇒ λ f gx. f (x) e.g. case, punct, aux, mark .\nAs an example of COPY, consider the lambda expression for dobj in (dobj won (det Oscar an)): λ f gx.∃y. f (x)∧ g(y)∧ arg2(xe,ya). This expression takes two functions f and g as input, where f represents the logical form of won and g represents the logical form of an Oscar. The predicateargument structure arg2(xe,ya) indicates that the arg2 of the event xe, i.e. won, is the individual ya, i.e. the entity Oscar. Since arg2(xe,ya) mimics the dependency structure dobj(won, Oscar), we refer to the expression kind evoked by dobj as COPY.\nExpressions that invert the dependency direction are referred to as INVERT (e.g. amod in running horse); expressions that merge two subexpressions without introducing any relation predicates are referred to as MERGE (e.g. compound in movie Frozen); and expressions that simply return the parent expression semantics are referred to as HEAD\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n(e.g. case in for Frozen). While this generalization applies to most dependency labels, several labels take a different logical form not listed here, some of which are discussed in Section 3.3. Sometimes the mapping of dependency label to lambda expression may depend on surrounding part-of-speech tags or dependency labels. For example, amod acts as INVERT when the modifier is a verb (e.g. in running horse), and as MERGE when the modifier is an adjective (e.g. in beautiful horse).3\nComposition The final logical form is computed by beta-reduction, treating expressions of the form (f x y) as the function f applied to the arguments x and y. For example, (dobj won (det Oscar an)) results in λx.∃z.won(xe)∧Oscar(za)∧ arg2(xe,za) when the expression for dobj is applied to those for won and (det Oscar an). Figure 1(c) shows the logical form for the s-expression in Figure 1(b)."
    }, {
      "heading" : "3 UDEPLAMBDA",
      "text" : "We now introduce UDEPLAMBDA, a semantic interface for Universal Dependencies.4 Whereas DEPLAMBDA only applies to English Stanford Dependencies, UDEPLAMBDA takes advantage of the cross-lingual nature of UD to facilitate an (almost) language independent semantic interface. This is accomplished by restricting the binarization, substitution, and composition steps described above to rely solely on information encoded in the UD representation. Importantly, UDEPLAMBDA is designed to not rely on lexical forms in a language to assign lambda expressions, but only on information contained in dependency labels and postags.\nHowever, some linguistic phenomena are language specific (e.g. pronoun-dropping) or meaning specific (e.g. every and the in English have very different semantics, despite being both determiners) and are not encoded in the UD schema. Furthermore, some cross-linguistic phenomena, such as long-distance dependencies, are not part of the core UD representation. To circumvent this limitation, a simple enhancement step enriches the original UD representation before binarization takes place (Section 3.1). This step adds to the dependency tree missing syntactic information and long-distance dependencies, thereby creating a\n3We use Tregex (Levy and Andrew, 2006) for substitution mappings and Cornell SPF (Artzi, 2013) as the lambdacalculus implementation. For example, in running horse, the tregex /label:amod/=target < /postag:verb/ matches amod to its INVERT expression λ f gx.∃y. f (x)∧g(y)∧ amodi(ye,xa).\n4In what follows, all references to UD are to UD v1.3.\ngraph. Whereas DEPLAMBDA is not able to handle graph-structured input, UDEPLAMBDA is designed to work directly with dependency graphs (Section 3.2). Finally, the representation of several linguistic constructions differ between UD and SD, which requires different handling in the semantic interface (Section 3.3)."
    }, {
      "heading" : "3.1 Enhancement",
      "text" : "Both Schuster and Manning (2016) and Nivre et al. (2016) note the necessity of an enhanced UD representation to enable semantic applications. However, such enhancements are currently only available for a subset of languages in UD. Instead, we rely on a small number of enhancements for our main application—semantic parsing for questionanswering—with the hope that this step can be replaced by an enhanced UD representation in the future. Specifically, we define three kinds of enhancements: (1) long-distance dependencies; (2) types of coordination; and (3) refined question word tags.\nFirst, we identify long-distance dependencies in relative clauses and control constructions. We follow Schuster and Manning (2016) and find these using the labels acl (relative) and xcomp (control). Figure 2(a) shows the long-distance dependency in the sentence Anna wants to marry Kristoff. Here, marry is provided with its missing nsubj (dashed arc). Second, UD conflates all coordinating constructions to a single dependency label, conj. To obtain the correct coordination scope, we refine conj to conj:verb, conj:vp, conj:sentence, conj:np, and conj:adj, similar to Reddy et al. (2016). Finally, unlike the PTB tags (Marcus et al., 1993) used by SD, the UD part-of-speech tags do\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nnot distinguish question words. Since these are crucial to question-answering, we use a small lexicon to refine the tags for determiners (DET), adverbs (ADV) and pronouns (PRON) to DET:WH, ADV:WH and PRON:WH, respectively. Specifically, we use a list of 12 (English), 14 (Spanish) and 35 (German) words, respectively. This is the only part of UDEPLAMBDA that relies on language-specific information. We hope that, as the coverage of morphological features in UD improves, this refinement can be replaced by relying on morphological features, such as the interrogative feature (INT)."
    }, {
      "heading" : "3.2 Graph Structures and BIND",
      "text" : "To handle graph structures that may result from the enhancement step, such as those in Figure 2(a), we propose a variable-binding mechanism that differs from that of DEPLAMBDA. First, each longdistance dependency is split into independent arcs as shown in Figure 2(b). Here, Ω is a placeholder for the subject of marry, which in turn corresponds to Anna as indicated by the binding of Ω via the pseudo-label BIND. We treat BIND like an ordinary dependency label with semantics MERGE and process the resulting tree as usual, via the s-expression: (nsubj (xcomp wants (nsubj (mark\n(dobj marry Kristoff) to) Ω) (BIND Anna Ω)) ,\nwith the lambda-expression substitutions: wants, marry ∈ EVENT; to ∈ FUNCTIONAL; Anna, Kristoff ∈ ENTITY; mark ∈ HEAD; BIND ∈ MERGE; xcomp = λ f gx.∃y. f (x)∧g(y)∧xcomp(xe,ye) .\nThese substitutions are based solely on unlexicalized context. For example, the part-of-speech tag PROPN of Anna invokes an ENTITY expression.\nThe placeholder Ω has semantics λx.EQ(x,ω), where EQ(u,ω) is true iff u and ω are equal (have the same denotation), which unifies the subject variable of wants with the subject variable of marry.\nAfter substitution and composition, we get: λz.∃xywv.wants(ze)∧Anna(xa)∧ arg1(ze,xa)∧ EQ(x,ω) ∧marry(ye)∧xcomp(ze,ye)∧ arg1(ye,va)∧ EQ(v,ω) ∧ Kristoff(wa)∧ arg2(ye,wa) ,\nThis expression may be simplified further by replacing all occurrences of v with x and removing the unification predicates EQ, which results in: λz.∃xyw.wants(ze)∧Anna(xa)∧ arg1(ze,xa) ∧marry(ye)∧xcomp(ze,ye)∧ arg1(ye,xa) ∧ Kristoff(wa)∧ arg2(ye,wa) .\nThis expression encodes the fact that Anna is the arg1 of the marry event, as desired. DEPLAMBDA, in contrast, cannot handle graph-structured input,\nsince it lacks a principled way of generating sexpressions from graphs. Even given the above s-expression, BIND in DEPLAMBDA is defined in a way such that the composition fails to unify v and x, which is crucial for the correct semantics. Moreover, the definition of BIND in DEPLAMBDA does not have a formal interpretation within the lambda calculus, unlike ours."
    }, {
      "heading" : "3.3 Linguistic Constructions",
      "text" : "Below, we highlight the most pertinent differences between UDEPLAMBDA and DEPLAMBDA, stemming from the different treatment of various linguistic constructions in UD versus SD.\nPrepositional Phrases UD uses a content-head analysis, in contrast to SD, which treats function words as heads of prepositional phrases, Accordingly, the s-expression for the phrase president in 2009 is (nmod president (case 2009 in)) in UDEPLAMBDA and (prep president (pobj in 2009)) in DEPLAMBDA. To achieve the desired semantics, λx.∃y.president(xa)∧president event(xe)∧\narg1(xe,xa)∧2009(ya)∧prep.in(xe,ya) ,\nDEPLAMBDA relies on an intermediate logical form that requires some post-processing, whereas UDEPLAMBDA obtains the desired logical form directly through the following entries: in ∈ FUNCTIONAL; 2009 ∈ ENTITY; case ∈ HEAD; president = λx.president(xa)∧president event(xe) ∧arg1(xe,xa) ; nmod = λ f gx.∃y. f (x)∧g(y)∧nmod.in(xe,ya) .\nOther nmod constructions, such as possessives (nmod:poss), temporal modifiers (nmod:tmod) and adverbial modifiers (nmod:npmod), are handled similarly. Note how the common noun president, evokes both entity and event predicates above.\nPassives DEPLAMBDA gives special treatment to passive verbs, identified by the fine-grained partof-speech tags in the PTB tag together with dependency context. For example, An Oscar was won is analyzed as λx.won.pass(xe)∧Oscar(ya)∧ arg1(xe,ya), where won.pass represents a passive event. However, UD does not distinguish between active and passive forms.5 While the labels nsubjpass or auxpass indicate passive constructions, such clues are sometimes missing, such as in reduced relatives. We therefore opt to not have separate entries for passives, but aim to produce identical logical forms for active and passive forms when\n5UD encodes voice as a morphological feature, but most syntactic analyzers do not produce this information yet.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nlanguage target people\nx e1 y e2 Ghana speak .arg2 speak .arg1 people .arg1 people .nmod.in\ntyp e\ntyp e\n(a) English\nsprache target\nx e1 Ghana gesprochen\n.arg2 gesprochen .nmod.in\ntyp e\n(b) German\nlengua target\nx e1 Ghana lengua .arg1 lengua .nmod.de\ntyp e\n(c) Spanish\nlanguage .human language\ntarget\nx m Ghana location.country\n.official language.2 location.country .official language.1\ntyp e\n(d) Freebase\nFigure 3: The ungrounded graphs for What language do the people in Ghana speak?, Welche Sprache wird in Ghana gesprochen? and Cuál es la lengua de Ghana?, and the corresponding grounded graph.\npossible (for example, by treating nsubjpass as direct object). With the following entries, won ∈ EVENT; an, was ∈ FUNCTIONAL; auxpass ∈ HEAD; nsubjpass = λ f gx.∃y. f (x)∧g(y)∧ arg2(xe,ya) ,\nthe lambda expression for An Oscar was won becomes λx.won(xe)∧Oscar(ya)∧arg2(xe,ya), identical to that of its active form. However, not having a special entry for passive verbs may have undesirable side-effects. For example, in the reducedrelative construction Pixar claimed the Oscar won for Frozen, the phrase the Oscar won ... will receive the semantics λx.Oscar(ya)∧won(xe)∧ arg1(xe,ya), which differs from that of an Oscar was won. We leave it to the target application to disambiguate the interpretation in such cases.\nLong-Distance Dependencies As discussed in Section 3.2, we handle long-distance dependencies evoked by clausal modifiers (acl) and control verbs (xcomp) with the BIND mechanism, whereas DEPLAMBDA cannot handle control constructions. For xcomp, as seen earlier, we use the mapping λ f gx.∃y. f (x)∧g(y)∧xcomp(xe,ye). For acl we use λ f gx.∃y. f (x)∧ g(y), to conjoin the main clause and the modifier clause. However, not all acl clauses evoke long-distance dependencies, e.g. in the news that Disney won an Oscar, the clause that Disney won an Oscar is a subordinating conjunction of news. In such cases, we instead assign acl the INVERT semantics.\nQuestions Question words are marked with the enhanced part-of-speech tags DET:WH, ADV:WH and PRON:WH, which are all assigned the semantics λx.${word}(xa)∧ TARGET(xa). The predicate TARGET indicates that xa represents the variable of interest, that is the answer to the question."
    }, {
      "heading" : "3.4 Limitations",
      "text" : "In order to achieve language independence, UDEPLAMBDA has to sacrifice semantic specificity, since in many cases the semantics is carried by lexical information. Consider the sentences John broke the\nwindow and The window broke. Although it is the window that broke in both cases, our inferred logical forms do not canonicalize the relation between broke and window. To achieve this, we would have to make the substitution of nsubj depend on lexical context, such that when window occurs as nsubj with broke, the predicate arg2 is invoked rather than arg1. We do not address this problem, and leave it to the target application to infer that arg2 and arg1 have the same semantic function in these cases. We anticipate that the ability to make such lexicalized semantic inferences in a task-agnostic cross-lingual framework would be highly useful and a crucial avenue for future work on universal semantics.\nOther constructions that require lexical information are quantifiers like every, some and most, negation markers like no and not, and intentional verbs like believe and said. UD does not have special labels to indicate these. Although not currently implemented, we discuss how to handle quantifiers in this framework in the supplementary material."
    }, {
      "heading" : "4 Cross-lingual Semantic Parsing",
      "text" : "To study the multilingual nature of UDEPLAMBDA, we conduct an empirical evaluation on question answering against Freebase in three different languages: English, Spanish, and German. Before discussing the details of this experiment, we briefly outline the semantic parsing framework employed."
    }, {
      "heading" : "4.1 Semantic Parsing as Graph Matching",
      "text" : "UDEPLAMBDA generates ungrounded logical forms that are independent of any knowledge base, such as Freebase. We use GRAPHPARSER (Reddy et al., 2016) to map these logical forms to their grounded Freebase graphs, via corresponding ungrounded graphs. Figures 3(a) to 3(c) show the ungrounded graphs corresponding to logical forms from UDEPLAMBDA, each grounded to the same Freebase graph in Figure 3(d). Here, rectangles denote entities, circles denote events, rounded rectangles denote entity types, and edges between events\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nWebQuestions\nen What language do the people in Ghana speak? de Welche Sprache wird in Ghana gesprochen? es ¿Cuál es la lengua de Ghana?\nen Who was Vincent van Gogh inspired by? de Von wem wurde Vincent van Gogh inspiriert? es ¿Qué inspiró a Van Gogh?\nGraphQuestions\nen NASA has how many launch sites? de Wie viele Abschussbasen besitzt NASA? es ¿Cuántos sitios de despegue tiene NASA?\nen Which loudspeakers are heavier than 82.0 kg? de Welche Lautsprecher sind schwerer als 82.0 kg? es ¿Qué altavoces pesan más de 82.0 kg?\nTable 1: Example questions and their translations.\nand entities denote predicates or Freebase relations. Finally, the TARGET node represents the set of values of x that are consistent with the Freebase graph, that is the answer to the question.\nGRAPHPARSER treats semantic parsing as a graph-matching problem with the goal of finding the Freebase graphs that are structurally isomorphic to an ungrounded graph and rank them according to a model. To account for structural mismatches, GRAPHPARSER uses two graph transformations: CONTRACT and EXPAND. In Figure 3(a) there are two edges between x and Ghana. CONTRACT collapses one of these edges to create a graph isomorphic to Freebase. EXPAND, in contrast, adds edges to connect the graph in the case of disconnected components. The search space is explored by beam search and model parameters are estimated with the averaged structured perceptron (Collins, 2002) from training data consisting of question-answer pairs, using answer F1-score as the objective."
    }, {
      "heading" : "4.2 Datasets",
      "text" : "We evaluate our approach on two public benchmarks of question answering against Freebase: WebQuestions (Berant et al., 2013), a widely used benchmark consisting of English questions and their answers, and GraphQuestions (Su et al., 2016), a recently released dataset of English questions with both their answers and grounded logical forms. While WebQuestions is dominated by simple entityattribute questions, GraphQuestions contains a large number of compositional questions involving aggregation (e.g. How many children of Eddard Stark were born in Winterfell? ) and comparison (e.g. In which month does the average rainfall of\nNew York City exceed 86 mm? ). The number of training, development and test questions is 2644, 1134, and 2032, respectively, for WebQuestions and 1794, 764, and 2608 for GraphQuestions.\nTo support multilingual evaluation, we created translations of WebQuestions and GraphQuestions to German and Spanish.6 For WebQuestions two professional annotators were hired per language, while for GraphQuestions we used a trusted pool of 20 annotators per language (with a single annotator per question). Examples of the original questions and their translations are provided in Table 1."
    }, {
      "heading" : "4.3 Implementation Details",
      "text" : "Here we provide details on the syntactic analyzers employed, our entity resolution algorithm, and the features used by the grounding model.\nDependency Parsing The English, Spanish, and German Universal Dependencies (UD) treebanks (v1.3; Nivre et al 2016) were used to train part of speech taggers and dependency parsers. We used a bidirectional LSTM tagger (Plank et al., 2016) and a bidirectional LSTM shift-reduce parser (Kiperwasser and Goldberg, 2016). Both the tagger and parser require word embeddings. For English, we used GloVe embeddings (Pennington et al., 2014) trained on Wikipedia and the Gigaword corpus.7 For German and Spanish, we used SENNA embeddings (Collobert et al., 2011; Al-Rfou et al., 2013) trained on Wikipedia corpora (589M words German; 397M words Spanish).8 Measured on the UD test sets, the tagger accuracies are 94.5 (English), 92.2 (German), and 95.7 (Spanish), with corresponding labeled attachment parser scores of 81.8, 74.7, and 82.2.\nEntity Resolution We follow Reddy et al. (2016) and resolve entities in three steps: (1) potential entity spans are identified using seven handcrafted part-of-speech patterns; (2) each span is associated\n6Translations will be publicly released upon publication. 7http://nlp.stanford.edu/projects/glove/. 8https://sites.google.com/site/rmyeid/projects/polyglot.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nWebQuestions GraphQuestions Method en de es en de es\nSINGLEEVENT 47.6 43.9 45.0 15.9 8.3 11.2 DEPTREE 47.8 43.9 44.5 15.8 7.9 11.0 CCGGRAPH 48.4 – – 15.9 – – UDEPLAMBDA 48.3 44.2 45.7 17.6 9.0 12.4\nTable 3: F1-scores on the test for models trained on the training set (excluding the development set).\nwith potential Freebase entities according to the Freebase/KG API;9 and (3) the 10-best entity linking lattices, scored by a structured perceptron, are input to GRAPHPARSER, leaving the final disambiguation to the semantic parsing problem. Table 2 shows the 1-best and 10-best entity disambiguation F1-scores for each language and dataset.10\nFeatures We use features similar to Reddy et al. (2016): basic features of words and Freebase relations, and graph features crossing ungrounded events with grounded relations, ungrounded types with grounded relations, and ungrounded answer type crossed with a binary feature indicating if the answer is a number. In addition, we add features encoding the semantic similarity of ungrounded events and Freebase relations. Specifically, we used the cosine similarity of the translation-invariant embeddings of Huang et al. (2015).11"
    }, {
      "heading" : "4.4 Comparison Systems",
      "text" : "We compared UDEPLAMBDA to prior work and three versions of GRAPHPARSER that operate on different representations: entity cliques, dependency trees, and CCG-based semantic derivations.\nSINGLEEVENT This model resembles the learning-to-rank model of Bast and Haussmann (2015). An ungrounded graph is generated by connecting all entities in the question with the TARGET node, representing a single event. Note that this baseline cannot handle compositional questions, or those with aggregation or comparison.\nDEPTREE An ungrounded graph is obtained directly from the original dependency tree. An event is created for each parent and its dependents in the tree. Each dependent is linked to this event with an edge labeled with its dependency relation, while the\n9http://developers.google.com/freebase/. 10Due to the recent Freebase API shutdown, we used the KG API for GraphQuestions. We observed that this leads to inferior entity linking results compared to those of Freebase.\n11http://128.2.220.95/multilingual/data/.\nparent is linked to the event with an edge labeled arg0. If a word is a question word, an additional TARGET predicate is attached to its entity node.\nCCGGRAPH This is the CCG-based semantic representation of Reddy et al. (2014). Note that this baseline exists only for English."
    }, {
      "heading" : "4.5 Results",
      "text" : "Table 3 shows the performance of GRAPHPARSER with these different representations. Here and in what follows, we use average F1-score of predicted answers (Berant et al., 2013) as the evaluation metric. We first observe that UDEPLAMBDA consistently outperforms the SINGLEEVENT and DEPTREE representations in all languages.12\nFor English, performance is almost on par with CCGGRAPH, which suggests that UDEPLAMBDA does not sacrifice too much specificity for universality. With both datasets, results are lower for German compared to Spanish. This agrees with the lower performance of the syntactic parser on the German portion of the UD treebank. Finally, while these results confirm that GraphQuestions is much harder compared to WebQuestions, we note that both datasets predominantly contain single-hop questions, as indicated by the competitive performance of SINGLEEVENT on both datasets.\nTable 4 compares UDEPLAMBDA with previ-\n12For the DEPTREE model, we CONTRACT each multihop path between the question word and an entity to a single edge. Without this constraint, DEPTREE F1 results are 45.5 (en), 42.9 (de), and 44.2 (es) on WebQuestions, and 11.0 (en), 6.6 (de), and 2.6 (es) on GraphQuestions.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nously published models which exist only for English and have been mainly evaluated on WebQuestions. These are either symbolic like ours (first block) or employ neural networks (second block). Results for models using additional task-specific training resources, such as ClueWeb09, Wikipedia, or SimpleQuestions (Bordes et al., 2015) are shown in parentheses. On GraphQuestions, we achieve a new state-of-the-art result with a gain of 4.8 F1points over the previously reported best result. On WebQuestions we are 2.1 points below the best model using comparable resources, and 3.8 points below the state of the art. Most related to our work is the English-specific system of Reddy et al. (2016). We attribute the 0.8 point difference in F1score to their use of the more fine-grained PTB tag set and Stanford Dependencies."
    }, {
      "heading" : "5 Related Work",
      "text" : "Our work continues the long tradition of building logical forms from syntactic representations initiated by Montague (1973). The literature is rife with attempts to develop semantic interfaces for HPSG (Copestake et al., 2005), LFG (Kaplan and Bresnan, 1982; Dalrymple et al., 1995; Crouch and King, 2006), TAG (Kallmeyer and Joshi, 2003; Gardent and Kallmeyer, 2003; Nesson and Shieber, 2006), and CCG (Steedman, 2000; Baldridge and Kruijff, 2002; Bos et al., 2004; Artzi et al., 2015). Unlike existing semantic interfaces, UDEPLAMBDA (like DEPLAMBDA) uses dependency syntax, taking advantage of recent advances in multilingual parsing (McDonald et al., 2013; Nivre et al, 2016).\nA common trend in previous work on semantic interfaces is the reliance on rich typed feature structures or semantic types coupled with strong type constraints, which can be very informative but unavoidably language specific. Creating rich semantic types from dependency trees which lack a typing system would be labor intensive and brittle in the face of parsing errors. Instead, UDEPLAMBDA relies on generic unlexicalized information present in dependency treebanks and uses a simple type system (one type for dependency labels, and one for words) along with a combinatory mechanism, which avoids type collisions. Earlier attempts at extracting semantic representations from dependencies have mainly focused on language-specific dependency representations (Spreyer and Frank, 2005; Simov and Osenova, 2011; Hahn and Meurers, 2011; Reddy et al., 2016; Falke et al., 2016;\nBeltagy, 2016), and multi-layered dependency annotations (Jakob et al., 2010; Bédaride and Gardent, 2011). In contrast, UDEPLAMBDA derives semantic representations for multiple languages in a common schema directly from Universal Dependencies. This work parallels a growing interest in creating other forms of multilingual semantic representations (Akbik et al., 2015; Vanderwende et al., 2015; White et al., 2016; Evang and Bos, 2016).\nWe evaluate UDEPLAMBDA on semantic parsing for question answering against a knowledge base. Here, the literature offers two main modeling paradigms: (1) learning of task-specific grammars that directly parse language to a grounded representation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015); and (2) converting language to a linguistically motivated taskindependent representation that is then mapped to a grounded representation (Kwiatkowski et al., 2013; Reddy et al., 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017). Our work belongs to the latter paradigm, as we map natural language to Freebase indirectly via logical forms. Capitalizing on natural-language syntax affords interpretability, scalability, and reduced duplication of effort across applications (Bender et al., 2015). Our work also relates to literature on parsing multiple languages to a common executable representation (Cimiano et al., 2013; Haas and Riezler, 2016). However, existing approaches (Kwiatkowksi et al., 2010; Jones et al., 2012; Jie and Lu, 2014) still map to the target meaning representations (more or less) directly."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We introduced UDEPLAMBDA, a semantic interface for Universal Dependencies, and showed that the resulting semantic representation can be used for question-answering against a knowledge base in multiple languages. We provided translations of benchmark datasets in German and Spanish, in the hope to stimulate further multilingual research on semantic parsing and question answering in general. We have only scratched the surface when it comes to applying UDEPLAMBDA to natural language understanding tasks. In the future, we would like to explore how this framework can benefit other tasks such as summarization and machine translation.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Universal Dependencies (UD) provides a cross-linguistically uniform syntactic representation, with the aim of advancing multilingual applications of parsing and natural language understanding. Reddy et al. (2016) recently developed a semantic interface for (English) Stanford Dependencies, based on the lambda calculus. In this work, we introduce UDEPLAMBDA, a similar semantic interface for UD, which allows mapping natural language to logical forms in an almost language-independent framework. We evaluate our approach on semantic parsing for the task of question answering against Freebase. To facilitate multilingual evaluation, we provide German and Spanish translations of the WebQuestions and GraphQuestions datasets. Results show that UDEPLAMBDA outperforms strong baselines across languages and datasets. For English, it achieves the strongest result to date on GraphQuestions, with competitive results on WebQuestions.",
    "creator" : "LaTeX with hyperref package"
  }
}