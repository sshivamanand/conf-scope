{
  "name" : "343.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016). Neural network models have been exploited due to their strength in non-sparse representation learning and non-linear power in feature combination, which have led to advances in many NLP tasks. So far, neural word segmentors have given comparable accuracies to the best statictical models.\nWith respect to non-sparse representation, character embeddings have been exploited as a foundation of neural word segmentors. They serve to reduce sparsity of character ngrams, allowing, for example, “猫(cat)躺(lie)在(in)墙角(corner)” to be connected with “狗(dog) 蹲(sit) 在(in) 墙 角(corner)” (Zheng et al., 2013), which is infeasible by using sparse one-hot character features. In\naddition to character embeddings, distributed representations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al., 2015; Zhang et al., 2016) have also been shown to improve segmentation accuracies.\nWith respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016). For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016).\nPrevious research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016). Pretraining can be regarded as one way of leveraging external resources to improve accuracies, which is practically highly useful and has become a standard practice in neural NLP. On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011), and making use of selfpredictions (Wang et al., 2011; Liu and Zhang, 2012). It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) and segmentation under different standards (Jiang et al., 2009). To our knowledge, such rich external information has not been systematically investigated for neural segmentation.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nS A\nhidden layer\noutput\n车 站 那 边\nw-k\nRecognized words Partial word Incoming chars\nw-2\n我 之前 去 过 火\nw-1 P c0 c1 . . . cm\n. . . . . . . . .\n. . .\nXW XP XC\nh\nFigure 1: Overall model.\nWe fill this gap by investigating rich external pretraining for neural segmentation. Following Cai and Zhao (2016) and Zhang et al. (2016), we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al., 2016; Zhou et al., 2015; Wiseman and Rush, 2016), which allows word information to be modelled explicitly. Different from previous work, we make our model conceptually simple and modular, so that the most important sub module, namely a five-character window context, can be pretrained using external data. We adopt a multi-task learning strategy (Collobert et al., 2011), casting each external source of information as a auxiliary classification task, sharing a five-character window network. After pretraining, the character window network is used to initialize the corresponding module in our segmentor.\nResults on 6 different benchmarks show that our method outperforms the best statistical and neural segmentation models consistently, giving the best reported results on 5 datasets in different domains and genres. Our code and models can be downloaded from GitHub.com/XXXXXXXXX"
    }, {
      "heading" : "2 Related Work",
      "text" : "Work on statistical word segmentation dates back to the 1990s (Sproat et al., 1996). State-of-the-art approaches include character sequence labeling models (Xue et al., 2003) using CRFs (Peng et al., 2004; Zhao et al., 2006) and max-margin structured models leveraging word features (Zhang and Clark, 2007; Sun et al., 2009; Sun, 2010). Semisupervised methods have been applied to both character-based and word-based models, exploring external training data for better segmentation (Sun and Xu, 2011; Wang et al., 2011; Liu and\nZhang, 2012; Zhang et al., 2013). Our work belongs to recent neural word segmentation.\nTo our knowledge, there has no work in the literature systematically investigating rich external resources for neural word segmentation training. Closest in spirit to our work, Sun and Xu (2011) empirically studied the use of various external resources for enhancing a statistical segmentor, including character mutual information, access variety information, punctuation and other statistical information. Their baseline is similar to ours in the sense that both character and word contexts are considered. On the other hand, their model is statistical while ours is neural. Consequently, they integrate external knowledge as features, while we integrate it by shared network parameters. Our results show a similar degree of error reduction compared to theirs by using external data.\nOur model inherits from previous findings on context representations, such as character windows (Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) and LSTMs (Chen et al., 2015b; Xu and Sun, 2016). Similar to Zhang et al. (2016) and Cai and Zhao (2016), we use word context on top of character context. However, words play a relatively less important role in our model, and we find that word LSTM, which has been used by all previous neural segmentation work, is unnecessary for our model. Our model is conceptually simpler and more modularised compared with Zhang et al. (2016) and Cai and Zhao (2016), allowing a central sub module, namely a five-character context window, to be pretrained."
    }, {
      "heading" : "3 Model",
      "text" : "Our segmentor works incrementally from left to right. At each step, the state consists of a sequence of words that have been fully recognized, denoted asW = [w−k, w−k+1, ..., w−1], a current partially recognized word P , and a sequence of next incoming characters, denoted as C = [c0, c1, ..., cm], as shown in Figure 1. Given an input sentence, W and P are initialized to [ ] and φ, respectively, and C contains all the input characters. At each step, a decision is made on c0, either appending it as a part of P , or seperating it as the beginning of a new word. The incremental process repeats until C is empty and P = φ again. Formally, the process can be regarded as a state-transition process, where a state is a tuple S = 〈W,P,C〉, and the transition actions include SEP and APP, as shown\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nAxiom: S = 〈[ ], φ, C〉, V = 0 Goal: S = 〈W,φ, [ ]〉, V = Vfinal\nSEP: S = 〈W,P, c0|C〉, V S ′ = 〈W |P, c0, C〉, V + Score(S, SEP)\nAPP: S = 〈W,P, c0|C〉, V S ′ = 〈W,P ⊕ c0, C〉, V + Score(S,APP)\nFigure 2: Deduction system, where ⊕ denotes string concatenation.\nby the deduction system in Figure 21. In the figure, V denotes the score of a state, given by a neural network model. The score of the initial state (i.e. axiom) is 0, and the score of a non-axiom state is the sum of scores of all incremental decisions resulting in the state. Similar to Zhang et al. (2016) and Cai and Zhao (2016), our model is a global structural model, using the overall score to disambiguate states, which correspond to sequences of inter-dependent transition actions.\nDifferent from previous work, the structure of our scoring network is shown in Figure 1. It consists of three main layers. On the bottom is a representation layer, which derives dense representations XW , XP and XC for W,P and C, respectively. We compare various distributed representations and neural network structures for learning XW , XP and XC , detailed in Section 3.1. On top of the representation layer, we use a hidden layer to merge XW , XP and XC into a single vector\nh = tanh(WhW ·XW +WhP ·XP +WhC ·XC+bh) (1) The hidden feature vector h is used to represent the state S = 〈W,P,C〉, for calculating the scores of the next action. In particular, a linear output layer with two nodes is employed:\no = Wo · h+ bo (2)\nThe first and second node of o represent the scores of SEP and APP given S, namely Score(S, SEP), Score(S,APP) respectively."
    }, {
      "heading" : "3.1 Representation Learning",
      "text" : "Characters. We investigate two different approaches to encoding incoming characters, namely\n1An end of sentence symbol 〈/s〉 is added to the input so that the last partial word can be put onto W as a full word before segmentation finishes.\na window approach and an LSTM approach. For the former, we follow prior methods (Xue et al., 2003; Pei et al., 2014), using five-character window [c−2, c−1, c0, c1, c2] to represent incoming characters. Shown in Figure 3, multi-layer perceptron (MLP) is employed to derive a five-character window vector DC from single-character vector representations Vc−2 , Vc−1 , Vc0 , Vc1 , Vc2 .\nDC = MLP([Vc−2 ;Vc−1 ;Vc0 ;Vc1 ;Vc2 ]) (3)\nFor the latter, we follow recent work (Chen et al., 2015b; Zhang et al., 2016), using a bidirectional LSTM to encode input character sequence.2 In particular, the bi-directional LSTM hidden vector [ ←− hC(c0); −→ hC(c0)] of the next incoming character c0 is used to represent the coming characters [c0, c1, ...] given a state. Intuitively, a five-character window provides a local context from which the meaning of the middle character can be better disambiguated. LSTM, on the other hand, captures larger contexts, which can contain more useful clues for dismbiguation but also irrelevant information. It is therefore interesting to investigate a combination of their strengths, by first deriving a locally-disambiguated version of c0, and then feed it to LSTM for a globally disambiguated representation.\nNow with regard to the single-character vector representation Vci(i ∈ [−2, 2]), we follow previous work and consider both character embedding ec(ci) and character-bigram embedding eb(ci, ci+1) , investigating the effect of each on the accuracies. When both ec(ci) and eb(ci, ci+1) are utilized, the concatenated vector is taken as Vci . Partial Word. We take a very simple approach to representing the partial word P , by using the embedding vectors of its first and last characters, as well as the embedding of its length. Length embeddings are randomly initialized and then tuned in model training. XP has relatively less influence on the empirical segmentation accuracies.\nXP = [e c(P [0]); ec(P [−1]); el(LEN(P ))] (4)\nWord. Similar to the character case, we investigate two different approaches to encoding incoming characters, namely a window approach and an LSTM approach. For the former, we follow prior\n2The LSTM variation with coupled input and forget gate but without peephole connections is applied (Gers and Schmidhuber, 2000)\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n. . .\n. . . . . . . . . .  .  . . . .\nMLP\n... ...... punc. silver hete. POS\nshared parameters\nmain training\npretrainingBi-LSTM\nS A\nhidden layer\noutput\n... ... ...\n...\nXW XP XC\nh\nDC\nVc-2 Vc-1 Vc0 Vc1 Vc2\nFigure 3: Shared character representation.\nmethods (Zhang and Clark, 2007; Sun, 2010), using the two-word window [w−2, w−1] to represent recognized words. A hidden layer is employed to derive a two-word vector XW from single word embeddings ew(w−2) and ew(w−1).\nXW = tanh(Ww[e w(w−2); e w(w−1)] + bw) (5)\nFor the latter, we follow Zhang et al. (2016) and Cai and Zhao (2016), using an uni-directional LSTM on words that have been recognized."
    }, {
      "heading" : "3.2 Pretraining",
      "text" : "Neural network models for NLP benefit from pretraining of word/character embeddings, learning distributed sementic information from large raw texts for reducing sparsity. The three basic elements in our neural segmentor, namely characters, character bigrams and words, can all be pretrained over large unsegmented data. We pretrain the fivecharacter window network in Figure 3 as an unit, learning the MLP parameter together with character and bigram embeddings. We consider four types of commonly explored external data to this end, all of which have been studied for statistical word segmentation, but not for neural network segmentors. Raw Text. Although raw texts do not contain explicit word boundary information, statistics such as mutual information between consecutive characters can be useful features for guiding segmentation (Sun and Xu, 2011). For neural segmentation, these distributional statistics can be implicitly learned by pretraining character embeddings. We therefore consider a more explicit clue for pretraining our character window network, namely punctuations (Li and Sun, 2009).\nPunctuation can serve as a type of explicit markup (Spitkovsky et al., 2010), indicating that the two characters on its left and right belong to two\ndifferent words. We leverage this source of information by extracting character five-grams excluding punctuation from raw sentences, using them as inputs to classify whether there is punctuation before middle character. Denoting the resulting five character window as [c−2, c−1, c0, c1, c2], the MLP in Figure 3 is used to derive its representation DC , which is then fed to a softmax layer for binary classification:\nP (punc) = softmax(Wpunc ·DC + bpunc) (6)\nHere P (punc) indicates the probability of a punctuation mark existing before c0. Standard backpropagation training of the MLP in Figure 3 can be done jointly with the training of Wpunc and bpunc. After such training, the embedding Vci and MLP values can be used to initialize the corresponding parameters for DC in the main segmentor, before its training. Automatically Segmented Text. Large texts automatically segmented by a baseline segmentor can be used for self-training (Liu and Zhang, 2012) or deriving statistical features (Wang et al., 2011). We adopt a simple strategy, taking automatically segmented text as silver data to pretrain the five-character window network. Given [c−2, c−1, c0, c1.c2], DC is derived using the MLP in Figure 3, and then used to classify the segmentation of c0 into B(begining)/M(middle)/E(end)/S(single character word) labels.\nP (silver) = softmax(Wsilv ·DC + bsilv) (7)\nHere Wsilv and bsilv are model parameters. Training can be done in the same way as training with punctuation. Heterogenous Training Data. Multiple segmentation corpora exist for Chinese, with different segmentation granularities. There has been investigation on leveraging two corpora under different annotation standards to improve statistical segmentation (Jiang et al., 2009). We try to utilize heterogenous treebanks by taking an external treebank as labeled data, training a B/M/E/S classifier for the character windows network.\nP (hete) = softmax(Whete ·DC + bhete) (8)\nPOS Data. Previous research has shown that POS information is closely related to segmentation (Ng\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nParamater Value Paramater Value α 0.01 size(ec) 50 λ 10−8 size(eb) 50 p 0.2 size(ew) 50 η 0.2 size(el) 20 MLP layer 2 size(XC) 150 beam B 8 size(XP ) 50 size(h) 200 size(XW ) 100\nTable 1: Hyper-parameter values.\nand Low, 2004; Zhang and Clark, 2008). We verify the utility of POS information for our segmentor by pretraining a classifier that predicts the POS on each character, according to the character window representation DC . In particular, given [c−2, c−1, c0, c1, c2], the POS of the word that c0 belongs to is used as the output.\nP (pos) = softmax(Wpos ·DC + bpos) (9)\nMultitask Learning. While each type of external training data can offer one source of segmentation information, different external data can be complimentary to each other. We aim to inject all sources of information into the character window representation DC by using it as a shared representation for different classification tasks. Neural model have been shown capable of doing multi-task learning via parameter sharing (Collobert et al., 2011). Shown in Figure 3, in our case, the output layer for each task is independent, but the hidden layer DC and all layers below DC are shared. We randomly sample from different training sources according to their sizes, performing mixed training."
    }, {
      "heading" : "4 Decoding and Training",
      "text" : "To train the main segmentor, we adopt the global transition-based learning and beam-search strategy of Zhang and Clark (2011). For decoding, standard beam search is used, where the B best partial output hypotheses at each step are maintained in an agenda. Initially, the agenda contains only the start state. At each step, all hypotheses in the agenda are expanded, by applying all possible actions and B highest scored resulting hypotheses are used as the agenda for the next step.\nFor training, the same decoding process is applied to each training example (xi, yi). At step j, if the gold-standard sequence of transition actions yij falls out of the agenda, max-margin update is\nAlgorithm 1: Training Input : (xi, yi) Parameters: Θ Process: agenda← (S = 〈[ ], φ,Xi〉, V = 0) for j in [0:LEN(Xi)] do\nbeam = [] for ŷ in agenda do\nŷ′ = ACTION(ŷ, SEP) ADD(ŷ′, beam) ŷ′ = ACTION(ŷ, APP) ADD(ŷ′, beam) end agenda← TOP(beam, B) if yij /∈ agenda then\nŷj = BESTIN(agenda) UPDATE(yij , ŷj ,Θ) return\nend end ŷ = BESTIN(agenda) UPDATE(yi, ŷ,Θ) return\nperformed by taking the current best hypothesis ŷj in the beam as a negative example, and yij as a positive example. The loss function is\nl(ŷj , y i j) = (score(ŷj) + η · δ(ŷj , yij))\n− score(yij), (10)\nwhere δ(ŷj , yij) is the number of incorrect local decisions in ŷj , and η controls the score margin.\nThe strategy above is early-update (Collins and Roark, 2004). On the other hand, if the goldstandard hypothesis does not fall out of the agenda until the full sentence has been segmented, yet does not score the highest in the agenda after the last step, a final update is made between the highest scored hypothesis ŷ in the agenda and the goldstandard yi, using exactly the same loss function. Pseudocode for the online learning algorithm is shown in Algorithm 1.\nWe use Adagrad (Duchi et al., 2011) to optimize model parameters, with an initial learning rate α. L2 regularization and dropout (Srivastava et al., 2014) on input are used to reduce overfitting, with a L2 weight λ and a dropout rate p. All the parameters in our model are randomly initialized to a value (−r, r), where r = √ 6.0\nfanin+fanout (Ben-\ngio, 2012). We fine-tune character and character\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nSource #Chars #Words #Sents Raw data Gigaword 116.5m – – Auto seg Gigaword 398.2m 238.6m 12.04m\nHete. People’s Daily 10.14m 6.17m 104k POS People’s Daily 10.14m 6.17m 104k\nTable 2: Statistics of external data.\nbigram embeddings, but not word embeddings, acccording to Zhang et al. (2016)."
    }, {
      "heading" : "5 Experiments",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Settings",
      "text" : "Data. We use Chinese Treebank 6.0 (CTB6) (Xue et al., 2005) as our main dataset. Training, development and test set splits follow previous work (Zhang et al., 2014). In order to verify the robustness of our model, we additionally use SIGHAN 2005 bake-off (Emerson, 2005) and NLPCC 2016 shared task for Weibo segmentation (Qiu et al., 2016) as test datasets, where the standard splits are used. For pretraining embedding of words, characters and character bigrams, we use Chinese Gigaword (simplified Chinese sections)3, automatically segmented using ZPar 0.6 off-theshelf (Zhang and Clark, 2007), the statictics of which are shown in Table 2.\nFor pretraining character representations, we extract punctuation classification data from the Gigaword corpus, and use the word-based ZPar and a standard character-based CRF model (Tseng et al., 2005) to obtain automatic segmentation results. We compare pretraining using ZPar results only and using results that both segmentors agree on. For heterogenous segmentation corpus and POS data, we use a People’s Daily corpus of 5 months4. Statistics are listed in Table 2. Evaluation. The standard word precision, recall and F1 measure (Emerson, 2005) are used to evaluate segmentation performances. Hyper-parameter Values. We adopt commonly used values for most hyperparameters, but tuned the sizes of hidden layers on the development set. The values are summarized in Table 1."
    }, {
      "heading" : "5.2 Development Experiments",
      "text" : "We perform development experiments to verify the usefulness of various context representations, network configurations and different pretraining methods, respectively.\n3https://catalog.ldc.upenn.edu/LDC2011T13 4http://www.icl.pku.edu.cn/icl res"
    }, {
      "heading" : "5.2.1 Context Representations",
      "text" : "The influence of character and word context representations are empirically studied by varying the network structures forXC andXW in Figure 1, respectively. All the experiments in this section are performed using a beam size of 8. Character Context. We fix the word representation XW to a 2-word window and compare different character context representations. The results are shown in Table 3, where “no char” represents our model without XC , “5-char window” represents a five-character window context, “char LSTM” represents character LSTM context and “5-char window + LSTM” represents a combination, detailed in Section 3.1. “-char emb” and “- bichar emb” represent the combined window and LSTM context without character and characterbigram information, respectively.\nAs can be seen from the table, without character information, the F-score is 84.62%, demonstrating the necessity of character contexts. Using window and LSTM representations, the Fscores increase to 95.41% and 95.51%, respectively. A combination of the two lead to further improvement, showing that local and global character contexts are indeed complementary, as hypothesized in Section 3.1. Finally, by removing character and character-bigram embeddings, the F-score decreases to 95.20% and 94.27%, respectively, which suggests that character bigrams are more useful compared to character unigrams. This is likely because they contain more distinct tokens and hence offer a larger parameter space. Word Context. The influence of various word contexts are shown in Table 4. Without using word information, our segmentor gives an F-score of 95.66% on the development data. Using a context of only w−1 (1-word window), the F-measure increases to 95.78%. This shows that word contexts are far less important in our model compared to character contexts, and also compared to word contexts in previous word-based segmentors\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nWord P R F No word 95.50 95.83 95.66 1-word window 95.70 95.85 95.78 2-word window 95.77 95.95 95.86 3-word window 95.80 95.85 95.83\nword LSTM 95.71 95.97 95.84 2-word window+LSTM 95.74 95.95 95.84\nTable 4: Influence of word contexts.\n(Zhang et al., 2016; Cai and Zhao, 2016). This is likely due to the difference in our neural network structures, and that we fine-tune both character and character bigram embeddings, which significantly enlarges the adjustable parameter space as compared with Zhang et al. (2016). The fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word-based neural segmentors do not outperform the best character-based models by large margins. Given that character context is what we pretrain, our model relies more heavily on them.\nWith both w−2 and w−1 being used for the context, the F-score further increases to 95.86%, showing that a 2-word window is useful by offering more contextual information. On the other hand, when w−3 is also considered, the F-score does not improve further. This is consistent with previous findings of statistical word segmentation (Zhang and Clark, 2007), which adopt a 2-word context. Interestingly, using a word LSTM does not bring further improvements, even when it is combined with a window context. This suggests that global word contexts may not offer crucial additional information compared with local word contexts. Intuitively, words are significantly less polysemous compared with characters, and hence can serve as effective contexts even if used locally, to supplement a more crucial character context."
    }, {
      "heading" : "5.2.2 Stuctured Learning and Inference",
      "text" : "We verify the effectiveness of structured learning and inference by measuring the influence of beam size on the baseline segmentor. Figure 4 shows the F-scores against different numbers of training iterations with beam size 1,2,4,8 and 16, respectively. When the beam size is 1, the inference is local and greedy. As the size of the beam increases, more global structural ambiguities can be resolved since learning is designed to guide search. A contrast between beam sizes 1 and 2 demonstrates the usefulness of structured learning and inference. As\nPretrain P R F ER% Baseline 95.77 95.95 95.86 0\n+Punc. pretrain 96.36 96.13 96.25 -9.4 +Auto-seg pretrain 96.23 96.29 96.26 -9.7 +Heter-seg pretrain 96.28 96.27 96.27 -9.9\n+POS pretrain 96.16 96.28 96.22 -8.7 +Multitask pretrain 96.49 96.39 96.44 -14.0\nTable 5: Influence of pretraining.\nthe beam size increases, the gain by doubling the beam size decreases. We choose a beam size of 8 for the remaining experiments for a tradeoff between speeds and accuraces."
    }, {
      "heading" : "5.2.3 Pretraining Results",
      "text" : "Table 5 shows the effectiveness of rich pretraining of Dc on the development set. In particular, by using punctuation information, the F-score increases from 95.86% to 96.25%, with a relative error reduction of 9.4%. This is consistent with the observation of Sun and Xu (2011), who show that punctuation is more effective compared with mutual information and access variety as semisupervised data for a statistical word segmentation model. With automatically-segmented data5, heterogenous segmentation and POS information, the F-score increases to 96.26%, 96.27% and 96.22%, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation (Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2013). Finally, by integrating all above information via multi-task learning, the F-score is further improved to 96.44%, with a 14% relative error reduction.\n5By using ZPar alone, the auto-segmented result is 96.02%, less than using results by matching ZPar and the CRF segmentor outputs.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nModels P R F Baseline 95.3 95.5 95.4 Multitask pretrain 96.2 96.0 96.1 Sun and Xu (2011) baseline 95.2 94.9 95.1 Sun and Xu (2011) multi-source semi 95.9 95.6 95.7 Zhang et al. (2016) neural 95.3 94.7 95.0\nZhang et al. (2016)* hybrid 96.1 95.8 96.0 Chen et al. (2015a) window 95.7 95.8 95.8 Chen et al. (2015b) char LSTM 96.2 95.8 96.0 Zhang et al. (2014) POS and syntax – – 95.7 Wang et al. (2011) statistical semi 95.8 95.8 95.8 Zhang and Clark (2011) statistical 95.5 94.8 95.1\nTable 6: Main results on CTB6."
    }, {
      "heading" : "5.3 Final Results",
      "text" : "Our final results on CTB6 are shown in Table 6, which lists the results of several current state-ofthe-art methods. Without multitask pretraining, our model gives an F-score of 95.44%, which is higher than the neural segmentor of Zhang et al. (2016), which gives the best accuracies among pure neural segments on this dataset. By using multitask pretraining, the result increases to 96.09%, with a relative error reduction of 14.5%. In comparison, Sun and Xu (2011) investigated heterogenous semi-supervised learning on a stateof-the-art statistical model, obtaining a relative error reduction of 13.8%. Our findings show that external data can be as useful for neural segmentation as for statistical segmentation.\nOur final results compares favourably to the best statistical models, including those using semisupervised learning (Sun and Xu, 2011; Wang et al., 2011), and those leveraging joint POS and syntactic information (Zhang et al., 2014). In addition, it also outperforms the best neural models, in particular Zhang et al. (2016)*, which is a hybrid neural and statistical model, integrating manual discrete features into their word-based neural model. We achieve the best reported F-score on this dataset. To our knowledge, this is the first time a pure neural network model outperforms all existing methods on this dataset, allowing the use of external data.6\nIn addition to CTB6, which has been the most commonly adopted by recent segmentation research, we additionally evaluate our results on the\n6 We did not investigate the use of lexicons (Chen et al., 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016).\nF1 measure PKU MSR AS CityU Weibo Multitask pretrain 96.2 97.3 95.6 96.7 95.4\nCai and Zhao (2016) 95.5 96.5 – – – Zhang et al. (2016) 95.1 97.0 – – – Zhang et al. (2016)* 95.7 97.7 – – – Pei et al. (2014) 95.2 97.2 – – – Sun et al. (2012) 95.4 97.4 – – – Zhang and Clark (2007) 94.5 97.2 94.6 95.1 – Zhang et al. (2006) 95.1 97.1 95.1 95.1 – Sun et al. (2009) 95.2 97.3 – 94.6 –\nSun (2010) 95.2 96.9 95.2 95.6 – Wang et al. (2014) 95.3 97.4 95.4 94.7 – Xia et al. (2016) – – – – 95.4\nTable 7: Main results on other test datasets.\nSIGHAN 2005 bakeoff and Weibo datasets, to examine cross domain robustness. Different stateof-the-art methods for which results are recorded on these datasets are listed in Table 7. Most neural models reported results only on the PKU and MSR datasets of the bakeoff test sets, which are in simplified Chinese. The AS and CityU corpora are in traditional Chinese, sourced from Taiwan and Hong Kong corpora, respectively. We map them into simplified Chinese before segmentation. The Weibo corpus is in a yet different genre, being social media text. Xia et al. (2016) achieved the best results on this dataset by using a statistical model with features learned using external lexicons, the CTB7 corpus and the People Daily corpus. Similar to Table 6, our method gives the best accuracies on all corpora except for MSR, where it underperforms the hybrid model of Zhang et al. (2016) by 0.4%. To our knowledge, we are the first to report results for a neural segmentor on more than 3 datasets, with competitive results consistently. It verifies that knowledge learned from a certain set of resources can be used to enhance cross-domain robustness in training a neural segmentor for different datasets, which is of practical importance."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We investigated rich external resources for enhancing neural word segmentation, by building a globally optimised beam-search model that leverages both character and word contexts. Taking each type of external resource as an auxiliary classification task, we use neural multi-task learning to pre-train a set of shared parameters for character contexts. Results show that rich pretraining leads to 14.5% relative error reduction, and our model gives results highly competitive to the best systems on six different benchmarks.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Globally normalized transition-based neural networks",
      "author" : [ "Daniel Andor", "Chris Alberti", "David Weiss", "Aliaksei Severyn", "Alessandro Presta", "Kuzman Ganchev", "Slav Petrov", "Michael Collins." ],
      "venue" : "Proceedings of the 54th Annual Meeting of the Association",
      "citeRegEx" : "Andor et al\\.,? 2016",
      "shortCiteRegEx" : "Andor et al\\.",
      "year" : 2016
    }, {
      "title" : "Practical recommendations for gradient-based training of deep architectures",
      "author" : [ "Yoshua Bengio." ],
      "venue" : "Neural networks: Tricks of the trade, Springer, pages 437–478.",
      "citeRegEx" : "Bengio.,? 2012",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2012
    }, {
      "title" : "Neural word segmentation learning for chinese",
      "author" : [ "Deng Cai", "Hai Zhao." ],
      "venue" : "Proceedings of ACL .",
      "citeRegEx" : "Cai and Zhao.,? 2016",
      "shortCiteRegEx" : "Cai and Zhao.",
      "year" : 2016
    }, {
      "title" : "Gated recursive neural network for chinese word segmentation",
      "author" : [ "Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Xuanjing Huang." ],
      "venue" : "In Proceedings of the Conference on Empirical Methods in Natural Language Processing.",
      "citeRegEx" : "Chen et al\\.,? 2015a",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory neural networks for chinese word segmentation",
      "author" : [ "Xinchi Chen", "Xipeng Qiu", "Chenxi Zhu", "Pengfei Liu", "Xuanjing Huang." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing. pages",
      "citeRegEx" : "Chen et al\\.,? 2015b",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Incremental parsing with the perceptron algorithm",
      "author" : [ "Michael Collins", "Brian Roark." ],
      "venue" : "Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, page 111.",
      "citeRegEx" : "Collins and Roark.,? 2004",
      "shortCiteRegEx" : "Collins and Roark.",
      "year" : 2004
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "Ronan Collobert", "Jason Weston", "Léon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa." ],
      "venue" : "Journal of Machine Learning Research 12(Aug):2493–2537.",
      "citeRegEx" : "Collobert et al\\.,? 2011",
      "shortCiteRegEx" : "Collobert et al\\.",
      "year" : 2011
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer." ],
      "venue" : "Journal of Machine Learning Research 12(Jul):2121–2159.",
      "citeRegEx" : "Duchi et al\\.,? 2011",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "The second international chinese word segmentation bakeoff",
      "author" : [ "Thomas Emerson." ],
      "venue" : "Proceedings of the fourth SIGHAN workshop on Chinese language Processing. volume 133.",
      "citeRegEx" : "Emerson.,? 2005",
      "shortCiteRegEx" : "Emerson.",
      "year" : 2005
    }, {
      "title" : "Recurrent nets that time and count",
      "author" : [ "Felix A Gers", "Jürgen Schmidhuber." ],
      "venue" : "Neural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNSENNS International Joint Conference on. IEEE, volume 3, pages 189–194.",
      "citeRegEx" : "Gers and Schmidhuber.,? 2000",
      "shortCiteRegEx" : "Gers and Schmidhuber.",
      "year" : 2000
    }, {
      "title" : "Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging: a case study",
      "author" : [ "Wenbin Jiang", "Liang Huang", "Qun Liu." ],
      "venue" : "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International",
      "citeRegEx" : "Jiang et al\\.,? 2009",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2009
    }, {
      "title" : "Punctuation as implicit annotations for chinese word segmentation",
      "author" : [ "Zhongguo Li", "Maosong Sun." ],
      "venue" : "Computational Linguistics 35(4):505–512.",
      "citeRegEx" : "Li and Sun.,? 2009",
      "shortCiteRegEx" : "Li and Sun.",
      "year" : 2009
    }, {
      "title" : "Unsupervised domain adaptation for joint segmentation and pos-tagging",
      "author" : [ "Yang Liu", "Yue Zhang." ],
      "venue" : "COLING (Posters). pages 745–754.",
      "citeRegEx" : "Liu and Zhang.,? 2012",
      "shortCiteRegEx" : "Liu and Zhang.",
      "year" : 2012
    }, {
      "title" : "Feature-based neural language model and chinese word segmentation",
      "author" : [ "Mairgup Mansur", "Wenzhe Pei", "Baobao Chang." ],
      "venue" : "IJCNLP. pages 1271– 1277.",
      "citeRegEx" : "Mansur et al\\.,? 2013",
      "shortCiteRegEx" : "Mansur et al\\.",
      "year" : 2013
    }, {
      "title" : "Morphological analysis for unsegmented languages using recurrent neural network language model",
      "author" : [ "Hajime Morita", "Daisuke Kawahara", "Sadao Kurohashi." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Morita et al\\.,? 2015",
      "shortCiteRegEx" : "Morita et al\\.",
      "year" : 2015
    }, {
      "title" : "Chinese part-ofspeech tagging: One-at-a-time or all-at-once? wordbased or character-based? In EMNLP",
      "author" : [ "Hwee Tou Ng", "Jin Kiat Low." ],
      "venue" : "pages 277– 284.",
      "citeRegEx" : "Ng and Low.,? 2004",
      "shortCiteRegEx" : "Ng and Low.",
      "year" : 2004
    }, {
      "title" : "Maxmargin tensor neural network for chinese word segmentation",
      "author" : [ "Wenzhe Pei", "Tao Ge", "Baobao Chang." ],
      "venue" : "ACL (1). pages 293–303.",
      "citeRegEx" : "Pei et al\\.,? 2014",
      "shortCiteRegEx" : "Pei et al\\.",
      "year" : 2014
    }, {
      "title" : "Chinese segmentation and new word detection using conditional random fields",
      "author" : [ "Fuchun Peng", "Fangfang Feng", "Andrew McCallum." ],
      "venue" : "Proceedings of the 20th international conference on Computational Linguistics. Association for Computational",
      "citeRegEx" : "Peng et al\\.,? 2004",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2004
    }, {
      "title" : "Overview of the nlpcc-iccpol 2016 shared task: Chinese word segmentation for micro-blog texts",
      "author" : [ "Xipeng Qiu", "Peng Qian", "Zhan Shi." ],
      "venue" : "International Conference on Computer Processing of Oriental Languages. Springer, pages 901–906.",
      "citeRegEx" : "Qiu et al\\.,? 2016",
      "shortCiteRegEx" : "Qiu et al\\.",
      "year" : 2016
    }, {
      "title" : "Profiting from mark-up: Hyper-text annotations for guided parsing",
      "author" : [ "Valentin I Spitkovsky", "Daniel Jurafsky", "Hiyan Alshawi." ],
      "venue" : "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Compu-",
      "citeRegEx" : "Spitkovsky et al\\.,? 2010",
      "shortCiteRegEx" : "Spitkovsky et al\\.",
      "year" : 2010
    }, {
      "title" : "A stochastic finite-state wordsegmentation algorithm for chinese",
      "author" : [ "Richard Sproat", "William Gale", "Chilin Shih", "Nancy Chang." ],
      "venue" : "Computational linguistics 22(3):377–404.",
      "citeRegEx" : "Sproat et al\\.,? 1996",
      "shortCiteRegEx" : "Sproat et al\\.",
      "year" : 1996
    }, {
      "title" : "Dropout: a simple way to prevent neural networks from overfitting",
      "author" : [ "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov." ],
      "venue" : "Journal of Machine Learning Research 15(1):1929–1958.",
      "citeRegEx" : "Srivastava et al\\.,? 2014",
      "shortCiteRegEx" : "Srivastava et al\\.",
      "year" : 2014
    }, {
      "title" : "Word-based and character-based word segmentation models: Comparison and combination",
      "author" : [ "Weiwei Sun." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics: Posters. Association for Computational Linguistics, pages",
      "citeRegEx" : "Sun.,? 2010",
      "shortCiteRegEx" : "Sun.",
      "year" : 2010
    }, {
      "title" : "Exploring representations from unlabeled data with co-training for chinese word segmentation",
      "author" : [ "Longkai Zhang", "Houfeng Wang", "Xu Sun", "Mairgup Mansur" ],
      "venue" : null,
      "citeRegEx" : "Zhang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    }, {
      "title" : "Character-level chinese dependency parsing",
      "author" : [ "Meishan Zhang", "Yue Zhang", "Wanxiang Che", "Ting Liu." ],
      "venue" : "ACL (1). pages 1326–1336.",
      "citeRegEx" : "Zhang et al\\.,? 2014",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2014
    }, {
      "title" : "Transition-based neural word segmentation",
      "author" : [ "Meishan Zhang", "Yue Zhang", "Guohong Fu." ],
      "venue" : "Proceedings of the 54nd ACL .",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    }, {
      "title" : "Subword-based tagging by conditional random fields for chinese word segmentation",
      "author" : [ "Ruiqiang Zhang", "Genichiro Kikui", "Eiichiro Sumita." ],
      "venue" : "Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume:",
      "citeRegEx" : "Zhang et al\\.,? 2006",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2006
    }, {
      "title" : "Chinese segmentation with a word-based perceptron algorithm",
      "author" : [ "Yue Zhang", "Stephen Clark." ],
      "venue" : "Annual Meeting-Association for Computational Linguistics. volume 45, page 840.",
      "citeRegEx" : "Zhang and Clark.,? 2007",
      "shortCiteRegEx" : "Zhang and Clark.",
      "year" : 2007
    }, {
      "title" : "Joint word segmentation and pos tagging using a single perceptron",
      "author" : [ "Yue Zhang", "Stephen Clark." ],
      "venue" : "ACL. pages 888–896.",
      "citeRegEx" : "Zhang and Clark.,? 2008",
      "shortCiteRegEx" : "Zhang and Clark.",
      "year" : 2008
    }, {
      "title" : "Syntactic processing using the generalized perceptron and beam search",
      "author" : [ "Yue Zhang", "Stephen Clark." ],
      "venue" : "Computational linguistics 37(1):105–151.",
      "citeRegEx" : "Zhang and Clark.,? 2011",
      "shortCiteRegEx" : "Zhang and Clark.",
      "year" : 2011
    }, {
      "title" : "Effective tag set selection in chinese word segmentation via conditional random field modeling",
      "author" : [ "Hai Zhao", "Chang-Ning Huang", "Mu Li", "Bao-Liang Lu." ],
      "venue" : "Proceedings of PACLIC. Citeseer, volume 20, pages 87–94.",
      "citeRegEx" : "Zhao et al\\.,? 2006",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2006
    }, {
      "title" : "Deep learning for chinese word segmentation and pos tagging",
      "author" : [ "Xiaoqing Zheng", "Hanyang Chen", "Tianyu Xu." ],
      "venue" : "EMNLP. pages 647–657.",
      "citeRegEx" : "Zheng et al\\.,? 2013",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2013
    }, {
      "title" : "A neural probabilistic structuredprediction model for transition-based dependency parsing",
      "author" : [ "Hao Zhou", "Yue Zhang", "Shujian Huang", "Jiajun Chen." ],
      "venue" : "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguis-",
      "citeRegEx" : "Zhou et al\\.,? 2015",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 31,
      "context" : "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 130,
      "endOffset" : 249
    }, {
      "referenceID" : 16,
      "context" : "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 130,
      "endOffset" : 249
    }, {
      "referenceID" : 14,
      "context" : "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 130,
      "endOffset" : 249
    }, {
      "referenceID" : 4,
      "context" : "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 130,
      "endOffset" : 249
    }, {
      "referenceID" : 2,
      "context" : "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 130,
      "endOffset" : 249
    }, {
      "referenceID" : 25,
      "context" : "There has been a recent shift of research attention in the word segmentation literature from statistical methods to deep learning (Zheng et al., 2013; Pei et al., 2014; Morita et al., 2015; Chen et al., 2015b; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 130,
      "endOffset" : 249
    }, {
      "referenceID" : 31,
      "context" : "They serve to reduce sparsity of character ngrams, allowing, for example, “猫(cat)躺(lie)在(in)墙角(corner)” to be connected with “狗(dog) 蹲(sit) 在(in) 墙 角(corner)” (Zheng et al., 2013), which is infeasible by using sparse one-hot character features.",
      "startOffset" : 159,
      "endOffset" : 179
    }, {
      "referenceID" : 13,
      "context" : "In addition to character embeddings, distributed representations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al.",
      "startOffset" : 86,
      "endOffset" : 125
    }, {
      "referenceID" : 16,
      "context" : "In addition to character embeddings, distributed representations of character bigrams (Mansur et al., 2013; Pei et al., 2014) and words (Morita et al.",
      "startOffset" : 86,
      "endOffset" : 125
    }, {
      "referenceID" : 14,
      "context" : ", 2014) and words (Morita et al., 2015; Zhang et al., 2016) have also been shown to improve segmentation accuracies.",
      "startOffset" : 18,
      "endOffset" : 59
    }, {
      "referenceID" : 25,
      "context" : ", 2014) and words (Morita et al., 2015; Zhang et al., 2016) have also been shown to improve segmentation accuracies.",
      "startOffset" : 18,
      "endOffset" : 59
    }, {
      "referenceID" : 31,
      "context" : "With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al.",
      "startOffset" : 204,
      "endOffset" : 283
    }, {
      "referenceID" : 13,
      "context" : "With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al.",
      "startOffset" : 204,
      "endOffset" : 283
    }, {
      "referenceID" : 16,
      "context" : "With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al.",
      "startOffset" : 204,
      "endOffset" : 283
    }, {
      "referenceID" : 3,
      "context" : "With respect to non-linear modeling power, various network structures have been exploited to represent contexts for segmentation disambiguation, including multi-layer perceptrons on fivecharacter windows (Zheng et al., 2013; Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a), as well as LSTMs on characters (Chen et al.",
      "startOffset" : 204,
      "endOffset" : 283
    }, {
      "referenceID" : 4,
      "context" : ", 2015a), as well as LSTMs on characters (Chen et al., 2015b; Xu and Sun, 2016) and words (Morita et al.",
      "startOffset" : 41,
      "endOffset" : 79
    }, {
      "referenceID" : 14,
      "context" : ", 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 37,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : ", 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 37,
      "endOffset" : 98
    }, {
      "referenceID" : 25,
      "context" : ", 2015b; Xu and Sun, 2016) and words (Morita et al., 2015; Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 37,
      "endOffset" : 98
    }, {
      "referenceID" : 16,
      "context" : "For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al.",
      "startOffset" : 97,
      "endOffset" : 135
    }, {
      "referenceID" : 4,
      "context" : "For structured learning and inference, CRF has been used for character sequence labelling models (Pei et al., 2014; Chen et al., 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al.",
      "startOffset" : 97,
      "endOffset" : 135
    }, {
      "referenceID" : 2,
      "context" : ", 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 116
    }, {
      "referenceID" : 25,
      "context" : ", 2015b) and structural beam search has been used for word-based segmentors (Cai and Zhao, 2016; Zhang et al., 2016).",
      "startOffset" : 76,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : "Previous research has shown that segmentation accuracies can be improved by pretraining character and word embeddings over large Chinese texts, which is consistent with findings on other NLP tasks, such as parsing (Andor et al., 2016).",
      "startOffset" : 214,
      "endOffset" : 234
    }, {
      "referenceID" : 11,
      "context" : "On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011), and making use of selfpredictions (Wang et al.",
      "startOffset" : 201,
      "endOffset" : 237
    }, {
      "referenceID" : 12,
      "context" : "On the other hand, statistical segmentation research has exploited raw texts for semi-supervised learning, by collecting clues from raw texts more thoroughly such as mutual information and punctuation (Li and Sun, 2009; Sun and Xu, 2011), and making use of selfpredictions (Wang et al., 2011; Liu and Zhang, 2012).",
      "startOffset" : 273,
      "endOffset" : 313
    }, {
      "referenceID" : 15,
      "context" : "It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) and segmentation under different standards (Jiang et al.",
      "startOffset" : 58,
      "endOffset" : 99
    }, {
      "referenceID" : 28,
      "context" : "It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) and segmentation under different standards (Jiang et al.",
      "startOffset" : 58,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "It has also utilised heterogenous annotations such as POS (Ng and Low, 2004; Zhang and Clark, 2008) and segmentation under different standards (Jiang et al., 2009).",
      "startOffset" : 143,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : "(2016), we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al., 2016; Zhou et al., 2015; Wiseman and Rush, 2016), which allows word information to be modelled explicitly.",
      "startOffset" : 93,
      "endOffset" : 156
    }, {
      "referenceID" : 32,
      "context" : "(2016), we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al., 2016; Zhou et al., 2015; Wiseman and Rush, 2016), which allows word information to be modelled explicitly.",
      "startOffset" : 93,
      "endOffset" : 156
    }, {
      "referenceID" : 6,
      "context" : "We adopt a multi-task learning strategy (Collobert et al., 2011), casting each external source of information as a auxiliary classification task, sharing a five-character window network.",
      "startOffset" : 40,
      "endOffset" : 64
    }, {
      "referenceID" : 1,
      "context" : "Following Cai and Zhao (2016) and Zhang et al.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "Following Cai and Zhao (2016) and Zhang et al. (2016), we adopt a globally optimised beam-search framework for neural structured prediction (Andor et al.",
      "startOffset" : 10,
      "endOffset" : 54
    }, {
      "referenceID" : 20,
      "context" : "Work on statistical word segmentation dates back to the 1990s (Sproat et al., 1996).",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : ", 2003) using CRFs (Peng et al., 2004; Zhao et al., 2006) and max-margin structured models leveraging word features (Zhang and Clark, 2007; Sun et al.",
      "startOffset" : 19,
      "endOffset" : 57
    }, {
      "referenceID" : 30,
      "context" : ", 2003) using CRFs (Peng et al., 2004; Zhao et al., 2006) and max-margin structured models leveraging word features (Zhang and Clark, 2007; Sun et al.",
      "startOffset" : 19,
      "endOffset" : 57
    }, {
      "referenceID" : 27,
      "context" : ", 2006) and max-margin structured models leveraging word features (Zhang and Clark, 2007; Sun et al., 2009; Sun, 2010).",
      "startOffset" : 66,
      "endOffset" : 118
    }, {
      "referenceID" : 22,
      "context" : ", 2006) and max-margin structured models leveraging word features (Zhang and Clark, 2007; Sun et al., 2009; Sun, 2010).",
      "startOffset" : 66,
      "endOffset" : 118
    }, {
      "referenceID" : 12,
      "context" : "Semisupervised methods have been applied to both character-based and word-based models, exploring external training data for better segmentation (Sun and Xu, 2011; Wang et al., 2011; Liu and Zhang, 2012; Zhang et al., 2013).",
      "startOffset" : 145,
      "endOffset" : 223
    }, {
      "referenceID" : 23,
      "context" : "Semisupervised methods have been applied to both character-based and word-based models, exploring external training data for better segmentation (Sun and Xu, 2011; Wang et al., 2011; Liu and Zhang, 2012; Zhang et al., 2013).",
      "startOffset" : 145,
      "endOffset" : 223
    }, {
      "referenceID" : 13,
      "context" : "Our model inherits from previous findings on context representations, such as character windows (Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) and LSTMs (Chen et al.",
      "startOffset" : 96,
      "endOffset" : 155
    }, {
      "referenceID" : 16,
      "context" : "Our model inherits from previous findings on context representations, such as character windows (Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) and LSTMs (Chen et al.",
      "startOffset" : 96,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "Our model inherits from previous findings on context representations, such as character windows (Mansur et al., 2013; Pei et al., 2014; Chen et al., 2015a) and LSTMs (Chen et al.",
      "startOffset" : 96,
      "endOffset" : 155
    }, {
      "referenceID" : 4,
      "context" : ", 2015a) and LSTMs (Chen et al., 2015b; Xu and Sun, 2016).",
      "startOffset" : 19,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : ", 2011; Liu and Zhang, 2012; Zhang et al., 2013). Our work belongs to recent neural word segmentation. To our knowledge, there has no work in the literature systematically investigating rich external resources for neural word segmentation training. Closest in spirit to our work, Sun and Xu (2011) empirically studied the use of various external resources for enhancing a statistical segmentor, including character mutual information, access variety information, punctuation and other statistical information.",
      "startOffset" : 8,
      "endOffset" : 298
    }, {
      "referenceID" : 2,
      "context" : ", 2014; Chen et al., 2015a) and LSTMs (Chen et al., 2015b; Xu and Sun, 2016). Similar to Zhang et al. (2016) and Cai and Zhao (2016), we use word context on top of character context.",
      "startOffset" : 8,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "(2016) and Cai and Zhao (2016), we use word context on top of character context.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "(2016) and Cai and Zhao (2016), we use word context on top of character context. However, words play a relatively less important role in our model, and we find that word LSTM, which has been used by all previous neural segmentation work, is unnecessary for our model. Our model is conceptually simpler and more modularised compared with Zhang et al. (2016) and Cai and Zhao (2016), allowing a central sub module, namely a five-character context window, to be pretrained.",
      "startOffset" : 11,
      "endOffset" : 357
    }, {
      "referenceID" : 2,
      "context" : "(2016) and Cai and Zhao (2016), we use word context on top of character context. However, words play a relatively less important role in our model, and we find that word LSTM, which has been used by all previous neural segmentation work, is unnecessary for our model. Our model is conceptually simpler and more modularised compared with Zhang et al. (2016) and Cai and Zhao (2016), allowing a central sub module, namely a five-character context window, to be pretrained.",
      "startOffset" : 11,
      "endOffset" : 381
    }, {
      "referenceID" : 22,
      "context" : "Similar to Zhang et al. (2016) and Cai and Zhao (2016), our model is a global structural model, using the overall score to disambiguate states, which correspond to sequences of inter-dependent transition actions.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "(2016) and Cai and Zhao (2016), our model is a global structural model, using the overall score to disambiguate states, which correspond to sequences of inter-dependent transition actions.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "For the former, we follow prior methods (Xue et al., 2003; Pei et al., 2014), using five-character window [c−2, c−1, c0, c1, c2] to represent incoming characters.",
      "startOffset" : 40,
      "endOffset" : 76
    }, {
      "referenceID" : 4,
      "context" : "For the latter, we follow recent work (Chen et al., 2015b; Zhang et al., 2016), using a bidirectional LSTM to encode input character sequence.",
      "startOffset" : 38,
      "endOffset" : 78
    }, {
      "referenceID" : 25,
      "context" : "For the latter, we follow recent work (Chen et al., 2015b; Zhang et al., 2016), using a bidirectional LSTM to encode input character sequence.",
      "startOffset" : 38,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "The LSTM variation with coupled input and forget gate but without peephole connections is applied (Gers and Schmidhuber, 2000)",
      "startOffset" : 98,
      "endOffset" : 126
    }, {
      "referenceID" : 27,
      "context" : "methods (Zhang and Clark, 2007; Sun, 2010), using the two-word window [w−2, w−1] to represent recognized words.",
      "startOffset" : 8,
      "endOffset" : 42
    }, {
      "referenceID" : 22,
      "context" : "methods (Zhang and Clark, 2007; Sun, 2010), using the two-word window [w−2, w−1] to represent recognized words.",
      "startOffset" : 8,
      "endOffset" : 42
    }, {
      "referenceID" : 22,
      "context" : "For the latter, we follow Zhang et al. (2016) and Cai and Zhao (2016), using an uni-directional LSTM on words that have been recognized.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "(2016) and Cai and Zhao (2016), using an uni-directional LSTM on words that have been recognized.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : "We therefore consider a more explicit clue for pretraining our character window network, namely punctuations (Li and Sun, 2009).",
      "startOffset" : 109,
      "endOffset" : 127
    }, {
      "referenceID" : 19,
      "context" : "Punctuation can serve as a type of explicit markup (Spitkovsky et al., 2010), indicating that the two characters on its left and right belong to two different words.",
      "startOffset" : 51,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : "Large texts automatically segmented by a baseline segmentor can be used for self-training (Liu and Zhang, 2012) or deriving statistical features (Wang et al.",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "There has been investigation on leveraging two corpora under different annotation standards to improve statistical segmentation (Jiang et al., 2009).",
      "startOffset" : 128,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "Neural model have been shown capable of doing multi-task learning via parameter sharing (Collobert et al., 2011).",
      "startOffset" : 88,
      "endOffset" : 112
    }, {
      "referenceID" : 27,
      "context" : "To train the main segmentor, we adopt the global transition-based learning and beam-search strategy of Zhang and Clark (2011). For decoding, standard beam search is used, where the B best partial output hypotheses at each step are maintained in an agenda.",
      "startOffset" : 103,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "The strategy above is early-update (Collins and Roark, 2004).",
      "startOffset" : 35,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "We use Adagrad (Duchi et al., 2011) to optimize model parameters, with an initial learning rate α.",
      "startOffset" : 15,
      "endOffset" : 35
    }, {
      "referenceID" : 21,
      "context" : "L2 regularization and dropout (Srivastava et al., 2014) on input are used to reduce overfitting, with a L2 weight λ and a dropout rate p.",
      "startOffset" : 30,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "0 fanin+fanout (Bengio, 2012).",
      "startOffset" : 15,
      "endOffset" : 29
    }, {
      "referenceID" : 23,
      "context" : "bigram embeddings, but not word embeddings, acccording to Zhang et al. (2016).",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 24,
      "context" : "Training, development and test set splits follow previous work (Zhang et al., 2014).",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : "In order to verify the robustness of our model, we additionally use SIGHAN 2005 bake-off (Emerson, 2005) and NLPCC 2016 shared task for Weibo segmentation (Qiu et al.",
      "startOffset" : 89,
      "endOffset" : 104
    }, {
      "referenceID" : 18,
      "context" : "In order to verify the robustness of our model, we additionally use SIGHAN 2005 bake-off (Emerson, 2005) and NLPCC 2016 shared task for Weibo segmentation (Qiu et al., 2016) as test datasets, where the standard splits are used.",
      "startOffset" : 155,
      "endOffset" : 173
    }, {
      "referenceID" : 27,
      "context" : "6 off-theshelf (Zhang and Clark, 2007), the statictics of which are shown in Table 2.",
      "startOffset" : 15,
      "endOffset" : 38
    }, {
      "referenceID" : 8,
      "context" : "The standard word precision, recall and F1 measure (Emerson, 2005) are used to evaluate segmentation performances.",
      "startOffset" : 51,
      "endOffset" : 66
    }, {
      "referenceID" : 25,
      "context" : "(Zhang et al., 2016; Cai and Zhao, 2016).",
      "startOffset" : 0,
      "endOffset" : 40
    }, {
      "referenceID" : 2,
      "context" : "(Zhang et al., 2016; Cai and Zhao, 2016).",
      "startOffset" : 0,
      "endOffset" : 40
    }, {
      "referenceID" : 27,
      "context" : "This is consistent with previous findings of statistical word segmentation (Zhang and Clark, 2007), which adopt a 2-word context.",
      "startOffset" : 75,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : ", 2016; Cai and Zhao, 2016). This is likely due to the difference in our neural network structures, and that we fine-tune both character and character bigram embeddings, which significantly enlarges the adjustable parameter space as compared with Zhang et al. (2016). The fact that word contexts can contribute relatively less than characters in a word is also not surprising in the sense that word-based neural segmentors do not outperform the best character-based models by large margins.",
      "startOffset" : 8,
      "endOffset" : 267
    }, {
      "referenceID" : 10,
      "context" : "22%, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation (Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2013).",
      "startOffset" : 169,
      "endOffset" : 228
    }, {
      "referenceID" : 23,
      "context" : "22%, respectively, showing the relevance of all information sources to neural segmentation, which is consistent with observations made for statistical word segmentation (Jiang et al., 2009; Wang et al., 2011; Zhang et al., 2013).",
      "startOffset" : 169,
      "endOffset" : 228
    }, {
      "referenceID" : 21,
      "context" : "This is consistent with the observation of Sun and Xu (2011), who show that punctuation is more effective compared with mutual information and access variety as semisupervised data for a statistical word segmentation model.",
      "startOffset" : 43,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : "1 Sun and Xu (2011) baseline 95.",
      "startOffset" : 2,
      "endOffset" : 20
    }, {
      "referenceID" : 20,
      "context" : "1 Sun and Xu (2011) baseline 95.2 94.9 95.1 Sun and Xu (2011) multi-source semi 95.",
      "startOffset" : 2,
      "endOffset" : 62
    }, {
      "referenceID" : 20,
      "context" : "1 Sun and Xu (2011) baseline 95.2 94.9 95.1 Sun and Xu (2011) multi-source semi 95.9 95.6 95.7 Zhang et al. (2016) neural 95.",
      "startOffset" : 2,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "1 Sun and Xu (2011) baseline 95.2 94.9 95.1 Sun and Xu (2011) multi-source semi 95.9 95.6 95.7 Zhang et al. (2016) neural 95.3 94.7 95.0 Zhang et al. (2016)* hybrid 96.",
      "startOffset" : 2,
      "endOffset" : 157
    }, {
      "referenceID" : 3,
      "context" : "0 Chen et al. (2015a) window 95.",
      "startOffset" : 2,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "0 Chen et al. (2015a) window 95.7 95.8 95.8 Chen et al. (2015b) char LSTM 96.",
      "startOffset" : 2,
      "endOffset" : 64
    }, {
      "referenceID" : 3,
      "context" : "0 Chen et al. (2015a) window 95.7 95.8 95.8 Chen et al. (2015b) char LSTM 96.2 95.8 96.0 Zhang et al. (2014) POS and syntax – – 95.",
      "startOffset" : 2,
      "endOffset" : 109
    }, {
      "referenceID" : 3,
      "context" : "0 Chen et al. (2015a) window 95.7 95.8 95.8 Chen et al. (2015b) char LSTM 96.2 95.8 96.0 Zhang et al. (2014) POS and syntax – – 95.7 Wang et al. (2011) statistical semi 95.",
      "startOffset" : 2,
      "endOffset" : 152
    }, {
      "referenceID" : 3,
      "context" : "0 Chen et al. (2015a) window 95.7 95.8 95.8 Chen et al. (2015b) char LSTM 96.2 95.8 96.0 Zhang et al. (2014) POS and syntax – – 95.7 Wang et al. (2011) statistical semi 95.8 95.8 95.8 Zhang and Clark (2011) statistical 95.",
      "startOffset" : 2,
      "endOffset" : 207
    }, {
      "referenceID" : 24,
      "context" : ", 2011), and those leveraging joint POS and syntactic information (Zhang et al., 2014).",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 22,
      "context" : "44%, which is higher than the neural segmentor of Zhang et al. (2016), which gives the best accuracies among pure neural segments on this dataset.",
      "startOffset" : 50,
      "endOffset" : 70
    }, {
      "referenceID" : 22,
      "context" : "In comparison, Sun and Xu (2011) investigated heterogenous semi-supervised learning on a stateof-the-art statistical model, obtaining a relative error reduction of 13.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "In comparison, Sun and Xu (2011) investigated heterogenous semi-supervised learning on a stateof-the-art statistical model, obtaining a relative error reduction of 13.8%. Our findings show that external data can be as useful for neural segmentation as for statistical segmentation. Our final results compares favourably to the best statistical models, including those using semisupervised learning (Sun and Xu, 2011; Wang et al., 2011), and those leveraging joint POS and syntactic information (Zhang et al., 2014). In addition, it also outperforms the best neural models, in particular Zhang et al. (2016)*, which is a hybrid neural and statistical model, integrating manual discrete features into their word-based neural model.",
      "startOffset" : 15,
      "endOffset" : 607
    }, {
      "referenceID" : 2,
      "context" : ", 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016). F1 measure PKU MSR AS CityU Weibo Multitask pretrain 96.",
      "startOffset" : 281,
      "endOffset" : 301
    }, {
      "referenceID" : 2,
      "context" : ", 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016). F1 measure PKU MSR AS CityU Weibo Multitask pretrain 96.2 97.3 95.6 96.7 95.4 Cai and Zhao (2016) 95.",
      "startOffset" : 281,
      "endOffset" : 400
    }, {
      "referenceID" : 2,
      "context" : ", 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016). F1 measure PKU MSR AS CityU Weibo Multitask pretrain 96.2 97.3 95.6 96.7 95.4 Cai and Zhao (2016) 95.5 96.5 – – – Zhang et al. (2016) 95.",
      "startOffset" : 281,
      "endOffset" : 436
    }, {
      "referenceID" : 2,
      "context" : ", 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016). F1 measure PKU MSR AS CityU Weibo Multitask pretrain 96.2 97.3 95.6 96.7 95.4 Cai and Zhao (2016) 95.5 96.5 – – – Zhang et al. (2016) 95.1 97.0 – – – Zhang et al. (2016)* 95.",
      "startOffset" : 281,
      "endOffset" : 472
    }, {
      "referenceID" : 2,
      "context" : ", 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016). F1 measure PKU MSR AS CityU Weibo Multitask pretrain 96.2 97.3 95.6 96.7 95.4 Cai and Zhao (2016) 95.5 96.5 – – – Zhang et al. (2016) 95.1 97.0 – – – Zhang et al. (2016)* 95.7 97.7 – – – Pei et al. (2014) 95.",
      "startOffset" : 281,
      "endOffset" : 507
    }, {
      "referenceID" : 2,
      "context" : ", 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016). F1 measure PKU MSR AS CityU Weibo Multitask pretrain 96.2 97.3 95.6 96.7 95.4 Cai and Zhao (2016) 95.5 96.5 – – – Zhang et al. (2016) 95.1 97.0 – – – Zhang et al. (2016)* 95.7 97.7 – – – Pei et al. (2014) 95.2 97.2 – – – Sun et al. (2012) 95.",
      "startOffset" : 281,
      "endOffset" : 541
    }, {
      "referenceID" : 2,
      "context" : ", 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016). F1 measure PKU MSR AS CityU Weibo Multitask pretrain 96.2 97.3 95.6 96.7 95.4 Cai and Zhao (2016) 95.5 96.5 – – – Zhang et al. (2016) 95.1 97.0 – – – Zhang et al. (2016)* 95.7 97.7 – – – Pei et al. (2014) 95.2 97.2 – – – Sun et al. (2012) 95.4 97.4 – – – Zhang and Clark (2007) 94.",
      "startOffset" : 281,
      "endOffset" : 580
    }, {
      "referenceID" : 2,
      "context" : ", 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016). F1 measure PKU MSR AS CityU Weibo Multitask pretrain 96.2 97.3 95.6 96.7 95.4 Cai and Zhao (2016) 95.5 96.5 – – – Zhang et al. (2016) 95.1 97.0 – – – Zhang et al. (2016)* 95.7 97.7 – – – Pei et al. (2014) 95.2 97.2 – – – Sun et al. (2012) 95.4 97.4 – – – Zhang and Clark (2007) 94.5 97.2 94.6 95.1 – Zhang et al. (2006) 95.",
      "startOffset" : 281,
      "endOffset" : 622
    }, {
      "referenceID" : 2,
      "context" : ", 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016). F1 measure PKU MSR AS CityU Weibo Multitask pretrain 96.2 97.3 95.6 96.7 95.4 Cai and Zhao (2016) 95.5 96.5 – – – Zhang et al. (2016) 95.1 97.0 – – – Zhang et al. (2016)* 95.7 97.7 – – – Pei et al. (2014) 95.2 97.2 – – – Sun et al. (2012) 95.4 97.4 – – – Zhang and Clark (2007) 94.5 97.2 94.6 95.1 – Zhang et al. (2006) 95.1 97.1 95.1 95.1 – Sun et al. (2009) 95.",
      "startOffset" : 281,
      "endOffset" : 662
    }, {
      "referenceID" : 2,
      "context" : ", 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016). F1 measure PKU MSR AS CityU Weibo Multitask pretrain 96.2 97.3 95.6 96.7 95.4 Cai and Zhao (2016) 95.5 96.5 – – – Zhang et al. (2016) 95.1 97.0 – – – Zhang et al. (2016)* 95.7 97.7 – – – Pei et al. (2014) 95.2 97.2 – – – Sun et al. (2012) 95.4 97.4 – – – Zhang and Clark (2007) 94.5 97.2 94.6 95.1 – Zhang et al. (2006) 95.1 97.1 95.1 95.1 – Sun et al. (2009) 95.2 97.3 – 94.6 – Sun (2010) 95.",
      "startOffset" : 281,
      "endOffset" : 692
    }, {
      "referenceID" : 2,
      "context" : ", 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016). F1 measure PKU MSR AS CityU Weibo Multitask pretrain 96.2 97.3 95.6 96.7 95.4 Cai and Zhao (2016) 95.5 96.5 – – – Zhang et al. (2016) 95.1 97.0 – – – Zhang et al. (2016)* 95.7 97.7 – – – Pei et al. (2014) 95.2 97.2 – – – Sun et al. (2012) 95.4 97.4 – – – Zhang and Clark (2007) 94.5 97.2 94.6 95.1 – Zhang et al. (2006) 95.1 97.1 95.1 95.1 – Sun et al. (2009) 95.2 97.3 – 94.6 – Sun (2010) 95.2 96.9 95.2 95.6 – Wang et al. (2014) 95.",
      "startOffset" : 281,
      "endOffset" : 733
    }, {
      "referenceID" : 2,
      "context" : ", 2015a,b) in our research, since lexicons might cover different OOV in the training and test data, and hence directly affecting the accuracies, which makes it relatively difficult to compare different methods fairly unless a single lexicon is used for all methods, as observed by Cai and Zhao (2016). F1 measure PKU MSR AS CityU Weibo Multitask pretrain 96.2 97.3 95.6 96.7 95.4 Cai and Zhao (2016) 95.5 96.5 – – – Zhang et al. (2016) 95.1 97.0 – – – Zhang et al. (2016)* 95.7 97.7 – – – Pei et al. (2014) 95.2 97.2 – – – Sun et al. (2012) 95.4 97.4 – – – Zhang and Clark (2007) 94.5 97.2 94.6 95.1 – Zhang et al. (2006) 95.1 97.1 95.1 95.1 – Sun et al. (2009) 95.2 97.3 – 94.6 – Sun (2010) 95.2 96.9 95.2 95.6 – Wang et al. (2014) 95.3 97.4 95.4 94.7 – Xia et al. (2016) – – – – 95.",
      "startOffset" : 281,
      "endOffset" : 773
    }, {
      "referenceID" : 23,
      "context" : "Similar to Table 6, our method gives the best accuracies on all corpora except for MSR, where it underperforms the hybrid model of Zhang et al. (2016) by 0.",
      "startOffset" : 131,
      "endOffset" : 151
    } ],
    "year" : 2017,
    "abstractText" : "Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important sub module using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.",
    "creator" : "LaTeX with hyperref package"
  }
}