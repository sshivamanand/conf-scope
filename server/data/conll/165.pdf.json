{
  "name" : "165.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Discovering Correspondences between Multiple Languages by MDL",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\nHow can we automatically discover the most important correspondences between words from two or more languages? How can we do so allowing for correspondences between any subset of languages, without drowning in redundant results, and at the same time maintaining control over the level of detail? These are exactly the questions we answer in this paper. We approach the problem with the Minimum Description Length principle, and give an efficient algorithm for discovering statistically important correspondences. We test the efficacy of our method against a set of Slavic languages. The experiments show our method automatically discovers non-trivial associations, allowing for both quantitative and qualitative analysis of multiple languages."
    }, {
      "heading" : "1 Introduction & Related Work",
      "text" : "Systematic correspondences between languages form the basis for much linguistic work. Researchers employ them to e.g. improve teaching, analyze and quantify the similarity or relatedness of languages, or to formulate hypotheses aboutmutual intelligibility. Beyond that, they are of importance in multi-language natural language processing, notably in machine translation.\nCorrespondence rules can be established on the basis of various linguistic features, such as the language's alphabets, their orthographies, their phonologies, or their inflectional and derivational morphologies. As an example, the Czech, Polish, Russian, andBulgarian forms of the pan-Slavic word for happiness could be analyzed to have the following ortho-phonetic correspondences:\nIn order to find correspondences, a linguist typically collects cognates from two ormore languages and compares them manually. If the linguist observes an often-occurring pattern, or one that fits well with other known changes that occurred between the languages, then she might conclude that this pattern is systematic and use it as basis for further investigations. This technique, called the comparative method, dates back to at least the 1800s (Szemerenyi, 1970). Recently, researchers have devised various statistical approaches to identifying the regular correspondences between languages. Much of these focus on cognate identification or reconstruction (Schulz et al., 2004; Snyder et al., 2010), on discovering and quantifying etymological relationships between languages (Wettig et al., 2011), or on discovery of pseudo-morphological sub-word alignments (Snyder and Barzilay, 2008). However, most existing statistical approaches are afflicted by a number of problems: they may impose arbitrary assumptions on the distribution or shape of correspondences, may not allow for integration of linguistic knowledge, or may be limited to pairs of languages. While imposing assumptions is sometimes necessary in order to obtain any results at all, it leads to finding not the \"true\" correspondences hidden in the data, but their closest similes from the assumed distribution. How, then, can we discover correspondences between more than two languages without prior assumptions about their shape or distribution? This is the question we answer in this paper. For this, we employ the Minimum Description Length (MDL) principle (Grünwald, 2007). MDL provides a statistically well-founded approach to identifying the\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nbest model for given data, and has amongst others been used to model changes in etymologicallyrelated words (Wettig et al., 2011).\nUsing MDL, we deem the set of correspondences that describes the data most succinctly to be the best. We propose an efficient, deterministic algorithm to infer good sets of correspondence rules directly from data. In our experiments, we present a phylogenetic analysis of a number of Slavic languages and show how our approach can be used for efficient, highly detailed quantification of stringlevel similarities among more than two languages.\nIn our pairwise analysis, we confirm that stringlevel similarity between languages is a strong reflection of linguistic classification. In our fourlanguage experiment, we find that our algorithm successfully identifies linguistic sub-groups while quantifying the similarity between all subsets of our four analyzed languages.\nThe paper is structured as follows: we give an overview of our approach and terminology in Section 2, then present our model in Section 4. After this, we report learned correspondences and provide information-theoretic analyses of language similarity in Section 5, and conclude in Section 6."
    }, {
      "heading" : "2 Approach & Terminology",
      "text" : "We seekmodels which consist of sets of correspondence rules. We propose that correspondences should be treated simply as associated strings of characters with no assumed underlying distribution. Doing this results in objective, unbiased string-level measures of linguistic similarity and allows to observe the actual distributions of correspondence rules. Furthermore, we want to learn our rules deterministically, to exclude chance's influence on our assessments.\nWe build our approach on these key principles and use a number of observations to realize them.\n1) In order to evaluate how good a given set of correspondence rules is, we should evaluate how well these rules describe the data. However, it is not immediately obvious how to do this. For example, if we are given the Polish-Czech correspondences (s,š), (sz,š), (c,t), (cz,t), and (szcz,št), then we can segment, or align, the initial sub-strings szcz and št from our happiness example in multiple ways. Three possible alignments are:\na) s z c z š t b) s z c z š t c) szcz št\nLacking an evaluation function, we cannot tell which of the three example alignments above is the best. However, if we are given the best alignment of our data, then we can straightforwardly compute probabilities for each of the correspondence rules, from which we can then compute the optimal rule costs. Similarly, knowing the costs of the rules allows us to compute the optimal alignment. Thus, our problem lends itself well to an Expectation-Maximization (EM) (Dempster et al., 1977) approach. To use EM, we must formulate both the expectation and the maximization steps. The expectation step is straightforward; we simply align the data with the current model. Themaximization step can be made intuitive with another observation. 2) If the optimal alignment is c), using rule (szcz,št), then any of the rules making up alignments a) and b) occur at least as often in the data as (szcz,št) does. Therefore, improving the model can be done by checking if an alignment is improved by merging any two compatible rules. 3) With this in mind, initialization of this model is straightforward: if we start by assuming no structure at all, then we will find the dominant structures in the data. Therefore, we start training from what we call a null alignment: one which uses only rules which contain exactly one symbol from exactly one language. An example for such a rule is (z,) in alignment a). We call such correspondence rules singleton rules. We next explain how we formalize our model in terms of MDL."
    }, {
      "heading" : "3 MDL for Correspondence Discovery",
      "text" : "The Minimum Description Length principle proposes that the optimal model is the one resulting in the most concise description of the modeled data. Importantly, in MDL-based modeling, we use only the data as evidence for our models and forego making assumptions about the nature of models in the form of prior probabilities. This makes it ideally suited for our purposes. For our model, we employ two-part MDL (Grünwald, 2007), which consists of a stack of two-part codes. The main formula for this is\nM = arg min M∈M L(M) + L(D|M),\nwith D being the data at hand, M the explaining model, andM themodel classwe draw ourmodels\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nfrom. L(M) is the length, in bits1, of the description of modelM . Similarly,L(D|M) is the length, in bits, of the data given model M . Description lengths are simply code lengths: Shannon's source coding theorem (Shannon, 2001) tells us that the best prefix-free code for some data is derived from the (negative logarithm of the) probabilities of the data, i.e.\nL(M) + L(D|M) = − log p(M)− log p(D|M).\nFrom this, we see that two-part MDL can be considered to be a regularized maximum likelihood approach very similar to Bayesian inference. A crucial difference is that in MDL, L(M) serves as statistical formalization of the desired model class, whereas in Bayesian inference the analogous term expresses prior beliefs about the distribution of model parameters.\nDue to MDL's roots in coding theory, it is common to call an MDL objective function a code, and to speak of encoding, transmitting, or sending the individual elements that make up a code.\nWe next detail how we design our code."
    }, {
      "heading" : "4 The PS-2 MDL Code",
      "text" : "Given a listD of cognate tuples fromN languages, we use the Minimum Description Length principle to infer the statistically significant correspondence rules. Our encoding builds upon and extends the encoding introduced by (Tatti and Vreeken, 2012) for discovering small sets of serial episodes in event sequences.\nWe proceed with a two-part code, requiring us to solve the optimization problem\nM = arg min M∈M L(M) + L(D|M).\nFirst, we must determine the model class, M. As explained in Section 2, we here seek to find associated character strings between languages. Thus, mathematically our model class is the set of sets of tuples associating strings from the individual languages' alphabets. Wemake no assumptions about either their shape or their distribution.\nWe begin by discussing our model code L(M).\n4.1 Model Code L(M) Our models consist of N alphabets Σi and a correspondence rule table which we call Π. Our total\n1We use log(.) = log2(.) throughout the paper.\nmodel description length is given by\nL(M) = N∑ i=1 (L(Σi)) + L(Π).\nIn order to describe the rules fromΠ, we require the code lengths for all letters σ from all alphabets Σi. Since we are interested only in complexities, we disregard the actual code words and focus only on their lengths. For ease of exposition, we first discuss L(Π). In essence, our Π is a list of independent correspondence rules. We next describe how we encode an individual rule.\nEncoding a Correspondence Rule Rules π ∈ Π are of the form π = (π1, ..., πN ) with πi ∈ Σ∗i . To encode one such rule, we must specify a) how long the string from each of the languages is and b) which letters it contains. However, if we specify lengths even when they are zero, we pay with higher description lengths than are necessary. This imposes a bias particularly against rules which are sparsely populated. Thus, we encode each rule π ∈ Π, π = (π1, ..., πN ) as follows: we transmit all N entries independently of each other, sending their lengths and character sequences only where defined:\nL(π) = N∑\nn=1 πn ̸=ϵ\n( LN(|πn|) +\n∑ σ∈πn L(code(σ))\n) .\nWe encode each string's length with LN, the universal code for the integers (Rissanen, 1983), which is the MDL-optimal code for natural numbers of unknown, arbitrary size. For transmitting the strings itself, we use code(σ), i.e. the optimal unigram for symbol usages in all rules.\nEncoding the Rule Table  If we want to use the code for individual rules given above, we must specify for every rule which subset of languages it is defined on. We can straightforwardly classify each rule according towhich subset it is defined on. Then, we must specify how many rules defined on each of the different subsets there are. There are 2N − 1 different language subsets on which a rule may be defined. We encode the number of rules of each kind via LN. These numbers must be offset since LN(n) is defined for n ≥ 1, and there may be zero rules of a certain kind.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nThus:\nL(Π) = 2N−1∑ i=1 LN(|ΠCi |+ 1) + ∑ π∈ΠCi L(π)  + LN(TΠ) + log( ( TΠ − 1 |Π| − 1 ) ).\nwhere count(π) is the number of occurrences of π, TΠ = ∑ π∈Π count(π) and where ΠCi is the set of rules defined on the i-th subset of languages enumerated in some canonical way.\nAdditionally, to describe our data items using rules, we must specify the optimal code lengths for using each of the rules. In L(Π), this corresponds to the last two summands. We do this by a datato-model code (Grünwald, 2007). Data-to-model codes are used to code uniformly from an enumeration of models, i.e. without preference towards any particular model. Since we know that none of the symbols of any alphabets will have a non-zero count, the data-to-model code is given by the weak number composition of the alphabets symbols' total counts over the number of symbols.\nEncoding the Alphabets For describing the strings of each rule, we use the Shannon-optimal code for the individual alphabets' symbols. Thus, we must first transmit the unigrams, i.e. code(σ) ∀σ ∈ Σi for every alphabet Σi. We again do this by a data-to-model code, i.e. by coding uniformly from all possible distributions.\nSetting TΣi = ∑\nσ∈Σi count(σ), the total transmission cost relating to some Σi becomes\nL(Σi) = LN(TΣi) + log( ( TΣi − 1 |Σi| − 1 ) ).\nThe alphabet sizes are constant for any given data set and therefore it is not necessary to include them in the code.\nHaving described our model, we next turn our attention to encoding the data with a given model.\n4.2 Data Code L(D|M) To encode data with our model, we simply transmit the correspondences best used to describe each data entry. Thus, we get\nL(D|M) = ∑ d∈D L(d|M)\nwhere L(d|M) = LN(|d|) + ∑ π∈d L(code(π)).\nAgain, it is not necessary to specify the number of data entries as they do not change for the same data set. For the individual data entries, we transmit their lengths via LN and specify which correspondence rules they are best aligned with via the best usage code for the rules, code(π). This leaves us to discuss finding the data's description given some rules, and how to infer rules."
    }, {
      "heading" : "4.3 Alignment Procedure",
      "text" : "Computationally, finding the best description for a data item boils down to finding the best alignment for it. We formulate this as a shortest-path problem in a weighted, directed graph and use Dijkstra's algorithm (Dijkstra, 1959) to find optimal alignments. Nodes in the graph represent index tuples, while edges describe the applicable rules. By partial order reduction, we make our graphs as small as possible. Nonetheless, due to the combinatorial nature of the problem, there are bottlenecks in memory consumption as well as in runtime. With our current implementation, we can process up to five languages from our most complicated data set within a few hours on a 4GB RAM, 2.5GHz single core desktop machine. We plan to extend this to higher N in future work."
    }, {
      "heading" : "4.4 Training Procedure",
      "text" : "Inferring correspondences of arbitrary length is a combinatorial, non-convex optimization problem defined over a large, unstructured search space. However, as we argued in Section 2, if we are given a rule table with costs, we can compute the optimal alignment of all data with these rules. Likewise, if we are given an alignment of all data, we can improve our model from it. Therefore, we can find good solutions by Expectation-Maximization (Dempster et al., 1977).\nInitialization At the beginning of training, we either initialize our model with a null alignment or with a greedy alignment from a given set of rules. A null alignment is one in which only singleton rules are used, i.e. only rules which consist of exactly one character from exactly one language. Starting from a given rule set allows to input linguistic knowledge in an intuitive way.\nExpectation Step In the Expectation step, we align all data items with the rules from the current rule table Π and the current usage costs for the rules. This results in new counts for all rules from which we compute costs in the next step. We\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nemploy Laplace correction in order to ensure that the algorithm is always able to explain all data and may choose to not use locally suboptimal patterns.\nThe time complexity of our E step is in O(|D| · |R|2), where R is the maximum number of possible rule applications in a single data entry.\nMaximization Step In the Maximization step, we optimize our code table. We do this by merging together the two patterns which lead to the highest decrease in overall description length. The intuition behind this is the observation that if a longer pattern is useful, then any sub-pattern of it will be at least as or more useful. It is important to note that in this way, the learned correspondences grow according to their statistical significance.\nEachM step has time complexityO(|D|·A2/2), where A is the maximum number of rules used to align a single data entry.\nIt is possible that a rule R is deemed good on the basis of the entire data set when in fact it is suboptimal for some subset S of the data. Let us assume that the entries in S have become aligned with ruleR even though an overlapping, but different rule Q would have been a better choice. Then, S will be \"lost\" in regards to discovering Q, as S will not be counted as evidence for Q. However, if the remaining data contain enough evidence to learn the rule Q independently of S, Q will be in fact learned and may then be used for aligning S.\nIn this fashion, we deterministically learn the important structures in the data, although occasionally we may miss some of the more subtle correspondence rules."
    }, {
      "heading" : "4.5 Data Over-Weighting",
      "text" : "MDL guards against overfitting by balancing the complexity of themodel with that of the data – only those correspondences with sufficient evidence in the data are included. By confining ourselves fully to the data and not relying on any further assumptions, we obtain objective results. However, it may be that our focus in on the obtained rules rather than on the objective statistical analysis of the data.\nFor example, in cognate reconstruction, we require rules which relate strings across languages, but not those which exclusively describe substrings from the separate languages – the latter contribute nothing to successfully adapting a word from its known form(s) to an unknown one.\nWe can also encounter problems working on under-resourced or very rich languages, or even\nwhen simply exploring correspondences between higher numbers of languages, where the need for data may exceed our ability to provide it. Sometimes, we simply may not have sufficient statistical evidence to discover all desired rules. In such cases we can turn to correspondences which objectively are not statistically significant for the given data, but are almost so. We can straightforwardly include those near-significant correspondences by over-representing the data. At the corpus level, this can be done by simply assigning the data complexity term a higher weight than the model term, i.e. via optimizing\nL(M) + αL(D|M)\nwith α ̸= 1 instead of the original formula. In this formula, a data weight of e.g. 2 corresponds to using twice the amount of identical data. More subtly, we can also assign individual cognate tuples a higher weight than others. Again, assigning some data entry a weight of 2 intuitively duplicates that entry in the data set. With the data overrepresented in such a fashion, the algorithm will have enough evidence to include larger correspondence rules than before, as they effectively become more useful for describing the (over-represented) data. Nonetheless, new rules will be discovered in the \"correct\" order; that is, ordered by statistical significance. Clearly, this moves results away from the objective territory of MDL and introduces a user's subjective judgment. However, it allows a linguistic expert to choose a desired level of detail for inferred correspondences. We illustrate the effects and helpfulness of this idea experimentally, in Section 5."
    }, {
      "heading" : "5 Experiments & Results",
      "text" : "We show our approach's efficacy in several ways. First, we present a standard pairwise analysis for a group of languages for which we have collected data we deem representative. We compute pairwise distances, construct a phylogenetic tree, and compare this tree to linguistic classifications. Second, we present a detailed analysis of four languages simultaneously. Because our approach is not limited to pairs of languages, we can give an information-theoretic quantification of linguistic similarity in a much more detailed fashion than was previously possible.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nIn both cases, we report some of the learned correspondences. We also show the effect of data over-weighting, as introduced in Section 4.5.\nWe first discuss our data, then present results."
    }, {
      "heading" : "5.1 Data Sets",
      "text" : "We compiled two data sets for our experiments. Firstly, we use Swadesh lists for 13 modern Slavic languages taken from the wiktionary.2 The languages are Czech, Polish, Slovak, Lower Sorbian, Upper Sorbian (west Slavic), Russian, Belarussian, Ukrainian, Rusyn (east Slavic), Bulgarian, Macedonian, Slovenian, and Serbo-Croatian (south Slavic). For Serbo-Croatian, we have both a version in Latin script and one in Cyrillic script.\nSecondly, we add a set of Slavic cognates containing internationalisms and pan-Slavic words for Czech, Polish, Russian, and Bulgarian.3\nAll our data is in raw orthographic form, without transcriptions of any kind. It consists mostly of verbs, adjectives, and nouns.\ndata all lang. RU-BG CS-PL CS-PL-RU-BG size 207 778 778 778\nT 1: Data set sizes for experiments.\nFor all of our experiments, we use only those entries that contain words for all languages in question. While our algorithm is agnostic to gaps in data, this makes for easier comparison."
    }, {
      "heading" : "5.2 Pairwise Analyses",
      "text" : "For pairwise analysis, we require some measure of distance between pairs of languages. In MDLbased modeling, it is common to use Normalized Compression Distance (NCD) (Cilibrasi and Vitanyi, 2005) for this. Intuitively, NCD measures how hard it is to describe X and Y together compared to how hard it is to describe them separately. It is defined as\nNCD(X,Y ) = L(X,Y )−min(L(X,X), L(Y, Y ))\nmax(L(X,X), L(Y, Y ))\nwhere L(X,Y ) is the description length when encoding languagesX and Y jointly. NCD is a mathematical distance; lower values mean that two data sets are more similar.\nWe train models for all pairs from our languages and obtain correspondence rules and NCDs.\n2Taken from https://en.wiktionary.org/wiki/ Appendix:Slavic_Swadesh_lists.\n3Compiled from (Likomanova, 2004) and (Angelov, 2004).\nNCDs In Table 2 we show the NCD values for all pairwise comparisons. We use ISO 639-1 and ISO 639-3 codes to identify the languages, except for Serbo-Croatian, which we denote by SCl in its Latin version and and SCc in its Cyrillic version. We indicate lowest and highest NCDs per row in bold and italic text, respectively. Our table reveals that languages from the same linguistic group tend to have lower NCD than languages from differing groups. The south Slavic group is linguistically further divided into a southwestern group (Slovene and Serbo-Croatian) and a southeastern sub-group (Macedonian and Bulgarian). Indeed we identify Slovenian and SerboCroatian as more similar to languages from the west Slavic group than to the east Slavic group. We can see that the Serbo-Croatian data in Latin script was assessed to be slightly closer to the other languages that use Latin script, while the Cyrillic version was deemed more similar to other languages using Cyrillic.\nusb lsb CS SK PL SL SCl SCc MK BG RU UK rue BE usb .00 .52 .53 .52 .60 .57 .61 .62 .76 .75 .68 .70 .67 .64 lsb .52 .00 .65 .66 .72 .67 .68 .71 .87 .85 .80 .82 .78 .74 CS .53 .65 .00 .41 .56 .50 .53 .55 .71 .69 .61 .64 .58 .59 SK .52 .66 .41 .00 .58 .48 .51 .56 .68 .66 .60 .65 .59 .60 PL .60 .72 .56 .58 .00 .64 .64 .67 .82 .79 .71 .74 .69 .63 SL .57 .67 .50 .48 .64 .00 .36 .39 .59 .58 .61 .65 .60 .61 SCl .61 .68 .53 .51 .64 .36 .00 .04 .54 .57 .63 .66 .62 .63 SCc .62 .71 .55 .56 .67 .39 .04 .00 .51 .53 .60 .63 .59 .59 MK .76 .87 .71 .68 .82 .59 .54 .51 .00 .54 .74 .78 .75 .75 BG .75 .85 .69 .66 .79 .58 .57 .53 .54 .00 .70 .77 .70 .71 RU .68 .80 .61 .60 .71 .61 .63 .60 .74 .70 .00 .52 .53 .51 UK .70 .82 .64 .65 .74 .65 .66 .63 .78 .77 .52 .00 .45 .45 rue .67 .78 .58 .59 .69 .60 .62 .59 .75 .70 .53 .45 .00 .54 BE .64 .74 .59 .60 .63 .61 .63 .59 .75 .71 .51 .45 .54 .00\nT 2: NCDs for 13 Slavic languages.\nInferred Phylogenetic Tree For easier viewing, we construct a phylogenetic tree from theNCDvalues, which we show in Figure 1. For this we use the neighbor joining method (Saitou and Nei, 1987) and place the root manually.4 The greater the horizontal distance between two languages, the less similar they are. As we see, the algorithm groups the languages according to their linguistic classification. It identifies Bulgarian and Macedonian as slight outliers\n4Picture generated with http://etetoolkit. org/treeview/, tree generated with scikit-bio: http://scikit-bio.org/.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nF 1: NCD-based Slavic phylogenetic tree.\nin the south Slavic group, and Polish, Upper and Lower Sorbian as such in the west Slavic group.\nThis is an expected result. Bulgarian and Macedonian are outliers in that they have largely lost case declension. Adjectives and verbs from these languages oftentimes employ zero endings or comparatively shorter endings than the other languages. This leads to overall lower BG-BG and MK-MK description lengths, but still high NCDs to the other languages. In the Swadesh list, Upper and Lower Sorbian words are often of a different etymological heritage than words from the other Slavic languages. This explains their outlier status. Polish is a slight outlier due to its frequent use of digraphs and prolific palatalization, which increases complexity of Polish patterns.\nInferred Correspondences In Table 3, we present some example alignments from the models for the CS-PL and RU-BG language pairs.\nTo give insight into what kinds of rules emerge naturally, and what kind emerge from overrepresenting data as proposed in Section 4.5, we present two different alignments per selected example: firstly one obtained from an objective model, and secondly one obtained from overrepresented data. For the latter, we over-weight our data until there are no more singleton rules in any of the alignments. We mark over-weighted versions with *.\n(PL) (CS) z e m ě z ie m i a *ziem ia *zem ě (PL) (CS) m i l c z e ć m l č e t *mi l cz eć *m l č et (PL) (CS) r ó g r o h *ró g *ro h (PL) (CS) r o z d z i e l i ć r o z d ě l i t *roz dzie li ć *roz dě li t\n(RU) (BG) м о л о д о с т ь м л а д о с т *м оло до сть *м ла до ст\n(RU) (BG) п ъ л е н п о л н ый *п ъл ен *п ол ный\nT 3: Example CS-PL, RU-BG correspondences.\nAs can be seen, the discovered correspondences are of different granularities and linguistic char-\nacter. We find purely phonological rules such as (g,h) along with purely orthographic ones such as (cz,č), verb endings such as (ć,t), aberrations thereof such as (eć,et), liquid metatheses such as (ла,оло), palatalizations such as (dzie,dě), and even stem correspondences such as (ziem,zem). Over-representing the data allows to select from a desired level of detail. We plan to discuss potential applications of the different resulting rules in future work."
    }, {
      "heading" : "5.3 Four-Way Analysis",
      "text" : "Next, we turn our attention to more fine-grained analysis of linguistic similarity. Restricting ourselves to pairwise analyses and grouping the most similar languages together in a phylogeny causes us to miss many subtle similarities. Existing approaches which are limited to pairwise correspondences incur infeasible amounts of computationwhen trying to use them for the simultaneous analysis of multiple languages. Our algorithm is agnostic to the number of input languages and can be used to efficiently analyze more than two languages at a time. This allows for highly detailed information-theoretic quantification of the similarities among groups of languages. To show how this can be done, we first present some four-way CS-PL-RU-BG example alignments in Figure 4. Theywere computedwithout over-weighting.\n(PL) (CS) (RU) (BG) p i ć p í t п и ть п и я (PL) (CS) (RU) (BG) s p e c j a l n y s p e c i á l n í с п е ц и а ль н ы й с п е ц и а л ен\n(PL) (CS) (RU) (BG) m i ł y m i l ý м и л ый м и л (PL) (CS) (RU) (BG) ś m i a ł y s m ě l ý с м е л ый с м е л\nT 4: Example CS-PL-RU-BG correspondences.\nObserve that some of the discovered rules link only two or three languages, with the other language(s) described by separate patterns. We have selected some examples to highlight the differences in i vowels. In the data, there is enough evidence to discover various rules, such as (,,и,и), (,i,и,и), but not enough evidence to include a fourway rule (j,i,и,и). In consequence, the internationalism specjalny is analyzed with the three-way i correspondence plus a Polish singleton rule (j,,,). There are also cases where using two rules, such as (i,í,,) plus (,,и,и) in pić (to drink), is a good choice.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nBecause we designed our algorithm to discover the correspondence rules according to their statistical significance, this can be exploited for finegrained analyses of similarity. Statistically, the observed correspondences between two or three languages are significant for the data, while potential larger ones are not. In other words: there is more regularity in some of the languages than others."
    }, {
      "heading" : "5.3.1 Shared Description Lengths",
      "text" : "To quantify the amount of structure that individual languages share, we can compare the description lengths of their rules. For this, we define the shared Description Length of languages Li1 , ..., Lik as\nsDL(Li1 , ..., Lik) := ∑\nπ∈Q(Li1 ,...,Lik )\nL(π).\nwhere Q(Li1 , ..., Lik) contains all rules which are non-empty exactly for languages Li1 , ..., Lik .\nFigures 2 and 3 show sDLs for the CS-PL, RUBG, and CS-PL-RU-BG models.\n.\n.cs-pl ru-bg\nPL\nBG\nCS\nRU\nCS-PL\nRU-BG\nshared Description Length\nF 2: sDLs for the CS-PL and RU-BGmodels. Total rule desc. lengths: 1852.43 bits (CS-PL), 1496.62 bits (RU-BG)\nFigure 2 reveals that RU diverges more than BG does from the RU-BG joint description, and that CS does so for CS-PL. Because we chose only cognate tuples defined for all four languages for this experiment, we can also compare the two language pairs. There, we see that CS-PL requires a larger description, and that Czech alone takes a somewhat larger fraction of total description length.\n. all CS PL RU BG\nRU -BGCS -PL PL -RU\nPL -RU\n-BG\nCS -PL\n-RU\nCS -PL\n-BG PL -BG CS -RU CS -BG\nCS -RU\n-BG 0\n500\n1,000\nF 3: sDLs for the CS-PL-RU-BG model.\nFigure 3 finally gives us a quantification of the similarities between all language sub-sets from our Czech-Polish-Russian-Bulgarian set. We see that\nthe four-way sDL is the biggest contributor overall. It quantifies the complexity of the structure shared by all four languages. We also see that each of the individual languages have significant overheads to the four-way shared description length. The language with the highest individual description length by far is Czech. This is not surprising, as Czech has the largest number of diacritically-modified symbols. For example, every Czech vowel can be marked as long with the čárka, giving us e.g. é as long version of e. The algorithm furthermore identifies the linguistic grouping between Czech and Polish. Both CS-PL and RU-BG share significant portions of description length. In the NCD table, Table 2, we can observe that the south Slavic languages were somewhat in between the west and the east Slavic languages. In this analysis, we can see that Bulgarian is in fact very similar to Russian, so much so that in the four-way analysis, grouping RU-BG seems as good a choice as grouping CS-PL.5 Beyond this, we identify further, more subtle similarities. Taking a closer look we see they are between Russian and other, not-yet-covered language subsets. To highlight this, we have plotted the similarities to Russian in red. This is a highly satisfying result, as in fact we expect Russian, the Slavic language with the largest amount of native speakers, to heavily influence the other languages."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We studied the problem of automatically inferring objective string-level correspondences from data. We introduced an MDL-based approach and gave an efficient algorithm for finding correspondences. Our experiments show that the approach works well in practice. We constructed a sensible phylogeny for our languages, demonstrated the discovered correspondences, and showed that our algorithm quantifies similarity not only between pairs, but between all subsets of analyzed languages. While our algorithm is deterministic in its pure form, it is easy to integrate non-determinism e.g. by simulated annealing, since the alignments can be randomized in any step. However, the possibility to input linguistic knowledge and obtain deterministic results makes our approach particularly promising for linguists. We plan to discuss its linguistic potential in future work.\n5Please note that this is purely on the basis of superficial word form similarity.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "EuroComSlav Basiskurs - der panslavische Wortschatz",
      "author" : [ "A. Angelov" ],
      "venue" : null,
      "citeRegEx" : "Angelov.,? \\Q2004\\E",
      "shortCiteRegEx" : "Angelov.",
      "year" : 2004
    }, {
      "title" : "B",
      "author" : [ "R. Cilibrasi", "P. M" ],
      "venue" : "Vitanyi.",
      "citeRegEx" : "Cilibrasi and Vitanyi2005",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "and D",
      "author" : [ "A.P. Dempster", "N.M. Laird" ],
      "venue" : "B. Rubin.",
      "citeRegEx" : "Dempster et al.1977",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "A note on two problems in connexion with graphs. NUMERISCHE MATHEMATIK, 1(1):269--271",
      "author" : [ "E.W. Dijkstra" ],
      "venue" : null,
      "citeRegEx" : "Dijkstra.,? \\Q1959\\E",
      "shortCiteRegEx" : "Dijkstra.",
      "year" : 1959
    }, {
      "title" : "The minimum description length principle. Adaptive computation and machine learning",
      "author" : [ "P.D. Grünwald" ],
      "venue" : null,
      "citeRegEx" : "Grünwald.,? \\Q2007\\E",
      "shortCiteRegEx" : "Grünwald.",
      "year" : 2007
    }, {
      "title" : "A universal prior for integers and estimation by minimum description length",
      "author" : [ "Jorma Rissanen" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "Rissanen.,? \\Q1983\\E",
      "shortCiteRegEx" : "Rissanen.",
      "year" : 1983
    }, {
      "title" : "The neighbor-joining method: a new method for reconstructing phylogenetic trees",
      "author" : [ "Saitou", "Nei1987] N. Saitou", "M. Nei" ],
      "venue" : "Molecular biology and evolution,",
      "citeRegEx" : "Saitou et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Saitou et al\\.",
      "year" : 1987
    }, {
      "title" : "PercyNohama",
      "author" : [ "Stefan Schulz", "Kornél Markó", "Eduardo Sbrissia" ],
      "venue" : "andUdoHahn.",
      "citeRegEx" : "Schulz et al.2004",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A mathematical theory of communication",
      "author" : [ "C.E. Shannon" ],
      "venue" : "SIGMOBILE Mob. Comput. Commun. Rev.,",
      "citeRegEx" : "Shannon.,? \\Q2001\\E",
      "shortCiteRegEx" : "Shannon.",
      "year" : 2001
    }, {
      "title" : "Unsupervised multilingual learning for morphological segmentation",
      "author" : [ "Snyder", "Barzilay2008] Benjamin Snyder", "Regina Barzilay" ],
      "venue" : "In Proceedings of ACL-08: HLT,",
      "citeRegEx" : "Snyder et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Snyder et al\\.",
      "year" : 2008
    }, {
      "title" : "Regina Barzilay",
      "author" : [ "Benjamin Snyder" ],
      "venue" : "and Kevin Knight.",
      "citeRegEx" : "Snyder et al.2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "L",
      "author" : [ "J O." ],
      "venue" : "Szemerenyi.",
      "citeRegEx" : "Szemerenyi1970",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "The long and the short of it: Summarising event sequences with serial episodes",
      "author" : [ "Tatti", "Vreeken2012] N. Tatti", "J. Vreeken" ],
      "venue" : "In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "Tatti et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tatti et al\\.",
      "year" : 2012
    }, {
      "title" : "and R",
      "author" : [ "H. Wettig", "S. Hiltunen" ],
      "venue" : "Yangarber.",
      "citeRegEx" : "Wettig et al.2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "How can we automatically discover the most important correspondences between words from two or more languages? How can we do so allowing for correspondences between any subset of languages, without drowning in redundant results, and at the same time maintaining control over the level of detail? These are exactly the questions we answer in this paper. We approach the problem with the Minimum Description Length principle, and give an efficient algorithm for discovering statistically important correspondences. We test the efficacy of our method against a set of Slavic languages. The experiments show our method automatically discovers non-trivial associations, allowing for both quantitative and qualitative analysis of multiple languages.",
    "creator" : "LaTeX with hyperref package"
  }
}