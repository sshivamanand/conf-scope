{
  "name" : "151.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Parsing for Universal Dependencies without training",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012). However, many data-driven approaches struggle with learning relations that match the conventions of the test data, e.g., Klein and Manning reported the tendency of their DMV parser to make determiners the heads of German nouns. Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences.\nThe Universal Dependencies (UD) project (Nivre et al., 2015) offers a dependency formalism that aims at providing a consistent representation across languages, while enforcing a few hard constraints. The arrival of such treebanks, expanded and improved on a regular basis, provides a new milestone for cross-lingual dependency parsing research (McDonald et al., 2013). Furthermore, we expect that such a formalism lends itself more naturally to a simple and linguistically sound rulebased approach to cross-lingual parsing. In this paper we present such an approach.\nOur system is a dependency parser that requires no training, and relies solely on explicit part-ofspeech (POS) constraints that UD imposes. In par-\nticular, UD prescribes that trees are single-rooted, and that function words like adpositions, auxiliaries, and determiners are always dependents of content words, while other formalisms might treat them as heads (De Marneffe et al., 2014).\nContributions Our method goes beyond the existing work on rule-aided unsupervised dependency parsing by a) adapting dependency head rules to UD-compliant POS relations, b) incorporating the UD restriction of function words being leaves, c) using personalized PageRank to improve main predicate identification, and d) making it completely free of language-specific parameters by estimating adposition attachment direction directly on test data.\nWe evaluate our system on 32 languages1 in three setups, depending on the reliability of available POS tags, and compare to a multi-source delexicalized transfer system. In addition, we evaluate the systems’ sensitivity to domain change for a subset of UD languages for which domain information was retrievable. The results expose a solid and competitive system for all UD languages. Our unsupervised parser compares favorably to delexicalized parsing, while being more robust to domain change across languages."
    }, {
      "heading" : "2 Related work",
      "text" : "Over the recent years, cross-lingual linguistic structure prediction based on model transfer or projection of POS tags and dependency trees has become a relevant line of work (Das and Petrov, 2011; McDonald et al., 2011). These works mostly use supervised learning and different target language adaptation techniques.\n1Out of 33 languages in UD v1.2. We exclude Japanese because the treebank is distributed without word forms and hence we can not provide results on predicted POS.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nThe first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages. Projection relies on the availability and quality of parallel corpora, sourceside taggers and parsers, but also tokenizers, sentence aligners, and word aligners for sources and targets. Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann (2014) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora (Ma and Xia, 2014; Rasooli and Collins, 2015).\nThe second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extend delexicalization to involve multiple sourceside parsers. This line of work depends on applying uniform POS and dependency representations (McDonald et al., 2013).\nBoth model transfer and annotation projection rely on a large number of presumptions to derive their competitive parsing models. By and large, these presumptions are unrealistic and exclusive to a group of very closely related, resource-rich Indo-European languages. Agić et al. (2015) expose some of the biases in their proposal for realistic cross-lingual POS tagging, as they emphasize the lack of perfect sentence and word splitting for truly low-resource languages. Johannsen et al. (2016) introduce joint projection of POS and dependencies from multiple sources while sharing the outlook on bias removal in real-world multilingual processing.\nCross-lingual learning, realistic or not, depends entirely on the availability of data: for the sources, for the targets, or most often for both sets of languages. Moreover, it typically does not exploit the constraints placed on the linguistic structures through the formalism, and it does so by design. With the emergence of UD as the practical standard for cross-language annotation of POS and syntactic dependencies, we argue for an approach that takes a fresh angle on both aspects. Namely, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploit-\ning the UD constraints on building POS and dependency annotations.\nThese two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of unsupervised parsers falls far behind the approaches involving any sort of supervision.\nOur work builds on the research on rule-aided unsupervised dependency parsing (Gillenwater et al., 2010; Naseem et al., 2010; Søgaard, 2012a; Søgaard, 2012b). In particular, we make use of Søgaard’s (2012b) PageRank method to rank words before decoding. Our system, however, has two key differences: i) the usage of PageRank personalization, and of ii) two-step decoding to treat content and function words differently according to the UD formalism. Through these differences, even without any training data, we parse nearly as well as a delexicalized transfer parser, and with increased stability to domain change."
    }, {
      "heading" : "3 Method",
      "text" : "Our approach does not use any training or unlabeled data. We have used the English treebank during development to assess the contribution of individual head rules, and to tune PageRank parameters (Sec. 3.1) and function-word directionality (Sec. 3.2). Adposition direction is calculated on the fly on test data. In the following, we refer to our UD parser as UDP."
    }, {
      "heading" : "3.1 PageRank setup",
      "text" : "Our system uses the PageRank (PR) algorithm (Page et al., 1999) to estimate the relevance of the content words of a sentence. PR gives higher rank to nodes with more incoming edges, as well as to nodes connected to those. Using PR to score word relevance requires an effective graphbuilding strategy. We have experimented with the strategies by Søgaard (2012b), but our system fares best strictly using the dependency rules in Table 1 to build the graph.\nWe build a multigraph of all words in the sentence covered by the head-dependent rules in Table 1, giving each word an incoming edge for each eligible dependent, i.e., ADV depends on ADJ and VERB. This strategy does not always yield connected graphs, and we use a teleport probability of 0.05 to ensure PR convergence. We chose this\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nvalue incrementally in intervals of 0.01 during development until we found the smallest value that guaranteed PR convergence. A high teleport probability is undesirable, because the resulting stationary distribution can be almost uniform. We did not have to re-adjust this value when running on the actual test data.\nThe the main idea behind our personalized PR approach is the observation that ranking is only relevant for content words.2 PR can incorporate a priori knowledge of the relevance of nodes by means of personalization, namely giving more weight to certain nodes. Intuitively, the higher the rank of a word, the closer it should be to the root node, i.e., the main predicate of the sentence is the node that should have the highest PR, making it the dependent of the root node (Fig. 1, lines 4-5). We use PR personalization to give 5 times more weight (over an otherwise uniform distribution) to the node that is estimated to be main predicate, i.e., the first verb or the first content word if there are no verbs."
    }, {
      "heading" : "3.2 Head direction",
      "text" : "Head direction is an important syntactic trait. Indeed, the UD feature inventory contains a trait to distinguish adposition between pre- and postpositions. Instead of relying on this feature from the treebanks, which is not always provided, we estimate the frequency of ADP-NOMINAL vs. NOMINAL-ADP bigrams.3 This estimation requires very few examples to converge (10-15 sentences), and we calculate it directly on test data.\nIf a language has more ADP-NOMINAL bigrams, we consider all its ADP to be prepositions (and thus dependent of elements at their right). Otherwise, we consider them to be postpositions.\nFor other function words, we have determined on the English dev data whether to make them strictly right- or left-attaching, or to allow either direction: AUX, DET, and SCONJ are right-attaching, while CONJ and PUNCT are leftattaching. There are no direction constraints for the rest."
    }, {
      "heading" : "3.3 Decoding",
      "text" : "Fig. 1 shows the tree-decoding algorithm. It has two blocks, namely a first block (3-11) where we assign the head of content words according to\n2ADJ, NOUN, PROPN, and VERB mark content words. 3NOMINAL= {NOUN, PROPN, PRON}\n1: H = ∅; D = ∅ 2: C = 〈c1, ...cm〉; F = 〈f1, ...fm〉 3: for c ∈ C do 4: if |H| = 0 then 5: h = root 6: else 7: h =argminj∈H {γ(j, c) | δ(j, c)∧κ(j, c)} 8: end if 9: H = H ∪ {c} 10: D = D ∪ {(h, c)} 11: end for 12: for f ∈ F do 13: h =argminj∈H {γ(j, f) | δ(j, f) ∧ κ(j, f)} 14: D = D ∪ {(h, f)} 15: end for 16: return D\nFigure 1: Two-step decoding algorithm for UDP.\nADJ −→ ADV VERB −→ ADV, AUX, NOUN, PROPN, PRON, SCONJ NOUN, PROPN −→ ADP, DET, NUM NOUN, PROPN −→ ADJ, NOUN, PROPN\nTable 1: UD dependency rules\ntheir PageRank and the constraints of the dependency rules, and a second block (12-15) where we assign the head of function words according to their proximity, direction of attachment, and dependency rules. The algorithm requires:\n1. The PR-sorted list of content words C. 2. The set of function words F . 3. A set H for the current possible heads, and\na set D for the dependencies assigned at each iteration, which we represent as headdependent tuples (h, d). 4. A symbol root for the root node. 5. A function γ(n,m) that gives the linear dis-\ntance between two nodes. 6. A function κ(h, d) that returns whether the\ndependency (h, d) has a valid attachment direction given the POS of the d (cf. Sec. 3.2). 7. A function δ(h, d) that determines whether (h, d) is licensed by the rules in Table 1.\nThe head assignations in lines 7 and 13 read as follow: the head h of a word (either c or f ) is the closest element of the current list of heads (H) that has the right direction (κ) and respects the POSdependency rules (δ). These assignations have a back-off option to ensure the final D is a tree. If the conditions determined by κ and δ are too strict,\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n378\n379\n380\n381\n382\n383\n384\n385\n386\n392\n393\n394\n395\n396\n397\n398\n399\ni.e. if the set of possible heads is empty, we drop the δ head-rule constraint and recalculate the closest possible head that respects the directionality imposed by κ. If the set is empty again, we drop both constraints and assign the closest head.\nLines 4 and 5 enforce the single-root constraint. To enforce the leaf status of function nodes, the algorithm first attaches all content words (C), and then all function words (F ) in the second block where H is not updated, thereby ensuring leafness for all f ∈ F . The order of head attachment is not monotonic wrt. PR between the first and second block, and can yield non-projectivities. Nevertheless, it still is a one-pass algorithm. Decoding runs in less than O(n2), namely O(n× |C|). However, running PR incurs the main computation cost."
    }, {
      "heading" : "4 Parser run example",
      "text" : "This section exemplifies a full run of UDP for the example sentence “They also had a special connection to some extremists”, an actual clause from the English test data."
    }, {
      "heading" : "4.1 PageRank",
      "text" : "Given an input sentence and its POS tags, we obtain rank of each word by building a graph using head rules and running PR on it. Table 2 provides the sentence, the POS of each word, the number of incoming edges for each word after building the graph with the head rules from Sec. 3.1, and the personalization vector for PR on this sentence. Note that all nodes have the same personalization weight, except the estimated main predicate, the verb “had”.\nWord: They also had a special connection to some extremists POS: PRON ADV VERB DET ADJ NOUN ADP DET NOUN\nPersonalization: 1 1 5 1 1 1 1 1 1 Incoming edges: 0 0 4 0 1 5 0 0 5\nTable 2: Words, POS, Personalization and incoming edges for the example sentence.\nTable 3 shows the directed multigraph used for PR in detail. We can see e.g. that the four incoming edges for the verb “come” from the two nouns, plus from the adverb “also” and the pronoun “They”.\nAfter running PR, we obtain the following ranking for content words: C = 〈had,connection,extremists,special〉 Even though the verb has four incoming edges and the nouns have six each, the personalization makes the verb the highest-ranked word."
    }, {
      "heading" : "4.2 Decoding",
      "text" : "Once C is calculated, we can follow the algorithm in Fig. 1 to obtain a dependency parse. Table 4 shows a trace of the algorithm, given C and F : C = 〈had,connection,extremists,special〉 F = {They, also, a, to, some}\nThe first four iterations calculate the head of content words following their PR, and the following iterations attach the function words in F .\nFinally, Fig. 2 shows the resulting dependency tree. Full lines are assigned in the first block (content dependents), dotted lines are assigned in the second block (function dependents). The edge labels indicate in which iteration the algorithm has assigned each dependency.\nNote that the algorithm is deterministic for a certain input POS sequence. Any 10-token sentence with the POS labels shown in Table 2 would yield the same dependency tree.4\n4The resulting trees always pass the validation script in\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nLanguage BLG UDPG MSDG MSDP UDPP UDPN\nAncient Greek 42.2 L 43.4 48.6 46.5 41.6 27.0 Arabic 34.8 R 47.8 52.8 52.6 47.6 41.0 Basque 47.8 R 45.0 51.2 49.3 43.1 22.8\nBulgarian 54.9 R 70.5 78.7 76.6 68.1 27.1 Church Slavonic 53.8 L 59.2 61.8 59.8 59.2 35.2\nCroatian 41.6 L 56.7 69.1 65.6 54.5 25.2 Czech 46.5 R 61.0 69.5 67.6 59.3 25.3\nDanish 47.3 R 57.9 70.2 65.6 53.8 26.9 Dutch 36.1 L 49.5 57.0 59.2 50.0 24.1\nEnglish 46.2 R 53.0 62.1 59.9 51.4 27.9 Estonian 73.2 R 70.0 73.4 66.1 65.0 25.3 Finnish 43.8 R 45.1 52.9 50.4 43.1 21.6 French 47.1 R 64.5 72.7 70.6 62.1 36.3 German 48.2 R 60.6 66.9 62.5 57.0 24.2 Gothic 50.2 L 57.5 61.7 59.2 55.8 34.1 Greek 45.7 R 58.5 68.0 66.4 57.0 29.3\nHebrew 41.8 R 55.4 62.0 58.6 52.8 35.7 Hindi 43.9 R 46.3 34.6 34.5 45.7 27.0 Hungarian 53.1 R 56.7 58.4 56.8 54.8 22.7 Indonesian 44.6 L 60.6 63.6 61.0 58.4 35.3\nIrish 47.5 R 56.6 62.5 61.3 53.9 35.8 Italian 50.6 R 69.4 77.1 75.2 67.9 37.6 Latin 49.4 L 56.2 59.8 54.9 52.4 37.1 Norwegian 49.1 R 61.7 70.8 67.3 58.6 29.8 Persian 37.8 L 55.7 57.8 55.6 53.6 33.9 Polish 60.8 R 68.4 75.6 71.7 65.7 34.6 Portuguese 45.8 R 65.7 72.8 71.4 64.9 33.5 Romanian 52.7 R 63.7 69.2 64.0 58.9 32.1\nSlovene 50.6 R 63.6 74.7 71.0 56.0 24.3 Spanish 48.2 R 63.9 72.9 70.7 62.1 35.0 Swedish 52.4 R 62.8 72.2 67.2 58.5 25.3\nTamil 41.4 R 34.2 44.2 39.5 32.1 20.3\nAverage 47.8 57.5 63.9 61.2 55.3 29.9\nTable 5: UAS for baseline with gold POS (BLG) with direction (L/R) for backoff attachments, UDP with gold POS (UDPG) and predicted POS (UDPP ), PR with naive content-function POS (UDPN ), and multi-source delexicalized with gold and predicted POS (MSDG and MSDP , respectively). BL values higher than UDPG are underlined, and UDPG values higher than MSDG are in boldface."
    }, {
      "heading" : "5 Experiments",
      "text" : "This section describes the data and metrics used to assess the performance of UDP, as well as the systems we compare against. We evaluate on the test sections of the UD1.2 treebanks (Nivre et al., 2015) that contain word forms. If there is more than one treebank per language, we use the treebank that has the canonical language name (e.g. Finnish instead of Finnish-FTB).\nIt is common to evaluate unsupervised dependency parsing using alternative metrics like undirected attachment score or neutralized edge direction, or to evaluate short sentences separately (Schwartz et al., 2011; Gelling et al., 2012). In contrast, we use standard unlabeled attachment score (UAS) and evaluate on all sentences of the\ngithub.com/UniversalDependencies/tools\ncanonical UD test sets."
    }, {
      "heading" : "5.1 Baseline",
      "text" : "We compare our UDP system with the performance of a rule-based baseline that uses the head rules in Table 5. The baseline identifies the first verb (or first content word if there are no verbs) as the main predicate, and assigns heads to all words according to the rules in Table 1.\nWe have selected the set of head rules to maximize precision on the development set, and they do not provide full coverage. The system makes any word not covered by the rules (e.g., a word with a POS such as X or SYM) either dependent of their left or right neighbor, depending on the estimated runtime parameter.\nWe report the best head direction and its score for each language in Table 5. This baseline finds the head of each token based on its closest possible head, or on its immediate left or right neighbor if there is no head rule for the POS at hand, which means that this system does not necessarily yield well-formed tress. Each token receives a head, and while the structures are single-rooted, they are not necessarily connected.\nNote that we do not include results for the DMV model by Klein and Manning (2004), as it has been outperformed by a system similar to ours (Søgaard, 2012b). The usual adjacency baseline for unsupervised dependency parsing, where all words depend on their left or right neighbor, fares much worse than our baseline (20% UAS below on average) even when we make an oracle pick for the best per-language direction. Therefore we do not report those scores."
    }, {
      "heading" : "5.2 Evaluation setup",
      "text" : "Our system relies solely on POS tags. To estimate the quality degradation of our system under non-gold POS scenarios, we evaluate UDP on two alternative scenarios. The first is predicted POS (UDPP ), where we tag the respective test set with TnT (Brants, 2000) trained on each language’s training set. The second is a naive typeconstrained two-POS tag scenario (UDPN ), and approximates a lower bound. We give each word either CONTENT or FUNCTION tag, depending on the word’s frequency. Words that belong to the 100 most frequent word types of the input test section receive the FUNCTION tag.\nFinally, we compare our system to a supervised cross-lingual system (MSD). It is a multi-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nsource delexicalized transfer parser, referred to as multi-dir in the original paper by McDonald et al. (2011). For this baseline we train TurboParser (Martins et al., 2013) on a delexicalized training set of 20k sentences, sampled uniformly from the UD training data excluding the target language. MSD is a competitive baseline in crosslingual transfer parsing work. This gives us an indication how our system compares to standard cross-lingual parsers."
    }, {
      "heading" : "5.3 Results",
      "text" : "Table 5 shows that UDP is a competitive system; because UDPG is remarkably close to the supervised MSDG system, with an average difference of 6.4%, even outperforming MSDG on one language (Hindi).\nMore interestingly, on the evaluation scenario with predicted POS we observe that our system drops only marginally (2.2%) compared to MSD (2.7%). In the least robust rule-based setup, the error propagation rate from POS to dependency would be doubled, as either a wrongly tagged head or dependent would break the dependency rules. However, with an average POS accuracy by TnT of 94.1%, the error propagation is 0.37, i.e each POS error causes 0.37 additional dependency errors. In contrast, for the MSD system this error propagation is 0.46, thus higher.5\nFor the extreme POS scenario, content vs. function POS (CF), the drop in performance for UDP is however very large. But this might be a too crude evaluation setup. Nevertheless, UDP, the simple unsupervised system with PageRank, outperforms the adjacency baselines (BL) by 4% on average on the two type-based naive POS tag scenario. This difference indicates that even with very deficient POS tags, UDP can provide better structures."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this section we provide a further error analysis of the UDP parser. We examine the contribution to the overal results of using PageRank to score content words, the behavior of the system across different parts of speech, and we assess the robustness of UDP when parsing text from different domains.\n5Err. prop. = (E(ParseP )−E(ParseG))/E(POSP ), where E(x) = 1−Accuracy(x)."
    }, {
      "heading" : "6.1 PageRank contribution",
      "text" : "The performance of UDP depends on PageRank to score content words, and on two-step decoding to ensure the leaf status of function words. In this section we isolate the constribution of both parts. We do so by comparing the performance of BL, UDP, and UDPNoPR, a version of UDP where we disable PR and rank content words according to their reading order, i.e. the first word in the ranking is the first word to be read, regardless of the specific language’s script direction\nThe baseline BL described in 5.1 already ensures function words are leaf nodes, because they have no listed dependent POS in the head rules. The task of the decoding steps is mainly to ensure the resulting structures are well-formed dependency trees.\nHowever, if we measure the difference between UDPNoPR and BL, we observe that UDPNoPR contributes with 4 UAS points on average over the baseline. Nevertheless, the baseline is oracleinformed about the language’s best branching direction, a property that UDP does not have. Instead, the decoding step determines head direction as described in Section 3.2.\nComplementary, we can measure the contribution of PR by observing the difference between regular UDP and UDPNoPR. The latter scores on average 9 UAS points lower than UDP. These 9 points are strictly determined by the better attachment of content words."
    }, {
      "heading" : "6.2 Breakdown by POS",
      "text" : "UD is a constantly-improving effort, and not all v1.2 treebanks have the same level of formalism compliance. Thus, the interpretation of, e.g., the AUX-VERB or DET-PRON distinctions might differ. However, we do not incorporate these differences in our analysis and consider all treebanks equally compliant.\nThe root accuracy scores oscillate around an average of 69%, with Arabic and Tamil (26%) and Estonian (93%) as outliers. Given the PR personalization (Sec. 3.1), UDP has a strong bias for chosing the first verb as main predicate. However, without personalization, performance drops 2% on average. This improvement is consistent even for verb-final languages like Hindi. Moreover, our personalization strategy makes PR converge a whole order of magnitude faster.\nThe bigram heuristic to determine adposition\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nLanguage BLG MSDG UDPG MSDP UDPP\nBulgarian 50.1±2.4 73.5 ±3.5 69.7±1.8 71.3±3.3 66.9±3.2 Croatian+Serbian 42.1±0.7 66.0±3.0 57.8±1.4 62.1±3.0 54.4 ±2.0\nEnglish 42.2±2.8 60.1±6.2 53.9±2.5 57.3±4.3 52.0 ±3.3 Italian 50.3±1.2 70.0±5.4 70.1±3.3 68.1±6.0 68.7±3.9\nAverage Std. 1.8 4.5 2.5 4.2 3.1\nTable 6: Average language-wise domain evaluation. We report average UAS and standard deviation per language. The bottom row provides the average standard deviation for each system.\ndirection succeeds at identifying the predominant pre- or postposition preference for all languages (average ADP UAS of 75%). The fixed direction for the other functional POS is largely effective, with few exceptions, e.g., DET is consistently right-attaching on all treebanks except Basque (average overall DET UAS of 84%, 32% for Basque). These alternations could also be estimated from the data in a manner similar to ADP. Our rules do not make nouns eligible heads for verbs. As a result, the system cannot infer relative clauses. We have excluded the NOUN → VERB head rule during development because it makes the hierarchical relation between verbs and nouns less conclusive.\nWe have not excluded punctuation from the evaluation. Indeed, the UAS for the PUNCT is low (an average of 21%, standard deviation of 9.6), even lower than the otherwise problematic CONJ. Even though conjunctions are pervasive and identifying their scope is one of the usual challenges for parsers, the average UAS for CONJ is much larger (an average of 38%, standard deviation of 13.5) than for PUNCT. Both POS show large standard deviations, which indicates great variability. This variability can be caused by linguistic properties of the languages or evaluation datasets, but also by differences in annotation convention."
    }, {
      "heading" : "6.3 Cross-domain consistency",
      "text" : "Models with fewer parameters are less likely to be overfit for a certain dataset. In our case, a system with few, general rules is less likely to make attachment decisions that are very particular of a certain language or dataset. Plank and van Noord (2010) have shown that rule-based parsers can be more stable to domain shift. We explore if their finding holds for UDP as well, by testing on i) the UD development data as a readily available proxy for domain shift, and ii) manually curated domain splits of select UD test sets.\nLanguage Domain BLG MSDG UDPG MSDP UDPP Bulgarian bulletin 48.3 67.5 67.4 65.4 61.5 legal 47.9 76.9 69.2 73.0 68.6 literature 53.6 74.2 69.0 72.8 66.6 news 49.3 74.6 70.2 73.0 68.2 various 51.4 74.2 72.5 72.6 69.5\nCroatian news 41.2 62.4 57.9 61.8 52.2 wiki 41.9 64.8 55.8 58.2 56.3\nEnglish answers 44.1 61.6 55.9 59.5 53.7 email 42.8 58.8 52.1 57.1 56.3 newsgroup 41.7 55.5 49.7 52.9 51.1 reviews 47.4 66.8 54.9 63.9 52.2 weblog 43.3 51.6 50.9 49.8 53.8 magazine† 41.4 60.9 55.6 58.4 53.3 bible† 38.4 56.2 56.2 56.8 48.6 questions† 38.7 69.7 55.6 60.5 47.2\nItalian europarl 50.8 64.1 70.6 62.7 69.7 legal 51.1 67.9 69.0 64.4 67.2 news 49.4 68.9 67.5 67.0 65.3 questions 48.7 80.0 77.0 79.1 76.1 various 49.7 67.8 69.0 65.3 67.6 wiki 51.8 71.2 68.1 70.3 66.6\nSerbian news 42.8 68.0 58.8 65.6 53.3 wiki 42.4 68.9 58.8 62.8 55.8\nTable 7: Evaluation across domains. UAS for baseline with gold POS (BLG), UDP with gold POS (UDPG) and predicted POS (UDPP ), and multi-source delexicalized with gold and predicted POS (MSDG and MSDP ). English datasets marked with † are in-house annotated. Lowest results per language underlined. Bold: UDP outperforms MSD.\nDevelopment sets We have used the English development data to choose which relations would be included as head rules in the final system (Cf. Table 1). It would be possible that some of the rules are indeed more befitting for the English data or for that particular section.\nHowever, if we regard the results for UDPG in Table 5, we can see that there are 24 languages (out of 32) for which the parser performs better than for English. This result indicates that the head rules are general enough to provide reasonable parses for languages other than the one chosen for development.\nIf we run UDPG on the development sections for the other languages, we find the results are very consistent. Any language scores on average ±1 UAS with regards to the test section. There is no clear tendency for either section being easier to parse with our system.\nCross-domain test sets To further assess the cross-domain robustness, we retrieved the domain (genre) splits6 from the test sections of the UD\n6The data splits are freely available at http://ANONYMIZED\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\ntreebanks where the domain information is available as sentence metadata: from Bulgarian, Croatian, and Italian. We also include a UD-compliant Serbian dataset which is not included in the UD release but which is based on the same parallel corpus as Croatian and has the same domain splits (Agić and Ljubešić, 2015). When averaging we pool Croatian and Serbian together as they come from the same dataset. Also, we use a tagger trained on the Croatian UD training data for tagging Serbian.\nFor English, we have obtained the data splits in the test section matching the sentences from the original distribution of the English Web Treebank. In addition to these already available data sets, we have annotated three different datasets to asses domain variation more extensively, namely the first 50 verses of the King James Bible, 50 sentences from a magazine, and 75 sentences from the test split in QuestionBank (Judge et al., 2006). We include the third dataset to evaluate strictly on questions, which we could do already in Italian. While the answers domain in English is made up of text from the Yahoo! Answers forum, only one fourth of the sentences are questions. Note these three small datasets are not included in the results on the canonical test sections in Table 5.7\nTable 6 summarizes the per-language average score and standard deviation, as well as the macroaveraged standard deviation across languages. UDP has a much lower standard deviation across domains compared to MSD. This holds across languages. We attribute this higher stability to UDP being developed to satisfy a set of general properties of the UD syntactic formalism, instead of being a data-driven method more sensitive to sampling bias. This holds for both the gold-POS and predicted-POS setup. The differences in standard deviation are unsurprisingly smaller in the predicted POS setup. In general, the rule-based UPD is less sensitive to domain shifts than the datadriven MSD counterpart, confirming earlier findings (Plank and van Noord, 2010).\nTable 7 gives the detailed scores per language and domain. From the scores we can see that presidental bulletin, legal and weblogs are amongst the hardest domains to parse. However, the systems often do not agree on which domain is hardest, with the exception of Bulgarian\n7The three in-house annotated datasets are freely available at http://ANONYMIZED\nbulletin. More importantly, this might even change between gold and predicted POS, highlighting the importance of evaluating systems beyond gold POS. Interestingly, for the Italian data and some of the hardest domains UDP outperforms MSD, confirming that it is a robust baseline."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have presented UDP, an unsupervised dependency parser for Universal Dependencies that makes use of personalized PageRank and a small set of head-dependent rules. The parser requires no training data and estimates adpositon direction directly from test data. We achieve competitive performance on all but two UD languages, and even beat a multi-source delexicalized parser (MSD) on Hindi. We evaluated the parser on three POS setups and across domains. Our results show that UDP is less affected by deteriorating POS tags than MSD, and is more resilient to domain changes. Both the parser and the in-domain annotated test sets are freely available.8\nFurther work includes extending the parser to handle multiword expressions, coordination, and proper names. Moreover, our usage of PR could be expanded to directly score the potential dependency edges—e.g., by means of edge reification— instead of words. Finally, we only considered unlabeled attachment, however, our system could easily be augmented with partial edge labeling."
    } ],
    "references" : [ {
      "title" : "Universal Dependencies for Croatian (that Work for Serbian, too)",
      "author" : [ "Željko Agić", "Nikola Ljubešić." ],
      "venue" : "BSNLP.",
      "citeRegEx" : "Agić and Ljubešić.,? 2015",
      "shortCiteRegEx" : "Agić and Ljubešić.",
      "year" : 2015
    }, {
      "title" : "If All You Have is a Bit of the Bible: Learning POS Taggers for Truly Low-Resource Languages",
      "author" : [ "Željko Agić", "Dirk Hovy", "Anders Søgaard." ],
      "venue" : "ACL.",
      "citeRegEx" : "Agić et al\\.,? 2015",
      "shortCiteRegEx" : "Agić et al\\.",
      "year" : 2015
    }, {
      "title" : "TnT – A Statistical Part-ofSpeech Tagger",
      "author" : [ "Thorsten Brants." ],
      "venue" : "ANLP.",
      "citeRegEx" : "Brants.,? 2000",
      "shortCiteRegEx" : "Brants.",
      "year" : 2000
    }, {
      "title" : "Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections",
      "author" : [ "Dipanjan Das", "Slav Petrov." ],
      "venue" : "ACL.",
      "citeRegEx" : "Das and Petrov.,? 2011",
      "shortCiteRegEx" : "Das and Petrov.",
      "year" : 2011
    }, {
      "title" : "Universal Stanford Dependencies: A Cross-Linguistic Typology",
      "author" : [ "Marie-Catherine De Marneffe", "Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D Manning." ],
      "venue" : "LREC.",
      "citeRegEx" : "Marneffe et al\\.,? 2014",
      "shortCiteRegEx" : "Marneffe et al\\.",
      "year" : 2014
    }, {
      "title" : "The Pascal Challenge on Grammar Induction",
      "author" : [ "Douwe Gelling", "Trevor Cohn", "Phil Blunsom", "Joao Graça." ],
      "venue" : "Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure.",
      "citeRegEx" : "Gelling et al\\.,? 2012",
      "shortCiteRegEx" : "Gelling et al\\.",
      "year" : 2012
    }, {
      "title" : "Sparsity in Dependency Grammar Induction",
      "author" : [ "Jennifer Gillenwater", "Kuzman Ganchev", "Joao Graça", "Fernando Pereira", "Ben Taskar." ],
      "venue" : "ACL.",
      "citeRegEx" : "Gillenwater et al\\.,? 2010",
      "shortCiteRegEx" : "Gillenwater et al\\.",
      "year" : 2010
    }, {
      "title" : "Bootstrapping Parsers via Syntactic Projection Across Parallel Texts",
      "author" : [ "Rebecca Hwa", "Philip Resnik", "Amy Weinberg", "Clara Cabezas", "Okan Kolak." ],
      "venue" : "Natural Language Engineering, 11(03):311– 325.",
      "citeRegEx" : "Hwa et al\\.,? 2005",
      "shortCiteRegEx" : "Hwa et al\\.",
      "year" : 2005
    }, {
      "title" : "Joint Part-of-Speech and Dependency Projection from Multiple Sources",
      "author" : [ "Anders Johannsen", "Željko Agić", "Anders Søgaard." ],
      "venue" : "ACL.",
      "citeRegEx" : "Johannsen et al\\.,? 2016",
      "shortCiteRegEx" : "Johannsen et al\\.",
      "year" : 2016
    }, {
      "title" : "Questionbank: Creating a Corpus of ParseAnnotated Questions",
      "author" : [ "John Judge", "Aoife Cahill", "Josef Van Genabith." ],
      "venue" : "ACL.",
      "citeRegEx" : "Judge et al\\.,? 2006",
      "shortCiteRegEx" : "Judge et al\\.",
      "year" : 2006
    }, {
      "title" : "CorpusBased Induction of Syntactic Structure: Models of Dependency and Constituency",
      "author" : [ "Dan Klein", "Christopher Manning." ],
      "venue" : "ACL.",
      "citeRegEx" : "Klein and Manning.,? 2004",
      "shortCiteRegEx" : "Klein and Manning.",
      "year" : 2004
    }, {
      "title" : "Unsupervised Dependency Parsing with Transferring Distribution via Parallel Guidance and Entropy Regularization",
      "author" : [ "Xuezhe Ma", "Fei Xia." ],
      "venue" : "ACL.",
      "citeRegEx" : "Ma and Xia.,? 2014",
      "shortCiteRegEx" : "Ma and Xia.",
      "year" : 2014
    }, {
      "title" : "Turning on the Turbo: Fast ThirdOrder Non-Projective Turbo Parsers",
      "author" : [ "André F.T. Martins", "Miguel Almeida", "Noah A. Smith." ],
      "venue" : "ACL.",
      "citeRegEx" : "Martins et al\\.,? 2013",
      "shortCiteRegEx" : "Martins et al\\.",
      "year" : 2013
    }, {
      "title" : "Multi-Source Transfer of Delexicalized Dependency Parsers",
      "author" : [ "Ryan McDonald", "Slav Petrov", "Keith Hall." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "McDonald et al\\.,? 2011",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2011
    }, {
      "title" : "Using Universal Linguistic Knowledge to Guide Grammar Induction",
      "author" : [ "Tahira Naseem", "Harr Chen", "Regina Barzilay", "Mark Johnson." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Naseem et al\\.,? 2010",
      "shortCiteRegEx" : "Naseem et al\\.",
      "year" : 2010
    }, {
      "title" : "The PageRank Citation Ranking: Bringing Order to the Web",
      "author" : [ "Lawrence Page", "Sergey Brin", "Rajeev Motwani", "Terry Winograd" ],
      "venue" : null,
      "citeRegEx" : "Page et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Page et al\\.",
      "year" : 1999
    }, {
      "title" : "Grammar-Driven versus Data-Driven: Which Parsing System Is More Affected by Domain Shifts",
      "author" : [ "Barbara Plank", "Gertjan van Noord" ],
      "venue" : "In Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground",
      "citeRegEx" : "Plank and Noord.,? \\Q2010\\E",
      "shortCiteRegEx" : "Plank and Noord.",
      "year" : 2010
    }, {
      "title" : "Density-Driven Cross-Lingual Transfer of Dependency Parsers",
      "author" : [ "Mohammad Sadegh Rasooli", "Michael Collins." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Rasooli and Collins.,? 2015",
      "shortCiteRegEx" : "Rasooli and Collins.",
      "year" : 2015
    }, {
      "title" : "Neutralizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation",
      "author" : [ "Roy Schwartz", "Omri Abend", "Roi Reichart", "Ari Rappoport." ],
      "venue" : "ACL.",
      "citeRegEx" : "Schwartz et al\\.,? 2011",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2011
    }, {
      "title" : "From Baby Steps to Leapfrog: How Less is More in Unsupervised Dependency Parsing",
      "author" : [ "Valentin Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Spitkovsky et al\\.,? 2010a",
      "shortCiteRegEx" : "Spitkovsky et al\\.",
      "year" : 2010
    }, {
      "title" : "Viterbi Training Improves Unsupervised Dependency Parsing",
      "author" : [ "Valentin Spitkovsky", "Hiyan Alshawi", "Daniel Jurafsky", "Christopher Manning." ],
      "venue" : "CoNLL.",
      "citeRegEx" : "Spitkovsky et al\\.,? 2010b",
      "shortCiteRegEx" : "Spitkovsky et al\\.",
      "year" : 2010
    }, {
      "title" : "Data Point Selection for CrossLanguage Adaptation of Dependency Parsers",
      "author" : [ "Anders Søgaard." ],
      "venue" : "NAACL.",
      "citeRegEx" : "Søgaard.,? 2011",
      "shortCiteRegEx" : "Søgaard.",
      "year" : 2011
    }, {
      "title" : "Two Baselines for Unsupervised Dependency Parsing",
      "author" : [ "Anders Søgaard." ],
      "venue" : "Proceedings of the NAACL-HLT Workshop on the Induction of Linguistic Structure.",
      "citeRegEx" : "Søgaard.,? 2012a",
      "shortCiteRegEx" : "Søgaard.",
      "year" : 2012
    }, {
      "title" : "Unsupervised dependency parsing without training",
      "author" : [ "Anders Søgaard." ],
      "venue" : "Natural Language Engineering, 18(02):187–203.",
      "citeRegEx" : "Søgaard.,? 2012b",
      "shortCiteRegEx" : "Søgaard.",
      "year" : 2012
    }, {
      "title" : "Rediscovering Annotation Projection for Cross-Lingual Parser Induction",
      "author" : [ "Jörg Tiedemann." ],
      "venue" : "COLING.",
      "citeRegEx" : "Tiedemann.,? 2014",
      "shortCiteRegEx" : "Tiedemann.",
      "year" : 2014
    }, {
      "title" : "Inducing Multilingual Text Analysis Tools via Robust Projection Across Aligned Corpora",
      "author" : [ "David Yarowsky", "Grace Ngai", "Richard Wicentowski." ],
      "venue" : "HLT.",
      "citeRegEx" : "Yarowsky et al\\.,? 2001",
      "shortCiteRegEx" : "Yarowsky et al\\.",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012).",
      "startOffset" : 115,
      "endOffset" : 162
    }, {
      "referenceID" : 5,
      "context" : "Grammar induction and unsupervised dependency parsing are active fields of research in natural language processing (Klein and Manning, 2004; Gelling et al., 2012).",
      "startOffset" : 115,
      "endOffset" : 162
    }, {
      "referenceID" : 13,
      "context" : "Even supervised transfer approaches (McDonald et al., 2011) suffer from target adaptation problems when facing word order differences.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 3,
      "context" : "Over the recent years, cross-lingual linguistic structure prediction based on model transfer or projection of POS tags and dependency trees has become a relevant line of work (Das and Petrov, 2011; McDonald et al., 2011).",
      "startOffset" : 175,
      "endOffset" : 220
    }, {
      "referenceID" : 13,
      "context" : "Over the recent years, cross-lingual linguistic structure prediction based on model transfer or projection of POS tags and dependency trees has become a relevant line of work (Das and Petrov, 2011; McDonald et al., 2011).",
      "startOffset" : 175,
      "endOffset" : 220
    }, {
      "referenceID" : 25,
      "context" : "The first group of approaches deals with annotation projection (Yarowsky et al., 2001), whereby parallel corpora are used to transfer annotations between resource-rich source languages and lowresource target languages.",
      "startOffset" : 63,
      "endOffset" : 86
    }, {
      "referenceID" : 11,
      "context" : "Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora (Ma and Xia, 2014; Rasooli and Collins, 2015).",
      "startOffset" : 98,
      "endOffset" : 143
    }, {
      "referenceID" : 17,
      "context" : "Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora (Ma and Xia, 2014; Rasooli and Collins, 2015).",
      "startOffset" : 98,
      "endOffset" : 143
    }, {
      "referenceID" : 10,
      "context" : "Data-driven unsupervised dependency parsing is a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b).",
      "startOffset" : 77,
      "endOffset" : 154
    }, {
      "referenceID" : 19,
      "context" : "Data-driven unsupervised dependency parsing is a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b).",
      "startOffset" : 77,
      "endOffset" : 154
    }, {
      "referenceID" : 20,
      "context" : "Data-driven unsupervised dependency parsing is a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b).",
      "startOffset" : 77,
      "endOffset" : 154
    }, {
      "referenceID" : 6,
      "context" : "Our work builds on the research on rule-aided unsupervised dependency parsing (Gillenwater et al., 2010; Naseem et al., 2010; Søgaard, 2012a; Søgaard, 2012b).",
      "startOffset" : 78,
      "endOffset" : 157
    }, {
      "referenceID" : 14,
      "context" : "Our work builds on the research on rule-aided unsupervised dependency parsing (Gillenwater et al., 2010; Naseem et al., 2010; Søgaard, 2012a; Søgaard, 2012b).",
      "startOffset" : 78,
      "endOffset" : 157
    }, {
      "referenceID" : 22,
      "context" : "Our work builds on the research on rule-aided unsupervised dependency parsing (Gillenwater et al., 2010; Naseem et al., 2010; Søgaard, 2012a; Søgaard, 2012b).",
      "startOffset" : 78,
      "endOffset" : 157
    }, {
      "referenceID" : 23,
      "context" : "Our work builds on the research on rule-aided unsupervised dependency parsing (Gillenwater et al., 2010; Naseem et al., 2010; Søgaard, 2012a; Søgaard, 2012b).",
      "startOffset" : 78,
      "endOffset" : 157
    }, {
      "referenceID" : 5,
      "context" : "Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann (2014) improved on their projection algorithm.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann (2014) improved on their projection algorithm.",
      "startOffset" : 0,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann (2014) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences.",
      "startOffset" : 0,
      "endOffset" : 396
    }, {
      "referenceID" : 5,
      "context" : "Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann (2014) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al.",
      "startOffset" : 0,
      "endOffset" : 569
    }, {
      "referenceID" : 5,
      "context" : "Hwa et al. (2005) were the first to project syntactic dependencies, and Tiedemann (2014) improved on their projection algorithm. Current state of the art in cross-lingual dependency parsing involves leveraging parallel corpora (Ma and Xia, 2014; Rasooli and Collins, 2015). The second group of approaches deals with transferring source parsing models to target languages. Zeman and Resnik (2008) were the first to introduce the idea of delexicalization: removing lexical features by training and cross-lingually applying parsers solely on POS sequences. Søgaard (2011) and McDonald et al. (2011) independently extend delexicalization to involve multiple sourceside parsers.",
      "startOffset" : 0,
      "endOffset" : 596
    }, {
      "referenceID" : 1,
      "context" : "Agić et al. (2015) expose some of the biases in their proposal for realistic cross-lingual POS tagging, as they emphasize the lack of perfect sentence and word splitting for truly low-resource languages.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "Agić et al. (2015) expose some of the biases in their proposal for realistic cross-lingual POS tagging, as they emphasize the lack of perfect sentence and word splitting for truly low-resource languages. Johannsen et al. (2016) introduce joint projection of POS and dependencies from multiple sources while sharing the outlook on bias removal in real-world multilingual processing.",
      "startOffset" : 0,
      "endOffset" : 228
    }, {
      "referenceID" : 1,
      "context" : "Agić et al. (2015) expose some of the biases in their proposal for realistic cross-lingual POS tagging, as they emphasize the lack of perfect sentence and word splitting for truly low-resource languages. Johannsen et al. (2016) introduce joint projection of POS and dependencies from multiple sources while sharing the outlook on bias removal in real-world multilingual processing. Cross-lingual learning, realistic or not, depends entirely on the availability of data: for the sources, for the targets, or most often for both sets of languages. Moreover, it typically does not exploit the constraints placed on the linguistic structures through the formalism, and it does so by design. With the emergence of UD as the practical standard for cross-language annotation of POS and syntactic dependencies, we argue for an approach that takes a fresh angle on both aspects. Namely, we propose a parser that i) requires no training data, and in contrast ii) critically relies on exploiting the UD constraints on building POS and dependency annotations. These two characteristics make our parser unsupervised. Data-driven unsupervised dependency parsing is a well-established discipline (Klein and Manning, 2004; Spitkovsky et al., 2010a; Spitkovsky et al., 2010b). Still, the performance of unsupervised parsers falls far behind the approaches involving any sort of supervision. Our work builds on the research on rule-aided unsupervised dependency parsing (Gillenwater et al., 2010; Naseem et al., 2010; Søgaard, 2012a; Søgaard, 2012b). In particular, we make use of Søgaard’s (2012b) PageRank method to rank words before decoding.",
      "startOffset" : 0,
      "endOffset" : 1581
    }, {
      "referenceID" : 15,
      "context" : "Our system uses the PageRank (PR) algorithm (Page et al., 1999) to estimate the relevance of the content words of a sentence.",
      "startOffset" : 44,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "Our system uses the PageRank (PR) algorithm (Page et al., 1999) to estimate the relevance of the content words of a sentence. PR gives higher rank to nodes with more incoming edges, as well as to nodes connected to those. Using PR to score word relevance requires an effective graphbuilding strategy. We have experimented with the strategies by Søgaard (2012b), but our system fares best strictly using the dependency rules in Table 1 to build the graph.",
      "startOffset" : 45,
      "endOffset" : 361
    }, {
      "referenceID" : 18,
      "context" : "It is common to evaluate unsupervised dependency parsing using alternative metrics like undirected attachment score or neutralized edge direction, or to evaluate short sentences separately (Schwartz et al., 2011; Gelling et al., 2012).",
      "startOffset" : 189,
      "endOffset" : 234
    }, {
      "referenceID" : 5,
      "context" : "It is common to evaluate unsupervised dependency parsing using alternative metrics like undirected attachment score or neutralized edge direction, or to evaluate short sentences separately (Schwartz et al., 2011; Gelling et al., 2012).",
      "startOffset" : 189,
      "endOffset" : 234
    }, {
      "referenceID" : 23,
      "context" : "Note that we do not include results for the DMV model by Klein and Manning (2004), as it has been outperformed by a system similar to ours (Søgaard, 2012b).",
      "startOffset" : 139,
      "endOffset" : 155
    }, {
      "referenceID" : 10,
      "context" : "Note that we do not include results for the DMV model by Klein and Manning (2004), as it has been outperformed by a system similar to ours (Søgaard, 2012b).",
      "startOffset" : 57,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "The first is predicted POS (UDPP ), where we tag the respective test set with TnT (Brants, 2000) trained on each language’s training set.",
      "startOffset" : 82,
      "endOffset" : 96
    }, {
      "referenceID" : 12,
      "context" : "For this baseline we train TurboParser (Martins et al., 2013) on a delexicalized training set of 20k sentences, sampled uniformly from the UD training data excluding the target language.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "source delexicalized transfer parser, referred to as multi-dir in the original paper by McDonald et al. (2011). For this baseline we train TurboParser (Martins et al.",
      "startOffset" : 88,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "We also include a UD-compliant Serbian dataset which is not included in the UD release but which is based on the same parallel corpus as Croatian and has the same domain splits (Agić and Ljubešić, 2015).",
      "startOffset" : 177,
      "endOffset" : 202
    }, {
      "referenceID" : 9,
      "context" : "In addition to these already available data sets, we have annotated three different datasets to asses domain variation more extensively, namely the first 50 verses of the King James Bible, 50 sentences from a magazine, and 75 sentences from the test split in QuestionBank (Judge et al., 2006).",
      "startOffset" : 272,
      "endOffset" : 292
    } ],
    "year" : 2016,
    "abstractText" : "We present UDP, an unsupervised parsing algorithm for Universal Dependencies (UD) based on PageRank and a small set of specific dependency head rules. The parser requires no training, and it is competitive with a delexicalized transfer system. UDP offers a linguistically sound unsupervised alternative to cross-lingual UD parsing. It is distinctly robust to domain change across languages.",
    "creator" : "TeX"
  }
}