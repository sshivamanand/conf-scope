{
  "name" : "13.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Event Linking with Sentential Features from Convolutional Neural Networks",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Event extraction aims at detecting mentions of realworld events and their arguments in text documents of different domains, e.g., news articles. The subsequent task of event linking is concerned with resolving coreferences between recognized event mentions in a document, and is the focus of this paper.\nSeveral studies investigate event linking and related problems such as relation mentions spanning multiple sentences. Swampillai and Stevenson (2010) find that 28.5 % of binary relation mentions in the MUC 6 dataset are affected, as are 9.4 % of\nrelation mentions in the ACE corpus from 2003. Ji and Grishman (2011) estimate that 15 % of slot fills in the training data for the “TAC 2010 KBP Slot Filling” task require cross-sentential inference. To confirm these numbers, we analyzed the event annotation of the ACE 2005 corpus and found that approximately 23 % of the event mentions are incomplete on the argument level, with respect to the information in other mentions of the same event instance in the respective document. These numbers suggest that event linking is an important task.\nPrevious approaches for modeling event mentions in context of coreference resolution (Bejan and Harabagiu, 2010; Sangeetha and Arock, 2012; Liu et al., 2014) make either use of external feature sources with limited cross-domain availability like WordNet (Fellbaum, 1998) and FrameNet (Baker et al., 1998), or show low performance. At the same time, recent literature proposes a new kind of feature class for modeling events (and relations) in order to detect mentions and extract their arguments, i.e., sentential features from event-/relationmention representations that have been created by taking the full extent and surrounding sentence of a mention into account (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). Their promising results motivate our work. We propose to use such features for event coreference resolution, hoping to thereby remove the need for extensive external semantic features while preserving the current stateof-the-art performance level.\nOur contributions in this paper are as follows: We design a system for event linking which in a first step models intra-sentential event mentions via the use of convolutional neural networks for the integration of sentential features. In the next step, our model learns to make coreference decisions for pairs of event mentions based on the previously generated representations. This approach does not\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nrely on external semantic features, but rather employs a combination of local and sentential features to describe individual event mentions, and combines these intermediate event representations with standard pairwise features for the coreference decision. The model achieves state-of-the-art performance in our experiments on two datasets, one of which is publicly available. Furthermore, we present an analysis of the system errors to identify directions for further research."
    }, {
      "heading" : "2 Problem definition",
      "text" : "We follow the notion of events from the ACE 2005 dataset (LDC, 2005; Walker et al., 2006). Consider the following example:\nBritish bank Barclays had agreed to buy Spanish rival Banco Zaragozano for 1.14 billion euros. The combination of the banking operations of Barclays Spain and Zaragozano will bring together two complementary businesses and will happen this year, in contrast to Barclays’ postponed merger with Lloyds.1\nProcessing these sentences in a prototypical, ACE-style information extraction (IE) pipeline would involve (a) the recognition of entity mentions. In the example, mentions of entities are underlined. Next, (b) words in the text are processed as to whether they elicit an event reference, i.e., event triggers are identified and their semantic type is classified. The above sentences contain three event mentions with type Business.MergeOrg, shown in boldface. The task of event extraction further requires that (c) participants of recognized events are determined among the entity mentions in the same sentence, i.e., an event’s arguments are identified and their semantic role wrt. to the event is classified. The three recognized event mentions are:\nE1: buy(British bank Barclays, Spanish rival Banco Zaragozano, 1.14 billion euros) E2: combination(Barclays Spain, Zaragozano, this year) E3: merger(Barclays, Lloyds)\nOften, an IE system involves (d) a disambiguation step of the entity mentions against one another in the same document. This allows to identify that the three mentions of “Barclays” in the text as referring to the same real-world entity. The analogous task on the level of event mentions is called (e) event linking (or: event coreference resolution) and is the focus of this paper. Specifically, the task is\n1Based on an example in (Araki and Mitamura, 2015).\nto determine that E3 is a singleton reference in this example, while E1 and E2 are coreferential, with the consequence that a document-level event instance can be produced from E1 and E2, listing four arguments (two companies, buying price, and acquisition date)."
    }, {
      "heading" : "3 Model design",
      "text" : "This section first motivates the design decisions of our model for event linking, before going into details about its two-step architecture.\nEvent features from literature So far, a wide range of features has been used for the representation of events and relations for extraction (Zhou et al., 2005; Mintz et al., 2009; Sun et al., 2011) and coreference resolution (Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Araki and Mitamura, 2015; Cybulska and Vossen, 2015) purposes. The following is an attempt to list the most common classes among them, along with examples:\n• lexical: surface string, lemma, word embeddings, context around trigger • syntactic: depth of trigger in parse tree, dependency arcs from/to trigger • discourse: distance between coreference candidates, absolute position in document • semantic (intrinsic): comparison of event arguments (entity fillers, present roles), event type of coreference candidates • semantic (external): coreference-candidates similarity in lexico-semantic resources (WordNet, FrameNet) and other datasets (VerbOcean corpus), enrichtment of arguments with alternative names from external sources (DBpedia, Geonames)\nWhile lexical, discourse, and intrinsic-semantic features are available in virtually all application scenarios of event extraction/linking, and even syntactic parsing is no longer considered an expensive feature source, semantic features from external knowledge sources pose a significant burden on the application of event processing systems, as these sources are created at high cost and come with limited domain coverage.\nFortunately, recent work has explored the use of a new feature class, sentential features, for tackling relation-/event-extraction related tasks with neural networks (Zeng et al., 2014; Nguyen and Grishman, 2015; Chen et al., 2015; dos Santos et al., 2015; Zeng et al., 2015). These approaches have shown that processing sentences with neural models yields representations suitable for IE, which motivates their use in our approach.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n... had agreed to buy Spanish rival ... -3 -2 -1 0 +1 +2\nto buy Spanish\nEmbedding look-up for words and positions Convolution layer Piecewise max-pooling Concat. Hidden layer\nGeneration of event-mention representation …\nSentence-level feature generation\nEmbedding look-up for trigger & context\nTrigger-local feature generation\nModel part (a).\n● type compatibility ● position in discourse ● realis match ● argument overlap\nDistributed similarity\nThe combination of the banking operations ...\n... had agreed to buy Spanish rival ...\n2x model part (a) Concat. Logistic regression\nCoreference scoring\nPairwise features\nTrigger-local & sentencelevel features\nModel part (b).\nFigure 1: The two parts of the model. The first part computes a representation for a single event mention. The second part is fed with two such event-mention representations plus a number of pairwise features for the input event-mention pair, and calculates a coreference score.\nData properties A preliminary analysis of one dataset used in our experiments (ACE++; see Section 5) further motivates the design of our model. We found that 50.97 % of coreferential event-mentions pairs share no arguments, either by mentioning distinct argument roles or because one/both mentions have no annotated arguments. Furthermore, 47.29 % of positive event-mention pairs have different trigger words. It is thus important to not solely rely on intrinsic event properties in order to model event mentions, but to additionally take the surrounding sentence’s semantics into account. Another observation regards the distance of coreferential event mentions in a document. 55.42% are more than five sentences apart. This indicates that a locality-based heuristic would not perform well and also encourages the use of sentential features for making coreference decisions."
    }, {
      "heading" : "3.1 Learning event representations",
      "text" : "The architecture of the model (Figure 1) is split into two parts. The first one aims at adequately representing individual event mentions. As is common in literature, words of the whole sentence of an input event mention are represented as real-valued vectors viw of a fixed size dw, with i being a word’s position in the sentence. These word embeddings\nare updated during model training and are stored in a matrix Ww ∈ Rdw×|V |; |V | being the vocabulary size of the dataset.\nFurthermore, we take the relative position of tokens with respect to the mention into account, as suggested by (Collobert et al., 2011; Zeng et al., 2014). The rationale is that while the absolute position of learned features in a sentence might not be relevant for an event-related decision, the position of them wrt. the event mention is. Embeddings v(·)p of size dp for relative positions are generated in a way similar to word embeddings. Embeddings for words and positions are concatenated into vectors v (·) t of size dt = dw + dp. A sentence with s words is thus represented by a matrix of dimensions s×dt. This matrix serves as input to a convolution layer.\nIn order to compress the semantics of s words into a sentence-level feature vector with constant size, the convolution layer applies dc filters to each window of n consecutive words, calculating dc features for each n-gram of a sentence. For a single filter and particular window, this operation is defined as\nvic = relu(wc · vi:i+n−1t + bc), (1)\nwhere wc ∈ Rn∗dt is a filter, vi:i+n−1t is the flattened concatenation of vectors v(·)t for words at\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\npositions i through i+n−1, bc is a bias, and relu is the activation function of a rectified linear unit. In Figure 1, dc = 3 and n = 2.\nIn order to identify the most indicative features in the sentence and to introduce invariance for the absolute position of these, we feed the n-gram representations to a max-pooling layer, which identifies the maximum value for each filter. We treat n-grams on each side of the trigger word separately, which allows the model to handle multiple event mentions per sentence, similar in spirit to (Chen et al., 2015; Zeng et al., 2015). The pooling is defined as\nvj,km = max (v i c), (2)\nwhere 1 ≤ j ≤ dc designates a feature, k ∈ {left, right} corresponds to a sentence part, and i runs through the convolution windows of k. The output of this step are sentential features vsent ∈ R2∗dc of the input event mention.\nAdditionally, we provide the network with trigger-local, lexical-level features by concatenating vsent with the word embeddings v (·) w of the trigger word and its left and right neighbor, resulting in vsent+lex ∈ R2∗dc+3∗dw . This encourages the model to take the lexical semantics of the trigger into account, as these can be a strong indicator for coreference. The result is processed by an additional hidden layer, generating the final event-mention representation ve with size de used for the subsequent event-linking decision:\nve = tanh(Wevsent+lex + be). (3)"
    }, {
      "heading" : "3.2 Learning coreference decisions",
      "text" : "The second part of the model (Figure 1 (b)) processes the representations for two event mentions, and augments these with pairwise comparison features to determine the compatibility of the event mentions. The following features are used, in parentheses we give the feature value for the pair E1, E2 from the example in Section 1: • Coarse-grained and/or fine-grained event type agree-\nment (yes, yes) • Antecedent event is in first sentence (yes) • (Bagged) distance between event mentions in #sen-\ntences/#intermediate event mentions (1, 0) • Agreement in event modality (yes) • Overlap in arguments (two shared arguments)\nThe concatenation of these vectors (vsent+lex+pairw) is processed by a single-layer neural network which calculates a distributed similarity of size dsim for the two event mentions:\nvsim = square(Wsimvsent+lex+pairw + bsim). (4)\n1: procedure GENERATECLUSTERS(Pd, score): 2: Pd = {(mi,mj)}i,j 3: score : Pd 7→ (0, 1) 4: Cd ← {(mi,mj) ∈ Pd : score(mi,mj) > 0.5} 5: while ∃(mi,mk), (mk,mj) ∈ Cd : (mi,mj) 6∈ Cd do 6: Cd ← Cd ∪ {(mi,mj)} 7: return Cd\nFigure 3: Generation of event clusters Cd for a document d based on the coreference scores from the model. Pd is the set of all event-mention pairs from a document, as implemented in Figure 2.\nThe use of the square function as the network’s non-linearity is backed by the intuition that for measuring similarity, an invariance under polarity changes is desirable. Having dsim similarity dimensions allows the model to learn multiple similarity facets in parallel; in our experiments, this setup outperformed model variants with different activation functions as well as a cosine-similarity based comparison.\nTo calculate the final output of the model, vsim is fed to a logistic regression classifier, whose output serves as the coreference score:\nscore = σ(Woutvsim + bout) (5)\nWe train the model parameters\nθ = {Ww,Wp, {wc}, {bc},We, be,Wsim, bsim,Wout, bout} (6)\nby minimizing the logistic loss over shuffled minibatches with gradient descent using Adam (Kingma and Ba, 2014)."
    }, {
      "heading" : "3.3 Example generation and clustering",
      "text" : "We investigated two alternatives for the generation of examples from documents with recognized event mentions. Figure 2 shows the strategy we found to perform best, which iterates over the event mentions of a document and pairs each mention (the “anaphors”) with all preceding ones (the “antecedent” candidates). This strategy applies to both training and inference time. Soon et al. (2001) propose an alternative strategy, which during training\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nACE ACE++\n# documents 599 1950 # event instances 3617 7520 # event mentions 4728 9956\nTable 1: Dataset properties.\ncreates positive examples only for the closest actual antecedent of an anaphoric event mention with intermediate event mentions serving as negative antecedent candidates. In our experiments, this strategy performed worse than the less elaborate algorithm in Figure 2.\nThe pairwise coreference decisions of our model induce a clustering of a document’s event mentions. In order to force the model to output a consistent view on a given document, a strategy for resolving conflicting decisions is needed. We followed the strategy detailed in Figure 3, which builds the transitive closure of all positive links. Additionally, we experimented with Ng and Gardent (2002)’s “BestLink” strategy, which discards all but the highest-scoring antecedent of an anaphoric event mention. Liu et al. (2014) reported that for event linking, BestLink outperforms naive transitive closure, however, in our experiments (Section 5) we come to a different conclusion."
    }, {
      "heading" : "4 Experimental setting, model training",
      "text" : "We implemented our model using the TensorFlow framework (Abadi et al., 2015, v0.6), and chose the ACE 2005 dataset (Walker et al., 2006, later: ACE) as our main testbed. The annotation of this corpus focuses on the event types Conflict.Attack, Movement.Transport, and Life.Die reporting about terrorist attacks, movement of goods and people, and deaths of people; but also contains many more related event types as well as mentions of businessrelevant and judicial events. The corpus consists of merely 599 documents, which is why we create a second dataset that encompasses these documents and additionally contains 1351 more web documents annotated in an analogous fashion. We refer to this second dataset as ACE++. Both datasets are split 9:1 into a development (dev) and test partition; we further split dev 9:1 into a training (train) and validation (valid) partition.2 Table 1 lists statistics for the datasets.\nThere are a number of architectural alternatives 2The list of documents in ACEvalid/ACEtest is published\nhere: https://git.io/vwEEP.\nin the model as well as hyperparameters to optimize. Besides the size of intermediate representations in the model (dw, dp, dc, de, dsim), we experimented with different convolution window sizes n, activation functions for the similarity-function layer in model part (b), whether to use the dual pooling and final hidden layer in model part (a), whether to apply regularization with `2 penalties or Dropout, and parameters to Adam (η, β1, β2, ). We started our exploration of this space of possibilities from previously reported hyperparameter values (Zhang and Wallace, 2015; Chen et al., 2015) and followed a combined strategy of random sampling from the hyperparameter space (180 points) and line search. Optimization was done by training on ACE++train and evaluating on ACE ++ valid. The final settings we used for all following experiments are listed in Table 2. Ww is initialized with pre-trained embeddings of (Mikolov et al., 2013)3, all other model parameters are randomly initialized. Model training is run for 2000 epochs, after which the best model on the respective valid partition is selected."
    }, {
      "heading" : "5 Evaluation",
      "text" : "This section elaborates on the conducted experiments. First, we compare our approach to state-ofart systems on dataset ACE, after which we report experiments on ACE++, where we contrast variations of our model to gain insights about the impact of the utilized feature classes. We conclude this section with an error analysis."
    }, {
      "heading" : "5.1 Comparison to state-of-the-art on ACE",
      "text" : "Table 3 depicts the performance of our model, trained on ACEtrain, on ACEtest, along with the performance of state-of-the-art systems from the literature. From the wide range of proposed metrics for the evaluation of coreference resolution, we believe\n3https://code.google.com/archive/p/ word2vec/\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nBLANC B-CUBED MUC Positive links\n4 ∗ (Precision / Recall / F1 score) in % This paper 71.80 75.16 73.31 90.52 86.12 88.26 61.54 45.16 52.09 47.89 56.20 51.71 (Liu et al., 2014) 70.88 70.01 70.43 89.90 88.86 89.38 53.42 48.75 50.98 55.86 40.52 46.97 (Bejan and Harabagiu, 2010) — — — 83.4 84.2 83.8 — — — 43.3 47.1 45.1 (Sangeetha and Arock, 2012) — — — — — 87.7 — — — — — —\nTable 3: Event-linking performance of our model & competitors on ACE. Best value per metric in bold.\nBLANC (Recasens and Hovy, 2011) has the highest validity, as it balances the impact of positive and negative event-mention links in a document. Negative links and consequently singleton event mentions are more common in this dataset (more than 90 % of links are negative). As Recasens and Hovy (2011) point out, the informativeness of metrics like MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), and the naive positivelink metric suffers from such imbalance. We still add these metrics for completeness, and because BLANC is not available for all systems.\nUnfortunately, there are two caveats to this comparison. Firstly, while a 9:1 train/test split is the commonly accepted way of using ACE, the exact documents in the partitions vary from system to system. Secondly, published methods follow different strategies regarding preprocessing components. While all systems in Table 3 use gold-annotated event-mention triggers, Bejan and Harabagiu (2010) and Liu et al. (2014) use a semantic-role labeler and other tools instead of gold-argument information. We argue that using gold-annotated event mentions is reasonable in order to mitigate error propagation along extraction pipeline and make performance values for the task at hand more informative.\nWe beat Liu et al. (2014)’s system in terms of F1 score on BLANC, MUC, and positive-links, while their system performs better in terms of B-CUBED. Even when taking into account the caveats mentioned above, it seems justified to assess that our model performs in general on-par with their stateof-the-art system. Their approach involves randomforest classification with best-link clustering and propagation of attributes between event mentions, and is grounded on a manifold of external feature sources, i.e., it uses a “rich set of 105 semantic features”. Thus, their approach is strongly tied to domains where these semantic features are available and is potentially hard to port to other text kinds. In contrast, our approach does not depend\nModel Dataset BLANC\n(P/R/F1 in %)\n1) Section 3 ACE 71.80 75.16 73.31 2) Sec. 3 + BestLink ACE 75.68 69.72 72.19\n3) Section 3 ACE++ 73.22 83.21 76.90 4) Sec. 3 + BestLink ACE++ 74.24 68.86 71.09\nTable 4: Impact of data amount and clustering.\non resources with restricted domain availability. Bejan and Harabagiu (2010) propose a nonparametric Bayesian model with standard lexicallevel features and WordNet-based similarity between event elements. We outperform their system in terms of B-CUBED and positive-links, which indicates that their system tends to over-merge event mentions, i.e., has a bias against singletons. They use a slightly bigger variant of ACE with 46 additional documents in their experiments.\nSangeetha and Arock (2012) hand-craft a similarity metric for event mentions based on the number of shared entities in the respective sentences, lexical terms, synsets in WordNet, which serves as input to a mincut-based cluster identification. Their system performs well in terms of B-cubed F1, however their paper provides few details about the exact experimental setup.\nAnother approach with results on ACE was presented by Chen et al. (2009), who employ a maximum-entropy classifier with agglomerative clustering and lexical, discourse, and semantic features, e.g., also a WordNet-based similarity measure. However, they report performance using a threshold optimized on the test set, thus we decided to not include the performance here.\n5.2 Further evaluation on ACE and ACE++\nWe now look at several aspects of the model performance to gain further insights about it’s behavior.\nImpact of dataset size and clustering strategy Table 4 shows the impact of increasing the amount of training data (ACE → ACE++). This increase\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nPw Loc Sen Dataset BLANC\n(P/R/F1 in %)\n1) X ACE++ 57.45 68.16 56.69 2) X X ACE++ 62.24 76.23 64.12 3) X X X ACE++ 73.22 83.21 76.90 4) X X ACE++ 82.60 70.71 74.97 5) X X ACE++ 59.67 66.25 61.28 6) X ACE++ 58.38 55.85 56.70\nTable 5: Impact of feature classes; Pw ≡ pairwise features, Loc ≡ trigger-local lexical features, Sen ≡ sentential features.\nModel Dataset BLANC\n(P/R/F1 in %)\nSection 3 ACE++ 73.22 83.21 76.90 All singletons ACE++ 45.29 50.00 47.53 One instance ACE++ 4.71 50.00 8.60 Same type ACE++ 62.73 84.75 61.35\nTable 6: Event-linking performance of our model against naive baselines.\n(rows 1, 3) leads to a boost in recall, from 75.16% to 83.21%, at the cost of a small decrease in precision. This indicates that the model can generalize much better using this additional training data.\nLooking into the use of the alternative clustering strategy BestLink recommended by Liu et al. (2014), we can make the expected observation of a precision improvement (row 1 vs. 2; row 3 vs. 4), due to fewer positive links being used before the transitive-closure clustering takes place. This is however outweighed by a large decline in recall, resulting in a lower F1 score (73.31→ 72.19; 76.90 → 71.09). The better performance of BestLink in Liu et al.’s model suggests that our model already weeds out many low confidence links in the classification step, which makes a downstream filtering unnecessary in terms of precision, and even counter-productive in terms of recall.\nImpact of feature classes Table 5 shows our model’s performance when particular feature classes are removed from the model (with retraining), with row 3 corresponding to the full model as described in Section 3. Unsurprisingly, classifying examples with just pairwise features (row 1) results in the worst performance, and adding first trigger-local lexical features (row 2), then sentential features (row 3) subsequently raises both precision and recall. Just using pairwise features and sentential ones (row 4), boosts precision,\nwhich is counter-intuitive at first, but may be explained by a different utilization of the sententialfeature part of the model during training. This part is then adapted to focus more on the trigger-word aspect, meaning the sentential features degrade to trigger-local features. While this allows to reach higher precision (recall that Section 3 finds that more than fifty percent of positive examples have trigger-word agreement), it substantially limits the model’s ability to learn other coreference-relevant aspects of event-mention pairs, leading to low recall. Further considering rows 5 & 6, we can conclude that all feature classes indeed positively contribute to the overall model performance.\nBaselines The result of applying three naive baselines to ACE++ is shown in Table 6. The all singletons/one instance baselines predict every input link to be negative/positive, respectively. In particular the all-singletons baseline performs well, due to the large fraction of singleton event mentions in the dataset. The third baseline, same event, predicts a positive link whenever there is agreement on the event type, namely, it ignores the possibility that there could be multiple event mentions of the same type in a document which do not refer to the same real-world event, e.g., referring to different terrorist attacks. This baseline also performs quite well, in particular in terms of recall, but shows low precision.\nError analysis We manually investigated a sample of 100 false positives and 100 false negatives from ACE++ in order to get an understanding of system errors.\nIt turns out that a significant portion of the false negatives would involve the resolution of a pronoun to a previous event mention, a very hard and yet unsolved problem. Consider the following examples:\n• “It’s crazy that we’re bombing Iraq. It sickens me.” • “Some of the slogans sought to rebut war supporters’\narguments that the protests are unpatriotic. [...] Nobody questions whether this is right or not.\nIn both examples, the event mentions (trigger words in bold font) are gold-annotated as coreferential, but our model failed to recognize this.\nAnother observation is that for 17 false negatives, we found analogous cases among the sampled false positives where annotators made a different annotation decision. Consider these examples:\n• The 1860 Presidential Election. [...] Lincoln won a plurality with about 40% of the vote.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n• She lost her seat in the 1997 election.\nEach bullet point has two event mentions (in bold font) taken from the same document and referring to the same event type, i.e., Personnel.Elect. While in the first example, the annotators identified the mentions as coreferential, the second pair of mentions is not annotated as such. Analogously, 22 out of the 100 analyzed false positives were cases where the misclassification of the system was plausible to a human rater. This exemplifies that this task has many boundary cases were a positive/negative decision is hard to make even for expert annotators, thus putting the overall performance of all models in Table 3 in perspective."
    }, {
      "heading" : "6 Related work",
      "text" : "We briefly point out other relevant approaches and efforts from the vast amount of literature.\nEvent coreference In addition to the competitors mentioned in Section 5, approaches for event linking were presented, e.g., by Chen and Ji (2009), who determine link scores with hand-crafted compatibility metrics for event mention pairs and a maximum-entropy model, and feed these to a spectral clustering algorithm. A variation of the eventcoreference resolution task extends the scope to cross-document relations. Cybulska and Vossen (2015) approach this task with various classification models and propose to use a type-specific granularity hierarchy for feature values. Lee et al. (2012) further extend the task definition by jointly resolving entity and event coreference, through several iterations of mention-cluster merge operations. Sachan et al. (2015) describe an active-learning based method for the same problem, where they derive a clustering of entities/events by incorporating bits of human judgment as constraints into the objective function. Araki and Mitamura (2015) simultaneously identify event triggers and disambiguate them wrt. one another with a structured-perceptron algorithm.\nResources Besides the ACE 2005 corpus, a number of other datasets with event-coreference annotation have been presented. Hovy et al. (2013) reports on the annotation process of two corpora from the domains of “violent events” and biographic texts; to our knowledge neither of them is publicly available. OntoNotes (Weischedel et al., 2013) comprises different annotation layers including coreference (Pradhan et al., 2012), however intermin-\ngles entity and event coreference. A series of releases of the EventCorefBank corpus (Bejan and Harabagiu, 2010; Lee et al., 2012; Cybulska and Vossen, 2014) combine linking of event mentions within and across documents, for which Liu et al. (2014) report a lack of completeness on the withindocument aspect. The ProcessBank dataset (Berant et al., 2014) provides texts with event links from the difficult biological domain.\nOther A few approaches to the upstream task of event extraction, while not considering withindocument event linking, still utilize discourse-level information or even cross-document inference. For example, Liao and Grishman (2010) showed how the output of sentence-based classifiers can be filtered wrt. discourse-level consistency. Yao et al. (2010) resolved coreferences between events from different documents in order to make a global extraction decision, similar to (Ji and Grishman, 2008) and (Li et al., 2011).\nIn addition to convolutional neural networks, more types of neural architectures lend themselves to the generation of sentential features. Recently many recursive networks and recurrent ones have been proposed for the task of relation classification, with state-of-the-art results (Socher et al., 2012; Hashimoto et al., 2013; Ebrahimi and Dou, 2015; Xu et al., 2015; Li et al., 2015)."
    }, {
      "heading" : "7 Conclusion",
      "text" : "Our proposed model for the task of event linking achieves state-of-the-art results without relying on external feature sources. We have thus shown that low linking performance, coming from a lack of semantic knowledge about a domain, is evitable. In addition, our experiments give further empirical evidence for the usefulness of neural models for generating latent-feature representations for sentences.\nAs next steps, we plan to test the model on more datasets and task variations, i.e., in a crossdocument setting or for joint trigger identification and coreference resolution. Furthermore, the generation of sentential features from other types of neural networks seems promising. Regarding our medium-term research agenda, we would like to investigate if the model can benefit from more finegrained information about the discourse structure underlying a text. This could guide the model when encountering the problematic case of pronoun resolution, described in the error analysis.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Coreference resolution for event mentions enables extraction systems to process document-level information. Current systems in this area base their decisions on rich semantic features from various knowledge bases, thus restricting them to domains where such external sources are available. We propose a model for this task which does not rely on such features but instead utilizes sentential features coming from convolutional neural networks. Two such networks first process coreference candidates and their respective context, thereby generating latent-feature representations which are tuned towards event aspects relevant for a linking decision. These representations are augmented with lexicallevel and pairwise features, and serve as input to a trainable similarity function producing a coreference score. Our model achieves state-of-the-art performance on two datasets, one of which is publicly available. An error analysis points out directions for further research.",
    "creator" : "TeX"
  }
}