{
  "name" : "91.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Compression of Neural Machine Translation Models via Pruning",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural Machine Translation (NMT) is a simple new architecture for translating texts from one language into another (Sutskever et al., 2014; Cho et al., 2014). NMT is a single deep neural network that is trained end-to-end, holding several advantages such as the ability to capture long-range dependencies in sentences, and generalization to unseen texts. Despite being relatively new, NMT has already achieved state-of-the-art translation results for several language pairs including EnglishFrench (Luong et al., 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015; Sennrich et al., 2015), English-\nTurkish (Sennrich et al., 2015), and English-Czech (Jean et al., 2015b; Luong and Manning, 2016). Figure 1 gives an example of an NMT system.\nWhile NMT has a significantly smaller memory footprint than traditional phrase-based approaches (which need to store gigantic phrase-tables and language models), the model size of NMT is still prohibitively large for mobile devices. For example, a recent state-of-the-art NMT system requires over 200 million parameters, resulting in a storage size of hundreds of megabytes (Luong et al., 2015a). Though the trend for bigger and deeper neural networks has brought great progress, it has also introduced over-parameterization, resulting in long running times, overfitting, and the large storage size discussed above. Thus a solution to the over-parameterization problem could potentially aid all three issues.\nOur contribution. In this paper we investigate the efficacy of weight pruning for NMT as a means of compression. We show that despite its simplicity, magnitude-based pruning with retraining is highly effective, and we compare three\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nmagnitude-based pruning schemes—class-blind, class-uniform and class-distribution. Though recent work has chosen to use the latter two, we find the first and simplest scheme—class-blind— the most successful. We are able to prune 40% of the weights of a state-of-the-art NMT system with negligible performance loss, and by adding a retraining phase after pruning, we can prune 80% with no performance loss. Our pruning experiments also reveal some patterns in the distribution of redundancy in NMT. In particular we find that higher layers, attention and softmax weights are the most important, while lower layers and the embedding weights hold a lot of redundancy. For the Long Short-Term Memory (LSTM) architecture, we find that at lower layers the parameters for the input are most crucial, but at higher layers the parameters for the gates also become important."
    }, {
      "heading" : "2 Related Work",
      "text" : "Pruning the parameters from a neural network, referred to as weight pruning or network pruning, is a well-established idea though it can be implemented in many ways. Among the most popular are the Optimal Brain Damage (OBD) (Le Cun et al., 1989) and Optimal Brain Surgeon (OBS) (Hassibi and Stork, 1993) techniques, which involve computing the Hessian matrix of the loss function with respect to the parameters, in order to assess the saliency of each parameter. Parameters with low saliency are then pruned from the network and the remaining sparse network is retrained. Both OBD and OBS were shown to perform better than the so-called ‘naive magnitude-based approach’, which prunes parameters according to their magnitude (deleting parameters close to zero). However, the high computational complexity of OBD and OBS compare unfavorably to the computational simplicity of the magnitude-based approach, especially for large networks (Augasta and Kathirvalavakumar, 2013).\nIn recent years, the deep learning renaissance has prompted a re-investigation of network pruning for modern models and tasks. Magnitudebased pruning (with iterative retraining) has yielded strong results for Convolutional Neural Nets (CNNs) performing visual tasks. (Collins and Kohli, 2014) prune 75% of AlexNet parameters with small accuracy loss on the ImageNet task, while (Han et al., 2015b) prune 89% of AlexNet parameters with no accuracy loss on the\nImageNet task.\nOther approaches focus on pruning neurons rather than parameters, via sparsity-inducing regularizers (Murray and Chiang, 2015) or ‘wiring together’ pairs of neurons with similar input weights (Srinivas and Babu, 2015). These approaches are much more constrained than weight-pruning schemes; they necessitate finding entire zero rows of weight matrices, or (near-) identical pairs of rows, before a single neuron can be pruned. By contrast weight-pruning approaches allow weights to be pruned freely and independently of each other. The neuron-pruning approach of (Srinivas and Babu, 2015) was shown to perform poorly (it suffered performance loss after removing only 35% of AlexNet parameters) compared to the weight-pruning approach of (Han et al., 2015b). Though (Murray and Chiang, 2015) demonstrates neuron-pruning for language modeling as part of a (non-neural) Machine Translation pipeline, their approach is more geared towards architecture selection than compression.\nThere are many other compression techniques for neural networks, including approaches based on on low-rank approximations for weight matrices (Jaderberg et al., 2014; Denton et al., 2014), or weight sharing via hash functions (Chen et al., 2015). Several methods involve reducing the precision of the weights or activations (Courbariaux et al., 2015), sometimes in conjunction with specialized hardware (Gupta et al., 2015), or even using binary weights (Lin et al., 2016). The ‘knowledge distillation’ technique of (Hinton et al., 2015) involves training a small ‘student’ network on the soft outputs of a large ‘teacher’ network. Some approaches use a sophisticated pipeline of several techniques to achieve impressive feats of compression (Han et al., 2015a; Iandola et al., 2016).\nMost of the above work has focused on compressing CNNs for vision tasks. We extend the magnitude-based pruning approach of (Han et al., 2015b) to recurrent neural networks (RNN), in particular LSTM architectures for NMT, and to our knowledge we are the first to do so. There has been some recent work on compression for RNNs (Lu et al., 2016; Prabhavalkar et al., 2016), but it focuses on other, non-pruning compression techniques. Nonetheless, our general observations on the distribution of redundancy in a LSTM are corroborated by (Lu et al., 2016).\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nstudentaamI Je\nJe suis\nsuis étudiant\nétudiant _\none-hot vectors length V\nword embeddings length n\nhidden layer 1 length n\nhidden layer 2 length n\nscores length V\none-hot vectors length V\n_\nsource language input target language input\ninitial (zero) states\ntarget language output\nsoftmax weights size: V × n\nKey to weight classes\nattention hidden layer length n\ncontext vector (one for each target word)\nlength n\ntarget layer 2\nweights size: 4n x 2n\nsource embedding weights\nsize: n x V\nattention weights size: n x 2n\ntarget layer 1\nweights size: 4n x 2n\nsource layer 2\nweights size: 4n x 2n\nsource layer 1\nweights size: 4n x 2n\ntarget embedding weights\nsize: n x V\nFigure 2: NMT architecture. This example has two layers, but our system has four. The different weight classes are indicated by arrows of different color (the black arrows in the top right represent simply choosing the highest-scoring word, and thus require no parameters). Best viewed in color."
    }, {
      "heading" : "3 Our Approach",
      "text" : "We first give a brief overview of Neural Machine Translation before delving into a model architecture of interest, the deep multi-layer recurrent model with LSTM. We then explain the different types of NMT weights together with our approaches to pruning and retraining."
    }, {
      "heading" : "3.1 Neural Machine Translation",
      "text" : "Neural machine translation aims to directly model the conditional probability p(y|x) of translating a source sentence, x1, . . . , xn, to a target sentence, y1, . . . , ym. It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). The encoder computes a representation s for each source sentence. Based on that source representation, the decoder generates a translation, one target word at a time, and hence, decomposes the log conditional probability as:\nlog p(y|x) = ∑m\nt=1 log p (yt|y<t, s) (1)\nMost NMT work uses RNNs, but approaches differ in terms of: (a) architecture, which can be unidirectional, bidirectional, or deep multilayer RNN; and (b) RNN type, which can be\nLong Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or the gated recurrent unit (Cho et al., 2014).\nIn this work, we specifically consider the deep multi-layer recurrent architecture with LSTM as the hidden unit type. Figure 1 illustrates an instance of that architecture during training in which the source and target sentence pair are input for supervised learning. During testing, the target sentence is not known in advance; instead, the most probable target words predicted by the model are fed as inputs into the next timestep. The network stops when it emits the end-of-sentence symbol— a special ‘word’ in the vocabulary, represented by a dash in Figure 1."
    }, {
      "heading" : "3.2 Understanding NMT Weights",
      "text" : "We show in Figure 2 the same system in more detail, highlighting the different types of parameters, or weights, in the model. We will go through the architecture from bottom to top. First, a vocabulary is chosen for each language, assuming that the top V frequent words are selected. Thus, every word in the source or target vocabulary can be represented by a one-hot vector of length V . The source input sentence and target input sentence, represented as a sequence of one-hot vec-\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\ntors, are transformed into a sequence of word embeddings by the embedding weights. These embedding weights, which are learned during training, are different for the source words and the target words. The word embeddings are vectors of length n, the dimension of the network.\nThe word embeddings are then fed as input into the main network, which consists of two multilayer RNNs ‘stuck together’—an encoder for the source language and a decoder for the target language, each with their own weights. The feedforward (vertical) weights connect the hidden unit from the layer below to the upper RNN block, and the recurrent (horizontal) weights connect the hidden unit from the previous time-step RNN block to the current time-step RNN block.\nThe hidden state at the top layer of the decoder is fed through an attention layer, which guides the translation by ‘paying attention’ to relevant parts of the source sentence; for more information see Section 3 of (Luong et al., 2015a). Finally, for each target word, the top layer hidden unit is transformed by the softmax weights into a score vector of length V . The target word with the highest score is selected as the output translation.\nWeight Subgroups in LSTM – For the aforementioned RNN block, we choose to use LSTM as a hidden unit type. To facilitate our discussion later on the different subgroups of weights within LSTM, we first revise details of an LSTM suggested by Zaremba et al. (2014) as follows: i f o\nĥ\n = \nsigm sigm sigm tanh\nT4n,2n(hl−1thlt−1 )\n(2)\nclt = f ◦ clt−1 + i ◦ ĥ (3) hlt = o ◦ tanh(clt) (4)\nHere, each LSTM block at time t and layer l computes as outputs a pair of hidden and memory vectors (hlt, c l t) given the previous pair (h l t−1, c l t−1) and an input vector hl−1t (either from the below LSTM block or the embedding weights if l = 1). All of these vectors have dimensions of n.\nThe core of an LSTM block is the weight matrix T4n,2n of size 4n×2n. This matrix can be decomposed into 8 subgroups that are responsible for the interractions between {input gate i, forget gate f , output gate o, input signal ĥ} × {feed-forward input hl−1t , recurrent input h l t−1}."
    }, {
      "heading" : "3.3 Pruning Schemes",
      "text" : "We follow the general magnitude-based approach of (Han et al., 2015b), which consists of pruning weights with smallest absolute value. However, we question the authors’ pruning scheme with respect to the different weight classes, and experiment with three pruning schemes. Suppose we wish to prune x% of the total parameters in the model. How do we distribute the pruning over the different weight classes (illustrated in Figure 2) of our model? We propose to examine three different pruning schemes:\n1. Class-blind: Take all parameters, sort them by magnitude and prune the x% with smallest magnitude, regardless of weight class. (So some classes are pruned proportionally more than others).\n2. Class-uniform: Within each class, sort the weights by magnitude and prune the x% with smallest magnitude. (So all classes have exactly x% of their parameters pruned).\n3. Class-distribution: For each class c, weights with magnitude less than λσc are pruned. Here, σc is the standard deviation of that class and λ is a universal parameter chosen such that in total, x% of all parameters are pruned. This is used by (Han et al., 2015b).\nAll these schemes have their seeming advantages. Class-blind pruning is the simplest and adheres to the principle that pruning weights (or equivalently, setting them to zero) is least damaging when those weights are small, regardless of their locations in an architecture. Class-uniform pruning and classdistribution pruning both seek to prune proportionally within each weight class, either absolutely, or relative to the standard deviation of that class. We find that class-blind pruning outperforms both other schemes (see Section 4.1)."
    }, {
      "heading" : "3.4 Retraining",
      "text" : "In order to prune NMT models aggressively without performance loss, we retrain our pruned networks. In our implementation, we keep “mask” matrices, which represent the sparse structure of a network, so as to ignore weights at pruned locations. We detail in Section 4.1 a successful “formula” to retrain pruned NMT models.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nso urc\ne l ay\ner 1\nso urc\ne l ay\ner 2\nso urc\ne l ay\ner 3\nso urc\ne l ay\ner 4\ntar ge\nt la ye\nr 1\ntar ge\nt la ye\nr 2\ntar ge\nt la ye\nr 3\ntar ge\nt la ye\nr 4\natt en\ntio n\nso ftm\nax\nso urc\ne e mb\ned din\ng\ntar ge\nt e mb\ned din\ng\n0\n5\n10 15 pe rp le xi ty ch an\nge class-blind class-uniform class-distribution\nFigure 3: ‘Breakdown’ of performance loss (i.e., perplexity increase) by weight class, when pruning 90% of weights using each of the three pruning schemes. Each of the first eight classes have 8 million weights, attention has 2 million, and the last three have 50 million weights each."
    }, {
      "heading" : "4 Experiments",
      "text" : "We evaluate the effectiveness of our pruning approaches on a state-of-the-art NMT model.1 Specifically, an attention-based English-German NMT system from (Luong et al., 2015a) is considered. Training data was obtained from WMT’14 consisting of 4.5M sentence pairs (116M English words, 110M German words). For more details on training hyperparameters, we refer readers to Section 4.1 of (Luong et al., 2015a). All models are tested on newstest2014 (2737 sentences). The model achieves a perplexity of 6.1 and a BLEU score of 20.5 (after unknown word replacement).2\nWhen retraining pruned NMT systems, we use the following settings: (a) we start with a smaller learning rate of 0.5 (the original model uses a learning rate of 1.0), (b) we train for fewer epochs, 4 instead of 12, using plain SGD, (c) a simple learning rate schedule is employed; after 2 epochs, we begin to halve the learning rate every half an epoch, and (d) all other hyperparameters are the same, such as mini-batch size 128, maximum gradient norm 5, and dropout with probability 0.2."
    }, {
      "heading" : "4.1 Comparing pruning schemes",
      "text" : "Despite its simplicity, we observe in Figure 4 that class-blind pruning outperforms both other\n1We thank the authors of (Luong et al., 2015a) for providing their trained models and assistance in using the codebase at https://github.com/lmthang/nmt.matlab.\n2The performance of this model is reported under row global (dot) in Table 4 of (Luong et al., 2015a).\nschemes in terms of translation quality at all pruning percentages. Furthermore, to explain the poor performance of class-uniform and classdistribution pruning, for each of the three pruning schemes, we pruned each class separately and recorded the effect on performance (as measured by perplexity). Figure 3 shows that with classuniform pruning, the overall performance loss is caused disproportionately by a few classes: target layer 4, attention and softmax weights. Looking at Figure 5, we see that the most damaging classes to prune also tend to be those with weights of greater magnitude—these classes have much larger weights than others at the same percentile, so pruning them under the class-uniform pruning scheme is more damaging. The situation is similar for class-distribution pruning.\nBy contrast, Figure 3 shows that under class-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n0 0.1 0.2 0.3 0.4 0.5\n100\n101\nmagnitude of largest deleted weight\npe rp\nle xi\nty ch\nan ge\nFigure 5: Magnitude of largest deleted weight vs. perplexity change, for the 12 different weight classes when pruning 90% of parameters by classuniform pruning.\nblind pruning, the damage caused by pruning softmax, attention and target layer 4 weights is greatly decreased, and the contribution of each class towards the performance loss is overall more uniform. In fact, the distribution begins to reflect the number of parameters in each class—for example, the source and target embedding classes have larger contributions because they have more weights. We use only class-blind pruning for the rest of the experiments.\nFigure 3 also reveals some interesting information about the distribution of redundancy in NMT architectures—namely it seems that higher layers are more important than lower layers, and that attention and softmax weights are crucial. We will explore the distribution of redundancy further in Section 4.3."
    }, {
      "heading" : "4.2 Pruning and retraining",
      "text" : "Pruning has an immediate negative impact on performance (as measured by BLEU score on the validation set) that is exponential in pruning percentage; this is demonstrated by the blue line in Figure 6. However we find that up to about 40% pruning, performance is mostly unaffected, indicating a large amount of redundancy and overparameterization in NMT.\nWe now consider the effect of retraining pruned models. The orange line in Figure 6 shows that after retraining the pruned models, baseline performance (20.48 BLEU) is both recovered and improved upon, up to 80% pruning (20.91 BLEU), with only a small performance loss at 90% pruning (20.13 BLEU). This may seem surprising, as we might not expect a sparse model to significantly\nout-perform a model with five times as many parameters. There are several possible explanations, two of which are given below.\nFirstly, we found that the less-pruned models perform better on the training set than the validation set, whereas the more-pruned models have closer performance on the two sets. This indicates that pruning has a regularizing effect on the retraining phase, though clearly more is not always better, as the 50% pruned and retrained model performs better than the 90% pruned and retrained model. Nonetheless, this regularization effect may explain why the pruned and retrained models outperform the baseline.\n0 1 2 3 4 5\n·105\n2\n4\n6\n8\ntraining iterations\nlo ss\nFigure 7: The loss function during training, pruning and retraining. The vertical dotted line marks the point when 80% of the parameters are pruned. The horizontal dotted line marks the best performance of the unpruned baseline.\nAlternatively, pruning may serve as a means to escape a local optimum. Figure 7 shows the loss function over time during the training, pruning and retraining process. During the original training\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\ntarget embedding weights\nsource embedding weights\nleast common wordmost common word\nsource layer 1 weights\nrecurrentfeed-forward\ninput gate\nforget gate\noutput gate\ninput\nsource layer 2 weights source layer 3 weights source layer 4 weights\ntarget layer 1 weights target layer 2 weights target layer 3 weights target layer 4 weights\nFigure 8: Graphical representation of the location of small weights in the model. Black pixels represent weights with absolute size in the bottom 80%; white pixels represent those with absolute size in the top 20%. Equivalently, these pictures illustrate which parameters remain after pruning 80% using our class-blind pruning scheme.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nprocess, the loss curve flattens out and seems to converge (note that we use early stopping to obtain our baseline model, so the original model was trained for longer than shown in Figure 7). Pruning causes an immediate increase in the loss function, but enables further gradient descent, allowing the retraining process to find a new, better local optimum. It seems that the disruption caused by pruning is beneficial in the long-run."
    }, {
      "heading" : "4.3 Distribution of redundancy in NMT",
      "text" : "We visualize in Figure 8 the redundancy structore of our NMT baseline model. Black pixels represent weights near to zero; white pixels represent larger ones. First we consider the embedding weight matrices, whose columns correspond to words in the vocabulary. Unsurprisingly, in Figure 8, we see that the parameters corresponding to the less common words are more dispensable. In fact, at the 80% pruning rate, for 100 uncommon source words and 1194 uncommon target words, we delete all parameters corresponding to that word. This is not quite the same as removing the word from the vocabulary—true out-ofvocabulary words are mapped to the embedding for the ‘unknown word’ symbol, whereas these ‘pruned-out’ words are mapped to a zero embedding. However in the original unpruned model these uncommon words already had near-zero embeddings, indicating that the model was unable to learn sufficiently distinctive representations.\nReturning to Figure 8, now look at the eight weight matrices for the source and target connections at each of the four layers. Each matrix corresponds to the 4n × 2n matrix T4n,2n in Equation (1). In all eight matrices, we observe—as does (Lu et al., 2016)—that the weights connecting to the input ĥ are most crucial, followed by the input gate i, then the output gate o, then the forget gate f . This is particularly true of the lower layers, which focus primarily on the input ĥ. However for higher layers, especially on the target side, weights connecting to the gates are as important as those connecting to the input ĥ. The gates represent the LSTM’s ability to add to, delete from or retrieve information from the memory cell. Figure 8 therefore shows that these sophisticated memory cell abilities are most important at the end of the NMT pipeline (the top layer of the decoder). This is reasonable, as we expect higher-level features to be learned later in a deep learning pipeline.\nWe also observe that for lower layers, the feedforward input is much more important than the recurrent input, whereas for higher layers the recurrent input becomes more important. This makes sense: lower layers concentrate on the low-level information from the current word embedding (the feed-forward input), whereas higher layers make use of the higher-level representation of the sentence so far (the recurrent input).\nLastly, on close inspection, we notice several white diagonals emerging within the subsquares of the matrices in Figure 8, indicating that even without initializing the weights to identity matrices, an identity-like weight matrix is learned. At higher pruning percentages, these diagonals become more pronounced."
    }, {
      "heading" : "5 Future Work",
      "text" : "The pruning method described in (Han et al., 2015b) includes several iterations of pruning and retraining. Implementing this for NMT would likely result in further compression and performance improvements. If possible it would be highly valuable to exploit the sparsity of the pruned models to speed up training and runtime, perhaps through sparse matrix representations and multiplications. Though we have found magnitude-based pruning to perform very well, it would be instructive to revisit the original claim that other pruning methods (for example Optimal Brain Damage and Optimal Brain Surgery) are more principled, and perform a comparative study."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have shown that weight pruning with retraining is a highly effective method of compression and regularization on a state-of-the-art NMT system, compressing the model to 20% of its size with no loss of performance. Though we are the first to apply compression techniques to NMT, we obtain a similar degree of compression to other current work on compressing state-of-the-art deep neural networks, with an approach that is simpler than most. We have found that the absolute size of parameters is of primary importance when choosing which to prune, leading to an approach that is extremely simple to implement, and can be applied to any neural network. Lastly, we have gained insight into the distribution of redundancy in the NMT architecture.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Pruning algorithms of neural networksa comparative study",
      "author" : [ "M Gethsiyal Augasta", "T Kathirvalavakumar." ],
      "venue" : "Central European Journal of Computer Science, 3(3):105–115.",
      "citeRegEx" : "Augasta and Kathirvalavakumar.,? 2013",
      "shortCiteRegEx" : "Augasta and Kathirvalavakumar.",
      "year" : 2013
    }, {
      "title" : "Compressing neural networks with the hashing trick",
      "author" : [ "Wenlin Chen", "James T Wilson", "Stephen Tyree", "Kilian Q Weinberger", "Yixin Chen." ],
      "venue" : "arXiv preprint arXiv:1504.04788.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Memory bounded deep convolutional networks",
      "author" : [ "Maxwell D Collins", "Pushmeet Kohli." ],
      "venue" : "arXiv preprint arXiv:1412.1442.",
      "citeRegEx" : "Collins and Kohli.,? 2014",
      "shortCiteRegEx" : "Collins and Kohli.",
      "year" : 2014
    }, {
      "title" : "Low precision arithmetic for deep learning",
      "author" : [ "Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David." ],
      "venue" : "International Conference on Learning Representations (ICLR) Workshop Contribution.",
      "citeRegEx" : "Courbariaux et al\\.,? 2015",
      "shortCiteRegEx" : "Courbariaux et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploiting linear structure within convolutional networks for efficient evaluation",
      "author" : [ "Emily L Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Denton et al\\.,? 2014",
      "shortCiteRegEx" : "Denton et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep learning with limited numerical precision",
      "author" : [ "Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan." ],
      "venue" : "arXiv preprint arXiv:1502.02551.",
      "citeRegEx" : "Gupta et al\\.,? 2015",
      "shortCiteRegEx" : "Gupta et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "author" : [ "Song Han", "Huizi Mao", "William J Dally." ],
      "venue" : "International Conference on Learning Representations (ICLR’16 oral).",
      "citeRegEx" : "Han et al\\.,? 2015a",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning both weights and connections for efficient neural network",
      "author" : [ "Song Han", "Jeff Pool", "John Tran", "William Dally." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1135–1143.",
      "citeRegEx" : "Han et al\\.,? 2015b",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2015
    }, {
      "title" : "Second order derivatives for network pruning: Optimal brain surgeon",
      "author" : [ "Babak Hassibi", "David G Stork." ],
      "venue" : "Morgan Kaufmann.",
      "citeRegEx" : "Hassibi and Stork.,? 1993",
      "shortCiteRegEx" : "Hassibi and Stork.",
      "year" : 1993
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS) Deep Learning Workshop.",
      "citeRegEx" : "Hinton et al\\.,? 2015",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber." ],
      "venue" : "Neural computation, 9(8):1735–1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and < 1mb model size",
      "author" : [ "Forrest N Iandola", "Matthew W Moskewicz", "Khalid Ashraf", "Song Han", "William J Dally", "Kurt Keutzer." ],
      "venue" : "arXiv preprint arXiv:1602.07360.",
      "citeRegEx" : "Iandola et al\\.,? 2016",
      "shortCiteRegEx" : "Iandola et al\\.",
      "year" : 2016
    }, {
      "title" : "Speeding up convolutional neural networks with low rank expansions",
      "author" : [ "Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman." ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS).",
      "citeRegEx" : "Jaderberg et al\\.,? 2014",
      "shortCiteRegEx" : "Jaderberg et al\\.",
      "year" : 2014
    }, {
      "title" : "On using very large target vocabulary for neural machine translation",
      "author" : [ "Sébastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio." ],
      "venue" : "ACL.",
      "citeRegEx" : "Jean et al\\.,? 2015a",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Montreal neural machine translation systems for WMT’15",
      "author" : [ "Sébastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio." ],
      "venue" : "WMT.",
      "citeRegEx" : "Jean et al\\.,? 2015b",
      "shortCiteRegEx" : "Jean et al\\.",
      "year" : 2015
    }, {
      "title" : "Recurrent continuous translation models",
      "author" : [ "Nal Kalchbrenner", "Phil Blunsom." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Kalchbrenner and Blunsom.,? 2013",
      "shortCiteRegEx" : "Kalchbrenner and Blunsom.",
      "year" : 2013
    }, {
      "title" : "Optimal brain damage",
      "author" : [ "Yann Le Cun", "John S Denker", "Sara A Solla." ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Cun et al\\.,? 1989",
      "shortCiteRegEx" : "Cun et al\\.",
      "year" : 1989
    }, {
      "title" : "Neural networks with few multiplications",
      "author" : [ "Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio." ],
      "venue" : "International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Lin et al\\.,? 2016",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning compact recurrent neural networks",
      "author" : [ "Zhiyun Lu", "Vikas Sindhwani", "Tara N Sainath." ],
      "venue" : "arXiv preprint arXiv:1604.02594.",
      "citeRegEx" : "Lu et al\\.,? 2016",
      "shortCiteRegEx" : "Lu et al\\.",
      "year" : 2016
    }, {
      "title" : "Stanford neural machine translation systems for spoken language domain",
      "author" : [ "Minh-Thang Luong", "Christopher D. Manning." ],
      "venue" : "IWSLT.",
      "citeRegEx" : "Luong and Manning.,? 2015",
      "shortCiteRegEx" : "Luong and Manning.",
      "year" : 2015
    }, {
      "title" : "Achieving open vocabulary neural machine translation with hybrid word-character models",
      "author" : [ "Minh-Thang Luong", "Christopher D. Manning." ],
      "venue" : "arXiv preprint arXiv:1604.00788.",
      "citeRegEx" : "Luong and Manning.,? 2016",
      "shortCiteRegEx" : "Luong and Manning.",
      "year" : 2016
    }, {
      "title" : "Effective approaches to attentionbased neural machine translation",
      "author" : [ "Minh-Thang Luong", "Hieu Pham", "Christopher D Manning." ],
      "venue" : "EMNLP.",
      "citeRegEx" : "Luong et al\\.,? 2015a",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Addressing the rare word problem in neural machine translation",
      "author" : [ "Minh-Thang Luong", "Ilya Sutskever", "Quoc V Le", "Oriol Vinyals", "Wojciech Zaremba." ],
      "venue" : "ACL.",
      "citeRegEx" : "Luong et al\\.,? 2015b",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2015
    }, {
      "title" : "Auto-sizing neural networks: With applications to n-gram language models",
      "author" : [ "Kenton Murray", "David Chiang." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP).",
      "citeRegEx" : "Murray and Chiang.,? 2015",
      "shortCiteRegEx" : "Murray and Chiang.",
      "year" : 2015
    }, {
      "title" : "On the compression of recurrent neural networks with an application to lvcsr acoustic modeling for embedded speech recognition",
      "author" : [ "Rohit Prabhavalkar", "Ouais Alsharif", "Antoine Bruguier", "Ian McGraw." ],
      "venue" : "International Conference on Acoustics, Speech",
      "citeRegEx" : "Prabhavalkar et al\\.,? 2016",
      "shortCiteRegEx" : "Prabhavalkar et al\\.",
      "year" : 2016
    }, {
      "title" : "Improving neural machine translation models with monolingual data",
      "author" : [ "Rico Sennrich", "Barry Haddow", "Alexandra Birch." ],
      "venue" : "arXiv preprint arXiv:1511.06709.",
      "citeRegEx" : "Sennrich et al\\.,? 2015",
      "shortCiteRegEx" : "Sennrich et al\\.",
      "year" : 2015
    }, {
      "title" : "Data-free parameter pruning for deep neural networks",
      "author" : [ "Suraj Srinivas", "R Venkatesh Babu." ],
      "venue" : "arXiv preprint arXiv:1507.06149.",
      "citeRegEx" : "Srinivas and Babu.,? 2015",
      "shortCiteRegEx" : "Srinivas and Babu.",
      "year" : 2015
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le." ],
      "venue" : "NIPS.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Recurrent neural network regularization",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals." ],
      "venue" : "arXiv preprint arXiv:1409.2329.",
      "citeRegEx" : "Zaremba et al\\.,? 2014",
      "shortCiteRegEx" : "Zaremba et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 28,
      "context" : "Neural Machine Translation (NMT) is a simple new architecture for translating texts from one language into another (Sutskever et al., 2014; Cho et al., 2014).",
      "startOffset" : 115,
      "endOffset" : 157
    }, {
      "referenceID" : 2,
      "context" : "Neural Machine Translation (NMT) is a simple new architecture for translating texts from one language into another (Sutskever et al., 2014; Cho et al., 2014).",
      "startOffset" : 115,
      "endOffset" : 157
    }, {
      "referenceID" : 23,
      "context" : "Despite being relatively new, NMT has already achieved state-of-the-art translation results for several language pairs including EnglishFrench (Luong et al., 2015b), English-German (Jean et al.",
      "startOffset" : 143,
      "endOffset" : 164
    }, {
      "referenceID" : 14,
      "context" : ", 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015; Sennrich et al., 2015), Englishstudent a am I Je Je\t\r   suis",
      "startOffset" : 25,
      "endOffset" : 114
    }, {
      "referenceID" : 22,
      "context" : ", 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015; Sennrich et al., 2015), Englishstudent a am I Je Je\t\r   suis",
      "startOffset" : 25,
      "endOffset" : 114
    }, {
      "referenceID" : 20,
      "context" : ", 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015; Sennrich et al., 2015), Englishstudent a am I Je Je\t\r   suis",
      "startOffset" : 25,
      "endOffset" : 114
    }, {
      "referenceID" : 26,
      "context" : ", 2015b), English-German (Jean et al., 2015a; Luong et al., 2015a; Luong and Manning, 2015; Sennrich et al., 2015), Englishstudent a am I Je Je\t\r   suis",
      "startOffset" : 25,
      "endOffset" : 114
    }, {
      "referenceID" : 26,
      "context" : "Turkish (Sennrich et al., 2015), and English-Czech (Jean et al.",
      "startOffset" : 8,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : ", 2015), and English-Czech (Jean et al., 2015b; Luong and Manning, 2016).",
      "startOffset" : 27,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : ", 2015), and English-Czech (Jean et al., 2015b; Luong and Manning, 2016).",
      "startOffset" : 27,
      "endOffset" : 72
    }, {
      "referenceID" : 22,
      "context" : "For example, a recent state-of-the-art NMT system requires over 200 million parameters, resulting in a storage size of hundreds of megabytes (Luong et al., 2015a).",
      "startOffset" : 141,
      "endOffset" : 162
    }, {
      "referenceID" : 9,
      "context" : ", 1989) and Optimal Brain Surgeon (OBS) (Hassibi and Stork, 1993) techniques, which involve computing the Hessian matrix of the loss function with respect to the parameters, in order to assess the saliency of each parameter.",
      "startOffset" : 40,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "However, the high computational complexity of OBD and OBS compare unfavorably to the computational simplicity of the magnitude-based approach, especially for large networks (Augasta and Kathirvalavakumar, 2013).",
      "startOffset" : 173,
      "endOffset" : 210
    }, {
      "referenceID" : 3,
      "context" : "(Collins and Kohli, 2014) prune 75% of AlexNet parameters with small accuracy loss on the ImageNet task, while (Han et al.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 8,
      "context" : "(Collins and Kohli, 2014) prune 75% of AlexNet parameters with small accuracy loss on the ImageNet task, while (Han et al., 2015b) prune 89% of AlexNet parameters with no accuracy loss on the ImageNet task.",
      "startOffset" : 111,
      "endOffset" : 130
    }, {
      "referenceID" : 24,
      "context" : "Other approaches focus on pruning neurons rather than parameters, via sparsity-inducing regularizers (Murray and Chiang, 2015) or ‘wiring together’ pairs of neurons with similar input weights (Srinivas and Babu, 2015).",
      "startOffset" : 101,
      "endOffset" : 126
    }, {
      "referenceID" : 27,
      "context" : "Other approaches focus on pruning neurons rather than parameters, via sparsity-inducing regularizers (Murray and Chiang, 2015) or ‘wiring together’ pairs of neurons with similar input weights (Srinivas and Babu, 2015).",
      "startOffset" : 192,
      "endOffset" : 217
    }, {
      "referenceID" : 27,
      "context" : "The neuron-pruning approach of (Srinivas and Babu, 2015) was shown to perform poorly (it suffered performance loss after removing only 35% of AlexNet parameters) compared to the weight-pruning approach of (Han et al.",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "The neuron-pruning approach of (Srinivas and Babu, 2015) was shown to perform poorly (it suffered performance loss after removing only 35% of AlexNet parameters) compared to the weight-pruning approach of (Han et al., 2015b).",
      "startOffset" : 205,
      "endOffset" : 224
    }, {
      "referenceID" : 24,
      "context" : "Though (Murray and Chiang, 2015) demonstrates neuron-pruning for language modeling as part of a (non-neural) Machine Translation pipeline, their approach is more geared towards architecture selection than compression.",
      "startOffset" : 7,
      "endOffset" : 32
    }, {
      "referenceID" : 13,
      "context" : "There are many other compression techniques for neural networks, including approaches based on on low-rank approximations for weight matrices (Jaderberg et al., 2014; Denton et al., 2014), or weight sharing via hash functions (Chen et al.",
      "startOffset" : 142,
      "endOffset" : 187
    }, {
      "referenceID" : 5,
      "context" : "There are many other compression techniques for neural networks, including approaches based on on low-rank approximations for weight matrices (Jaderberg et al., 2014; Denton et al., 2014), or weight sharing via hash functions (Chen et al.",
      "startOffset" : 142,
      "endOffset" : 187
    }, {
      "referenceID" : 1,
      "context" : ", 2014), or weight sharing via hash functions (Chen et al., 2015).",
      "startOffset" : 46,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "Several methods involve reducing the precision of the weights or activations (Courbariaux et al., 2015), sometimes in conjunction with specialized hardware (Gupta et al.",
      "startOffset" : 77,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : ", 2015), sometimes in conjunction with specialized hardware (Gupta et al., 2015), or even using binary weights (Lin et al.",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 18,
      "context" : ", 2015), or even using binary weights (Lin et al., 2016).",
      "startOffset" : 38,
      "endOffset" : 56
    }, {
      "referenceID" : 10,
      "context" : "The ‘knowledge distillation’ technique of (Hinton et al., 2015) involves training a small ‘student’ network on the soft outputs of a large ‘teacher’ network.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "Some approaches use a sophisticated pipeline of several techniques to achieve impressive feats of compression (Han et al., 2015a; Iandola et al., 2016).",
      "startOffset" : 110,
      "endOffset" : 151
    }, {
      "referenceID" : 12,
      "context" : "Some approaches use a sophisticated pipeline of several techniques to achieve impressive feats of compression (Han et al., 2015a; Iandola et al., 2016).",
      "startOffset" : 110,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "We extend the magnitude-based pruning approach of (Han et al., 2015b) to recurrent neural networks (RNN), in particular LSTM architectures for NMT, and to our knowledge we are the first to do so.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 19,
      "context" : "There has been some recent work on compression for RNNs (Lu et al., 2016; Prabhavalkar et al., 2016), but it focuses on other, non-pruning compression techniques.",
      "startOffset" : 56,
      "endOffset" : 100
    }, {
      "referenceID" : 25,
      "context" : "There has been some recent work on compression for RNNs (Lu et al., 2016; Prabhavalkar et al., 2016), but it focuses on other, non-pruning compression techniques.",
      "startOffset" : 56,
      "endOffset" : 100
    }, {
      "referenceID" : 19,
      "context" : "Nonetheless, our general observations on the distribution of redundancy in a LSTM are corroborated by (Lu et al., 2016).",
      "startOffset" : 102,
      "endOffset" : 119
    }, {
      "referenceID" : 16,
      "context" : "It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014).",
      "startOffset" : 63,
      "endOffset" : 137
    }, {
      "referenceID" : 28,
      "context" : "It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014).",
      "startOffset" : 63,
      "endOffset" : 137
    }, {
      "referenceID" : 2,
      "context" : "It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014).",
      "startOffset" : 63,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "Most NMT work uses RNNs, but approaches differ in terms of: (a) architecture, which can be unidirectional, bidirectional, or deep multilayer RNN; and (b) RNN type, which can be Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or the gated recurrent unit (Cho et al.",
      "startOffset" : 207,
      "endOffset" : 241
    }, {
      "referenceID" : 2,
      "context" : "Most NMT work uses RNNs, but approaches differ in terms of: (a) architecture, which can be unidirectional, bidirectional, or deep multilayer RNN; and (b) RNN type, which can be Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or the gated recurrent unit (Cho et al., 2014).",
      "startOffset" : 270,
      "endOffset" : 288
    }, {
      "referenceID" : 22,
      "context" : "The hidden state at the top layer of the decoder is fed through an attention layer, which guides the translation by ‘paying attention’ to relevant parts of the source sentence; for more information see Section 3 of (Luong et al., 2015a).",
      "startOffset" : 215,
      "endOffset" : 236
    }, {
      "referenceID" : 22,
      "context" : "The hidden state at the top layer of the decoder is fed through an attention layer, which guides the translation by ‘paying attention’ to relevant parts of the source sentence; for more information see Section 3 of (Luong et al., 2015a). Finally, for each target word, the top layer hidden unit is transformed by the softmax weights into a score vector of length V . The target word with the highest score is selected as the output translation. Weight Subgroups in LSTM – For the aforementioned RNN block, we choose to use LSTM as a hidden unit type. To facilitate our discussion later on the different subgroups of weights within LSTM, we first revise details of an LSTM suggested by Zaremba et al. (2014) as follows:  i f o ĥ  =  sigm sigm sigm tanh T4n,2n(hl−1 t ht−1 ) (2)",
      "startOffset" : 216,
      "endOffset" : 707
    }, {
      "referenceID" : 8,
      "context" : "We follow the general magnitude-based approach of (Han et al., 2015b), which consists of pruning weights with smallest absolute value.",
      "startOffset" : 50,
      "endOffset" : 69
    }, {
      "referenceID" : 8,
      "context" : "This is used by (Han et al., 2015b).",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 22,
      "context" : "1 Specifically, an attention-based English-German NMT system from (Luong et al., 2015a) is considered.",
      "startOffset" : 66,
      "endOffset" : 87
    }, {
      "referenceID" : 22,
      "context" : "1 of (Luong et al., 2015a).",
      "startOffset" : 5,
      "endOffset" : 26
    }, {
      "referenceID" : 22,
      "context" : "We thank the authors of (Luong et al., 2015a) for providing their trained models and assistance in using the codebase at https://github.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 22,
      "context" : "The performance of this model is reported under row global (dot) in Table 4 of (Luong et al., 2015a).",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 19,
      "context" : "In all eight matrices, we observe—as does (Lu et al., 2016)—that the weights connecting to the input ĥ are most crucial, followed by the input gate i, then the output gate o, then the forget gate f .",
      "startOffset" : 42,
      "endOffset" : 59
    }, {
      "referenceID" : 8,
      "context" : "The pruning method described in (Han et al., 2015b) includes several iterations of pruning and retraining.",
      "startOffset" : 32,
      "endOffset" : 51
    } ],
    "year" : 2016,
    "abstractText" : "Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT’14 EnglishGerman translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model.",
    "creator" : "TeX"
  }
}