{
  "name" : "25.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Identifying Temporal Orientation of Word Senses",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\nThe ability to capture the time information conveyed in natural language is essential to many natural language processing applications such as information retrieval, question answering, automatic summarization, targeted marketing, loan repayment forecasting, and understanding economic patterns. Therefore, a lexical temporal resource associating word senses with their underlying temporal orientation would be crucial for the computational tasks aiming at interpretation of language of time in text.\nIn this paper, we propose a semisupervised minimum cuts paradigm that makes use of WordNet definitions or ‘glosses’, its conceptual-semantic and lexical relations to supplement WordNet entries with information on the temporality of its word senses. Intrinsic and extrinsic evaluation results show that the proposed approach outperforms prior semisupervised, non-graph classification approaches to the temporality recognition of word senses/concepts, and confirm the soundness of the proposed approach."
    }, {
      "heading" : "1 Introduction",
      "text" : "There is considerable academic and commercial interest in processing time information in text, where that information is expressed either explicitly, or implicitly, or connotatively. Recognizing such information and exploiting it for Information Retrieval (IR) and Natural Language Processing (NLP) tasks are important features that can significantly improve the functionality of NLP/IR applications.\nAutomatic identification of temporal expressions in text is usually performed either via time taggers (Strötgen and Gertz, 2013), which contain pattern files, such as uni-grams and bi-grams used to express temporal expressions in a given language (e.g. names of months) or various grammatical rules. As a rule-based system, time taggers are limited by the coverage of the rules for the different types of temporal expressions that it recognizes. To exemplify, the word ‘present’ in the sentence “Apple’s iPhone is one of the most popular smartphones at present” when labeled by SUTime1 is tagged as:\n<TIMEX3 tid=“t1” type=“DATE”\nvalue=“PRESENT_REF”>present</TIMEX3> It rightly tags the word ‘present’ in the above example, and refers to it as the present time when reference date is considered as same as the tagging date. However, such word based indicators can be misleading. For example, below is the tag from SUTime for the word ‘present’ in the sentence “I was in Oxford Street getting the wife her birthday present”. The tag gives us a false impression by wrongly labeling the word as a temporal one.\n<TIMEX3 tid=“t1” type=“DATE”\nvalue=“PRESENT_REF”>present</TIMEX3> Reasons for this misleading information are i) time taggers usually do not use contextual indicators while deciding on temporality ii) different word senses of a single word can actually be either temporal or atemporal. A typical temporallyambiguous word, i.e. a word that has at least one temporal and at least one atemporal sense, is ‘present’, as shown by the two examples above.\nWhereas most of the prior computational linguistics and text mining temporal studies have focused on temporal expressions and events, there has been a lack of work looking at the tempo-\n1http://nlp.stanford.edu:8080/sutime/ process\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nral orientation of word senses. Therefore, we focus our study on automatically time-tagging word senses into past, present, future, or atemporal using their WordNet (Miller, 1995) definition, instead of tagging temporal words.\nIn this paper, we put forward a semi-supervised graph-based classification paradigm build on an optimization theory namely the max-flow min-cut theorem (Papadimitriou and Steiglitz, 1998). In particular, we propose minimum cut in a connected graph to time-tag each synset of WordNet to one of the four dimensions: atemporal, past, present, and future. Our methodology was evaluated both intrinsically and extrinsically. It outperformed prior approaches to the temporality recognition of word senses/concepts. First, a gold standard is created using a crowdsourced annotation service to test our methodology. Second, temporal classification task is performed. Results show qualitative improvements when compared to previous state-of the-art approaches. Results also evidence that to achieve the performance of a standard supervised approach with our model, we need less than 10% of the training data."
    }, {
      "heading" : "2 Related Work",
      "text" : "Temporality in NLP and IR: Temporality has recently received increased attention in Natural Language Processing (NLP) and Information Retrieval (IR). Initial works proposed in NLP and IR are exhaustively summarized in (Mani et al., 2005). The introduction of the TempEval task (Verhagen et al., 2009) and subsequent challenges (TempEval-2 and -3) in the Semantic Evaluation workshop series have clearly established the importance of time to deal with different NLP tasks. In IR, the work of (Baeza-Yates, 2005) defines the foundations of Temporal-IR. Since, research have been tackling several topics such as query understanding (Metzler et al., 2009), temporal snippets generation (Alonso et al., 2007), temporal ranking (Kanhabua et al., 2011), temporal clustering (Alonso et al., 2009), or future retrieval (Radinsky and Horvitz, 2013).\nIn order to push forward further research in temporal NLP and IR, (Dias et al., 2014) developed TempoWordNet (TWn), an extension of WordNet (Miller, 1995), where each synset is augmented with its temporal connotation (past, present, future, or atemporal). It mainly relies on the quantitative analysis of the glosses associated to synsets,\nand on the use of the resulting vectorial term representations for semi-supervised synset classification. While (Hasanuzzaman et al., 2014a) show that TWn can be useful to time-tag web snippets, less comprehensive results are shown in (Filannino and Nenadic, 2014), where TWn learning features did not lead to any classification improvements. In order to propose a more reliable resource, (Hasanuzzaman et al., 2014b) recently defined two new propagation strategies: probabilistic and hybrid respectively leading to TWnP and TWnH. Although some improvements was evidenced, no conclusive remarks could be reached.\nThere are several disadvantages of their approaches. First, it relies mostly on WordNet glosses and do not effectively exploit WordNet’s relation structure. Whereas we concentrate on the use of WordNet relations, glosses, and other attributes for the classification process. Second, strategies adopted to build TWnP and TWnH mainly depend on the probability estimates for the classes from Support Vector Machines (SVM) classifiers. However, probabilities are derived without using any post-calibration method. Converting scores to accurate probability estimates for multiclass problems requires a post-calibration procedure. In addition, there is no standard evaluation as to the accuracy of their approach. Graph-based Classification: To the best of our knowledge, we present the first work, which aims to recognize temporal dimension of word senses via graph-based classification algorithm. However, graph-based algorithms have been used to classify sentences and documents into subjective/objective or positive/negative level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005), instead of aiming at tagging at word sense level as we do. At the word level, a semi-supervised spin model is used for word polarity determination, where the graph is constructed using a variety of information such as gloss co-occurrences and WordNet links (Takamura et al., 2005). However, their model differs from ours."
    }, {
      "heading" : "3 Semi-supervised Mincuts",
      "text" : ""
    }, {
      "heading" : "3.1 Minimum cuts: Main Idea",
      "text" : "Underlying idea behind classification with minimum cuts (Mincuts) in graph is that similar items should be grouped together in the same cut. Suppose we have n items x1, ......xn to divide into two classes C1 and C2 based on the two types of in-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nformation at hand. The first one, individual score ind j(xi) is the non-negative estimate of each xi’s preference for being in class C j based on the features of xi alone. While the later one, association scores assoc(xi,xk) represent a non-negative estimates of how important it is that xi and xk be in the same class. Overall idea is to maximize each item’s net score i.e. individual score for the class it is assigned to minus its individual score for the other class and penalize putting tightly associated items into different classes. It can be seen as the following optimization problem: assign the xis to C1 and C2 so as to minimize the partition cost.\n∑ x∈C1 ind2(x)+ ∑ x∈C2 ind1(x)+ ∑ xi∈C1,xk∈C2 assoc(xi,xk) (1)\nWe could represent the situation by building an undirected graph G with vertices {v1, .....,vn,s, t}; s and t are source and sink respectively. Add n edges (s,vi), each with weight ind1(xi), n edges (vi, t), each with weight ind2(xi). Finally, add (n 2\n) edges (vi,vk), each with weight assoc(xi,xk). Finally, cuts in G are defined as follows:\nDefinition 1. A cut (S,T ) of G is a partition of its nodes into sets S= {s}∪ S′ and T = {t}∪ T ′ where s /∈ S′, t /∈ T ′. Its cost cost(S,T ) is the sum of the weights of all edges crossing from S to T . A minimum cut of G is one of minimum cost.\nFigure 1 illustrates an example of the concepts for classifying three items. Brackets enclose example values; here, the individual scores happen to be probabilities. Based on individual scores alone, we would put Y (“Promise.\") in s (Temporal class), N (“Chair\") in t (Atemporal class), and be undecided about M (\"Oath\"). But the association scores favour cuts that put Y and M in the same class, as shown in the table. Thus, the minimum cut, indicated by the dashed red line, places M together Y in s."
    }, {
      "heading" : "3.2 Advantages",
      "text" : "Formulating the task of temporality detection problem on word senses in terms of graphs allows us to model item-specific and pair-wise information independently. Therefore, it is a very flexible paradigm where we have two different views on the data. For example, rule based approaches or machine learning algorithms employing linguistic and other features representing temporal indicators can be used to derive individual scores for a particular item in isolation. The\nedges weighted by the individual scores of a vertex (=word sense) to the source/sink can be interpretative as the probability of a word sense being temporal or atemporal without taking similarity to other senses into account. And we could also simultaneously use conceptual-semantic and lexical relations from WordNet to derive the association scores. The edges between two items weighted by the association scores can indicate how similar/different two senses are. If two senses are connected via a temporality-preserving relation they are likely to be both temporal or in opposite atemporal. An example here is the hyponymy relation, a temporality-preserving relation (Dias et al., 2014), where two hyponymy such as present, nowadays—the period of time that is happening now and now—the momentary present are both temporal. To detect the temporal orientation of word senses, authors in (Dias et al., 2014) adopted a single view instead of two on the data. The ability to combine two views on the data is precisely one of the strengths of our approach.\nSecond, Mincuts can be easily expanded into a semi-supervised framework. This is essential as the existing labeled datasets for our problem are small. In addition, glosses are short, leading to sparse high dimensional vectors in standard feature representations. Also, WordNet connections between different parts of the WordNet hierarchy can also be sparse, leading to relatively isolated senses in a graph in a supervised framework.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nSemi-supervised Mincuts allow us to import unlabeled data that can serve as bridges to isolated components. More importantly, the unlabeled data can be related to the labeled data (by some WordNet relation), it might help pull unlabeled data to the right cuts (categories)."
    }, {
      "heading" : "3.3 Formulation of Semi-Supervised Mincuts",
      "text" : "Formulation of our semi-supervised Mincut for synset temporality classification involves the following steps.\nI We depute two vertices s (source) and t (sink) which corresponds to the “temporal” and “atemporal” category respectively. We call the vertices s and t classification vertices, and all other vertices (labeled, test, and unlabeled data) example vertices. Each example vertex corresponds to one WordNet synset and is connected to both s and t through an undirected weighted edge. This confirms that the graph is connected.\nII The labeled training examples are connected to classification vertices they belong to via edges with high constant non-negative weight. Unlabeled examples are connected to the classification vertices via edges weighted with non-negative scores that indicate the degree of association with temporal/atemporal category. We use a classifier to assign these edges weights.\nIII Conceptual-semantic and lexical relations available in the WordNet are used to construct edges between two example vertices. Such edges can exist between any pair of example vertices, for instance between two unlabeled vertices.\nIV After building graph, we then employ a maximum-flow algorithm to find the minimum s− t cuts of the graph. The cut in which the source vertex s lies is classified as “temporal” and the cut in which sink vertex t lies is labeled as “atemporal”.\nV In order to fine tune the temporal part, we follow hierarchical strategy by organizing the classes (past, present, future) according to a hierarchy. The hierarchy of classes is decided based on the classes that are easier to discriminate to improve the overall classification accuracy. First, we define two vertices s (source) and t (sink) which correspond to the\n“past” and “Not_Past” temporal categories respectively. Then we follow the above steps I through IV . This divides the temporal part into two disjoint subsets: past synsets and synsets belong to present and future temporal category. Finally, we repeat steps I through IV where vertices s (source) and t (sink) corresponds to “future” and “present” category respectively.\nLabeled and unlabeled data selection: We use the same temporal (past, present, future) and atemporal sets of synsets considered as training data at the time of building TempoWordNet (TWnL) (Dias et al., 2014) as training /labeled data for our experiments. For test set, sample of synsets outside the labeled data is selected and annotated using crowdsourcing service. All other synsets outside labeled and test set are considered as unlabeled data. Weighting of edges to the classification vertices: The edge weight (non-negative) to the source s and the sink t denotes how likely it is that an example vertex is put in the cut in which s (temporal) or t (atemporal) lies. For the unlabeled and test examples, a supervised learning strategy (over the labeled data as training set) is used to assign the edge weights. Each synset is represented by its gloss encoded as a vector of word unigrams weighted by their frequency in the gloss. As for classifier, we used SVM from the Weka platform2. In order to ensure that Mincut does not reverse the labels of the labeled training data, we assign a high3 constant non-negative weight of 3 to the edge between a labeled vertex and its corresponding classification vertex, and a low weight of 0.001 to the edge to the other classification vertex. Deriving weights for WordNet relations: While formulating the graph, we connect two vertices by an edge if they are linked by one of the ten (10) WordNet relations in Table 1. Main motivation towards using other relations in addition to the most frequently encoded relations (hypernym, hyponym) among synsets in WordNet is to achieve high graph connectivity. Moreover, we can assign different weights to different relations to reflect the degree to which they are temporality preserving. Therefore, we adopt two strategies to assign\n2http://www.cs.waikato.ac.nz/ml/weka/ [Last access: 12/04/2015].\n3w.r.t. the probability estimates (after calibration) of the classes from SVM.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nweights to different WordNet relations. The first method (ScWt), assigns the same constant weight of 1.0 to all WordNet relations.\nThe second method (DiffWt), considers several degrees of preserving temporality. In order to do this, we adopt a simple strategy to produce large noisy set of temporal 4 and atemporal synsets from WordNet. This method uses a list of 30 handcrafted temporal seeds (equally distributed over past, present, and future temporal categories) proposed in (Dias et al., 2014) along with their direct hyponym5 to classify each WordNet synset with at least one temporal word6 in its gloss as temporal and all other synsets as atemporal. We then simply count how often two synsets connected by a given relation (edge) have the same or different temporal dimension. Finally, weight is calculated by #same/(#same+#different). Results are reported in Table 1.\nWordnet Relation #same #different Weight Direct-Hypernym 61914 9600 0.76 Direct-Hyponym 73268 7246 0.91\nAntonym 1905 3614 0.35 Similar-to 6587 1914 0.77 Derived-from 3630 1947 0.65 Also-see 1037 337 0.75 Attribute 350 109 0.76\nTroponym 6917 2651 0.72 Domain 2380 2895 0.45 Domain-member 2380 2895 0.45\nTable 1: WordNet relation weights (DiffWt Method)"
    }, {
      "heading" : "4 Experiments and Evaluation",
      "text" : ""
    }, {
      "heading" : "4.1 Datasets",
      "text" : "Labeled Data: We used a list that consists of 632 temporal synsets in WordNet and equal number of atemporal synsets provided by (Dias et al., 2014) as labeled data for our experiments. Temporal synsets are distributed as follows: 210 synsets marked as past, 291 as present, and 131 as future. Building of a Gold Standard: Since to our best knowledge, there is no gold standard resource with temporal association of words (except 30 handcrafted temporal seeds proposed by Dias et al., 2014), we designed our own annotation task using the crowdsourcing service of CrowdFlower platform7. For the annotation task, three hundred\n4To fine tune the temporal part we used the same strategy for tagging past, present, and future synsets following hierarchical strategy\n5Relation which preserves temporality according to (Dias et al., 2014)\n6Most frequent sense of the temporal word from WordNet is selected\n7http://www.crowdflower.com/\nninety eight (398) synsets equally distributed over nouns, verbs, adjectives and adverbs POS categories along with their lemmas and glosses are selected randomly from WordNet 8 as representative of the whole WordNet. Note that this number of synset is statistically significant representative sample of total WordNet synsets (117000 plus synsets) and derived using the formula described in (Israel, 1992). Afterwards, we designed two question that the annotators were expected to answer for a given synset (lemmas and gloss are also provided). While the first question is related to the decision which reflects a synset being temporal or atemporal, the motivation behind the second question is to collect a more fine-grained (past, present, future) gold-standard for synset-temporality association. Details of annotation guideline is out of scope of this paper.\nThe reliability of the annotators was evaluated on the basis of 60 control synsets 9 provided by (Dias et al., 2014) which were clearly associated either with a specific temporal or atemporal dimension and 10 temporally ambiguous synsets associated with more than one temporal dimensions. Similar to (Tekiroğlu et al., ), the raters who scored at least 70% accuracy on average on both sets of control synsets were considered to be reliable. Each unit was annotated by at least 10 reliable raters.\nTable 2 demonstrates the observed agreement. Similar to (Mohammad, 2011; Özbal et al., 2011), annotations with a majority class greater than 5 is considered as reliable. Indeed for temporal vs atemporal) classification, 84.83 % of the synset annotations the absolute majority agreed on the same decision, while for past, present, and future, 72.36% of the annotations have majority class greater than 5. The high agreement confirms the quality of the resulting gold standard data."
    }, {
      "heading" : "4.2 Semi-supervised Graph Mincuts",
      "text" : "Temporal Vs Atemporal Classification: Using our formulation in Section 3.3, we construct a connected graph by importing 1264 training set (632 temporal and 632 atemporal synsets), 398 gold standard test set created using a crowdsourced service, and 115996 unlabeled synsets10. We con-\n8WordNet version 3.0 used and selected outside from the labeled data set\n930 temporal synsets (equally distributed over past, present, future) and 30 atemporal synsets\n10All synset of WordNet−(training set+test set)\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nMajority Class 3 4 5 6 7 8 9 10 Synset as temporal or atemporal 0 .20 1.21 4.32 10.69 14.56 29.34 19.23 11.01\nTemporal synset into past, present, or future 1.23 3.01 10.45 20.22 16.56 12.34 14.23 9.01\nTable 2: Percentage of synsets in each majority class.\nstruct edge weights to classification vertices, s (temporal) and t (atemporal) by using a SVM classifier discussed above. WordNet relations for links between example vertices are weighted either by non-negative constant value (ScWt) or by the method ‘DiffWt’ illustrated in Table 1. Temporally tagged synsets into past, present, and future: In order to fine tune the temporal part, we construct another connected graph by importing 632 labeled temporal synsets, temporal part of the gold standard test data (127 synsets out of 398 synsets), temporally tagged synsets as unlabeled data 11 and follow the same strategy presented in Section 3.3 where classification vertices, s and t correspond to ‘past’ and ‘Not_past’ temporal categories respectively. Finally, temporal synsets tagged as ‘Not_past’ classified either as present or future following the same analogy."
    }, {
      "heading" : "4.3 Evaluation",
      "text" : "The underlying idea being that a reliable resource must evidence high quality time-tagging as well as improved performance for some external tasks."
    }, {
      "heading" : "4.4 Intrinsic Evaluation",
      "text" : "Baseline: In order to compare our semisupervised Mincut approach to a reasonable baseline, we use rule based approach to classify goldstandard data into past, present, future, or atemporal based on its lemmas and glosses. First, time expressions in the glosses of synsets are labeled and resolved via Standford’s SUTime tagger, which give accuracy in line with the stateof-the-art systems at identifying time expressions at TempEval. For each synset, Named-entity time tag provided by SUTime (e.g.“future_ref” or “present_ref” etc.) for the time expression present in its gloss is considered as the temporal class for that particular synset. In case of more than one temporal expression present12 in the gloss of a synset, majority class of the time tags is se-\n11Total number of synsets classified as temporal − (Total number of temporal synsets in training data+Total number of gold standard temporal synsets tagged as temporal at the time of temporal vs atemporal classification process)\n12Found very rarely < 1%\nlected. Secondly, if no time expression is identified by the time tagger, a list composed of 30 handcrafted temporal seeds proposed in (Dias et al., 2014) along with their direct hyponym and standard temporal adverbials (since), prepositions (before/after), adjectives (former) etc. is used to classify synsets with at least one temporal word13 in its lemma(s) and gloss as temporal (past, present, future) and all other synsets as atemporal. Finally, performance of the rule based approach is measured for the gold standard data set and presented in Table 3. To figure out the contribution of word sense disambiguation, classic Lesk algorithm (Lesk, 1986) is used to choose right sense/synset for a word instead of most frequent sense. We found that contribution is negligible (< 0.4% improvement in overall accuracy).\nTo strengthen comparative evaluation of our semi-supervised Mincut approach, we propose to test our methodology with prior works (TempoWordNet: TWnL, TWnP, and TWnH). Comparative evaluation results are presented in Table 3. Results show the Mincut approach (CFG2) outperforms state-of-the-art approaches. It achieves highest accuracies for both temporal vs. atemporal and past, present, future classification with improvement of 11.3% and 10.3% respectively over the second best TempoWordNet versions (TWnH). Considering the above findings, we select our best Mincut configuration CFG2 for the remaining experiments. Distribution of time-tag synsets produced by this configuration: atemporal=110002, past=1733, present=4193, future=1730. Some examples are given below: • late–having died recently. (Past) • present, nowadays–the period of time that is\nhappening now. (Present) • promise–a verbal commitment by one person to\nanother. (Future) • field–a piece of land cleared of trees and usually\nenclosed. (Atemporal) Performance with different size training data: We randomly generate subsets of labeled\n13Most frequent sense of the temporal word from WordNet is selected\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nTable 3: Accuracy (percentage classified correct) for temporal vs. atemporal and temporal into past, present, future classification using different methods namely TempoWordNet (TWnL, TWnP, TWnH) and Mincuts measured over created gold-standard data. CFG1 corresponds to the Mincut that uses SVM classifier to infer edges weights of unlabeled and test examples to the classification vertices s and t and predefined constant weights for WordNet relations (ScWt). CFG2 corresponds to the Mincut approach that uses the same SVM classifier to infer edges weights but uses a list of temporal synsets to infer weights of Wordnet relations (DiffWt). Both of our configurations perform significantly better than previous approaches. Results are also broken down by precision (p), recall (r), and f1 score for past, present, future, and atemporal categories.\nMethod Baseline TWnL TWnP TWnH CFG1 CFG2 Accuracy 48.8 65.6 62.0 68.4 74.4 79.7\ntemporal (p, r, f1) (52.0, 56.3, 54.0) (63.5, 82.1, 71.6) (55.8, 84.2, 67.1) (67.4, 81.9, 73.9) (84.5, 79.8, 82.0) (89.1, 79.3, 83.9) atemporal (p, r, f1) (58.2, 54.2, 56.1) (68.3, 79.2, 73.3) (58.9, 75.6, 66.2) (69.3, 82.6, 75.3) (81.3, 86.6, 83.8 ) (87.4, 90.8, 89.1)\nAccuracy 45.6 62.0 59.6 65.7 72.7 76.0 past (p, r, f1) (49.3, 46.7, 47.9) (61.2, 73.0, 66.5) (59.3, 79.1, 67.7) (63.1, 75.0, 68.0) (71.1, 79.5, 75.0) (81.2, 78.5, 79.8) present (p, r, f1) (55.3, 48.2, 51.5) (63.0, 75.2, 68.5) (58.0, 78.2 66.0 ) (77.4, 69.2, 73.0) (73.0, 71.5, 72.2) (85.1, 74.7, 79.0) future (p, r, f1) (48.5, 49.0, 48.7) (62.1, 71.9, 66.6) (57.0, 83.1, 67.6) (60.0, 75.6, 66.8) (79.4, 69.5, 74.0) (86.1, 70.0, 77.2)\ndata/training data (1064 synsets: 632 temporal and 632 atemporal) L1,L2,L3......Ln, and ensures that L1 ⊂ L2 ⊂ L3...... ⊂ Ln. As proposed in (Dias et al., 2014), binary classification models based on the generated subsets of labeled data are learned. Using the same subsets of labeled data, we formulate our best performing minimum cut (CFG2). Accuracies of both approaches are presented in Table 4. As can be seen from the table, semi-supervised Mincut performs consistently better than the previous semi-supervised non-graph classification approach (SVM). Moreover, our proposed graph classification framework with only 400 labeled data/training data examples achieves even higher accuracies than SVM with 1264 training items (73.7% vs 68.4% ).\nNumber of labeled data SVM (TWnH) Minncut (CFG2) 100 59.8 64.3 200 62.6 67.5 400 65.5 73.7 600 67.4 77.6 800 67.9 79.2 1000 68.0 79.0\n1264 (all) 68.4 79.7\nTable 4: Accuracy with different sizes of label data for temporal vs. atemporal classification."
    }, {
      "heading" : "4.5 Extrinsic Evaluation",
      "text" : "For extrinsic evaluation, we focus on the problem of classifying temporal relations task of TempEval-3, assuming that identification of events, times are already performed. The underlying idea is that proposed method to yield a timeenhanced WordNet has a greater positive impact on more applied temporal information extraction tasks, whose output is useful for many information retrieval applications.\nIn order to produce comparative results with best performing system at TempEval-3 namely\nUTTime (Laokulrat et al., 2013) for the above task, we follow the guidelines and use the same data sets provided by the evaluation campaign organizers. We restrict14 our experiment to a subset of relations namely BEFORE (CORR. Past), AFTER (CORR. Future), and INCLUDES (CORR. Present) with all other relation mapped to the ‘NA-RELATION’ for event to document creation time and event to same sentence event. For the task, we adopt a very simple strategy and implement the following features. • String features: The tokens and lemmas of each\nentity pair. • Grammatical features: The part-of-speech\n(PoS) tags of the entity pair (only for eventevent pairs). The grammatical information is obtained using the Standford CoreNLPtool15. • Entity attributes: Entity pair attributes as pro-\nvided in the data set. • Dependency relation: We used the information\nrelated to the dependency relation between two entities such as type of dependency, dependency order. • Textual context: The textual order of entity pair. • Lexica: The relative frequency of temporal cat-\negories, based on the resource developed in this research, in the text appearing between entity pair (event to same sentence event), text of all tokens in the time expression, 5 tokens following and preceding time expression/event. The features are encoded as the frequency with which a word from a temporal category (past, present, future) appeared in the text divided by the total number of tokens in the text.\n14Because of the complexity to map 14 relations of TempEval-3 into three temporal classes (past, present, future) considered for this experiment\n15http://stanfordnlp.github.io/CoreNLP/\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nWe build our system namely T RelMincuts (composed of two classifiers) using Support Vector Machine (SVM) implementation of Weka over the features set and training data provided by the organizers. The best classifier for event to document creation time and event to same sentence event relation are selected via a grid search over parameter settings. The grid is evaluated with a 5- fold cross validation on the training data. We also measure the performance of UTTime for the above stated settings. Additionally, we build T RelTWnH by adopting same strategy and features set ‘Lexica’ is computed from prior time-tagging approach namely TWnH. Table 5 presents comparative evaluation results.\nApproaches Precision(%) Recall(%) F1(%) UT Time 57.5 58.7 58.1 T RelMincuts 66.9 68.7 67.7 T RelTWnH 61.2 62.5 61.8\nTable 5: Performance of different approaches on temporal relation classification based on TempEval-3 Evaluation strategy.\nResults evidence that T RelMincuts significantly outperforms all other approaches and achieve highest performance in terms of precision (+5.7), recall (+6.2), and F1 score (+5.9). Results also demonstrate that our approach achieves 9.6% improvement in terms of F1 score over the best performing system in TempEval-3.\nWe perform feature ablation analyses as presented in Table 6 in order to measure features contribution. As can be seen, every features set produced improvement over baseline (mfc) and highest improvement is achieved with features set (Lexica) that comes from the proposed temporal lexical resource. The result also implies that while each feature type contains certain temporal information, there is also some redundancy across the feature types.\nFeatures F1(%) Features F1(%) mfc baseline 33.55 all features 67.7 string alone 45.06 w/o string 65.70 grammatical alone 46.96 w/o grammatical 64.85 entity alone 52.23 w/o entity 62.08 dependency alone 48.65 w/o dependency 65.06 textual alone 46.82 w/o textual 64.96 lexica alone 51.62 w/o lexica 62.76\nTable 6: Feature ablation analysis of F1 score. The most frequent class baseline (mfc) indicates accuracy if only predicting the present class."
    }, {
      "heading" : "5 Discussion",
      "text" : "One important remark can be made related to the difficulty of the task. This is particularly due to the fact that the temporal dimension of synsets is mainly judged upon their definition. For example, “dinosaur” can be classified as temporal or atemporal as its gloss “any of numerous extinct terrestrial reptiles of the Mesozoic era” allows both interpretations. Apart from it, while digging into the results we observed that classifying the temporal synsets into past, present, or future is more difficult than temporal vs. atemporal classification. It is due to the fact that past, present and future connotations are only indicative of the temporal orientation of the synset but cannot be taken as a strict class. Indeed, there are many temporal synsets, which are neither past, present nor future (e.g.monthly – a periodical that is published every month)."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we proposed a semi-supervised minimum cut framework to address the relatively unexplored problem of associating word senses with their underlying temporal dimensions. The basic idea is that instead of using single view, multiple views on the data would result in better temporal classification accuracy thus lead to a accurate and reliable temporal lexical resource. Thorough and comparative evaluations are performed to measure the quality of the resource. The results confirm the soundness of the proposed approach and the usefulness of the resource for temporal relation classification task. The resource is publicly available on https://www.anonymous.anonymous so that the community can benefit from it for relevant tasks and applications.\nFrom a resource point of view, we would like to explore the effect of other graph construction methods, such as the use of freely available online dictionaries including thesaurus and distributional similarity measures. We would also like to use the resource for various applicative scenarios such as automatic analysis of time-oriented clinical narratives. As an example, one can imagine a system that automatically analyzes medical discharge summaries including previous diseases related to the current conditions, treatments, and the family history for medical decision making, data modeling and biomedical research.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Sentiment analysis: A new approach for effective use of linguistic knowledge and exploiting similarities in a set of documents to be classified",
      "author" : [ "Alekh Agarwal", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of the International Conference on Natural",
      "citeRegEx" : "Agarwal and Bhattacharyya.,? 2005",
      "shortCiteRegEx" : "Agarwal and Bhattacharyya.",
      "year" : 2005
    }, {
      "title" : "Exploratory search using timelines",
      "author" : [ "O. Alonso", "R. Baeza-Yates", "M. Gertz." ],
      "venue" : "Proceedings of the ACM SIGCHI Workshop on Exploratory Search and HCI.",
      "citeRegEx" : "Alonso et al\\.,? 2007",
      "shortCiteRegEx" : "Alonso et al\\.",
      "year" : 2007
    }, {
      "title" : "Clustering and exploring search results using timeline constructions",
      "author" : [ "O. Alonso", "M. Gertz", "R. Baeza-Yates." ],
      "venue" : "Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM), pages 97–106. ACM.",
      "citeRegEx" : "Alonso et al\\.,? 2009",
      "shortCiteRegEx" : "Alonso et al\\.",
      "year" : 2009
    }, {
      "title" : "Searching the future",
      "author" : [ "Ricardo Baeza-Yates." ],
      "venue" : "Proceedings of the ACM SIGIR Workshop on Mathematical/Formal Methods in Information Retrieval, pages 1–6.",
      "citeRegEx" : "Baeza.Yates.,? 2005",
      "shortCiteRegEx" : "Baeza.Yates.",
      "year" : 2005
    }, {
      "title" : "Tempowordnet for sentence time tagging",
      "author" : [ "Gaël Dias", "Mohammed Hasanuzzaman", "Stéphane Ferrari", "Yann Mathet." ],
      "venue" : "Companion Publication of the 23rd International Conference on World Wide Web Companion (WWW), pages 833–838.",
      "citeRegEx" : "Dias et al\\.,? 2014",
      "shortCiteRegEx" : "Dias et al\\.",
      "year" : 2014
    }, {
      "title" : "Using machine learning to predict temporal orientation of search engines’ queries in the temporalia challenge",
      "author" : [ "Michele Filannino", "Goran Nenadic." ],
      "venue" : "NTCIR-11 Conference (NTCIR), pages 438–442.",
      "citeRegEx" : "Filannino and Nenadic.,? 2014",
      "shortCiteRegEx" : "Filannino and Nenadic.",
      "year" : 2014
    }, {
      "title" : "Hultech at the ntcir-11 temporalia task: Ensemble learning for temporal query intent classification",
      "author" : [ "Mohammed Hasanuzzaman", "Gaël Dias", "Stéphane Ferrari." ],
      "venue" : "NTCIR-11 Conference (NTCIR), pages 478–482.",
      "citeRegEx" : "Hasanuzzaman et al\\.,? 2014a",
      "shortCiteRegEx" : "Hasanuzzaman et al\\.",
      "year" : 2014
    }, {
      "title" : "Propagation strategies for building temporal ontologies",
      "author" : [ "Mohammed Hasanuzzaman", "Gaël Dias", "Stéphane Ferrari", "Yann Mathet." ],
      "venue" : "14th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages 6–11.",
      "citeRegEx" : "Hasanuzzaman et al\\.,? 2014b",
      "shortCiteRegEx" : "Hasanuzzaman et al\\.",
      "year" : 2014
    }, {
      "title" : "Determining sample size",
      "author" : [ "Glenn D Israel." ],
      "venue" : "University of Florida Cooperative Extension Service, Institute of Food and Agriculture Sciences, EDIS.",
      "citeRegEx" : "Israel.,? 1992",
      "shortCiteRegEx" : "Israel.",
      "year" : 1992
    }, {
      "title" : "Ranking related news predictions",
      "author" : [ "N. Kanhabua", "R. Blanco", "M. Matthews." ],
      "venue" : "Proceedings of the 34th International ACM Conference on Research and Development in Information Retrieval (SIGIR), pages 755–764.",
      "citeRegEx" : "Kanhabua et al\\.,? 2011",
      "shortCiteRegEx" : "Kanhabua et al\\.",
      "year" : 2011
    }, {
      "title" : "Uttime: Temporal relation classification using deep syntactic features",
      "author" : [ "Natsuda Laokulrat", "Makoto Miwa", "Yoshimasa Tsuruoka", "Takashi Chikayama." ],
      "venue" : "Second Joint Conference on Lexical and Computational Semantics (* SEM), volume 2,",
      "citeRegEx" : "Laokulrat et al\\.,? 2013",
      "shortCiteRegEx" : "Laokulrat et al\\.",
      "year" : 2013
    }, {
      "title" : "Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone",
      "author" : [ "Michael Lesk." ],
      "venue" : "Proceedings of the 5th annual international conference on Systems documentation, pages 24–26. ACM.",
      "citeRegEx" : "Lesk.,? 1986",
      "shortCiteRegEx" : "Lesk.",
      "year" : 1986
    }, {
      "title" : "The language of time: a reader, volume 126",
      "author" : [ "Inderjeet Mani", "James Pustejovsky", "Robert Gaizauskas." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Mani et al\\.,? 2005",
      "shortCiteRegEx" : "Mani et al\\.",
      "year" : 2005
    }, {
      "title" : "Improving search relevance for implicitly temporal queries",
      "author" : [ "D. Metzler", "R. Jones", "F. Peng", "R. Zhang." ],
      "venue" : "Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 700–",
      "citeRegEx" : "Metzler et al\\.,? 2009",
      "shortCiteRegEx" : "Metzler et al\\.",
      "year" : 2009
    }, {
      "title" : "Wordnet: a lexical database for english",
      "author" : [ "Georges A. Miller." ],
      "venue" : "Communications of the ACM, 38(11):39–41.",
      "citeRegEx" : "Miller.,? 1995",
      "shortCiteRegEx" : "Miller.",
      "year" : 1995
    }, {
      "title" : "Colourful language: Measuring word-colour associations",
      "author" : [ "Saif Mohammad." ],
      "venue" : "Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 97–106. Association for Computational Linguistics.",
      "citeRegEx" : "Mohammad.,? 2011",
      "shortCiteRegEx" : "Mohammad.",
      "year" : 2011
    }, {
      "title" : "A comparison of unsupervised methods to associate colors with words",
      "author" : [ "Gözde Özbal", "Carlo Strapparava", "Rada Mihalcea", "Daniele Pighin." ],
      "venue" : "Affective Computing and Intelligent Interaction, pages 42–51. Springer.",
      "citeRegEx" : "Özbal et al\\.,? 2011",
      "shortCiteRegEx" : "Özbal et al\\.",
      "year" : 2011
    }, {
      "title" : "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Com-",
      "citeRegEx" : "Pang and Lee.,? 2004",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2004
    }, {
      "title" : "Combinatorial optimization: algorithms and complexity",
      "author" : [ "Christos H Papadimitriou", "Kenneth Steiglitz." ],
      "venue" : "Courier Corporation.",
      "citeRegEx" : "Papadimitriou and Steiglitz.,? 1998",
      "shortCiteRegEx" : "Papadimitriou and Steiglitz.",
      "year" : 1998
    }, {
      "title" : "Mining the web to predict future events",
      "author" : [ "K. Radinsky", "E. Horvitz." ],
      "venue" : "Proceedings of the 6th ACM International Conference on Web Search and Data Mining (WSDM), pages 255–264.",
      "citeRegEx" : "Radinsky and Horvitz.,? 2013",
      "shortCiteRegEx" : "Radinsky and Horvitz.",
      "year" : 2013
    }, {
      "title" : "Multilingual and cross-domain temporal tagging",
      "author" : [ "J. Strötgen", "M. Gertz." ],
      "venue" : "Language Resources and Evaluation (LRE), 47(2):269–298.",
      "citeRegEx" : "Strötgen and Gertz.,? 2013",
      "shortCiteRegEx" : "Strötgen and Gertz.",
      "year" : 2013
    }, {
      "title" : "Extracting semantic orientations of words using spin model",
      "author" : [ "Hiroya Takamura", "Takashi Inui", "Manabu Okumura." ],
      "venue" : "Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 133–140. Association for Computational",
      "citeRegEx" : "Takamura et al\\.,? 2005",
      "shortCiteRegEx" : "Takamura et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Automatic identification of temporal expressions in text is usually performed either via time taggers (Strötgen and Gertz, 2013), which contain pattern files, such as uni-grams and bi-grams used to express temporal expressions in a given language (e.",
      "startOffset" : 102,
      "endOffset" : 128
    }, {
      "referenceID" : 14,
      "context" : "Therefore, we focus our study on automatically time-tagging word senses into past, present, future, or atemporal using their WordNet (Miller, 1995) definition, instead of tagging temporal words.",
      "startOffset" : 133,
      "endOffset" : 147
    }, {
      "referenceID" : 18,
      "context" : "In this paper, we put forward a semi-supervised graph-based classification paradigm build on an optimization theory namely the max-flow min-cut theorem (Papadimitriou and Steiglitz, 1998).",
      "startOffset" : 152,
      "endOffset" : 187
    }, {
      "referenceID" : 12,
      "context" : "Initial works proposed in NLP and IR are exhaustively summarized in (Mani et al., 2005).",
      "startOffset" : 68,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "In IR, the work of (Baeza-Yates, 2005) defines the foundations of Temporal-IR.",
      "startOffset" : 19,
      "endOffset" : 38
    }, {
      "referenceID" : 13,
      "context" : "Since, research have been tackling several topics such as query understanding (Metzler et al., 2009), temporal snippets generation (Alonso et al.",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 1,
      "context" : ", 2009), temporal snippets generation (Alonso et al., 2007), temporal ranking (Kanhabua et al.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : ", 2007), temporal ranking (Kanhabua et al., 2011), temporal clustering (Alonso et al.",
      "startOffset" : 26,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : ", 2011), temporal clustering (Alonso et al., 2009), or future retrieval (Radinsky and Horvitz, 2013).",
      "startOffset" : 29,
      "endOffset" : 50
    }, {
      "referenceID" : 19,
      "context" : ", 2009), or future retrieval (Radinsky and Horvitz, 2013).",
      "startOffset" : 29,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "In order to push forward further research in temporal NLP and IR, (Dias et al., 2014) developed TempoWordNet (TWn), an extension of WordNet (Miller, 1995), where each synset is augmented with its temporal connotation (past, present, future, or atemporal).",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 14,
      "context" : ", 2014) developed TempoWordNet (TWn), an extension of WordNet (Miller, 1995), where each synset is augmented with its temporal connotation (past, present, future, or atemporal).",
      "startOffset" : 62,
      "endOffset" : 76
    }, {
      "referenceID" : 6,
      "context" : "While (Hasanuzzaman et al., 2014a) show that TWn can be useful to time-tag web snippets, less comprehensive results are shown in (Filannino and Nenadic, 2014), where TWn learning features did not lead to any classification improvements.",
      "startOffset" : 6,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : ", 2014a) show that TWn can be useful to time-tag web snippets, less comprehensive results are shown in (Filannino and Nenadic, 2014), where TWn learning features did not lead to any classification improvements.",
      "startOffset" : 103,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "In order to propose a more reliable resource, (Hasanuzzaman et al., 2014b) recently defined two new propagation strategies: probabilistic and hybrid respectively leading to TWnP and TWnH.",
      "startOffset" : 46,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : "However, graph-based algorithms have been used to classify sentences and documents into subjective/objective or positive/negative level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005), instead of aiming at tagging at word sense level as we do.",
      "startOffset" : 136,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "However, graph-based algorithms have been used to classify sentences and documents into subjective/objective or positive/negative level (Pang and Lee, 2004; Agarwal and Bhattacharyya, 2005), instead of aiming at tagging at word sense level as we do.",
      "startOffset" : 136,
      "endOffset" : 189
    }, {
      "referenceID" : 21,
      "context" : "At the word level, a semi-supervised spin model is used for word polarity determination, where the graph is constructed using a variety of information such as gloss co-occurrences and WordNet links (Takamura et al., 2005).",
      "startOffset" : 198,
      "endOffset" : 221
    }, {
      "referenceID" : 4,
      "context" : "An example here is the hyponymy relation, a temporality-preserving relation (Dias et al., 2014), where two hyponymy such as present, nowadays—the period of time that is happening now and now—the momentary present are both temporal.",
      "startOffset" : 76,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "To detect the temporal orientation of word senses, authors in (Dias et al., 2014) adopted a single view instead of two on the data.",
      "startOffset" : 62,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "Labeled and unlabeled data selection: We use the same temporal (past, present, future) and atemporal sets of synsets considered as training data at the time of building TempoWordNet (TWnL) (Dias et al., 2014) as training /labeled data for our experiments.",
      "startOffset" : 189,
      "endOffset" : 208
    }, {
      "referenceID" : 4,
      "context" : "This method uses a list of 30 handcrafted temporal seeds (equally distributed over past, present, and future temporal categories) proposed in (Dias et al., 2014) along with their direct hyponym5 to classify each WordNet synset with at least one temporal word6 in its gloss as temporal and all other synsets as atemporal.",
      "startOffset" : 142,
      "endOffset" : 161
    }, {
      "referenceID" : 4,
      "context" : "1 Datasets Labeled Data: We used a list that consists of 632 temporal synsets in WordNet and equal number of atemporal synsets provided by (Dias et al., 2014) as labeled data for our experiments.",
      "startOffset" : 139,
      "endOffset" : 158
    }, {
      "referenceID" : 4,
      "context" : "For the annotation task, three hundred 4To fine tune the temporal part we used the same strategy for tagging past, present, and future synsets following hierarchical strategy 5Relation which preserves temporality according to (Dias et al., 2014) 6Most frequent sense of the temporal word from WordNet is selected 7http://www.",
      "startOffset" : 226,
      "endOffset" : 245
    }, {
      "referenceID" : 8,
      "context" : "Note that this number of synset is statistically significant representative sample of total WordNet synsets (117000 plus synsets) and derived using the formula described in (Israel, 1992).",
      "startOffset" : 173,
      "endOffset" : 187
    }, {
      "referenceID" : 4,
      "context" : "The reliability of the annotators was evaluated on the basis of 60 control synsets 9 provided by (Dias et al., 2014) which were clearly associated either with a specific temporal or atemporal dimension and 10 temporally ambiguous synsets associated with more than one temporal dimensions.",
      "startOffset" : 97,
      "endOffset" : 116
    }, {
      "referenceID" : 15,
      "context" : "Similar to (Mohammad, 2011; Özbal et al., 2011), annotations with a majority class greater than 5 is considered as reliable.",
      "startOffset" : 11,
      "endOffset" : 47
    }, {
      "referenceID" : 16,
      "context" : "Similar to (Mohammad, 2011; Özbal et al., 2011), annotations with a majority class greater than 5 is considered as reliable.",
      "startOffset" : 11,
      "endOffset" : 47
    }, {
      "referenceID" : 4,
      "context" : "Secondly, if no time expression is identified by the time tagger, a list composed of 30 handcrafted temporal seeds proposed in (Dias et al., 2014) along with their direct hyponym and standard temporal adverbials (since), prepositions (before/after), adjectives (former) etc.",
      "startOffset" : 127,
      "endOffset" : 146
    }, {
      "referenceID" : 11,
      "context" : "To figure out the contribution of word sense disambiguation, classic Lesk algorithm (Lesk, 1986) is used to choose right sense/synset for a word instead of most frequent sense.",
      "startOffset" : 84,
      "endOffset" : 96
    }, {
      "referenceID" : 4,
      "context" : "As proposed in (Dias et al., 2014), binary classification models based on the generated subsets of labeled data are learned.",
      "startOffset" : 15,
      "endOffset" : 34
    }, {
      "referenceID" : 10,
      "context" : "In order to produce comparative results with best performing system at TempEval-3 namely UTTime (Laokulrat et al., 2013) for the above task, we follow the guidelines and use the same data sets provided by the evaluation campaign organizers.",
      "startOffset" : 96,
      "endOffset" : 120
    } ],
    "year" : 2016,
    "abstractText" : "The ability to capture the time information conveyed in natural language is essential to many natural language processing applications such as information retrieval, question answering, automatic summarization, targeted marketing, loan repayment forecasting, and understanding economic patterns. Therefore, a lexical temporal resource associating word senses with their underlying temporal orientation would be crucial for the computational tasks aiming at interpretation of language of time in text. In this paper, we propose a semisupervised minimum cuts paradigm that makes use of WordNet definitions or ‘glosses’, its conceptual-semantic and lexical relations to supplement WordNet entries with information on the temporality of its word senses. Intrinsic and extrinsic evaluation results show that the proposed approach outperforms prior semisupervised, non-graph classification approaches to the temporality recognition of word senses/concepts, and confirm the soundness of the proposed approach.",
    "creator" : "TeX"
  }
}