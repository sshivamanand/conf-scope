{
  "name" : "86.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "App2Check and Tweet2Check: machine learning-based tools for Sentiment Analysis of Apps Reviews and Tweets",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Sentiment Analysis has nowadays a crucial role in social media analysis and, more generally, in analysing user opinions about general topics or user reviews about product/services, enabling a huge number of applications. For instance, sentiment analysis can be applied to monitoring the\nreputation or opinion of a company or a brand with the analysis of reviews of consumer products or services (Hu and Liu, 2004), marketing campaigns in politics (Tumasjan et al., 2010), and financial applications (Oliveira et al., 2013) (Bollen et al., 2010). App stores can be seen as another, not yet well explored, field of application of sentiment analysis. Indeed, they are another social media where users can freely express their own opinion through app reviews about a product, i.e. the specific app under evaluation, or a service, to which the considered app is connecting the user (e.g., a mobile banking app connects users to mobile banking services). In addition, reading user reviews on app stores shows that people frequently talk about and evaluate also the brand associated to the app under review: thus, it is possible to extract people opinion about a brand or the sentiment about a company or the provided service quality.\nTwitter, on the other hand, is one of the most popular and largely used social networks which is very interesting to monitor from the perspective of sentiment analysis, and that is already considered in research challenges in this research area.\nIn this paper, we focus on the app store as a social media platform and on the sentiment evaluation of app reviews and tweets. The former are examples of reviews related to a product, or a service or the associated brand; the latter can include more general sentences, which can be significantly different from reviews as for app reviews, and can be even more difficult to evaluate respect to sentiment analysis. On the other hand, apps reviews are also interesting because they include a score which is not available for tweets. Moreover, in apps reviews, the sentiment score detected in a comment can significantly differ from the score assigned by the user to the app under evaluation. For example, a user can assign his good score to the app (i.e. assigning 5 stars) but also express in natu-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nral language some suggestions or highlight some minor bugs that do not influence his overall app evaluation. For example, the comment Excellent app, but it crashes and closes while code scanning of with camera. Do something! was rated 4 stars by the user, but the sentence contains overall a negative sentiment from the perspective of the app developer since it explains a serious bug. All of this non-structured information is fully missing by only superficially evaluating an app through a 1 to 5 overall score – or any other product evaluated by the user with a sentence and a score–.\nAbout the methods of processing user reviews and tweets, many methods and software implementing different approaches exist and there is not a clear best approach for Sentiment classification/quantification (Araújo et al., 2016) (Chen, 2010) (Gao and Sebastiani, 2015). In our opinion, performance reached by machine learning approaches is a key advantage to apply to sentiment analysis in order to reach a performance which is very close to the one obtained by a group of humans evaluating subjective sentences such as user reviews. As a reference sentiment score, it is well known in literature that a group of humans agree only about in the 80% of cases when evaluating sentiment sentences (Wilson et al., 2009) (Grimes, 2010).\nIn this paper, we present the App2Check and Tweet2Check tools in their English and Italian versions, built on top of two specialized predictive models developed applying supervised learning techniques, and present the results of our experimental evaluation. This shows that App2Check outperforms 19 state-of-the-art research tools on 11 thousand apps reviews in Italian, by overcoming the theoretical reference of 80% of accuracy; and it is better than those tools on 11 thousand app reviews in English. About apps reviews we considered for experiments, these are new test sets we are making available for the research community. About tweets, we show that Tweet2Check is better than competitors on 3899 tweets in Italian language and on 1 thousand randomly selected tweets in English from the latest competitions.\nThe structure of the paper is the following. In section 2 we report a brief description of research tools iFeel platform, SentiStrength for Italian and our tools, App2Check and Tweet2Check. In section 3 we present and discuss our experimental evaluation and in section 4 we show the conclu-\nsions."
    }, {
      "heading" : "2 State-of-the-art Research Tools",
      "text" : "In this section we describe our tools and briefly describe iFeel (Araújo et al., 2014) (v.2.0), a research platform which allows to perform experimental evaluation on 19 state-of-the-art research tools for sentiment analysis, SentiStrength for Italian, which we used to perform our experimental evaluation on Italian language –which is currently our main focus– and on English language, and we describe our systems ."
    }, {
      "heading" : "2.1 App2Check/Tweet2Check description",
      "text" : "App2Check and Tweet2Check implement supervised learning techniques that allowed us to create two predictive models for sentiment quantification: one specialized on apps reviews and one on tweets. Training of predictive models is performed by considering a huge variety of language domains and different kinds of user reviews. Both tools provide, as answer to a sentence in Italian or English language, a quantification of the sentiment polarity scored from 1 to 5, according to the most recent trend shown in the last sentiment evaluation SemEval (Sem, 2016), where tracks considering quantification have been introduced. Thus, we consider the following quantification: as positive, sentences with score 4 (positive) or 5 (very positive); as negative, sentences with score 1 (very negative) or 2 (negative); as neutral, sentences with score 3. In order to compute the final answer, App2Check/Tweet2Check also apply a set of algorithms which take into account some natural language processing techniques, allowing e.g. to also automatically perform topic/named entity extraction. It is not possible to give more details on the engine due to non-disclosure restrictions.\nApp2Check and Tweet2Check are not only constituted by a web service providing access to the sentiment prediction of sentences (with free trial for research purposes), but also provide a full userfriendly web application whose features are out of the scope of this paper. A full demo of the tools will be available for paper presentation."
    }, {
      "heading" : "2.2 iFeel Platform",
      "text" : "iFeel is a research web platform (Araújo et al., 2016; Araújo et al., 2014) allowing to run multiple sentiment analysis tools on the specified list of sentences. It allows to natively run tools support-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\ning English and to first translate sentences from other languages to English and then run 19 tools on the English translated sentences. Since it has been experimentally shown in (Araújo et al., 2016) that well known language specific methods do not have a significant advantage over a simple machine translation approach before sentiment evaluation, and since most of the research tools do not have a publicly available Italian version, the results of the tools ran by iFeel are used for both Italian and English. However, we also considered for the comparison the only research tool we found which natively processes Italian language (Sentistrenght) and it is our reference tool with no translation before sentiment evaluation.\nThe tools included in iFeel are (in alphabetical order): AFINN, Emolex, Emoticon Distance Supervised, Emoticons, Happiness Index, NRC Hashtag, Opinion Finder, Opinion Lexicon, Panas-t, SANN, SASA, Senticnet, Sentiment140, SentiStrength, SentiWordNet, SO-CAL, Stanford Deep Learning, Umigon, Vader."
    }, {
      "heading" : "2.3 Sentistrength for Italian",
      "text" : "SentiStrength was produced as part of the CyberEmotions project, supported by EU FP7. It estimates the strength of positive and negative sentiment in short texts, even for informal language. According to the authors, it has human-level accuracy for short social web texts in English, except political texts (Thelwall et al., 2010). SentiStrength authors make available a version of the tool which natively manages Italian language. All tests have been carried out with both average emotion and strongest emotion options, but in this paper we only report the results obtained with the latter option turned on, due to better performance. Since the English version of SentiStrength is also included in iFeel, in the following we call the Italian version SentiStrengthIta."
    }, {
      "heading" : "3 Experimental Evaluation",
      "text" : "In our experimental evaluation we evaluated App2Check and Tweet2Check sentiment predictors on user reviews of apps from Apple App Store and Google Play Store and on tweets, for both Italian and English language.\nApps reviews benchmark set is constituted by: i. Test sets A-ita and A-eng , both constituted by 1 thousand comments from the famous Candy Crush Saga app, respectively in Italian and En-\nglish. We performed a manual quantification (in the 1-5 range) of the sentiment, performed by a trained person, in order to have a reference sentiment score (from now on called human sentiment quantification, or HSQ). ii. Test set B made of 10 thousands comments from the following 10 different very popular apps (one thousand comments per app).\n• Test set B-ita: Angry Birds, Banco Posta, Facebook, Fruit Ninja, Gmail, Mobile Banking Unicredit, My Vodafone, PayPal, Twitter, Whatsapp.\n• B-eng: Candy Crush Soda Saga, Chase Mobile, Clash Of Clans, Facebook Messenger, Gmail, Instagram, My Verizon, PayPal, Snapchat and Wells Fargo.\nEvery comment has a score, called app rating, associated to the app.\nTweet benchmark set is constituted by 3899 tweets in Italian language from (Araújo et al., 2016) (and (Sem, 2016)) and 1000 tweets in English randomly selected from the English dataset from the same source. While the latter does not contain the neutral class, the former contains also neutral sentences.\nAll tables show columns macro F1 (MF1), accuracy (Acc), F1 on each class (resp. F1(-) for negative, F1(x) for neutral –if present–, and F1(+) for positive), and are sorted by macro F1, considered as scoring system for tools. In all of the tables We highlight in bold the winner tool per column. We ran App2Check/Tweet2Check, the 19 research tools through iFeel on all test sets, and SentiStrengthIta on Test set A-ita, Test set B-ita and Italian tweets, in order to add a comparison with a tool supporting Italian.\nAll of the benchmarks are submitted with the paper, together with a demo access to App2Check and Tweet2Check prediction web services, in order to extend or make repeatable the experiments."
    }, {
      "heading" : "3.1 Evaluation on App reviews",
      "text" : "Results shown in Table 1 highlight that App2Check outperforms all of the research tools in Italian with respect to HSQ, overcoming the theoretical 80% of accuracy; in Table 2 is shown that our tool is better than the research ones also for English apps reviews. Very good results are obtained in both languages by Sentiment 140, which has the best macro-F1 and accuracy among\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nTool M-F1 Acc F1(-) F1(x) F1(+) App2Check 65.8 81.8 85.9 25.2 86.4 Sentiment140 58.1 71.6 80.4 21.4 72.5 SentiWordNet 57.2 67.3 73.5 26.6 71.4 Stanford DL 53.7 60.5 70.0 21.2 69.9 NRC Hashtag 52.9 81.0 76.4 16.4 66.0 Umigon 50.9 56.4 54.0 20.5 78.1 Sentistrength 50.4 57.8 47.5 25.6 78.0 Op. Lexicon 49.8 54.2 53.1 23.7 72.6 AFINN 49.7 57.4 52.4 21.8 74.9 Vader 40.9 42.8 31.9 22.8 68.1 Senticnet 40.2 50.6 37.4 19.7 63.6 Emolex 40.0 43.3 43.1 16.1 60.9 SASA 39.3 44.3 40.8 15.8 61.3 SO-CAL 39.0 40.8 45.2 17.0 54.8 SentiStr. Ita 38.2 41.5 34.3 20.5 59.7 H. Index 32.3 39.8 16.9 18.6 61.4 Op. Finder 23.6 22.9 24.9 18.1 27.9 Emoticon DS 21.5 41.6 1.8 4.1 58.7 SANN 16.5 17.8 1.8 19.9 27.7 Panas-t 7.4 11.0 1.3 18.7 2.2 Emoticons - 10.3 - 18.6 -\nTable 1: Comparison on 1K Candy Crush Saga app reviews in Italian wrt HSQ\nTool M-F1 Acc F1(-) F1(x) F1(+) App2Check 61.9 74.5 82.0 23.2 80.5 Sentiment140 59.9 70.1 77.2 29.7 73.0 Stanford DL 57.6 61.9 71.2 33.3 68.3 NRC Hashtag 56.8 64.9 75.6 31.7 63.2 Umigon 56.4 59.9 59.1 33.3 76.8 Op. Lexicon 53.0 55.6 54.7 36.5 67.7 AFINN 52.8 57.6 54.1 33.6 70.9 SentiWordNet 52.5 58.9 63.4 31.5 62.5 SentiStrength 46.3 49.9 44.0 29.1 65.9 SO-CAL 43.1 44.5 50.4 27.2 51.6 Emolex 41.3 43.1 38.0 32.6 53.4 Vader 40.4 43.0 24.8 31.8 64.7 SASA 40.0 44.4 40.0 19.2 60.7 Senticnet 38.1 43.0 32.9 29.0 52.4 H. Index 33.9 40.1 19.7 26.4 55.7 Op. Finder 27.9 66.9 29.5 26.3 28.0 SANN 19.2 22.8 0.9 29.9 26.8 Panas-t 11.1 17.7 3.6 29.1 0.5 Emoticon DS - 39.9 - 15.8 56.2 Emoticons - 16.8 - 28.8 -\nTable 2: Comparison on 1K Candy Crush Saga app reviews in English wrt HSQ\nresearch tools, and is closer to App2Check on Test set A-eng. We can see that for all of the tools is hard to identify neutral reviews, according to the low F1(x) score, despite a good performance on positive and negative class.\nWe experienced that, as already said, considering a single comment, the score/rating expressed by a user respect to an app can be in general substantially different respect to the sentiment expressed by a human. However, the average score/rating of many (hundreds of) comments can\nbe an approximation of the average sentiment expressed by a human on the same set. Indeed, we compared HSQ with Candy Crush Saga app rating and they agree on about 80% of cases: this data allows us to consider app rating as an approximated indicator of sentiment when averaging thousands of comments.\nTool M-F1 Acc F1(-) F1(x) F1(+) App2Check 73.3 85.7 82.7 45.6 91.7 SentiWordNet 47.9 65.9 60.4 6.2 77.1 AFINN 47.5 60.3 49.2 16.6 76.7 SentiStrength 47.5 59.7 46.3 19.3 76.8 Stanford DL 45.6 54.0 56.5 13.5 66.8 Op. Lexicon 44.9 55.3 45.1 17.5 72.2 Sentiment140 44.1 58.7 57.4 6.7 68.2 Umigon 42.8 50.1 47.8 14.6 66.2 SO-CAL 41.8 49.3 45.8 13.8 65.6 NRC Hashtag 41.2 65.7 53.6 8.3 61.7 Senticnet 40.9 63.1 36.6 9.1 76.9 Vader 38.5 46.2 29.5 19.7 66.3 Emolex 38.3 45.5 38.5 14.1 62.3 SASA 37.7 48.8 29.6 16.4 67.1 SentiStr. Ita 34.0 39.6 32.0 13.9 56.2 H. Index 31.1 39.0 21.9 13.4 57.9 Emoticon DS 27.8 63.6 3.0 2.5 77.8 Op. Finder 26.0 26.6 25.3 15.4 37.2 SANN 12.2 14.9 3.6 16.5 16.4 Panas-t 6.0 9.0 1.2 16.0 0.9 Emoticons 5.4 8.8 0.1 15.9 0.4\nTable 3: Comparison on 10.000 apps reviews in Italian wrt app rating.\nTool M-F1 Acc F1(-) F1(x) F1(+) App2Check 53.4 61.2 66.1 22.5 71.7 Umigon 48.8 51.1 52.7 29.4 64.4 Stanford DL 47.5 51.4 60.7 24.2 57.4 AFINN 45.5 49.9 46.0 27.3 63.4 Op. Lexicon 45.0 48.1 43.4 28.5 63.2 SentiStrength 45.0 47.5 42.9 30.1 62.0 SO-CAL 42.4 44.7 46.5 25.0 55.7 Sentiment140 42.0 53.7 64.3 9.2 52.5 Vader 40.5 42.1 26.8 34.3 60.4 NRC Hashtag 40.5 51.8 63.4 9.2 48.9 SentiWordNet 40.3 50.8 52.9 8.0 60.0 Emolex 39.3 40.7 37.9 27.6 52.3 SASA 38.0 40.2 34.8 26.4 52.7 H. Index 34.4 37.7 22.2 28.3 52.7 Senticnet 33.5 44.1 32.7 10.7 57.1 Op. Finder 33.1 32.9 30.6 29.9 38.7 Emoticon DS 20.1 39.6 1.5 2.0 56.7 SANN 17.2 23.5 1.5 34.0 16.1 Panas-t 12.1 20.4 2.1 33.2 1.1 Emoticons - 20.0 - 33.4 0.1\nTable 4: Comparison on 10.000 apps reviews in English wrt app rating.\nIn Tables 3 and 4 we extend our evaluation respectively on Test set B-ita and Test set B-eng with respect to app rating, in order to have a reference\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nindicator when HSQ is not available. We can see that App2Check generalizes well when tested on many 10 thousand reviews and is the best tool for all measures on the Italian test set, outperforming the research tools with about a 20% of better accuracy from the second tool in the table, and shows also a 45% of better accuracy respect to SentiStrenght for Italian. App2Check is also the best tool for the English app reviews on all but the F1(x) measure.\nOn these test sets, it is difficult to identify the best research tool or the tool having results that are similar to the ones obtained by App2Check. Also Sentiment140, which has been the second best tool (and the best among the research ones) on Candy Crush Saga reviews, is the seventh and the eighth on, respectively, Test set B-ita and Test set B-eng, even if it still has the highest accuracy among the research tools. We can see that its low macro-F1 is due to the bad results on neutral sentences, which, as previously noticed, are the most difficult to identify.\nFigure 1: Sentiment quantification plot on 10 thousand app reviews in Italian language.\nFigure 2: Sentiment quantification plot on 10 thousand app reviews in English language.\nIn order to understand the reason why App2Check is so much better than the other tools on apps reviews, we show in Figures 1 and 2 two plots of the average sentiment per month of the comments, which have been chronologically\nsorted. In order to obtain a better representation of the plot, only the most significant research tools are plotted together with Rating and App2Check.\nIn Figure 1, we choose SentiWordNet (SWN in the plot), which has the best Macro-F1 on this test set, NRC Hashtag (NRC H.), which has the best Accuracy, and SentiStrengthIta (SS. ITA), which is the only research tool supporting Italian. We can see that all tools have similar trend, but NRC Hashtag plot is widely shifted down, and the points in the plots of both SentiWordNet and SentiStrengthIta are closer to the global average sentiment. App2Check plot, instead, is almost overlapped to the rating plot.\nIn Figure 2, we choose Umigon and Stanford Deep Learning (Stanford DL), which have the best Macro-F1, and Sentiment140 (S140), which has the best accuracy, despite its low Macro-F1. In order to have a clearer chart, we show only the last 12 months. First of all, we can see that Sentiment140 plot highlights a tendency to classify sentences as negative. Once again we can see that all tools follows the trend of the rating, but Stanford Deep Learning is, except for 2015/12, on average more negative than the rating, and Umigon has a plot closer to the rating among the research tools. App2Check is again the closest to the app rating.\nWe think that App2Check on Test set B has even higher accuracy, considering as a reference the HSQ instead of the app rating: this is made clear while using the web application and evaluating the system answers on every single user comment."
    }, {
      "heading" : "3.2 Evaluation on Tweets",
      "text" : "The first highlighted rows (with *) in Tables 5, 6 and 7 show our tools feature –native thanks to the machine learning approach–, allowing to learn and extend our model with the provided examples. This means that the predicted model has been updated including in the training set also the sentences belonging to the specific benchmarks under evaluation. We show such results for tweets since, compared to app reviews, all of the tools show a lower accuracy which is always under the theoretical 80%. In this cases, having a tool allowing a training on a specific domain, may in general help to reach a higher accuracy: this is useful when a client asks for a high accuracy on a specific domain, e.g. politics, movies, etc. In this cases, our tool –and in general– machine learning-based approaches– allow to meet this important goal by\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nperforming learning on a subset of sentences of the required domain, while different approaches with no learning feature cannot reach it or anyway reach it with more effort.\nTool MF1 Acc F1(-) F1(x) F1(+) Tweet2Check* 88.2 88.7 89.0 89.8 85.9 Tweet2Check 46.0 46.4 52.1 39.3 46.6 SentiStr. Ita 44.7 46.7 39.4 54.3 40.5 SentiStrength 41.4 43.1 38.8 52.9 32.5 SO-CAL 40.4 42.7 37.9 54.1 29.3 AFINN 40.4 42.2 36.3 54.4 30.5 Umigon 40.1 46.3 30.4 61.7 28.2 Op. Lexicon 39.0 42.9 32.9 56.6 27.6 Op. Finder 37.0 44.9 27.7 60.1 23.0 Vader 36.1 44.0 21.4 60.1 26.9 Emolex 36.0 37.3 39.6 42.1 26.2 SASA 35.7 37.2 30.8 48.1 28.1 Stanford DL 35.0 38.3 44.3 38.3 22.4 H. Index 33.0 36.3 20.0 49.0 29.9 SentiWordNet 32.0 32.9 40.3 24.4 31.2 Sentiment140 29.3 33.5 49.3 12.3 26.4 NRC Hashtag 28.3 35.1 50.5 11.8 22.6 Senticnet 25.1 26.8 28.2 14.8 32.2 Emoticons 22.5 42.9 1.0 60.0 6.6 Panas-t 22.1 42.6 1.6 59.7 5.0 SANN 20.4 42.7 0.7 59.8 0.7 Emoticon DS 12.1 21.0 0.6 1.4 34.4\nTable 5: Comparison on 3899 tweets in Italian\nEvaluating tools on tweets in Italian language, we can see in Table 5 that Tweet2Check is better than research tools with respect to M-F1, but in this case tools show a closer performance, and the Italian version of SentiStrength has almost the same (slightly higher) accuracy.\nIn table 6 we show the results on the same test set in a different way, i.e. excluding the tweets that have been manually classified as neutral, since this helps to clear the influence of the neutral class on the performance of all systems. Tweets, indeed, can be as said in general very different than app reviews, and tweets manually labeled as neutral may not contain any opinion and being objective sentences; these sentences can be more difficult to recognize as neutral by tools.\nWe can see that the MF1 ranking changed, but Tweet2Check is still the best tool, and obtained a Macro-F1 that is 12% higher than Sentiment140, which, once again, is the second best tool.\nFinally, in Table 7, results for English tweets are reported, and we can see that, considering only the macro-F1 measure, three tools, Tweet2Check, AFINN, and SentiStrength outperform the others and obtained similar scores, but, considering accuracy too, Tweet2Check is the best one, since it reaches a score of 78.4%, while AFINN, which is\nTool MF1 Acc F1(-) F1(+) Tweet2Check* 90.5 86.7 91.4 89.7 Tweet2Check 63.3 57.8 66.9 59.7 Sentiment140 51.1 53.1 63.8 38.4 NRC Hashtag 49.5 56.0 69.5 29.5 SentiStr. Ita 47.7 36.5 45.5 50.0 SentiWordNet 47.2 46.0 50.5 43.9 Alchemy 45.9 34.9 36.5 55.3 Semantria 45.2 33.0 40.7 49.7 SentiStrength 43.3 36.9 46.8 39.7 Stanford DL 42.8 43.1 60.3 25.3 Emolex 41.8 36.4 49.7 34.0 SO-CAL 40.4 33.5 43.5 37.4 AFINN 40.3 34.2 42.1 38.5 Senticnet 40.3 40.3 31.3 49.3 SASA 36.5 29.8 34.8 38.1 Op. Lexicon 35.7 27.9 37.4 33.9 Umigon 32.9 24.4 33.4 32.4 H. Index 31.3 24.0 21.9 40.7 Op. Finder 28.3 19.9 30.6 26.1 Vader 27.1 18.5 22.9 31.4 Emoticon DS 26.8 36.0 0.6 53.1 Emoticons 3.9 1.7 1.0 6.7 Panas-t 3.4 1.5 1.7 5.1 SANN 0.7 0.4 0.7 0.7\nthe second best tool, only reached 70.9%."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper we presented our models for sentiment analysis, App2Check and Tweet2Check, in their Italian and English versions, based on two predictive models for apps reviews and tweets applying supervised learning techniques. We showed that App2Check outperforms research competitors on apps reviews, and that Tweet2Check is better than competitors on tweets in both Italian and English language."
    } ],
    "references" : [ {
      "title" : "ifeel: A system that compares and combines sentiment analysis methods",
      "author" : [ "Matheus Araújo", "Pollyanna Gonçalves", "Meeyoung Cha", "Fabrı́cio Benevenuto" ],
      "venue" : "In Proceedings of the 23rd International Conference on World Wide Web,",
      "citeRegEx" : "Araújo et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Araújo et al\\.",
      "year" : 2014
    }, {
      "title" : "An evaluation of machine translation for multilingual sentence-level sentiment analysis",
      "author" : [ "Matheus Araújo", "Julio C.S. Reis", "Adriano C.M. Pereira", "Fabrı́cio Benevenuto" ],
      "venue" : null,
      "citeRegEx" : "Araújo et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Araújo et al\\.",
      "year" : 2016
    }, {
      "title" : "Twitter mood predicts the stock market",
      "author" : [ "Johan Bollen", "Huina Mao", "Xiao-Jun Zeng." ],
      "venue" : "CoRR, abs/1010.3003.",
      "citeRegEx" : "Bollen et al\\.,? 2010",
      "shortCiteRegEx" : "Bollen et al\\.",
      "year" : 2010
    }, {
      "title" : "Al and opinion mining, part 2",
      "author" : [ "Hsinchun Chen." ],
      "venue" : "I.E.E.E. Computer Society, pages 72–79.",
      "citeRegEx" : "Chen.,? 2010",
      "shortCiteRegEx" : "Chen.",
      "year" : 2010
    }, {
      "title" : "Tweet sentiment: From classification to quantification",
      "author" : [ "Wei Gao", "Fabrizio Sebastiani." ],
      "venue" : "ASONAM, pages 97–104. ACM.",
      "citeRegEx" : "Gao and Sebastiani.,? 2015",
      "shortCiteRegEx" : "Gao and Sebastiani.",
      "year" : 2015
    }, {
      "title" : "Mining and summarizing customer reviews",
      "author" : [ "Minqing Hu", "Bing Liu." ],
      "venue" : "KDD, pages 168–177. ACM.",
      "citeRegEx" : "Hu and Liu.,? 2004",
      "shortCiteRegEx" : "Hu and Liu.",
      "year" : 2004
    }, {
      "title" : "On the predictability of stock market behavior using stocktwits sentiment and posting volume",
      "author" : [ "Nuno Oliveira", "Paulo Cortez", "Nelson Areal." ],
      "venue" : "EPIA, volume 8154 of Lecture Notes in Computer Science, pages 355–365. Springer.",
      "citeRegEx" : "Oliveira et al\\.,? 2013",
      "shortCiteRegEx" : "Oliveira et al\\.",
      "year" : 2013
    }, {
      "title" : "Sentiment strength detection in short informal text",
      "author" : [ "Mike Thelwall", "Kevan Buckley", "Georgios Paltoglou", "Di Cai", "Arvid Kappas." ],
      "venue" : "JASIST, 61(12):2544–2558.",
      "citeRegEx" : "Thelwall et al\\.,? 2010",
      "shortCiteRegEx" : "Thelwall et al\\.",
      "year" : 2010
    }, {
      "title" : "Predicting elections with twitter: What 140 characters reveal about political sentiment",
      "author" : [ "Andranik Tumasjan", "Timm Oliver Sprenger", "Philipp G. Sandner", "Isabell M. Welpe." ],
      "venue" : "ICWSM. The AAAI Press.",
      "citeRegEx" : "Tumasjan et al\\.,? 2010",
      "shortCiteRegEx" : "Tumasjan et al\\.",
      "year" : 2010
    }, {
      "title" : "Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis",
      "author" : [ "Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann." ],
      "venue" : "Computational Linguistics, 35(3):399–433.",
      "citeRegEx" : "Wilson et al\\.,? 2009",
      "shortCiteRegEx" : "Wilson et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "For instance, sentiment analysis can be applied to monitoring the reputation or opinion of a company or a brand with the analysis of reviews of consumer products or services (Hu and Liu, 2004), marketing campaigns in politics (Tumasjan et al.",
      "startOffset" : 174,
      "endOffset" : 192
    }, {
      "referenceID" : 8,
      "context" : "For instance, sentiment analysis can be applied to monitoring the reputation or opinion of a company or a brand with the analysis of reviews of consumer products or services (Hu and Liu, 2004), marketing campaigns in politics (Tumasjan et al., 2010), and financial applications (Oliveira et al.",
      "startOffset" : 226,
      "endOffset" : 249
    }, {
      "referenceID" : 6,
      "context" : ", 2010), and financial applications (Oliveira et al., 2013) (Bollen et al.",
      "startOffset" : 36,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : ", 2013) (Bollen et al., 2010).",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "About the methods of processing user reviews and tweets, many methods and software implementing different approaches exist and there is not a clear best approach for Sentiment classification/quantification (Araújo et al., 2016) (Chen, 2010) (Gao and Sebastiani, 2015).",
      "startOffset" : 206,
      "endOffset" : 227
    }, {
      "referenceID" : 3,
      "context" : ", 2016) (Chen, 2010) (Gao and Sebastiani, 2015).",
      "startOffset" : 8,
      "endOffset" : 20
    }, {
      "referenceID" : 4,
      "context" : ", 2016) (Chen, 2010) (Gao and Sebastiani, 2015).",
      "startOffset" : 21,
      "endOffset" : 47
    }, {
      "referenceID" : 9,
      "context" : "As a reference sentiment score, it is well known in literature that a group of humans agree only about in the 80% of cases when evaluating sentiment sentences (Wilson et al., 2009) (Grimes, 2010).",
      "startOffset" : 159,
      "endOffset" : 180
    }, {
      "referenceID" : 0,
      "context" : "In this section we describe our tools and briefly describe iFeel (Araújo et al., 2014) (v.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "2 iFeel Platform iFeel is a research web platform (Araújo et al., 2016; Araújo et al., 2014) allowing to run multiple sentiment analysis tools on the specified list of sentences.",
      "startOffset" : 50,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "2 iFeel Platform iFeel is a research web platform (Araújo et al., 2016; Araújo et al., 2014) allowing to run multiple sentiment analysis tools on the specified list of sentences.",
      "startOffset" : 50,
      "endOffset" : 92
    }, {
      "referenceID" : 1,
      "context" : "Since it has been experimentally shown in (Araújo et al., 2016) that well known language specific methods do not have a significant advantage over a simple machine translation approach before sentiment evaluation, and since most of the research tools do not have a publicly available Italian version, the results of the tools ran by iFeel are used for both Italian and English.",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "According to the authors, it has human-level accuracy for short social web texts in English, except political texts (Thelwall et al., 2010).",
      "startOffset" : 116,
      "endOffset" : 139
    }, {
      "referenceID" : 1,
      "context" : "Tweet benchmark set is constituted by 3899 tweets in Italian language from (Araújo et al., 2016) (and (Sem, 2016)) and 1000 tweets in English randomly selected from the English dataset from the same source.",
      "startOffset" : 75,
      "endOffset" : 96
    } ],
    "year" : 2016,
    "abstractText" : "Sentiment Analysis has nowadays a crucial role in social media analysis and, more generally, in analyzing user opinions about general topics or user reviews about product/services, enabling a huge number of applications. Many methods and software implementing different approaches exist and there is not a clear best approach for Sentiment classification/quantification. We believe that performance reached by machine learning approaches is a key advantage to apply to sentiment analysis in order to reach a performance which is very close to the one obtained by a group of humans, who evaluate subjective sentences such as user reviews. In this paper, we present the results of our experimental evaluation of both research and commercial state-of-the-art tools for Sentiment Analysis, considering as benchmarks, both user reviews related to the evaluation of apps published to app stores and tweets. Thus, we show that our tools App2Check and Tweet2Check –developed mainly applying supervised learning techniques– are the best tools for sentiment evaluation on apps reviews and tweets for both Italian and English language, compared to the state-of-the-art research tools.",
    "creator" : "TeX"
  }
}