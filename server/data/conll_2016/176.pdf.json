{
  "name" : "176.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sentence Pair Scoring: Towards Unified Framework for Text Comprehension",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We argue that all such tasks are similar from the model perspective and propose new baselines by comparing the performance of common IR metrics and popular convolutional, recurrent and attentionbased neural models across many Sentence Pair Scoring tasks and datasets. We discuss the problem of evaluating randomized models, propose a statistically grounded methodology, and attempt to improve comparisons by releasing new datasets that are much harder than some of the currently used well explored benchmarks. We introduce a unified open source software framework with easily pluggable models and tasks, which enables us to experiment with multi-task reusability of trained sentence models."
    }, {
      "heading" : "1 Introduction",
      "text" : "AnNLPmachine learning task often involves classifying a sequence of tokens such as a sentence or a document, i.e. approximating a function f1(s) ∈ [0, 1] (where f1 may determine a domain, sentiment, etc.). But there is a large class of problems that involve classifying a pair of sentences, f2(s0, s1) ∈ R (where s0, s1 are sequences of tokens, typically sentences).\nTypically, the function f2 represents some sort of semantic similarity, that is whether (or how much) the two sequences are semantically related.\nThis formulation allows f2 to be a measure for tasks as different as topic relatedness, paraphrasing, degree of entailment, a pointwise ranking task for answer-bearing sentences or next utterance classification. In this work, we adopt the working assumption that there exist certain universal f2 type measures that may be successfuly applied to a wide variety of semantic similarity tasks — in the case of neural network models trained to represent universal semantic comprehension of sentences and adapted to the given task by just fine-tuning or adapting the output neural layer (in terms of architecture or just weights). Our argument for preferring f2 to f1 in this pursuit is the fact that the other sentence in the pair is essentially a very complex label when training the sequence model, which can therefore discern semantically rich structures and dependencies. Determining and demonstrating such universal semantic comprehension models across multiple tasks remains a few steps ahead, since the research landscape is fragmented in this regard. Model research is typically reported within the context of just a single f2-type task, each dataset requires sometimes substantial engineering work before measurements are possible, and results are reported in ways that make meaningful model comparisons problematic. Our main aims are as follows. (A) Unify research within a single framework that employs task-independent models and task-specific adaptation modules. (B) Improve the methodology of model evaluation in terms of statistics, comparing with strong non-neural IR baselines, and introducing new datasets with better characteristics. (C) Demonstrate the feasibility of pursuing universal, task-independent f2 models, showing that even simple neural models learn universal semantic comprehension by employing cross-task transfer learning.\nThe paper is structured as follows. In Sec. 2, we outline possible specific f2 tasks and available datasets; in Sec. 3, we survey the popular nonneural and neural baselines in the context of these tasks; finally, in Sec. 4, we present model-task evaluations within a unified framework to establish the watermark for future research as well as gain insight into the suitability of models across a variety of tasks. In Sec. 5, we demonstrate that transfer learning across tasks is helpful to powerfully seed models. We conclude with Sec. 6, summarizing our findings and outlining several future research directions."
    }, {
      "heading" : "2 Tasks and Datasets",
      "text" : "The tasks we are aware of that can be phrased as f2-type problems are listed below. In general, we primarily focus on tasks that have reasonably large and realistically complex datasets freely available. On the contrary, we have explicitly avoided datasets that have licence restrictions on availability or commercial usage."
    }, {
      "heading" : "2.1 Answer Sentence Selection",
      "text" : "Given a factoid question and a set of candidate answer-bearing sentences in encyclopedic style, the first task is to rank higher sentences that are more likely to contain the answer to the question. As it is fundamentally an Information Retrival task in nature, the model performance is commonly evaluated in terms of Mean Average Precision (MAP) and Mean Reciprocial Rank (MRR).\nThis task is popular in the NLP research community thanks to the dataset introduced in (Wang et al., 2007) (which we refer to as wang), with six papers published between February 2015 and 2016 alone and neural models substantially improving over classical approaches based primarily on parse tree edits.1 It is possibly the main research testbed for f2-style task models. This task has also immediate applications e.g. in Question Answering systems.\nIn the context of practical applications, the sofar standard wang dataset has several downsides we observed when tuning and evaluating our models, illustrated numerically in Fig. 1 — the set of candidate sentences is often very small and quite uneven (which also makes rank-based measures\n1http://aclweb.org/aclwiki/index.php? title=Question_Answering_(State_of_the_ art)\nunstable) and the total number of individual sentence pairs as well as questions is relatively small. Furthermore, the validation and test set are very small, which makes for noisy performance measurements; the splits also seem quite different in the nature of questions since we see minimum correlation between performance on the validation and test sets, which calls the parameter tuning procedures and epoch selection for early stopping into question. Alternative datasets WikiQA (Yang et al., 2015) and InsuranceQA (Tan et al., 2015) were proposed, but are encumbered by licence restrictions. Furthermore, we speculate that they may suffer from many of the problems above2 (even if they are somewhat larger). To alleviate the problems listed above, we are introducing a new dataset yodaqa/large2470 based on an extension of the curatedv2 question dataset (introduced in (Baudiš and Šedivý, 2015), further denoisified by Mechanical Turkers) with candidate sentences as retrieved by the YodaQA question answering system (Baudiš, 2015) from English Wikipedia and labelled by matching the gold standard answers in the passages.3\nMotivated by another problem related to the YodaQA system, we also introduce another dataset wqmprop, where s0 are again question sentences, but s1 are English labels of properties that make a path within the Freebase knowledge base that connects an entity linked in the question to the correct answer. This task (Property Selection) can be evaluated identically to the previous task, and solutions often involving Convolutional Neural Networks have been studied in the Question Answering literature (Yih et al., 2015) (Xu et al., 2016). Our sentences have been derived from the WebQuestions dataset (Berant et al., 2013) extended with the moviesE dataset questions (originally introduced in (Baudiš and Šedivý, 2015)); the property paths are based on the Freebase knowledge graph dump, generated based on entity linking and exploration procedure of YodaQA v1.5.4\nFig. 1 compares the critical characteristics of\n2Moreover, InsuranceQA is effectively a classification task rather than a ranking task, which we do not find as appealing in the context of practical applications.\n3Note that the wang and yodaqa datasets however share a common ancestry regarding the set of questions and there may be some overlaps, even across train and test splits. Therefore, mixing training and evaluation on wang and yodaqa datasets within a single model instance is not advisable.\n4https://github.com/brmson/ dataset-factoid-webquestions branch movies\nthe datasets. Furthermore, as apparent below, the baseline performances on the newly proposed datasets are much lower, which suggests that future model improvements will be more apparent in evaluation."
    }, {
      "heading" : "2.2 Next Utterance Ranking",
      "text" : "(Lowe et al., 2015) proposed the new large-scale real-world Ubuntu Dialogue dataset for an f2-style task of ranking candidates for the next utterance in a chat dialog, given the dialog context. The technical formulation of the task is the same as for Answer Sentence Selection, but semantically, choosing the best followup has different concerns than choosing an answer-bearing sentence. Recall at top-ranked 1, 2 or 5 utterances out of either 2 or 10 candidates is reported; we also propose reporting the utterance MRR as a more aggregate measure. The newly proposed Ubuntu Dialogue dataset is based on IRC chat logs of the Ubuntu community technical support channels and contains casually typed interactions regarding computer-related problems.5 While the training set consists of individual labelled pairs, during evaluation 10 followups to given message(s) are ranked. The sequences might be over 200 tokens long.\nOur primary motivation for using this dataset is its size. The numerical characteristics of this dataset are shown in Table 1.6 We use the v2 version of the dataset.7 Research published on this dataset so far relies on simple neural models. (Lowe et al., 2015) (Kadlec et al., 2015)"
    }, {
      "heading" : "2.3 Recognizing Textual Entailment and Semantic Textual Similarity",
      "text" : "One of the classic tasks at the boundary of Natural Language Processing and Artificial Intelligence is the inference problem of Recognizing Textual Entailment (Dagan et al., 2006) — given a pair of a factual sentence and a hypothesis sentence, we are to determine whether the hypothesis represents a contradiction, entailment or is neutral (cannot be proven or disproven).\nWe include two current popular machine learning datasets for this task. The Stanford Natural Language Inference SNLI dataset (Bowman et al.,\n5In a manner, they resemble tweet data, but without the length restriction and with heavily technical jargon, interspersed command sequences etc.\n6As in past papers, we use only the first 1M pairs (10%) of the training set.\n7https://github.com/rkadlec/ ubuntu-ranking-dataset-creator\n2015) consists of 570k English sentence pairs with the facts based on image captions, and 10k + 10k of the pairs held out as validation and test sets. The SICK-2014 dataset (Marelli et al., 2014) was introduced as Task 1 of the SemEval 2014 conference and in contrast to SNLI, it is geared at specifically benchmarking semantic compositional methods, aiming to capture only similarities on purely language and common knowledge level, without relying on domain knowledge, and there are no named entities or multi-word idioms; it consists of 4500 training pairs, 500 validation pairs and 4927 testing pairs. For the SICK-2014 dataset, we also report results on the Semantic Textual Similarity. This task originates in the STS track of the SemEval conferences (Agirre et al., 2015) and involves scoring pairs of sentences from 0 to 5 with the objective of maximizing correlation (Pearson’s r) with manually annotated gold standard."
    }, {
      "heading" : "3 Models",
      "text" : "As our goal is a universal text comprehension model, we focus on neural network models architecture-wise. We assume that the sequence is transformed usingN -dimensional word embeddings on input, and employ models that produce a pair of sentence embeddings E0, E1 from the sequences of word embeddings e0, e1. Unless noted otherwise, a Siamese architecture is used that shares weights among both sentenes. A scorer module that compares the E0, E1 sentence embeddings to produce a scalar result is connected to the model; for specific task-model configurations, we use either the dot-product module E0 · ET1 (representing non-normalized vector angle, as in e.g. (Yu et al., 2014) or (Weston et al., 2014)) or theMLPmodule that takes elementwise product and sum of the embeddings and feeds them to a two-layer perceptron with hidden layer of width 2N (as in e.g. (Tai et al., 2015)).8 For the STS task, we follow this by score regression using class interpolation as in (Tai et al., 2015). When training for a ranking task (Answer Sentence Selection), we use the bipartite ranking version of Ranknet (Burges et al., 2005) as the objective; when training for STS task, we use Pearson’s r formula as the objective; for binary classification\n8The motivation is to capture both angle and euclid distance in multiple weighed sums. Past literature uses absolute difference rather than sum, but both performed equally in our experiments and we adopted sum for technical reasons.\ntasks, we use the binary crossentropy objective."
    }, {
      "heading" : "3.1 Baselines",
      "text" : "In order to anchor the reported performance, we report several basic methods. Weighed word overlaps metrics TF-IDF and BM25 (Robertson et al., 1995) are inspired by IR research and provide strong baselines for many tasks. We treat s0 as the query and s1 as the document, counting the number of common words and weighing them appropriately. IDF is determined on the training set.\nThe avg metric represents the baseline method when using word embeddings that proved successful e.g. in (Yu et al., 2014) or (Weston et al., 2014), simply taking the mean vector of the word embedding sequence and training an U weight matrix N×2N that projects both embeddings to the same vector space, Ei = tanh(U · ēi), where the MLP scorer compares them. During training, p = 1/3 standard (elementwise) dropout is applied on the input embeddings.\nA simple extension of the above are the DAN Deep Averaging Networks (Iyyer et al., 2015), which were shown to adequately replace much more complex models in some tasks. Two dense perceptron layers are stacked between the mean and projection, relu is used instead of tanh as the non-linearity, and word-level dropout is used instead of elementwise dropout."
    }, {
      "heading" : "3.2 Recurrent Neural Networks",
      "text" : "RNN with memory units are popular models for processing sentenes (Tan et al., 2015) (Lowe et al., 2015) (Bowman et al., 2015). We use a bidirectional network with 2N GRU memory units9 (Cho et al., 2014) in each direction; the final unit states are summed across the per-direction GRUs\n9While the LSTM architecture is more popular, we have found the GRU results are equivalent while the number of parameters is reduced.\nto yield a 2N vector representation of the sentence. Like in the avg baseline, a projection matrix is applied on this representation and final vectors compared by an MLP scorer. We have found that applying massive dropout p = 4/5 both on the input and output of the network helps to avoid overfitting even early in the training."
    }, {
      "heading" : "3.3 Convolutional Neural Networks",
      "text" : "CNN with sentence-wide pooling layer are also popular models for processing sentences (Yu et al., 2014) (Tan et al., 2015) (Severyn and Moschitti, 2015) (He et al., 2015) (Kadlec et al., 2015). We apply a multi-channel convolution (Kim, 2014) with single-token channel of N convolutions and 2, 3, 4 and 5-token channels of N/2 convolutions each, relu transfer function, max-pooling over the whole sentence, and as above a projection to shared space and an MLP scorer. Dropout is not applied."
    }, {
      "heading" : "3.4 RNN-CNN Model",
      "text" : "The RNN-CNN model aims to combine both recurrent and convolutional networks by using the memory unit states in each token as the new representation of the token which is then fed to the convolutional network. Inspired by (Tan et al., 2015), the aim of this model is to allow the RNN to model long-term dependencies and model contextual representations of words, while taking advantage of the CNN and pooling operation for crisp selection of the gist of the sentence. We use the same parameters as for the individual models, but with no dropout and reducing the number of parameters by using only N memory units per direction."
    }, {
      "heading" : "3.5 Attention-Based Models",
      "text" : "The idea of attention models is to attend preferrentially to some parts of the sentence when building its representation (Hermann et al., 2015) (Tan et al., 2015) (dos Santos et al., 2016) (Rocktäschel\net al., 2015). There are many ways to model attention, we adopt the (Tan et al., 2015) model attn1511 as a conceptually simple and easy to implement baseline. It asymmetrically extends the RNN-CNN model by extra links from s0 CNN output to the post-recurrent representation of each s1 token, determining an attention level for each token by weighed sum of the token vector elements, focusing on the relevant s1 segment by transforming the attention levels using softmax and multiplying the token representations by the attention levels before they are fed to the convolutional network.\nConvolutional network weights are not shared between the two sentences and the convolutional network output is not projected before applying the MLP scorer. The CNN used here is singlechannel with 2N convolution filters 3 tokens wide.\n4 Model Performance\n4.1 dataset-sts framework\nTo easily implement models, dataset loaders and task adapters in a modular fashion so that any model can be easily run on any f2-type task, we have created a new software package dataset-sts that integrates a variety of datasets, a Python dataset adapter PySTS and a Python library for easy construction of deep neural NLP models for semantic sentence pair scoring KeraSTS that uses the Keras machine learning library (Chollet, 2015). The framework is available for other researchers as open source on GitHub.10"
    }, {
      "heading" : "4.2 Experimental Setting",
      "text" : "We use N = 300 dimensional GloVe embeddings matrix pretrained on Wikipedia 2014 + Gigaword 5 (Pennington et al., 2014) that we keep adaptable during training; words in the training set not included in the pretrained model are initialized by random vectors uniformly sampled from [−0.25,+0.25] to match the embedding standard deviation.\nWord overlap is an important feature in many f2-type tasks (Yu et al., 2014) (Severyn and Moschitti, 2015), especially when the sentences may contain named entities, numeric or other data for which no embedding is available. As a workaround, ensemble of world overlap count and neural model score is typically used to produce\n10URL redacted but included as supplementary material\nthe final score. In line with this idea, in the Answer Sentence Selection wang and large2470 datasets, we use the BM25 overlap baseline as an additional input to the MLP scoring module, and prune the scored samples to top 20 based on BM25.11 Furthermore, we extend the embedding of each input token by several extra dimensions carrying boolean flags— bigram overlap, unigram overlap (except stopwords and interpunction), and whether the token starts with a capital letter or is a number. Particular hyperparameters are tuned primarily on the yodaqa/large2470 dataset unless noted otherwise in the respective results table caption. We apply 10−4 L2 regularization and use Adam optimization with standard parameters (Kingma and Ba, 2014). In the answer selection tasks, we train on 1/4 of the dataset in each epoch. After training, we use the epoch with best validation performance; sadly, we typically observe heavy overfitting as training progresses and rarely use a model from later than a couple of epochs."
    }, {
      "heading" : "4.3 Evaluation Methodology",
      "text" : "We report model performance averaged across 16 training runs (with different seeds). A consideration we must emphasize is that randomness plays a large role in neural models both in terms of randomized weight initialization and stochastic dropout. For example, the typical methodology for reporting results on the wang dataset is to evaluate and report a single test run after tuning on the dev set,12 but wang test MRR has empirical standard deviation of 0.025 across repeated runs of our attn1511 model, which is more than twice the gap between every two successive papers pushing the state-of-art on this dataset! See the ∗- marked sample in Fig. 2 for a practical example of this phenomenon. Furthermore, on more complex tasks (Answer Sentence Selection in particular, see Fig. 1) the validation set performance is not a great approximator for test set performance and a strategy like picking the training run with best validation performance would lead just to overfitting on the validation set. To allow comparison between models (and with 11This reduces the number of (massively irrelevant) training samples, but we observed no adverse effects of that, while it speeds up training greatly and models well a typical Information Retrieval scenario where fast pre-scoring of candidates is essential.\n12Confirmed by personal communication with paper authors.\nfuture models), we therefore report also 95% confidence intervals for each model performance estimate, as determined from the empirical standard deviation using Student’s t-distribution.13"
    }, {
      "heading" : "4.4 Results",
      "text" : "In Fig. 2 to 4, we show the cross-task performance of our models. We can observe an effect analogous to what has been described in (Kadlec et al., 2015)—when the dataset is smaller, CNNmodels are preferrable, while larger dataset allows RNN models to capture the text comprehension task better. IR baselines provide strong competition and finding new ways to ensemble them with models should prove beneficial in the future.14 This is especially apparent in the new Answer Sentence Selection datasets that have very large number of sentence candidates per question. The attention mechanism also has the highest impact in this kind of Information Retrieval task.\nWhile our models clearly yet lag behind the state-of-art on the RTE and STS tasks, it establishes the new baseline on the Ubuntu Dialogue dataset and it is not possible to statistically determine its relation to state-of-art on the wang Answer Sentence Selection dataset."
    }, {
      "heading" : "5 Model Reusability",
      "text" : "To confirm the hypothesis that our models learn a generic task akin to some form of text comprehension, we trained a model on the large Ubuntu Dialogue dataset (Next Utterance Ranking task) and transferred the weights and retrained the model instance on other tasks. We used the RNN model for the experiment in a configuration with dot-product scorer and smaller dimensionality (which works much better on the Ubuntu dataset). This configuration is shown in the respective result tables as Ubu. RNN and it consistently ranks as the best or among the best classifiers, dramatically outperforing the baseline RNN model.15\nDuring our experiments, we have noticed that it is important not to apply dropout during re-\n13Over larger number of samples, this estimate converges to the normal distribution confidence levels. Note that the confidence interval determines the range of the true expected evaluation, not evaluation of any measured sample.\n14We have tried simple averaging of predictions (as per (Kadlec et al., 2015)), but the benefit was small and inconsistent.\n15The RNN configuration used for the transfer, when trained only on the target task, is not shown in the tables but has always been worse than the baseline RNN configuration.\ntraining if it wasn’t applied during the source model training, to balance the dataset labels, and we used the RMSprop training procedure since Adam’s learning rate annealing schedule might not be appropriate for weight re-training. We have also tried freezing the weights of some layers, but this never yielded a significant improvement."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have unified a variety of tasks in a single scientific framework of sentence pair scoring, and demonstrated a platform for general modelling of this problem and aggregate benchmarking of these models across many datasets. Promising initial transfer learning results suggest that a quest for generic neural model capable of task-independent text comprehension is becoming a meaningful pursuit. The open source nature of our framework and the implementation choice of a popular and extensible deep learning library allows for high reusability of our research and easy extensions with further more advanced models. Based on our benchmarks, as a primary model for applications on new f2-type tasks, we can recommend either the RNN-CNN model or transfer learning based on the Ubu. RNN model."
    }, {
      "heading" : "6.1 Future Work",
      "text" : "Due to the very wide scope of the f2-problem scope, we leave some popular tasks and datasets as future work. A popular instance of sentence pair scoring is the question answering task of the Memory Networks (supported by the baBi dataset) (Weston et al., 2015). A realistic large question Paraphrasing dataset based on the AskUbuntu Stack Overflow forum had been recently proposed (Lei et al., 2015).16 In a multi-lingual context, sentence-level MT Quality Estimation is a meta-task with several available datasets.17 While the tasks of Semantic Textual Similarity (supported by a dataset from the STS track of the SemEval conferences (Agirre et al., 2015)) and Paraphrasing (based on the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) right now) are available within our framework, we do not report the results here as the models lag behind the state-of-art significantly and\n16The task resembles paraphrasing, but is evaluated as an Information Retrieval task much closer to Answer Sentence Selection.\n17http://www.statmt.org/wmt15/ quality-estimation-task.html\nshow little difference in results. Advancing the models to be competitive remains future work. A generalization of our proposed architecture could be applied to the Hypothesis Evidencing task of binary classification of a hypothesis sentence s0 based on a number of memory sentences s1, for example within the MCText (Richardson et al., 2013) dataset. We also did not include several major classes of models in our initial evaluation. Most notably, this includes serial RNNs with attention as used e.g. for the RTE task (Rocktäschel et al., 2015), and the skip-thoughts method of sentence embedding. (Kiros et al., 2015)\nWe believe that the Ubuntu Dialogue Dataset results demonstrate that the time is ripe to push the research models further towards the real-world by allowing for wider sentence variability and less explicit supervision. But in particular, we believe that new models should be developed and tested on tasks with long sentences and wide vocabulary. In terms of models, recent work in many NLP domains (dos Santos et al., 2016) (Cheng et al.,\n2016) (Kumar et al., 2015) clearly points towards various forms of attention modelling to remove the bottleneck of having to compress the full spectrum of semantics into a single vector of fixed dimensionality. In this paper, we have shown the benefit of training a model on a single dataset and then applying it on another dataset. One open question is whether we could jointly train a model on multiple tasks simultaneously (even if they do not share some output layers). Another option would be to include extra supervision similar to the token overlap features that we already employ; for example, in the new Answer Sentence Selection task datasets, we can explicitly mark the actual tokens representing the answer."
    }, {
      "heading" : "Acknowledgments",
      "text" : "redacted"
    } ],
    "references" : [ {
      "title" : "Semeval-2015 task 2: Semantic textual similarity, english, spanish and pi",
      "author" : [ "Eneko Agirre", "Carmen Banea", "Claire Cardie", "Daniel Cer", "Mona Diab", "Aitor Gonzalez-Agirre", "Weiwei Guof", "Inigo Lopez-Gazpio", "Montse Maritxalar", "Rada Mihalcea" ],
      "venue" : null,
      "citeRegEx" : "Agirre et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Agirre et al\\.",
      "year" : 2015
    }, {
      "title" : "Modeling of the question answering task in the YodaQA system",
      "author" : [ "Petr Baudiš", "Jan Šedivý." ],
      "venue" : "Experimental IR Meets Multilinguality, Multimodality, and Interaction, pages 222–228. Springer.",
      "citeRegEx" : "Baudiš and Šedivý.,? 2015",
      "shortCiteRegEx" : "Baudiš and Šedivý.",
      "year" : 2015
    }, {
      "title" : "YodaQA: A Modular Question Answering System Pipeline",
      "author" : [ "Petr Baudiš." ],
      "venue" : "POSTER 2015 - 19th International Student Conference on Electrical Engineering.",
      "citeRegEx" : "Baudiš.,? 2015",
      "shortCiteRegEx" : "Baudiš.",
      "year" : 2015
    }, {
      "title" : "Semantic parsing on freebase from question-answer pairs",
      "author" : [ "Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang." ],
      "venue" : "EMNLP, pages 1533– 1544.",
      "citeRegEx" : "Berant et al\\.,? 2013",
      "shortCiteRegEx" : "Berant et al\\.",
      "year" : 2013
    }, {
      "title" : "A large annotated corpus for learning natural language inference",
      "author" : [ "Samuel R. Bowman", "Gabor Angeli", "Christopher Potts", "Christopher D. Manning." ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to rank using gradient descent",
      "author" : [ "Chris Burges", "Tal Shaked", "Erin Renshaw", "Ari Lazier", "Matt Deeds", "Nicole Hamilton", "Greg Hullender." ],
      "venue" : "Proceedings of the 22nd international conference on Machine learning, pages 89–96. ACM.",
      "citeRegEx" : "Burges et al\\.,? 2005",
      "shortCiteRegEx" : "Burges et al\\.",
      "year" : 2005
    }, {
      "title" : "Long short-term memory-networks for machine reading",
      "author" : [ "Jianpeng Cheng", "Li Dong", "Mirella Lapata." ],
      "venue" : "CoRR, abs/1601.06733.",
      "citeRegEx" : "Cheng et al\\.,? 2016",
      "shortCiteRegEx" : "Cheng et al\\.",
      "year" : 2016
    }, {
      "title" : "On the properties of neural machine translation: Encoder-decoder approaches",
      "author" : [ "KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio." ],
      "venue" : "CoRR, abs/1409.1259.",
      "citeRegEx" : "Cho et al\\.,? 2014",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Keras",
      "author" : [ "Franois Chollet." ],
      "venue" : "https://github. com/fchollet/keras.",
      "citeRegEx" : "Chollet.,? 2015",
      "shortCiteRegEx" : "Chollet.",
      "year" : 2015
    }, {
      "title" : "The PASCAL recognising textual entailment challenge",
      "author" : [ "Ido Dagan", "Oren Glickman", "Bernardo Magnini." ],
      "venue" : "Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pages 177–",
      "citeRegEx" : "Dagan et al\\.,? 2006",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 2006
    }, {
      "title" : "Automatically constructing a corpus of sentential paraphrases",
      "author" : [ "William B Dolan", "Chris Brockett" ],
      "venue" : null,
      "citeRegEx" : "Dolan and Brockett.,? \\Q2005\\E",
      "shortCiteRegEx" : "Dolan and Brockett.",
      "year" : 2005
    }, {
      "title" : "Multiperspective sentence similarity modeling with convolutional neural networks",
      "author" : [ "Hua He", "Kevin Gimpel", "Jimmy Lin" ],
      "venue" : null,
      "citeRegEx" : "He et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "He et al\\.",
      "year" : 2015
    }, {
      "title" : "Teaching machines to read and comprehend",
      "author" : [ "Karl Moritz Hermann", "Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1684–",
      "citeRegEx" : "Hermann et al\\.,? 2015",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep unordered composition rivals syntactic methods for text classification",
      "author" : [ "Mohit Iyyer", "Varun Manjunatha", "Jordan Boyd-Graber", "Hal Daumé III" ],
      "venue" : null,
      "citeRegEx" : "Iyyer et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Iyyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Improved deep learning baselines for ubuntu corpus dialogs",
      "author" : [ "Rudolf Kadlec", "Martin Schmid", "Jan Kleindienst." ],
      "venue" : "arXiv preprint arXiv:1510.03753.",
      "citeRegEx" : "Kadlec et al\\.,? 2015",
      "shortCiteRegEx" : "Kadlec et al\\.",
      "year" : 2015
    }, {
      "title" : "Convolutional neural networks for sentence classification",
      "author" : [ "Yoon Kim." ],
      "venue" : "arXiv preprint arXiv:1408.5882.",
      "citeRegEx" : "Kim.,? 2014",
      "shortCiteRegEx" : "Kim.",
      "year" : 2014
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik Kingma", "Jimmy Ba." ],
      "venue" : "arXiv preprint arXiv:1412.6980.",
      "citeRegEx" : "Kingma and Ba.,? 2014",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Skip-thought vectors",
      "author" : [ "Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun", "Antonio Torralba", "Sanja Fidler." ],
      "venue" : "Advances in Neural Information Processing Systems, pages 3276–3284.",
      "citeRegEx" : "Kiros et al\\.,? 2015",
      "shortCiteRegEx" : "Kiros et al\\.",
      "year" : 2015
    }, {
      "title" : "Ask me anything: Dynamic memory networks for natural language processing",
      "author" : [ "Ankit Kumar", "Ozan Irsoy", "Jonathan Su", "James Bradbury", "Robert English", "Brian Pierce", "Peter Ondruska", "Ishaan Gulrajani", "Richard Socher." ],
      "venue" : "CoRR, abs/1506.07285.",
      "citeRegEx" : "Kumar et al\\.,? 2015",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2015
    }, {
      "title" : "Denoising bodies to titles: Retrieving similar questions with recurrent convolutional models",
      "author" : [ "Tao Lei", "Hrishikesh Joshi", "Regina Barzilay", "Tommi S. Jaakkola", "Kateryna Tymoshenko", "Alessandro Moschitti", "Lluı́s Màrquez i Villodre" ],
      "venue" : null,
      "citeRegEx" : "Lei et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lei et al\\.",
      "year" : 2015
    }, {
      "title" : "The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
      "author" : [ "Ryan Lowe", "Nissan Pow", "Iulian Serban", "Joelle Pineau." ],
      "venue" : "CoRR, abs/1506.08909.",
      "citeRegEx" : "Lowe et al\\.,? 2015",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2015
    }, {
      "title" : "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual",
      "author" : [ "Marco Marelli", "Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli" ],
      "venue" : null,
      "citeRegEx" : "Marelli et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Marelli et al\\.",
      "year" : 2014
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Empirical Methods in Natural Language Processing (EMNLP), pages 1532– 1543.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "Mctest: A challenge dataset for the open-domain machine comprehension of text",
      "author" : [ "Matthew Richardson", "Christopher JC Burges", "Erin Renshaw" ],
      "venue" : null,
      "citeRegEx" : "Richardson et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Richardson et al\\.",
      "year" : 2013
    }, {
      "title" : "Okapi at trec-3",
      "author" : [ "Stephen E Robertson", "Steve Walker", "Susan Jones" ],
      "venue" : null,
      "citeRegEx" : "Robertson et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Robertson et al\\.",
      "year" : 1995
    }, {
      "title" : "Reasoning about entailment with neural attention",
      "author" : [ "Tim Rocktäschel", "Edward Grefenstette", "Karl Moritz Hermann", "Tomás Kociský", "Phil Blunsom." ],
      "venue" : "CoRR, abs/1509.06664.",
      "citeRegEx" : "Rocktäschel et al\\.,? 2015",
      "shortCiteRegEx" : "Rocktäschel et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to rank short text pairs with convolutional deep neural networks",
      "author" : [ "Aliaksei Severyn", "Alessandro Moschitti." ],
      "venue" : "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages",
      "citeRegEx" : "Severyn and Moschitti.,? 2015",
      "shortCiteRegEx" : "Severyn and Moschitti.",
      "year" : 2015
    }, {
      "title" : "Improved semantic representations from tree-structured long short-term memory networks",
      "author" : [ "Kai Sheng Tai", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "CoRR, abs/1503.00075.",
      "citeRegEx" : "Tai et al\\.,? 2015",
      "shortCiteRegEx" : "Tai et al\\.",
      "year" : 2015
    }, {
      "title" : "Lstmbased deep learning models for non-factoid answer selection",
      "author" : [ "Ming Tan", "Bing Xiang", "Bowen Zhou." ],
      "venue" : "CoRR, abs/1511.04108.",
      "citeRegEx" : "Tan et al\\.,? 2015",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2015
    }, {
      "title" : "What is the jeopardy model? a quasisynchronous grammar for qa",
      "author" : [ "Mengqiu Wang", "Noah A Smith", "Teruko Mitamura." ],
      "venue" : "EMNLP-CoNLL, volume 7, pages 22–32.",
      "citeRegEx" : "Wang et al\\.,? 2007",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2007
    }, {
      "title" : "Memory networks",
      "author" : [ "Jason Weston", "Sumit Chopra", "Antoine Bordes." ],
      "venue" : "CoRR, abs/1410.3916.",
      "citeRegEx" : "Weston et al\\.,? 2014",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards ai-complete question answering: A set of prerequisite toy tasks",
      "author" : [ "Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov." ],
      "venue" : "CoRR, abs/1502.05698.",
      "citeRegEx" : "Weston et al\\.,? 2015",
      "shortCiteRegEx" : "Weston et al\\.",
      "year" : 2015
    }, {
      "title" : "Enhancing freebase question answering using textual evidence",
      "author" : [ "Kun Xu", "Yansong Feng", "Siva Reddy", "Songfang Huang", "Dongyan Zhao." ],
      "venue" : "CoRR, abs/1603.00957.",
      "citeRegEx" : "Xu et al\\.,? 2016",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Wikiqa: A challenge dataset for open-domain question answering",
      "author" : [ "Yi Yang", "Wen-tau Yih", "Christopher Meek" ],
      "venue" : null,
      "citeRegEx" : "Yang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2015
    }, {
      "title" : "Semantic parsing via staged query graph generation: Question answering with knowledge base",
      "author" : [ "Wen-tau Yih", "Ming-Wei Chang", "Xiaodong He", "Jianfeng Gao." ],
      "venue" : "July.",
      "citeRegEx" : "Yih et al\\.,? 2015",
      "shortCiteRegEx" : "Yih et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep learning for answer sentence selection",
      "author" : [ "Lei Yu", "Karl Moritz Hermann", "Phil Blunsom", "Stephen Pulman." ],
      "venue" : "CoRR, abs/1412.1632.",
      "citeRegEx" : "Yu et al\\.,? 2014",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "munity thanks to the dataset introduced in (Wang et al., 2007) (which we refer to as wang), with six papers published between February 2015 and 2016 alone and neural models substantially improving over classical approaches based primarily on parse tree edits.",
      "startOffset" : 43,
      "endOffset" : 62
    }, {
      "referenceID" : 33,
      "context" : "Alternative datasets WikiQA (Yang et al., 2015) and InsuranceQA (Tan et al.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 28,
      "context" : ", 2015) and InsuranceQA (Tan et al., 2015) were proposed, but are encumbered by licence restrictions.",
      "startOffset" : 24,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "tion dataset (introduced in (Baudiš and Šedivý, 2015), further denoisified by Mechanical Turkers) with candidate sentences as retrieved by the YodaQA question answering system (Baudiš, 2015) from English Wikipedia and labelled by matching the gold standard answers in the passages.",
      "startOffset" : 28,
      "endOffset" : 53
    }, {
      "referenceID" : 2,
      "context" : "tion dataset (introduced in (Baudiš and Šedivý, 2015), further denoisified by Mechanical Turkers) with candidate sentences as retrieved by the YodaQA question answering system (Baudiš, 2015) from English Wikipedia and labelled by matching the gold standard answers in the passages.",
      "startOffset" : 176,
      "endOffset" : 190
    }, {
      "referenceID" : 34,
      "context" : "This task (Property Selection) can be evaluated identically to the previous task, and solutions often involving Convolutional Neural Networks have been studied in the Question Answering literature (Yih et al., 2015) (Xu et al.",
      "startOffset" : 197,
      "endOffset" : 215
    }, {
      "referenceID" : 32,
      "context" : ", 2015) (Xu et al., 2016).",
      "startOffset" : 8,
      "endOffset" : 25
    }, {
      "referenceID" : 3,
      "context" : "Our sentences have been derived from the WebQuestions dataset (Berant et al., 2013) extended with the moviesE dataset questions (originally introduced in (Baudiš and Šedivý, 2015)); the property paths are based on the Freebase knowledge graph dump, generated based on entity linking and exploration procedure of YodaQA v1.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : ", 2013) extended with the moviesE dataset questions (originally introduced in (Baudiš and Šedivý, 2015)); the property paths are based on the Freebase knowledge graph dump, generated based on entity linking and exploration procedure of YodaQA v1.",
      "startOffset" : 78,
      "endOffset" : 103
    }, {
      "referenceID" : 20,
      "context" : "(Lowe et al., 2015) proposed the new large-scale real-world Ubuntu Dialogue dataset for an f2-style task of ranking candidates for the next utterance in a chat dialog, given the dialog context.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 20,
      "context" : "(Lowe et al., 2015) (Kadlec et al.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 14,
      "context" : ", 2015) (Kadlec et al., 2015)",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : "One of the classic tasks at the boundary of Natural Language Processing and Artificial Intelligence is the inference problem of Recognizing Textual Entailment (Dagan et al., 2006) — given a pair of a factual sentence and a hypothesis sentence, we are to determine whether the hypothesis represents a contradiction, entailment or is neutral (cannot be proven or disproven).",
      "startOffset" : 159,
      "endOffset" : 179
    }, {
      "referenceID" : 21,
      "context" : "The SICK-2014 dataset (Marelli et al., 2014) was introduced as Task 1 of the SemEval 2014 conference and in contrast to SNLI, it is geared at specifically benchmarking semantic compositional methods, aiming to capture only similarities on purely language and common knowledge level, without relying on domain knowledge, and there are no named entities or multi-word idioms; it consists of 4500 training pairs, 500 validation pairs and 4927 testing pairs.",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "ferences (Agirre et al., 2015) and involves scoring pairs of sentences from 0 to 5 with the objective of maximizing correlation (Pearson’s r) with manu-",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 35,
      "context" : "(Yu et al., 2014) or (Weston et al.",
      "startOffset" : 0,
      "endOffset" : 17
    }, {
      "referenceID" : 30,
      "context" : ", 2014) or (Weston et al., 2014)) or theMLPmodule that takes elementwise product and sum of the embeddings and feeds them to a two-layer perceptron with hidden layer of width 2N (as in e.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 27,
      "context" : "(Tai et al., 2015)).",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 27,
      "context" : "8 For the STS task, we follow this by score regression using class interpolation as in (Tai et al., 2015).",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 5,
      "context" : "When training for a ranking task (Answer Sentence Selection), we use the bipartite ranking version of Ranknet (Burges et al., 2005) as the objective; when training for STS task, we use Pearson’s r formula as the objective; for binary classification",
      "startOffset" : 110,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "Weighed word overlaps metrics TF-IDF and BM25 (Robertson et al., 1995) are inspired by IR research and provide strong baselines for many tasks.",
      "startOffset" : 46,
      "endOffset" : 70
    }, {
      "referenceID" : 35,
      "context" : "in (Yu et al., 2014) or (Weston et al.",
      "startOffset" : 3,
      "endOffset" : 20
    }, {
      "referenceID" : 30,
      "context" : ", 2014) or (Weston et al., 2014), simply taking the mean vector of the word embedding sequence and training an U weight matrix N×2N that projects both embeddings to the same vector space, Ei = tanh(U · ēi), where the MLP scorer compares them.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 13,
      "context" : "A simple extension of the above are the DAN Deep Averaging Networks (Iyyer et al., 2015), which were shown to adequately replace much more complex models in some tasks.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 28,
      "context" : "RNN with memory units are popular models for processing sentenes (Tan et al., 2015) (Lowe et al.",
      "startOffset" : 65,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : ", 2015) (Lowe et al., 2015) (Bowman et al.",
      "startOffset" : 8,
      "endOffset" : 27
    }, {
      "referenceID" : 4,
      "context" : ", 2015) (Bowman et al., 2015).",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "We use a bidirectional network with 2N GRU memory units9 (Cho et al., 2014) in each direction; the final unit states are summed across the per-direction GRUs",
      "startOffset" : 57,
      "endOffset" : 75
    }, {
      "referenceID" : 35,
      "context" : "CNN with sentence-wide pooling layer are also popular models for processing sentences (Yu et al., 2014) (Tan et al.",
      "startOffset" : 86,
      "endOffset" : 103
    }, {
      "referenceID" : 28,
      "context" : ", 2014) (Tan et al., 2015) (Severyn and Moschitti, 2015) (He et al.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 26,
      "context" : ", 2015) (Severyn and Moschitti, 2015) (He et al.",
      "startOffset" : 8,
      "endOffset" : 37
    }, {
      "referenceID" : 11,
      "context" : ", 2015) (Severyn and Moschitti, 2015) (He et al., 2015) (Kadlec et al.",
      "startOffset" : 38,
      "endOffset" : 55
    }, {
      "referenceID" : 14,
      "context" : ", 2015) (Kadlec et al., 2015).",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : "We apply a multi-channel convolution (Kim, 2014) with single-token channel of N convolutions and 2, 3, 4 and 5-token channels of N/2 convolutions each, relu transfer function, max-pooling over the whole sentence, and as above a projection to shared space and an MLP scorer.",
      "startOffset" : 37,
      "endOffset" : 48
    }, {
      "referenceID" : 28,
      "context" : "Inspired by (Tan et al., 2015), the aim of this model is to allow the RNN to model long-term dependencies and model contextual representations of words, while taking advantage of the CNN and pooling operation for crisp selection of the gist of the sentence.",
      "startOffset" : 12,
      "endOffset" : 30
    }, {
      "referenceID" : 12,
      "context" : "The idea of attention models is to attend preferrentially to some parts of the sentence when building its representation (Hermann et al., 2015) (Tan et al.",
      "startOffset" : 121,
      "endOffset" : 143
    }, {
      "referenceID" : 28,
      "context" : ", 2015) (Tan et al., 2015) (dos Santos et al.",
      "startOffset" : 8,
      "endOffset" : 26
    }, {
      "referenceID" : 28,
      "context" : "There are many ways to model attention, we adopt the (Tan et al., 2015) model attn1511 as a conceptually simple and easy to implement baseline.",
      "startOffset" : 53,
      "endOffset" : 71
    }, {
      "referenceID" : 8,
      "context" : "To easily implement models, dataset loaders and task adapters in a modular fashion so that any model can be easily run on any f2-type task, we have created a new software package dataset-sts that integrates a variety of datasets, a Python dataset adapter PySTS and a Python library for easy construction of deep neural NLP models for semantic sentence pair scoring KeraSTS that uses the Keras machine learning library (Chollet, 2015).",
      "startOffset" : 418,
      "endOffset" : 433
    }, {
      "referenceID" : 22,
      "context" : "We use N = 300 dimensional GloVe embeddings matrix pretrained on Wikipedia 2014 + Gigaword 5 (Pennington et al., 2014) that we keep adaptable during training; words in the training set not included in the pretrained model are initialized by random vectors uniformly sampled from [−0.",
      "startOffset" : 93,
      "endOffset" : 118
    }, {
      "referenceID" : 35,
      "context" : "Word overlap is an important feature in many f2-type tasks (Yu et al., 2014) (Severyn and Moschitti, 2015), especially when the sentences may contain named entities, numeric or other data for which no embedding is available.",
      "startOffset" : 59,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : ", 2014) (Severyn and Moschitti, 2015), especially when the sentences may contain named entities, numeric or other data for which no embedding is available.",
      "startOffset" : 8,
      "endOffset" : 37
    }, {
      "referenceID" : 16,
      "context" : "We apply 10−4 L2 regularization and use Adam optimization with standard parameters (Kingma and Ba, 2014).",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 14,
      "context" : "We can observe an effect analogous to what has been described in (Kadlec et al., 2015)—when the dataset is smaller, CNNmodels are preferrable, while larger dataset allows RNN models to capture the text comprehension task better.",
      "startOffset" : 65,
      "endOffset" : 86
    }, {
      "referenceID" : 14,
      "context" : "We have tried simple averaging of predictions (as per (Kadlec et al., 2015)), but the benefit was small and inconsistent.",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 31,
      "context" : "A popular instance of sentence pair scoring is the question answering task of the Memory Networks (supported by the baBi dataset) (Weston et al., 2015).",
      "startOffset" : 130,
      "endOffset" : 151
    }, {
      "referenceID" : 19,
      "context" : "A realistic large question Paraphrasing dataset based on the AskUbuntu Stack Overflow forum had been recently proposed (Lei et al., 2015).",
      "startOffset" : 119,
      "endOffset" : 137
    }, {
      "referenceID" : 0,
      "context" : "17 While the tasks of Semantic Textual Similarity (supported by a dataset from the STS track of the SemEval conferences (Agirre et al., 2015)) and Paraphrasing (based on the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) right now) are available within our framework, we do not report the results here as the models lag behind the state-of-art significantly and",
      "startOffset" : 120,
      "endOffset" : 141
    }, {
      "referenceID" : 10,
      "context" : ", 2015)) and Paraphrasing (based on the Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) right now) are available within our framework, we do not report the results here as the models lag behind the state-of-art significantly and",
      "startOffset" : 77,
      "endOffset" : 103
    }, {
      "referenceID" : 28,
      "context" : "(Tan et al., 2015) 0.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 28,
      "context" : "5%) scored better than (Tan et al., 2015).",
      "startOffset" : 23,
      "endOffset" : 41
    }, {
      "referenceID" : 23,
      "context" : "A generalization of our proposed architecture could be applied to the Hypothesis Evidencing task of binary classification of a hypothesis sentence s0 based on a number of memory sentences s1, for example within the MCText (Richardson et al., 2013) dataset.",
      "startOffset" : 222,
      "endOffset" : 247
    }, {
      "referenceID" : 25,
      "context" : "for the RTE task (Rocktäschel et al., 2015), and the skip-thoughts method of sentence embedding.",
      "startOffset" : 17,
      "endOffset" : 43
    }, {
      "referenceID" : 17,
      "context" : "(Kiros et al., 2015)",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : ", 2016) (Cheng et al., 2016) (Kumar et al.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : ", 2016) (Kumar et al., 2015) clearly points towards various forms of attention modelling to remove the bottleneck of having to compress the full spectrum of semantics into a single vector of fixed dimensionality.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 20,
      "context" : "∗ Exact models from (Lowe et al., 2015) reran on the v2 version of the dataset (by personal communication with Ryan Lowe) — note that the results in (Lowe et al.",
      "startOffset" : 20,
      "endOffset" : 39
    }, {
      "referenceID" : 20,
      "context" : ", 2015) reran on the v2 version of the dataset (by personal communication with Ryan Lowe) — note that the results in (Lowe et al., 2015) and (Kadlec et al.",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 14,
      "context" : ", 2015) and (Kadlec et al., 2015) are on dataset v1 and not directly comparable.",
      "startOffset" : 12,
      "endOffset" : 33
    }, {
      "referenceID" : 27,
      "context" : "(Tai et al., 2015) 0.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 4,
      "context" : "(Bowman et al., 2015) LSTM 1.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "(Bowman et al., 2015) Tran.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 6,
      "context" : "(Cheng et al., 2016) 0.",
      "startOffset" : 0,
      "endOffset" : 20
    } ],
    "year" : 2016,
    "abstractText" : "We review the task of Sentence Pair Scoring, popular in the literature in various forms — viewed as Answer Sentence Selection, Semantic Text Scoring, Next Utterance Ranking, Recognizing Textual Entailment, Paraphrasing or e.g. a component",
    "creator" : " XeTeX output 2016.05.06:2355"
  }
}