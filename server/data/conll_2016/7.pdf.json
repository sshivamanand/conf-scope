{
  "name" : "7.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Random Positive-Only Projections: PPMI-Enabled Incremental Semantic Space Construction",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "The development of data-driven methods of natural language processing starts with an educated guess, a distributional hypothesis: We assume that some properties of linguistic entities can be modelled by ‘some statistical’ observations in language data. In the second step, this statistical information (which is determined by the hypothesis) is collected and represented in a mathematical framework. In the third step, tools provided by the chosen mathematical framework are used to implement a similarity-based logic to identify linguistic structures, and/or to verify the proposed\nhypothesis. Harris’s distributional hypothesis is a well-known example of step one that states that meanings of words correlate with the context in which the words appear. Vector space models and η-normed-based similarity measures are renowned examples for steps two and three, respectively (i.e., word space models or word embeddings).\nHowever, as pointed out for instance by Baroni et al. (2014), the count-based models resulting from the steps two and three are not discriminative enough to achieve satisfactory results; instead, predictive models are required. To this end, an additional transformation step is often added. Turney and Pantel (2010) describe this additional step as a combination of weighting and dimensionality reduction.1 This transformation from count-based to predictive models can be implemented simply via a collection of rules of thumb (i.e., heuristics), and/or it can involve more sophisticated mathematical transformations, such as converting raw counts to probabilities and using matrix factorization techniques. Likewise, by exploiting large amounts of computational power available nowadays, this transformation can be achieved via neural word embedding techniques (Mikolov et al., 2013; Levy and Goldberg, 2014).\nTo a large extend, the need for such transformations arises from the heavy-tailed distributions that we often find in statistical natural language models (such as the Zipfian distribution of words in contexts when building word spaces). Consequently, count-based models are sparse and highdimensional and therefore both computationally expensive to manipulate (due of the high dimensionality of models) and nondiscriminatory (due to the combination of the high-dimensionality of the models and the sparseness of observations, see Minsky and Papert (1969, chap. 12)).\n1Similar to topics of feature weighting, selection, and engineering in statistical machine learning.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nOn the one hand, although neural networks are the top performers for addressing this problem, their usage is highly costly: they need to be trained that is often very time-consuming,2 and their performance can vary from one task to another depending on their objective function.3 On the other hand, although methods based on random projections efficiently address the problem of reducing the dimensionality of vectors—such as RI (Kanerva et al., 2000), reflective random indexing (RRI), (Cohen et al., 2010), ISA (Baroni et al., 2007) and random Manhattan indexing (RMI) (Zadeh and Handschuh, 2014)—in effect they retain distances between entities in the original space.4 In addition, since these methods use asymptotic Gaussian or Cauchy random projection matrices R with E(R) = 0, their resulting vectors cannot be adjusted and transformed using weighting techniques such as PPMI. Hence these methods do not outperform neural embeddings and combinations of PPMI weighting of count-baseds model followed by matrix factorization—such as truncation of weighted-vectors using singular value decomposition (SVD).\nIn order to overcome these problems, we propose a new method called positive-only projection (PoP). PoP is an incremental semantic space construction method that is similar to RI in the sense that it employs random projections. Hence, the construction of models using PoP does not require prior computation of embeddings but simply generating random vectors. However, in contrast to RI and similar methods, the PoP-constructed spaces can undergo weighting transformations such as PPMI. This is due to the fact that PoP uses random vectors that contain only positive integer values. Since the method is based on random projections, models can be built incrementally and efficiently. Since the vectors in PoP-constructed models are small (i.e., dimensionality of a few hundred), applying weighting methods such as PPMI to these models is incredibly faster than applying them to classical count-based models. Combined with a suitable weighting method such PPMI, the PoP method yields competitive results concern-\n2Baroni et al. (2014) state that it took Ronan Collobert two months to train a set of embeddings from a Wikipedia dump. Even using GPU-accelerated computing, the required computation and training time for inducing neural word embeddings is high.\n3Ibid, see results reported in supplemental materials. 4For η-normed space that they are designed for, i.e., η = 2\nfor RI, RRI, and ISA and η = 1 for RMI.\ning accuracy in semantic similarity assessment, compared for instance to neural net-based approaches and combinations of count-based models with weighting and matrix factorization. These results, however, are achieved without the need for heavy computations. Thus, instead of hours, days or months, models can be built in a matter of a few seconds or minutes. Note that even without weighting transformation, PoP-constructed models display a better performance than RI on tasks of semantic similarity assessments.\nWe describe the PoP method in § 2. In order to evaluate our models, in § 3, we report the performance of the PoP method in the MEN relatedness test. Finally, § 4 concludes with a discussion."
    }, {
      "heading" : "2 Method",
      "text" : ""
    }, {
      "heading" : "2.1 PoP-based Model Construction",
      "text" : "Any transformation from a count-based model to a predictive one can be expressed using a matrix notation such as:\nCp×n ×Tn×x = Pp×x. (1)\nIn Equation 1, C denotes the count-based model consisting of p vectors and n context elements (i.e., n dimensions). T is the transformation matrix that maps the p n-dimensional vectors in C to an x-dimensional space (often, but not necessarily, x 6= n and x n). Finally, P is the resulting x-dimensional predictive model. Note that T can be a composition of several transformations, e.g., a weighting transformation W followed by a projection onto a space of lower dimensionality R, i.e., Tn×x = Wn×n ×Rn×x.\nIn the proposed PoP technique, the transformation Tn×m (for m n, e.g., 100 ≤ m ≤ 5000) is simply a randomly generated matrix. The elements tij of Tn×m have the following distribution:\ntij = { 0 with probability 1− s b 1Uα c with probability s , (2)\nin which U is an independent uniform random variable in (0, 1], s is an extremely small number (e.g., s = 0.01) such that each row vector of T has at least one element that is not 0 (i.e.,∑m\ni=1 tji 6= 0 for each row vector tj ∈ T). For α, we choose α = 0.5. Given Equations 1 and 2 and using the distributive property of multiplication over addition in matrices,5 the desired semantic space (i.e., P in Equation 1) can be constructed\n5That is (A+B)×C = A×C+B×C.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nusing the two-step procedure of incremental word space construction known from RI:\nStep 1. Each context element is mapped to one m-dimensional index vector ~r. ~r is randomly generated such that most elements in ~r are 0 and only a few are positive integers (i.e., the elements of ~r have the distribution given in Equation 2).\nStep 2. Each target entity that is being analysed in the model is represented by a context vector ~v in which all the elements are initially set to 0. For each encountered occurrence of this target entity together with a context element (e.g., through a sequential scan of a corpus), we update ~v by adding the index vector ~r of the context element to it.\nThis process results in a model built directly at the reduced dimensionality m (i.e., P in Equation 1). The first step corresponds to the construction of the randomly generated transformation matrix T: Each index vector is a row of the transformation matrix T. The second step is an implementation of the matrix multiplication in Equation 1 which is distributed over addition: Each context vector is a row of P, which is computed in an iterative process."
    }, {
      "heading" : "2.2 Measuring Similarity",
      "text" : "Once P is constructed, if desirable, similarities between entities can be computed by their Kendall’s τb (−1 ≤ τb ≤ 1) correlation (Kendall, 1938). In order to compute τb, we need to define a number of values. Given vectors ~x and ~y of the same dimension, we call a pair of observations (xj , yj) and (xj+1, yj+1) in ~x and ~y concordant if (xj < xj+1 ∧ yj < yj+1) ∨ (xj > xj+1 ∧ yj > yj+1). The pair is called discordant if (xj < xj+1∧ yj > yj+1) ∨ (xj > xj+1 ∧ yj < yj+1). Finally, the pair is called tied if xj = xj+1 ∨ yj = yj+1. Note that a tied pair is neither concordant nor discordant. We define n1 and n2 as the number of pairs with tied values in ~x and ~y, respectively. We use nc and nd to denote the number of concordant and discordant pairs, respectively. If m is the dimension of the two vectors, then n0 is defined as the total number of observation pairs: n0 = m(m−1) 2 . Given these definitions, Kendall’s τb is given by\nτb = nc − nd√\n(n0 − n1)(n0 − n2) .\nTo compute τb, we adopt an implementation of the algorithm proposed by Knight (1966), which has\na computational complexity of O(n log n).6 Since the vectors resulting from the PoP method have a very low dimensionality, this complexity does not harm the overall efficiency of the approach.\nThe choice of τb is motivated by generalising the role that cosine plays for computing similarities between vectors that are derived from a standard Gaussian random projection. In random projections with R of (asymptotic) N (0, 1) distribution, despite the common interpretation of the cosine similarity as the angle between two vectors, cosine can be seen as a measure of productmoment correlation coefficient between the two vectors. Since R and thus the obtained projected spaces have E = (0), Pearson’s correlation and the cosine measure have the same definition in these spaces (see also Jones and Furnas (1987) for a similar claim and on the relationships between correlation and the inner product and cosine). Subsequently, one can propose that in Gaussian random projections, Pearson’s correlation is used to compute similarities between vectors.\nHowever, the use of projections proposed in this paper (i.e., T with a distribution set in Equation 2) will result in vectors that have a non-Gaussian distribution. In this case, τb becomes a reasonable candidate for measuring similarities (i.e., correlations between vectors) since it is a nonparametric correlation coefficient measure that does not assume a Gaussian distribution of projected spaces. However, we do not exclude the use of other similarity measures and may apply them in future work. In particular, we envisage additional transformations of PoP-constructed spaces to induce vectors with Gaussian distributions (see for instance the log-based PPMI transformation used in the next section). If a transformation to a Gaussian-like distribution is performed, then the use of Pearson’s correlation, which works under the assumption of Gaussian distribution, yields better results than Kendall’s correlation (as confirmed by our experiments)."
    }, {
      "heading" : "2.3 Some Delineation of the PoP Method",
      "text" : "The PoP method is a randomized algorithm. In this class of algorithms, at the expense of a tolerable loss in accuracy of the outcome of the computations (of course, with a certain acceptable amount of probability) and by the help of ran-\n6In our evaluation, we use the implementation of Knight’s algorithm in the Apache Commons Mathematics Library.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\ndom decisions, the computational complexity of algorithms for solving a problem is reduced (see, e.g., Karp (1991), for an introduction to randomized algorithms).7 For instance, using Gaussianbased sparse random projections in RI, the computation of eigenvectors (often of complexity of O(n2 logm)) is replaced by a much simpler process of random matrix construction (of an approximate complexity of O(n))—see (Bingham and Mannila, 2001). In return, a randomized algorithm such as the RI and PoP methods give different results even for the same input.\nAssume the difference between the optimum result and the result from a randomized algorithm is given by δ (i.e., the error caused by replacing deterministic decisions with random ones). Much research in theoretical computer science and applied statistics focuses on specifying bounds for δ, which is often expressed as a function of the probability of encountered errors. For instance, δ and in Gaussian random projections are often derived from the lemma proposed by Johnson and Lindenstrauss (1984) and its variations. Similar studies for random projections in `1-normed spaces and deep neural networks are Indyk (2000) and Arora et al. (2014), respectively.\nAt this moment, unfortunately, we are not able to provide a detailed mathematical account for specifying δ and for the results obtained by the PoP method (nor are we able to pinpoint theoretical discussion about PoP’s underlying random projection). Instead, we rely on the outcome of our simulations and the performance of the method in an NLP task. Note that this is not an unusual situation. For instance, Kanerva et al. (2000) proposed RI with no mathematical justification. In fact, it was only a few years later that Li et al. (2006) proposed mathematical lemmas for justifying very sparse Gaussian random projections such as RI (QasemiZadeh, 2015). At any rate, projection onto manifolds is a vibrant research both in theoretical computer science and in mathematical statistics. Our research will benefit from this in the near future. Concerning δ, it can be shown that it and its variance σ2δ are functions of the dimension m of the projected space, that is: σ2δ ≈ 1 m , based on similar mathematical principles proposed by Kaski (1998) (and of Hecht-Nielsen (1994)) for random mapping methods.\n7Such as many classic search algorithms that are proposed for solving NP-complete problems in artificial intelligence.\nOur empirical research and observations on language data show that projections using the PoP method exhibit similar behavioural patterns as other sparse random projections in α-normed spaces. The dimensionm of random index vectors can be seen as the capacity of the method to memorize and distinguish entities. Form up to a certain number (100 ≤ m ≤ 4000) in our experiments, as was expected, a PoP-constructed model for a large m shows a better performance and smaller δ than a model for a small m. Since observations in semantic spaces have a very-long-tailed distribution, choosing different values of non-zero elements for index vectors does not effect the performance (as mentioned, in most cases 2 or 3 non-zero elements are sufficient). Furthermore, changes in the adopted distribution of tij only slightly effect the performance of the system, due to the use of τb as a similarity measure.\nIn the next section, using empirical investigations we show the advantages of the PoP model and support the claims from this section."
    }, {
      "heading" : "3 Evaluation & Empirical Investigations",
      "text" : ""
    }, {
      "heading" : "3.1 Comparing PoP and RI",
      "text" : "For evaluation purposes, we use the MEN relatedness test set (Bruni et al., 2014) and the UKWaC corpus (Baroni et al., 2009). The dataset consists of 3000 pairs of words (from 751 distinct tagged lemmas). Similar to other ‘relatedness tests’, Spearman’s rank correlation ρ score from the comparison of human-based ranking and system-induced rankings is the figure of merit. We use these resources for evaluation since they are in public domain, both the dataset and corpus are large, and they have been used for evaluating several word space models—for example, see Levy et al. (2015), Tsvetkov et al. (2015), Baroni et al. (2014), Kiela and Clark (2014). In this section, unless otherwise stated, we use cosine for similarity measurements.\nFigure 1 shows the performance of the simple count-based word space model for lemmatizedcontext-windows that extend symmetrically around lemmas from MEN.8 As expected, up to a certain context-window size, the performance using count-based methods increases with an\n8We use the tokenized preprocessed UKWaC. However, except for using part-of-speech tags for locating lemmas listed in MEN, we do not use any additional information or processes (i.e., no frequency cut-off for context selection, no syntactic information, etc.).\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nextension of the window.9 For context-windows larger than 25+25 the performance gradually declines. More importantly, in all cases we have ρ < 0.50.\nWe performed the same experiments using the RI technique. For each context window size, we performed 10 runs of the RI model construction. Figure 1 reports for each context-window size the average of the observed performances for the 10 RI models. In this experiment, we used index vectors of dimensionality 1000 containing 4 nonzero elements. As shown in Figure 1, the average performance of the RI is almost identical to the performance of the count-based model. This is an expected result since RI’s objective is to retain Euclidean distances between vectors (thus cosine) but in spaces of lowered dimensionality. In this sense, RI is successful and achieves its goal of lowering the dimensionality while keeping distances between vectors. But it does not yield any improvements in the similarity assessment task.\nWe then performed similar experiments using PoP-constructed models, with the same context window sizes and the same dimensions as in the RI experiments, averaging again over 10 runs for each context window size. The performance is also reported in Figure 1. For the PoP method, however, instead of using the cosine measure we use τb for measuring similarity. The PoP-constructed models converge faster than RI and count-based method and for smaller context-windows they outperform the count-based and RI methods with a large margin. However, as the size of the windows grow, performances of these methods become more similar (but PoP still outperforms the others). One possible interpretation is that PoP is more sensitive to noise; if that is the case, this can be addressed by increasing the dimensionality of index vectors in order to reduce distortions in the projected spaces. In any case, the performance of PoP remains above 0.50 (i.e., ρ > 0.50)."
    }, {
      "heading" : "3.2 PPMI Transformation of PoP Vectors",
      "text" : "Although PoP outperforms RI and count-based models, its performance is still not satisfying since its index vectors are not sufficiently discriminative. In order to remedy this, transformations based on association measures such as positive pointwise mutual information (PPMI) have been\n9After all, in models for relatedness tests, relationships of topical nature play a more important role than other relationships such as synonymy.\n1+ 1 4+ 4\n9+ 9\n17 +1 7\n0.65\n0.7\n0.75\nContext Window Size\nSp ea\nrm an\n’s C\nor re\nla tio\nn ρ\nPMI-Dense+Cos PPMI-Dense+Cos\nOur PoP+PPMI+Kendall Our PoP+PPMI+Pearson\nFigure 2: Performances of (P)PMI-transformed models for various sizes of context-windows. From context size 4+4, the performance remains almost intact (0.72 for PMI and 0.75 for PPMI). We also report the average performance for PoP-constructed models constructed at the dimensionality m = 1000 and s = 0.002. PoP+PPMI+Pearson exhibits a performance similar as dense PPMI-weighted models, however, much faster and using far less amount of computational resources. Note that reported PoP+PMI performances can be enhanced by using m > 1000.\nproposed for the adjustment of weights (which correspond to coordinates of vectors). This often enhances the discriminatory power of the models and thus increases their performance in semantic similarity assessment tasks (see Church and Hanks (1990), Turney (2001), Turney (2008), and Levy et al. (2015)). For a given set of vectors, PMI is interpreted as a measure of information overlap between vectors. As put by Bouma (2009), PMI is a mathematical tool for measuring how much the actual probability of a particular co-occurrence (e.g., two words in a word space) deviate from the expected probability of their individual occurrences (e.g., the probability of occurrences of each word in a words space) under the assumption of independence (i.e., the occurrence of one word does not affect the occurrences of other words).\nIn Figure 2, we show the performance of PMI-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\ntransformed spaces for small context-windows. One major problem with PMI is that it requires a lot of resources for its computation (as put by, PMI-induced spaces are dense). Even for a small number of entities in the model, due to the powerlaw distribution of word co-occurrences, the resulting spaces from large context-windows are high-dimensional so that the computation of PMI weights becomes intractable. But still, even for small context-windows, PMI+Cosine models outperform other techniques including the introduced count-based PoP method. The performance of PMI models can be further enhanced by its normalization, often discarding negative values10 and using Positive PMI values (PPMI). Also, SVD truncation of PPMI-weighted spaces can improve the performance slightly (see the above mentioned references) requiring, however, expensive computations of eigenvectors.11 For a p× n matrix with elements vxy, 1 ≤ x ≤ p and 1 ≤ y ≤ n, we compute the PPMI weight for a component vxy as follows:\nppmi(vxy) = max(0, log vxy×\n∑p i=1 ∑n j=1 vij∑p\ni=1 viy× ∑n j=1 vxj ). (3)\nThe most important benefit of the PoP method is that PoP-constructed models, in contrast to previously suggested random projection-based models, can be still weighted using PPMI (or any other weighting techniques applicable to the original count-based models). In an RI-constructed model, the sum of values of row and column vectors of the model are always 0 (i.e., ∑p i=1 viy and ∑n j=1 vxj in Equation 3 are always 0). As mentioned earlier, this is due to the fact that a random projection matrix in RI has an asymptotic standard Gaussian distribution (i.e., transformation matrix R has E(R) = 0). As a result, PPMI weights for the RIinduced vector elements are undefined. In contrast to RI, the sum of values of vector elements in the PoP-constructed models is always greater than 0 (since the transformation is carried out by a projection matrix R of E(R) > 0). Also, depending on the structure of data in the underlying countbased model, by choosing a suitably large value of s, it can be guaranteed that the sum of column vectors is always a non-zero value. Hence, vectors in PoP models can undergo the PPMI transformation defined in Equation 3. Moreover, the PPMI\n10See Bouma (2009) for a mathematical delineation. Jurafsky and Martin (2015) also provide an intuitive description.\n11In our experiments, applying SVD truncation to models results in negligible improvements between 0.01 and 0.001.\ntransformation is much faster, compared to the one performed on count-based models, due to the low dimensionality of vectors in the PoP-constructed model. Therefore, the PoP method makes it possible to benefit both from the high efficiency of randomized techniques as well as from the high accuracy of PPMI transformation in semantic similarity tasks.\nIf we put aside the information-theoretic interpretation of PPMI weighting (i.e., distilling statistical information that really matters), the logarithmic transformation of probabilities in the PPMI definition plays the role of a power transformation process for converting long-tailed distributions in count-based models to Gaussian-like distributions in predictive models. From a statistical perspective, any variation of PMI transformation can be seen as an attempt to stabilize variance of vector coordinates and therefore to make the observations more similar/fit to Gaussian distribution (a practice with long history in many research, particularly in biological and psychological sciences).\nTo exemplify this phenomenon, in Figure 3, we show histograms of the distributions of the assigned weights to the vector that represents the lemmatized form of the verb ‘abandon’ in various models. As shown, the raw collected frequencies in the count-based model have a long tail distribution (see Figure 3a). Applying the log transformation to this vector yields a vector of weights with a Gaussian distribution (Figure 3b). Weights in the RI-constructed vector (Figure 3c) have a perfect Gaussian distribution but with expected value of 0 (i.e., N (0, 1)). The PoP method, however, largely preserves the long tail distribution of coordinates from the original space (Figure 3d), which in turn can be weighted using PPMI and thereby transformed into a Gaussian-like distribution.\nGiven that models after PPMI transformation have bell-shaped Gaussian distributions, we expect that a correlation measure such as Pearson’s r, which takes advantage of the prior knowledge about the distribution of data, outperforms the non-parametric Kendall’s τb for computing similarities in PPMI-transformed spaces.12 This is indeed the case (see Figure 2).\n12Note that using correlation measures such as Pearson’s r and Kendall’s τb in count-based model may excel measures such as cosine. However, their application is limited due to the high-dimensionality of count-based methods.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n100 10,000 1\n10\n100\n1,000\nlog(Raw Frequencies)\nlog(Frequency)\n(a) Count-based\n0 50\n100\n200\n300\n400\nPMI Weights\nFrequency\n(b) PMI\n−1,700 0 1,700\n20\n40\nRI Weights\nFrequency\n(c) RI\n10 1,000 100,000\n10\nlog(POP Weights)\nlog(Frequency)\n(d) POP\n−50 0 50 100\n100\n200\n300\n400\nPOP PMI Weights\nFrequency\n(e) POP+PMI\nFigure 3: A histogram of the distribution of frequencies of weights in various models from 1+1 contextwindows for the lemmatized form of the verb ‘abandon’ in the UKWaC corpus.\n10 0 50 0 10 00 15 00 20 00\n30 00\n40 00\n50 00\n0.5\n0.55\n0.6\n0.7\n0.75\nModels’ Dimensionality (m)\nSp ea\nrm an\n’s C\nor re\nla tio\nn ρ\nPoP+PPMI+Pearson PoP+PPMI+ Kendall\nPoP+Kendall\nFigure 4: Changes in PoP’s performance when the dimensionality of models increases. The average performance in each set-up is shown by marked lines. The margins around these lines show the minimum and maximum performance observed in 10 independent executions."
    }, {
      "heading" : "3.3 PoP’s Parameters and its Random Behaviour and Performance",
      "text" : "As discussed in § 2.3, PoP is a randomized algorithm and its performance is influenced by a number of parameters. In this section, we study the PoP method’s behaviour by reporting its performance in the MEN relatedness test under different parameter settings. To keep evaluations and reports in a manageable size, we focus on models built using context-windows of size 4+4.\nFigure 4 shows the method’s performance when the dimensionm of the projected index vectors increases. In these experiments, index vectors are built using 4 non-zero elements; thus, as m increases, s in Equation 2 decreases. For each m, 100 ≤ m ≤ 5000, the models are built 10 times and the average as well as the maximum and the minimum observed performances in these experiments are reported. For PPMI transformed PoP spaces, with increasing dimensions, the performance boosts and, furthermore, the variance in performance (i.e., the shaded areas)13 gets smaller.\n13Evidently, the probability of worst and best performances can be inferred from the reported average results.\n1 2 4 8 16 32 0.5\n0.55\n0.6\n0.7\n0.75\n1\nNumber of Non-Zero Elements in Index Vectors\nSp ea\nrm an\n’s C\nor re\nla tio\nn ρ\nPoP+PPMI+Pearson PoP+PPMI+ Kendall\nPoP+Kendall\nFigure 5: Changes in PoP’s performances when the dimensionality of models are fixed to m = 3000 and the number of non-zero elements in index vectors (i.e., s) increases. The average performances in each set-up are shown by marked lines. The margins around these lines show the minimum and maximum performance observed in 10 independent executions.\nHowever, for the count-based PoP method without PPMI transformation (shown by dash-dotted lines) and with the number of non-zero elements fixed to 4, increasing m over 2000 decreases the performance. This is unexpected since an increase in dimensionality is usually assumed to entail an increase in performance. This behaviour, however, can be the result of using a very small s; simply put, the number of non-zero elements are not sufficient to build projected spaces with adequate distribution. To investigate this matter, we study the performance of the method with the dimension m fixed to 3000 but with index vectors built using different numbers of non-zero elements, i.e., different values of s.\nFigure 5 shows the observed performances. For PPMI-weighted spaces, increasing the number of non-zero elements clearly deteriorates the performance. For unweighted PoP models, an increase in s up to the limit that does not result in nonorthogonal index vectors enhances performances. As shown in Figure 6, when the dimensionality\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n4 6 8 10 12 14 16 18 20 0\n0.2\n0.4\n0.6\n0.8\n1\nnon-zero elements\nP 6⊥\n#index vectors n = 104\n1 4\n·104n\n#non-zero elements = 8\nm = 100 m = 1000 m = 2000\nFigure 6: Proportion of non-orthogonal pairs of index vectors (i.e., P6⊥) obtained in a simulation for various dimensionality and number of non-zero elements. The left figure shows the changes of P 6⊥ For a fixed set of index vectors n = 104 when number of non-zero elements increases. The right figure shows P6⊥ when the number of non-zero elements is fixed to 8 but the number of index vectors n increases. As shown, P 6⊥ is determined by the number of non-zero elements and dimensionality of index vectors and independently of n.\nof the index vectors is fixed and s increases, the chances of having non-orthogonal vectors in index vectors boosts. Hence, the chances of distortions in similarities increase. These distortions can enhance the result if they are controlled (e.g., using a training procedure such as the one used in neural net embedding). However, when left to chance, they can often lower the performance. Evidently, this is a simplified justification: in fact s plays the role of a switch that controls resemblance between distribution of data in the original space and the projected/transformed spaces. It seems that the sparsity of vectors in the original matrix plays a role for finding the optimal value for s. If PoP-constructed models are used directly (together with τb) for computing similarities, then we propose 0.002 < s. If PoP-constructed models are subject to an additional weighting process for stabilising vector distributions into Gaussianlike distributions such as PPMI, we propose using only 1 or 2 non-zero elements.\nLast but not least, we confirm that by carefully selecting context elements (i.e., removing stop words and using lower and upper bound frequency cut-offs for context selection) and fine tuning PoP+PPMI+Pearson (i.e., increasing the dimension of models and scaling PMI weights as in Levy et al. (2015)) we achieve an even higher score in the MEN test (i.e., an average of 0.78 with the max of 0.787). Moreover, although improvements from applying SVD truncation are negligible, we can employ it for reducing the dimensionality of PoP vectors (e.g., from 6000 to 200)."
    }, {
      "heading" : "4 Discussion",
      "text" : "We introduced a new technique called PoP for incremental construction of semantic spaces. PoP can be seen as a dimensionality reduction method, which is based on a newly devised random projection matrix R of E(R). The major benefit of PoP is that it transfers vectors onto spaces of lower dimensionality without changing their distribution to a Gaussian shape with E = 0. Transformed spaces obtained using PoP can therefore be manipulated similarly to count-based models, only much faster and consequently requiring a considerably lower amount of computational resources.\nPPMI weighting can be easily applied to POP-constructed models. In our experiments, we observe that PoP+PPMI+Pearson can be used to build models that achieve a high performance in semantic relatedness tests. More concretely, for index vector dimensions m ≥ 3000, PoP+PPMI+Pearson achieves an average score of 0.75 in the MEN relatedness test, which is comparable to many neural embedding techniques (see scores reported in Chen and de Melo (2015) and Tsvetkov et al. (2015)). However, in contrast to these approaches, PoP+PPMI+Pearson achieves this competitive performance without the need for time-consuming training of neural nets. Moreover, the involved processes are all done on vectors of low dimensionality. Hence, the PoP method can dramatically enhance the performance of a number of distributional natural language analyses.\nThis research can be extended in several ways. Firstly, theoretical accounts for PoP still need to be investigated. Secondly, many new methods can be designed by combining the proposed PoP and serialization of various weighting techniques (such as previously done for RI). Moreover, since weighting processes can be applied at a very low dimensionality, many computationallyintense techniques which cannot be applied to high-dimensional models for stabilising distributions of vectors (e.g., the use of power transforms such as BoxCox that maximizes the correlation coefficient of a Gaussian distribution of the weighted vectors) are now available to be used. On a similar basis, in the process of developing neural embeddings, PoP-constructed models can replace the high-dimensional count-based models as an input. Lastly, an important avenue of research is the use of so-called derandomization techniques to make PoP models linguistically more informed.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    }, {
      "heading" : "A Supplemental Material",
      "text" : "If the paper accepted, codes and resulting embeddings from experiments will be shared alongside the camera ready version."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "We introduce positive-only projection (PoP), a novel technique for constructing semantic spaces and word embeddings. The PoP method is based on random projections. Hence, it is highly scalable and computationally efficient. In contrast to previous methods that use random projection matrices R with the expected value of 0 (i.e., E(R) = 0), the proposed method uses R with E(R) > 0. We use Kendall’s τb distance to compute vector similarities in the resulting non-Gaussian spaces. Most importantly, since E(R) > 0, weighting methods such as positive pointwise mutual information (PPMI) can be applied to PoP-constructed spaces after their construction for efficiently transferring PoP embeddings onto spaces that are discriminative for semantic similarity assessments. Our PoP-constructed models, combined with PPMI, achieve an average score of 0.75 in the MEN relatedness test, which is comparable to results obtained by state-ofthe-art top-performing algorithms.",
    "creator" : "LaTeX with hyperref package"
  }
}