{
  "name" : "137.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "When a Red Herring is Not a Red Herring: Using Compositional Methods to Improve the Detection of Non-Compositional Phrases",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "In recent years, distributional representations of words have received a lot of interest. In many applications, the ability to cluster or find similar words in terms of their distribution in text or their hypothesised semantic similarity has a massive potential to reduce the sparse data problem. Early research in the field (Hindle, 1990; Grefenstette, 1994; Lin, 1998; Lee, 1999; Curran, 2004; Weeds and Weir, 2005; Padó and Lapata, 2007) investigated distributional representations which were built directly from corpus cooccurrence counts; such representations are now commonly referred to as count or explicit vector representations. The models considered varied in terms of the use of different association functions (Curran, 2004), the use of different similarity measures (Lee, 1999; Weeds and Weir, 2005) and whether to define context in terms of proximity or grammatical dependencies (Padó and Lapata, 2007). More recently the trend has been towards using neural models (Mikolov et al., 2013;\nPennington et al., 2014) to create dense, low dimensional representations, commonly referred to as word embeddings, built by training the models to predict corpus co-occurrences. As has been noted elsewhere (Pennington et al., 2014), whilst appearing at first sight very different, both count-based methods and prediction-based methods have in common the fact that they probe the underlying co-occurrence statistics in the corpus. In fact, Levy and Goldberg (2014) demonstrated that the skip-gram model with negative sampling (SGNS) proposed by Mikolov et al. (2013) is an implicit factorisation of the positive pointwise mutual information (PPMI) matrix commonly used in count-based methods.\nOne current focus within the field of distributional semantics is enabling systems to make inferences about phrase-level or sentence-level similarity. One approach (Turney, 2012) is to model the similarity of two phrases or sentences as a function of word-level similarities. An alternative approach (Mitchell and Lapata, 2010) is to build phrase or sentence-level representations by composing word-level representations and then measuring similarity directly. However, whilst this second approach has proved popular, success, usually measured in terms of correlation with human similarity judgments, has been limited. For example, Dinu et al. (2013) reported results for a number of state-of-the art compositional methods on a number of phrase-based benchmark tasks. In those experiments, correlation with human judgements for intransitive sentences (Mitchell and Lapata, 2008) does not exceed 0.30.\nHowever, evaluating measures of phrase-level similarity directly against human judgments of similarity ignores the problem that it is not always possible to determine meaning in a compositional manner. If we compose the meaning representations for red and herring, we would ex-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\npect to get a very different representation from the one which could be directly inferred from corpus observations of the phrase red herring. This would undoubtedly have an impact on our similarity measurements with other words and phrases. Whilst it should be possible to construct evaluation datasets which avoid clearly idiomatic phrases such as red herring, non-compositionality is an integral part of language which should not be ignored (Sag et al., 2002). Further, McCarthy et al. (2003) noted that the compositionality of a phrase should not be judged categorically, rather it should be viewed as existing on a continuum or a scale. Thus any judgements of the similarity of two composed phrases are confounded by the degree to which those phrases are compositional.\nReddy et al. (2011) introduced a new dataset of 90 compound nouns together with human judgments of compositionality. Using this benchmark, compositional methods can be evaluated by correlating the similarity of composed and observed (or holistic) phrase representations with human judgments of compositionality.\nIn this paper, we use this dataset to investigate the extent to which the underlying definition of context has an effect on a model’s ability to support composition. In particular, we distinguish between typed and untyped contextual features. For example, in traditional count-based models, contextual features based on proximity are usually untyped whereas contextual features based on dependency relations may be typed (i.e., include the name of the dependency relation) or untyped (Baroni and Lenci, 2010). Further, following Weeds et al. (2014) we investigate the use of higher order dependency paths in contextual features in order to support composition in typed vector space models. We compare these models, where composition is an integral part of the distributional model, with the commonly employed approach of applying naı̈ve compositional operations to state-of-theart distributional representations."
    }, {
      "heading" : "2 Related Work",
      "text" : "In distributional semantic models, composition has typically been tackled by taking vector representations for words (Turney and Pantel, 2010) and combining them using some function to produce a data structure that represents the phrase or sentence. The simplest functions which can be used in composition are pointwise addition\nand multiplication. From a linguistic perspective, these are naı̈ve since they are commutative, completely ignoring the order and structure of the phrase or sentence. However, Mitchell and Lapata (2008, 2010) found that additive and multiplicative functions applied to proximity-based vector representations were no less effective than more complex functions when performance was assessed against human similarity judgements of simple paired phrases. More recently, Berant and Liang (2014) achieved state-of-the-art results in question-answering where the paraphrase model included a vector space model component. This vector space model component carried out composition by adding neural word embeddings obtained with the word2vec tool (Mikolov et al., 2013).\nOver the past 6 years, other more linguistically motivated models of composition have been proposed e.g., the full additive model (Guevara, 2010), the lexical function method (Baroni and Zamparelli, 2010), the full lexical model (Socher et al., 2012) and various tensor methods (Coecke et al., 2011; Grefenstette et al., 2013). These methods all share the idea, taken from formal semantics, of function application derived from syntactic structure. In an evaluation across 3 different benchmark tasks (Dinu et al., 2013), the lexical function model was shown to be consistently the best-performing. However, in the composition of adjective-noun phrases, the simple additive and multiplicative models were still shown to be highly competitive.\nMilajevs et al. (2014) compared neural word representations with count-based vectors in a variety of tasks using a variety of naı̈ve and tensorbased compositional models. Across 4 different tasks (word sense disambiguation, sentence similarity, paraphrase detection and dialogue act tagging), the neural word representations consistently outperformed the traditional count-based vectors. However, as concluded by the authors, this may well be due to differences in the size and nature of the corpora from which the different representations were obtained. Considering the results for the neural word representations, pointwise addition outperformed all of the other compositional models considered on 3 out of the 4 tasks. Tensorbased composition performed better than pointwise addition on just the verb disambiguation task, where the authors argue that verb senses depend strongly on the arguments of the verb.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nHashimoto et al. (2014) integrated a variety of syntactic and semantic dependencies into neural models in order to jointly learn composition functions and word representations. Whilst these models are well-motivated and achieved some state-ofthe art results on the Mitchell and Lapata (2010) phrase similarity task, the baseline of adding standard neural word embeddings produced by the word2vec tool proved particularly hard to beat.\nHermann et al. (2012) proposed using generative models for modeling the compositionality of noun-noun compounds. Using interpolation to mitigate the sparse data problem, their model beat the baseline of weighted addition on the Reddy et al. (2011) evaluation task when trained on the BNC. However, these results were still siginificantly lower than those reported by Reddy et al. (2011) using the larger ukWaC corpus."
    }, {
      "heading" : "3 Composition via Anchored Packed Trees",
      "text" : "Elsewhere (reference suppressed), we proposed a distributional compositional approach where distributional features are based on anchored packed trees (APTs). The essence of the APT approach is that distributional features should encode complete dependency paths from the target word to each context word and that these representations should be properly aligned before composition.\ntoken 1st order 2nd order\ngrad/N ⟨NMOD, student⟩ ⟨NMOD.NSUBJ, fold⟩ student/N ⟨NMOD, grad⟩,\n⟨NSUBJ, fold⟩ ⟨NSUBJ.DOBJ, clothes⟩\nfold/V ⟨NSUBJ, student⟩, ⟨DOBJ, clothes⟩ ⟨NSUBJ.AMOD, grad⟩, ⟨DOBJ.NMOD, dry⟩ dry/J ⟨AMOD, clothes⟩ ⟨AMOD.DOBJ, fold⟩, ⟨AMOD.AMOD, clean⟩ clothes/N ⟨AMOD, dry⟩, ⟨DOBJ, fold⟩ ⟨DOBJ.NSUBJ, student⟩\nTable 1: Typed higher order distributional features\nFor example, consider the sentence “The grad student folded the dry clothes.” Table 1 shows some of the features that this would generate in an APT lexicon for each of the content tokens1. Features are typed using the complete dependency\n1We assume lemmatisation in these examples since in practice it reduces sparsity, but it is not necessary from a theoretical point of view. In these examples, we also omit the POS tags on the context words since they can be easily inferred by the reader.\npath from the token to the context word. The inverse of dependency relation R is denoted by R. The length of the dependency path is referred to as the order of the distributional feature. For compactness, we only show features up to order 2.\nFrom Table 1, it is clear, as noted by Weeds et al. (2014), that features of words with different parts of speech do not immediately sit in the same space. This is because words of different parts of speech play different roles in syntactic (and the associated semantic) relations. In this example, all of the observed first order features of adjectives are of type AMOD whereas the nouns have observed first order features of types AMOD, NMOD, NMOD, NSUBJ and DOBJ. The use of an intersective pointwise composition operation such as multiply would largely lead to zero vectors, which is obviously not the desired result of composition. A pointwise composition operation which performs a union of the features such as add would largely lead to a concatenation of the two vectors.\nHowever, the second observation that we make based on Table 1 is that the 2nd order features of, say, adjectives, correspond to the 1st order features of nouns. Similarly, whilst not shown, the 3rd order features correspond to the 2nd order features and the 4th order features correspond to the 3rd order features. The only difference is that the features in the adjective space have the prefix AMOD which is the path which is traversed between the adjective and the noun in the parse tree.\nAccordingly, to construct a vector for a phrase from the perspective of its head (i.e., so that it has features in the same type space as its head), we first offset all of the dependent vectors in accordance with δ, the path to the dependent from the head in the phrase or sentence being composed. During offsetting each feature type is prepended by the given path and reduction applied. The reduced co-occurrence type produced from τ is denoted ↓(τ), and defined as follows:\n↓(τ) = ⎧⎪⎪⎪⎨⎪⎪⎪⎩ ↓(τ1τ2) if τ = τ1 r r τ2 or τ = τ1 r r τ2 for some r ∈ R\nτ otherwise (1)\nReduction essentially means that a relation cancels with its inverse relation i.e., the occurrence of a relation adjacent to its inverse relation will be replaced by the empty relation. Note, as discussed in (reference suppressed), reduction does introduce\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nδ student/N clothes/N\n⟨NMOD, grad⟩, ⟨NSUBJ, fold⟩, ⟨NSUBJ.DOBJ, clothes⟩ ⟨AMOD, dry⟩, ⟨DOBJ, fold⟩, ⟨DOBJ.NSUBJ, student⟩\nNSUBJ ⟨ , fold⟩, ⟨DOBJ, clothes⟩, ⟨NSUBJ.NMOD, grad⟩\n⟨NSUBJ.AMOD, dry⟩\nNMOD ⟨NMOD.NMOD, grad⟩, ⟨NMOD.NSUBJ, fold⟩ ⟨NMOD.AMOD, dry⟩\nTable 2: Features of student/N and clothes/N given different offsets δ.\nzero’th order features (with type ) in both elementary and offset representations. Further, all cooccurrence types are required to have a tree-based interpretation (τ ∈ R∗R∗) which leads to the elimination of some incompatible co-occurrence types. Table 2 shows the result of offsetting the features of student/N and clothes/N in Table 1 with three different paths 1) , 2) NSUBJ and 3) NMOD as required in the compositions of the phrases lazy student, student submits and student halls respectively.\nHaving aligned the vector spaces, it is possible to carry out any pointwise composition operation, such as add or multiply. The overall result will be sensitive to the structure in the composed phrase because different structures lead to different alignments.\nHowever, the focus of the current study is noun compounds, the majority of which are generally tagged and parsed as noun-noun compounds (e.g., grad student/NN). In this particular instance, it is less apparent that the vector spaces need to be aligned before composition. There will be other instances of the word grad/N where it is used as the object or subject of verbs. These contexts may well be good indicators of what make good contexts for grad student/N (particularly in this example since grad/N and grad student/NN are often used synonymously). Therefore, we also consider the typed dependency model where features are higher order dependency paths but alignment is not carried out before composition."
    }, {
      "heading" : "4 Compositionality of compound nouns",
      "text" : "Compositionality detection, as described in Reddy et al. (2011), involves deciding whether a given multiword expression is compositional or not i.e., whether the meaning can be understood from the literal (simplex) meaning of its parts. Reddy et al. (2011) introduced a dataset consisting of 90\ncompound nouns along with human judgments of their literality or compositionally at both the constituent and the phrase level. All judgments are given on a scale of 0 to 5, where 5 is high. For example, the phrase climate change is deemed to take its meaning literally from both constituents and also deemed to be a literal phrase. Conversely, the phrase gravy train has low scores for the literalness of the phrase and of the use of each constituent within the phrase. The phrase cocktail dress is deemed to be literal in its use of the second constituent but not the first whereas the phrase spelling bee is deemed to have high literalness in its use of the first constituent but not in its use of the second. Both of these examples are considered to have a medium level of literalness with respect to the whole phrase.\nReddy et al. (2011) further investigated a number of ways of detecting compositionality using vector based models of word meaning. They experimented with both constituent based models and compositionality function based models. In constituent based models, the compositionality of the phrase is considered to be a function of the similarity of each of the constituent’s vectors to the observed phrase vector. This is based on the intuition that if a constituent is used literally within a phrase then it is highly likely that the compound and the constituent share co-occurrences. In compositionality function based methods, the compositionality of the phrase is determined by first composing the constituents using some function and then measuring the similarity of the composed vector to the observed vector. The intuition here is that a good compositionality function is highly likely to return vectors which are similar to observed phrasal vectors for compositional phrases but much less likely to return similar vectors for non-compositional phrases. Reddy et al. (2011) carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). They used 3-fold cross-validation to estimate model parameters and found that using weighted addition outperformed multiplication as a compositionality function and that this also outperformed all of the constituent based models. With the optimal settings in their experiments they achieved a Spearman’s rank correlation coefficient of 0.714 with the human judgments.\nWe adapt the experiment described above to\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nlook at the effectiveness of APT composition in predicting the first-order dependency features of compound nouns. When a phrase is compositional, we expect that the second- (and zero-) order features of the modifier composed with the firstorder features of the head noun will be a good predictor of the first-order features of the compound noun. For example, we expect the second-order dependency features of spelling, which would include evidence from other uses of spelling as a modifier (e.g. spelling test) to be more indicative of the co-occurrences of spelling bee than the firstorder dependency features of spelling."
    }, {
      "heading" : "5 Experimental set-up",
      "text" : "For consistency with the experiments of Reddy et al. (2011), the corpus used in this experiment is the same fully-annotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008). As described in Grefenstette et al. (2013), this corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependencyparsed with the Malt Parser (Nivre, 2004). It contains about 1.9 billion tokens.\nBefore creating our APT lexicon and word embeddings from the dependency-parsed corpus, we further preprocessed the corpus by identifying occurrences of the 90 target noun phrases and recombining them into a single lexical item. Note that whilst all of the phrases are considered to be compound nouns, there are a number of possible dependency relationships which can occur between them. In the corpus we identified 1,236,264 occurrences of the candidate lemmas occurring contiguously (in the correct order). 76% of these had some kind of dependency relationship where the second lemma was the head. We note that the majority of occurrences where no dependency relationship was observed, this was due to the parser incorrectly parsing a three noun compound phrase, e.g., parsing the phrase interest rate rise so that both interest and rate modify rise. Of the occurrences where some kind of dependency relationship in the correct direction was identified, 95% of were an NMOD relationship. However, the modifier in an NMOD relationship can be an adjective or a noun. Many compounds (e.g. graduate student and silver screen) are seen with the modifier tagged both as a noun and as an adjective. So that we can carry out composition of the correct tokens (e.g. graduate as an adjective as opposed to grad-\nuate as a noun) according to observed dependency relations, the token for the compound records both the individual lemmas and the dependency relation observed between them in the corpus. Where a compound is seen with multiple dependency relationships we selected the dependency relationship which occurred most frequently.\nDuring this preprocessing stage, other dependency paths including the head constituent of the phrase are modified to include the new compound token. In this experiment, dependency paths including the modifying constituent are ignored. The rationale for this is that if the modifier is being modified e.g. as in (recently graduated) student, then this is not a modification which can be applied to the phrase — we do not apply adverbs to compound nouns — rather it is a modification of an internal part of the phrase.\nHaving preprocessed the corpus to contain compound nouns, we created elementary representations for every token in the corpus. Note that the elementary representation for the constituent of a compound phrase will not contain any of the contextual features associated with the compound phrase token unless they occurred with the constituent in some other context. If we allowed the same single observed contextual feature to feed into the representation of the compound and of the constituents, then intersective methods of composition would do very well at recreating the observed vector — ignoring any feature selection, recall would be 100%. Following Weeds et al. (2014) we expect a good method of composition to be able to infer the representation of a phrase without ever having observed the phrase in the corpus.\nWe will now discuss the parameters we have explored in terms of the construction and composition of elementary representations in each model."
    }, {
      "heading" : "5.1 APT model",
      "text" : "In relation to the construction of the elementary APTs, the most obvious parameter is the nature of the weight associated with each feature. We consider both the use of normalised counts and PPMI values. If w is a target word, w′ a context word and τ a dependency path from w to w′ then the normalised count is simply:\np(w′, τ ∣w) = #⟨w, τ, w ′⟩\n#⟨w, ∗, ∗⟩ (2)\nLevy et al. (2015) showed that the use of context distribution smoothing (α = 0.75) in the PMI cal-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nculation can lead to performance comparable with state-of-the-art word embeddings on word similarity tasks. We use this modified definition of PMI and experiment with α = 0.75 and α = 1\nPMI (w,w′; τ,α) = #⟨w, τ, w ′⟩#⟨∗, τ, ∗⟩α\n#⟨w, τ, ∗⟩#⟨∗, τ, w′⟩α (3)\nWe also carried out some experiments with shifted PMI which is analogous to the use of negative sampling in word embeddings (Levy et al., 2015). However, in this study we found that shifting PMI tended to have a strong negative effect on results. We suspect this is due to the low frequency of many of the compound nouns in the corpus. Removing features which tend to go with lots of things (low positive PMI) means that these phrases appear to have been observed in a very small number of (highly informative) contexts. If the composition process fails to recover one or more of these contexts, it has a very big impact on similarity. For compactness, we do not include the results with shifted PMI here.\nHaving constructed elementary APTs2, the APT composition process involves aligning those elementary APTs and composing the associated vectors of weights. However, the composition operation used after alignment is not fixed. Here, we have investigated using⊔INT, which takes the minimum of each of the constituent’s feature values and ⊔UNI, which performs pointwise addition on the aligned vector spaces. Following Reddy et al. (2011), who found that weighted addition worked best in their experiments, when using the ⊔UNI operation, we have experimented with weighting the contributions of each constituent to the composed APT representation using the parameter, h. For example, if A2 is the APT associated with the head of the phrase and Aδ1 is the properly aligned APT associated with the modifier where δ is the dependency path from the head to the modifier (e.g. NMOD or AMOD), the composition operations can be defined as:\n⊔ INT {Aδ1,A2 } (4)\n⊔ UNI { (1 − h)Aδ1, hA2 } (5)\n2The size of the feature space for nouns is approximately 80,000 dimensions (when including only first-order paths) and approximately 230,000 dimensions (when including paths up to order 2).\nWe have also considered composition without alignment of the modifier’s APT, i.e, using A1:\n⊔ INT {A1,A2 } (6)\n⊔ UNI { (1 − h)A1, hA2 } (7)\nIn general, we would expect there to be little overlap between APTs which have not been properly aligned. However, in the case where δ is the NMOD relation, i.e., the internal relation in the vast majority of the compound phrases, there may well be considerable overlap between the conventional first-order dependency features of the modifier and the head. In order to examine the contribution of both the aligned and unaligned APTs in the composition process, we used a hybrid method where the composed representation is defined as:\n⊔ INT { (qAδ1 + (1 − q)A1),A2 } (8)\n⊔ UNI { (1 − h)(qAδ1 + (1 − q)A1), hA2 } (9)\nIn the case where representations consist of APT weights which are normalised counts, PPMI is estimated after composition. Similarity between composed and observed phrasal vectors (restricted to 1st order dependency features3) is then computed using the cosine measure."
    }, {
      "heading" : "5.2 Neural word embeddings",
      "text" : "For each word and compound phrase, neural representations were constructed using the word2vec tool (Mikolov et al., 2013). Whilst it is not possible or appropriate to carry out an exhaustive parameter search, we experimented with a number of commonly used and recommended parameter settings. In particular, we investigate both the cbow and skip-gram models with 50, 100 and 300 dimensions. We also experiment with the subsampling threshold, trying 10−3, 10−4 and 10−5. As recommended in the documentation, in the cbow model we use a window size of 5 and and in the skip-gram model we use a window size of 10.\n3The rationale for this is that, whilst both composed and observed representations contain higher-order dependency features, the second-order (and third-order) paths in the composed representations are not reliable since the elementary APTs only contained paths up to order 2.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nCompositional Model\nPPMI α = 1 PPMI α = 0.75\nCF CS CF CS\nAligned ⊔INT (Eq. 4) 0.72 0.70 0.75 0.72 Aligned ⊔UNI (Eq. 5) 0.71 0.72 0.72 0.75 Unaligned ⊔INT (Eq. 6) 0.74 0.72 0.72 0.73 Unaligned ⊔UNI (Eq. 7) 0.77 0.75 0.78 0.77 Hybrid ⊔INT (Eq. 8) 0.74 0.73 0.73 0.73 Hybrid ⊔UNI (Eq. 9) 0.78 0.78 0.79 0.76\nTable 3: Average ρ between human judgements and phrase compositionality scores using APT representations.\nEarly experiments with different composition operations, showed add to be the only promising option. Consequently, all of the results reported here are for weighted addition. Similarity between composed and observed representations is computed using the cosine measure."
    }, {
      "heading" : "6 Results",
      "text" : "In order to estimate the model parameters h and q, we use repeated 3-fold cross-validation. Optimum values of the parameters are selected using the training samples. Results are reported in terms of average Spearman rank correlation scores (ρ) of phrase compositionality scores with human judgements on the corresponding testing samples. For clarity, we do not include the errors in the tables, but we have used a sufficiently large number of repetitions that these are all small (≤ 0.0015) and thus any difference observed which is greater than 0.005 is statistically significant at the 95% level. Boldface is used to indicate the best performing configuration of parameters for a particular compositional model.\nTable 3 summarises results for different composition operations and parameter settings using APT representations. We see that all of the results using standard PPMI (α = 1) and smoothed PPMI (α = 0.75) significantly outperform the result reported in Reddy et al. (2011), which used an untyped dependency space. Smoothing the PPMI calcu-\nlation with a value of α = 0.75 generally has a small positive effect. On average, the results when normalised counts are composed and PPMI is calculated as part of the similarity calculation (CF) are slightly higher than the results when PPMI weights are composed (CS) . However, the differences are small and it is possible that different parameter settings would lead to better results for CS. In general, the unaligned model outperforms the aligned model. However, a small but significant performance gain is generally made using the hybrid model. This suggests that aligned APT composition and unaligned APT composition are predicting different contexts for compound nouns which all contribute to forming a better estimate of the compositionality of the phrase. Regarding different composition operations, ⊔UNI generally outperforms ⊔INT. However, it would appear that it may be better to use the intersective operation when composing aligned APTs where the weights are normalised counts.\nTable 4 summarises results for different parameter settings for the neural word embeddings. Looking at the results in Table 4, we see that the cbow model significantly outperforms the skip-gram model. Using the cbow model with 100 dimensions and a subsampling threshold of t = 10−3 gives a performance of 0.74 which is significantly higher than the previous state-ofthe-art reported in Reddy et al. (2011). Since both of these models are based on untyped cooccurrences, this performance gain can be seen as the result of parameter optimisation.\nWe note that the cbow model seems to benefit from a higher subsampling threshold than recommend elsewhere in the literature (Levy et al., 2015). Since subsampling with lower thresholds is analogous to shifted PPMI, we hypothesise this\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nModel Average ρ RAND 0.02 FREQ 0.63 REDDY 0.71 skip-gram 0.68 cbow 0.74 aligned APT 0.75 unaligned APT 0.78 hybrid APT 0.79\nTable 5: Summary of optimal ρ values between phrase compositionality scores and human judgements for each compositional model\n‘optimisation’ was not beneficial in the current task for the same reason. Many of the compound phrases are comparatively low frequency tokens in the corpus and therefore subsampling their cooccurrences with high frequency words damages their representations. We also tried using lower subsampling thresholds with the 50-dimensional and 100-dimensional cbow model, but results did not improve.\nFinally, we note that consistently across all applicable models, optimal values of both h and q were in the range [0.3,0.5]. This suggests that for this dataset the modifier is slightly more informative than the head in determining the compositionality of compound nouns. It also suggests that the unaligned typed dependency features are slightly more informative than the aligned typed dependency features.\nTable 5 summarises the results across all of the compositional models. We also report three baselines. The RAND baseline assigns a random compositionality score to a compound. The FREQ baseline uses the observed frequency of the compound in the corpus as a predictor of compositionality. The REDDY baseline is the reported stateof-the-art result (Reddy et al., 2011) which uses weighted addition of untyped co-occurrence vectors obtained from the ukWaC corpus.\nLooking at the results in Table 5, we first note that the FREQ baseline performs much better than the RAND baseline. For this dataset, there is a significant amount of correlation between frequency and human judgments of compositionality i.e. the more frequently occurring compounds tend to be compositional. However, we see that all of the methods for predicting compositionality outperform this baseline. Further the methods based\non APT representations outperform the methods based on neural word embeddings. This suggests that the dependency paths encoded in the typed contextual features are highly informative in the task of determining the compositionality of a noun compound. When a linear combination of aligned and unaligned APTs is allowed, i.e. using the hybrid method, optimal performance is achieved."
    }, {
      "heading" : "7 Conclusions and Further Work",
      "text" : "We have demonstrated a number of ways in which compositionality detection for compound nouns can be improved. First, combining traditional compositional methods with state-of-theart low-dimensional word representations significantly improves results. However, further improvements can be achieved using an integrated compositional distributional approach. This approach maintains syntactic structure within the contextual features of words which is then central to the compositional process. We argue that some knowledge of syntactic structure is crucial in the fine-grained understanding of language. Since compositionality detection also provides a way of evaluating compositional methods without confounding judgements of phrase similarity with judgements of compositionality, it appears that the APT approach to compositionality is reasonably promising. Further work is of course needed with other datasets and other types of phrase.\nThe use of compositional methods to detect compositionality may also lead to improved compositional methods. For example, one reason why intersective approaches to composition may work less well than additive approaches is the sparse nature of the elementary representations. Improvements can be made (reference suppressed) by carrying out distributional smoothing (Dagan et al., 1993) on the elementary representations. However, further improvements might be made by first detecting non-compositional phrases. If a phrase is judged to be non-compositional, i.e., its composed representation is too dissimilar from its observed representation, the observed phrasal representation should be smoothed rather than its constituents’ representations."
    } ],
    "references" : [ {
      "title" : "Distributional memory: A general framework for corpus",
      "author" : [ "Marco Baroni", "Alessandro Lenci" ],
      "venue" : null,
      "citeRegEx" : "Baroni and Lenci.,? \\Q2010\\E",
      "shortCiteRegEx" : "Baroni and Lenci.",
      "year" : 2010
    }, {
      "title" : "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space",
      "author" : [ "Marco Baroni", "Roberto Zamparelli." ],
      "venue" : "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages",
      "citeRegEx" : "Baroni and Zamparelli.,? 2010",
      "shortCiteRegEx" : "Baroni and Zamparelli.",
      "year" : 2010
    }, {
      "title" : "Semantic parsing via paraphrasing",
      "author" : [ "Jonathan Berant", "Percy Liang." ],
      "venue" : "Proceedings of ACL.",
      "citeRegEx" : "Berant and Liang.,? 2014",
      "shortCiteRegEx" : "Berant and Liang.",
      "year" : 2014
    }, {
      "title" : "Mathematical foundations for a compositional distributed model of meaning",
      "author" : [ "Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark." ],
      "venue" : "Linguistic Analysis, 36(1-4):345–384.",
      "citeRegEx" : "Coecke et al\\.,? 2011",
      "shortCiteRegEx" : "Coecke et al\\.",
      "year" : 2011
    }, {
      "title" : "From Distributional to Semantic Similarity",
      "author" : [ "James Curran." ],
      "venue" : "Ph.D. thesis, University of Edinburgh.",
      "citeRegEx" : "Curran.,? 2004",
      "shortCiteRegEx" : "Curran.",
      "year" : 2004
    }, {
      "title" : "Contextual word similarity and estimation from sparse data",
      "author" : [ "Ido Dagan", "Shaul Marcus", "Shaul Markovitch." ],
      "venue" : "Proceedings of the 31st Annual Meeting on Association for Computational Linguistics, ACL ’93, pages 164–171, Stroudsburg, PA,",
      "citeRegEx" : "Dagan et al\\.,? 1993",
      "shortCiteRegEx" : "Dagan et al\\.",
      "year" : 1993
    }, {
      "title" : "General estimation and evaluation of compositional distributional semantic models",
      "author" : [ "Georgiana Dinu", "Nghia The Pham", "Marco Baroni." ],
      "venue" : "Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 50–58,",
      "citeRegEx" : "Dinu et al\\.,? 2013",
      "shortCiteRegEx" : "Dinu et al\\.",
      "year" : 2013
    }, {
      "title" : "Introducing and evaluating ukwac, a very large web-derived corpus of english",
      "author" : [ "Adriano Ferraresi", "Eros Zanchetta", "Marco Baroni", "Silvia Bernardini." ],
      "venue" : "Proceedings of LREC.",
      "citeRegEx" : "Ferraresi et al\\.,? 2008",
      "shortCiteRegEx" : "Ferraresi et al\\.",
      "year" : 2008
    }, {
      "title" : "Multi-step regression learning for compositional distributional semantics",
      "author" : [ "Edward Grefenstette", "Georgiana Dinu", "Yao-Zhong Zhang", "Mehrnoosh Sadrzadeh", "Marco Baroni." ],
      "venue" : "Proceedings of the 10th International Conference on Computational",
      "citeRegEx" : "Grefenstette et al\\.,? 2013",
      "shortCiteRegEx" : "Grefenstette et al\\.",
      "year" : 2013
    }, {
      "title" : "Explorations in Automatic Thesaurus Discovery",
      "author" : [ "Gregory Grefenstette." ],
      "venue" : "Kluwer Academic Publishers, Norwell, MA, USA.",
      "citeRegEx" : "Grefenstette.,? 1994",
      "shortCiteRegEx" : "Grefenstette.",
      "year" : 1994
    }, {
      "title" : "A regression model of adjective-noun compositionality in distributional semantics",
      "author" : [ "Emiliano Guevara." ],
      "venue" : "Proceedings of the ACL GEMS Workshop, pages 33–37.",
      "citeRegEx" : "Guevara.,? 2010",
      "shortCiteRegEx" : "Guevara.",
      "year" : 2010
    }, {
      "title" : "Jointly learning word representations and composition functions using predicate-argument structures",
      "author" : [ "Kazuma Hashimoto", "Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods",
      "citeRegEx" : "Hashimoto et al\\.,? 2014",
      "shortCiteRegEx" : "Hashimoto et al\\.",
      "year" : 2014
    }, {
      "title" : "An unsupervised ranking model for nounnoun compositionality",
      "author" : [ "Karl M Hermann", "Phil Blunsom", "Stephen Pulman." ],
      "venue" : "Proceedings of the First Joint Conference on Lexical and Computational Semantics (*SEM).",
      "citeRegEx" : "Hermann et al\\.,? 2012",
      "shortCiteRegEx" : "Hermann et al\\.",
      "year" : 2012
    }, {
      "title" : "Noun classification from predicate-argument structures",
      "author" : [ "Donald Hindle." ],
      "venue" : "Proceedings of the 28th annual meeting on Association for Computational Linguistics, pages 268–275.",
      "citeRegEx" : "Hindle.,? 1990",
      "shortCiteRegEx" : "Hindle.",
      "year" : 1990
    }, {
      "title" : "Measures of distributional similarity",
      "author" : [ "Lillian Lee." ],
      "venue" : "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 25–32, College Park, Maryland, USA, June. Association for Computational Linguistics.",
      "citeRegEx" : "Lee.,? 1999",
      "shortCiteRegEx" : "Lee.",
      "year" : 1999
    }, {
      "title" : "Neural word embedding as implicit matrix factorization",
      "author" : [ "Omer Levy", "Yoav Goldberg." ],
      "venue" : "Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages",
      "citeRegEx" : "Levy and Goldberg.,? 2014",
      "shortCiteRegEx" : "Levy and Goldberg.",
      "year" : 2014
    }, {
      "title" : "Improving distributional similarity with lessons learned from word embeddings",
      "author" : [ "Omer Levy", "Yoav Goldberg", "Ido Dagan." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 3:211–225.",
      "citeRegEx" : "Levy et al\\.,? 2015",
      "shortCiteRegEx" : "Levy et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic retrieval and clustering of similar words",
      "author" : [ "Dekang Lin." ],
      "venue" : "Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2, pages 768–774,",
      "citeRegEx" : "Lin.,? 1998",
      "shortCiteRegEx" : "Lin.",
      "year" : 1998
    }, {
      "title" : "Detecting a continuum of compositionality in phrasal verbs",
      "author" : [ "Diana McCarthy", "Bill Keller", "John Carroll." ],
      "venue" : "Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and Treatment.",
      "citeRegEx" : "McCarthy et al\\.,? 2003",
      "shortCiteRegEx" : "McCarthy et al\\.",
      "year" : 2003
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean." ],
      "venue" : "arXiv preprint arXiv:1301.3781.",
      "citeRegEx" : "Mikolov et al\\.,? 2013",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Evaluating neural word representations in tensor-based compositional settings",
      "author" : [ "Dmitrijs Milajevs", "Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Matthew Purver." ],
      "venue" : "Proceedings of the 2014 Conference on Empirical Methods in Natural Language",
      "citeRegEx" : "Milajevs et al\\.,? 2014",
      "shortCiteRegEx" : "Milajevs et al\\.",
      "year" : 2014
    }, {
      "title" : "Vector-based models of semantic composition",
      "author" : [ "Jeff Mitchell", "Mirella Lapata." ],
      "venue" : "Proceedings of ACL-08: HLT, pages 236–244, Columbus, Ohio, June. Association for Computational Linguistics.",
      "citeRegEx" : "Mitchell and Lapata.,? 2008",
      "shortCiteRegEx" : "Mitchell and Lapata.",
      "year" : 2008
    }, {
      "title" : "Composition in distributional models of semantics",
      "author" : [ "Jeff Mitchell", "Mirella Lapata." ],
      "venue" : "Cognitive Science, 34(8):1388–1429.",
      "citeRegEx" : "Mitchell and Lapata.,? 2010",
      "shortCiteRegEx" : "Mitchell and Lapata.",
      "year" : 2010
    }, {
      "title" : "Incrementality in deterministic dependency parsing",
      "author" : [ "Joakim Nivre." ],
      "venue" : "Proceedings of the ACL Workshop on Incremental Parsing, pages 50–57.",
      "citeRegEx" : "Nivre.,? 2004",
      "shortCiteRegEx" : "Nivre.",
      "year" : 2004
    }, {
      "title" : "Dependency-based construction of semantic space models",
      "author" : [ "Sebastian Padó", "Mirella Lapata." ],
      "venue" : "Comput. Linguist., 33(2):161–199, June.",
      "citeRegEx" : "Padó and Lapata.,? 2007",
      "shortCiteRegEx" : "Padó and Lapata.",
      "year" : 2007
    }, {
      "title" : "Glove: Global vectors for word representation",
      "author" : [ "Jeffrey Pennington", "Richard Socher", "Christopher D. Manning." ],
      "venue" : "Proceedings of EMNLP.",
      "citeRegEx" : "Pennington et al\\.,? 2014",
      "shortCiteRegEx" : "Pennington et al\\.",
      "year" : 2014
    }, {
      "title" : "An empirical study on compositionality in compound nouns",
      "author" : [ "Siva Reddy", "Diana McCarthy", "Suresh Manandhar." ],
      "venue" : "Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 210–218.",
      "citeRegEx" : "Reddy et al\\.,? 2011",
      "shortCiteRegEx" : "Reddy et al\\.",
      "year" : 2011
    }, {
      "title" : "Multiword expressions: A pain in the neck for nlp",
      "author" : [ "Ivan Sag", "Timothy Baldwin", "Francis Bond", "Ann Copestake", "Dan Flickinger." ],
      "venue" : "Proceedings of CICLING.",
      "citeRegEx" : "Sag et al\\.,? 2002",
      "shortCiteRegEx" : "Sag et al\\.",
      "year" : 2002
    }, {
      "title" : "Probabilistic part-of-speech tagging using decision trees",
      "author" : [ "Helmut Schmid" ],
      "venue" : null,
      "citeRegEx" : "Schmid.,? \\Q1994\\E",
      "shortCiteRegEx" : "Schmid.",
      "year" : 1994
    }, {
      "title" : "Semantic compositionality through recursive matrix-vector spaces",
      "author" : [ "Richard Socher", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng." ],
      "venue" : "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and",
      "citeRegEx" : "Socher et al\\.,? 2012",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2012
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics",
      "author" : [ "Peter D. Turney", "Patrick Pantel." ],
      "venue" : "J. Artif. Int. Res., 37(1):141–188, jan.",
      "citeRegEx" : "Turney and Pantel.,? 2010",
      "shortCiteRegEx" : "Turney and Pantel.",
      "year" : 2010
    }, {
      "title" : "Domain and function: A dualspace model of semantic relations and compositions",
      "author" : [ "Peter D. Turney." ],
      "venue" : "J. Artif. Int. Res., 44(1):533–585, may.",
      "citeRegEx" : "Turney.,? 2012",
      "shortCiteRegEx" : "Turney.",
      "year" : 2012
    }, {
      "title" : "Co-occurrence retrieval: a flexible framework for distributional similarity",
      "author" : [ "Julie Weeds", "David Weir." ],
      "venue" : "Computational Linguistics, 31(4).",
      "citeRegEx" : "Weeds and Weir.,? 2005",
      "shortCiteRegEx" : "Weeds and Weir.",
      "year" : 2005
    }, {
      "title" : "Distributional composition using higher-order dependency vectors",
      "author" : [ "Julie Weeds", "David Weir", "Jeremy Reffin." ],
      "venue" : "Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 11–20, Gothen-",
      "citeRegEx" : "Weeds et al\\.,? 2014",
      "shortCiteRegEx" : "Weeds et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Early research in the field (Hindle, 1990; Grefenstette, 1994; Lin, 1998; Lee, 1999; Curran, 2004; Weeds and Weir, 2005; Padó and Lapata, 2007) investigated distributional representations which were built directly from corpus cooccurrence counts; such representations are now commonly referred to as count or explicit vector representations.",
      "startOffset" : 28,
      "endOffset" : 143
    }, {
      "referenceID" : 9,
      "context" : "Early research in the field (Hindle, 1990; Grefenstette, 1994; Lin, 1998; Lee, 1999; Curran, 2004; Weeds and Weir, 2005; Padó and Lapata, 2007) investigated distributional representations which were built directly from corpus cooccurrence counts; such representations are now commonly referred to as count or explicit vector representations.",
      "startOffset" : 28,
      "endOffset" : 143
    }, {
      "referenceID" : 17,
      "context" : "Early research in the field (Hindle, 1990; Grefenstette, 1994; Lin, 1998; Lee, 1999; Curran, 2004; Weeds and Weir, 2005; Padó and Lapata, 2007) investigated distributional representations which were built directly from corpus cooccurrence counts; such representations are now commonly referred to as count or explicit vector representations.",
      "startOffset" : 28,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : "Early research in the field (Hindle, 1990; Grefenstette, 1994; Lin, 1998; Lee, 1999; Curran, 2004; Weeds and Weir, 2005; Padó and Lapata, 2007) investigated distributional representations which were built directly from corpus cooccurrence counts; such representations are now commonly referred to as count or explicit vector representations.",
      "startOffset" : 28,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "Early research in the field (Hindle, 1990; Grefenstette, 1994; Lin, 1998; Lee, 1999; Curran, 2004; Weeds and Weir, 2005; Padó and Lapata, 2007) investigated distributional representations which were built directly from corpus cooccurrence counts; such representations are now commonly referred to as count or explicit vector representations.",
      "startOffset" : 28,
      "endOffset" : 143
    }, {
      "referenceID" : 32,
      "context" : "Early research in the field (Hindle, 1990; Grefenstette, 1994; Lin, 1998; Lee, 1999; Curran, 2004; Weeds and Weir, 2005; Padó and Lapata, 2007) investigated distributional representations which were built directly from corpus cooccurrence counts; such representations are now commonly referred to as count or explicit vector representations.",
      "startOffset" : 28,
      "endOffset" : 143
    }, {
      "referenceID" : 24,
      "context" : "Early research in the field (Hindle, 1990; Grefenstette, 1994; Lin, 1998; Lee, 1999; Curran, 2004; Weeds and Weir, 2005; Padó and Lapata, 2007) investigated distributional representations which were built directly from corpus cooccurrence counts; such representations are now commonly referred to as count or explicit vector representations.",
      "startOffset" : 28,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "The models considered varied in terms of the use of different association functions (Curran, 2004), the use of different similarity measures (Lee, 1999; Weeds and Weir, 2005) and whether to define context in terms of proximity or grammatical dependencies (Padó and Lapata, 2007).",
      "startOffset" : 84,
      "endOffset" : 98
    }, {
      "referenceID" : 14,
      "context" : "The models considered varied in terms of the use of different association functions (Curran, 2004), the use of different similarity measures (Lee, 1999; Weeds and Weir, 2005) and whether to define context in terms of proximity or grammatical dependencies (Padó and Lapata, 2007).",
      "startOffset" : 141,
      "endOffset" : 174
    }, {
      "referenceID" : 32,
      "context" : "The models considered varied in terms of the use of different association functions (Curran, 2004), the use of different similarity measures (Lee, 1999; Weeds and Weir, 2005) and whether to define context in terms of proximity or grammatical dependencies (Padó and Lapata, 2007).",
      "startOffset" : 141,
      "endOffset" : 174
    }, {
      "referenceID" : 24,
      "context" : "The models considered varied in terms of the use of different association functions (Curran, 2004), the use of different similarity measures (Lee, 1999; Weeds and Weir, 2005) and whether to define context in terms of proximity or grammatical dependencies (Padó and Lapata, 2007).",
      "startOffset" : 255,
      "endOffset" : 278
    }, {
      "referenceID" : 19,
      "context" : "More recently the trend has been towards using neural models (Mikolov et al., 2013; Pennington et al., 2014) to create dense, low dimensional representations, commonly referred to as word embeddings, built by training the models to predict corpus co-occurrences.",
      "startOffset" : 61,
      "endOffset" : 108
    }, {
      "referenceID" : 25,
      "context" : "More recently the trend has been towards using neural models (Mikolov et al., 2013; Pennington et al., 2014) to create dense, low dimensional representations, commonly referred to as word embeddings, built by training the models to predict corpus co-occurrences.",
      "startOffset" : 61,
      "endOffset" : 108
    }, {
      "referenceID" : 25,
      "context" : "As has been noted elsewhere (Pennington et al., 2014), whilst appearing at first sight very different, both count-based methods and prediction-based methods have in common the fact that they probe the underlying co-occurrence statistics in the corpus.",
      "startOffset" : 28,
      "endOffset" : 53
    }, {
      "referenceID" : 31,
      "context" : "One approach (Turney, 2012) is to model the similarity of two phrases or sentences as a function of word-level similarities.",
      "startOffset" : 13,
      "endOffset" : 27
    }, {
      "referenceID" : 22,
      "context" : "An alternative approach (Mitchell and Lapata, 2010) is to build phrase or sentence-level representations by composing word-level representations and then measuring similarity directly.",
      "startOffset" : 24,
      "endOffset" : 51
    }, {
      "referenceID" : 21,
      "context" : "In those experiments, correlation with human judgements for intransitive sentences (Mitchell and Lapata, 2008) does not exceed 0.",
      "startOffset" : 83,
      "endOffset" : 110
    }, {
      "referenceID" : 4,
      "context" : "Early research in the field (Hindle, 1990; Grefenstette, 1994; Lin, 1998; Lee, 1999; Curran, 2004; Weeds and Weir, 2005; Padó and Lapata, 2007) investigated distributional representations which were built directly from corpus cooccurrence counts; such representations are now commonly referred to as count or explicit vector representations. The models considered varied in terms of the use of different association functions (Curran, 2004), the use of different similarity measures (Lee, 1999; Weeds and Weir, 2005) and whether to define context in terms of proximity or grammatical dependencies (Padó and Lapata, 2007). More recently the trend has been towards using neural models (Mikolov et al., 2013; Pennington et al., 2014) to create dense, low dimensional representations, commonly referred to as word embeddings, built by training the models to predict corpus co-occurrences. As has been noted elsewhere (Pennington et al., 2014), whilst appearing at first sight very different, both count-based methods and prediction-based methods have in common the fact that they probe the underlying co-occurrence statistics in the corpus. In fact, Levy and Goldberg (2014) demonstrated that the skip-gram model with negative sampling (SGNS) proposed by Mikolov et al.",
      "startOffset" : 85,
      "endOffset" : 1171
    }, {
      "referenceID" : 4,
      "context" : "Early research in the field (Hindle, 1990; Grefenstette, 1994; Lin, 1998; Lee, 1999; Curran, 2004; Weeds and Weir, 2005; Padó and Lapata, 2007) investigated distributional representations which were built directly from corpus cooccurrence counts; such representations are now commonly referred to as count or explicit vector representations. The models considered varied in terms of the use of different association functions (Curran, 2004), the use of different similarity measures (Lee, 1999; Weeds and Weir, 2005) and whether to define context in terms of proximity or grammatical dependencies (Padó and Lapata, 2007). More recently the trend has been towards using neural models (Mikolov et al., 2013; Pennington et al., 2014) to create dense, low dimensional representations, commonly referred to as word embeddings, built by training the models to predict corpus co-occurrences. As has been noted elsewhere (Pennington et al., 2014), whilst appearing at first sight very different, both count-based methods and prediction-based methods have in common the fact that they probe the underlying co-occurrence statistics in the corpus. In fact, Levy and Goldberg (2014) demonstrated that the skip-gram model with negative sampling (SGNS) proposed by Mikolov et al. (2013) is an implicit factorisation of the positive pointwise mutual information (PPMI) matrix commonly used in count-based methods.",
      "startOffset" : 85,
      "endOffset" : 1273
    }, {
      "referenceID" : 4,
      "context" : "Early research in the field (Hindle, 1990; Grefenstette, 1994; Lin, 1998; Lee, 1999; Curran, 2004; Weeds and Weir, 2005; Padó and Lapata, 2007) investigated distributional representations which were built directly from corpus cooccurrence counts; such representations are now commonly referred to as count or explicit vector representations. The models considered varied in terms of the use of different association functions (Curran, 2004), the use of different similarity measures (Lee, 1999; Weeds and Weir, 2005) and whether to define context in terms of proximity or grammatical dependencies (Padó and Lapata, 2007). More recently the trend has been towards using neural models (Mikolov et al., 2013; Pennington et al., 2014) to create dense, low dimensional representations, commonly referred to as word embeddings, built by training the models to predict corpus co-occurrences. As has been noted elsewhere (Pennington et al., 2014), whilst appearing at first sight very different, both count-based methods and prediction-based methods have in common the fact that they probe the underlying co-occurrence statistics in the corpus. In fact, Levy and Goldberg (2014) demonstrated that the skip-gram model with negative sampling (SGNS) proposed by Mikolov et al. (2013) is an implicit factorisation of the positive pointwise mutual information (PPMI) matrix commonly used in count-based methods. One current focus within the field of distributional semantics is enabling systems to make inferences about phrase-level or sentence-level similarity. One approach (Turney, 2012) is to model the similarity of two phrases or sentences as a function of word-level similarities. An alternative approach (Mitchell and Lapata, 2010) is to build phrase or sentence-level representations by composing word-level representations and then measuring similarity directly. However, whilst this second approach has proved popular, success, usually measured in terms of correlation with human similarity judgments, has been limited. For example, Dinu et al. (2013) reported results for a number of state-of-the art compositional methods on a number of phrase-based benchmark tasks.",
      "startOffset" : 85,
      "endOffset" : 2050
    }, {
      "referenceID" : 27,
      "context" : "Whilst it should be possible to construct evaluation datasets which avoid clearly idiomatic phrases such as red herring, non-compositionality is an integral part of language which should not be ignored (Sag et al., 2002).",
      "startOffset" : 202,
      "endOffset" : 220
    }, {
      "referenceID" : 0,
      "context" : ", include the name of the dependency relation) or untyped (Baroni and Lenci, 2010).",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 17,
      "context" : "Further, McCarthy et al. (2003) noted that the compositionality of a phrase should not be judged categorically, rather it should be viewed as existing on a continuum or a scale.",
      "startOffset" : 9,
      "endOffset" : 32
    }, {
      "referenceID" : 17,
      "context" : "Further, McCarthy et al. (2003) noted that the compositionality of a phrase should not be judged categorically, rather it should be viewed as existing on a continuum or a scale. Thus any judgements of the similarity of two composed phrases are confounded by the degree to which those phrases are compositional. Reddy et al. (2011) introduced a new dataset of 90 compound nouns together with human judgments of compositionality.",
      "startOffset" : 9,
      "endOffset" : 331
    }, {
      "referenceID" : 0,
      "context" : ", include the name of the dependency relation) or untyped (Baroni and Lenci, 2010). Further, following Weeds et al. (2014) we investigate the use of higher order dependency paths in contextual features in order to support composition in typed vector space models.",
      "startOffset" : 59,
      "endOffset" : 123
    }, {
      "referenceID" : 30,
      "context" : "In distributional semantic models, composition has typically been tackled by taking vector representations for words (Turney and Pantel, 2010) and combining them using some function to produce a data structure that represents the phrase or sentence.",
      "startOffset" : 117,
      "endOffset" : 142
    }, {
      "referenceID" : 19,
      "context" : "This vector space model component carried out composition by adding neural word embeddings obtained with the word2vec tool (Mikolov et al., 2013).",
      "startOffset" : 123,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "More recently, Berant and Liang (2014) achieved state-of-the-art results in question-answering where the paraphrase model included a vector space model component.",
      "startOffset" : 15,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : ", the full additive model (Guevara, 2010), the lexical function method (Baroni and Zamparelli, 2010), the full lexical model (Socher et al.",
      "startOffset" : 26,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : ", the full additive model (Guevara, 2010), the lexical function method (Baroni and Zamparelli, 2010), the full lexical model (Socher et al.",
      "startOffset" : 71,
      "endOffset" : 100
    }, {
      "referenceID" : 29,
      "context" : ", the full additive model (Guevara, 2010), the lexical function method (Baroni and Zamparelli, 2010), the full lexical model (Socher et al., 2012) and various tensor methods (Coecke et al.",
      "startOffset" : 125,
      "endOffset" : 146
    }, {
      "referenceID" : 3,
      "context" : ", 2012) and various tensor methods (Coecke et al., 2011; Grefenstette et al., 2013).",
      "startOffset" : 35,
      "endOffset" : 83
    }, {
      "referenceID" : 8,
      "context" : ", 2012) and various tensor methods (Coecke et al., 2011; Grefenstette et al., 2013).",
      "startOffset" : 35,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "In an evaluation across 3 different benchmark tasks (Dinu et al., 2013), the lexical function model was shown to be consistently the best-performing.",
      "startOffset" : 52,
      "endOffset" : 71
    }, {
      "referenceID" : 33,
      "context" : "From Table 1, it is clear, as noted by Weeds et al. (2014), that features of words with different parts of speech do not immediately sit in the same space.",
      "startOffset" : 39,
      "endOffset" : 59
    }, {
      "referenceID" : 25,
      "context" : "Compositionality detection, as described in Reddy et al. (2011), involves deciding whether a given multiword expression is compositional or not i.",
      "startOffset" : 44,
      "endOffset" : 64
    }, {
      "referenceID" : 25,
      "context" : "Compositionality detection, as described in Reddy et al. (2011), involves deciding whether a given multiword expression is compositional or not i.e., whether the meaning can be understood from the literal (simplex) meaning of its parts. Reddy et al. (2011) introduced a dataset consisting of 90 compound nouns along with human judgments of their literality or compositionally at both the constituent and the phrase level.",
      "startOffset" : 44,
      "endOffset" : 257
    }, {
      "referenceID" : 7,
      "context" : "(2011) carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100).",
      "startOffset" : 74,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : "(2011), the corpus used in this experiment is the same fully-annotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008).",
      "startOffset" : 111,
      "endOffset" : 135
    }, {
      "referenceID" : 28,
      "context" : "(2013), this corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependencyparsed with the Malt Parser (Nivre, 2004).",
      "startOffset" : 82,
      "endOffset" : 96
    }, {
      "referenceID" : 23,
      "context" : "(2013), this corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependencyparsed with the Malt Parser (Nivre, 2004).",
      "startOffset" : 139,
      "endOffset" : 152
    }, {
      "referenceID" : 22,
      "context" : "For consistency with the experiments of Reddy et al. (2011), the corpus used in this experiment is the same fully-annotated version of the web-derived ukWaC corpus (Ferraresi et al.",
      "startOffset" : 40,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "(2011), the corpus used in this experiment is the same fully-annotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008). As described in Grefenstette et al. (2013), this corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependencyparsed with the Malt Parser (Nivre, 2004).",
      "startOffset" : 112,
      "endOffset" : 180
    }, {
      "referenceID" : 7,
      "context" : "(2011), the corpus used in this experiment is the same fully-annotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008). As described in Grefenstette et al. (2013), this corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependencyparsed with the Malt Parser (Nivre, 2004). It contains about 1.9 billion tokens. Before creating our APT lexicon and word embeddings from the dependency-parsed corpus, we further preprocessed the corpus by identifying occurrences of the 90 target noun phrases and recombining them into a single lexical item. Note that whilst all of the phrases are considered to be compound nouns, there are a number of possible dependency relationships which can occur between them. In the corpus we identified 1,236,264 occurrences of the candidate lemmas occurring contiguously (in the correct order). 76% of these had some kind of dependency relationship where the second lemma was the head. We note that the majority of occurrences where no dependency relationship was observed, this was due to the parser incorrectly parsing a three noun compound phrase, e.g., parsing the phrase interest rate rise so that both interest and rate modify rise. Of the occurrences where some kind of dependency relationship in the correct direction was identified, 95% of were an NMOD relationship. However, the modifier in an NMOD relationship can be an adjective or a noun. Many compounds (e.g. graduate student and silver screen) are seen with the modifier tagged both as a noun and as an adjective. So that we can carry out composition of the correct tokens (e.g. graduate as an adjective as opposed to graduate as a noun) according to observed dependency relations, the token for the compound records both the individual lemmas and the dependency relation observed between them in the corpus. Where a compound is seen with multiple dependency relationships we selected the dependency relationship which occurred most frequently. During this preprocessing stage, other dependency paths including the head constituent of the phrase are modified to include the new compound token. In this experiment, dependency paths including the modifying constituent are ignored. The rationale for this is that if the modifier is being modified e.g. as in (recently graduated) student, then this is not a modification which can be applied to the phrase — we do not apply adverbs to compound nouns — rather it is a modification of an internal part of the phrase. Having preprocessed the corpus to contain compound nouns, we created elementary representations for every token in the corpus. Note that the elementary representation for the constituent of a compound phrase will not contain any of the contextual features associated with the compound phrase token unless they occurred with the constituent in some other context. If we allowed the same single observed contextual feature to feed into the representation of the compound and of the constituents, then intersective methods of composition would do very well at recreating the observed vector — ignoring any feature selection, recall would be 100%. Following Weeds et al. (2014) we expect a good method of composition to be able to infer the representation of a phrase without ever having observed the phrase in the corpus.",
      "startOffset" : 112,
      "endOffset" : 3180
    }, {
      "referenceID" : 16,
      "context" : "PMI (w,w; τ,α) = #⟨w, τ, w ⟩#⟨∗, τ, ∗⟩ #⟨w, τ, ∗⟩#⟨∗, τ, w′⟩ (3) We also carried out some experiments with shifted PMI which is analogous to the use of negative sampling in word embeddings (Levy et al., 2015).",
      "startOffset" : 189,
      "endOffset" : 208
    }, {
      "referenceID" : 16,
      "context" : "PMI (w,w; τ,α) = #⟨w, τ, w ⟩#⟨∗, τ, ∗⟩ #⟨w, τ, ∗⟩#⟨∗, τ, w′⟩ (3) We also carried out some experiments with shifted PMI which is analogous to the use of negative sampling in word embeddings (Levy et al., 2015). However, in this study we found that shifting PMI tended to have a strong negative effect on results. We suspect this is due to the low frequency of many of the compound nouns in the corpus. Removing features which tend to go with lots of things (low positive PMI) means that these phrases appear to have been observed in a very small number of (highly informative) contexts. If the composition process fails to recover one or more of these contexts, it has a very big impact on similarity. For compactness, we do not include the results with shifted PMI here. Having constructed elementary APTs2, the APT composition process involves aligning those elementary APTs and composing the associated vectors of weights. However, the composition operation used after alignment is not fixed. Here, we have investigated using⊔INT, which takes the minimum of each of the constituent’s feature values and ⊔UNI, which performs pointwise addition on the aligned vector spaces. Following Reddy et al. (2011), who found that weighted addition worked best in their experiments, when using the ⊔UNI operation, we have experimented with weighting the contributions of each constituent to the composed APT representation using the parameter, h.",
      "startOffset" : 190,
      "endOffset" : 1205
    }, {
      "referenceID" : 19,
      "context" : "For each word and compound phrase, neural representations were constructed using the word2vec tool (Mikolov et al., 2013).",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 26,
      "context" : "75) significantly outperform the result reported in Reddy et al. (2011), which used an untyped dependency space.",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 16,
      "context" : "We note that the cbow model seems to benefit from a higher subsampling threshold than recommend elsewhere in the literature (Levy et al., 2015).",
      "startOffset" : 124,
      "endOffset" : 143
    }, {
      "referenceID" : 16,
      "context" : "Using the cbow model with 100 dimensions and a subsampling threshold of t = 10 gives a performance of 0.74 which is significantly higher than the previous state-ofthe-art reported in Reddy et al. (2011). Since both of these models are based on untyped cooccurrences, this performance gain can be seen as the result of parameter optimisation.",
      "startOffset" : 54,
      "endOffset" : 203
    }, {
      "referenceID" : 26,
      "context" : "The REDDY baseline is the reported stateof-the-art result (Reddy et al., 2011) which uses weighted addition of untyped co-occurrence vectors obtained from the ukWaC corpus.",
      "startOffset" : 58,
      "endOffset" : 78
    }, {
      "referenceID" : 5,
      "context" : "Improvements can be made (reference suppressed) by carrying out distributional smoothing (Dagan et al., 1993) on the elementary representations.",
      "startOffset" : 89,
      "endOffset" : 109
    } ],
    "year" : 2016,
    "abstractText" : "Non-compositional phrases such as red herring and weakly compositional phrases such as spelling bee are an integral part of natural language. They are also the phrases that are difficult, or even impossible, for good compositional distributional models of semantics. Compositionality detection therefore provides a good testbed for compositional methods. We compare an integrated compositional distributional approach, using sparse high dimensional representations, with the adhoc compositional approach of applying simple composition operations to state-ofthe-art neural embeddings.",
    "creator" : "TeX"
  }
}