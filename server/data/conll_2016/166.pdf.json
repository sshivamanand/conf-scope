{
  "name" : "166.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Cross-Lingual Named Entity Recognition via Wikification",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n050\n051\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099\n100\n101\n102\n103"
    }, {
      "heading" : "1 Introduction",
      "text" : "Named Entity Recognition (NER) is the task of identifying and typing phrases that contain the names of persons, organizations, locations, and so on. It is an information extraction task that is important for understanding large bodies of text and\nis considered an essential pre-processing stage in Natural Language Processing (NLP) and Information Retrieval systems.\nNER is successful for languages which have a large amount of annotated data, but for languages with little to no annotated data, this task becomes very challenging. There are two common approaches to address the lack of training data problem. The first approach is to automatically generate annotated training data in the target language from Wikipedia articles or from parallel corpora. The performance of this method depends on the quality of the generated data and how well the language-specific features are explored. The second approach is to train a model on another language which has abundant training data, and then apply the model directly on test documents in the target language. This direct transfer technique relies on developing language-independent features. Note that these two approaches are orthogonal and can be used together.\nIn this paper, we focus on the direct transfer setting. We propose a cross-lingual NER model which is trained on annotated documents in one or multiple source languages, and can be applied to all languages in Wikipedia. The model depends on a cross-lingual wikifier, which only requires multilingual Wikipedia, no sentence-aligned or wordaligned parallel text is needed.\nRecently, much attention has been given to cross-lingual wikification and entity linking research (Ji et al., 2015; Ji et al., 2016; Moro et al., 2014; Tsai and Roth, 2016). The key contribution of the current paper is the development of a method that makes use of cross-lingual wikification to generate language-independent features for NER, and showing how useful this can be to training NER models with no annotation in the target language. Given a mention (sub-string) from a document written in a foreign language, the goal\n2\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\nSchwierigkeiten beim nachvollziehenden Verstehen Albrecht Lehmann läßt Flüchtlinge und Vertriebene in Westdeutschland\nProblem_solving Understanding Albert,_Duke_of_Prussia Jens_Lehmann Refugee Western_Germany\nhobby media_genre media_common quotation_subject\nperson noble_person\nperson athlete\nfield_of_study literature_subject\nlocation country\nWikipedia titles: FreeBase types:\nNER Tags: Person Location Sentence:\nFigure 1: An example of German sentence. We ground each word to the English Wikipedia by a crosslingual wikifier. A word is not linked if it is a stop word or the wikifier returns NIL. We can see that the FreeBase types are strong signals to NER even with imperfect disambiguation.\nof cross-lingual wikification is to find the corresponding title in the English Wikipedia. Traditionally, wikification has been considered a downstream task of NER. That is, a named entity recognizer is firstly applied to identify the mentions of interest, and then the wikifier grounds the extracted mentions to Wikipedia entries. In contrast to this traditional pipeline, we show that the ability to disambiguate words is very useful in identifying named entities. By grounding every n-gram to the English Wikipedia, we can obtain useful clues regardless of the language used in testing documents.\nFigure 1 shows an example of a German sentence. We use a cross-lingual wikifier to ground each word to the English Wikipedia. We can see that even though disambiguation is not perfect, the FreeBase types still provide valuable information. That is, although “Albrecht Lehmann” is not an entry in Wikipedia, the wikifier still links “Albrecht” and “Lehmann” to people. Since words in any language are grounded to the English Wikipedia, the corresponding Wikipedia categories and Freebase types can be used as language-independent features.\nThe proposed model significantly outperforms comparable direct transfer methods on Spanish, Dutch, and German CoNLL data. We also evaluate the model on five low-resource languages: Turkish, Tagalog, Yoruba, Bengali, and Tamil. Due to small sizes of Wikipedia, the overall performance is not as good as the CoNLL experiments. Nevertheless, the wikifier features still give significant improvement, and the proposed direct transfer model outperforms the state of the art, which assumes parallel text and some interaction with a native speaker of the target language. In addition, we show that the proposed languageindependent features not only perform well on the direct transfer scenario, but also improve monolingual models, which are trained on the target language. Another advantage of the proposed direct\ntransfer model is that we can train on documents from multiple languages together, and further improve the results."
    }, {
      "heading" : "2 Related Work",
      "text" : "There are three main branches of work for extending NLP systems to many languages: projection across parallel data, Wikipedia-based approaches, and direct transfer. Projection and direct transfer take advantage of the success of NLP tools on high-resource languages. Wikipediabased approaches exploit the fact that by editing Wikipedia, thousands of people have made annotations in hundreds of languages."
    }, {
      "heading" : "2.1 Projection",
      "text" : "Projection methods take a parallel corpus between source and target languages, annotate the source side, and push annotations across learned alignment edges. Assuming that source side annotations are high quality, success depends largely on the quality of the alignments, which depends, in turn, on the size of the parallel data, and the difficulty of aligning with the target language.\nThere is work on projection for POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; Duong et al., 2014), NER (Wang and Manning, 2014; Kim et al., 2012; Ehrmann et al., 2011), and parsing (Hwa et al., 2005; McDonald et al., 2011).\nWang and Manning (2014) show that projecting expectations of labels instead of hard labels can improve results. They experiment in two different settings: weakly-supervised, where only parallel data is available, and semi-supervised, where annotated training data is available along with unlabeled parallel data."
    }, {
      "heading" : "2.2 Using Wikipedia",
      "text" : "Wikipedia has been used for a large number of NLP tasks, from use as a semantic space (Gabrilovich and Markovitch, 2007; Song and Roth, 2014), to generating parallel data (Smith et\n3\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\nal., 2010), to use in open information extraction (Wu and Weld, 2010). It has also been used to extract training data for NER, under the intuition that Wikipedia is already (partially) annotated with NER labels, in the form of links to pages. Nothman et al. (2012) generate silver-standard NER data from Wikipedia using link targets, and other heuristics. This can be gathered for any language in Wikipedia, but several of the heuristics depend on language-specific rules. Al-Rfou et al. (2015) generate training data from Wikipedia articles using a similar manner. The polyglot word embeddings (Al-Rfou et al., 2013) are used as features in their NER model. Although the features are delexicalized, the embeddings are unique to each language, and so the model cannot transfer.\nKim et al. (2012) use Wikipedia to generate parallel sentences with NE annotations. They propose a semi-CRF model for aligning entities in parallel sentences. Results are very strong on Wikipedia data. This is a hybrid approach in that it is supervised projection using Wikipedia.\nOur work is most closely related to Kazama and Torisawa (2007). They do NER using Wikipedia category features for each mention. However, their method for wikifying text is not robust to ambiguity, and they only do monolingual NER.\nSil and Yates (2013) create a joint model for NER and entity linking (EL) in English. They avoid the traditional pipeline of NER then EL by overgenerating mentions in the first stage and using NER features to rank candidates. While the results are promising, the model is not scalable to other languages because it requires a trained NER system and NP chunker."
    }, {
      "heading" : "2.3 Direct Transfer",
      "text" : "The idea of direct transfer is to train a model in a high-resource setting using delexicalized features, that is, features that do not depend on word forms, and to directly apply it to text in a new language.\nTäckström et al. (2012) experiments with direct transfer of dependency parsing and NER, and shows that using word cluster features can help, especially if the clusters are forced to conform across languages. The cross-lingual word clusters are induced using large parallel corpora.\nBuilding on this work, Täckström (2012) focuses solely on NER, and includes experiments on self-training, and multi-source transfer for NER. Their experiments are orthogonal to ours, and\ncould be combined nicely. This work is closest to ours in terms of method, and so we compare against it in our experiments.\nOur work falls under the umbrella of direct transfer methods combined with use of Wikipedia. We introduce wikifier features, which are truly delexicalized, and use Wikipedia as a source of information for each language."
    }, {
      "heading" : "3 Named Entity Recognition Model",
      "text" : "We use the state of the art English NER model from Ratinov and Roth (2009) as a the base model. This model approaches NER as a multiclass classification problem with greedy decoding, using the BIO labeling scheme. The underlying classifier is averaged perceptron.\nTable 1 summarizes the features used in our model. These can be divided into a base set of standard features which are included in Ratinov and Roth (2009), a set of gazetteer features which are based on titles in multilingual Wikipedia, and our novel cross-lingual wikifier features. The base set of features can be further divided into nonlexical and lexical categories."
    }, {
      "heading" : "3.1 Base Features",
      "text" : "Non-Lexical Features Ratinov and Roth (2009) uses a small number of non-lexical features. For example, the previous tag feature is useful in pre-\n4\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\ndicting I- tags, because the previous tag should never be an O. The tag context feature looks in a 1000 word history and gathers statistics over tags assigned to words [wi, wi+1, wi+2]. These features are included in all experiments.\nIn contrast with (Täckström et al., 2012), we do not use POS tags as features. We could not get the universal POS tags for all languages in our experiments, and an earlier experiment indicated that adding POS tags do not improve the performance due to the accuracy of tagger. Lexical Features Lexical features are very important for monolingual NER. In the direct transfer setting, lexical features are useful if the target language is close to the training language. We use a small number of simple features, including word forms, affixes, capitalization, and tag patterns. The latter feature looks at a small window of text (at most 2 tokens) before the word in question. If there is a named entity in the window, it makes a feature out of NETag+wi−2 + wi−1. Word type features simply indicate whether the word in question is all capitalized, is all digits, or is all letters."
    }, {
      "heading" : "3.2 Gazetteer Features",
      "text" : "One of the larger performance improvements in Ratinov and Roth (2009) came from the use of gazetteers. We include gazetteers also in our model, except we gather them in each language from Wikipedia. As in Ratinov and Roth (2009), we use the gazetteers as features in the model. Specifically, we group gazetteers by topic, and use the name of the gazetteer file as the feature.\nThe method is to iteratively extend a short window to the right of the word in question. As the window increases in size, we search all gazetteers for occurrences of the phrase in the window. If we find a match, we add a feature to each word in the phrase according to its position in the phrase, either B for beginning, I for inside, or L for last. If the phrase is a single word, it is given a U feature.\nThis method generalizes gazetteers to unseen entities. For example, given the phrase “Bill and Melinda Gates Foundation”, “Bill” is marked as both B-PersonNames and B-Organizations, while “Foundation” is marked as L-Organizations. Imagine encountering at test time a fictional organization called “Dave and Sue Harris Foundation.” Although there is no gazetteer that contains this name, we have learned that “B-PersonName and B-PersonName B-PersonName Foundation” is a\nstrong signal for an organization."
    }, {
      "heading" : "3.3 Cross-lingual Wikifier Features",
      "text" : "As shown in Figure 1, disambiguating words to Wikipedia entries allows us to obtain useful information for NER from the corresponding FreeBase types and Wikipedia categories. A crosslingual wikifier grounds words and phrases of non-English languages to the English Wikipedia, which provides language-independent features for transferring an NER model directly.\nWe use the system proposed in Tsai and Roth (2016), which grounds input strings to the intersection of the English Wikipedia and the target language Wikipedia. The only requirement of this system is the multilingual Wikipedia dump and it can be applied on all languages in Wikipedia.\nSince we want to ground every n-gram (n ≤ 4) in the document, deviating from the normal usage that only considers few mentions of interest, we modify the system in the following two ways:\n• The original candidate generation process queries the index by both whole input string and the individual tokens of the string. For the n-grams where n > 1, we generate title candidates only according to the whole string, not individual tokens. For instance, if it is allowed to generate title candidates based on individual tokens, the bigram “in Germany” will be linked to the title Germany thus wrongly considered as a named entity.\n• The original ranking model includes the embeddings of other mentions in the document as features. It is clear that if we know what other important entities exist in the document, they provide useful clues to disambiguate a mention. However, if we want to wikify all n-grams, it makes no sense to include all of them as features, since the ranking model has already included features from TF-IDF weighted context words.\nAfter wikifying every n-gram 1, we set the types of each n-gram as the coarse- and fine-grained FreeBase types and Wikipedia categories from the top 2 title candidates returned by wikifier. For each word wi, we use the types of wi, wi+1, and wi−1, and the types of the n-grams which contain wi as features. Moreover, we also include the\n1We set n to 4 in all our experiments.\n5\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\nranker features in wikifier from the top candidate as features. This could serve as a linker (Ratinov et al., 2011), which rejects the top prediction if it has a low confidence."
    }, {
      "heading" : "4 Experiments and Analysis",
      "text" : "In this section, we conduct experiments to validate and analyze the proposed NER model. First, we show that adding wikifier features improves results on monolingual NER. Second, we show that wikifier features are strong signals in direct transfer of a trained NER model across languages. Finally, we explore the importance of Wikipedia size to the quality of wikifier features and study using multiple source languages."
    }, {
      "heading" : "4.1 Datasets",
      "text" : "We use data from CoNLL2002/2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The 4 languages represented are English, German, Spanish, and Dutch, each annotated using the IOB1 labeling scheme, which we convert to the BIO labeling scheme. All training is on the train set, and testing is on the test set. The evaluation metric for all experiments is phrase level F1, as explained in (Tjong Kim Sang, 2002).\nIn order to experiment on a broader range of languages, we also use data from the REFLEX (Simpson et al., 2008) and LORELEI projects. From LORELEI, we use Turkish,2 From REFLEX, we use Bengali, Tagalog, Tamil, and Yoruba.3 While Turkish, Tagalog, and Yoruba each have a few non-Latin characters, Bengali and Tamil are with an entirely non-Latin script. This is a major reason for inclusion in our experiments. We use the same set of test documents as used in Zhang et al. (2016). All other documents in the REFLEX and LORELEI packages are used as the training documents in our monolingual experiments. We refer to these five languages collectively as low-resource languages.\nBesides PER, LOC, and ORG, some lowresource languages contain TIME tags and TTL tags, which represented titles in text, such as Secretary, President, or Minister. Since such words are not tagged in CoNLL training data, we opted to simply remove these tags. On the other hand, there is no MISC tag in the low-resource languages. Instead, many MISC-tagged entities in the\n2LDC2014E115 3LDC2015E13,LDC2015E90,LDC2015E83,LDC2015E91\nCoNLL datasets have LOC tags in the REFLEX and LORELEI packages, e.g., Italian and Chinese. We modify a MISC-tagged word to LOC tag if it is grounded to an entity with location as a FreeBase type, and remove all the other MISC tags in the training data. This process of changing MISC tags is only done when we train on CoNLL documents and test on low-resource languages.\nThe only requirement to build the cross-lingual wikifier model is a multilingual Wikipedia dump, and it can be trivially applied to all languages in Wikipedia. The top section of Table 2 lists Wikipedia sizes in terms of articles,4 the number of titles linked to English titles, and the number of training and test mentions for each language.\nBesides the English gazetteers used in Ratinov and Roth (2009), we collect gazetteers for each language using Wikipedia titles. A Wikipedia title is included in the list for person names if it contains FreeBase type person. Similarly, we also create a location list and an organization list for each language. The total number of names in the gazetteers of each language is listed in Table 2."
    }, {
      "heading" : "4.2 Monolingual Experiments",
      "text" : "We begin by showing that wikifier features help when we train and test on the same language. The middle section of Table 2 shows the results.\nIn the ‘Wikifier only’ row, we use only wikifier features and previous tags features. This is intended to show the predictive power of wikifier features alone. Without using any lexical features, it gets good scores on the languages that have a large Wikipedia. These numbers represent the quality of the cross-lingual wikifier in that language, which in turn is correlated with the size of Wikipedia and size of the intersection with English Wikipedia.\nThe next row, ‘Base features’, shows that lexical features are always better than wikifier features only. This squares with the common wisdom that lexical features are important for NER.\nAdding gazetteers to the base features improves more than 3 points for every language except Bengali and Tamil. Since these two languages use entirely non-Latin scripts, the words will not match any names in the English gazetteers, which have higher coverage than other languages’ gazetteers.\nFinally, the ‘+Wikifier’ row shows that our pro-\n4From https://en.wikipedia.org/wiki/ List_of_Wikipedias, retrieved March 2016\n6\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n600\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\nLatin Script Non-Latin Script\nAPPROACH EN NL DE ES TR TL YO BN TA AVG\nWiki size 5.1M 1.9M 1.9M 1.3M 269K 64K 31K 42K 85K - En. intersection - 755K 964K 757K 169K 49K 30K 34K 51K - Gazetteer size 8.5M 579K 1M 943K 168K 54K 20K 29K 10K - Entities (train) 23.5K 18.8K 11.9K 13.3K 5.1K 4.6K 4.1K 8.8K 7.0K - Entities (test) 5.6K 3.6K 3.7K 3.9K 2.2K 3.4K 3.4K 3.5K 4.6K -\nMonolingual Experiments\nWikifier only 72.90 57.38 51.95 59.33 52.63 51.41 33.63 45.96 37.83 51.45 Base Features 85.24 77.34 65.11 79.98 65.21 74.34 54.78 69.11 55.30 69.60 +Gazetteers 88.92 82.69 68.74 83.51 70.67 77.53 57.89 69.50 56.76 72.91 +Wikifier 89.47 84.90 72.97 84.25 73.50 78.18 59.27 70.62 60.00 74.80\nDirect Transfer Experiments\nWikifier only 36.66 38.55 40.04 43.09 36.97 25.09 41.81 27.85 36.26 Base Features 44.10 25.24 41.81 30.50 50.45 32.48 2.30 1.74 28.58 +Gazetteers 49.66 35.06 55.04 30.90 64.07 34.42 3.14 0.30 34.07 +Wikifier 62.10 47.14 60.97 48.41 65.32 36.79 6.72 2.99 41.31\nTäckström baseline 48.4 23.5 45.6 - - - - - - Täckström bitext clusters 58.4 40.4 59.3 - - - - - - Zhang et al. (2016) - - - 43.6 51.3 36.0 34.8 26.0 38.3\nTable 2: Data sizes, monolingual experiments, and direct transfer experiments. Wiki size is the number of articles in Wikipedia. For monolingual experiments, we train the proposed model on the training data of the target languages. ‘Wikifier only’ uses the previous tags features also. For direct transfer experiments, all models are trained on CoNLL English training set. The rows marked Täckström come from (Täckström et al., 2012), and are the baseline and clustering result. The plus signs (+) signify cumulative addition. EN: English, NL: Dutch, DE: German, ES: Spanish, TR: Turkish, TL: Tagalog, YO: Yoruba, BN: Bengali, TA: Tamil.\nposed features are valuable even in combination with strong features. It improves upon base features and gazetteer features for all 9 languages. These numbers may be less than state of the art because the features we use are designed for English, and may not capture lexical subtleties in every language. Nevertheless, they show that wikifier features have a non-trivial signal that has not been captured by other features."
    }, {
      "heading" : "4.3 Direct Transfer Experiments",
      "text" : "We evaluate our direct transfer experiments by training on English and testing on the target language. The results from these experiments are shown in the bottom section of Table 2.\nThe ‘Wikifier only’ row shows that the wikifier features alone preserve a signal across languages. Interestingly, for both Bengali and Tamil, this is the strongest signal, and gets the highest score. If the lexical features are included when we train the English model, the learning algorithm will give them too much emphasis, thus decreasing the importance of the wikifier features. Since Bengali and Tamil use non-Latin scripts, no lexical feature\nin English will fire at test time. Thus, approaches that include base features perform poorly.\nThe results of ‘Base features’ can be viewed as a sort of language similarity to English, which, in this case, is related to lexical overlap and similarity between the scripts. Comparing to monolingual experiments, we can see that the lexical features become weak in the cross-lingual setting.\nThe gazetteer features are again shown to be very useful for almost all languages except Bengali and Tamil due to the reason explained in the monolingual experiment and to the inclusion of lexical features. For the rest of languages, the improvement from adding gazetteers is even more than the improvement in the monolingual setting.\nFor nearly every language, wikifier features help dramatically, which indicates that they are very good delexicalized features. It adds more than 10 points on Dutch, German, and Turkish.\nThe trend in Table 2 suggests the following strategy when we want to extract named entities in a new foreign language: It is better to include all features if the foreign language uses Latin script, since the names are likely mentioned using the\n7\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n700\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\nsame way as in English. Otherwise, using wikifier features could be the best setting.\nTäckström et al. (2012) also directly transfer an English NER model using the same setting as ours: train on the CoNLL English training set and predict on the test set of other three languages. We compare our baseline transfer model (Base Features) to the row denoted by “Täckström baseline”. Even though we do not use gold POS tags, we see that our results are comparable. The second Täckström row uses parallel text to induce multilingual word clustering. While this approach is orthogonal to ours, and could be used in tandem to get even better scores, we compare against it for lack of a more closely aligned scenario. We see that for each language, we significantly outperform their approach.\nWe note that our numbers are comparable to those reported for WIKI-2 in Nothman et al. (2012) for the CoNLL languages (with the exception of German, where theirs is higher). Their system requires language-specific heuristics to generate their silver-standard training data from Wikipedia articles. What they gain for single languages, they likely lose in generalizability. This approach is orthogonal to ours and we can also use their silverstandard data in training.\nFor the low-resource languages, we compare our direct transfer model with the expectation learning model proposed in Zhang et al. (2016). This model is not a direct transfer model, but it does not use any training data in the target languages either. Instead, for each target language, it generates patterns from parallel documents between English and the target language, a large monolingual corpus in the target language, and one-hour interaction with a native speaker of the target language. Note that they also use a crosslingual wikifier, but only for refining the entity types. On the other hand, in our model, the features from the wikifier are used in both detecting entity mention boundaries and the entity types. We can see that our approach performs better than their model on all five languages even though we assume much fewer resources. The difference is most significant on Turkish, Tagalog, and Bengali."
    }, {
      "heading" : "4.4 Quality of Wikifier Features",
      "text" : "One immediate question is why wikifier features are less helpful on the low-resource languages results than on the CoNLL languages? In this exper-\nFEATURES SPANISH GERMAN #inter. F1 #inter. F1\nWikifier only 757K 40.04 964K 38.55 W.−FB query 757K 34.69 964K 28.27 W.−FB−50% inter. 379K 30.32 482K 27.24 W.−FB−90% inter. 76K 29.44 96K 25.94\nTable 3: The F1 scores of using only wikifier features with removing the support from FreeBase and varying the number of titles linked to the English Wikipedia. ‘W.−FB query’ removes the component of querying FreeBase by the target language title from ‘Wikifier only’. ‘−X% inter.’ indicates removing X% of the interlanguage links with English titles. The column #inter. shows the number of titles that intersect with English.\niment, we show that smaller Wikipedia sizes result in worse Wikipedia features, which is the reason Yoruba has bad ‘Wikifier only’ results and the improvement from the wikifier features is much smaller.\nThe cross-lingual wikifier that we use in our system only grounds words to the intersection of the English and target language Wikipedia. Given a Wikipedia title in the target language, we first retrieve FreeBase ID by querying FreeBase API. If it fails, we find the corresponding English Wikipedia title via interlanguage links and then query the API with the English title. However, FreeBase does not contain entities in Yoruba, Bengali, and Tamil, so the first step will always fail for these three languages. We remove this step in the experiments of high-resource languages and the results are shown in the row ‘W.−FB query ’ of Table 3. We can see that the performance drops a lot, because many words have no features from FreeBase types.\nNext, we randomly remove 50% and 90% of the interlanguage links to English titles. This will not only reduce the number of fired features from Wikipedia categories, but also FreeBase types since English titles are used to query FreeBase IDs. When 90% of interlanguage links are removed, the scores of Spanish and German are closer to Yoruba’s score (27.85)."
    }, {
      "heading" : "4.5 Training Languages",
      "text" : "In all previous experiments, the training language is always English. In order to test the efficacy of training with languages other than English, we create a train/test matrix with all combinations of languages, as seen in Figure 2.\n8\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n800\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\nFigure 2: Different training/test language pairs. Scores shown are the F1 scores. The red boxes signify the best non-target training languages.\nThe vertical axis represents training language, and the horizontal axis represents test language. A darker color signifies a higher score. For example, if we train on Spanish (es) and test on Yoruba (yo), we get an F1 of 39.4. When the test language is Bengali (bn) or Tamil (ta), we only use wikifier features. For other test languages, all features are included. Note that we ignore all MISC tags in the CoNLL languages (en, nl, de, es) in evaluation, since there is no MISC tag in the low-resource languages. The diagonals represent the monolingual setting in which we use all features for all languages. Since we are interested in transferring a model, we ignore the diagonals, and identify the best training language for a given test language as the largest off-diagonal in each column. These are demarcated with red boxes.\nEnglish is the best for most languages, with the exception of Dutch, best for Spanish, and Spanish, best for Yoruba. It makes sense that highresource languages are better training languages because 1) there are more annotated training instances, 2) larger Wikipedia creates denser wikifier features, therefore providing better estimation of the weights to these features.\nTable 4 shows the results of training on multiple languages. We use all features in this experiment. The row “EN” only trains the model on the English training documents, and the results are identical to those shown in Table 2. Adding\nSpanish training data yields the best score on Yoruba, which agrees with Figure 2 where Spanish is the best training language for Yoruba. Using all CoNLL languages (EN+ES+NL+DE) adds more than 1 point F1 in average comparing to using English only. Finally, training on all but the test languages further improves the results.\nThis experiment shows that we can augment training data from other languages’ annotated documents. Although the performance only increases a little, it does not hurt most of the time."
    }, {
      "heading" : "5 Conclusion and Discussion",
      "text" : "We propose a language-independent model for cross-lingual NER using a cross-lingual wikifier to disambiguate every n-grams. This model works on all languages in Wikipedia and the only requirement is a Wikipedia dump. We study a wide range of languages in both monolingual and crosslingual settings, and show significant improvements over strong baselines. An analysis shows that the quality of wikifier features depends on the Wikipedia size of the test language.\nThis work shows that if we can disambiguate words and phrases to the English Wikipedia, the typing information from Wikipedia categories and FreeBase are useful language-independent features for NER. However, there is other information from Wikipedia which we do not use, such as words from documents and relations between titles, which would require additional research.\nIn the future, we would like to experiment with including other techniques for multilingual NER that we discuss in Section 2 into our model, such as parallel projection and generating training data from Wikipedia automatically.\n9\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n900\n901\n902\n903\n904\n905\n906\n907\n908\n909\n910\n911\n912\n913\n914\n915\n916\n917\n918\n919\n920\n921\n922\n923\n924\n925\n926\n927\n928\n929\n930\n931\n932\n933\n934\n935"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Named Entity Recognition (NER) models for language L are typically trained using annotated data in that language. We study cross-lingual NER, where a model for NER in L is trained on a source language (or multiple source languages). We introduce a language independent method for NER, building on cross-lingual wikification, a technique that grounds words and phrases in a non-English text into English Wikipedia entries. Thus, mentions in text in any language can be described using a set of categories and FreeBase types, yielding, as we show, strong languageindependent features. With this insight, we propose an NER model that can be applied to all languages in Wikipedia. When trained on English, our model outperforms comparable approaches on the standard CoNLL datasets (Spanish, German, and Dutch) and also performs very well on low-resource languages (Turkish, Tagalog, Yoruba, Bengali, and Tamil) that have significantly smaller Wikipedia. Moreover, our methods allows us to train on multiple source languages, typically improving NER results on the target languages. Finally, we show that our languageindependent features can be used also to enhance monolingual NER systems, yielding improved results for all 9 languages.",
    "creator" : "TeX"
  }
}