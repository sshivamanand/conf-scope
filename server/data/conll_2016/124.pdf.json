{
  "name" : "124.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Leveraging Cognitive Features for Sentiment Analysis",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "This paper addresses the task of Sentiment Analysis (SA) - automatic detection of the sentiment polarity as positive versus negative - of usergenerated short texts and sentences. Several sentiment analyzers exist in literature today (Liu and Zhang, 2012). Recent works, such as Kouloumpis et al. (2011), Agarwal et al. (2011) and Barbosa and Feng (2010), attempt to conduct such analyses on user-generated content. Sentiment analysis remains a hard problem, due to the challenges it poses at the various levels, as summarized below."
    }, {
      "heading" : "1.1 Lexical Challenges",
      "text" : "Sentiment analyzers face the following three challenges at the lexical level: (1) Data Sparsity, i.e., handling the presence of unseen words/phrases. (e.g., The movie is messy, uncouth, incomprehensible, vicious and absurd) (2) Lexical Ambiguity,\ne.g., finding appropriate senses of a word given the context (e.g., His face fell when he was dropped from the team vs The boy fell from the bicycle, where the verb “fell” has to be disambiguated) (3) Domain Dependency, tackling words that change polarity across domains. (e.g., the word unpredictable being positive in case of unpredictable movie in movie domain and negative in case of unpredictable steering in car domain). Several methods have been proposed to address the different lexical level difficulties by - (a) using WordNet synsets and word cluster information to tackle lexical ambiguity and data sparsity (Akkaya et al., 2009; Balamurali et al., 2011; Go et al., 2009; Maas et al., 2011; Popat et al., 2013; Saif et al., 2012) and (b) mining domain dependent words (Sharma and Bhattacharyya, 2013; Wiebe and Mihalcea, 2006)."
    }, {
      "heading" : "1.2 Syntactic Challenges",
      "text" : "Difficulty at the syntax level arises when the given text follows a complex phrasal structure and, phrase attachments are expected to be resolved before performing SA. For instance, the sentence A somewhat crudely constructed but gripping, questing look at a person so racked with self-loathing, he becomes an enemy to his own race. requires processing at the syntactic level, before analyzing the sentiment. Approaches leveraging syntactic properties of text include generating dependency based rules for SA (Poria et al., 2014) and leveraging local dependency (Li et al., 2010)."
    }, {
      "heading" : "1.3 Semantic and Pragmatic Challenges",
      "text" : "This corresponds to the difficulties arising in the higher layers of NLP, i.e., semantic and pragmatic layers. Challenges in these layers include handling: (a) Sentiment expressed implicitly (e.g., Guy gets girl, guy loses girl, audience falls asleep.) (b) Presence of sarcasm and other\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nforms of irony (e.g., This is the kind of movie you go because the theater has air-conditioning.) and (c) Thwarted expectations (e.g., The acting is fine. Action sequences are top-notch. Still, I consider it as a below average movie due to its poor storyline.).\nSuch challenges are extremely hard to tackle with traditional NLP tools, as these need both linguistic and pragmatic knowledge. Most attempts towards handling thwarting (Ramteke et al., 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.g., leveraging hashtags) and/or stylistic/pragmatic features (emoticons, laughter expressions such as “lol” etc). Addressing difficulties for linguistically well-formed texts, in absence of explicit cues (like emoticons), proves to be difficult using textual/stylistic features alone."
    }, {
      "heading" : "1.4 Introducing Cognitive Features",
      "text" : "We empower our systems by augmenting cognitive features along with traditional linguistic features used for general sentiment analysis, thwarting and sarcasm detection. Cognitive features are derived from the eye-movement patterns of human annotators recorded while they annotate short-text with sentiment labels. Our hypothesis is that cognitive processes in the brain are related to eye-movement activities (Parasuraman and Rizzo, 2006). Hence, considering readers’ eye-movement patterns while they read sentiment bearing texts may help tackle linguistic nuances better. We perform statistical classification using various classifiers and different feature combinations. With our augmented feature-set, we observe a significant improvement of accuracy across all classifiers for two different datasets. Experiments on a carefully curated held-out dataset indicate a significant improvement in sentiment polarity detection over the state of the art, specifically text with complex constructs like irony and sarcasm. Through feature significance analysis, we show that cognitive features indeed empower sentiment analyzers to handle complex constructs like irony and sarcasm. Our approach is the first of its kind to the best of our knowledge.\nThe rest of the paper is organized as follows. Section 2 presents a summary of past work done\nin traditional SA and SA from a psycholinguistic point of view. Section 3 describes the available datasets we have taken for our analysis. Section 4 presents an our features that comprise both traditional textual features, used for sentiment analysis and cognitive features derived from annotators’ eye-movement patterns. In section 5, we discuss the results for various sentiment classification techniques under different combinations of textual and cognitive features, showing the effectiveness of cognitive features. In section 7, we discuss on the feasibility of our approach before concluding the paper in section 8."
    }, {
      "heading" : "2 Related Work",
      "text" : "Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task.\nSupervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role. Apart from the commonly used bagof-words features based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011) and effect of negators (Ikeda et al., 2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches.\nEye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to space limitations). Joshi et al. (2014) develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking. Mishra et al. (2014) study sentiment detection, and subjectivity extraction through anticipation and homing, with the use of eye tracking. Regarding other NLP tasks, Joshi et al. (2013) proposed a studied the cognitive aspects if Word Sense Disambiguation (WSD) through eye-\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nNB SVM RB P R F P R F P R F\nD1 66.15 66 66.15 64.5 65.3 64.9 56.8 60.9 53.5 D2 74.5 74.2 74.3 77.1 76.5 76.8 75.9 53.9 63.02\nTable 1: Classification results for different SA systems for dataset 1 (D1) and dataset 2 (D2). P→ Precision, R→ Recall, F→ F˙score\ntracking. Earlier, Mishra et al. (2013) measured translation annotation difficulty of a given sentence based on gaze input of translators used to label training data. The recent advancements in the literature discussed above, motivate us to explore gaze-based cognition for sentiment analysis.\nWe acknowledge that some of the well performing sentiment analyzers use Deep Learning techniques (like Convolutional Neural Network based approach by Maas et al. (2011) and Recursive Neural Network based approach by dos Santos and Gatti (2014)). In these, the features are automatically learned from the input text. Since our approach is feature based, we do not consider these approaches for our current experimentation. Taking inputs from gaze data and using them in a deep learning setting sounds intriguing, though, it is beyond the scope of this work."
    }, {
      "heading" : "3 Eye-tracking and Sentiment Analysis Datasets",
      "text" : "We use two publicly available datasets for our experiments. Dataset 1 has been released by Mishra et al. (2016) which they use for the task of sarcasm understandability prediction. Dataset 2 has been used by Joshi et al. (2014) for the task of sentiment annotation complexity prediction. These datasets contain many instances with higher level nuances like presence of implicit sentiment, sarcasm and thwarting. We describe the datasets below."
    }, {
      "heading" : "3.1 Dataset 1",
      "text" : "It contains 994 text snippets with 383 positive and 611 negative examples. Out of this, 350 are sarcastic or have other forms of irony. The snippets are a collection of reviews, normalized-tweets and quotes. Each snippet is annotated by seven participants with binary positive/negative polarity labels. Their eye-movement patterns are recorded with a high quality SR-Research Eyelink-1000 eyetracker (sampling rate 500Hz). The annotation accuracy varies from 70%-90% with a Fleiss kappa inter-rater agreement of 0.62."
    }, {
      "heading" : "3.2 Dataset 2",
      "text" : "This dataset consists of 1059 snippets comprising movie reviews and normalized tweets. Each snippet is annotated by five participants with positive, negative and objective labels. Eye-tracking is done using a low quality Tobii T120 eye-tracker (sampling rate 120Hz). The annotation accuracy varies from 75%-85% with a Fleiss kappa interrater agreement of 0.68. We rule out the objective ones and consider 843 snippets out of which 443 are positive and 400 are negative."
    }, {
      "heading" : "3.3 Performance of Existing SA Systems Considering Dataset -1 and 2 as Test Data",
      "text" : "It is essential to check whether our selected datasets really pose challenges to existing sentiment analyzers or not. For this, we implement two statistical classifiers and a rule based classifier to check the test accuracy of Dataset 1 and Dataset 2. The statistical classifiers are based on Support Vector Machine (SVM) and Näive Bayes (NB) implemented using Weka (Hall et al., 2009) and LibSVM (Chang and Lin, 2011) APIs. These are on trained on 10662 snippets comprising movie reviews and tweets, randomly collected from standard datasets released by Pang and Lee (2004) and Sentiment 140 (http:// www.sentiment140.com/). The feature-set comprises traditional features for SA reported in a number of papers. They are discussed in section 4 under the category of Sentiment Features. The in-house rule based (RB) classifier decides the sentiment labels based on the counts of positive and negative words present in the snippet, computed using MPQA lexicon (Wilson et al., 2005). It also considers negators as explained by Jia et al. (2009) and intensifiers as explained by Dragut and Fellbaum (2014).\nTable 1 presents the accuracy of the three systems. The F-scores are not very high for all the systems (especially for dataset 1 that contains more sarcastic/ironic texts), possibly indicating that the snippets in our dataset pose challenges for\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nexisting sentiment analyzers. Hence, the selected datasets are ideal for our current experimentation that involves cognitive features."
    }, {
      "heading" : "4 Enhanced feature set for SA",
      "text" : "Our feature-set into four categories viz. (1) Sentiment features (2) Sarcasm, Irony and Thwarting related Features (3) Cognitive features from eyemovement (4) Textual features related to reading difficulty. We describe our feature-set below."
    }, {
      "heading" : "4.1 Sentiment Features",
      "text" : "We consider a series of textual features that have been extensively used in sentiment literature (Liu and Zhang, 2012). The features are described below. Each feature is represented by a unique abbreviated form, which are used in the subsequent discussions.\n1. Presence of Unigrams (NGRAM˙PCA) i.e. Presence of unigrams appearing in each sentence that also appear in the vocabulary obtained from the training corpus. To avoid overfitting (since our training data size is less), we reduce the dimension to 500 using Principal Component Analysis.\n2. Subjective words (Positive words, Negative words) i.e. Presence of positive and negative words computed against MPQA lexicon (Wilson et al., 2005), a popular lexicon used for sentiment analysis.\n3. Subjective scores (PosScore, NegScore) i.e. Scores of positive subjectivity and negative subjectivity using SentiWordNet (Esuli and Sebastiani, 2006).\n4. Sentiment flip count (FLIP) i.e. Number of times words polarity changes in the text. Word polarity is determined using MPQA lexicon.\n5. Part of Speech ratios (VERB, NOUN, ADJ, ADV) i.e. Ratios (proportions) of verbs, nouns, adjectives and adverbs in the text. This is computed using NLTK1.\n6. Count of Named Entities (NE) i.e. Number of named entity mentions in the text. This is computed using NLTK.\n1http://www.nltk.org/\n7. Discourse connectors (DC) i.e. Number of discourse connectors in the text computed using an in-house list of discourse connectors (like however, although etc.)"
    }, {
      "heading" : "4.2 Sarcasm, Irony and Thwarting related Features",
      "text" : "To handle complex texts containing constructs irony, sarcasm and thwarted expectations as explained earlier, we consider the following features. The features are taken from Riloff et al. (2013), Ramteke et al. (2013) and Joshi et al. (2015).\n1. Implicit incongruity (IMPLICIT PCA) i.e. Presence of positive phrases followed by negative situational phrase (computed using bootstrapping technique suggested by Riloff et al. (2013)). We consider the top 500 principal components of these phrases to reduce dimension, in order to avoid overfitting.\n2. Punctuation marks (PUNC) i.e. Count of punctuation marks in the text.\n3. Largest pos/neg subsequence (LAR) i.e. Length of the largest series of words with polarities unchanged. Word polarity is determined using MPQA lexicon.\n4. Lexical polarity (LP) i.e. Sentence polarity found by supervised logistic regression using the dataset used by Joshi et al. (2015)."
    }, {
      "heading" : "4.3 Cognitive features from eye-movement",
      "text" : "Eye-movement patterns are characterized by two basic attributes: (1) Fixations, corresponding to a longer stay of gaze on a visual object (like characters, words etc. in text) (2) Saccades, corresponding to the transition of eyes between two fixations. Moreover, a saccade is called a Regressive Saccade or simply, Regression if it represents a phenomenon of going back to a pre-visited segment. A portion of a text is said to be skipped if it does not have any fixation. Figure 1 shows eye-movement behavior during annotation of the given sentence in dataset-1. The circles represent fixation and the line connecting the circles represent saccades. Our cognition driven features are derived from these basic eye-movement attributes. We divide our features in two sets as explained ahead.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nFigure 1: Snapshot of eye-movement behavior during annotation of an opinionated text. The circles represent fixations and lines connecting the circles represent saccades. Boxes represent Areas of Interest (AoI) which are words of the sentence in our case."
    }, {
      "heading" : "4.4 Basic gaze features",
      "text" : "Readers’ eye-movement behavior, characterized by fixations, forward saccades, skips and regressions, can be directly quantified by simple statistical aggregation (i.e., computing features for individual participants and then averaging). Since these behaviors intuitively relate to the cognitive process of the readers (Rayner and Sereno, 1994), we consider simple statistical properties of these factors as features to our model. Some of these features have been reported by Mishra et al. (2016) for modeling sarcasm understandability of readers. However, as far as we know, these features are being introduced in NLP tasks like sentiment analysis for the first time.\n1. Average First-Fixation Duration per word (FDUR) i.e. Sum of first-fixation duration divided by word count. First fixations are fixations occurring during the first pass reading. Intuitively, an increased first fixation duration is associated to more time spent on the words, which accounts for lexical complexity. This is motivated by Rayner and Duffy (1986).\n2. Average Fixation Count (FC) i.e. Sum of fixation counts divided by word count. If the reader reads fast, the first fixation duration may not be high even if the lexical complexity is more. But the number of fixations may increase on the text. So, fixation count may help capture lexical complexity in such cases.\n3. Average Saccade Length (SL) i.e. Sum of saccade lengths (measured by number of words) divided by word count. Intuitively, lengthy saccades represent the text being structurally/syntactically complex. This is also supported by von der Malsburg and Vasishth (2011).\n4. Regression Count (REG) i.e. Total number of gaze regressions. Regressions correspond to both lexical and syntactic reanalysis (Malsburg et al., 2015). Intuitively,\nregression count should be useful in capturing both syntactic and semantic difficulties.\n5. Skip count (SKIP) i.e. Number of words skipped divided by total word count. Intuitively, higher skip count should correspond lesser semantic processing requirement (assuming that skipping is not done intentionally).\n6. Count of regressions from second half to first half of the sentence (RSF) i.e. Number of regressions from second half of the sentence to the first half of the sentence (given the sentence is divided into two equal half of words). Constructs like sarcasm, irony often have phrases that are incongruous (e.g. ”The book is so great that it can be used as a paperweight”- the incongruous phrases are ”book is so great” and ”used as a paperweight”.. Intuitively, when a reader encounters such incongruous phrases, the second phrases often cause a surprisal resulting in a long regression to the first part of the text. Hence, this feature is considered.\n7. Largest Regression Position (LREG) i.e. Ratio of the absolute position of the word from which a regression with the largest amplitude (in terms of number of characters) is observed, to the total word count of sentence. This is chosen under the assumption that regression with the maximum amplitude may occur from the portion of the text which causes maximum surprisal (in order to get more information about the portion causing maximum surprisal). The relative starting position of such portion, captured by LREG, may help distinguish between sentences with different linguistic subtleties."
    }, {
      "heading" : "4.5 Complex gaze features",
      "text" : "We propose a graph structure constructed from the gaze data to derive more complex gaze features. We term the graph as gaze-saliency graphs.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599"
    }, {
      "heading" : "I will always cherish the",
      "text" : "original mis-conception I had of you\nFigure 2: Saliency graph of a human annotator for the sentence I will always cherish the original misconception I had of you.\nA gaze-saliency graph for a sentence S for a reader R, represented as G = (V,E), is a graph with vertices (V ) and edges (E) where each vertex v ∈ V corresponds to a word in S (may not be unique) and there exists an edge e ∈ E between vertices v1 and v2 if R performs at least one saccade between the words corresponding to v1 and v2. Figure 2 shows an example of such a graph.\n1. Edge density of the saliency gaze graph (ED) i.e. Ratio of number of edges in the gaze saliency graph and total number of possible links ((|V |×|V |−1|)/2) in the saliency graph. As, Edge Density of a saliency graph increases with the number of distinct saccades, it is supposed to increase if the text is semantically more difficult.\n2. Fixation Duration at Left/Source (F1H, F1S) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the fixation duration on the word of node i of edge Eij as edge weight.\n3. Fixation Duration at Right/Target (F2H, F2S) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the fixation duration of the word of node i of edge Eij as edge weight.\n4. Forward Saccade Word Count of Source (PSH, PSS) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the number of forward saccades between nodes i and j of an edge Eij as edge weight..\n5. Forward Saccade Word Count of Destination (PSDH, PSDS) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the total distance (word count) of forward saccades between nodes i and j of an edge Eij as edge weight.\n6. Regressive Saccade Word Count of Source (RSH, RSS) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the number of regressive saccades between nodes i and j of an edge Eij as edge weight.\n7. Regressive Saccade Word Count of Destination (RSDH,RSDS) i.e. Largest weighted degree and second largest weighted degree of the saliency graph considering the number of regressive saccades between nodes i and j of an edge Eij as edge weight.\nThe ”highest and second highest degree” based gaze features derived from saliency graphs are motivated by our qualitative observations from the gaze data. Intuitively, the highest weighted degree of a graph is expected to be higher if some phrases have complex semantic relationships with others."
    }, {
      "heading" : "4.6 Features Related to Reading Difficulty",
      "text" : "Eye-movement during reading text with sentiment related nuances (like sarcasm) can be similar to text with other forms of difficulties. To address the effect of sentence length, word length and syllable count that affect reading behavior, we consider the following features.\n1. Readability Ease (RED) i.e. Flesch Readability Ease score of the text (Kincaid et al., 1975). Higher the score, easier is the text to comprehend.\n2. Sentence Length (LEN) i.e. Number of words in the sentence.\nWe now explain our experimental setup and results."
    }, {
      "heading" : "5 Experiments and results",
      "text" : "We test the effectiveness of the enhanced featureset by implementing three classifiers viz., SVM (with linear kernel), NB and Multi-layered Neural Network. These systems are implemented using the Weka (Hall et al., 2009) and LibSVM (Chang and Lin, 2011) APIs. Several classifier hyperparameters are kept to the default values given in Weka. We separately perform a 10-fold cross validation on both Dataset 1 and 2 using different sets of feature combinations. The average F-scores for the class-frequency based random classifier are 33% and 46.93% for dataset 1 and dataset 2 respectively.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nClassifier Näive Bayes SVM Multi-layer NN Dataset 1\nP R F P R F P R F Uni 58.5 57.3 57.9 67.8 68.5 68.14 65.4 65.3 65.34 Sn 58.7 57.4 58.0 69.6 70.2 69.8 67.5 67.4 67.5\nSn + Sr 63.0 59.4 61.14 72.8 73.2 72.9 69.0 69.2 69.1 Gz 61.8 58.4 60.05 54.3 52.6 53.4 59.1 60.8 60\nSn+Gz 60.2 58.8 59.2 69.5 70.1 69.6 70.3 70.5 70.4 Sn+ Sr+Gz 63.4 59.6 61.4 73.3 73.6 73.5 70.5 70.7 70.6\nDataset 2 Uni 51.2 50.3 50.74 57.8 57.9 57.8 53.8 53.9 53.8 Sn 51.1 50.3 50.7 62.5 62.5 62.5 58.0 58.1 58.0 Sn+Sr 50.7 50.1 50.39 70.3 70.3 70.3 66.8 66.8 66.8 Gz 49.9 50.9 50.39 48.9 48.9 48.9 53.6 54.0 53.3 Sn+Gz 49.9 50.9 48.5 48.9 48.9 48.9 53.6 54.0 53.8 Sn+ Sr+Gz 50.2 49.7 50 71.9 71.8 71.8 69.1 69.2 69.1\nTable 2: Results for different feature combinations. (P,R,F)→ Precision, Recall, F-score. Feature labels Uni→Unigram features, Sn→Sentiment features, Sr→Sarcasm features and Gz→Gaze features along with features related to reading difficulty\nThe classification accuracy is reported in Table 2. We observe the maximum accuracy with the complete feature-set comprising Sentiment, Sarcasm and Thwarting, and Cognitive features derived from gaze data. For this combination, SVM outperforms the other classifiers. The novelty of our feature design lies in (a) First augmenting sarcasm and thwarting based features (Sr) with sentiment features (Sn), which shoots up the accuracy by 3.1% for Dataset1 and 7.8% for Dataset2 (b) Augmenting gaze features with Sn+Sr, which further increases the accuracy by 0.6% and 1.5%for Dataset 1 and 2 respectively, amounting to an overall improvement of 3.7% and 9.3% respectively.\nSince the best and the second best features are close in terms of accuracy for dataset 1 (difference of 0.5%), we perform a statistical significance test using McNemar test (α = 0.05). The difference in the F-scores turns out to be significant with p = 0.0001.\nWe also perform a chi-squared test based feature significance analysis, shown in Table 3. For dataset 1, 10 out of the top 20 ranked features are gaze-based features and for dataset 2, 7 out of top 20 features are gaze-based, as shown in bold letters."
    }, {
      "heading" : "5.1 Importance of cognitive features",
      "text" : "To study whether the cognitive features actually help in classifying complex output as hypothesized earlier, we repeat the experiment on a heldout dataset, randomly derived from Dataset-1. It has 294 text snippets out of which 131 contain complex constructs like irony/sarcasm and rest of the snippets are relatively simpler. We choose\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nIrony Non-Irony Sn 58.2 75.5\nSn+Sr 60.1 75.9 Gz+Sn+Sr 64.3 77.6\nTable 4: F-scores on held-out dataset for Complex Constructs (Irony), Simple Constructs (Nonirony)\nSVM, our best performing classifier, with similar configuration as explained in section 5. As seen in Table 4, the relative improvement of F-score, when gaze features are included, is 6.1% for complex texts and is 2.1% for simple texts (all the values are statistically significant with p < 0.05 for McNemar test, except Sn and Sn + Sr for Nonirony case.). This demonstrates the efficacy of the gaze based features."
    }, {
      "heading" : "6 Error Analysis",
      "text" : "Errors committed by our system arise from multiple factors starting from limitations of the eyetracker hardware to errors committed by linguistic tools and resources. Moreover, aggregating various eye-tracking parameters to extract the cognitive features may have caused loss of information. For example, the graph based features are computed for each participant and eventually averaged to get the graph features for a sentence, thereby not leveraging the power of individual eye-movement patterns."
    }, {
      "heading" : "7 Feasibility of our approach",
      "text" : "Since our method requires gaze data from human readers to be available, the methods practicability becomes questionable. We present our views on this below."
    }, {
      "heading" : "7.1 Availability of Mobile Eye-trackers",
      "text" : "Availability of inexpensive embedded eye-trackers on hand-held devices has come close to reality now. This opens avenues to get eye-tracking data from inexpensive mobile devices from a huge population of online readers non-intrusively, and derive cognitive features to be used in predictive frameworks like ours. For instance, Cogisen: (http://www.sencogi.com) has a patent (ID: EP2833308-A1) on “eye-tracking using inexpensive mobile web-cams”."
    }, {
      "heading" : "7.2 Applicability Scenario",
      "text" : "We believe, mobile eye-tracking modules could be a part of mobile applications built for e-commerce, online learning, gaming etc. where automatic analysis of online reviews calls for better solutions to detect and handle linguistic nuances in sentiment analysis setting. To give an example, let’s say a book gets different reviews on Amazon. Our system could watch how readers read the review using mobile eye-trackers, and thereby, decide the polarity of opinion, especially when sentiment is not expressed explicitly (e.g., using strong polar words) in the text. Such an application can horizontally scale across the web, helping to improve automatic classification of online reviews."
    }, {
      "heading" : "7.3 Getting Users’ Consent for Eye-tracking",
      "text" : "Eye-tracking technology has already been utilized by leading mobile technology developers (like Samsung) to facilitate richer user experiences through services like Smart-scroll (where a user’s eye movement determines whether a page has to be scrolled or not) and Smart-lock (where user’s gaze position decided whether to lock the screen or not). The growing interest of users in using such services takes us to a promising situation where getting users’ consent to record eyemovement patterns will not be difficult, though it is yet not the current state of affairs."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We combined traditional sentiment features with (a) different textual features used for sarcasm and thwarting detection, and (b) cognitive features derived from readers’ eye movement behavior. The combined feature set improves the overall accuracy over the traditional feature set based SA by a margin of 3.6% and 9.3% respectively for Datasets 1 and 2. It is significantly effective for text with complex constructs, leading to an improvement of 6.1% on our held-out data. In future, we propose to explore (a) devising deeper gaze-based features and (b) multi-view classification using independent learning from linguistics and cognitive data. We also plan to explore deeper graph and gaze features, and models to learn complex gaze feature representation. Our general approach may be useful in other problems like emotion analysis, text summarization and question answering, where textual clues alone do not prove to be sufficient.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Sentiment analysis of twitter data",
      "author" : [ "Apoorv Agarwal", "Boyi Xie", "Ilia Vovsha", "Owen Rambow", "Rebecca Passonneau." ],
      "venue" : "Proceedings of the Workshop on Languages in Social Media, pages 30–38. ACL.",
      "citeRegEx" : "Agarwal et al\\.,? 2011",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2011
    }, {
      "title" : "Subjectivity word sense disambiguation",
      "author" : [ "Cem Akkaya", "Janyce Wiebe", "Rada Mihalcea." ],
      "venue" : "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 190–199. ACL.",
      "citeRegEx" : "Akkaya et al\\.,? 2009",
      "shortCiteRegEx" : "Akkaya et al\\.",
      "year" : 2009
    }, {
      "title" : "Harnessing wordnet senses for supervised sentiment classification",
      "author" : [ "AR Balamurali", "Aditya Joshi", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1081–1091.",
      "citeRegEx" : "Balamurali et al\\.,? 2011",
      "shortCiteRegEx" : "Balamurali et al\\.",
      "year" : 2011
    }, {
      "title" : "Modelling sarcasm in twitter, a novel approach",
      "author" : [ "Francesco Barbieri", "Horacio Saggion", "Francesco Ronzano." ],
      "venue" : "ACL 2014, page 50.",
      "citeRegEx" : "Barbieri et al\\.,? 2014",
      "shortCiteRegEx" : "Barbieri et al\\.",
      "year" : 2014
    }, {
      "title" : "Robust sentiment detection on twitter from biased and noisy data",
      "author" : [ "Luciano Barbosa", "Junlan Feng." ],
      "venue" : "Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 36–44. ACL.",
      "citeRegEx" : "Barbosa and Feng.,? 2010",
      "shortCiteRegEx" : "Barbosa and Feng.",
      "year" : 2010
    }, {
      "title" : "Sentiment analysis: Adjectives and adverbs are better than adjectives alone",
      "author" : [ "Farah Benamara", "Carmine Cesarano", "Antonio Picariello", "Venkatramana S Subrahmanian." ],
      "venue" : "ICWSM.",
      "citeRegEx" : "Benamara et al\\.,? 2007",
      "shortCiteRegEx" : "Benamara et al\\.",
      "year" : 2007
    }, {
      "title" : "Clues for detecting irony in user-generated contents: oh...!! it’s so easy;-)",
      "author" : [ "Paula Carvalho", "Luı́s Sarmento", "Mário J Silva", "Eugénio De Oliveira" ],
      "venue" : "In Proceedings of the 1st international CIKM workshop on Topic-sentiment analysis for",
      "citeRegEx" : "Carvalho et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Carvalho et al\\.",
      "year" : 2009
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "Chih-Chung Chang", "Chih-Jen Lin." ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "Chang and Lin.,? 2011",
      "shortCiteRegEx" : "Chang and Lin.",
      "year" : 2011
    }, {
      "title" : "Mining the peanut gallery: Opinion extraction and semantic classification of product reviews",
      "author" : [ "Kushal Dave", "Steve Lawrence", "David M Pennock." ],
      "venue" : "Proceedings of the 12th international conference on World Wide Web, pages 519–528. ACM.",
      "citeRegEx" : "Dave et al\\.,? 2003",
      "shortCiteRegEx" : "Dave et al\\.",
      "year" : 2003
    }, {
      "title" : "Deep convolutional neural networks for sentiment analysis of short texts",
      "author" : [ "Cı́cero Nogueira dos Santos", "Maira Gatti" ],
      "venue" : "In Proceedings of COLING",
      "citeRegEx" : "Santos and Gatti.,? \\Q2014\\E",
      "shortCiteRegEx" : "Santos and Gatti.",
      "year" : 2014
    }, {
      "title" : "The role of adverbs in sentiment analysis",
      "author" : [ "Eduard C Dragut", "Christiane Fellbaum." ],
      "venue" : "ACL 2014, 1929:38–41.",
      "citeRegEx" : "Dragut and Fellbaum.,? 2014",
      "shortCiteRegEx" : "Dragut and Fellbaum.",
      "year" : 2014
    }, {
      "title" : "Sentiwordnet: A publicly available lexical resource for opinion mining",
      "author" : [ "Andrea Esuli", "Fabrizio Sebastiani." ],
      "venue" : "Proceedings of LREC, volume 6, pages 417–422. Citeseer.",
      "citeRegEx" : "Esuli and Sebastiani.,? 2006",
      "shortCiteRegEx" : "Esuli and Sebastiani.",
      "year" : 2006
    }, {
      "title" : "Mining opinions in comparative sentences",
      "author" : [ "Murthy Ganapathibhotla", "Bing Liu." ],
      "venue" : "Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 241–248. Association for Computational Linguistics.",
      "citeRegEx" : "Ganapathibhotla and Liu.,? 2008",
      "shortCiteRegEx" : "Ganapathibhotla and Liu.",
      "year" : 2008
    }, {
      "title" : "Twitter sentiment classification using distant supervision",
      "author" : [ "Alec Go", "Richa Bhayani", "Lei Huang." ],
      "venue" : "CS224N Project Report, Stanford, 1:12.",
      "citeRegEx" : "Go et al\\.,? 2009",
      "shortCiteRegEx" : "Go et al\\.",
      "year" : 2009
    }, {
      "title" : "The weka data mining software: an update",
      "author" : [ "Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H Witten." ],
      "venue" : "ACM SIGKDD explorations newsletter, 11(1):10–",
      "citeRegEx" : "Hall et al\\.,? 2009",
      "shortCiteRegEx" : "Hall et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning to shift the polarity of words for sentiment classification",
      "author" : [ "Daisuke Ikeda", "Hiroya Takamura", "Lev-Arie Ratinov", "Manabu Okumura." ],
      "venue" : "IJCNLP, pages 296–303.",
      "citeRegEx" : "Ikeda et al\\.,? 2008",
      "shortCiteRegEx" : "Ikeda et al\\.",
      "year" : 2008
    }, {
      "title" : "The effect of negation on sentiment analysis and retrieval effectiveness",
      "author" : [ "Lifeng Jia", "Clement Yu", "Weiyi Meng." ],
      "venue" : "Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM ’09.",
      "citeRegEx" : "Jia et al\\.,? 2009",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2009
    }, {
      "title" : "More than meets the eye: Study of human cognition in sense annotation",
      "author" : [ "Salil Joshi", "Diptesh Kanojia", "Pushpak Bhattacharyya." ],
      "venue" : "HLTNAACL, pages 733–738.",
      "citeRegEx" : "Joshi et al\\.,? 2013",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2013
    }, {
      "title" : "Measuring sentiment annotation complexity of text",
      "author" : [ "Aditya Joshi", "Abhijit Mishra", "Nivvedan Senthamilselvan", "Pushpak Bhattacharyya." ],
      "venue" : "ACL (2), pages 36–41.",
      "citeRegEx" : "Joshi et al\\.,? 2014",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2014
    }, {
      "title" : "Harnessing context incongruity for sarcasm detection",
      "author" : [ "Aditya Joshi", "Vinita Sharma", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of 53rd Annual Meeting of the ACL, Beijing, China, page 757.",
      "citeRegEx" : "Joshi et al\\.,? 2015",
      "shortCiteRegEx" : "Joshi et al\\.",
      "year" : 2015
    }, {
      "title" : "Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel",
      "author" : [ "J Peter Kincaid", "Robert P Fishburne Jr", "Richard L Rogers", "Brad S Chissom." ],
      "venue" : "Technical report, DTIC",
      "citeRegEx" : "Kincaid et al\\.,? 1975",
      "shortCiteRegEx" : "Kincaid et al\\.",
      "year" : 1975
    }, {
      "title" : "Twitter sentiment analysis: The good the bad and the omg! ICWSM, 11:538–541",
      "author" : [ "Efthymios Kouloumpis", "Theresa Wilson", "Johanna Moore" ],
      "venue" : null,
      "citeRegEx" : "Kouloumpis et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kouloumpis et al\\.",
      "year" : 2011
    }, {
      "title" : "Sentiment analysis with global topics and local dependency",
      "author" : [ "Fangtao Li", "Minlie Huang", "Xiaoyan Zhu." ],
      "venue" : "AAAI, volume 10, pages 1371–1376.",
      "citeRegEx" : "Li et al\\.,? 2010",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "The perfect solution for detecting sarcasm in tweets# not",
      "author" : [ "Christine Liebrecht", "Florian Kunneman", "Antal van den Bosch." ],
      "venue" : "WASSA 2013, page 29.",
      "citeRegEx" : "Liebrecht et al\\.,? 2013",
      "shortCiteRegEx" : "Liebrecht et al\\.",
      "year" : 2013
    }, {
      "title" : "Joint sentiment/topic model for sentiment analysis",
      "author" : [ "Chenghua Lin", "Yulan He." ],
      "venue" : "Proceedings of the 18th ACM conference on Information and knowledge management, pages 375–384. ACM.",
      "citeRegEx" : "Lin and He.,? 2009",
      "shortCiteRegEx" : "Lin and He.",
      "year" : 2009
    }, {
      "title" : "A survey of opinion mining and sentiment analysis",
      "author" : [ "Bing Liu", "Lei Zhang." ],
      "venue" : "Mining text data, pages 415–463. Springer.",
      "citeRegEx" : "Liu and Zhang.,? 2012",
      "shortCiteRegEx" : "Liu and Zhang.",
      "year" : 2012
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "Andrew L Maas", "Raymond E Daly", "Peter T Pham", "Dan Huang", "Andrew Y Ng", "Christopher Potts." ],
      "venue" : "Proceedings of the 49th Annual Meeting of the ACL: Human Language Technologies-Volume 1,",
      "citeRegEx" : "Maas et al\\.,? 2011",
      "shortCiteRegEx" : "Maas et al\\.",
      "year" : 2011
    }, {
      "title" : "Determinants of scanpath regularity in reading",
      "author" : [ "Titus Malsburg", "Reinhold Kliegl", "Shravan Vasishth." ],
      "venue" : "Cognitive science, 39(7):1675–1703.",
      "citeRegEx" : "Malsburg et al\\.,? 2015",
      "shortCiteRegEx" : "Malsburg et al\\.",
      "year" : 2015
    }, {
      "title" : "Delta tfidf: An improved feature space for sentiment analysis",
      "author" : [ "Justin Martineau", "Tim Finin." ],
      "venue" : "ICWSM, 9:106.",
      "citeRegEx" : "Martineau and Finin.,? 2009",
      "shortCiteRegEx" : "Martineau and Finin.",
      "year" : 2009
    }, {
      "title" : "Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis",
      "author" : [ "Diana Maynard", "Mark A Greenwood." ],
      "venue" : "Proceedings of LREC.",
      "citeRegEx" : "Maynard and Greenwood.,? 2014",
      "shortCiteRegEx" : "Maynard and Greenwood.",
      "year" : 2014
    }, {
      "title" : "Topic sentiment mixture: modeling facets and opinions in weblogs",
      "author" : [ "Qiaozhu Mei", "Xu Ling", "Matthew Wondra", "Hang Su", "ChengXiang Zhai." ],
      "venue" : "Proceedings of the 16th international conference on World Wide Web, pages 171–180. ACM.",
      "citeRegEx" : "Mei et al\\.,? 2007",
      "shortCiteRegEx" : "Mei et al\\.",
      "year" : 2007
    }, {
      "title" : "Automatically predicting sentence translation difficulty",
      "author" : [ "Abhijit Mishra", "Pushpak Bhattacharyya", "Michael Carl", "IBC CRITT." ],
      "venue" : "ACL (2), pages 346–351.",
      "citeRegEx" : "Mishra et al\\.,? 2013",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2013
    }, {
      "title" : "A cognitive study of subjectivity extraction in sentiment annotation",
      "author" : [ "Abhijit Mishra", "Aditya Joshi", "Pushpak Bhattacharyya." ],
      "venue" : "ACL 2014, page 142.",
      "citeRegEx" : "Mishra et al\\.,? 2014",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2014
    }, {
      "title" : "Predicting readers’ sarcasm understandability by modeling gaze behavior",
      "author" : [ "Abhijit Mishra", "Diptesh Kanojia", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of AAAI.",
      "citeRegEx" : "Mishra et al\\.,? 2016",
      "shortCiteRegEx" : "Mishra et al\\.",
      "year" : 2016
    }, {
      "title" : "Sentiment analysis using support vector machines with diverse information sources",
      "author" : [ "Tony Mullen", "Nigel Collier." ],
      "venue" : "EMNLP, volume 4, pages 412–418.",
      "citeRegEx" : "Mullen and Collier.,? 2004",
      "shortCiteRegEx" : "Mullen and Collier.",
      "year" : 2004
    }, {
      "title" : "Dependency tree-based sentiment classification using crfs with hidden variables",
      "author" : [ "Tetsuji Nakagawa", "Kentaro Inui", "Sadao Kurohashi." ],
      "venue" : "NAACL-HLT, pages 786–794. Association for Computational Linguistics.",
      "citeRegEx" : "Nakagawa et al\\.,? 2010",
      "shortCiteRegEx" : "Nakagawa et al\\.",
      "year" : 2010
    }, {
      "title" : "Examining the role of linguistic knowledge sources in the automatic identification and classification of reviews",
      "author" : [ "Vincent Ng", "Sajib Dasgupta", "SM Arifin." ],
      "venue" : "Proceedings of the COLING/ACL on Main conference poster sessions.",
      "citeRegEx" : "Ng et al\\.,? 2006",
      "shortCiteRegEx" : "Ng et al\\.",
      "year" : 2006
    }, {
      "title" : "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Proceedings of the 42nd annual meeting on ACL, page 271. ACL.",
      "citeRegEx" : "Pang and Lee.,? 2004",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2004
    }, {
      "title" : "Opinion mining and sentiment analysis",
      "author" : [ "Bo Pang", "Lillian Lee." ],
      "venue" : "Foundations and trends in information retrieval, 2(1-2):1–135.",
      "citeRegEx" : "Pang and Lee.,? 2008",
      "shortCiteRegEx" : "Pang and Lee.",
      "year" : 2008
    }, {
      "title" : "Thumbs up?: sentiment classification using machine learning techniques",
      "author" : [ "Bo Pang", "Lillian Lee", "Shivakumar Vaithyanathan." ],
      "venue" : "ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. ACL.",
      "citeRegEx" : "Pang et al\\.,? 2002",
      "shortCiteRegEx" : "Pang et al\\.",
      "year" : 2002
    }, {
      "title" : "Neuroergonomics: The brain at work",
      "author" : [ "Raja Parasuraman", "Matthew Rizzo." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Parasuraman and Rizzo.,? 2006",
      "shortCiteRegEx" : "Parasuraman and Rizzo.",
      "year" : 2006
    }, {
      "title" : "The haves and the have-nots: Leveraging unlabelled corpora for sentiment analysis",
      "author" : [ "Kashyap Popat", "Balamurali Andiyakkal Rajendran", "Pushpak Bhattacharyya", "Gholamreza Haffari." ],
      "venue" : "ACL 2013. ACL.",
      "citeRegEx" : "Popat et al\\.,? 2013",
      "shortCiteRegEx" : "Popat et al\\.",
      "year" : 2013
    }, {
      "title" : "Sentic patterns: Dependency-based rules for concept-level sentiment analysis",
      "author" : [ "Soujanya Poria", "Erik Cambria", "Gregoire Winterstein", "Guang-Bin Huang." ],
      "venue" : "Knowledge-Based Systems, 69:45–63.",
      "citeRegEx" : "Poria et al\\.,? 2014",
      "shortCiteRegEx" : "Poria et al\\.",
      "year" : 2014
    }, {
      "title" : "Detecting turnarounds in sentiment analysis: Thwarting",
      "author" : [ "Ankit Ramteke", "Akshat Malu", "Pushpak Bhattacharyya", "J Saketha Nath." ],
      "venue" : "ACL (2), pages 860–865.",
      "citeRegEx" : "Ramteke et al\\.,? 2013",
      "shortCiteRegEx" : "Ramteke et al\\.",
      "year" : 2013
    }, {
      "title" : "Lexical complexity and fixation times in reading: Effects of word frequency, verb complexity, and lexical ambiguity",
      "author" : [ "Keith Rayner", "Susan A Duffy." ],
      "venue" : "Memory & Cognition, 14(3):191–201.",
      "citeRegEx" : "Rayner and Duffy.,? 1986",
      "shortCiteRegEx" : "Rayner and Duffy.",
      "year" : 1986
    }, {
      "title" : "Eye movements in reading: Psycholinguistic studies",
      "author" : [ "Keith Rayner", "Sara C Sereno" ],
      "venue" : null,
      "citeRegEx" : "Rayner and Sereno.,? \\Q1994\\E",
      "shortCiteRegEx" : "Rayner and Sereno.",
      "year" : 1994
    }, {
      "title" : "Sarcasm as contrast between a positive sentiment and negative situation",
      "author" : [ "Ellen Riloff", "Ashequl Qadir", "Prafulla Surve", "Lalindra De Silva", "Nathan Gilbert", "Ruihong Huang." ],
      "venue" : "EMNLP, pages 704– 714.",
      "citeRegEx" : "Riloff et al\\.,? 2013",
      "shortCiteRegEx" : "Riloff et al\\.",
      "year" : 2013
    }, {
      "title" : "Alleviating data sparsity for twitter sentiment analysis",
      "author" : [ "Hassan Saif", "Yulan He", "Harith Alani." ],
      "venue" : "CEUR Workshop Proceedings (CEUR-WS. org).",
      "citeRegEx" : "Saif et al\\.,? 2012",
      "shortCiteRegEx" : "Saif et al\\.",
      "year" : 2012
    }, {
      "title" : "Detecting domain dedicated polar words",
      "author" : [ "Raksha Sharma", "Pushpak Bhattacharyya." ],
      "venue" : "Proceedings of the International Joint Conference on Natural Language Processing.",
      "citeRegEx" : "Sharma and Bhattacharyya.,? 2013",
      "shortCiteRegEx" : "Sharma and Bhattacharyya.",
      "year" : 2013
    }, {
      "title" : "What is the scanpath signature of syntactic reanalysis",
      "author" : [ "Titus von der Malsburg", "Shravan Vasishth" ],
      "venue" : "Journal of Memory and Language,",
      "citeRegEx" : "Malsburg and Vasishth.,? \\Q2011\\E",
      "shortCiteRegEx" : "Malsburg and Vasishth.",
      "year" : 2011
    }, {
      "title" : "Word sense and subjectivity",
      "author" : [ "Janyce Wiebe", "Rada Mihalcea." ],
      "venue" : "International Conference on Computational Linguistics and the 44th annual meeting of the ACL, pages 1065–1072. ACL.",
      "citeRegEx" : "Wiebe and Mihalcea.,? 2006",
      "shortCiteRegEx" : "Wiebe and Mihalcea.",
      "year" : 2006
    }, {
      "title" : "Recognizing contextual polarity in phraselevel sentiment analysis",
      "author" : [ "Theresa Wilson", "Janyce Wiebe", "Paul Hoffmann." ],
      "venue" : "EMNLP-HLT, pages 347–354. Association for Computational Linguistics.",
      "citeRegEx" : "Wilson et al\\.,? 2005",
      "shortCiteRegEx" : "Wilson et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 25,
      "context" : "Several sentiment analyzers exist in literature today (Liu and Zhang, 2012).",
      "startOffset" : 54,
      "endOffset" : 75
    }, {
      "referenceID" : 19,
      "context" : "Recent works, such as Kouloumpis et al. (2011), Agarwal et al.",
      "startOffset" : 22,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "(2011), Agarwal et al. (2011) and Barbosa and Feng (2010), attempt to conduct such analyses on user-generated content.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "(2011), Agarwal et al. (2011) and Barbosa and Feng (2010), attempt to conduct such analyses on user-generated content.",
      "startOffset" : 8,
      "endOffset" : 58
    }, {
      "referenceID" : 1,
      "context" : "Several methods have been proposed to address the different lexical level difficulties by - (a) using WordNet synsets and word cluster information to tackle lexical ambiguity and data sparsity (Akkaya et al., 2009; Balamurali et al., 2011; Go et al., 2009; Maas et al., 2011; Popat et al., 2013; Saif et al., 2012) and (b) mining domain dependent words (Sharma and Bhattacharyya, 2013; Wiebe and Mihalcea, 2006).",
      "startOffset" : 193,
      "endOffset" : 314
    }, {
      "referenceID" : 2,
      "context" : "Several methods have been proposed to address the different lexical level difficulties by - (a) using WordNet synsets and word cluster information to tackle lexical ambiguity and data sparsity (Akkaya et al., 2009; Balamurali et al., 2011; Go et al., 2009; Maas et al., 2011; Popat et al., 2013; Saif et al., 2012) and (b) mining domain dependent words (Sharma and Bhattacharyya, 2013; Wiebe and Mihalcea, 2006).",
      "startOffset" : 193,
      "endOffset" : 314
    }, {
      "referenceID" : 13,
      "context" : "Several methods have been proposed to address the different lexical level difficulties by - (a) using WordNet synsets and word cluster information to tackle lexical ambiguity and data sparsity (Akkaya et al., 2009; Balamurali et al., 2011; Go et al., 2009; Maas et al., 2011; Popat et al., 2013; Saif et al., 2012) and (b) mining domain dependent words (Sharma and Bhattacharyya, 2013; Wiebe and Mihalcea, 2006).",
      "startOffset" : 193,
      "endOffset" : 314
    }, {
      "referenceID" : 26,
      "context" : "Several methods have been proposed to address the different lexical level difficulties by - (a) using WordNet synsets and word cluster information to tackle lexical ambiguity and data sparsity (Akkaya et al., 2009; Balamurali et al., 2011; Go et al., 2009; Maas et al., 2011; Popat et al., 2013; Saif et al., 2012) and (b) mining domain dependent words (Sharma and Bhattacharyya, 2013; Wiebe and Mihalcea, 2006).",
      "startOffset" : 193,
      "endOffset" : 314
    }, {
      "referenceID" : 41,
      "context" : "Several methods have been proposed to address the different lexical level difficulties by - (a) using WordNet synsets and word cluster information to tackle lexical ambiguity and data sparsity (Akkaya et al., 2009; Balamurali et al., 2011; Go et al., 2009; Maas et al., 2011; Popat et al., 2013; Saif et al., 2012) and (b) mining domain dependent words (Sharma and Bhattacharyya, 2013; Wiebe and Mihalcea, 2006).",
      "startOffset" : 193,
      "endOffset" : 314
    }, {
      "referenceID" : 47,
      "context" : "Several methods have been proposed to address the different lexical level difficulties by - (a) using WordNet synsets and word cluster information to tackle lexical ambiguity and data sparsity (Akkaya et al., 2009; Balamurali et al., 2011; Go et al., 2009; Maas et al., 2011; Popat et al., 2013; Saif et al., 2012) and (b) mining domain dependent words (Sharma and Bhattacharyya, 2013; Wiebe and Mihalcea, 2006).",
      "startOffset" : 193,
      "endOffset" : 314
    }, {
      "referenceID" : 48,
      "context" : ", 2012) and (b) mining domain dependent words (Sharma and Bhattacharyya, 2013; Wiebe and Mihalcea, 2006).",
      "startOffset" : 46,
      "endOffset" : 104
    }, {
      "referenceID" : 50,
      "context" : ", 2012) and (b) mining domain dependent words (Sharma and Bhattacharyya, 2013; Wiebe and Mihalcea, 2006).",
      "startOffset" : 46,
      "endOffset" : 104
    }, {
      "referenceID" : 42,
      "context" : "Approaches leveraging syntactic properties of text include generating dependency based rules for SA (Poria et al., 2014) and leveraging local dependency (Li et al.",
      "startOffset" : 100,
      "endOffset" : 120
    }, {
      "referenceID" : 22,
      "context" : ", 2014) and leveraging local dependency (Li et al., 2010).",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 43,
      "context" : "Most attempts towards handling thwarting (Ramteke et al., 2013) and sarcasm and irony (Carvalho et al.",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : ", 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.",
      "startOffset" : 30,
      "endOffset" : 170
    }, {
      "referenceID" : 46,
      "context" : ", 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.",
      "startOffset" : 30,
      "endOffset" : 170
    }, {
      "referenceID" : 23,
      "context" : ", 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.",
      "startOffset" : 30,
      "endOffset" : 170
    }, {
      "referenceID" : 29,
      "context" : ", 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.",
      "startOffset" : 30,
      "endOffset" : 170
    }, {
      "referenceID" : 3,
      "context" : ", 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.",
      "startOffset" : 30,
      "endOffset" : 170
    }, {
      "referenceID" : 19,
      "context" : ", 2013) and sarcasm and irony (Carvalho et al., 2009; Riloff et al., 2013; Liebrecht et al., 2013; Maynard and Greenwood, 2014; Barbieri et al., 2014; Joshi et al., 2015), rely on distant supervision based techniques (e.",
      "startOffset" : 30,
      "endOffset" : 170
    }, {
      "referenceID" : 40,
      "context" : "Our hypothesis is that cognitive processes in the brain are related to eye-movement activities (Parasuraman and Rizzo, 2006).",
      "startOffset" : 95,
      "endOffset" : 124
    }, {
      "referenceID" : 39,
      "context" : "Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al.",
      "startOffset" : 83,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al.",
      "startOffset" : 83,
      "endOffset" : 152
    }, {
      "referenceID" : 28,
      "context" : "Sentiment classification has been a long standing NLP problem with both supervised (Pang et al., 2002; Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised (Mei et al.",
      "startOffset" : 83,
      "endOffset" : 152
    }, {
      "referenceID" : 30,
      "context" : ", 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task.",
      "startOffset" : 52,
      "endOffset" : 88
    }, {
      "referenceID" : 24,
      "context" : ", 2007; Martineau and Finin, 2009) and unsupervised (Mei et al., 2007; Lin and He, 2009) machine learning based approaches existing for the task.",
      "startOffset" : 52,
      "endOffset" : 88
    }, {
      "referenceID" : 34,
      "context" : "Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role.",
      "startOffset" : 84,
      "endOffset" : 130
    }, {
      "referenceID" : 38,
      "context" : "Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008) and in such approaches, feature engineering plays an important role.",
      "startOffset" : 84,
      "endOffset" : 130
    }, {
      "referenceID" : 8,
      "context" : "(Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al.",
      "startOffset" : 0,
      "endOffset" : 36
    }, {
      "referenceID" : 36,
      "context" : "(Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al.",
      "startOffset" : 0,
      "endOffset" : 36
    }, {
      "referenceID" : 28,
      "context" : ", 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al.",
      "startOffset" : 30,
      "endOffset" : 80
    }, {
      "referenceID" : 35,
      "context" : ", 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al.",
      "startOffset" : 30,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : ", 2010), semantic properties (Balamurali et al., 2011) and effect of negators (Ikeda et al.",
      "startOffset" : 29,
      "endOffset" : 54
    }, {
      "referenceID" : 15,
      "context" : ", 2011) and effect of negators (Ikeda et al., 2008) are also used as features for the task of sentiment classification.",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : ", 2010), semantic properties (Balamurali et al., 2011) and effect of negators (Ikeda et al., 2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches.",
      "startOffset" : 30,
      "endOffset" : 338
    }, {
      "referenceID" : 2,
      "context" : ", 2010), semantic properties (Balamurali et al., 2011) and effect of negators (Ikeda et al., 2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to space limitations). Joshi et al. (2014) develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking.",
      "startOffset" : 30,
      "endOffset" : 644
    }, {
      "referenceID" : 2,
      "context" : ", 2010), semantic properties (Balamurali et al., 2011) and effect of negators (Ikeda et al., 2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to space limitations). Joshi et al. (2014) develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking. Mishra et al. (2014) study sentiment detection, and subjectivity extraction through anticipation and homing, with the use of eye tracking.",
      "startOffset" : 30,
      "endOffset" : 773
    }, {
      "referenceID" : 2,
      "context" : ", 2010), semantic properties (Balamurali et al., 2011) and effect of negators (Ikeda et al., 2008) are also used as features for the task of sentiment classification. The fact that sentiment expression may be complex to be handled by traditional features is evident from a study of comparative sentences by Ganapathibhotla and Liu (2008). This, however has not been addressed by feature based approaches. Eye-tracking technology has been used recently for sentiment analysis and annotation related research (apart from the huge amount of work in psycholinguistics that we find hard to enlist here due to space limitations). Joshi et al. (2014) develop a method to measure the sentiment annotation complexity using cognitive evidence from eye-tracking. Mishra et al. (2014) study sentiment detection, and subjectivity extraction through anticipation and homing, with the use of eye tracking. Regarding other NLP tasks, Joshi et al. (2013) proposed a studied the cognitive aspects if Word Sense Disambiguation (WSD) through eye-",
      "startOffset" : 30,
      "endOffset" : 938
    }, {
      "referenceID" : 29,
      "context" : "Earlier, Mishra et al. (2013) measured translation annotation difficulty of a given sentence based on gaze input of translators used to label training data.",
      "startOffset" : 9,
      "endOffset" : 30
    }, {
      "referenceID" : 25,
      "context" : "We acknowledge that some of the well performing sentiment analyzers use Deep Learning techniques (like Convolutional Neural Network based approach by Maas et al. (2011) and Recursive Neural Network based approach by dos Santos and Gatti (2014)).",
      "startOffset" : 150,
      "endOffset" : 169
    }, {
      "referenceID" : 9,
      "context" : "(2011) and Recursive Neural Network based approach by dos Santos and Gatti (2014)).",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 28,
      "context" : "Dataset 1 has been released by Mishra et al. (2016) which they use for the task of sarcasm understandability prediction.",
      "startOffset" : 31,
      "endOffset" : 52
    }, {
      "referenceID" : 17,
      "context" : "Dataset 2 has been used by Joshi et al. (2014) for the task of sentiment annotation complexity prediction.",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 14,
      "context" : "The statistical classifiers are based on Support Vector Machine (SVM) and Näive Bayes (NB) implemented using Weka (Hall et al., 2009) and LibSVM (Chang and Lin, 2011) APIs.",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 7,
      "context" : ", 2009) and LibSVM (Chang and Lin, 2011) APIs.",
      "startOffset" : 19,
      "endOffset" : 40
    }, {
      "referenceID" : 51,
      "context" : "The in-house rule based (RB) classifier decides the sentiment labels based on the counts of positive and negative words present in the snippet, computed using MPQA lexicon (Wilson et al., 2005).",
      "startOffset" : 172,
      "endOffset" : 193
    }, {
      "referenceID" : 7,
      "context" : ", 2009) and LibSVM (Chang and Lin, 2011) APIs. These are on trained on 10662 snippets comprising movie reviews and tweets, randomly collected from standard datasets released by Pang and Lee (2004) and Sentiment 140 (http:// www.",
      "startOffset" : 20,
      "endOffset" : 197
    }, {
      "referenceID" : 7,
      "context" : ", 2009) and LibSVM (Chang and Lin, 2011) APIs. These are on trained on 10662 snippets comprising movie reviews and tweets, randomly collected from standard datasets released by Pang and Lee (2004) and Sentiment 140 (http:// www.sentiment140.com/). The feature-set comprises traditional features for SA reported in a number of papers. They are discussed in section 4 under the category of Sentiment Features. The in-house rule based (RB) classifier decides the sentiment labels based on the counts of positive and negative words present in the snippet, computed using MPQA lexicon (Wilson et al., 2005). It also considers negators as explained by Jia et al. (2009) and intensifiers as explained by Dragut and Fellbaum (2014).",
      "startOffset" : 20,
      "endOffset" : 664
    }, {
      "referenceID" : 7,
      "context" : ", 2009) and LibSVM (Chang and Lin, 2011) APIs. These are on trained on 10662 snippets comprising movie reviews and tweets, randomly collected from standard datasets released by Pang and Lee (2004) and Sentiment 140 (http:// www.sentiment140.com/). The feature-set comprises traditional features for SA reported in a number of papers. They are discussed in section 4 under the category of Sentiment Features. The in-house rule based (RB) classifier decides the sentiment labels based on the counts of positive and negative words present in the snippet, computed using MPQA lexicon (Wilson et al., 2005). It also considers negators as explained by Jia et al. (2009) and intensifiers as explained by Dragut and Fellbaum (2014). Table 1 presents the accuracy of the three systems.",
      "startOffset" : 20,
      "endOffset" : 724
    }, {
      "referenceID" : 25,
      "context" : "1 Sentiment Features We consider a series of textual features that have been extensively used in sentiment literature (Liu and Zhang, 2012).",
      "startOffset" : 118,
      "endOffset" : 139
    }, {
      "referenceID" : 51,
      "context" : "Presence of positive and negative words computed against MPQA lexicon (Wilson et al., 2005), a popular lexicon used for sentiment analysis.",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 11,
      "context" : "Scores of positive subjectivity and negative subjectivity using SentiWordNet (Esuli and Sebastiani, 2006).",
      "startOffset" : 77,
      "endOffset" : 105
    }, {
      "referenceID" : 42,
      "context" : "The features are taken from Riloff et al. (2013), Ramteke et al.",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 40,
      "context" : "(2013), Ramteke et al. (2013) and Joshi et al.",
      "startOffset" : 8,
      "endOffset" : 30
    }, {
      "referenceID" : 17,
      "context" : "(2013) and Joshi et al. (2015).",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 46,
      "context" : "Presence of positive phrases followed by negative situational phrase (computed using bootstrapping technique suggested by Riloff et al. (2013)).",
      "startOffset" : 122,
      "endOffset" : 143
    }, {
      "referenceID" : 17,
      "context" : "Sentence polarity found by supervised logistic regression using the dataset used by Joshi et al. (2015).",
      "startOffset" : 84,
      "endOffset" : 104
    }, {
      "referenceID" : 45,
      "context" : "Since these behaviors intuitively relate to the cognitive process of the readers (Rayner and Sereno, 1994), we consider simple statistical properties of these factors as features to our model.",
      "startOffset" : 81,
      "endOffset" : 106
    }, {
      "referenceID" : 31,
      "context" : "Some of these features have been reported by Mishra et al. (2016) for modeling sarcasm understandability of readers.",
      "startOffset" : 45,
      "endOffset" : 66
    }, {
      "referenceID" : 44,
      "context" : "This is motivated by Rayner and Duffy (1986).",
      "startOffset" : 21,
      "endOffset" : 45
    }, {
      "referenceID" : 49,
      "context" : "This is also supported by von der Malsburg and Vasishth (2011).",
      "startOffset" : 34,
      "endOffset" : 63
    }, {
      "referenceID" : 27,
      "context" : "Regressions correspond to both lexical and syntactic reanalysis (Malsburg et al., 2015).",
      "startOffset" : 64,
      "endOffset" : 87
    }, {
      "referenceID" : 20,
      "context" : "Flesch Readability Ease score of the text (Kincaid et al., 1975).",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "These systems are implemented using the Weka (Hall et al., 2009) and LibSVM (Chang and Lin, 2011) APIs.",
      "startOffset" : 45,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : ", 2009) and LibSVM (Chang and Lin, 2011) APIs.",
      "startOffset" : 19,
      "endOffset" : 40
    } ],
    "year" : 2016,
    "abstractText" : "Sentiments expressed in user-generated short text and sentences are nuanced by subtleties at lexical, syntactic, semantic and pragmatic levels. To address this, we propose to augment traditional features used for sentiment analysis and sarcasm detection, with cognitive features derived from the eye-movement patterns of readers. Statistical classification using our enhanced feature set improves the performance (F-score) of polarity detection by a maximum of 3.7% and 9.3% on two datasets, over the systems that use only traditional features. We perform feature significance analysis, and experiment on a held-out dataset, showing that cognitive features indeed empower sentiment analyzers to handle complex constructs.",
    "creator" : "TeX"
  }
}