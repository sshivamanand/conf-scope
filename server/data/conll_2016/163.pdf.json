{
  "name" : "163.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Redefining part-of-speech classes with distributional semantic models",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Parts of speech (PoS) are useful abstractions, but still abstractions. Boundaries between them in natural languages are flexible. Sometimes, large open classes of words are situated on the verge between several parts of speech: for example, participles in English are in many respects both verbs and adjectives. In other cases, closed word classes ‘intersect’ between themselves: it is often difficult to tell a determiner from a possessive pronoun, etc. As (Houston, 1985) puts it, ‘Grammatical categories exist along a continuum which does not exhibit sharp boundaries between the categories’.\nWhen annotating natural texts for parts of speech, the choice of a PoS tag in many ways depends on the human annotators themselves, but also on the quality of linguistic conventions behind the division into different word classes. That is\nwhy there were always attempts to refine the definitions of parts of speech and to make them more ‘real’ and data-driven, produced from corpora of real texts: see, among others, the seminal work of (Biber et al., 1999). The aim of such attempts is to identify clusters of words occurring naturally and corresponding to what we usually call ‘parts of speech’. One of the main distance metrics that can be used in detecting such clusters is a distance between distributional features of words (their contexts in a reference training corpus).\nIn this paper, we test this approach using predictive models developed in the field of distributional semantics. Recent achievements in training distributional models of language using machine learning allows for robust representations of natural language semantics created in a completely unsupervised way, using only large corpora of raw text. Relations between dense word vectors (embeddings) in the resulting vector space are of course mostly semantic. But can they be used to discover something new about grammar and syntax, particularly parts of speech? Do learned semantic vectors help here? Below we show that such models do contain a lot of interesting data related to PoS classes.\nThe rest of the paper is organized as follows. In Section 2 we briefly cover the previous work on the subject of parts of speech and distributional models. Section 3 describes data processing and the training of a PoS predictor based on word embeddings. In Section 4 errors of this predictor are analyzed and insights gained from them described. Section 5 introduces an attempt to build a fullfledged PoS tagger within the same approach. It also analyzes the correspondence between particular vector components and PoS affiliation, before we conclude in Section 6.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199"
    }, {
      "heading" : "2 Related work",
      "text" : "Traditionally three types of criteria are used to distinguish different parts of speech: formal (or morphological), syntactic (or distributional) and semantic (Aarts and McMahon, 2008). Arguably, syntactic and semantic criteria are not much different from each other, if one follows the famous distributional hypothesis stating that meaning is determined by context (Firth, 1957). Below we show that unsupervised distributional semantic models obviously contain data related to parts of speech.\nFor several years already it has been known that some information about morphological word classes is indeed stored in distributional models; (Tsuboi, 2014) even employed it to improve PoStagging. Words belonging to different parts of speech possess different contexts: in English, articles are typically followed by nouns, verbs are typically accompanied by adverbs and so on. It means that during the training stage, words of one PoS should theoretically cluster together or at least their embeddings should retain some similarity allowing to separate them from words belonging to other parts of speech.\n(Mikolov et al., 2013b) showed that there are indeed also regular relations between words from different classes: the vector of ‘Brazil’is related to ‘Brazilian’ in the same way as ‘England’ is related to ‘English’ and so on. Later, (Liu et al., 2016) demonstrated how words of the same part of speech cluster into distinct groups in a distributional model, and (Tsvetkov et al., 2015) proved that dimensions of distributional models are correlated with different linguistic features, releasing the evaluation dataset based on this.\nIt seems that one can infer data about PoS classes of words from embedding models. But then, it can be useful for deeper analysis of part of speech boundaries, leading to discovery of separate words or whole classes that tend to behave ‘strangely’. Discovering such cases is one possible way to improve performance of existing automatic PoS taggers (Manning, 2011). These ‘outliers’ may signal the necessity to revise the annotation strategy or classification system in general. Section 3 describes the process of constructing typical PoS clusters and detecting words which seem to belong to a cluster different from their traditional annotation."
    }, {
      "heading" : "3 Part of speech clusters in distributional models",
      "text" : "Our hypothesis is that for the majority of words their part of speech can be inferred from their embeddings in a distributional model. This inference can be considered a classification problem: we are to train an algorithm that takes a word vector as input and outputs its part of speech. If the word embeddings do contain PoS-related data, the properly trained classifier will correctly predict PoS tags for the majority of words: it means that these lexical entities conform to a dominant distributional pattern of their part of speech class. At the same time, the words for which the classifier outputs incorrect predictions, are expected to be ‘outliers’, with different distributional patterns, different from other words in the same class. These cases are the points of linguistic interest, and in the rest of the paper we mostly concentrate on them.\nTo test the initial hypothesis, we used the XML Edition of British National Corpus (BNC), a balanced and representative corpus of English language of about 98 million word tokens in size. As stated in the corpus documentation, ‘it was [PoS]tagged automatically, using the CLAWS4 automatic tagger developed by Roger Garside at Lancaster, and a second program, known as Template Tagger, developed by Mike Pacey and Steve Fligelstone’ (Burnard, 2007). The corpus authors report a precision of 0.96 and recall of 0.99 for their tools, based on a manually checked sample. For this research, it is important that BNC is an established and well-studied corpus of English with PoS-tags and lemmas assigned to all words.\nWe produced a version of BNC where all the words were replaced with their lemmas and PoStags converted into the Universal Part-of-Speech Tagset (Petrov et al., 2012)1. Thus, each token was represented as a concatination of its lemma and PoS tag (for example, ‘love_VERB’ and ‘love_NOUN’ yield different word types). The mappings between BNC tags and Universal tags were created manually by us and released online2. We worked with the following 16 Universal tags: ADJ, ADP, ADV, AUX, CONJ, DET, INTJ, NOUN, NUM, PART, PRON, PROPN, SCONJ, SYM, VERB, X (tokens marked with PUNCT tag were excluded).\n1We used the latest version of the tagset available at http://universaldependencies.org\n2Anonymized\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nThen, a Continuous Skipgram embedding model (Mikolov et al., 2013a) was trained on this corpus, using a vector size of 300, 10 negative samples, a symmetric window of 2 words, no down-sampling, and 5 iterations over the training data. Words with corpus frequency less than 5 were ignored. This model was then taken to represent the semantics of the words it contained. But at the same time, for each word, a gold standard PoS tag is known (from the BNC annotation). It means that is is possible to test how good the word embeddings are in grouping words according to their parts of speech.\nTo this end, we extracted vectors for the 10 000 most frequent words from the resulting model (roughly, these are the words with corpus frequency more than 500). Then, these vectors were used to train a simple logistic regression multinomial classifier aimed to predict the word’s part of speech3.\nNote that during training (and subsequent testing), each word’s vector was used several times, proportional to frequency of the word in the corpus, so the classifier was trained on 177 343 (sometimes repeating) instances, instead of the original 10 000. This was done to alleviate the classifier’s part of speech bias. There are much fewer word types in the closed PoS classes (pronouns, conjunctions, etc.) than in the open ones (nouns, verbs, etc.), so without considering word frequency, the model does not have a chance to learn good predictors for ‘rare’ classes and ends up never predicting them. At the same time, words from closed classes occur very frequently in the running text, so after ‘weighting’ training instances by corpus frequency, the balance is restored and the classifier model has enough training instances to learn to predict closed PoS classes as well. As an additional benefit, by this modification we make frequent words from all classes to be more ‘influential’ in training the classifier.\nThe resulting classifier showed a weighted average F-score equal to 0.979 with 10-fold crossvalidation on the training set. This is a significant improvement over the majority class baseline classifier (classify everything as nouns), which showed an F-score of 0.07 and over the onefeature baseline classifier (classify using only one\n3It is important that we applied classification, not clustering here. Attempts to naively cluster word vectors into the number of clusters equal to the number of PoS tags inescapably failed.\nvector dimension with maximum F-value in relation to class tags), with F-score equal to only 0.22. Thus, the results support the hypothesis that word embeddings contain information allowing to group words together based on their parts of speech. At the same time, we see that this information is not restricted to some particular vector component: rather, it is distributed among several axis of the vector space.\nAfter training the classifier, we were able to use it to detect ‘outlying’ words in the BNC (judging by the distributional model). So as not to experiment on the same data we had trained our classifier on, we compiled another test set of 17 000 vectors for words with BNC frequencies between 100 and 500. They were weighted by word frequencies in the same way as the training set, and the resulting test set contained 30 710 instances. Then, we predicted parts of speech for these words using our classifier and evaluated its performance on them. The results on totally unseen data were not much worse, with an F-score equal to 0.91.\nFurthermore, to make sure that the results can potentially be extended to other texts, we applied the trained classifier to all lemmas from the human-annotated Universal Dependencies English Treebank (Silveira et al., 2014). The words not present in the distributional model were omitted (they sum to 27% of word types and 10% of word tokens). The classifier showed an F-Score equal to 0.99, further demonstrating the robustness of the classifier.\nIn sum, the vast majority of words are classified correctly, which means that their embeddings enables detecting their parts of speech. In fact, one can even visualize ‘prototypical’ vectors for each PoS by simply averaging vectors of words belonging to this part of speech. We did this for 10 000 words from our training set.\nPlots for prototypical vectors of coordinating and subordinating conjunctions are shown in the Figures 1 and 2 respectively. Even visually one can notice a very strongly expressed feature near the ‘100’ mark in the horizontal axis. In fact, this is vector component number 94, and it is indeed an idiosyncratic feature of conjunctions: none of other parts of speech shows such a property. More details about what vector components are relevant to part of speech affiliation are given in Section 5.\nWith prototypical PoS vectors we can even find out how similar different parts of speech are to\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nFigure 1. Prototypical embedding for coordinating conjunctions\nFigure 2. Prototypical embedding for subordinating conjunctions\neach other, by simply measuring cosine similarity between them. If we rank PoS pairs according to their similarity, what we see is that nominative parts of speech are close to each other, determiners and pronouns are also similar, as well as prepositions and subordinating conjunctions, everything quite in accordance with language intuition. It is interesting that proper nouns are not much similar to common nouns, with cosine similarity between them only 0.67 (even adverbs are closer). As we show below, this helps the model to successfully separate the former from the latter.\nDespite generally good performance of the classifier, if we look at our BNC test set, 1741 word types (about 10% of the whole test set vocabulary) were still classified incorrectly. Thus, they are somehow dissimilar to ‘prototypical’ words of their parts of speech. These are the ‘outliers’ we were after. We investigate the patterns found among them in the next section."
    }, {
      "heading" : "4 Not from this crowd: analyzing outliers",
      "text" : "We filtered out mis-classified word types with ‘X’ gold annotation (they are mostly foreign or nonsense words). This left us with 1558 words; the classifier assigned them part of speech tags\ndifferent from the ones in the BNC. It probably means that these words’ distributional patterns differ somehow from the ‘mainstream’, and they tend to exhibit behavior similar to another part of speech. Table 1 shows the most frequent misclassification cases, together accounting for more than 85% of errors.\nAdditionally, we ranked mis-classification cases by ‘part of speech coverage’, that is by the ratio of the words belonging to a particular PoS for which our classifier outputs this particular type of misclassification. For example, proper nouns misclassified as common nouns constitute the most numerous error type in Table 1, but in fact only 9% of all proper nouns in the test set were misclassified in this way. There are parts of speech with a much larger portion of word-types predicted erroneously: e.g., 22% of subordinate conjunctions were classified as adverbs. Table 2 lists error types with the highest coverage (we excluded error types with absolute frequency equal to 1, as it is impossible to speculate on solitary cases).\nWe now describe some of the interesting cases. Almost 30% of error types (judging by absolute amount of mis-classified words) consist of proper nouns predicted to be common ones and vice versa. These cases do not tell us anything new, as it is obvious that distributionally these two classes of words are very similar, take the same syntactic contexts and hardly can be considered differ-\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nent parts of speech at all. At the same time, it is interesting that the majority of proper nouns in the test set (88%) was correctly predicted as such. It means that in spite of contextual similarity, the distributional model has managed to extract features typical for proper names. Errors mostly cover comparatively rare names, such as ‘luftwaffe’, ‘stasi’, ‘stonehenge’, or ‘himalayas’. Our guess is that the model was just not presented with enough contexts for these words to learn meaningful representations. Also, they are mostly not personal names but toponyms or organization names. Most probably, the model has trained to distinguish proper names by contexts like ‘My name is’, etc, obviously not appropriate here.\nAnother 30% of errors are due to vague boundaries between nominal and adjectival distribution patterns in English: nouns can be modified by both (it seems that cases where a proper noun is mistaken for an adjective are often caused by the same factor). Words like ‘materialist_NOUN’, ‘starboard_NOUN’ or ‘hypertext_NOUN’ are tagged as nouns in the BNC, but they often modify other nouns, and their contexts are so ‘adjectival’ that the distributional model actually assigned them semantic features highly similar to those of adjectives. Vice versa, ‘white-collar_ADJ’ (an adjective in BNC) is surely a noun from the point of view of our model. Indeed, there can be contradicting views on the correct part of speech for this word in phrases like ‘and all the other whitecollar workers’. Thus, in this case the distributional model highlights the already known similarity between two word classes.\nThe cases with verbs mistaken for adjectives seem to be caused mostly by passive participles (‘was overgrown’, ‘is indented’, ‘’), which intuitively are indeed very adjective-like. So, this gives us a set of verbs dominantly (or almost exclusively, like ‘to intertwine’ or ‘to disillusion’) used in passive. Of course, we will hardly announce such verbs to be adjectives based on that evidence, but at least we can be sure that this subclass of verbs is clearly semantically and distributionally different from other verbs.\nThe next numerous type of errors consists of common nouns predicted to be numerals. A quick glance at the data reveals that 90% of these ‘nouns’ are in fact currency amounts and percentages (‘£70’, ‘33%’, ‘$1’, etc). It seems pretty log-\nical to classify these as numerals, despite of them containing some kind of nominative entities inside. Judging by the classifier’s decisions, their contexts do not differ much from those of simple numbers, and their semantics is similar. The Universal Dependencies Treebank is more consistent in this respect: it separates entities like ‘1$’ into two tokens: a numeral (NUM) and a symbol (SYM). Consequently, when our classifier was tested on words from the UD Treebank, there was only one occurrence of this type of error.\nRelated to this is the inverse case of numerals predicted to be common or proper nouns. It is interesting that this error type is also quite massive in its coverage: if we combine numerals predicted to be common and proper nouns, we will see that 17% of all numerals in the test set were subject to this error. The majority of these ‘numerals’ are years (‘1804’, ‘1776’, ‘1822’) and decades (‘1820s’, ‘60s’ and even ‘twelfths’). Intuitively, such entities do indeed functions as nouns (‘I’d like to return to the sixties’). Anyway, it is difficult to invent a persuasive reason for why ‘fifty pounds’ should be tagged as a noun, but ‘the year 1776’ as a numeral. So, this points to possible (minor) inconsistencies in the annotation strategy of the BNC. Note that a similar problem exists in the Penn Treebank as well (Manning, 2011).\nAdverbs classified as nouns (53 words in total for both common and proper nouns) are possibly the ones often followed by verbs or appearing in company of adjectives (examples are ‘ultra’ and ‘kinda’). This made the model treat them as close to the nominative classes. Interestingly, most ‘adverbs’ predicted to be proper nouns are time indicators (‘7pm’, ‘11am’); this also raises questions about what adverbial features are present in these entities. Once again, the UD Treebank does not tag them as adverbs.\nThe cases we described above revealed some inconsistencies in the BNC annotation. However, it seems that with adverbs mistaken for adjectives, we actually found a systematic error in the BNC tagging: these cases are mostly connected to adjectives like ‘plain’, ‘clear’ or ‘sharp’ (including comparative and superlative forms) erroneously tagged in the corpus as adverbs. These cases are not rare: just the three adjectives we mentioned alone appear in the BNC about 600 times with an adverb tag, mostly in phrases of the kind ‘the author makes it plain that. . . ’. Sometimes these to-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nTable 2. Coverage of mis-classifications (from all word types of this PoS) with distributional predictor\nCoverage Actual PoS\nPredicted PoS Absolute amount\n0.22 SCONJ ADV 2 0.17 INTJ PROPN 8 0.11 ADP ADJ 3 0.09 ADJ NOUN 313 0.09 PROPN NOUN 347 0.09 NUM NOUN 52 0.08 NUM PROPN 45\nkens are tagged as ambiguous, and the adjective tag is there as a second variant; however, the corpus documentation states that in such cases the first variant is always more likely. Thus, distributional models can actually detect outright errors in PoS-tagged corpora, when incorrectly tagged words strongly tend to cluster with another part of speech. In the UD treebank such examples can also be observed, but they are much fewer and more ‘adverbial’, like ‘it goes clear through’.\nSome of the entries from Table 2 were already covered above, except the first three cases. They are related to closed word classes (functional words), that’s why the absolute number of influenced word types is low, but the coverage (ratio of all words of this PoS) is quite high.\nFirst, of 9 distinct subordinate conjunctions in the test set, two were predicted to be adverbs. This is not surprising, as these words are ‘seeing’ and ‘immediately’. For ‘seeing’ the prediction seems to be just a random guess (the prediction confidence was as low as 0.3), but with ‘immediately’ the classifier was actually more correct than the BNC tagger (the prediction confidence was about 0.5). In BNC, these words are mostly tagged as subordinate conjunctions in cases when they are in the beginning of sentences (‘Immediately, she lowered the gun’). The other words marked as SCONJ in the test set are really such, and the classifier made correct predictions matching the BNC tags.\nInterjections mistaken for proper names do not seem very interpretable (examples are ‘gee’, ‘oy’ and ‘farewell’). At the same time, 3 prepositions predicted to be adjectives clearly form a separate group: they are ‘cross’, ‘pre’ and ‘pro’. They are\nnot often used as separate words, but when they are (‘Did anyone encounter any trouble from Hibs fans in Edinburgh pre season?’), they are very close to adjectives or adverbs, so the predictions of the distributional classifier once again suggest shifting parts of speech boundaries a bit.\nError analysis on the vocabulary from the Universal Dependencies Treebank showed pretty much the same results, except for some differences mentioned above.\nThere exists another way to retrieve this kind of data: to process gold standard data with a mainstream PoS tagger and analyze the resulting confusion matrix. We tested this approach by processing the whole BNC with the Stanford PoS Tagger (Toutanova et al., 2003). Note that as an input to the tagger we used not the whole sentences from the corpora, but separate tokens, to mimic our workflow with the distributional predictor. Prior to this, BNC tags were converted to the Penn Treebank tagset4 to match the output of the tagger. As we are interested in coarse, ‘overarching’ word classes, inflectional forms were merged into one tag (for example plural and singular nouns NNS and NN were considered to belong to one noun class NN, etc). That was easy to accomplish by dropping all characters of the tags after the first two (excluding proper noun tags, which were all converted to NNP).\nAnalysis of the confusion matrix (cases where the tag predicted by the Stanford tagger was different from the BNC tag) revealed the most frequent error types shown in Table 3. Despite similar top positions of errors types ‘proper noun predicted as common noun’ and ‘nouns and adjectives mistaken for each other’, there are also very frequent errors of types ‘verb to noun’ and ‘adjective to verb’, not observed in the distributional confusion matrix (Table 1). We would not be able to draw the same insights that we drew from the distributional confusion matrix: the case with verbs mistaken for adjective is ranked only 12th, adverbs mistaken for nouns - 13th, etc.\nTable 4 shows top mis-classification types by their word type coverage. Once again, interesting cases we discovered with the distributional confusion matrix (like subordinating conjunctions mistaken for adverbs and prepositions mistaken for adjectives) did not show up. Obviously, a lot of other insights can be extracted from the Stanford\n4https://www.cis.upenn.edu/~treebank/\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nTable 3. Most frequent PoS mis-classifications using the Stanford tagger\nAmount (word types)\nActual PoS Predicted PoS\n172675 NNP NN 47202 VB NN 40218 JJ NN 24075 NN JJ 9723 JJ VB\nTable 4. Coverage of mis-classifications (from all word types of this PoS) with the Stanford tagger\nCoverage Actual PoS\nPredicted PoS Absolute amount\n0.91 NNP NN 172675 0.8 UH NN 576 0.79 DT NN 217 0.78 EX JJ 11 0.78 PR NN 517\nTagger errors (and it was studied before), but it seems that employing a distributional predictor reveals different error cases and thus might be a useful tool.\nTo sum it up, analysis of ‘boundary cases’ detected by a classifier trained on distributional vectors, indeed reveals sub-classes of words lying on the verge between different parts of speech. It also allows for quickly discovering systematic errors or inconsistencies in PoS tags, whether they be automatic or manual. Thus, discussions about PoS boundaries definitely should take into consideration this data (expanded and revised)."
    }, {
      "heading" : "5 Distributional vectors as part-of-speech predictors",
      "text" : "In the experiment described in the previous section, we used the model trained on words concatenated with their PoS tags. Thus, our ‘classifier’ was a bit artificial in that it demanded a word plus a tag as an input, and then its output is a judgment about what part of speech is most applicable to this combination from the point of view of the BNC distributional patterns. This was not a problem for us, as our aim was exactly to discover lexical outliers.\nBut is it possible to construct a proper predictor in the same way, which is able to predict a PoS tag for a word without any pre-existing tags as hints? Preliminary experiments seem to indicate that it is.\nWe trained a Continuous Skipgram distributional model on the BNC lemmas without PoS tags. After that, we constructed a vocabulary of all unambiguous lemmas from the UD Treebank training set. ‘Unambiguous’ here means that the lemma either was always tagged with one and the same PoS tag in the Treebank, or has one ‘dominant’ tag, with frequencies of other PoS assignments not exceeding 1/2 of the dominant assignment frequency. Our hypothesis was that these words are prototypical examples of their PoS classes, with corresponding prototypical features most pronounced. We also removed words with frequency less than 10 in the Treebank. This left us with 1564 words from all Universal Tag classes (excluding PUNCT, X and SYM, as we hardly want to predict punctuation or symbol tag).\nThen the same simple logistic regression classifier was trained on the distributional vectors from the model for these 1564 words only, using UD Treebank tags as class labels (the training instances were again weighted proportionally to the words’ frequencies in the Treebank). The resulting classifier showed an accuracy of 0.938 after 10-fold cross-validation on the training set.\nWe then evaluated the classifier on tokens from the UD Treebank test set. Now the input to the classifier consisted of lemma only. Lemmas which were missing from the model’s vocabulary were omitted (860 of a total of 21759 tokens in the test set). It reached an accuracy of 0.84 (weighted precision 0.85, weighted recall 0.84).\nThese numbers may not seem very impressive in comparison with the performance of modern state-of-the-art PoS taggers. However, one should remember that this classifier knows absolutely nothing about a word’s context in the current sentence. It assigns PoS tags based solely on the proximity of the word’s distributional vector in an unsupervised model to those of prototypical PoS examples. The classifier was in fact based only on knowledge of what words occurred in the BNC near other words within a symmetric window of 2 word to the left and to the right. It did not even have access to the information about exact word order within this sliding window, which makes its performance even more impressive.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nFigure 3. Classifier accuracy depending on the number of used vector components (k)\nIt is also interesting that one needs as few as a thousand example words to train a decent classifier. Thus, it seems that PoS affiliation is expressed quite strongly and robustly in word embeddings. It can be employed, for example, in preliminary tagging of large corpora of resource-poor languages. Only a handful of non-ambiguous words need be manually PoS-tagged, and the rest is done by a distributional model trained on the corpus.\nTo find out how many features are important for the classifier, we used the same training and test set, and ranked all embedding components (features, vector dimensions) by their ANOVA F-value related to PoS class. Then we successively trained the classifier on increasing amounts of top-ranked features (top k best) and measured the training set accuracy.\nThe results are shown in Figure 3. One can see that the accuracy smoothly grows with the number of used features, eventually reaching almost ideal performance on the training set. It is difficult to define the point where the influence of adding features reaches a plateau; it may lie somewhere near k = 100. It means that the knowledge about PoS affiliation is distributed among at least one hundred components of the word embeddings, quite consistent with the underlying idea of embedding models.\nOne might argue that the largest gap in performance is between k = 2 and k = 3 (from 0.38 to 0.51) and thus most PoS-related information is contained in the 3 components with the largest Fvalue (in our case, these 3 features were components 31, 51 and 11). But an accuracy of 0.51 is certainly not an adequate result, so even if im-\nportant, these components are not sufficient to robustly predict part of speech affiliation for a word. Further research is needed to study the effects of adding features to the classifier training.\nRegardless, an interesting finding is that part of speech affiliation is distributed among many components of the word embeddings, not concentrated in one or two specific features. Thus, the strongly expressed component 94 in the average vector of conjunctions (Figures 1 and 2) seems to be a solitary case."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We showed that semantic features derived in the process of training distributional vector models, can be employed both in supporting linguistic hypotheses about part of speech class changes and in detecting and fixing possible annotation errors in corpora. Word embeddings contain rather robust information about the PoS class of the corresponding words. Moreover, this knowledge seems to be distributed among several components (at least a hundred in our case of 300-dimensional model).\nDistributional models trained in a nondeterministic and stochastic way on large amounts of word contexts learn knowledge about part of speech clusters. Arguably, they are good at this precisely because part of speech boundaries are not strict, and even sometimes considered to be a non-categorical linguistic phenomenon (Manning, 2015).\nThe reported experiment form part of ongoing research, and we plan to extend it, particularly conducting similar experiments with other languages typologically different from English. We also plan to continue studying the issue of correspondence between particular embedding components and part of speech affiliation. Another direction of future work is finding out how different hyperparameters for training distributional models (including training corpus pre-processing) influence their performance in PoS discrimination."
    } ],
    "references" : [ {
      "title" : "The handbook of English linguistics",
      "author" : [ "Aarts", "McMahon2008] Bas Aarts", "April McMahon" ],
      "venue" : null,
      "citeRegEx" : "Aarts et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Aarts et al\\.",
      "year" : 2008
    }, {
      "title" : "Edward Finegan",
      "author" : [ "Douglas Biber", "Stig Johansson", "Geoffrey Leech", "Susan Conrad" ],
      "venue" : "and Randolph Quirk.",
      "citeRegEx" : "Biber et al.1999",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Users Reference Guide for British National Corpus (XML Edition)",
      "author" : [ "Lou Burnard" ],
      "venue" : null,
      "citeRegEx" : "Burnard.,? \\Q2007\\E",
      "shortCiteRegEx" : "Burnard.",
      "year" : 2007
    }, {
      "title" : "A synopsis of linguistic theory, 1930-1955",
      "author" : [ "John Firth" ],
      "venue" : null,
      "citeRegEx" : "Firth.,? \\Q1957\\E",
      "shortCiteRegEx" : "Firth.",
      "year" : 1957
    }, {
      "title" : "Continuity and change in English morphology: The variable (ING)",
      "author" : [ "Ann Celeste Houston" ],
      "venue" : "Ph.D. thesis",
      "citeRegEx" : "Houston.,? \\Q1985\\E",
      "shortCiteRegEx" : "Houston.",
      "year" : 1985
    }, {
      "title" : "Hui Jiang",
      "author" : [ "Quan Liu", "Zhen-Hua Ling" ],
      "venue" : "and Yu Hu.",
      "citeRegEx" : "Liu et al.2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Partof-speech tagging from 97% to 100%: is it time for some linguistics",
      "author" : [ "Christopher D Manning" ],
      "venue" : "In Computational Linguistics and Intelligent Text Processing,",
      "citeRegEx" : "Manning.,? \\Q2011\\E",
      "shortCiteRegEx" : "Manning.",
      "year" : 2011
    }, {
      "title" : "Computational linguistics and deep learning",
      "author" : [ "Christopher D Manning" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "Manning.,? \\Q2015\\E",
      "shortCiteRegEx" : "Manning.",
      "year" : 2015
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "Linguistic regularities in continuous space word representations",
      "author" : [ "Wen-tau Yih", "Geoffrey Zweig" ],
      "venue" : "In Proceedings of NAACL-HLT",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2013
    }, {
      "title" : "2012",
      "author" : [ "Slav Petrov", "Dipanjan Das", "Ryan McDonald" ],
      "venue" : "A universal part-of-speech tagset. In LREC",
      "citeRegEx" : "Petrov et al.2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "and Christopher D",
      "author" : [ "Natalia Silveira", "Timothy Dozat", "Marie-Catherine de Marneffe", "Samuel Bowman", "Miriam Connor", "John Bauer" ],
      "venue" : "Manning.",
      "citeRegEx" : "Silveira et al.2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Christopher D Manning",
      "author" : [ "Kristina Toutanova", "Dan Klein" ],
      "venue" : "and Yoram Singer.",
      "citeRegEx" : "Toutanova et al.2003",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Neural networks leverage corpus-wide information for part-of-speech tagging",
      "author" : [ "Yuta Tsuboi" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Tsuboi.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tsuboi.",
      "year" : 2014
    }, {
      "title" : "Guillaume Lample",
      "author" : [ "Yulia Tsvetkov", "Manaal Faruqui", "Wang Ling" ],
      "venue" : "and Chris Dyer.",
      "citeRegEx" : "Tsvetkov et al.2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "This paper studies how word embeddings trained on the British National Corpus interact with part of speech boundaries. We experiment with training classifiers for predicting PoS tags for words based on their embeddings. The results show that the distributional vectors do contain information about PoS affiliation. This approach allowed us to discover word groups with distributional patterns different from other words of the same part of speech. This often reveals hidden inconsistencies of the annotation process or guidelines. At the same time, it supports the notion of ‘soft’ or ‘graded’ part of speech affiliations. Finally, we found out that information about PoS is distributed among dozens of vector components, not limited to only one or two features.",
    "creator" : "LaTeX with hyperref package"
  }
}