{
  "name" : "18.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Broad-Coverage Semantic Parsing: A Transition-Based Approach",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "In order for a grounded semantic representation to cover the full range of semantic structures exhibited by natural language, there are three structural properties that should be supported. The first is multiple parents, representing arguments and relations (semantic units) that are shared between predicates. For instance, in the sentence “After graduation, John moved to Paris”, “John” is an argument of both “graduation” and “moved”, yielding a DAG structure (Figure 1a), rather than a tree.\nThe second is non-terminal nodes for representing units comprising more than one word. While bi-lexical dependencies partially circumvent this requirement, by representing complex units in terms of their headwords, they fall short when representing units that have no clear head.\nFrequent examples of such constructions include coordination structures (e.g., “John and Mary went home”; Figure 1b), some multi-word expressions (e.g., “The Haves and the Have Nots”), and prepositional phrases. In these cases, dependency schemes often apply some convention selecting one of the sub-units as the head, but as different head selections are needed for different purposes, standardization problems arise (Ivanova et al., 2012). For example, selecting the preposition to head prepositional phrases yields better parsing results (Schwartz et al., 2012), while the head noun is more useful for information extraction.\nThird, semantic units may be discontinuous in the text. For instance, in “John gave everything up” (Figure 1c), the phrasal verb “gave ... up” forms a single semantic unit. Discontinuities are also pervasive with other multi-word expressions (Schneider et al., 2014). We call formal representations supporting all three properties Broadcoverage Semantic Structures (BSS).\nHowever, to our knowledge, no existing parser for a grounded semantic annotation scheme supports the combination of these criteria. The only such scheme supporting them is UCCA (Abend and Rappoport, 2013), which has no parser. Several other models either support some of these properties (Oepen et al., 2015), or avoid grounding semantic units altogether (notably, AMR (Banarescu et al., 2013); see Section 2).\nIn this work we are first in proposing techniques for BSS parsing. We adopt a transition-based approach, which has recently produced some of the best results in syntactic dependency parsing (Dyer et al., 2015; Ballesteros et al., 2015), and has also demonstrated strong performance in a variety of other semantic and syntactic settings (Maier, 2015; Wang et al., 2015, among others). Transition-based methods are a natural starting point for UCCA parsing, as the set of distinctions\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n(a) After\nL\ngraduation P\nH\n, U\nJohn\nA\nmoved\nP\nto R\nParis\nC\nA\nH\nA\n(b)\nJohn\nC\nand\nN\nMary\nC\nA\nwent\nP\nhome\nA\n(c) John\nA\ngave\nC\neverything up\nC\nP\nA\nFigure 1: Semantic representation of the three structural properties required for BSS, according to the UCCA scheme. (a) includes a remote edge (dashed), resulting in “John” having two parents. (b) includes a coordination construction (“John and Mary”). (c) includes a discontinuous unit (“gave ... up”). Legend: P – a Scene’s main relation, A – participant, L – inter-scene linker, H – linked Scene, C – center, R – relator, N – connector, U – punctuation, F – function unit. Pre-terminal nodes are omitted for brevity.\nit represents, centered around predicate-argument structures and their inter-relations, is similar to distinctions conveyed by dependency schemes.\nWe pursue two complementary parsing strategies. First, we assess the ability of existing technology to tackle the task, by developing conversion protocols between UCCA structures and two related formalisms: dependency trees and discontinuous constituency trees. As these formalisms are more restrictive than BSS, the conversion is necessarily lossy. Nonetheless, we find that it is effective in practice (Section 3). Second, we present a novel transition-based broadcoverage parser, Broad-coverage Semantic Parser (BSP; Section 4) supporting multiple parents, nonterminal nodes and discontinuous units, based on extending existing transition-based parsers with new transitions and features.\nWe experiment with the English UCCAannotated corpora (Abend and Rappoport, 2013) as a test case, in both in-domain and out-ofdomain scenarios, reaching nearly 70% labeled Fscore for the highest scoring parser. The results suggest concrete paths for further improvement. All converters and parsers will be made publicly available upon publication."
    }, {
      "heading" : "2 Background",
      "text" : "Broad-coverage Semantic Representation. While earlier work on grounded semantic parsing has mostly concentrated on shallow semantic analysis, focusing on semantic role labeling of verbal argument structures, the focus has recently shifted to parsing of more elaborate representations that account for a wider range of phenomena. Most closely related to this work is Broad-coverage Semantic Dependency Parsing (SDP), addressed in two SemEval tasks (Oepen et al., 2014; Oepen et al., 2015), experimenting with the Prague tectogrammatical layer (Sgall et al., 1986; Böhmová et al., 2003), and with dependencies derived from the Enju parser,1 and Lingo ERG (Flickinger, 2002). Like BSS parsing, SDP addresses a wide range of semantic phenomena, and supports discontinuous units and multiple parents. However, SDP uses bi-lexical dependencies, disallowing non-terminal nodes, and thus faces difficulties in supporting structures that have no clear head, such as coordination (Ivanova et al., 2012).\nAnother line of work addresses parsing into non-grounded2 semantic representation, notably Abstract Meaning Representation (AMR) (Flanigan et al., 2014; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015). While sharing much of this work’s motivation, not grounding the representation in the text complicates the parsing task, as it requires that the alignment between words and logical symbols be automatically (and imprecisely) detected.3 Furthermore, grounding allows breaking down sentences into semantically meaningful sub-spans, which is useful for many applications (see discussion in Fernández-González and Martins (2015)). Wang et al. (2015) applied a transition-based approach to AMR parsing. Their method involved first syntactically parsing the input, and then converting the result into AMR. Other approaches for semantic representation, such as MRS (Copestake et al., 2005) and DRT (Kamp et al., 2011), involve considerably different representation and parsing approaches, and so fall beyond the scope of our discussion.\n1See http://kmcs.nii.ac.jp/enju 2By grounded we mean the text tokens are directly annotated as part of the representation, as opposed to abstract formalisms approximating logical form, for example.\n3Considerable technical effort has been invested in the AMR alignment task under various approaches (Flanigan et al., 2014; Pourdamghani et al., 2014; Pust et al., 2015).\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nThe UCCA Annotation Scheme. Universal Cognitive Conceptual Annotation (UCCA) is a cross-linguistically applicable semantic representation scheme, that builds on the established “Basic Linguistic Theory” framework for typological description (Dixon, 2010a; Dixon, 2010b; Dixon, 2012), and on the Cognitive Linguistics literature. UCCA is a multi-layered representation, where each layer corresponds to a “module” of semantic distinctions (e.g., predicate-argument structures, adverbials, coreference etc.).\nFormally, a UCCA structure over a sentence is a DAG, whose leaves correspond to the sentence’s words. The nodes of the graph, or its “units”, are either terminals or several sub-units (not necessarily contiguous) jointly viewed as a single entity according to some semantic or cognitive consideration. Edges bear a category, indicating the role of the sub-unit in the relation that the parent represents. UCCA structures support all three criteria of BSS: multiple parents, non-terminal nodes, and discontinuous units.\nUCCA’s foundational layer, which we use here, covers the predicate-argument structures evoked by predicates of all grammatical categories (verbal, nominal, adjectival and others), the interrelations between them, as well as other major linguistic phenomena such as coordination and multi-word expressions. This set of categories has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation, and semantic stability in translation: UCCA annotations of translated text usually contain the same set of relationships (Sulem et al., 2015). This finding supports the claim that UCCA represents an abstract level of semantics, shared by different languages.\nThe layer’s basic notion is the Scene, which describes a movement, action or state. Each Scene contains one Main Relation, as well as one or more Participants. For example, the sentence “After graduation, John moved to Paris” contains two Scenes, whose main relations are “graduation” and “moved”. “John” is a Participant in both Scenes, while “Paris” only in the latter. UCCA marks one of the incoming edges for each non-root as “primary” and the others as “remote” edges. The two Scenes in this sentence are both arguments of the Linker “After”, which in this case expresses a temporal relation between them. Figure 1 presents the UCCA annotation of this and other examples.\nFurther categories account for relations between Scenes and the internal structures of complex arguments (e.g., coordination) and relations (e.g., complex adverbials, such a “very clearly”). UCCA graphs may contain implicit units that have no correspondent in the text, but the parsing of these units is deferred to future work, as it is likely to require different methods than those explored here (Roth and Frank, 2015)."
    }, {
      "heading" : "3 Conversion-Based Parsing",
      "text" : "We begin by assessing the ability of existing technology to address the task, by taking a conversionbased approach. Training proceeds by converting BSS into a different representation, and training an existing parser on the converted structures. We evaluate the trained parsers by applying them to the test set, and converting the results back to BSS, where they are compared with the gold standard. The error resulting from this back and forth conversion is discussed in Section 6. Notation. Let L be the set of possible edge labels. A BSS is a directed acyclic graph G = (V,E, `) over a sequence of tokens w1, . . . , wn, where ` : E → L is a function of edge labels. For each token wi (i = 1, . . . , n), there exists a leaf (or a terminal) ti ∈ V . Conversion to Constituency Trees. We convert BSS to constituency trees by removing a subset of the edges.4 Specifically, when converting UCCA structures, we simply remove all remote edges, leaving only primary edges, which form a tree structure (see Figure 1a). The inverse conversion from trees to BSS is simply the identity function, as every constituency tree is a BSS. Conversion to Dependency Trees. In the conversion to dependency trees, we first convert BSS to constituency trees using the above procedure, and then convert the result to dependency trees. Assume Tc = (Vc, Ec, `c) is a constituency tree with labels `c : Ec → L, where L is the set of possible labels. The conversion from Tc to a dependency tree involves the removal of all nonterminals from Tc and the addition of edges between terminals. The nodes of the converted dependency tree are simply the terminals of Tc.\nWe define a linear order over possible edge labels L. For each node u ∈ V , denote with\n4For trees, labeling nodes is equivalent to labeling edges. Thus, we do not distinguish between the two options. Note also that as the original structures may contain discontinuities, so may the resulting trees.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nData: constituency tree Tc = (Vc, Ec, `c) Result: dependency tree Td = (Vd, Ed, `d) foreach u ∈ Vc do\nh(u)← argminv Priority(`c(u, v)); end Vd ← Terminals(Tc), Ed ← ∅; foreach t ∈ Vd do\nu← t; while u = h(Parentc(u)) do\nh∗(u)← t; u← Parentc(u);\nend n(t)← h(u); end foreach t ∈ Vd do\nu← Parentc(n(t)); t′ ← h∗(u); Ed ← Ed ∪ {(t′, t)}; `d(t\n′, t)← `c(u, n(t))}; end\nAlgorithm 1: Constituency to dependency conversion pro-\ncedure.\nh(u) its child with the highest edge label. Denote with h∗(u) the terminal reached by recursively applying h(·) over u. For each terminal t, we define n(t) as the highest non-terminal such that t = h∗(n(t)), i.e., n(t) is the only node such that t = h∗(n(t)) and t 6= h∗(Parentc(n(t))). The head of t according to the dependency graph is the terminal h∗(Parentc(n(t))). The complete conversion procedure from constituency to dependency is given in Algorithm 1.\nWe note that this conversion procedure is simpler than the head percolation procedure used for converting syntactic constituency trees to dependency trees. This is because h(u) of a node u (similar to u’s head-containing child), depends only on the label of the edge (h(u), u), and not on the subtree spanned by u, because edge labels in UCCA directly express the role of the child in the parent unit, and are thus sufficient for determining which of u’s children contains the head node.\nThe inverse conversion introduces non-terminal nodes back into the tree. As the distinction between low- and high-attaching nodes is lost in the constituency to dependency conversion, we heuristically assume that attachments are always high-attaching. Assume Td = (Vd, Ed, `d) is a dependency tree. We begin by creating a root node r. Then, iterating over Vd in topological order, we add its members as terminals to the constituency tree and create a pre-terminal parent for each, with an edge labeled as Terminal between them. The parents of the pre-terminals are determined by the terminal’s parent in the dependency tree: if a de-\nData: dependency tree Td = (Vd, Ed, `d) Result: constituency tree Tc = (Vc, Ec, `c) r ← Node(); Vc ← {r}, Ec ← ∅; foreach t ∈ TopologicalSort(Vd) do\nu← Node(); Vc ← Vc ∪ {u, t}; Ec ← Ec ∪ {(u, t)}; `c(u, t)← Terminal ; t′ ← Parentd(t); if t′ = ROOT then\nEc ← Ec ∪ {(r, u)}; `c(r, u)← Label(r);\nelse if ∃v ∈ Vd : (t, v) ∈ Ed then\nu′ ← Node(); Ec ← Ec ∪ {(u′, u)}; `c(u\n′, u)← Label(u′); else\nu′ ← u; end p← Parentc(t′); Ec ← Ec ∪ {(p, u′)}; `c(p, u\n′)← `d(t′, t); end\nend Algorithm 2: Dependency to constituency conversion pro-\ncedure.\npendency node t is a child of the root in Td, then t’s pre-terminal will also be a child of the root node. Otherwise, t’s pre-terminal is the child of the preterminal associated with t’s head in Td. We add an intermediate node in between if t has any dependents in Td, to allow adding their pre-terminals as children. Edge labels for the intermediate edges are determined by a rule-based function, denoted by Label(u).5 In practice, it mostly selects the UCCA label Center. This conversion procedure is given in Algorithm 2."
    }, {
      "heading" : "4 Broad-Coverage Semantic Parsing",
      "text" : "We now turn to presenting BSP, a transition-based parser that supports the three criteria of broadcoverage semantic structures.\nTransition-based parsing (Nivre, 2003) creates the parse as it scans the text from left to right. The parse is created incrementally by applying a transition at each step to the parser state, defined using three data structures: a buffer B of tokens and nodes to be processed, a stack S of nodes currently being processed, and a graph G = (V,E, `) of constructed nodes and labeled edges. Some of the states are marked as terminal, meaning that G is the final output. A classifier is used at each step\n5See Supplementary Material for the definition of Label.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nto select the next transition based on features that encode the parser’s current state. During training, an oracle creates training instances for the classifier, based on the gold-standard annotation.\nDespite being based on local decisions, transition-based methods have yielded excellent results in a variety of parsing tasks. Within syntactic dependency parsing, transition-based methods have been successfully applied to corpora in many languages and domains, yielding some of the best reported results (Dyer et al., 2015; Ballesteros et al., 2015). The approach has also yielded results comparable with the state-of-the-art in constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013), discontinuous constituency parsing (Maier, 2015), as well as dependency DAG structures (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015), CCG structures (Ambati et al., 2015) and AMR parsing (Wang et al., 2015).\nBSP mostly builds on recent advances in discontinuous constituency and dependency DAG parsing techniques, and further introduces novel UCCA-oriented features for parsing BSS.\nTransition Set. Given a sequence of tokens w1, . . . , wn, we predict a BSS G whose leaves correspond to the tokens. Parsing starts with a single node on the stack (the root node), and the input tokens w1, . . . , wn in the buffer. The set of transitions is given in Figure 2. In addition to the standard SHIFT and REDUCE operations, we follow previous work in transition-based constituency parsing (Sagae and Lavie, 2005), and include the NODE transition for creating new nonterminal nodes. Concretely, NODEX creates a new node on the buffer as a parent of the first element on the stack, with an X-labeled edge.\nLEFT-EDGEX and RIGHT-EDGEX create a new primary X-labeled edge between the first two elements on the stack, where the parent is the left or the right node, respectively. As a UCCA node may only have one incoming primary edge, EDGE transitions are disallowed where the child node already has an incoming primary edge. LEFTREMOTEX and RIGHT-REMOTEX do not have this restriction, and the created edge is marked as remote. We distinguish between these two pairs of transitions, for the parser to be able to determine whether an edge is a primary or a remote one. In order to support the prediction of multiple parents, node and edge transitions do not automatically ap-\nply REDUCE. This is in line with other work on transition-based DAG dependency parsing (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015). Once all edges for a particular node have been created, it is removed from the stack by applying REDUCE.\nSWAP allows handling discontinuous nodes, by popping the second node on the stack and adding it to the top of the buffer, as with the similarly named transition in previous work (Nivre, 2009; Maier, 2015). Finally, FINISH pops the root node and marks the state as terminal.\nFeatures. Figure 3 presents the feature templates used by the parser. For some of the features, we used the notion of head word, defined by the h∗(·) function (Section 3). While head words are not explicitly represented in the UCCA scheme, these features proved useful as means of encoding word-to-word relations.\nIn addition to the binary features defined by the feature templates, we employ a real-valued feature, ratio, corresponding to the ratio between the number of terminals to number of nodes in the graph G. This novel feature serves as a regularizer for the creation of new nodes, and should be beneficial for other transition-based constituency parsers too. Features are generally adapted from the related parsers of (Zhang and Clark, 2009; Zhu et al., 2013; Tokgöz and Eryiğit, 2015; Maier, 2015), with a small additional set of features encoding relevant information for the novel LEFTREMOTEX and RIGHT-REMOTEX transitions.\nTraining. Following Maier (2015), we use a linear classifier, using the averaged structured perceptron algorithm for training it (Collins and Roark, 2004) with the MINUPDATE (Goldberg and Elhadad, 2011) procedure: a minimum number of updates to a feature has to occur in training for it to be included in the model. Inference is performed greedily (i.e., without beam search).\nFor training the local classifier, we use a dynamic oracle (Goldberg and Nivre, 2012), i.e., an oracle that outputs a set of optimal transitions: when applied to the current parser state, the gold standard graph is reachable from the resulting state. For example, the oracle would predict a NODE transition if the stack has on its top a parent in the gold graph that has not been created, but would predict a RIGHT-EDGE transition if the second stack element is a parent of the first element according to the gold graph and the edge between them has not been created. The transition\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nInitial State Final State Stack Buffer Nodes Edges Terminal? Stack Buffer Nodes Edges Terminal? [root] w1:n {root} ∪\nw1:n\n∅ − ∅ ∅ V E +\nBefore Transition Transition After Transition Condition Stack Buffer Nodes Edges Stack Buffer Nodes Edges Terminal? S x | B V E SHIFT S | x B V E − S | x B V E REDUCE S B V E − S | x B V E NODEX S | x y | B V ∪ {y} E ∪ {(y, x)X} − x 6= root S | y, x B V E LEFT-EDGEX S | y, x B V E ∪ {(x, y)X} − \nx 6∈ w1:n, y 6= root, y 6;G x\nS | x, y B V E RIGHT-EDGEX S | x, y B V E ∪ {(x, y)X} − S | y, x B V E LEFT-REMOTEX S | y, x B V E ∪ {(x, y)∗X} − S | x, y B V E RIGHT-REMOTEX S | x, y B V E ∪ {(x, y)∗X} − S | x, y B V E SWAP S | y x | B V E − i(x) < i(y) [root] ∅ V E FINISH ∅ ∅ V E +\nFigure 2: The transition set of BSP. We write the stack with its top to the right and the buffer with its head to the left. (·, ·)X denotes a primary X-labeled edge, and (·, ·)∗X a remote X-labeled edge. i(x) is a running index for the created nodes. EDGE transitions have an additional condition: the prospective child may not already have a primary parent.\npredicted by the classifier is deemed correct and is applied to the parser state to reach the subsequent state, if the transition is included in the set of optimal transitions. Otherwise, a random optimal transition is applied and the classifier’s weights are updated according to the perceptron update rule."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : "Data. We conduct our main experiments on the UCCA Wikipedia corpus (henceforth, Wiki), and use the English part of the UCCA Twenty Thousand Leagues Under the Sea English-French parallel corpus (henceforth, 20K Leagues) as out-ofdomain data.6 Table 1 presents some statistics for the two corpora, demonstrating that while the Wiki corpus is over ten times larger, the overall statistics are similar. We use passages of indices up to 655 of the Wiki corpus as our training set, passages 656–700 as development set, and passages 701– 695 as in-domain test set. While UCCA edges can cross sentence boundaries, we adhere to the common practice in semantic parsing and train our parsers on individual sentences, discarding interrelations between them (0.18% of the edges). We also discard linkage nodes and edges, as they often express inter-sentence relations and are thus mostly redundant when applied at the sentence level, as well as implicit nodes (Section 2). In the out-of-domain experiments, we apply the same parser (trained on the Wiki corpus) to the 20K Leagues corpus without re-tuning the parameters. Evaluation. Since there are no standard evaluation measures for BSS, we define two simple\n6Both are available at http://www.cs.huji.ac. il/˜oabend/ucca.html\nmeasures for comparing such structures. Assume Gp = (Vp, Ep, `p) and Gg = (Vg, Eg, `g) are the predicted and gold-standard DAGs over the same sequence of terminals W = {w1, . . . , wn}, respectively. For an edge e = (u, v) in either graph, where u is the parent and v is the child, define its yield y(e) ⊆ W as the set of terminals in W that are descendants of v. We define the set of mutual edges between Gp and Gg:\nM(Gp, Gg) =\n{(e1, e2) ∈ Ep × Eg | y(e1) = y(e2) ∧ `p(e1) = `g(e2)}\nLabeled precision and recall are defined by dividing |M(Gp, Gg)| by |Ep| and |Eg|, respectively. We report two variants of this measure, one where we consider only non-remote edges, and another where we consider remote edges. We note that the measure collapses to the standard PARSEVAL constituency evaluation measure if Gp are\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nFeatures from (Zhang and Clark, 2009):\nunigrams s0te, s0we, s1te, s1we, s2te, s2we, s3te, s3we,\nb0wt, b1wt, b2wt, b3wt,\ns0lwe, s0rwe, s0uwe, s1lwe, s1rwe, s1uwe bigrams s0ws1w, s0ws1e, s0es1w, s0es1e, s0wb0w, s0wb0t,\ns0eb0w, s0eb0t, s1wb0w, s1wb0t, s1eb0w, s1eb0t,\nb0wb1w, b0wb1t, b0tb1w, b0tb1t trigrams s0es1es2w, s0es1es2e, s0es1eb0w, s0es1eb0t,\ns0es1wb0w, s0es1wb0t, s0ws1es2e, s0ws1eb0t separator s0wp, s0wep, s0wq, s0wcq, s0es1ep, s0es1eq,\ns1wp, s1wep, s1wq, s1weq\nextended (Zhu et al., 2013) s0llwe, s0lrwe, s0luwe, s0rlwe, s0rrwe,\ns0ruwe, s0ulwe, s0urwe, s0uuwe, s1llwe,\ns1lrwe, s1luwe, s1rlwe, s1rrwe, s1ruwe\ndisco (Maier, 2015) s0xwe, s1xwe, s2xwe, s3xwe,\ns0xte, s1xte, s2xte, s3xte,\ns0xy, s1xy, s2xy, s3xy\ns0xs1e, s0xs1w, s0xs1x, s0ws1x, s0es1x,\ns0xs2e, s0xs2w, s0xs2x, s0ws2x, s0es2x,\ns0ys1y, s0ys2y, s0xb0t, s0xb0w\nFeatures from (Tokgöz and Eryiğit, 2015):\ncounts s0P, s0C, s0wP, s0wC, b0P, b0C, b0wP, b0wC edges s0s1, s1s0, s0b0, b0s0, s0b0e, b0s0e history a0we, a1we\nremote (Novel, UCCA-specific features) s0R, s0wR, b0R, b0wR\nFigure 3: Feature templates for BSP. Notation: si, bi are the ith stack and buffer items, respectively. w and t are the word form and part-of-speech tag of the terminal returned by the h∗(·) function (Section 3). e is the edge label to the node returned by the h(·) function. l, r (ll, rr) are the leftmost and rightmost (grand)children, respectively. u (uu) is the unary (grand)child, when only one exists. p is a unique separator punctuation and q is the separator count between s0 and s1. x is the gap type (“none”, “pass” or “gap”) at the sub-graph under the current node, and y is the sum of gap lengths (Maier and Lichte, 2011). P and C are the number of parents and children, respectively, and R is the number of remote children. ai is the transition taken i steps back. All feature templates correspond to binary features.\nGg are trees. Punctuation marks are excluded from the evaluation, but not from the datasets. Conversions. We explore two conversion scenarios: one into (possibly discontinuous) constituency trees, and one into CoNLL-style dependencies. In the first setting we experiment with UPARSE, the only transition-based constituency parser, to our knowledge, able to parse trees with discontinuous constituents. In the second setting we use the MaltParser with arc-standard and arceager transition sets (Nivre et al., 2007),7 and the stack LSTM-based arc-standard parser (Dyer et al., 2015). For MaltParser, we try both SVM and perceptron classifiers, reporting results obtained with SVM (about 1% F-score higher). Default settings are used in all cases. We do not use existing dependency DAG parsers since we could not obtain their code. We note that UPARSE uses beam search by default, with a beam size of 4, where the other parsers use greedy search.\nUpper bounds for the conversion-based methods are computed by applying the conversion and inverse conversion on the gold standard graphs and comparing them to the original gold standard. BSP. We train BSP for 16 iterations, using MINUPDATE = 5 and IMPORTANCE = 2, doubling weight updates for gold SWAP transitions to address the sparseness of discontinuous structures, as in Maier (2015). We train BSP both with and without remote edge transitions, to allow better comparison with conversion-based methods that only predict trees."
    }, {
      "heading" : "6 Results",
      "text" : "Table 2 presents the results of our main experiments, as well as upper bounds for the conversionbased methods. BSP obtains comparable F-scores to MaltParser and UPARSE in terms of primary edges, but unlike them, is able to predict some of the remote edges too. Removing remote edge transitions from BSP does not change results considerably on the primary edges, improving them by 0.9% F-score in the in-domain setting, but reduces them by the same amount when applied out-ofdomain. Out-of-domain results are largely comparable with the in-domain results, demonstrating robustness by BSP to domain variation.\nThe LSTM parser obtains the highest primary\n7Preliminary experiments with non-projective variants of MaltParser yielded lower scores than projective ones, and were thus discarded from the final evaluation.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nF-score, with a considerable margin. Importantly, it obtains 9.1% F-score higher than the arcstandard MaltParser, differing from it only in its classifier. This suggests that applying a similar approach to BSP is likely to improve results, and further underscores the effectiveness of transitionbased methods for BSS parsing.\nThe conversion to constituency format only removes remote edges, and thus obtains a perfect primary edge score. The conversion to dependency format loses considerably more information, since all non-terminal nodes are lost and have to be reconstructed by a simple rule-based inverse conversion. Both conversions yield zero scores on remote edges, since these are invariably removed when converting to trees.\nPrimary Remote LP LR LF LP LR LF\nConstituency Tree Conversion\nUPARSE 64 67.3 65.4 − 0 0 Upper Bound 100 100 100 − 0 0\nDependency Tree Conversion\nMaltarc-standard 63.4 57.3 60.1 − 0 0 Maltarc-eager 63.9 57.9 60.5 − 0 0 LSTM 73.2 66.2 69.2 − 0 0 Upper Bound 93.8 83.7 88.4 − 0 0\nDirect Approach BSP 62.4 56 59 15.3 11.8 13.3 BSPTree 63.8 56.5 59.9 − 0 0\nOut-of-domain BSP 60.6 53.9 57.1 20.2 10.3 13.6 BSPTree 60.2 52.8 56.2 − 0 0\nTable 2: Main experimental results in percents (on the Wiki test set, except for the bottom part). Columns correspond to labeled precision, recall and F-score for the different parsers, for both primary (left-hand side) and remote (right-hand side) edges. Top: results for UPARSE after conversion to constituency tree annotation. Upper middle: results for the MaltParser arc-eager and arc-standard, and the LSTM parser, after conversion to dependency tree annotation. Lower middle: results for our BSP, when trained on the complete UCCA DAGs (BSP), and when trained on UCCA trees, obtained by removing remote edges (BSPTree). Bottom: results for BSP and BSPTree when tested on out-of-domain data (20K Leagues).\nFeature Ablation. To evaluate the relative impact of the different feature sets on BSP, we remove a set of features at a time, and evaluate the resulting parser on the development set (Table 3). Almost all feature sets have a positive contribution to the primary edge F-score, or otherwise to the prediction of remote edges. unigrams and bigrams features are especially important, and the ratio feature greatly improves recall on primary edges. disco features have a positive contribution,\nlikely to be amplified in languages with a higher percentage of discontinuous units, e.g. German."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We have introduced the first parser that supports multiple parents, non-terminal nodes and discontinuous units. We further explored a conversionbased parsing approach to assess the ability of existing technology to address the task. The work makes a further contribution by first experimenting on UCCA parsing. Results show that UCCA can be parsed with 69.2% primary F-score, and suggest means for improvement by taking an LSTM-based approach to the local classifier of BSP. The quality of the results is underscored by UCCA’s inter-annotator agreement (often taken as an upper bound) of 80–85% F-score on primary edges (Abend and Rappoport, 2013).\nWhile much recent work focused on semantic parsing of different types, the relations between the different representations have not been clarified. We intend to further explore conversionbased parsing approaches, including different target representations and more sophisticated conversion procedures (Kong et al., 2015), to shed light on the commonalities and differences between representations, suggesting ways to design better semantic representations. We believe that UCCA’s merits in providing a cross-linguistically applicable, broad-coverage annotation will support ongoing efforts to incorporate deeper semantic structures into a variety of applications, such as machine translation (Jones et al., 2012) and summarization (Liu et al., 2015).\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899"
    } ],
    "references" : [ {
      "title" : "Universal Conceptual Cognitive Annotation (UCCA)",
      "author" : [ "Omri Abend", "Ari Rappoport." ],
      "venue" : "Proc. of ACL, pages 228–238.",
      "citeRegEx" : "Abend and Rappoport.,? 2013",
      "shortCiteRegEx" : "Abend and Rappoport.",
      "year" : 2013
    }, {
      "title" : "An incremental algorithm for transition-based CCG parsing",
      "author" : [ "Bharat Ram Ambati", "Tejaswini Deoskar", "Mark Johnson", "Mark Steedman." ],
      "venue" : "Proc. of NAACL, pages 53–63.",
      "citeRegEx" : "Ambati et al\\.,? 2015",
      "shortCiteRegEx" : "Ambati et al\\.",
      "year" : 2015
    }, {
      "title" : "Broad-coverage CCG semantic parsing with AMR",
      "author" : [ "Yoav Artzi", "Kenton Lee", "Luke Zettlemoyer." ],
      "venue" : "Proc. of EMNLP, pages 1699–1710.",
      "citeRegEx" : "Artzi et al\\.,? 2015",
      "shortCiteRegEx" : "Artzi et al\\.",
      "year" : 2015
    }, {
      "title" : "Improved transition-based parsing by modeling characters instead of words with LSTMs",
      "author" : [ "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proc. of EMNLP, pages 349–359.",
      "citeRegEx" : "Ballesteros et al\\.,? 2015",
      "shortCiteRegEx" : "Ballesteros et al\\.",
      "year" : 2015
    }, {
      "title" : "Abstract Meaning Representation for sembanking",
      "author" : [ "Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Martha Palmer", "Nathan Schneider." ],
      "venue" : "Proc. of the Linguistic Annotation",
      "citeRegEx" : "Banarescu et al\\.,? 2013",
      "shortCiteRegEx" : "Banarescu et al\\.",
      "year" : 2013
    }, {
      "title" : "The Prague dependency treebank",
      "author" : [ "Alena Böhmová", "Jan Hajič", "Eva Hajičová", "Barbora Hladká." ],
      "venue" : "Treebanks, pages 103–127. Springer.",
      "citeRegEx" : "Böhmová et al\\.,? 2003",
      "shortCiteRegEx" : "Böhmová et al\\.",
      "year" : 2003
    }, {
      "title" : "Incremental parsing with the perceptron algorithm",
      "author" : [ "Michael Collins", "Brian Roark." ],
      "venue" : "Proc. of ACL, pages 111–118.",
      "citeRegEx" : "Collins and Roark.,? 2004",
      "shortCiteRegEx" : "Collins and Roark.",
      "year" : 2004
    }, {
      "title" : "Minimal recursion semantics: An introduction",
      "author" : [ "Ann Copestake", "Dan Flickinger", "Carl Pollard", "Ivan A Sag." ],
      "venue" : "Research on Language and Computation, 3(2-3):281–332.",
      "citeRegEx" : "Copestake et al\\.,? 2005",
      "shortCiteRegEx" : "Copestake et al\\.",
      "year" : 2005
    }, {
      "title" : "Basic Linguistic Theory: Grammatical Topics, volume 2",
      "author" : [ "Robert M.W. Dixon." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Dixon.,? 2010a",
      "shortCiteRegEx" : "Dixon.",
      "year" : 2010
    }, {
      "title" : "Basic Linguistic Theory: Methodology, volume 1",
      "author" : [ "Robert M.W. Dixon." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Dixon.,? 2010b",
      "shortCiteRegEx" : "Dixon.",
      "year" : 2010
    }, {
      "title" : "Basic Linguistic Theory: Further Grammatical Topics, volume 3",
      "author" : [ "Robert M.W. Dixon." ],
      "venue" : "Oxford University Press.",
      "citeRegEx" : "Dixon.,? 2012",
      "shortCiteRegEx" : "Dixon.",
      "year" : 2012
    }, {
      "title" : "Transitionbased dependeny parsing with stack long short-term memory",
      "author" : [ "Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith." ],
      "venue" : "Proc. of ACL, pages 334–343.",
      "citeRegEx" : "Dyer et al\\.,? 2015",
      "shortCiteRegEx" : "Dyer et al\\.",
      "year" : 2015
    }, {
      "title" : "Parsing as reduction",
      "author" : [ "Daniel Fernández-González", "André FT Martins." ],
      "venue" : "Proc. of ACL, pages 1523–1533.",
      "citeRegEx" : "Fernández.González and Martins.,? 2015",
      "shortCiteRegEx" : "Fernández.González and Martins.",
      "year" : 2015
    }, {
      "title" : "A discriminative graph-based parser for the Abstract Meaning Representation",
      "author" : [ "Jeffrey Flanigan", "Sam Thomson", "Jaime Carbonell", "Chris Dyer", "Noah A. Smith." ],
      "venue" : "Proc. of ACL, pages 1426–1436.",
      "citeRegEx" : "Flanigan et al\\.,? 2014",
      "shortCiteRegEx" : "Flanigan et al\\.",
      "year" : 2014
    }, {
      "title" : "On building a more efficient grammar by exploiting types",
      "author" : [ "Daniel Flickinger." ],
      "venue" : "Jun’ichi Tsujii, Stefan Oepen, Daniel Flickinger, and Hans Uszkoreit, editors, Collaborative Language Engineering. CLSI, Stanford, CA.",
      "citeRegEx" : "Flickinger.,? 2002",
      "shortCiteRegEx" : "Flickinger.",
      "year" : 2002
    }, {
      "title" : "Learning sparser perceptron models",
      "author" : [ "Yoav Goldberg", "Michael Elhadad." ],
      "venue" : "Technical report. Available online at http://www.cs.bgu.ac. il/ ̃yoavg/publications. Yoav Goldberg and Joakim Nivre. 2012. A dynamic",
      "citeRegEx" : "Goldberg and Elhadad.,? 2011",
      "shortCiteRegEx" : "Goldberg and Elhadad.",
      "year" : 2011
    }, {
      "title" : "Who did what to whom? A contrastive study of syntacto-semantic dependencies",
      "author" : [ "Angelina Ivanova", "Stephan Oepen", "Lilja Øvrelid", "Dan Flickinger." ],
      "venue" : "Proc. of LAW, pages 2–11.",
      "citeRegEx" : "Ivanova et al\\.,? 2012",
      "shortCiteRegEx" : "Ivanova et al\\.",
      "year" : 2012
    }, {
      "title" : "Semantics-based machine translation with hyperedge replacement grammars",
      "author" : [ "Bevan Jones", "Jacob Andreas", "Daniel Bauer", "Karl Moritz Hermann", "Kevin Knight." ],
      "venue" : "Proc. of COLING, pages 1359–1376.",
      "citeRegEx" : "Jones et al\\.,? 2012",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 2012
    }, {
      "title" : "Discourse representation theory",
      "author" : [ "Hans Kamp", "Josef Van Genabith", "Uwe Reyle." ],
      "venue" : "Handbook of philosophical logic, pages 125–394. Springer.",
      "citeRegEx" : "Kamp et al\\.,? 2011",
      "shortCiteRegEx" : "Kamp et al\\.",
      "year" : 2011
    }, {
      "title" : "Transforming dependencies into phrase structures",
      "author" : [ "Lingpeng Kong", "Alexander M. Rush", "Noah A. Smith." ],
      "venue" : "Proc. of NAACL HLT.",
      "citeRegEx" : "Kong et al\\.,? 2015",
      "shortCiteRegEx" : "Kong et al\\.",
      "year" : 2015
    }, {
      "title" : "Toward abstractive summarization using semantic representations",
      "author" : [ "Fei Liu", "Jeffrey Flanigan", "Sam Thomson", "Norman Sadeh", "Noah A. Smith." ],
      "venue" : "Proc. of NAACL, pages 1077–1086.",
      "citeRegEx" : "Liu et al\\.,? 2015",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Characterizing discontinuity in constituent treebanks",
      "author" : [ "Wolfgang Maier", "Timm Lichte." ],
      "venue" : "Formal Grammar. 14th International Conference, FG 2009. Bordeaux, France, July 25-26, 2009. Revised Selected Papers, number 5591 in Lecture Notes",
      "citeRegEx" : "Maier and Lichte.,? 2011",
      "shortCiteRegEx" : "Maier and Lichte.",
      "year" : 2011
    }, {
      "title" : "Discontinuous incremental shift-reduce parsing",
      "author" : [ "Wolfgang Maier." ],
      "venue" : "Proc. of ACL, pages 1202– 1212.",
      "citeRegEx" : "Maier.,? 2015",
      "shortCiteRegEx" : "Maier.",
      "year" : 2015
    }, {
      "title" : "MaltParser: A language-independent system for data-driven dependency parsing",
      "author" : [ "Joakim Nivre", "Johan Hall", "Jens Nilsson", "Atanas Chanev", "Gülsen Eryigit", "Sandra Kübler", "Svetoslav Marinov", "Erwin Marsi." ],
      "venue" : "Natural Language Engineering,",
      "citeRegEx" : "Nivre et al\\.,? 2007",
      "shortCiteRegEx" : "Nivre et al\\.",
      "year" : 2007
    }, {
      "title" : "An efficient algorithm for projective dependency parsing",
      "author" : [ "Joakim Nivre." ],
      "venue" : "Proc. of IWPT, pages 149–160.",
      "citeRegEx" : "Nivre.,? 2003",
      "shortCiteRegEx" : "Nivre.",
      "year" : 2003
    }, {
      "title" : "Non-projective dependency parsing in expected linear time",
      "author" : [ "Joakim Nivre." ],
      "venue" : "Proc. of ACL, pages 351–359.",
      "citeRegEx" : "Nivre.,? 2009",
      "shortCiteRegEx" : "Nivre.",
      "year" : 2009
    }, {
      "title" : "SemEval 2014 task 8: Broad-coverage semantic dependency parsing",
      "author" : [ "Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Dan Flickinger", "Jan Hajič", "Angelina Ivanova", "Yi Zhang." ],
      "venue" : "Proc. of SemEval, pages 63–72.",
      "citeRegEx" : "Oepen et al\\.,? 2014",
      "shortCiteRegEx" : "Oepen et al\\.",
      "year" : 2014
    }, {
      "title" : "SemEval 2015 task 18: Broad-coverage semantic dependency parsing",
      "author" : [ "Stephan Oepen", "Marco Kuhlmann", "Yusuke Miyao", "Daniel Zeman", "Silvie Cinková", "Dan Flickinger", "Jan Hajič", "Zdeňka Urešová." ],
      "venue" : "Proc. of SemEval, pages 915–926.",
      "citeRegEx" : "Oepen et al\\.,? 2015",
      "shortCiteRegEx" : "Oepen et al\\.",
      "year" : 2015
    }, {
      "title" : "Aligning english strings with abstract meaning representation graphs",
      "author" : [ "Nima Pourdamghani", "Yang Gao", "Ulf Hermjakob", "Kevin Knight." ],
      "venue" : "Proc. of EMNLP, pages 425–429.",
      "citeRegEx" : "Pourdamghani et al\\.,? 2014",
      "shortCiteRegEx" : "Pourdamghani et al\\.",
      "year" : 2014
    }, {
      "title" : "Parsing English into Abstract Meaning Representation using syntaxbased machine translation",
      "author" : [ "Michael Pust", "Ulf Hermjakob", "Kevin Knight", "Daniel Marcu", "Jonathan May." ],
      "venue" : "Proc. of EMNLP, pages 1143–1154.",
      "citeRegEx" : "Pust et al\\.,? 2015",
      "shortCiteRegEx" : "Pust et al\\.",
      "year" : 2015
    }, {
      "title" : "Inducing Implicit Arguments from Comparable Texts: A Framework and its Applications",
      "author" : [ "Michael Roth", "Anette Frank." ],
      "venue" : "Computational Linguistics, 41:625–664.",
      "citeRegEx" : "Roth and Frank.,? 2015",
      "shortCiteRegEx" : "Roth and Frank.",
      "year" : 2015
    }, {
      "title" : "A classifier-based parser with linear run-time complexity",
      "author" : [ "Kenji Sagae", "Alon Lavie." ],
      "venue" : "Proc. of IWPT, pages 125–132.",
      "citeRegEx" : "Sagae and Lavie.,? 2005",
      "shortCiteRegEx" : "Sagae and Lavie.",
      "year" : 2005
    }, {
      "title" : "Shift-reduce dependency DAG parsing",
      "author" : [ "Kenji Sagae", "Jun’ichi Tsujii" ],
      "venue" : "In Proc. of COLING,",
      "citeRegEx" : "Sagae and Tsujii.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sagae and Tsujii.",
      "year" : 2008
    }, {
      "title" : "Discriminative lexical semantic segmentation with gaps: running the MWE gamut",
      "author" : [ "Nathan Schneider", "Emily Danchik", "Chris Dyer", "Noah A Smith." ],
      "venue" : "Transactions of the Association for Computational Linguistics, 2:193–206.",
      "citeRegEx" : "Schneider et al\\.,? 2014",
      "shortCiteRegEx" : "Schneider et al\\.",
      "year" : 2014
    }, {
      "title" : "Learnability-based syntactic annotation design",
      "author" : [ "Roy Schwartz", "Omri Abend", "Ari Rappoport." ],
      "venue" : "Proc. of COLING, pages 2405–2422.",
      "citeRegEx" : "Schwartz et al\\.,? 2012",
      "shortCiteRegEx" : "Schwartz et al\\.",
      "year" : 2012
    }, {
      "title" : "The Meaning of the Sentence and Its Semantic and Pragmatic Aspects",
      "author" : [ "Petr Sgall", "Eva Hajičová", "Jarmila Panevová." ],
      "venue" : "Academia/Reidel Publishing Company, Prague, Czech Republic/Dordrecht, Netherlands.",
      "citeRegEx" : "Sgall et al\\.,? 1986",
      "shortCiteRegEx" : "Sgall et al\\.",
      "year" : 1986
    }, {
      "title" : "Conceptual annotations preserve structure across translations: A French-English case study",
      "author" : [ "Elior Sulem", "Omri Abend", "Ari Rappoport." ],
      "venue" : "Proc. of S2MT, pages 11–22.",
      "citeRegEx" : "Sulem et al\\.,? 2015",
      "shortCiteRegEx" : "Sulem et al\\.",
      "year" : 2015
    }, {
      "title" : "Transitionbased dependency DAG parsing using dynamic oracles",
      "author" : [ "Alper Tokgöz", "Gülsen Eryiğit." ],
      "venue" : "Proc. of ACL Student Research Workshop, pages 22–27.",
      "citeRegEx" : "Tokgöz and Eryiğit.,? 2015",
      "shortCiteRegEx" : "Tokgöz and Eryiğit.",
      "year" : 2015
    }, {
      "title" : "An AMR parser for English, French, German, Spanish and Japanese and a new AMR-annotated corpus",
      "author" : [ "Lucy Vanderwende", "Arul Menezes", "Chris Quirk." ],
      "venue" : "Proc. of NAACL, pages 26–30.",
      "citeRegEx" : "Vanderwende et al\\.,? 2015",
      "shortCiteRegEx" : "Vanderwende et al\\.",
      "year" : 2015
    }, {
      "title" : "A transition-based algorithm for AMR parsing",
      "author" : [ "Chuan Wang", "Nianwen Xue", "Sameer Pradhan", "Sameer Pradhan." ],
      "venue" : "Proc. of NAACL, pages 366–375.",
      "citeRegEx" : "Wang et al\\.,? 2015",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Transition-based parsing of the Chinese treebank using a global discriminative model",
      "author" : [ "Yue Zhang", "Stephen Clark." ],
      "venue" : "Proc. of IWPT, pages 162– 171.",
      "citeRegEx" : "Zhang and Clark.,? 2009",
      "shortCiteRegEx" : "Zhang and Clark.",
      "year" : 2009
    }, {
      "title" : "Fast and accurate shiftreduce constituent parsing",
      "author" : [ "Muhua Zhu", "Yue Zhang", "Wenliang Chen", "Min Zhang", "Jingbo Zhu." ],
      "venue" : "Proc. of ACL, pages 434–443.",
      "citeRegEx" : "Zhu et al\\.,? 2013",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "In these cases, dependency schemes often apply some convention selecting one of the sub-units as the head, but as different head selections are needed for different purposes, standardization problems arise (Ivanova et al., 2012).",
      "startOffset" : 206,
      "endOffset" : 228
    }, {
      "referenceID" : 34,
      "context" : "For example, selecting the preposition to head prepositional phrases yields better parsing results (Schwartz et al., 2012), while the head noun is more useful for information extraction.",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 33,
      "context" : "Discontinuities are also pervasive with other multi-word expressions (Schneider et al., 2014).",
      "startOffset" : 69,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "The only such scheme supporting them is UCCA (Abend and Rappoport, 2013), which has no parser.",
      "startOffset" : 45,
      "endOffset" : 72
    }, {
      "referenceID" : 27,
      "context" : "Several other models either support some of these properties (Oepen et al., 2015), or avoid grounding semantic units altogether (notably, AMR (Banarescu et al.",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : ", 2015), or avoid grounding semantic units altogether (notably, AMR (Banarescu et al., 2013); see Section 2).",
      "startOffset" : 68,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : "We adopt a transition-based approach, which has recently produced some of the best results in syntactic dependency parsing (Dyer et al., 2015; Ballesteros et al., 2015), and has also demonstrated strong performance in a variety of other semantic and syntactic settings (Maier, 2015; Wang et al.",
      "startOffset" : 123,
      "endOffset" : 168
    }, {
      "referenceID" : 3,
      "context" : "We adopt a transition-based approach, which has recently produced some of the best results in syntactic dependency parsing (Dyer et al., 2015; Ballesteros et al., 2015), and has also demonstrated strong performance in a variety of other semantic and syntactic settings (Maier, 2015; Wang et al.",
      "startOffset" : 123,
      "endOffset" : 168
    }, {
      "referenceID" : 0,
      "context" : "We experiment with the English UCCAannotated corpora (Abend and Rappoport, 2013) as a test case, in both in-domain and out-ofdomain scenarios, reaching nearly 70% labeled Fscore for the highest scoring parser.",
      "startOffset" : 53,
      "endOffset" : 80
    }, {
      "referenceID" : 26,
      "context" : "Most closely related to this work is Broad-coverage Semantic Dependency Parsing (SDP), addressed in two SemEval tasks (Oepen et al., 2014; Oepen et al., 2015), experimenting with the Prague tectogrammatical layer (Sgall et al.",
      "startOffset" : 118,
      "endOffset" : 158
    }, {
      "referenceID" : 27,
      "context" : "Most closely related to this work is Broad-coverage Semantic Dependency Parsing (SDP), addressed in two SemEval tasks (Oepen et al., 2014; Oepen et al., 2015), experimenting with the Prague tectogrammatical layer (Sgall et al.",
      "startOffset" : 118,
      "endOffset" : 158
    }, {
      "referenceID" : 35,
      "context" : ", 2015), experimenting with the Prague tectogrammatical layer (Sgall et al., 1986; Böhmová et al., 2003), and with dependencies derived from the Enju parser,1 and Lingo ERG (Flickinger, 2002).",
      "startOffset" : 62,
      "endOffset" : 104
    }, {
      "referenceID" : 5,
      "context" : ", 2015), experimenting with the Prague tectogrammatical layer (Sgall et al., 1986; Böhmová et al., 2003), and with dependencies derived from the Enju parser,1 and Lingo ERG (Flickinger, 2002).",
      "startOffset" : 62,
      "endOffset" : 104
    }, {
      "referenceID" : 14,
      "context" : ", 2003), and with dependencies derived from the Enju parser,1 and Lingo ERG (Flickinger, 2002).",
      "startOffset" : 76,
      "endOffset" : 94
    }, {
      "referenceID" : 16,
      "context" : "However, SDP uses bi-lexical dependencies, disallowing non-terminal nodes, and thus faces difficulties in supporting structures that have no clear head, such as coordination (Ivanova et al., 2012).",
      "startOffset" : 174,
      "endOffset" : 196
    }, {
      "referenceID" : 13,
      "context" : "Another line of work addresses parsing into non-grounded2 semantic representation, notably Abstract Meaning Representation (AMR) (Flanigan et al., 2014; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).",
      "startOffset" : 129,
      "endOffset" : 217
    }, {
      "referenceID" : 38,
      "context" : "Another line of work addresses parsing into non-grounded2 semantic representation, notably Abstract Meaning Representation (AMR) (Flanigan et al., 2014; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).",
      "startOffset" : 129,
      "endOffset" : 217
    }, {
      "referenceID" : 29,
      "context" : "Another line of work addresses parsing into non-grounded2 semantic representation, notably Abstract Meaning Representation (AMR) (Flanigan et al., 2014; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).",
      "startOffset" : 129,
      "endOffset" : 217
    }, {
      "referenceID" : 2,
      "context" : "Another line of work addresses parsing into non-grounded2 semantic representation, notably Abstract Meaning Representation (AMR) (Flanigan et al., 2014; Vanderwende et al., 2015; Pust et al., 2015; Artzi et al., 2015).",
      "startOffset" : 129,
      "endOffset" : 217
    }, {
      "referenceID" : 7,
      "context" : "Other approaches for semantic representation, such as MRS (Copestake et al., 2005) and DRT (Kamp et al.",
      "startOffset" : 58,
      "endOffset" : 82
    }, {
      "referenceID" : 18,
      "context" : ", 2005) and DRT (Kamp et al., 2011), involve considerably different representation and parsing approaches, and so fall beyond the scope of our discussion.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : ", 2015; Artzi et al., 2015). While sharing much of this work’s motivation, not grounding the representation in the text complicates the parsing task, as it requires that the alignment between words and logical symbols be automatically (and imprecisely) detected.3 Furthermore, grounding allows breaking down sentences into semantically meaningful sub-spans, which is useful for many applications (see discussion in Fernández-González and Martins (2015)).",
      "startOffset" : 8,
      "endOffset" : 453
    }, {
      "referenceID" : 2,
      "context" : ", 2015; Artzi et al., 2015). While sharing much of this work’s motivation, not grounding the representation in the text complicates the parsing task, as it requires that the alignment between words and logical symbols be automatically (and imprecisely) detected.3 Furthermore, grounding allows breaking down sentences into semantically meaningful sub-spans, which is useful for many applications (see discussion in Fernández-González and Martins (2015)). Wang et al. (2015) applied a transition-based approach to AMR parsing.",
      "startOffset" : 8,
      "endOffset" : 474
    }, {
      "referenceID" : 13,
      "context" : "Considerable technical effort has been invested in the AMR alignment task under various approaches (Flanigan et al., 2014; Pourdamghani et al., 2014; Pust et al., 2015).",
      "startOffset" : 99,
      "endOffset" : 168
    }, {
      "referenceID" : 28,
      "context" : "Considerable technical effort has been invested in the AMR alignment task under various approaches (Flanigan et al., 2014; Pourdamghani et al., 2014; Pust et al., 2015).",
      "startOffset" : 99,
      "endOffset" : 168
    }, {
      "referenceID" : 29,
      "context" : "Considerable technical effort has been invested in the AMR alignment task under various approaches (Flanigan et al., 2014; Pourdamghani et al., 2014; Pust et al., 2015).",
      "startOffset" : 99,
      "endOffset" : 168
    }, {
      "referenceID" : 8,
      "context" : "Universal Cognitive Conceptual Annotation (UCCA) is a cross-linguistically applicable semantic representation scheme, that builds on the established “Basic Linguistic Theory” framework for typological description (Dixon, 2010a; Dixon, 2010b; Dixon, 2012), and on the Cognitive Linguistics literature.",
      "startOffset" : 213,
      "endOffset" : 254
    }, {
      "referenceID" : 9,
      "context" : "Universal Cognitive Conceptual Annotation (UCCA) is a cross-linguistically applicable semantic representation scheme, that builds on the established “Basic Linguistic Theory” framework for typological description (Dixon, 2010a; Dixon, 2010b; Dixon, 2012), and on the Cognitive Linguistics literature.",
      "startOffset" : 213,
      "endOffset" : 254
    }, {
      "referenceID" : 10,
      "context" : "Universal Cognitive Conceptual Annotation (UCCA) is a cross-linguistically applicable semantic representation scheme, that builds on the established “Basic Linguistic Theory” framework for typological description (Dixon, 2010a; Dixon, 2010b; Dixon, 2012), and on the Cognitive Linguistics literature.",
      "startOffset" : 213,
      "endOffset" : 254
    }, {
      "referenceID" : 36,
      "context" : "This set of categories has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation, and semantic stability in translation: UCCA annotations of translated text usually contain the same set of relationships (Sulem et al., 2015).",
      "startOffset" : 273,
      "endOffset" : 293
    }, {
      "referenceID" : 30,
      "context" : "UCCA graphs may contain implicit units that have no correspondent in the text, but the parsing of these units is deferred to future work, as it is likely to require different methods than those explored here (Roth and Frank, 2015).",
      "startOffset" : 208,
      "endOffset" : 230
    }, {
      "referenceID" : 24,
      "context" : "Transition-based parsing (Nivre, 2003) creates the parse as it scans the text from left to right.",
      "startOffset" : 25,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : "Within syntactic dependency parsing, transition-based methods have been successfully applied to corpora in many languages and domains, yielding some of the best reported results (Dyer et al., 2015; Ballesteros et al., 2015).",
      "startOffset" : 178,
      "endOffset" : 223
    }, {
      "referenceID" : 3,
      "context" : "Within syntactic dependency parsing, transition-based methods have been successfully applied to corpora in many languages and domains, yielding some of the best reported results (Dyer et al., 2015; Ballesteros et al., 2015).",
      "startOffset" : 178,
      "endOffset" : 223
    }, {
      "referenceID" : 31,
      "context" : "The approach has also yielded results comparable with the state-of-the-art in constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013), discontinuous constituency parsing (Maier, 2015), as well as dependency DAG structures (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015), CCG structures (Ambati et al.",
      "startOffset" : 99,
      "endOffset" : 163
    }, {
      "referenceID" : 40,
      "context" : "The approach has also yielded results comparable with the state-of-the-art in constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013), discontinuous constituency parsing (Maier, 2015), as well as dependency DAG structures (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015), CCG structures (Ambati et al.",
      "startOffset" : 99,
      "endOffset" : 163
    }, {
      "referenceID" : 41,
      "context" : "The approach has also yielded results comparable with the state-of-the-art in constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013), discontinuous constituency parsing (Maier, 2015), as well as dependency DAG structures (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015), CCG structures (Ambati et al.",
      "startOffset" : 99,
      "endOffset" : 163
    }, {
      "referenceID" : 22,
      "context" : ", 2013), discontinuous constituency parsing (Maier, 2015), as well as dependency DAG structures (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015), CCG structures (Ambati et al.",
      "startOffset" : 44,
      "endOffset" : 57
    }, {
      "referenceID" : 32,
      "context" : ", 2013), discontinuous constituency parsing (Maier, 2015), as well as dependency DAG structures (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015), CCG structures (Ambati et al.",
      "startOffset" : 96,
      "endOffset" : 146
    }, {
      "referenceID" : 37,
      "context" : ", 2013), discontinuous constituency parsing (Maier, 2015), as well as dependency DAG structures (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015), CCG structures (Ambati et al.",
      "startOffset" : 96,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : ", 2013), discontinuous constituency parsing (Maier, 2015), as well as dependency DAG structures (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015), CCG structures (Ambati et al., 2015) and AMR parsing (Wang et al.",
      "startOffset" : 163,
      "endOffset" : 184
    }, {
      "referenceID" : 39,
      "context" : ", 2015) and AMR parsing (Wang et al., 2015).",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 31,
      "context" : "In addition to the standard SHIFT and REDUCE operations, we follow previous work in transition-based constituency parsing (Sagae and Lavie, 2005), and include the NODE transition for creating new nonterminal nodes.",
      "startOffset" : 122,
      "endOffset" : 145
    }, {
      "referenceID" : 32,
      "context" : "This is in line with other work on transition-based DAG dependency parsing (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015).",
      "startOffset" : 75,
      "endOffset" : 125
    }, {
      "referenceID" : 37,
      "context" : "This is in line with other work on transition-based DAG dependency parsing (Sagae and Tsujii, 2008; Tokgöz and Eryiğit, 2015).",
      "startOffset" : 75,
      "endOffset" : 125
    }, {
      "referenceID" : 25,
      "context" : "SWAP allows handling discontinuous nodes, by popping the second node on the stack and adding it to the top of the buffer, as with the similarly named transition in previous work (Nivre, 2009; Maier, 2015).",
      "startOffset" : 178,
      "endOffset" : 204
    }, {
      "referenceID" : 22,
      "context" : "SWAP allows handling discontinuous nodes, by popping the second node on the stack and adding it to the top of the buffer, as with the similarly named transition in previous work (Nivre, 2009; Maier, 2015).",
      "startOffset" : 178,
      "endOffset" : 204
    }, {
      "referenceID" : 40,
      "context" : "Features are generally adapted from the related parsers of (Zhang and Clark, 2009; Zhu et al., 2013; Tokgöz and Eryiğit, 2015; Maier, 2015), with a small additional set of features encoding relevant information for the novel LEFTREMOTEX and RIGHT-REMOTEX transitions.",
      "startOffset" : 59,
      "endOffset" : 139
    }, {
      "referenceID" : 41,
      "context" : "Features are generally adapted from the related parsers of (Zhang and Clark, 2009; Zhu et al., 2013; Tokgöz and Eryiğit, 2015; Maier, 2015), with a small additional set of features encoding relevant information for the novel LEFTREMOTEX and RIGHT-REMOTEX transitions.",
      "startOffset" : 59,
      "endOffset" : 139
    }, {
      "referenceID" : 37,
      "context" : "Features are generally adapted from the related parsers of (Zhang and Clark, 2009; Zhu et al., 2013; Tokgöz and Eryiğit, 2015; Maier, 2015), with a small additional set of features encoding relevant information for the novel LEFTREMOTEX and RIGHT-REMOTEX transitions.",
      "startOffset" : 59,
      "endOffset" : 139
    }, {
      "referenceID" : 22,
      "context" : "Features are generally adapted from the related parsers of (Zhang and Clark, 2009; Zhu et al., 2013; Tokgöz and Eryiğit, 2015; Maier, 2015), with a small additional set of features encoding relevant information for the novel LEFTREMOTEX and RIGHT-REMOTEX transitions.",
      "startOffset" : 59,
      "endOffset" : 139
    }, {
      "referenceID" : 6,
      "context" : "Following Maier (2015), we use a linear classifier, using the averaged structured perceptron algorithm for training it (Collins and Roark, 2004) with the MINUPDATE (Goldberg and Elhadad, 2011) procedure: a minimum number of updates to a feature has to occur in training for it to be included in the model.",
      "startOffset" : 119,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : "Following Maier (2015), we use a linear classifier, using the averaged structured perceptron algorithm for training it (Collins and Roark, 2004) with the MINUPDATE (Goldberg and Elhadad, 2011) procedure: a minimum number of updates to a feature has to occur in training for it to be included in the model.",
      "startOffset" : 164,
      "endOffset" : 192
    }, {
      "referenceID" : 20,
      "context" : "SWAP allows handling discontinuous nodes, by popping the second node on the stack and adding it to the top of the buffer, as with the similarly named transition in previous work (Nivre, 2009; Maier, 2015). Finally, FINISH pops the root node and marks the state as terminal. Features. Figure 3 presents the feature templates used by the parser. For some of the features, we used the notion of head word, defined by the h∗(·) function (Section 3). While head words are not explicitly represented in the UCCA scheme, these features proved useful as means of encoding word-to-word relations. In addition to the binary features defined by the feature templates, we employ a real-valued feature, ratio, corresponding to the ratio between the number of terminals to number of nodes in the graph G. This novel feature serves as a regularizer for the creation of new nodes, and should be beneficial for other transition-based constituency parsers too. Features are generally adapted from the related parsers of (Zhang and Clark, 2009; Zhu et al., 2013; Tokgöz and Eryiğit, 2015; Maier, 2015), with a small additional set of features encoding relevant information for the novel LEFTREMOTEX and RIGHT-REMOTEX transitions. Training. Following Maier (2015), we use a linear classifier, using the averaged structured perceptron algorithm for training it (Collins and Roark, 2004) with the MINUPDATE (Goldberg and Elhadad, 2011) procedure: a minimum number of updates to a feature has to occur in training for it to be included in the model.",
      "startOffset" : 192,
      "endOffset" : 1244
    }, {
      "referenceID" : 40,
      "context" : "Features from (Zhang and Clark, 2009): unigrams s0te, s0we, s1te, s1we, s2te, s2we, s3te, s3we, b0wt, b1wt, b2wt, b3wt, s0lwe, s0rwe, s0uwe, s1lwe, s1rwe, s1uwe bigrams s0ws1w, s0ws1e, s0es1w, s0es1e, s0wb0w, s0wb0t, s0eb0w, s0eb0t, s1wb0w, s1wb0t, s1eb0w, s1eb0t, b0wb1w, b0wb1t, b0tb1w, b0tb1t trigrams s0es1es2w, s0es1es2e, s0es1eb0w, s0es1eb0t, s0es1wb0w, s0es1wb0t, s0ws1es2e, s0ws1eb0t separator s0wp, s0wep, s0wq, s0wcq, s0es1ep, s0es1eq, s1wp, s1wep, s1wq, s1weq",
      "startOffset" : 14,
      "endOffset" : 37
    }, {
      "referenceID" : 41,
      "context" : "extended (Zhu et al., 2013) s0llwe, s0lrwe, s0luwe, s0rlwe, s0rrwe, s0ruwe, s0ulwe, s0urwe, s0uuwe, s1llwe, s1lrwe, s1luwe, s1rlwe, s1rrwe, s1ruwe",
      "startOffset" : 9,
      "endOffset" : 27
    }, {
      "referenceID" : 22,
      "context" : "disco (Maier, 2015) s0xwe, s1xwe, s2xwe, s3xwe, s0xte, s1xte, s2xte, s3xte, s0xy, s1xy, s2xy, s3xy s0xs1e, s0xs1w, s0xs1x, s0ws1x, s0es1x, s0xs2e, s0xs2w, s0xs2x, s0ws2x, s0es2x, s0ys1y, s0ys2y, s0xb0t, s0xb0w",
      "startOffset" : 6,
      "endOffset" : 19
    }, {
      "referenceID" : 37,
      "context" : "Features from (Tokgöz and Eryiğit, 2015): counts s0P, s0C, s0wP, s0wC, b0P, b0C, b0wP, b0wC edges s0s1, s1s0, s0b0, b0s0, s0b0e, b0s0e history a0we, a1we",
      "startOffset" : 14,
      "endOffset" : 40
    }, {
      "referenceID" : 21,
      "context" : "x is the gap type (“none”, “pass” or “gap”) at the sub-graph under the current node, and y is the sum of gap lengths (Maier and Lichte, 2011).",
      "startOffset" : 117,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "In the second setting we use the MaltParser with arc-standard and arceager transition sets (Nivre et al., 2007),7 and the stack LSTM-based arc-standard parser (Dyer et al.",
      "startOffset" : 91,
      "endOffset" : 111
    }, {
      "referenceID" : 11,
      "context" : ", 2007),7 and the stack LSTM-based arc-standard parser (Dyer et al., 2015).",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 11,
      "context" : ", 2007),7 and the stack LSTM-based arc-standard parser (Dyer et al., 2015). For MaltParser, we try both SVM and perceptron classifiers, reporting results obtained with SVM (about 1% F-score higher). Default settings are used in all cases. We do not use existing dependency DAG parsers since we could not obtain their code. We note that UPARSE uses beam search by default, with a beam size of 4, where the other parsers use greedy search. Upper bounds for the conversion-based methods are computed by applying the conversion and inverse conversion on the gold standard graphs and comparing them to the original gold standard. BSP. We train BSP for 16 iterations, using MINUPDATE = 5 and IMPORTANCE = 2, doubling weight updates for gold SWAP transitions to address the sparseness of discontinuous structures, as in Maier (2015). We train BSP both with and without remote edge transitions, to allow better comparison with conversion-based methods that only predict trees.",
      "startOffset" : 56,
      "endOffset" : 826
    }, {
      "referenceID" : 0,
      "context" : "The quality of the results is underscored by UCCA’s inter-annotator agreement (often taken as an upper bound) of 80–85% F-score on primary edges (Abend and Rappoport, 2013).",
      "startOffset" : 145,
      "endOffset" : 172
    }, {
      "referenceID" : 19,
      "context" : "We intend to further explore conversionbased parsing approaches, including different target representations and more sophisticated conversion procedures (Kong et al., 2015), to shed light on the commonalities and differences between representations, suggesting ways to design better semantic representations.",
      "startOffset" : 153,
      "endOffset" : 172
    }, {
      "referenceID" : 17,
      "context" : "We believe that UCCA’s merits in providing a cross-linguistically applicable, broad-coverage annotation will support ongoing efforts to incorporate deeper semantic structures into a variety of applications, such as machine translation (Jones et al., 2012) and summarization (Liu et al.",
      "startOffset" : 235,
      "endOffset" : 255
    }, {
      "referenceID" : 20,
      "context" : ", 2012) and summarization (Liu et al., 2015).",
      "startOffset" : 26,
      "endOffset" : 44
    } ],
    "year" : 2016,
    "abstractText" : "The representation of many common semantic phenomena requires structural properties beyond those commonly used for syntactic parsing. We discuss a set of structural properties required for broad-coverage semantic representation, and note that existing parsers support some of these properties, but not all. We propose two transition-based techniques for parsing such semantic structures: (1) applying conversion procedures to map them into related formalisms, and using existing state-of-the-art parsers on the converted representations; and (2) constructing a parser that directly supports the full set of properties. We experiment with UCCA-annotated corpora, the only ones with all these structural semantic properties. Results demonstrate the effectiveness of transition-based methods for the task.",
    "creator" : "TeX"
  }
}