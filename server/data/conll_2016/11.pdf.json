{
  "name" : "11.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Coreference in Wikipedia: Main Concept Resolution",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 000\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\n046\n047\n048\n049\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n092\n093\n094\n095\n096\n097\n098\n099"
    }, {
      "heading" : "1 Introduction",
      "text" : "Coreference Resolution (CR) is the task of identifying all mentions of entities in a document and grouping them into equivalence classes. CR is a prerequisite for many NLP tasks. For example, in Open Information Extraction (OIE) (Yates et al., 2007), one acquires subject-predicate-object relations, many of which (e.g., <the foundation stone, was laid by, the Queen s daughter>) are useless because the subject or the object contains material coreferring to other mentions in the text being mined.\nMost CR systems, including state-of-the-art ones (Durrett and Klein, 2014; Martschat and Strube, 2015; Clark and Manning, 2015) are essentially adapted to news-like texts. This is basically imputable to the availability of large datasets where this text genre is dominant. This includes\nresources developed within the Message Understanding Conferences (e.g., (Hirshman and Chinchor, 1998)) or the Automatic Content Extraction (ACE) program (e.g., (Doddington et al., 2004)), as well as resources developed within the collaborative annotation project OntoNotes (Pradhan et al., 2007).\nIt is now widely accepted that coreference resolution systems trained on newswire data performs poorly when tested on other text genres (Hendrickx and Hoste, 2009; Schäfer et al., 2012), including Wikipedia texts, as we shall see in our experiments.\nWikipedia is a large, multilingual, highly structured, multi-domain encyclopedia, providing an increasingly large wealth of knowledge. It is known to contain well-formed, grammatical and meaningful sentences, compared to say, ordinary internet documents. It is therefore a resource of choice in many NLP systems, see (Medelyan et al., 2009) for a review of some pioneering works.\nWhile being a ubiquitous resource in the NLP community, we are not aware of much work conducted to adapt CR to this text genre. Two notable exceptions are (Nguyen et al., 2007) and (Nakayama, 2008), two studies dedicated to extract tuples from Wikipedia articles. Both studies demonstrate that the design of a dedicated rulebased CR system leads to improved extraction accuracy. The focus of those studies being information extraction, the authors did not spend much efforts in designing a fully-fledged CR designed for Wikipedia, neither did they evaluate it on a coreference resolution task.\nOur main contribution in this work is to revisit the task initially discussed in (Nakayama, 2008) which consists in identifying in a Wikipedia article all the mentions of the concept being described by this article. We refer to this concept as the “main concept” (MC) henceforth. For instance, within\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nthe article Chilly Gonzales, the task is to find all proper (e.g. Gonzales, Beck), nominal (e.g. the performer) and pronominal (e.g. he) mentions that refer to the MC “Chilly Gonzales”.\nFor us, revisiting this task means that we propose a testbed for evaluating systems designed for it, and we compare a number of state-of-the-art systems on this testbed. More specifically, we frame this task as a binary classification problem, where one has to decide whether a detected mention refers to the MC. Our classifier exploits carefully designed features extracted from Wikipedia markup and characteristics, as well as from Freebase; many of which we borrowed from the related literature.\nWe show that our approach outperforms stateof-the-art generic coreference resolution engines on this task. We further demonstrate that the integration of our classifier into the state-of-the-art rule-based coreference system of Lee et al. (2013) improves the detection of coreference chains in Wikipedia articles.\nThe paper is organized as follows. We discuss related works in Section 2. We describe in Section 3 the baselines we built on top of two state-ofthe-art coreference resolution systems, and present our approach in Section 4. We describe the dataset we exploited in Section 5. We explain experiments we conducted on a Wikipedia dataset in section 6, and conclude in Section 7."
    }, {
      "heading" : "2 Related Works",
      "text" : "Our approach is inspired by, and extends, previous works on coreference resolution which show that incorporating external knowledge into a CR system is beneficial. In particular, a variety of approaches (Ponzetto and Strube, 2006; Ng, 2007; Haghighi and Klein, 2009) have been shown to benefit from using external resources such as Wikipedia, WordNet (Miller, 1995), or YAGO (Suchanek et al., 2007). Ratinov and Roth (2012) and Hajishirzi et al. (2013) both investigate the integration of named-entity linking into machine learning and rule-based coreference resolution system respectively. They both use GLOW (Ratinov et al., 2011) a wikification system which associates detected mentions with their equivalent entity in Wikipedia. In addition, they assign to each mention a set of highly accurate knowledge attributes extracted from Wikipedia and Freebase (Bollacker et al., 2008), such as the\nWikipedia categories, gender, nationality, aliases, and NER type (ORG, PER, LOC, FAC, MISC).\nOne issue with all the aforementioned studies is that inaccuracies often cause cascading errors in the pipeline (Zheng et al., 2013). Consequently, most authors concentrate on high-precision linking at the cost of low recall.\nDealing specifically with Wikipedia articles, we can directly exploit the wealth of markup available (redirects, internal links, links to Freebase) without resorting to named-entity linking, thus benefiting from much less ambiguous information on mentions."
    }, {
      "heading" : "3 Baselines",
      "text" : "Since there is no system readily available for our task, we devised four baselines on top of two available coreference resolution systems. Given the output of a CR system applied on a Wikipedia article, our goal here is to isolate the coreference chain that represents the main concept. We experimented with several heuristics, yielding the following baselines.\nB1 picks the longest coreference chain identified and considers that its mentions are those that co-refer to the main concept. The underlying assumption is that the most mentioned concept in a Wikipedia article is the main concept itself.\nB2 picks the longest coreference chain identified if it contains a mention that exactly matches the MC title, otherwise it checks in decreasing order (longest to shortest) for a chain containing the title. We expect this baseline to be more precise than the previous one overall.\nIt turns out that, for both CR systems, mentions of the MC often are spread over several coreference chains. Therefore we devised two more baselines that aggregate chains, with an expected increase in recall.\nB3 conservatively aggregates chains containing a mention that exactly matches the MC title.\nB4 more loosely aggregates all chains that contain at least one mention whose span is a substring of the title.1 For instance, given the main concept Barack Obama, we concatenate all chains containing either Obama or Barack\n1Grammatical words are not considered for matching.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nin their mentions. Obviously, this baseline should show a higher recall than the previous ones, but risks aggregating mentions that are not related to the MC. For instance, it will aggregate the coreference chain referring to University of Sydney concept with a chain containing the mention Sydney.\nWe observed that, for pronominal mentions, those baselines were not performing very well in terms of recall. With the aim of increasing recall, we added to the chain all the occurrences of pronouns found to refer to the MC (at least once) by the baseline. This heuristic was first proposed by Nguyen et al. (2007). For instance, if the pronoun he is found in the chain identified by the baseline, all pronouns he in the article are considered to be mentions of the MC Barack Obama. Obviously, there are cases where those pronouns do not corefer to the MC, but this step significantly improves the performance on pronouns."
    }, {
      "heading" : "4 Approach",
      "text" : "Our approach is composed of a preprocessor which computes a representation of each mention in an article as well as its main concept; and a feature extractor which compares both representations for inducing a set of features."
    }, {
      "heading" : "4.1 Preprocessing",
      "text" : "We extract mentions using the same mention detection algorithm embedded in Dcoref and Scoref. This algorithm described in (Raghunathan et al., 2010) extracts all named-entities, noun phrases and pronouns, and then removes spurious mentions.\nWe leverage the hyperlink structure of the article in order to enrich the list of mentions with shallow semantic attributes. For each link found within the article under consideration, we look through the candidate list for all mentions that match the surface string of the link. We assign to those mentions the attributes (entity type, gender and number) extracted from the Freebase entry (if it exists) corresponding to the Wikipedia article the hyperlink points to. This module behaves as a substitute to the named-entity linking pipelines used in other works, such as (Ratinov and Roth, 2012; Hajishirzi et al., 2013). We expect it to be of high quality because it exploits human-made links.\nWe use the WikipediaMiner (Milne and Witten, 2008) API for easily accessing any piece\nof structure (clean text, labels, internal links, redirects, etc) in Wikipedia, and Jena2 to index and query Freebase.\nIn the end, we represent a mention by three strings (actual mention span, head word, and span up to the head noun), as well as its coarse attributes (entity type, gender and number). Figure 1 shows the representation collected for the mention San Fernando Valley region of the city of Los Angeles found in the Los Angeles Pierce College article.\nWe represent the main concept of a Wikipedia article by its title, its inferred type (a common noun inferred from the first sentence of the article). Those attributes were used by Nguyen et al. (2007) to heuristically link a mention to the main concept of an article. We further extend this representation by the MC name variants extracted\n2http://jena.apache.org\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nfrom the markup of Wikipedia (redirects, text anchored in links) as well as aliases from Freebase; the MC entity types we extracted from the Freebase notable types attribute, and its coarse attributes extracted from Freebase, such as its NER type, its gender and number. If the concept category is a person (PER), we import the profession attribute. Figure 2 illustrates the information we collect for the Wikipedia concept Los Angeles Pierce College."
    }, {
      "heading" : "4.2 Feature Extraction",
      "text" : "We experimented with a few hundred features for characterizing each mention, focusing on the most promising ones that we found simple enough to compute. In part, our features are inspired by coreference systems that use Wikipedia and Freebase as feature sources (see Section 2). These features, along with others related to the characteristics of Wikipedia texts, allow us to recognize mentions of the MC more accurately than current CR systems. We make a distinction between features computed for pronominal mentions and features computed from the other mentions."
    }, {
      "heading" : "4.2.1 Non-pronominal Mentions",
      "text" : "For each mention, we compute seven families of features we sketch below.\nbase Number of occurrences of the mention span and the mention head found in the list of candidate mentions. We also add a normalized version of those counts (frequency / total number of mentions).\ntitle, inferred type, name variants, entity type Most often, a concept is referred to by its name, one of its variants, or its type which are encoded in the four first fields of our MC representation. We define four families of comparison features, each corresponding to one of the first four fields of a MC representation (see Figure 2). For instance, for the title family, we compare the title text span with each of the text spans of the mention representation (see Figure 1). A comparison between a field of the MC representation and a mention text span yields 10 boolean features. These features encode string similarities (exact match, partial match, one being the substring of another, sharing of a number of words, etc.). An eleventh feature is the semantic relatedness score of Wu and\nPalmer (1994). For title, we therefore end up with 3 sets of 11 feature vectors.\ntag Part-of-speech tags of the first and last words of the mention, as well as the tag of the words immediately before and after the mention in the article. We convert this into 34×4 binary features (presence/absence of a specific combination of tags).\nmain Boolean features encoding whether the MC and the mention coarse attributes matches; also we use conjunctions of all pairs of features in this family."
    }, {
      "heading" : "4.2.2 Pronominal Mentions",
      "text" : "We characterize pronominal mentions by five families of features, which, with the exception of the first one, all capture information extracted from Wikipedia.\nbase The pronoun span itself, number, gender and person attributes, to which we add the number of occurrences of the pronoun, as well as its normalized count. The most frequently occurring pronoun in an article is likely to co-refer to the main concept, and we expect these features to capture this to some extent.\nmain MC coarse attributes, such as NER type, gender, number (see Figure 2).\ntag Part-of-speech of the previous and following tokens, as well as the previous and the next POS bigrams (this is converted into 2380 binary features).\nposition Often, pronouns at the beginning of a new section or paragraph refer to the main concept. Therefore, we compute 5 (binary) features encoding the relative position (first, first tier, second tier, last tier, last) of a mention in the sentence, paragraph, section and article.\ndistance Within a sentence, we search before and after the mention for an entity that is compatible (according to Freebase information) with the pronominal mention of interest. If a match is found, one feature encodes the distance between the match and the mention; another feature encodes the number of other compatible pronouns in the same sentence. We expect that this family of features will\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nhelp the model to capture the presence of local (within a sentence) co-references."
    }, {
      "heading" : "5 Dataset",
      "text" : "As our approach is dedicated to Wikipedia articles, we used a dedicated resource we call WCR3, and whose details will be described in the nonanonymized version of this paper. It consists of 30 documents, comprising 60k tokens annotated with the OntoNotes project guidelines (Pradhan et al., 2007). Each mention is annotated with three attributes: the mention type (named-entity, noun phrase, or pronominal), the coreference type (identity, attributive or copular) and the equivalent Freebase entity if it exists. The resource contains roughly 7000 non singleton mentions, among which 1800 refer to the main concept, which is to say that 30 chains out of 1469 make up for 25% of the mentions annotated.\nSince most coreference resolution systems for English are trained and tested on ACE (Doddington et al., 2004) or OntoNotes (Hovy et al., 2006) resources, it is interesting to measure how state-ofthe art systems perform on the WCR dataset. To this end, we ran a number of recent CR systems: the rule-based system of (Lee et al., 2013) we call it Dcoref; the Berkeley systems described in (Durrett and Klein, 2013; Durrett and Klein, 2014); the latent model of Martschat and Strube (2015) we call it Cort in Table 1; and the system described in (Clark and Manning, 2015) we call it Scoref which achieved the best results to date on the CoNLL 2012 test set.\nSystem WCR OntoNotes Dcoref 51.77 55.59 Durrett and Klein (2013) 51.01 61.41 Durrett and Klein (2014) 49.52 61.79 Cort 49.94 62.47 Scoref 46.39 63.61\nTable 1: CoNLL F1 score of recent state of the art systems on the WCR dataset, and the 2012 OntoNotes test data for predicted mentions.\nWe evaluate the systems on the whole dataset, using the v8.01 of the CoNLL scorer4 (Pradhan et al., 2014). The results are reported in Table 1 along with the performance of the systems on the\n3http://www.anonymized.org 4http://conll.github.io/\nreference-coreference-scorers\nCoNLL 2012 test data (Pradhan et al., 2012). Expectedly, the performance of all systems dramatically decrease on WCR, which calls for further research on adapting the coreference resolution technology to new text genres. What is more surprising is that the rule-based system of (Lee et al., 2013) works better than the machine-learning based systems on the WCR dataset. Also, the ranking of the statistical systems on this dataset differs from the one obtained on the OntoNotes test set.\nThe WCR dataset is far smaller than the OntoNotes one; still, we paid attention to sample Wikipedia articles of various characteristics: size, topic (people, organizations, locations, events, etc.) and internal link density. Therefore, we believe our results to be representative. Those results further confirm the conclusions in (Hendrickx and Hoste, 2009), which show that a CR system trained on news-paper significantly underperforms on data coming from users comments and blogs. Nevertheless, statistical systems can be trained or adapted to the WCR dataset, a point we leave for future investigations.\nWe generated baselines for all the systems discussed in this section, but found results derived from statistical approaches to be close enough that we only include results of two systems in the sequel: Dcoref (Lee et al., 2013) and Scoref (Clark and Manning, 2015). We choose these two because they use the same pipeline (parser, mention detection, etc), while applying very different techniques (rules versus machine learning)."
    }, {
      "heading" : "6 Experiments",
      "text" : "In this section, we first describe the data preparation we conducted (section 6.1), and provide details on the classifier we trained (section 6.2). Then, we report experiments we carried out on the task of identifying the mentions co-referent (positive class) to the main concept of an article (section 6.3). We compare our approach to the baselines described in section 3, and analyze the impact of the families of features described in section 4. We also investigate a simple extension of Dcoref which takes advantage of our classifier for improving coreference resolution (section 6.4)."
    }, {
      "heading" : "6.1 Data Preparation",
      "text" : "Each article in WCR was part-of-speech tagged, syntactically parsed and the named-entities were\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n592\n593\n594\n595\n596\n597\n598\n599\nidentified. This was done thanks to the Stanford CoreNLP toolkit (Manning et al., 2014). Since WCR does not contain singleton mentions (in conformance to the OntoNotes guidelines), we consider the union of WCR mentions and all mentions predicted by the method described in (Raghunathan et al., 2010). Overall, we added about 13 400 automatically extracted mentions (singletons) to the 7 000 coreferent mentions annotated in WCR. In the end, our training set consists of 20 362 mentions: 1 334 pronominal ones (627 of them referring to the MC), and 19 028 non-pronominal ones (16% of them referring to the MC)."
    }, {
      "heading" : "6.2 Classifier",
      "text" : "We trained two Support Vector Machine classifiers (Cortes and Vapnik, 1995), one for pronominal mentions and one for non-pronominal ones, making use of the LIBSVM library (Chang and Lin, 2011) and the features described in Section 4.2. For both models, we selected5 the Csupport vector classification and used a linear kernel. Since our dataset is unbalanced (at least for non-pronominal mentions), we penalized the negative class with a weight of 2.0.\nDuring training, we do not use gold mention attributes, but we automatically enrich mentions with the information extracted from Wikipedia and Freebase, as described in Section 4."
    }, {
      "heading" : "6.3 Main Concept Resolution Performance",
      "text" : "We focus on the task of identifying all the mentions referring to the main concept of an article. We measure the performance of the systems we devised by average precision, recall and F1 rates computed by a 10-fold cross-validation procedure.\nThe results of the baselines and our approach are reported in Table 4. Clearly, our approach outperforms all baselines for both pronominal and non-pronominal mentions, and across all metrics. On all mentions, our best classifier yields an absolute F1 increase of 13 points over Dcoref, and 15 points over Scoref.\nIn order to understand the impact of each family of features we considered in this study, we trained various classifiers in a greedy fashion. We started with the simplest feature set (base) and gradually added one family of features at a time, keeping\n5We tried with less success other configurations on a heldout dataset.\nat each iteration the one leading to the highest increase in F1. The outcome of this process for the pronominal mentions is reported in Table 2.\nA baseline that always considers that a pronominal mention is co-referent to the main concept results in an F1 measure of 63.7%. This naive baseline is outperformed by the simplest of our model (base) by a large margin (over 10 absolute points). We observe that recall significantly improves when those features are augmented with the MC coarse attributes (+main). In fact, this variant already outperforms all the Dcoref-based baselines in terms of F1 score. Each feature family added further improves the performance overall, leading to better precision and recall than any of the baselines tested.\nP R F1 always positive 46.70 100.00 63.70\nbase 70.34 78.31 74.11 +main 74.15 90.11 81.35 +position 80.43 89.15 84.57 +tag 82.12 90.11 85.93 +distance 85.46 92.82 88.99\nTable 2: Performance of our approach on the pronominal mentions, as a function of the features.\nInspection shows that most of the errors on pronominal mentions are introduced by the lack of information on noun phrase mentions surrounding the pronouns. In example (f) shown in Figure 3, the classifier associates the mention it with the MC instead of the Johnston Atoll “ Safeguard C ” mission.\nTable 3 reports the results obtained for the nonpronominal mentions classifier. The simplest classifier is outperformed by most baselines in terms of F1. Still, this model is able to correctly match mentions in example (a) and (b) of Figure 3 simply\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nPronominal Non Pronominal All P R F1 P R F1 P R F1\nDcoref B1 64.51 76.55 70.02 70.33 63.09 66.51 67.92 67.77 67.85 B2 76.45 50.23 60.63 83.52 49.57 62.21 80.90 49.80 61.65 B3 76.39 65.55 70.55 83.67 56.20 67.24 80.72 59.45 68.47 B4 71.74 83.41 77.13 74.39 75.59 74.98 73.30 78.31 75.77\nScoref B1 76.59 78.30 77.44 54.66 39.37 45.77 64.11 52.91 57.97 B2 89.59 74.16 81.15 69.90 31.20 43.15 79.69 46.14 58.44 B3 83.91 77.35 80.49 73.17 55.44 63.08 77.39 63.06 69.49 B4 78.48 90.74 84.17 67.51 67.85 67.68 71.68 75.81 73.69\nthis work 85.46 92.82 88.99 91.65 85.88 88.67 89.29 88.30 88.79\nTable 4: Performance of the baselines on the task of identifying all MC coreferent mentions.\nbecause those mentions are frequent within their respective article. Of course, such a simple model is often wrong as in example (c), where all mentions the United States are associated to the MC, simply because this is a frequent mention.\na MC= Anatole France France is also widely believed to be the model for narrator Marcel’s literary idol Bergotte in Marcel Proust’s In Search of Lost Time. b MC= Harry Potter and the Chamber of Secrets Although Rowling found it difficult to finish the book, it won . . . . c MC= Barack Obama On August 31, 2010, Obama announced that the United States* combat mission in Iraq was over. d MC= Houston Texans In 2002, the team wore a patch commemorating their inaugural season... e MC= Houston Texans The name Houston Oilers was unavailable to the expansion team... f MC= Johnston Atoll In 1993 , Congress appropriated no funds for the Johnston Atoll Safeguard C mission , bringing it* to an end. g MC= Houston Texans The Houston Texans are a professional American football team based in Houston* , Texas.\nFigure 3: Examples of mentions (underlined) associated with the MC. An asterisk indicates wrong decisions.\nThe title feature family drastically increases precision, and the resulting classifier (+title) outperforms all the baselines in terms of F1 score. Adding the inferred type feature family gives a further boost in recall (7 absolute points) with no loss in precision (gain of almost 2 points). For instance, the resulting classifier can link the mention the team to the MC Houston Texans (see example (d)) because it correctly identifies the term team as a type. The family name variants also gives a nice boost in recall, in a slight expense of precision. This drop is due to some noisy redirects in Wikipedia, misleading our classifier. For instance, Johnston and Sand Islands is a redirect of the Johnston Atoll article. The entity type family further improves performance, mainly because it plays a role similar to the inferred type features extracted from Freebase. This indicates that the noun type induced directly from the first sentence of a Wikipedia article is pertinent and can complement the types extracted from Freebase when available or serve as proxy when they are missing. Finally, the main family significantly increases precision (over 4 absolute points) with no loss in recall. To illustrate a negative example, the resulting classifier wrongly recognizes mentions referring to the town Houston as coreferent to the football team in example (g). We handpicked a number of classification errors and found that most of these are difficult coreference cases. For instance, our best classifier fails to recognize that the mention the expansion team refers to the main concept Houston Texans in example (e).\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nSystem MUC B3 CEAFφ4 CoNLL P R F1 P R F1 P R F1 F1 Dcoref 61.59 60.42 61.00 53.55 43.33 47.90 42.68 50.86 46.41 51.77 D&K (2013) 68.52 55.96 61.61 59.08 39.72 47.51 48.06 40.44 43.92 51.01 D&K (2014) 63.79 57.07 60.24 52.55 40.75 45.90 45.44 39.80 42.43 49.52 M&S (2015) 70.39 53.63 60.88 60.81 37.58 46.45 47.88 38.18 42.48 49.94 C&M (2015) 69.45 49.53 57.83 57.99 34.42 43.20 46.61 33.09 38.70 46.58 Dcoref++ 66.06 62.93 64.46 57.73 48.58 52.76 46.76 49.54 48.11 55.11\nTable 5: Performance of Dcoref++ on WCR compared to state of the art systems, including in order: Lee et al. (2013); Durrett and Klein (2013) - Final; Durrett and Klein (2014) - Joint; Martschat and Strube (2015) - Ranking:Latent; Clark and Manning (2015) - Statistical mode with clustering."
    }, {
      "heading" : "6.4 Coreference Resolution Performance",
      "text" : "Identifying all the mentions of the MC in a Wikipedia article is certainly useful in a number of NLP tasks (Nguyen et al., 2007; Nakayama, 2008). Finding all coreference chains in a Wikipedia article is worth studying. In the following, we describe an experiment where we introduced in Dcoref a new high-precision sieve which uses our classifier6. Sieves in Dcoref are ranked in decreasing order of precision, and we ranked this new sieve first. The aim of this sieve is to construct the coreference chain equivalent to the main concept. It merges two chains whenever they both contain mentions to the MC according to our classifier. We further prevent other sieves from appending new mentions to the MC coreference chain.\nWe ran this modified system (called Dcoref++) on the WCR dataset, where mentions were automatically predicted. The results of this system are reported in Table 5, measured in terms of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFφ4 (Luo, 2005) and the average F1 CoNLL score (Denis and Baldridge, 2009).\nWe observe an improvement for Dcoref++ over the other systems, for all the metrics. In particular, Dcoref++ increases by 4 absolute points the CoNLL F1 score. This shows that early decisions taken by our classifier benefit other sieves as well. It must be noted, however, that the overall gain in precision is larger than the one in recall."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We developed a simple yet powerful approach that accurately identifies all the mentions that co-refer\n6We use predicted results from 10-fold cross-validation.\nto the concept being described in a Wikipedia article. We tackle the problem with two (pronominal and non-pronominal) models based on well designed features. The resulting system is compared to baselines built on top of state-of-the-art systems adapted to this task. Despite being relatively simple, our model reaches 89 % in F1 score, an absolute gain of 13 F1 points over the best baseline. We further show that incorporating our system into the Stanford deterministic rule-based system (Lee et al., 2013) leads to an improvement of 4% in F1 score on a fully fledged coreference task.\nIn order to allow other researchers to reproduce our results, and report on new ones, we share all the datasets we used in this study. We also provide a dump of all the mentions in English Wikipedia our classifier identified as referring to the main concept, along with information we extracted from Wikipedia and Freebase. This will be available at www.somewhere.country.\nA natural extension of this work is to identify all coreference relations in a Wikipedia article, a task we are currently investigating."
    }, {
      "heading" : "Acknowledgments",
      "text" : ""
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Wikipedia is a resource of choice exploited in many NLP applications, yet we are not aware of recent attempts to adapt coreference resolution to this resource. In this work, we revisit a seldom studied task which consists in identifying in a Wikipedia article all the mentions of the main concept being described. We show that by exploiting the Wikipedia markup of a document, as well as links to external knowledge bases such as Freebase, we can acquire useful information on entities that helps to classify mentions as coreferent or not. We designed a classifier which drastically outperforms fair baselines built on top of state-of-the-art coreference resolution systems. We also measure the benefits of this classifier in a full coreference resolution pipeline applied to Wikipedia texts.",
    "creator" : "TeX"
  }
}